<phrase>Social Media</phrase> as Windows on the Social Life of the Mind This is a programmatic paper, marking out two directions in which the study of <phrase>social media</phrase> can contribute to broader problems of <phrase>social science</phrase>: understanding cultural evolution and understanding collective cogni-tion. Under the first heading, I discuss some difficulties with the usual, adaptationist explanations of cultural phenomena, alternative explanations involving network diffusion effects, and some ways these could be tested using <phrase>social-media</phrase> data. Under the second I describe some of the ways in which <phrase>social media</phrase> could be used to study how the social organization of an <phrase>epistemic</phrase> community supports its collective cognitive performance. Let me begin by considering two 1 senses in which we might speak of human thought as being " social " , and how they might <phrase>orient</phrase> the study of social <phrase>information processing</phrase> and <phrase>social media</phrase>. The first sense is a commonplace of many schools in the <phrase>social sciences</phrase> and <phrase>humanities</phrase>: our thought relies on the cultural transmission of cognitive tools. Every individual thinker, no matter how innovative or even lonely they may be, depends crucially on a vast array of cognitive tools (concepts, procedures, languages, assumptions , values, ...) which they did not devise themselves , and could not have devised for themselves. Instead they inherited these cognitive tools from interacting with other people, who for the most part themselves did not invent them. (Whether this dependence on tradition is a logical necessity, or merely a reflection 1 Of course, people think a lot about their own and others' <phrase>social interactions</phrase>, and a big use of <phrase>social media</phrase> is sharing these thoughts. But in this <phrase>social media</phrase> are no different from any other form of human, or for that matter <phrase>primate</phrase>, association. 2 " [K]nowledge is a function of association and communication ; it depends upon tradition, upon tools and methods socially transmitted, developed and sanctioned. Faculties of effectual observation, reflection and desire are habits acquired under the influence of the <phrase>culture</phrase> and institutions of society, not ready-made inherent powers " of our peculiar <phrase>bounded rationality</phrase> and bounded lifespan , is a deep question, fortunately not relevant here.) While individual thinkers invent and discover, it is nonetheless true that innovations are typically refined, extended and perfected by groups, and that it is very rare indeed for highly developed concepts and ideas to emerge from a single, isolated thinker, rather than from a process of interaction The branches of <phrase>social science</phrase> for which these facts are …
Design and Implementation of Therapeutic Ultrasound Generating Circuit for Dental Tissue Formation and Tooth-Root Healing Biological tissue healing has recently attracted a great deal of research interest in various medical fields. Trauma to teeth, deep and root <phrase>caries</phrase>, and <phrase>orthodontic</phrase> treatment can all lead to various degrees of root resorption. In our previous study, we showed that low-intensity pulsed ultrasound (LIPUS) enhances the growth of lower <phrase>incisor</phrase> apices and accelerates their rate of eruption in <phrase>rabbits</phrase> by inducing dental tissue growth. We also performed clinical studies and demonstrated that LIPUS facilitates the healing of orthodontically induced teeth-root resorption in humans. However, the available LIPUS devices are too large to be used comfortably inside the <phrase>mouth</phrase>. In this paper, the design and implementation of a low-power LIPUS generator is presented. The generator is the core of the final intraoral device for preventing tooth root loss and enhancing tooth root tissue healing. The generator consists of a <phrase>power-supply</phrase> subsystem, an ultrasonic transducer, an <phrase>impedance-matching</phrase> circuit, and an <phrase>integrated circuit</phrase> composed of a digital controller circuitry and the associated driver circuit. Most of our efforts focus on the design of the <phrase>impedance-matching</phrase> circuit and the integrated system-on-chip circuit. The chip was designed and fabricated using 0.8- ¿m high-voltage technology from Dalsa Semiconductor, Inc. The power supply subsystem and its <phrase>impedance-matching</phrase> network are implemented using discrete components. The LIPUS generator was tested and verified to function as designed and is capable of producing ultrasound <phrase>power up</phrase> to 100 mW in the vicinity of the transducer's <phrase>resonance frequency</phrase> at 1.5 MHz. The power efficiency of the circuitry, excluding the power supply subsystem, is estimated at 70%. The final products will be tailored to the exact size of teeth or biological tissue, which is needed to be used for stimulating dental tissue (dentine and <phrase>cementum</phrase>) healing.
Measuring Invariances in <phrase>Deep Networks</phrase> For many <phrase>pattern recognition</phrase> tasks, the ideal input feature would be invariant to multiple <phrase>confounding</phrase> properties (such as illumination and viewing angle, in computer vision applications). Recently, <phrase>deep architectures</phrase> trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, it is difficult to evaluate the learned features by any means other than using them in a classifier. In this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different input transformations. We find that stacked autoencoders learn modestly increasingly <phrase>invariant features</phrase> with depth when trained on <phrase>natural images</phrase>. We find that convolutional <phrase>deep belief networks</phrase> learn substantially more <phrase>invariant features</phrase> in each layer. These results further justify the use of " deep " vs. " shallower " representations , but suggest that mechanisms beyond merely stacking one autoencoder on top of another may be important for achieving invariance. Our evaluation met-<phrase>rics</phrase> can also be used to evaluate future work in <phrase>deep learning</phrase>, and thus help the development of future algorithms.
<phrase>Systems engineering</phrase> principles for the design of biomedical <phrase>signal processing</phrase> systems <phrase>Systems engineering</phrase> aims to produce reliable systems which function according to specification. In this paper we follow a <phrase>systems engineering</phrase> approach to design a biomedical <phrase>signal processing</phrase> system. We discuss requirements capturing, specification definition, implementation and testing of a classification system. These steps are executed as formal as possible. The requirements, which motivate the system design, are based on diabetes research. The main requirement for the classification system is to be a reliable component of a machine which controls diabetes. Reliability is very important, because uncontrolled diabetes may lead to hyperglycaemia (raised <phrase>blood sugar</phrase>) and over a period of time may cause serious damage to many of the body systems, especially the nerves and <phrase>blood vessels</phrase>. In a second step, these requirements are refined into a formal CSP‖ B model. The formal model expresses the system functionality in a clear and semantically strong way. Subsequently, the proven system model was translated into an implementation. This implementation was tested with use cases and failure cases. Formal modeling and automated <phrase>model checking</phrase> gave us deep insight in the system functionality. This insight enabled us to create a reliable and trustworthy implementation. With <phrase>extensive tests</phrase> we established trust in the reliability of the implementation.
Segmentation and Interpretation of <phrase>MR Brain Images</phrase>: An Improved <phrase>Active Shape Model</phrase> This paper reports a novel method for fully automated segmentation that is based on description of shape and its variation using point distribution models (PDM's). An improvement of the active shape procedure introduced by Cootes and Taylor to find new examples of previously learned shapes using PDM's is presented. The new method for segmentation and interpretation of deep <phrase>neuroanatomic structures</phrase> such as thalamus, <phrase>putamen</phrase>, ventricular system, etc. incorporates a priori knowledge about shapes of the <phrase>neuroanatomic structures</phrase> to provide their robust segmentation and labeling in <phrase>magnetic resonance</phrase> (MR) <phrase>brain images</phrase>. The method was trained in eight <phrase>MR brain images</phrase> and tested in 19 <phrase>brain images</phrase> by comparison to observer-defined independent standards. <phrase>Neuroanatomic structures</phrase> in all testing images were successfully identified. Computer-identified and observer-defined <phrase>neuroanatomic structures</phrase> agreed well. The average labeling error was 7%+/-3%. Border positioning errors were quite small, with the average border positioning error of 0.8+/-0.1 pixels in 256 x 256 <phrase>MR images</phrase>. The presented method was specifically developed for segmentation of <phrase>neuroanatomic structures</phrase> in <phrase>MR brain images</phrase>. However, it is generally applicable to virtually any task involving deformable shape analysis.
Multi-language programming with Ada Building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place. Thinking of a project with a high integrity kernel written in Ada, using a set of <phrase>low level</phrase> libraries and drivers written in C or C++, with a <phrase>graphical interface</phrase> done in Java and unit tests driven by <phrase>python</phrase> is not thinking of sciencefiction anymore. It's actual concrete and day-to-day work. Unfortunately, having all of these technologies talking to each other is not straightforward, and often requires a deep knowledge of both sides of the technology and extensive manual work.  In this tutorial, we'll first study how to interface directly Ada with native languages, such as C or C++. We'll then have a deep look at communications with languages running on <phrase>virtual machines</phrase>, such as Java, <phrase>Python</phrase> and the .<phrase>NET framework</phrase>. Finally, we'll see how Ada can be interfaced with an arbitrary language using a <phrase>middleware</phrase> solution, such as <phrase>SOAP</phrase> or <phrase>CORBA</phrase> We?ll see how the communication can be manually done using <phrase>low level</phrase> features and APIs, and how a substantial part of this process can be automated using <phrase>high level</phrase> binding generators.
Aging in Language Dynamics Human languages evolve continuously, and a puzzling problem is how to reconcile the apparent robustness of most of the <phrase>deep linguistic</phrase> structures we use with the evidence that they undergo possibly slow, yet ceaseless, changes. Is the state in which we observe languages today closer to what would be a dynamical <phrase>attractor</phrase> with statistically stationary properties or rather closer to a non-<phrase>steady state</phrase> slowly evolving in time? Here we address this question in the framework of the emergence of shared linguistic categories in a population of individuals interacting through language games. The observed emerging asymptotic categorization, which has been previously tested--with success--against <phrase>experimental data</phrase> from human languages, corresponds to a <phrase>metastable</phrase> state where global shifts are always possible but progressively more unlikely and the response properties depend on the age of the system. This aging mechanism exhibits striking quantitative analogies to what is observed in the <phrase>statistical mechanics</phrase> of glassy systems. We argue that this can be a general scenario in language dynamics where shared linguistic conventions would not emerge as attractors, but rather as <phrase>metastable</phrase> states.
Ultra-<phrase>low-voltage</phrase> Robust Design Issues in Deep- Submicron Cmos Abstracf-Design challenges for operating <phrase>CMOS circuits</phrase> fabricated in 0.13pm and liner technologies at <phrase>ultra-low</phrase>-voltages are analyzed. The design goal consists in minimizing energy by reducing V,, while maintaining delay and yield at acceptable levels in the presence of increasing variability of process parameters. First, an estimation model developed to accurately predict operation of bulk-and SOI-CMOS in subthreshold is described. The relation between yield, energy, delay and device parameter distributions is examined next along with tradeoNs necessary to achieve the desired performance point. The main objective of minimizing energy is explored for S U M cells by predicting the minimum V,, based on the <phrase>data-retention</phrase> voltage, DRV, and, acceptable <phrase>signal-to-noise</phrase> margins, SNM. <phrase>Experimental data</phrase> from a 4kB-SR4M <phrase>test chip</phrase> in 0.13pm CMOS are presented demonstrating a 90% leakage <phrase>reduction potential</phrase> in standby under reduced bias of 250mV.
Can we really do without the support of <phrase>formal methods</phrase> in the verification of large designs? 1. The <phrase>mystery</phrase> question From the IC industry's standpoint, the incubation of <phrase>formal methods</phrase> for deployment in EDA verification flows has been very long and is still occurring. <phrase>Formal methods</phrase> applied at functional verification have interpreted different roles-e.g. are they good for proving correctness or for catching bugs in deep behavioral corner cases-have played with different techniques-e.g. BDD's, SAT, ATPG, symbolic-and finally have federated with simulation for the purpose of achieving coverage closure. Further, in the last 10 years a <phrase>fair</phrase> number of start-up's have emerged, that have been acquired by major vendors in the meantime, and new start-up's have been appearing also this year. What are the reasons that make the IC industry to accept an unusually long maturation period of the formal methodology & tools and the EDA vendors to put money in the basket of their developments ? 2. Whose affair is this ? I lead a group devoted to investigating new verification techniques for System on Chip (SoC) and System in Package (SiP). The general approach of the design team leaders is to allocate a significant part of the verification budget to Random <phrase>Test Pattern Generation</phrase> (RTPG) driven simulation, by the use of popular Testbench Automation tools, but then sometimes one design team leader fears exposure to unexpected bugs, for a number of different reasons: the team has pioneered a new design configuration never tried before, i.e. it is not possible to find an existing Verification IP (VIP) already proven and working; the design and the verification teams are distributed in the company organization and geographically far away from each others-multiple design teams and multiple verification teams; total quality commitment is becoming ever more demanding in terms of defectiveness margins and some actions have to be taken to improve the verification flow. Very often the budget to <phrase>formal verification</phrase> is not planned in advance: usually <phrase>formal verification</phrase> is asked to clear uncertainty areas but the different usage options with respect to simulation are not clearly perceived. In <phrase>STMicroelectronics</phrase>, the verifications of a SoC or a SiP are becoming everyday more challenging. At the 90nm and 65nm <phrase>technology nodes</phrase>, hundred million transistors will be implemented in few tenth square millimeter of silicon. Even considering that a fraction of the chip will be occupied by sparse logic, tens of million gates RTL is foreseeable in the next years: no one is going to design such an RTL from …
Genetic fuzzy classifier for sleep stage identification <phrase>Soft-computing</phrase> techniques are commonly used to detect medical phenomena and help with clinical diagnoses and treatment. In this work, we propose a design for a computerized sleep scoring method, which is based on a fuzzy classifier and a genetic algorithm (GA). We design the fuzzy classifier based on the GA using a single electroencephalogram (EEG) signal that detects differences in spectral features. <phrase>Polysomnography</phrase> was performed on four healthy young adults (males with a mean age of 27.5 years). The sleep classifier was designed using a sleep record and tested on the sleep records of the subjects. Our results show that the genetic fuzzy classifier (GFC) agreed with visual sleep staging approximately 84.6% of the time in detection of wakefulness (WA), shallow sleep (<phrase>SS</phrase>), <phrase>deep sleep</phrase> (DS), and <phrase>rapid eye movement</phrase> (REM) stages.
On-chip delay measurement for <phrase>silicon debug</phrase> Efficient test and debug techniques are indispensable for performance characterization of large complex <phrase>integrated circuits</phrase> in <phrase>deep-submicron</phrase> and <phrase>nanometer technologies</phrase>. Performance characterization of such chips requires on-chip hardware and efficient debug schemes in order to reduce time to market and ensure shipping of chips with lower defect levels. In this paper we present an on-chip scheme for delay fault detection and performance characterization. The proposed technique allows for accurate measurement of delays of speed paths for speed binning and facilitates a systematic and efficient test and debug scheme for <phrase>delay faults</phrase>. The <phrase>area overhead</phrase> associated with the proposed technique is very low.
<phrase>High-Dimensional</phrase> Probability Estimation with Deep Density Models One of the fundamental problems in <phrase>machine learning</phrase> is the estimation of a <phrase>probability distribution</phrase> from data. Many techniques have been proposed to study the structure of data, most often building around the assumption that observations lie on a lower-dimensional <phrase>manifold</phrase> of high probability. It has been more difficult, however, to exploit this insight to build explicit, tractable density models for <phrase>high-dimensional data</phrase>. In this paper, we introduce the deep density model (DDM), a new approach to <phrase>density estimation</phrase>. We exploit insights from <phrase>deep learning</phrase> to construct a <phrase>bijective</phrase> map to a representation space, under which the transformation of the distribution of the data is approximately factorized and has identical and known marginal densities. The simplicity of the latent distribution under the model allows us to feasibly explore it, and the invertibility of the map to characterize contraction of measure across it. This enables us to compute normalized densities for out-of-sample data. This combination of tractability and flexibility allows us to tackle a variety of probabilistic tasks on <phrase>high-dimensional</phrase> datasets, including: rapid computation of normalized densities at test-time without evaluating a partition function; generation of samples without MCMC; and characterization of the joint entropy of the data.
Deep <phrase>Convolutional Neural Networks</phrase> for Smile Recognition Declaration I herewith certify that all material in this report which is not my own work has been properly acknowledged. Abstract This thesis describes the design and implementation of a smile detector based on deep <phrase>convolutional neural networks</phrase>. It starts with a summary of <phrase>neural networks</phrase>, the difficulties of training them and new training methods, such as Restricted Boltzmann Machines or autoencoders. It then provides a <phrase>literature review</phrase> of <phrase>convolutional neural networks</phrase> and <phrase>recurrent neural networks</phrase>. In order to select databases for smile recognition, comprehensive statistics of databases popular in the field of <phrase>facial expression recognition</phrase> were generated and are summarized in this thesis. It then proposes a model for smile detection, of which the main part is implemented. The experimental results are discussed in this thesis and justified based on a comprehensive <phrase>model selection</phrase> performed. All experiments were run on a <phrase>Tesla</phrase> K40c GPU benefiting from a speedup of up to factor 10 over the computations on a CPU. A smile detection test accuracy of 99.45% is achieved for the <phrase>Denver</phrase> Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches with accuracies ranging from 65.55% to 79.67%. This experiment is <phrase>rerun</phrase> under various variations, such as retaining less neutral images or only the low or high intensities, of which the results are extensively compared. 3 First and foremost I offer my sincerest gratitude to my supervisor Dr. Stavros PETRIDIS who has supported me throughout my thesis with his enthusiasm, patience and expertise. I would also like to thank Professor Maja PANTIC for her passion, setting the direction of this thesis and valuable regular feedback.
Crosstalk driven routing resource assignment <phrase>Crosstalk noise</phrase> is one of the emerging issues in deep sub-micrometer technology which causes many undesired effects on the <phrase>circuit performance</phrase>. In this paper, a CDRRA algorithm, which integrates the routing layers and tracks to address the <phrase>crosstalk noise</phrase> issue during the track/layer assignment stage, is proposed. The CDRRA problem is formulated as a weighted <phrase>bipartite</phrase> matching problem and solved using the linear assignment algorithm. The crosstalk risks between nets are represented by an undirected graph and the maximum number of the concurrent crosstalk risking nets is computed as the max-clique of the graph. Then the nets in each max-clique are assigned to disadjacent tracks. Thus the <phrase>crosstalk noise</phrase> can be avoided based on the clique concept. The algorithm is tested by a set of bench mark examples and the experimental results show that it can improve the final routing layout a lot with little loss of the completion rate.
An On-chip <phrase>Esd Protection</phrase> Circuit with Low Trigger Voltage in Bicmos Technology —A novel low-trigger dual-direction on-chip electro-static discharge (ESD) protection circuit is designed to protect <phrase>integrated circuits</phrase> (ICs) against ESD surges in two opposite directions. The compact <phrase>ESD protection</phrase> circuit features low triggering voltage (7.5 V), short response time (0.18–0.4 ns), symmetric deep-snap-back – characteristics, and low on-resistance (). It passed the 14-kV <phrase>human body</phrase> model (HBM) ESD test and is very area efficient (80 V/ m width). The new <phrase>ESD protection</phrase> design is particularly suitable for <phrase>low-voltage</phrase> or multiple-<phrase>power-supply</phrase> IC chips.
Taming Complexity at MAYA Design MAYA Design is a full-service <phrase>product design</phrase> consultancy offering services at the intersection of <phrase>computer science</phrase>, psychology, and visual design. We have developed efficient techniques for facilitating interdisciplinary design and for communicating clearly with our clients. DELIVERING USABILITY TO CONSUMERS There is a significant disparity between the state of the art in academic usability design and the average level of usability delivered to consumers. Despite a clear understanding of how to improve usability, hopelessly confusing technology products continue to hit the market daily. There is a solid consensus that interdisciplinary collaboration can vastly improve the usability and marketability of complex devices. There remains , however, a scarcity of commercial venues for interdis-ciplinary, <phrase>user-centered design</phrase>. With the exception of a few superb in-<phrase>house</phrase> groups <phrase>at large</phrase> technology firms, there are very few opportunities in industry for careers that focus on meaningful interdisciplinary work. As a result, the best and brightest tend to gravitate toward academic work – even if their interests are fundamentally oriented toward professional practice rather than research, MAYA Design Group was founded in 1989 to address this problem. A spin-off of <phrase>Carnegie Mellon University</phrase>, MAYA was modeled after the great <phrase>industrial design</phrase> consultancies of the 1930s and 1940s. But it was built from the ground up as a place where engineers, <phrase>human-factors</phrase> specialists, graphic designers, and industrial designers could practice their disciplines in deep collaboration (Figure 1). This interdisciplinary collaboration allows MAYA to deliver a wide range of strategic , user-centered <phrase>product-design</phrase> services to the technology industries. <phrase>Cognitive Psychology</phrase> / / Figure 1: Deep collaboration among disciplines is MAYA's recipe for taming complexity. MAYA specializes in making complicated computer-based products easier for average people to use. Our goal is to " tame complexity, " not necessarily to eliminate it. Our 23-member staff is more-or-less equally distributed among the three disciplines of engineering and computer sciencq <phrase>human factors</phrase> and <phrase>cognitive psychology</phrase>; and industrial and <phrase>graphic design</phrase>. We work closely with a diverse group of clients, primarily in the high-technology industries, augmenting their in-<phrase>house</phrase> capabilities with our areas of expertise. We encourage clients to view MAYA as a " virtual employee " whom they can rely on for assistance throughout the whole product development cycle, which includes strategic planning , iterative design, <phrase>rapid prototyping</phrase>, user testing, and transition to manufacturing. Even when a project is focused on only one or two of our core competencies, our clients benefit from the …
Dropout: a simple way to prevent <phrase>neural networks</phrase> from overfitting <phrase>Deep neural nets</phrase> with a large number of parameters are very powerful <phrase>machine learning</phrase> systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large <phrase>neural nets</phrase> at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of <phrase>neural networks</phrase> on <phrase>supervised learning</phrase> tasks in vision, <phrase>speech recognition</phrase>, <phrase>document classification</phrase> and <phrase>computational biology</phrase>, obtaining state-of-the-art results on many benchmark <phrase>data sets</phrase>.
<phrase>Iddq Testing</phrase> for Cmos Vlsi It is little more than 15-years since the idea of <phrase>Iddq testing</phrase> was first proposed. Many semiconductor companies now consider <phrase>Iddq testing</phrase> as an integral part of the overall testing for all IC's. This paper describes the present status of <phrase>Iddq testing</phrase> along with the essential items and necessary data related to <phrase>Iddq testing</phrase>. As part of the introduction, a historical background and discussion is given on why this <phrase>test method</phrase> has drawn attention. A section on <phrase>physical defects</phrase> with in-depth discussion and examples is used to illustrate why a <phrase>test method</phrase> outside the voltage environment is required. Data with <phrase>additional information</phrase> from <phrase>case studies</phrase> is used to explain the effectiveness of <phrase>Iddq testing</phrase>. In Section IV, design issues, design styles, <phrase>Iddq test</phrase> <phrase>vector generation</phrase> and simulation methods are discussed. The concern of whether <phrase>Iddq testing</phrase> will remain useful in <phrase>deep submicron technologies</phrase> is addressed (Section V). The use of <phrase>Iddq testing</phrase> for reliability screening is described (Section VI). The current measurement methods for <phrase>Iddq testing</phrase> are given (Section VII) followed by comments on the <phrase>economics</phrase> of <phrase>Iddq testing</phrase> (Section VIII). In Section IX pointers to some <phrase>recent research</phrase> are given and finally, concluding remarks are given in Section X. Keywords—Burn-in, current measurement, current sensor, current testing, <phrase>deep sub-micron technology</phrase>, <phrase>design-for-test</phrase>, <phrase>fault diagnosis</phrase> , <phrase>fault models</phrase>, IC testing, <phrase>Iddq testing</phrase>, <phrase>physical defects</phrase>, reliability screening, reliability testing, semiconductor testing, simulation , system-on-a-chip testing, test <phrase>economics</phrase>, test effectiveness, <phrase>test vectors</phrase>.
Performance Analysis of a Hybrid Branch Predictor Nowadays, one of the main concerns of computer <phrase>architects</phrase> is choosing an accurate branch prediction scheme. The gravity of the issue is due to the deep pipeline architectures of contemporary processors. It is reflected in the range and variety of the related research which contains many studies of various prediction schemes and in depth analysis of their performance. The aim of this paper is to study a new hybrid scheme, <phrase>PYTHIA</phrase>, based on a set of tested and popular branch predictors. The main idea is to combine different predictors which perform better for different patterns of branch behaviors, measure the <phrase>PYTHIA</phrase> performance and compare it with each involved standalone branch predictor using the same benchmarks. The basic difference of <phrase>PYTHIA</phrase> in comparison to other combined branch predictors is that polling for the best predictor is performed, using <phrase>shared memory</phrase>, during a trial period and not on the entire program execution. Afterwards, the most accurate scheme is kept and the others are deactivated. In that way, we choose the best scheme for the specific branch pattern and additionally we can gain from the resources released by the rejected predictors.
Promoting vicarious learning of physics using deep questions with explanations Two experiments explored the role of vicarious " self " explanations in facilitating student learning gains during computer-presented instruction. In Exp. 1, college students with low or high knowledge on Newton's laws were tested in four conditions: (a) <phrase>monologue</phrase> (M), (b) questions (Q), (c) explanation (E), and (d) question þ explanation (Q þ E). Those with low pre-experimental knowledge levels showed marginally significant yet consistently greater gains than those with <phrase>high levels</phrase> and condition Q þ E outperformed the other three (M, Q, E). Among those with high knowledge, the Q þ E presentations actually inhibited learning. In Exp. 2, <phrase>high school</phrase> physics students in standard and honors classes were studied during their introduction to Newton's laws. Brief (12 min) computer videos that introduced key <phrase>Newtonian</phrase> concepts preceded teacher presentations in seven daily sessions. Both standard and honors students who received Q þ E presentations prior to regular classroom activities learned more in daily sessions than those who received either M or Q presentations. It was concluded that when key concepts are introduced in the context of deep questions along with explanations new learning was facilitated both in vicarious environments and in subsequent standard classroom activities.
Intuitive strategy for parameter setting in <phrase>video segmentation</phrase> In this paper, we propose an original framework for an intuitive tuning of parameters in image and <phrase>video segmentation</phrase> algorithms. The proposed framework is very flexible and generic and does not depend on a specific <phrase>segmentation algorithm</phrase>, a particular evaluation metric, or a specific optimization approach, which are the three main components of its <phrase>block diagram</phrase>. This framework requires a manual segmentation input provided by a human operator as he/she would have performed intuitively. This input allows the framework to search for the optimal set of parameters which will provide results similar to those obtained by manual segmentation. On one hand, this allows researchers and designers to quickly and automatically find the best parameters in the segmentation algorithms they have developed. It helps them to better understand the degree of importance of each parameter's value on the final segmentation result. It also identifies the potential of the <phrase>segmentation algorithm</phrase> under study in terms of best possible performance level. On the other hand, users and operators of systems with segmentation components, can efficiently identify the optimal sets of parameters for different classes of images or video sequences. In a large extent, this optimization can be performed without a deep understanding of the underlying algorithm, which would facilitate the exploitations and optimizations in real applications by non-experts in segmentation. A specific implementation of the proposed framework was obtained by adopting a <phrase>video segmentation</phrase> algorithm invariant to shadows as segmentation component, a full reference segmentation quality metric based on a perceptually motivated spatial context, as the evaluation component, and a <phrase>downhill</phrase> <phrase>simplex</phrase> method, as optimization component. <phrase>Simulation results</phrase> on various <phrase>test sequences</phrase>, covering a representative set of indoor and ourdoor video, show that optimal set of parameters can be obtained efficiently and largely improve the <phrase>results obtained</phrase> when compared to a simple implementation of the same <phrase>segmentation algorithm</phrase> with <phrase>ad-hoc</phrase> parameter setting strategy.
IDDT <phrase>Test Methodologies</phrase> for Very Deep Sub-micron <phrase>CMOS Circuits</phrase> In this paper, we investigate three i DDT-based <phrase>test methodologies</phrase>, Double Threshold i DDT , Delta i DDT , and Delayed i DDT , and we compare their effectiveness in the detection of defects in very deep sub-micron random logic circuits. The target defects are <phrase>resistive opens</phrase> and resistive bridges. We present preliminary <phrase>simulation results</phrase> of 49 defects to study the defect sensitivity of each of the three <phrase>test methods</phrase>. This paper reports our <phrase>preliminary results</phrase> on these three <phrase>test methods</phrase> using a relatively small transistor-level sample circuit, and is not intended to imply any feasibility in a production environment. The <phrase>test methods</phrase> presented herein are the subject of a current <phrase>invention</phrase> <phrase>disclosure</phrase>.
Critical Fault-Based <phrase>Pattern Generation</phrase> for Screening SDDs — Testing for <phrase>small-delay defects</phrase> (SDDs) becomes necessary as technology further scales. Traditional timing-unaware transition-<phrase>delay fault</phrase> (TDF) ATPGs are not adequate for detecting SDDs due to sensitization of short paths. <phrase>Timing-aware</phrase> ATPGs suffer from multiple paths sensitization limitation and significant <phrase>test cost</phrase>. In this paper, we present a critical fault-based methodology to generate <phrase>high-quality</phrase> SDD patterns. By focusing on critical faults, <phrase>high quality</phrase> original pattern repository could be generated applicably with í µí±-detect ATPG. Novel pattern evaluation and selection method is presented to further minimize <phrase>pattern count</phrase> while maintaining the SDD detection ability. Finally, top-off ATPG is performed to ensure meeting the target <phrase>fault coverage</phrase>. <phrase>Experimental results demonstrate</phrase> that the proposed critical fault-<phrase>based method</phrase> improves <phrase>long path</phrase> sensitization efficiency by 2.5X and saves approximately 80% CPU runtime compared with total fault-<phrase>based method</phrase>. Comparing with <phrase>timing-aware ATPG</phrase>, our pattern set detects equivalent or even more SDDs with significantly reduced <phrase>pattern count</phrase>. I. INTRODUCTION As manufacturing technology scales, the fabricated chips become more vulnerable to <phrase>timing-related defects</phrase>. <phrase>Semiconductor industry</phrase> increasingly relies on <phrase>delay fault test</phrase> for higher <phrase>defect coverage</phrase>. <phrase>Small-delay</phrase> defect (SDD) only introduces a small amount of extra delay to the design making it very difficult to detect. A SDD detected on a short path may not fail the test but is highly possible to cause a failure when it gets sensitized on a <phrase>long path</phrase> in the filed. Therefore, SDDs require a serious consideration for ensuring product quality and in-field reliability in very deep-submicron regime [1]. Traditional TDF ATPG is developed to target gross <phrase>delay defects</phrase> thus has a limited ability in meeting the high SDD <phrase>test coverage</phrase> requirement in practice. Commercial <phrase>timing-aware ATPG</phrase> tools [2] [3] have been developed to address the deficiencies of the traditional TDF ATPGs. With timing information, the <phrase>timing-aware ATPG</phrase> can target a fault along the path with minimum timing slack. However, it lacks ability in sensitizing multiple long paths through a fault. In addition, it consumes large storage and runtime limit in practice. í µí±-detect ATPG can be considered as an alternative for SDD detection [4] [5]. The í µí±-detect ATPG tries to generate patterns to detect each fault í µí± times via different paths. However, it is also limited by large <phrase>pattern count</phrase>. To improve the effectiveness of test patterns for screening SDDs, several techniques have been proposed in literature. 1. <phrase>Path delay fault</phrase> (PDF)-<phrase>based methods</phrase> [6] [7] [8], which concentrate on identifying …
Efficient on-line testing of FPGAs with provable diagnosabilities We present novel and <phrase>efficient methods</phrase> for on-line testing in FPGAs. The testing approach uses a ROving TEster (ROTE), which has provable diagnosabilities and is also faster than prior FPGA <phrase>testing methods</phrase>. We present 1- and 2-diagnosable built-in self-tester (BISTer) designs that make up the ROTE, and that avoid expensive adaptive diagnosis. To the best of our knowledge, this is the first time that a BISTer design with diagnosability greater than one has been developed for FPGAs. We also develop <phrase>functional testing</phrase> methods that test PLBs in only two circuit functions that will be mapped to them (as opposed to testing PLBs in all their operational modes) as the ROTE moves across a functioning FPGA. Simulation results show that our 1-diagnosable BISTer and our <phrase>functional testing</phrase> technique leads to significantly more accurate (98% <phrase>fault coverage</phrase> at a fault/defect density of 10%) and faster test-and-diagnosis of FPGAs than achieved by previous work. The fault coverage of ROTE is also expected to be high at fault/defect densities of up to 25% using our 1-diagnosable BISTer and up to 33% using our 2-diagnosable BISTer. Our methods should thus prove useful for testing current very deep submicron FPGAs as well as future nano-CMOS and <phrase>molecular nanotechnology</phrase> FPGAs in which defect densities are expected to be in the 10% range.
‘<phrase>Case Study</phrase>: An FPSO with DP for <phrase>Deep Water</phrase> Applications’ In the coming years, there will be a growing demand for Floating Production and Storage Units (FPSOs) for ultra <phrase>deep waters</phrase> (greater than 2000 m [6,000 feet]). In the <phrase>Gulf of Mexico</phrase>, the technical and economical limitations inherent to other type of concepts, the lack of pipeline infrastructure in such deep areas, and the wide acceptance of the FPSO concept by Shelf Authorities will accelerate this process. One of the most critical issues in the design of FPSOs for ultra <phrase>deep waters</phrase> will be the selection of the most cost-efficient station keeping system for the specified operational requirements. Standard solutions based on internal <phrase>turret</phrase> and thruster assisted mooring systems are already being offered by the industry. However, beyond certain water depths, the technical and economical constraints associated with the use of mooring systems may favour other concepts potentially more attractive and cost-efficient, such as a fully dynamically positioned FPSO (DP-FPSO). This concept marries state-of-the-art FPSO technology and latest generation drill ship technology for <phrase>dynamic positioning</phrase> and operation in ultra <phrase>deep waters</phrase>. This system can either be utilised as an early production system or as a full-fledged field development solution. The areas most suited for this application will be the <phrase>Gulf of Mexico</phrase>, <phrase>Brazil</phrase> and <phrase>West Africa</phrase>. The paper describes a joint study undertaken by the various companies represented by the authors to develop a design for a fully dynamically positioned FPSO for ultra <phrase>deep waters</phrase>. The various technical challenges and regulatory issues for a fully DP-FPSO will be identified and solutions to them provided. Detailed design information on the vessel design, the DP thruster, Power Generation and control systems, and the disconnectable <phrase>turret</phrase> and riser system shall be provided for a hypothetical field development in ultra <phrase>deep water</phrase>. The system performance shall be illustrated by the results from a comprehensive study involving state-of-the-art computer simulations and model <phrase>test program</phrase>. Results and conclusions from a reliability and safety study performed on the system shall also be presented, as well as those from a thorough <phrase>power consumption</phrase> analysis for the geographical areas of interest. Finally, a comparison is made between components associated with the stationkeeping system of the DP-FPSO and a conventionally <phrase>turret</phrase> moored FPSO, to provide input for CAPEX/OPEX estimates which in turn can be used to identify the range of water depths and field development scenarios for which the DP-FPSO is commercially feasible.
Accurate, Dense, and Robust Multi-View <phrase>Stereopsis</phrase> This paper proposes a novel algorithm for multiview <phrase>stereopsis</phrase> that outputs a dense set of small rectangular patches covering the surfaces visible in the images. <phrase>Stereopsis</phrase> is implemented as a match, expand, and filter procedure, starting from a sparse set of matched keypoints, and repeatedly expanding these before using visibility constraints to filter away false matches. The keys to the performance of the proposed algorithm are effective techniques for enforcing local photometric consistency and global visibility constraints. Simple but effective methods are also proposed to turn the resulting patch model into a mesh which can be further refined by an algorithm that enforces both photometric consistency and regularization constraints. The proposed approach automatically detects and discards outliers and obstacles and does not require any initialization in the form of a visual <phrase>hull</phrase>, a bounding box, or valid depth ranges. We have tested our algorithm on various <phrase>data sets</phrase> including objects with fine surface details, deep concavities, and thin structures, <phrase>outdoor scenes</phrase> observed from a restricted set of viewpoints, and "crowded" scenes where moving obstacles appear in front of a static structure of interest. A quantitative evaluation on the <phrase>Middlebury</phrase> benchmark shows that the proposed method outperforms all others submitted so far for four out of the six <phrase>data sets</phrase>.
Randomized directed testing (REDIRECT) for <phrase>Simulink</phrase>/Stateflow models The <phrase>Simulink</phrase>/Stateflow (SL/SF) environment from Math-works is becoming the <i>de facto</i> standard in industry for <phrase>model based development</phrase> of embedded control systems. Many commercial tools are available in the market for <phrase>test case</phrase> generation from SL/SF designs; however, we have observed that these tools do not achieve satisfactory coverage in cases when designs involve nonlinear blocks and Stateflow blocks occur <phrase>deep inside</phrase> the <phrase>Simulink</phrase> blocks.  The recent past has seen the emergence of several novel techniques for testing large C, C++ and Java programs; prominent among them are directed automated <phrase>random testing</phrase> (DART), hybrid <phrase>concolic testing</phrase> and feedback-directed <phrase>random testing</phrase>. We believe that some of these techniques could be lifted to testing of SL/SF based designs; REDIRECT (RandomizEd DIRECted Testing), the proposed testing method of this paper, is an attempt towards this direction. Specifically, REDIRECT uses a careful combination of the above techniques, and in addition, the method uses a set of pattern-guided heuristics for tackling nonlinear blocks. A prototype tool has been developed and the tool has been applied to many industrial strength <phrase>case studies</phrase>. Our experiments indicate that a careful choice of heuristics and certain combinations of random and directed testing achieve better coverages as compared to the existing commercial tools. <sup>1</sup>
Adaptive <phrase>Autonomous Navigation</phrase> of <phrase>Mobile Robots</phrase> in Unknown Environments Adaptive <phrase>Autonomous Navigation</phrase> of <phrase>Mobile Robots</phrase> in Unknown Environments <phrase>Autonomous navigation</phrase> of a <phrase>mobile robot</phrase> is a challenging task. Much work has been done in indoor navigation in the last decade. Fewer results have been obtained in outdoor robotics. Since the early 90's, the Global Positioning System (GPS) has been the main navigation system for ships and aircrafts. In open fields, <phrase>satellite navigation</phrase> gives absolute position accuracy. The absolute heading information is also obtained by <phrase>satellite navigation</phrase> when the <phrase>mobile robot</phrase> is in motion. However, the use of GPS <phrase>satellite navigation</phrase> is mainly restricted to open areas where at least three satellites can be seen. For example, <phrase>mobile robots</phrase> working in underground or deep open mines cannot use <phrase>satellite navigation</phrase> at all, and in forest or city areas, there are serious limitations to its use. <phrase>Laser range</phrase> finder technology has evolved remarkably over the last decade, and offers a <phrase>fast and accurate</phrase> method for environment modeling. Furthermore, it can be used to define robot position and heading relative to the environment. It is obvious that the use of several alternative sensors according to the environment will make the navigation system more flexible. <phrase>Laser range</phrase> finder technology is particularly suitable for indoors or feature rich outdoor environments. The goal of this thesis is to develop a multi sensor navigation system for unknown outdoor environments, and to verify the system with a service robot. Navigation should be possible in unstructured outdoors as well as indoor environments. The system should use all available sensor information and emphasize those that best suit the particular environment. The sensors considered in this thesis include a scanning <phrase>laser range</phrase> finder, a <phrase>GPS receiver</phrase>, and a heading gyro. The main contribution of the thesis is a flexible navigation system developed and tested with a service robot performing versatile tasks in an outdoor environment. The used range matching method is novel and has not been verified earlier in outdoor environments. No unique solution can be guaranteed in the developed map <phrase>matching algorithm</phrase>, although it seems to work well in the practical tests. Position and heading errors grow without bound in successive map matchings, which could be referred to as laser odometry. Therefore, the position and heading have been corrected by means of global matching when the robot returns to a place it has previously visited. Alternatively, structured landmarks have been used for position and heading correction. In <phrase>field tests</phrase>, tree trunks and walls have been used as structured …
<phrase>Rank Aggregation</phrase> for Similar Items The problem of combining the ranked preferences of many experts is an old and surprisingly deep problem that has gained renewed importance in many <phrase>machine learning</phrase>, <phrase>data mining</phrase>, and <phrase>information retrieval</phrase> applications. Effective <phrase>rank aggregation</phrase> becomes difficult in <phrase>real-world</phrase> situations in which the rankings are noisy, incomplete, or even disjoint. We address these difficulties by extending several standard methods of <phrase>rank aggregation</phrase> to consider similarity between items in the various ranked lists, in addition to their rankings. The intuition is that similar items should receive similar rankings, given an appropriate measure of similarity for the domain of interest. In this paper, we propose several algorithms for merging ranked lists of items with defined similarity. We establish evaluation criteria for these algorithms by extending previous definitions of distance between ranked lists to include the role of similarity between items. Finally, we test these new methods on both synthetic and <phrase>real-world</phrase> data, including data from an application in keywords expansion for sponsored search advertisers. Our results show that incorporating similarity knowledge within <phrase>rank aggregation</phrase> can significantly improve the performance of several standard <phrase>rank aggregation</phrase> methods, especially when used with noisy, incomplete, or disjoint rankings.
Evaluation of <phrase>global and local</phrase> training techniques over <phrase>feed-forward neural network</phrase> architecture spaces for <phrase>computer-aided</phrase> <phrase>medical diagnosis</phrase> In most cases authors are permitted to post their version of the article (e.g. in Word or Tex form) to their personal website or <phrase>institutional repository</phrase>. Authors requiring further information regarding Elsevier's archiving and <phrase>manuscript</phrase> policies are encouraged to visit: a b s t r a c t In this paper, we investigate the performance of global vs. local techniques applied to the training of <phrase>neu</phrase>-ral network classifiers for solving <phrase>medical diagnosis</phrase> problems. The presented methodology of the investigation involves systematic and exhaustive evaluation of the classifier performance over a <phrase>neural network architecture</phrase> space and with respect to training depth for a particular problem. In this study, the architecture space is defined over <phrase>feed-forward</phrase>, <phrase>fully-connected</phrase> <phrase>artificial neural networks</phrase> (ANNs) which have been widely used in <phrase>computer-aided</phrase> <phrase>decision support systems</phrase> in medical domain, and for which two popular <phrase>neural network</phrase> training methods are explored: conventional backpropagation (BP) and <phrase>particle swarm optimization</phrase> (PSO). Both training techniques are compared in terms of classification performance over three <phrase>medical diagnosis</phrase> problems (<phrase>breast cancer</phrase>, <phrase>heart disease</phrase>, and diabetes) from Pro-ben1 benchmark dataset and computational and architectural analysis are performed for an extensive assessment. The results clearly demonstrate that it is not possible to compare and evaluate the performance of the two algorithms over a single network and with a fixed set of training parameters, as most of the earlier work in this field has been carried out, since <phrase>training and test</phrase> classification performances vary significantly and depend directly on the <phrase>network architecture</phrase>, the training depth and method used and the available dataset. We, therefore, show that an extensive evaluation method such as the one proposed in this paper is basically needed to obtain a reliable and detailed <phrase>performance assessment</phrase>, in that, we can conclude that the PSO algorithm has usually a better <phrase>generalization ability</phrase> across the architecture space whereas BP can occasionally provide better training and/or test classification performance for some network configurations. Furthermore, we can in general say that the PSO, as a global training algorithm, is capable of achieving minimum test classification errors regardless of the training depth, i.e. shallow or deep, and its average classification performance shows less variations with respect to <phrase>network architecture</phrase>. In terms of <phrase>computational complexity</phrase>, BP is in general superior to PSO for the entire architecture space used. <phrase>Artificial neural networks</phrase> (ANNs) are known as ''universal approximators " and ''computational models " with particular characteristics such as the ability …
UDSM (ultra-<phrase>deep sub-micron</phrase>)-aware post-layout <phrase>power optimization</phrase> for ultra <phrase>low-power</phrase> CMOS VLSI In this paper, we propose an efficient approach to minimize total power (switching, <phrase>short-circuit</phrase>, and <phrase>leakage power</phrase>) without performance loss for ultra-<phrase>low power</phrase> <phrase>CMOS circuits</phrase> in <phrase>nanometer technologies</phrase>. We present a framework for combining supply/<phrase>threshold voltage</phrase> scaling, gate sizing, and interconnect scaling techniques for <phrase>power optimization</phrase> and propose an efficient heuristic algorithm which ensures that the total slack budget is maximal and the total power is minimal in the presence of back end (post-layout-based) UDSM effects. We have tested the proposed algorithms on a set of <phrase>benchmark circuits</phrase> and some building blocks of a synthesizable ARM core. The experimental results show that our polynomial-time solvable strategy delivers over an <phrase>order of magnitude</phrase> savings in total power without compromising performance.
Lazy suspect-set computation: <phrase>fault diagnosis</phrase> for deep electrical bugs Current silicon <phrase>test methods</phrase> are highly effective at sensitizing and propagating most electrical faults. Unfortunately, with ever increasing chip complexity and shorter time-to-market windows, an increasing number of faults escape undetected. To address this problem, we propose a novel technique to help identify hard-to-find electrical faults that are not detected using conventional <phrase>test methods</phrase>, but manifest themselves as <phrase>observable</phrase> functional errors during functional test, system test, or during actual use in the field. These faults are too sequentially deep to be diagnosed using simulation, ATPG, or formal tools. Our technique relies on repeated full-speed chip runs that witness the functional bug, combined with some additional on-chip functional debug support and <phrase>off-line</phrase> analysis, to compute a possible set of suspected faults. The technique quickly prunes the suspect set, and for each suspect, it can provide a short <phrase>test vector</phrase> for further analysis. Experiments on the <phrase>ITC</phrase>'99 benchmarks demonstrate the effectiveness of our approach.
Neural Decision <phrase>Forests</phrase> for Semantic Image Labelling In this work we present Neural Decision <phrase>Forests</phrase>, a novel approach to jointly tackle data representation-and dis-criminative learning within randomized <phrase>decision trees</phrase>. <phrase>Recent advances</phrase> of <phrase>deep learning</phrase> architectures demonstrate the power of embedding representation learning within the classifier – An idea that is intuitively supported by the hierarchical nature of the decision forest model where the input space is typically left unchanged during <phrase>training and testing</phrase>. We bridge this gap by introducing randomized <phrase>Multi-Layer</phrase> Perceptrons (rMLP) as new split nodes which are capable of learning non-linear, data-specific representations and taking advantage of them by finding optimal predictions for the emerging child nodes. To prevent overfitting, we i) randomly select the <phrase>image data</phrase> fed to the input layer, ii) automatically adapt the rMLP topology to meet the complexity of the data arriving at the node and iii) introduce an 1-norm based regularization that additionally sparsifies the network. The key findings in our experiments on three different semantic image labelling datasets are consistently improved results and significantly compressed trees compared to conventional classification trees.
The Idiosyncrasy of Business Cycles across Eu Countries the Idiosyncrasy of Business Cycles across Eu Countries * This paper analyses the underlying dynamics of business cycles in the EU-15. Existing literature mainly focuses on the comovement of expansion and contraction phases, while this paper seeks to test the idiosyncrasy of business cycles by studying growth pattern and deepness of industrial production. Hypotheses are tested using formal <phrase>statistical methods</phrase> while much existing literature in this field rely on judgements of correlation coefficients. The <phrase>results obtained</phrase> here does not give much rise to concern about the possibility of the <phrase>ECB</phrase> to choose an appropriate timing and magnitude of changes in <phrase>monetary policy</phrase> in order to satisfy the <phrase>economic development</phrase> in its member countries. * I owe Claus Thustrup Kreiner many thanks a bunch of useful comments and for good discussions of issues dealt with in this paper. A great deal of the conclusions in this study is obtained using the Bry-Boschan algorithm for MATLAB. The effort programming this algorithm is not mine, however; therefore also many thanks to <phrase>Mark Watson</phrase> for kindly providing the algorithm in <phrase>Gauss</phrase>. Address for correspondence: Jesper.Linaa@econ.ku.dk. † The activities of EPRU (Economic Policy Research Unit) are financed through a grant from The <phrase>Danish</phrase> National Research Foundation.
[Body <phrase>symbolics</phrase> of <phrase>anorectic women</phrase>]. AIM <phrase>Body image</phrase> and its disorder is an <phrase>important issue</phrase> in <phrase>anorexia</phrase> nervosa. The aim of the present paper is to approach this issue by a symbol test and to demonstrate the image what <phrase>anorectic women</phrase> create about their own body through several symbols.   METHOD 29, 13-35 year old anorectic and 29 15-35 year old healthy women were tested with Jacqueline Royer's <phrase>Metamorphosis</phrase> Test. The test is based on the <phrase>Jungian</phrase> and Bourdonian psychology, and examines the dynamic, altering part of the personality through the motivation and identification system.   RESULTS Among <phrase>anorectic women</phrase>--opposed to healthy women--similar answers, symbols appeared, which differ from the vulger answers of the test, and show a plain or deep analogy with the physical and psychical symptoms of <phrase>anorexia</phrase> nervosa. 15.7% of the answers given to the Figure question by <phrase>anorectic women</phrase> have a symbolic reference to "deformed" quality. In the category of Material, 6.2% of their answers show the <phrase>symbolics</phrase> of "fire" and "burning". 21.0% of their answers given to the category of <phrase>Landscape</phrase> symbolise "withering" and "dying" parts of nature.   CONCLUSIONS The imaginative expression of the similar symbols given by <phrase>anorectic women</phrase> help us to conceive the psychodynamical background and the subjective somatic-psychical meaning of the disease in a more complex way.
Age and <phrase>Gender</phrase> Estimation of Unfiltered Faces —This paper concerns the estimation of facial attributes – namely, age and <phrase>gender</phrase> – from images of faces acquired in challenging, " in the wild " conditions. This problem has received far less attention than the related problem of <phrase>face recognition</phrase>, and in particular, has not enjoyed the same dramatic improvement in capabilities demonstrated by contemporary <phrase>face recognition</phrase> systems. Here we address this problem by making the following contributions. First, (i), in answer to one of the key problems of age estimation research – absence of data – we offer a unique dataset of <phrase>face images</phrase>, labeled for age and <phrase>gender</phrase>, acquired by <phrase>smart-phones</phrase> and other <phrase>mobile devices</phrase>, and uploaded without manual filtering to online image repositories. We show the images in our collection to be more challenging than those offered by other face-photo benchmarks. (ii) We describe the dropout-SVM approach used by our system for face attribute estimation, in order to avoid over-fitting. This method, inspired by the dropout <phrase>learning techniques</phrase> now popular with <phrase>deep belief networks</phrase>, is applied here for training <phrase>support vector machines</phrase>, to our knowledge, for the first time. Finally, (iii), we present a robust face alignment technique which explicitly considers the uncertainties of facial feature detectors. We report <phrase>extensive tests</phrase> analyzing both the difficulty levels of contemporary benchmarks, as well as the capabilities of our own system. These show our method to outperform state-of-the-art by a wide margin.
ARPIA: A High-Level Evolutionary Test <phrase>Signal Generator</phrase> The <phrase>integrated circuits</phrase> <phrase>design flow</phrase> is rapidly moving towards higher description levels. However, test-related activities are lacking behind this trend, mainly since effective <phrase>fault models</phrase> and test signals generators are still missing. This paper proposes ARPIA, a new <phrase>simulation-based</phrase> evolutionary test generator. ARPIA adopts an innovative <phrase>high-level</phrase> <phrase>fault model</phrase> that enables efficient fault simulation and guarantees good correlation with <phrase>gate-level</phrase> results. The approach exploits an <phrase>evolutionary algorithm</phrase> to drive the search of effective patterns within the gigantic space of all possible signal sequences. ARPIA operates on <phrase>register-transfer level</phrase> VHDL descriptions and generates effective <phrase>test patterns</phrase>. Experimental results show that the achieved results are comparable or better than those obtained by <phrase>high-level</phrase> similar approaches or even by <phrase>gate-level</phrase> ones. 1 Background In recent years the <phrase>application specific integrated circuit</phrase> (ASIC) <phrase>design flow</phrase> experienced radical changes. <phrase>Deep sub-micron</phrase> <phrase>integrated circuit</phrase> (IC) manufacturing technology is enabling designers to put millions of transistors on a single <phrase>integrated circuit</phrase>. Following <phrase>Moore's law</phrase>, <phrase>design complexity</phrase> is doubling every 12-18 months. In addition, there is an ever-increasing demand on reducing time to market. With complexity skyrocketing and such a competitive pressure, designing at <phrase>high levels</phrase> of abstraction has become more of a necessity than an option. At the present time, exploiting design-partitioning techniques, <phrase>register-transfer level</phrase> (<phrase>RT-level</phrase>) automatic <phrase>logic synthesis</phrase> tools can be successfully adopted in many ASIC design flows. However, not all activities have already migrated from gate-to <phrase>RT-level</phrase> and are not yet mature enough to. <phrase>High-level</phrase> <phrase>design for testability</phrase>, testable synthesis and <phrase>test pattern generation</phrase> are increasing their industrial relevance [1]. During the development of ASIC, designers would like to be able to foresee its testability before starting the <phrase>logic synthesis</phrase> process. Furthermore, <phrase>RT-level</phrase> automatic test signals generators are expected to exploit <phrase>higher-level</phrase> compact information about design structure and behavior, and to be able to produce more effective sequences within reduced CPU time. The test signals may possibly be completed after synthesis by <phrase>ad-hoc</phrase> <phrase>gate-level</phrase> tools.
Cultural–historical <phrase>Activity Theory</phrase> the Theoretical Framework A long-standing challenge in educational research is to describe and explain the <phrase>complex dynamics</phrase> of learning and development that occur in educational settings. This article summarizes ways in which qualitative methods are essential to this enterprise from the perspective of scholars who approach the issues using the theoretical <phrase>lens</phrase> of cultural–historical <phrase>activity theory</phrase> (CHAT). After summarizing basic principles of this theoretical approach we provide four examples involving different levels of analysis and methodologies. (Methodology is used to refer to the ensemble of methods that mediate between theoretical statements and data used to evaluate them.) To some researchers who employ qualitative methods , the very fact that we enter into this topic guided by a theoretical framework disqualifies our claim to be qualitative researchers. Smith argues that ''qualitative approaches in psychology are generally engaged with exploring, describing and interpreting the personal and social experiences of participants. An attempt is usually made to understand a small number of participants' own frames of reference or view of the world rather than trying to test a preconceived hypothesis on a large sample'' (Smith, 2003: 2). Our approach involves small samples, and we are interested in participants' own understandings; however, we do operate from a preconceived theoretical base and in that sense we have preconceived hypotheses. Moreover, the approach we espouse does not preclude quantifica-tion. However, such quantification is more likely to be used for purposes of comparative analysis of qualitatively different activities (Cole et al., 1978) or summary evaluations of products than for a deep analysis of the process of change (cf., Hayes, 1997). CHAT refers to an interdisciplinary approach to studying human learning and development associated with the names of the <phrase>Soviet</phrase> <phrase>Russian</phrase> <phrase>psychologists</phrase>, L.There has been a lively debate in recent years about the extent to which these three thinkers represent a single theoretical perspective. According to one line of interpretation, those who follow <phrase>Vygotsky</phrase> have focused attention on processes of mediation , adopting mediated action in context as a basic unit of analysis (Wertsch et al., 1995). This line of work is often referred to as sociocultural research. By contrast, followers of Leontiev are said to choose activity as a basic unit of analysis (Kaptelinin, 1996). For our present purposes, these distinctions are not central and we will treat the differing formulations as expressions of a single family of theoretical commitments.) The following are some theoretical principles of this approach: 1. …
Beyond the Limits of Planning Theory: Response to My Critics 'Little Things' that Re-enchant the World I want to thank the editors for giving me this opportunity to respond to the comments on Rationality and Power made above what follows I shall give my response to their criticisms. I shall focus less on the many positive things they also say about the book. First let me mention, however, that I was particularly happy to learn that the critics are favourable to the depth of detail in the book's case-study of planning in <phrase>Aalborg</phrase>. This is especially important to me, because during the years when I was working in the <phrase>archives</phrase>, doing interviews, making observations, talking with my informants, etc. a nagging question kept resurfacing in my mind. This is a question bound to haunt many carrying out what Peattie calls 'dense data <phrase>case-studies</phrase>': 'Who will want to learn about a case like this, and in this kind of detail?' I wanted the <phrase>case-study</phrase> to be particularly dense because I wished to test the thesis that the most interesting phenomena in planning and policy making, and those of most general import, would be found in the most minute and most concrete of details. Or to put the matter differently, I wanted to see whether the dualisms general–specic and abstract–concrete would <phrase>metamorphose</phrase> or vanish if I went into sufciently deep detail. Following <phrase>Dewey</phrase>, Rorty has perceptively observed that the way to re-enchant the world is to stick to the concrete. <phrase>Nietzsche</phrase> similarly advocates a focus on 'little things' if we are to understand the problems of <phrase>politics</phrase> and social organization, which, needless to say, include problems of planning. Both Rorty and <phrase>Nietzsche</phrase> seem right to me. I saw the <phrase>Aalborg</phrase> case as being made up of the type of concrete, little things they talk about. Indeed, I saw the case itself as such a thing, what <phrase>Nietzsche</phrase> calls a discreet and apparently insignicant truth, which, when closely examined, would reveal itself to be pregnant with paradigms, metaphors and general signicance. That was my thesis, but theses may be wrong and the study could have fallen at on its face. This has not happened, and it is especially satisfying to me to see that this particular aspect—the focus on 'little things'—is emphasized by many reviewers, including the present ones, as a strength of the <phrase>Aalborg</phrase> study.
Enhancement of <phrase>Fault Injection</phrase> Techniques Using Saboteurs and Mutants for Modification of Vhdl Code Assistant Professor, #4 Professor Keywords— Hardware Implemented <phrase>Fault Injection</phrase> (hwifi), <phrase>Single Event</phrase> Upsets (seus), Unidirectional Serial Saboteur (<phrase>uss</phrase>), N -bit Unidirectional Serial Saboteur (nuss) — Various <phrase>fault modeling</phrase> methods have been proposed for tackling the problem of increasing <phrase>test-data volume</phrase> of contemporary. The <phrase>test pattern generation</phrase> by random <phrase>fault injection</phrase> does not produce efficient results. So, the proper <phrase>fault modeling</phrase> becomes important since the deep sub micrometer devices are expected to be increasingly sensitive to physical faults. For this reason, <phrase>fault-tolerance</phrase> mechanisms are more and more required in <phrase>VLSI circuits</phrase>. So, validating their dependability is a prior concern in the design process. <phrase>fault injection</phrase> techniques based on the use of <phrase>hardware description</phrase> languages offer important advantages with regard to other techniques. First, as this type of techniques can be applied during the design phase of the system, they permit reducing the time-to-market. Second, they present high controllability and reach ability. Among the different techniques, those based on the use of saboteurs and mutants are especially attractive due to their high <phrase>fault modeling</phrase> capability. However, implementing automatically these techniques in a <phrase>fault injection</phrase> tool is difficult. Especially complex are the insertion of saboteurs and the generation of mutants. In this paper, we present new proposals to implement saboteurs and mutants for models in VHDL which are easy-to-automate, and whose philosophy can be generalized to other <phrase>hardware description</phrase> languages. I. INTRODUCTION The new deep sub micrometer technologies are increasingly sensitive to physical faults, both to those due to external phenomena (i.e., <phrase>transient faults</phrase> such as <phrase>single event</phrase> upsets (SEUs), <phrase>single event</phrase> transient (SETs), etc.) and to internal defects (i.e., intermittent and <phrase>permanent faults</phrase>). Moreover, this sensitivity implies not only a raise of the fault rate, but also an increment of the likelihood of appearing multiple faults [1]–[3]. For this reason, the dependability of systems must be analyzed. This analysis can be either the study of the incidence of faults on the system (called error syndrome analysis) or checking the design specifications (called validation). The objective of the error syndrome analysis is to detect those parts of the system which are most sensitive to faults, and eventually, to choose the most suitable <phrase>fault-tolerance</phrase> mechanisms (FTMs). The aim of the validation is to verify that the system and/or its built-in FTMs accomplish the design specifications in presence of faults. If the dependability is analyzed at early phases of the <phrase>design cycle</phrase>, both time and money can be saved in the development
Direct Measures of <phrase>Path Delays</phrase> on Commercial Fpga Chips We present a general technique for measuring the <phrase>propagation delay</phrase> on the internal wires of FPGA chips. The measure is based on the comparison between the <phrase>operating frequencies</phrase> of two ring oscillators that differ only for the structure under test, that is included (or not) in the loop. <phrase>Experimental results</phrase> are presented for a device of the <phrase>Xilinx</phrase> XC4000 family. 1. Introduction Achieving the timing closure is one of the main challanges in <phrase>deep submicron</phrase> (DSM) design. This is mainly due to the great impact of interconnect delay, that is hard to predict during early design steps, since the actual <phrase>electrical parameters</phrase> depend on <phrase>physical design</phrase>, and require complex procedures to be accurately extracted. As a matter of fact, the timing closure is usually achieved either by highly conservative designs, or by several trial and error design iterations. The regular structure of FPGAs makes the <phrase>propagation delay</phrase> more predictable, since: i) <phrase>ad-hoc</phrase> models can be derived for each wire segment, ii) the <phrase>electrical parameters</phrase> can be pre-characterized once for all, and iii) the routing options are finite. Most design flows for FPGAs provide accurate delay estimates that can be used either to meet detailed timing constraints or to effectively optimize performance [1,2,6,7]. However, any delay model is necessarily conservative, since it cannot account for intra/inter-chip <phrase>process variations</phrase> and temperature drifts that may cause the actual parameters to differ from the nominal (typical) ones. On the other hand, the short re-configuration time of FPGAs makes it possible to perform preliminary measurements on the same device the design will be mapped on. Such <phrase>real-world</phrase> measurement could even be included in an aggressive trial-and-error design loop. In general, timing measures could be used to refine and/or validate delay models. In this paper we propose a general technique for measuring the actual <phrase>propagation delay</phrase> across an arbitrary path within a FPGA chip. The technique is based on the comparison between the <phrase>operating frequency</phrase> of a <phrase>ring oscillator</phrase> that includes the path under test, and that of a reference <phrase>ring oscillator</phrase> that does not. Both ring oscillators are realized by (re)programming the target device. The proposed technique can be used to characterize (or refine) the delay models using device-specific information. Moreover, similar <phrase>test structures</phrase> could be integrated with the functional logic to grant delay self-monitoring capabilities.
<phrase>Physicalism</phrase> from a Probabilistic <phrase>Point of View</phrase> <phrase>Physicalism</phrase>-like other isms-has meant different things to different people. The main physicalistic thesis I will discuss here is the claim that all occurrences supervene on physical occurrences-that the physical events and states of affairs at a time determine everything that happens at that time. This synchronic claim has been discussed most in the context of the mind/body problem, but it is fruitful to consider as well how the supervenience thesis applies to what might be termed the <phrase>organism</phrase>/body problem. How are the biological properties of a system at a time related to its physical properties at that time? <phrase>Philosophers</phrase> have discussed the meaning of supervenience claims at considerable length, mainly with an eye to getting clear on modal issues. Less <phrase>ink</phrase> has been spilled on the question of why anyone should believe this physicalistic thesis. In what follows, I'll discuss both the <phrase>metaphysics</phrase> and the <phrase>epistemology</phrase> of supervenience from a probabilistic <phrase>point of view</phrase>. The first half of this paper will explore how supervenience claims are related to other issues; these will include the thesis that physics is causally complete, the claim that there are emergent properties, the idea that mental properties are causally efficacious, and the notion that there are scientific laws about supervenient properties that generalize over systems that deploy different physical realizations of the properties in question.. The second half will examine the question of how observational evidence can lend support to supervenience claims. This problem turns out to raise some surprisingly deep issues about the nature of <phrase>hypothesis testing</phrase> in science. 1 1. Preliminaries What, exactly, is the supervenience thesis supposed to mean? It adverts to " physical " properties, but what are these? This question apparently leads to a dilemma (Hempel 1969, Hellman 1985, Crane and Mellor 1990). If the supervenience thesis presupposes that the list of fundamental <phrase>magnitudes</phrase> countenanced by present physics is true and complete, then the claim is almost certainly false, since physics is probably not over as a subject. Alternatively, if we interpret " <phrase>physical property</phrase> " to mean what an ideally complete physics would describe, then the supervenience thesis lapses into vagueness. What does it mean to say of some future <phrase>scientific theory</phrase> that it is part of the discipline we now call " physics? " Papineau (1990, 1991) suggests one way to address this dilemma. Rather than worry about what " physics " means, we should instead exploit the …
[<phrase>Subthalamic nucleus</phrase> stimulation using a <phrase>Fisher</phrase> ZD stereotactic frame, MR-CT fusion guidance and peroperative orthogonal radiographs, in <phrase>Parkinson disease</phrase>]. BACKGROUND We present the method and results of an original technique to implant electrodes in the subthalamic nucleus (STN) to treat <phrase>Parkinson's disease</phrase>, based on adaptations of the <phrase>Fisher</phrase> ZD stereotactic frame.   METHODS Targets coordinates were calculated after fusion of stereotactic <phrase>CT-scan</phrase> and MRI images. STN was localized by its theoretical coordinates according to AC-PC and by its direct visualization on T2 images. Electrodes were implanted after <phrase>local anesthesia</phrase>, using peroperative multicanal microrecordings and test stimulation. Electrodes location was checked by peroperative <phrase>perpendicular</phrase> radiographs. To avoid projection of the frame arm on the area of interest on anteroposterior and lateral radiographs, the arm was fixed at 45 degrees from the usual 90 degrees position. This original fixation needed a <phrase>trigonometric</phrase> transformation of the X and Y stereotactic coordinates. Radiopaque markers, fixed on the frame, were identified on the radiographs, allowing the calculation of the stereotactic coordinates of the electrode tip, which were then entered in the stereotactic MRI, to check its location from the defined target.   RESULTS No problem due to adaptations of the frame occurred in the 60 patients. In all cases, peroperative radiographs allowed to confirm the correct location of electrodes. Six months after surgery, UPDRS III score without medication was decreased by 52% with stimulation "on". UPDRS IV items 32, 33 and 39 scores were decreased by 75,7, 79,5 and 72%. Daily dopa-<phrase>equivalent dose</phrase> was decreased by 71%. One <phrase>asymptomatic</phrase> thalamic <phrase>hematoma</phrase> and two wound infections occurred.   CONCLUSION This method was efficient and safe to implant deep electrodes.
<phrase>High-Throughput</phrase> Asynchronous Pipelines for Fine-Grain Dynamic Datapaths Personal use of this material is permitted. However, permission to reprint/republish this material for <phrase>advertising</phrase> or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any <phrase>copyrighted</phrase> component of this work in other works, must be obtained from the IEEE. Abstract This paper introduces several new asynchronous pipeline designs which offer <phrase>high throughput</phrase> as well as low latency. The designs target dynamic datapaths, both dual-rail as well as single-rail. The new pipelines are latch-free and therefore are particularly well-suited for fine-grain pipelining, i.e., where each pipeline stage is only a single gate deep. The pipelines employ new control structures and protocols aimed at reducing the <phrase>handshaking</phrase> delay, the principal impediment to achieving <phrase>high throughput</phrase> in asyn-chronous pipelines. As a test vehicle, a 4-bit <phrase>FIFO</phrase> was designed using 0.6 <phrase>micron technology</phrase>. The results of careful HSPICE simulations of the <phrase>FIFO</phrase> designs are very encouraging. The dual-rail designs deliver a throughput of up to 860 million data items per second. This performance represents an improvement by a factor of 2 over a widely-used comparable approach by <phrase>Williams</phrase> [16]. The new single-rail designs deliver a throughput of up to 1208 million data items per second .
Efficient Web Harvesting Strategies for Monitoring Deep <phrase>Web Content</phrase> <phrase>Web content</phrase> changes rapidly [18]. In Focused Web Harvesting [17] which aim it is to achieve a complete harvest for a given topic, this dynamic nature of the web creates problems for users who need to access a set of all the relevant <phrase>web data</phrase> to their topics of interest. Whether you are a fan following your favorite idol or a <phrase>journalist</phrase> investigating a topic, you may need not only to access all the relevant information but also the recent changes and updates. General <phrase>search engines</phrase> like Google apply several techniques to enhance the freshness of their crawled data. However, in focused web harvesting, we lack an efficient approach that detects changes for a given topic over time. In this paper, we focus on techniques that can keep the relevant content to a given query up-to-date. To do so, we test four different approaches to efficiently harvest all the changed documents matching a given entity by querying <phrase>web search engines</phrase>. We define a document with changed content or a newly created or removed document as a changed document. Among the proposed <phrase>change detection</phrase> approaches, the FedWeb method outperforms the other approaches in finding the changed content on the web for a given query with 20 percent, on average, better performance.
Intelligent Diagnostic Tutoring Using Qualitative Symptom Information An intelligent diagnostic tutor, DIAG 1, selects problems adaptively and generates all tutoring dialogs from its analysis of a model of the target system. The learner performs tests on a graphical representation of the target system and calls on DIAG for assistance when needed. During exercises, the tutoring functions can advise the learner about the implications of particular test outcomes, the rationality of suspecting particular replaceable units, and advisability of performing various diagnostic actions. After exercises, DIAG can critique the learner's testing strategy, and it can generate and explain an expert diagnostic strategy for the previous fault. A prototype application of a very complex system demonstrates the range of tutoring capabilities achieved by the system. Overview <phrase>Fault diagnosis</phrase> is one of the most ubiquitous and difficult tasks performed in everyday life, and it is an unavoidable component of a wide range of medical, commercial, industrial, and military operations. From the broadest viewpoint, the diagnostic process is surprisingly equivalent across domains. While the particular costs, risks, and payoffs of testing actions vary greatly over different applications, the diagnostic process can be characterized in a manner that takes these variables into account. Consequently, excellent diagnostic strategies can be artificially generated in highly specific domains, based upon a deep generalized conception of the diagnostic reasoning task. It would seem, therefore, that this class of human performance would be a natural and highly amenable subject area for <phrase>intelligent tutoring</phrase> approaches. In fact, a number of intelligent diagnostic systems have been produced, and several have demonstrated excellent instructional power. Those approaches based upon <phrase>expert systems</phrase> (Lesgold, Eggan, Katz, and Rat 1992) are capable of generating discussions of impressive verbal and technical content. Unfortunately, this instructional depth and facility has come at a very high expense in capturing and structuring the expert knowledge. At least two other methodologies have been explored to combat this discouraging cost-to-power relationship: 1) structural ttwdel approach (Johnson, Norton, Duncan, and Hunt 1988), in which the target system is specified in terms of its normal <phrase>input-output</phrase> structure, and 2) fimctional model approach (Towne, Munro, Pizzini, Surmon, and Wogulis 1990), in which the target system is modeled in a manner that it can be executed in various normal and abnormal conditions. The structural model approach specifies the target system as a network of system elements, connected via directed paths, and it then reasons about symptoms by assuming that failures will propagate along the …
Towards inverse modeling of <phrase>turbidity currents</phrase>: The inverse lock-exchange problem Keywords: Turbidite modeling Flow inversion <phrase>Turbidity current</phrase> Surrogate management framework a b s t r a c t A new approach is introduced for turbidite modeling, leveraging the potential of <phrase>computational fluid dynamics</phrase> methods to simulate the flow processes that led to turbidite formation. The practical use of numerical flow simulation for the purpose of turbidite modeling so far is hindered by the need to specify parameters and initial flow conditions that are a priori unknown. The present study proposes a method to determine optimal simulation parameters via an automated optimization process. An iterative procedure matches deposit predictions from successive flow simulations against available localized reference data, as in practice may be obtained from well logs, and aims at convergence towards the best-fit scenario. The final result is a prediction of the entire deposit thickness and local <phrase>grain size</phrase> distribution. The optimization strategy is based on a derivative-free, surrogate-based technique. Direct numerical simulations are performed to compute the flow dynamics. A proof of concept is successfully conducted for the simple <phrase>test case</phrase> of a two-dimensional lock-exchange <phrase>turbidity current</phrase>. The optimization approach is demonstrated to accurately retrieve the initial conditions used in a reference calculation. The modern <phrase>sea floor</phrase> in <phrase>deep water</phrase> is in large part composed of <phrase>turbidites</phrase>, the deposits of submarine <phrase>turbidity currents</phrase>. <phrase>Turbidity currents</phrase> are believed to constitute the principal mechanism of <phrase>sediment transport</phrase> from <phrase>shallow water</phrase> into the deep sea (see Meiburg and Kneller, 2010 and references therein). Over geological time scales, stacked <phrase>turbidites</phrase> may accumulate on abyssal plains and local <phrase>deep-sea</phrase> basins, or form submarine fans fed by <phrase>river</phrase> <phrase>deltas</phrase>. Deeply buried, sandy turbidite sheets represent an important class of <phrase>hydrocarbon</phrase> <phrase>reservoirs</phrase>, many of which are situated in <phrase>deep water</phrase> (Weimer and Slatt, 2007). Many of these <phrase>turbidites</phrase> are essentially sheetlike in nature. There is a large body of experimental work on gravity and <phrase>turbidity currents</phrase> that has established the dimensional relations (see the review by Huppert, 2006), and the geometries and properties of their deposits at laboratory scales (e. Although the results of many of these experiments have been explicitly applied to submarine deposits at natural scales, such extrapolations are subject to considerable scaling uncertainties, as pointed out by Parsons et al. (2007). The interaction of <phrase>turbidity currents</phrase> with the <phrase>sea floor</phrase>, via deposition and <phrase>erosion</phrase> of <phrase>sediment</phrase>, leads to complex <phrase>sedimentary</phrase> features that can only be understood by investigating the flow processes that created …
Effects of Light Curing Method and Exposure Time on <phrase>Mechanical Properties</phrase> of <phrase>Resin</phrase> Based Dental Materials OBJECTIVES The aim of this study was to investigate microhardness and <phrase>compressive strength</phrase> of composite <phrase>resin</phrase> (Tetric-Ceram, Ivoclar Vivadent), compomer (Compoglass, Ivoclar, Vivadent), and <phrase>resin</phrase> modified glass ionomer <phrase>cement</phrase> (Fuji II LC, GC Corp) <phrase>polymerized</phrase> using <phrase>halogen</phrase> light (Optilux 501, Demetron, Kerr) and LED (Bluephase C5, Ivoclar Vivadent) for different curing times.   METHODS Samples were placed in disc shaped plastic molds with uniform size of 5 mm diameter and 2 mm in thickness for surface microhardness test and placed in a diameter of 4 mm and a length of 2 mm <phrase>teflon</phrase> cylinders for <phrase>compressive strength</phrase> test. For each <phrase>subgroup</phrase>, 20 samples for microhardness (n=180) and 5 samples for <phrase>compressive strength</phrase> were prepared (n=45). In group 1, samples were <phrase>polymerized</phrase> using <phrase>halogen</phrase> light source for 40 seconds; in group 2 and 3 samples were <phrase>polymerized</phrase> using LED light source for 20 seconds and 40 seconds respectively. All data were analyzed by two way analysis of <phrase>ANOVA</phrase> and Tukey's post-hoc tests.   RESULTS Same exposure time of 40 seconds with a low intensity LED was found similar or more efficient than a high intensity <phrase>halogen</phrase> light unit (P>.05), however application of LED for 20 seconds was found less efficient than 40 seconds curing time (P=.03).   CONCLUSIONS It is important to increase the light curing time and use appropriate light curing devices to <phrase>polymerize</phrase> <phrase>resin</phrase> composite in deep cavities to maximize the hardness and <phrase>compressive strength</phrase> of restorative materials.
Debugging <phrase>Machine Learning</phrase> – Extended Abstract Paste the appropriate copyright statement here. ACM now supports three different copyright statements: • ACM copyright: ACM holds the copyright on the work. This is the historical approach. • License: The author(s) retain copyright, but ACM receives an exclusive publication license. • <phrase>Open Access</phrase>: The author(s) wish to pay for the work to be <phrase>open access</phrase>. The additional fee must be paid to ACM. This text field is large enough to hold the appropriate release statement assuming it is single spaced in a <phrase>sans-serif</phrase> 7 point <phrase>font</phrase>. Every submission will be assigned their own unique DOI string to be included here. Abstract Creating a <phrase>machine learning</phrase> solution for a real world problem often requires multiple iterations of investigation and improvement until it reaches satisfactory performance. Even after deployment, it is common to discover limitations of the model or changes in the target concept that necessitate modifications to the training data and parameters. However, as of today, there is no common wisdom about what these iterations consist of, nor what debugging tools are needed to aid the investigative process. In this work we present a novel technique to help model developers find the root causes of prediction error on test items (henceforth 'bugs') and so help the developer to fix them. Given an observed bug our method aims to identify the training items most responsible for biasing the model towards giving the wrong prediction on the specific test item. This set of training items can aid in discovery of common errors like faulty training labels or poor <phrase>training data</phrase> coverage. Our method is applicable over many different learners, including <phrase>deep neural nets</phrase> with large and complex model representations, as well as many different data types.
Using a Periodic <phrase>Square Wave</phrase> Test Signal to Detect <phrase>Crosstalk Faults</phrase> <phrase>DEEP-SUBMICRON</phrase> TECHNOLOGY'S advanced <phrase>high-density</phrase> and <phrase>high-speed</phrase> VLSI has reduced distances between wires and devices. Perhaps because of manufacturing defects, parasitic capacitors become important sources of internal circuit noise. In 0.18-µm technology, for example, two parallel lines that are 240 µm long with a spacing of 0.23 µm will produce a 25-fF parasitic capac-itance in the normal fabrication condition. 1 A manufacturing defect, such as narrower spacing between two conduction lines, will make the parasitic capacitances even larger. Noise induced by these parasitic elements interferes with normal VLSI operation by generating unexpected pulses, speeding up or slowing down the transition speed on interconnecting (victim) lines when the nearby aggressor line changes state. 2-4 If unexpected pulses appear, and <phrase>flip-flops</phrase> catch these pulses during their sampling time, the system will fall into erroneous states. If the slowed-down transitions exceed the <phrase>flip-flops</phrase>' clock period, erroneous states also result. Many researchers have focused on analysis, testing, and reduction of <phrase>crosstalk faults</phrase>. 2-10 Chen et al. simply used Kirchoff's voltage law to analyze <phrase>crosstalk noise</phrase> with a three-step approach. 2 Lee et al. analyzed crosstalk in both frequency and time domains to gain insight into effects that cause errors. 3 Sabet and Ilponse considered crosstalk on clock or reset lines and contrived a fault simulator to estimate the fault effect and the detectability of <phrase>crosstalk faults</phrase>. 4 Others used proposed algorithms to study circuit timing characteristics. 5-7 Bai et al. proposed an at-speed test technique to detect circuit interconnects' <phrase>crosstalk faults</phrase> by generating a six-vector <phrase>test sequence</phrase>, 8 and Lai et al. proposed a <phrase>software-based self</phrase>-<phrase>test methodology</phrase> to detect <phrase>crosstalk faults</phrase> on interconnects. 9 Several researchers used a <phrase>timing analysis</phrase> technique to help generate test patterns for <phrase>crosstalk faults</phrase> in the desired time window. Testing for <phrase>crosstalk faults</phrase> is difficult because they are pattern dependent and highly unpredictable. Generating <phrase>test patterns</phrase> to deterministically detect these faults requires a <phrase>timing analysis</phrase> program of <phrase>high precision</phrase> , and this takes much computation time. Previously, we proposed a test scheme based on an oscillation <phrase>square wave</phrase> signal. 11 If a <phrase>crosstalk fault</phrase> between two lines exists, applying the <phrase>square wave</phrase> test signal on the aggressor line induces glitches on the victim line that can be detected. This scheme eliminates the need to consider the fault timing issue because any glitches induced (usually unexpectedly) by the <phrase>crosstalk fault</phrase> are detectable at the victim line's output. For the work described …
Art Expertise Reduces Influence of Visual Salience on Fixation in Viewing Abstract-Paintings When viewing a <phrase>painting</phrase>, artists perceive more information from the <phrase>painting</phrase> on the basis of their experience and knowledge than art novices do. This difference can be reflected in eye scan paths during viewing of paintings. Distributions of scan paths of artists are different from those of novices even when the paintings contain no figurative object (i.e. abstract paintings). There are two possible explanations for this difference of scan paths. One is that artists have high sensitivity to <phrase>high-level</phrase> features such as textures and composition of colors and therefore their fixations are more driven by such features compared with novices. The other is that fixations of artists are more attracted by salient features than those of novices and the fixations are driven by <phrase>low-level</phrase> features. To test these, we measured eye fixations of artists and novices during the free viewing of various abstract paintings and compared the distribution of their fixations for each <phrase>painting</phrase> with a topological attentional map that quantifies the conspicuity of <phrase>low-level</phrase> features in the <phrase>painting</phrase> (i.e. saliency map). We found that the fixation distribution of artists was more distinguishable from the saliency map than that of novices. This difference indicates that fixations of artists are less driven by <phrase>low-level</phrase> features than those of novices. Our result suggests that artists may extract <phrase>visual information</phrase> from paintings based on <phrase>high-level</phrase> features. This ability of artists may be associated with artists' deep <phrase>aesthetic</phrase> appreciation of paintings.
<phrase>Process-Variation</phrase> Aware Multi-temperature <phrase>Test Scheduling</phrase> —Chips manufactured with <phrase>deep submicron technologies</phrase> are prone to large <phrase>process variation</phrase> and temperature-dependent defects. In order to provide high test efficiency, the tests for temperature-dependent defects should be applied at appropriate temperature ranges. Existing static scheduling techniques achieve these specified temperatures by scheduling the tests, specially developed heating sequences, and cooling intervals together. Because of the temperature uncertainty induced by <phrase>process variation</phrase>, a static test schedule is not capable of applying the tests at intended temperatures in an efficient manner. As a result the test cost will be very high. In this paper, an adaptive <phrase>test scheduling</phrase> method is introduced that utilizes on-chip temperature sensors in order to adapt the test schedule to the actual temperatures. The proposed method generates a low cost schedule tree based on the variation statistics and thermal simulations in the design phase. During the test, a chip selects an appropriate schedule dynamically based on temperature sensor readings. A ૛૜% decrease in the likelihood that tests are not applied at the intended temperatures is observed in the experimental studies in addition to ૛૙% reduction in test application time. I. INTRODUCTION Temperature-dependent defects are a challenge for <phrase>achieving high</phrase> <phrase>test quality</phrase> for chips manufactured with modern technologies [1]. This entails the need to apply tests within specified temperature ranges and also the necessity of having a variety of tests applied at different temperatures in order to achieve high <phrase>defect coverage</phrase>. Therefore, it is important to develop <phrase>efficient methods</phrase> to apply tests at the specified temperatures with a minimal cost [2]. Tests could be performed at specified temperatures using a temperature-aware schedule that adjusts the temperature by introducing cooling and heating intervals [2, 3]. A heating interval is a period when the chip under test is consuming large amount of power that is achieved by test controls. A cooling interval, on the other hand, corresponds to a period with very small <phrase>power consumption</phrase>. Heating could be achieved by applying a section of the normally generated <phrase>test pattern</phrase> that has the maximal power or a sequence of patterns that is especially generated to heat up the chip rapidly; while cooling can be simply done by not applying any patterns. This way, multi-temperature tests are performed without costly extra <phrase>test equipment</phrase> (such as external heating mechanisms). The challenge is that the test application time (which is already long) could become excessively long, resulting in an extremely high cost of …
Cognitive and Motivational Consequences of Adapting an Agent Metaphor in Multimedia Learning: Do the Benefits Outweigh the Cognitive and Motivational Consequences of Adapting an Agent Metaphor in Multimedia Learning: Do the Benefits Outweigh the Costs? This paper reviews a set of studies designed to test the hypothesis that the presence of <phrase>animated</phrase> pedagogical agents in multimedia environments can promote <phrase>deep learning</phrase>. This was done by first comparing the learning and motivational outcomes of students who learned in the context of social-agency to students who learned in a more traditional text and graphics context. Second, the particular features of the social-agency environment were manipulated to examine which of its attributes (e.g., visual and auditory presence, students' interaction, and agents' learning style) are most important in the promotion of meaningful learning. The theoretical and practical implications of the findings are'discussed. (Contains 23 references.) (MES) Reproductions supplied by EDRS are the best that can be made from the original document. EDUCATIONAL RESOURCES INFORMATION CENTER (ERIC) This document has been reproduced as received from the person or organization originating it. Minor changes have been made to improve reproduction quality. Points of view or opinions stated in this document do not necessarily represent official OERI position or policy. Abstract: What are the cognitive and motivational consequences of adapting an agent metaphor in multimedia learning? The present paper reviews a set of studies designed to test the hypothesis that the presence of <phrase>animated</phrase> pedagogical agents in multimedia environments can promote <phrase>deep learning</phrase>. This was done by first, comparing the learning and motivational outcomes of students who learned in the context of social-agency was to those of students who learned in a more traditional text and graphics context. Second, the particular features of the social agency environment were manipulated to examine which of its attributes are most important in the promotion of meaningful learning. The theoretical and practical implications of the findings are discussed.
A computational analysis to assess the influence of specimen geometry on cleavage <phrase>fracture toughness</phrase> of metallic materials The fracture response of <phrase>mild steel</phrase> in the domain of brittle behavior, i.e., the cleavage range, has been carefully evaluated using a <phrase>weakest link</phrase> <phrase>statistical model</phrase>, assuming the existence of a distribution of cracked <phrase>carbide</phrase> particles in the microstructures. Experiments have provided an evidence of both scatter in <phrase>test results</phrase> and the existence of constraints. Statistical-<phrase>based model</phrase> to include micromechanics were developed in an attempt to study and analyze the problem. The Weibull stress micro-mechanical model was used in this study to quantify the constraint effects. This was done numerically using a constraint function (g(M)) derived from the Weibull stress model. The non-dimensional function (g(M)) describes the evolution of the effects of constraint loss on <phrase>fracture toughness</phrase> relative to the reference condition, i.e., plane-strain, <phrase>small scale</phrase> yielding (SSY) (T-stress = 0). Single-edge SE(B) notched bending specimens having different crack lengths, different cross-sections and side-grooves were modeled and the constraint function (g(M)) was calculated. In this paper, we compare the loss in constraint for both the deep notch and shallow notch specimens for a given <phrase>cross-section</phrase> of the single-edge notched bend specimen (SE(B)).
<phrase>Pathway analysis</phrase> of <phrase>genetic markers</phrase> associated with a functional MRI faces paradigm implicates polymorphisms in calcium responsive pathways Several lines of evidence suggest that common polygenic variation influences <phrase>brain function</phrase> in humans. Combining <phrase>high-density</phrase> <phrase>genetic markers</phrase> with <phrase>brain imaging</phrase> techniques is constricted by the practicalities of collecting sufficiently large <phrase>brain imaging</phrase> samples. <phrase>Pathway analysis</phrase> promises to leverage knowledge on function of genes to detect recurring signals of moderate effect. We adapt this approach, exploiting the deep information collected on <phrase>brain function</phrase> by fMRI methods, to identify molecular pathways containing genetic variants which influence brain activation during a commonly applied experiment based on a face matching task (n=246) which was developed to study neural processing of faces displaying negative emotions. <phrase>Genetic markers</phrase> moderately associated (p<10(-4)) with whole brain activation phenotypes constructed by applying principal components to contrast maps, were tested for pathway enrichment using <phrase>permutation</phrase> <phrase>based methods</phrase>. The most significant pathways are related to post <phrase>NMDA</phrase> receptor activation events, driven by genetic variants in calcium/<phrase>calmodulin</phrase>-dependent <phrase>protein kinase</phrase> II (CAMK2G, CAMK2D) and a calcium-regulated <phrase>nucleotide</phrase> exchange factor (RASGRF2) in which all are activated by intracellular calcium/<phrase>calmodulin</phrase>. The most significant effect of the combined polygenic model were localized to the left <phrase>inferior frontal gyrus</phrase> (p=1.03 × 10(-9)), a region primarily involved in semantic processing but also involved in processing negative emotions. These <phrase>findings suggest</phrase> that <phrase>pathway analysis</phrase> of GWAS results derived from <phrase>principal component analysis</phrase> of fMRI data is a promising method, to our knowledge, not previously described.
A programmable built-in self-test core for <phrase>embedded memories</phrase> —Testing <phrase>embedded memories</phrase> is becoming an industry-wide concern with the advent of <phrase>deep-submicron technology</phrase> and system-on-chip applications. We present a prototype chip for a programmable built-in self-test (BIST) design that is used for testing <phrase>embedded memories</phrase>, especially DRAMs. The BIST chip supports various memory test algorithms by a novel controller and <phrase>sequencer</phrase> design. The area of the core circuit is about 1, 020 × 1, 020µm 2 using a 0.6µm <phrase>CMOS process</phrase>, and the <phrase>clock speed</phrase> is over 100MHz.
An Imaging HF GPR Using Stationary Antennas: Experimental Validation Over the <phrase>Antarctic</phrase> <phrase>Ice Sheet</phrase> — <phrase>Ground penetrating</phrase> <phrase>radars</phrase> (GPR) are commonly used on the Earth to probe the subsurface and the moderate mass and power resources they require make them a most useful tool in planetary exploration. In most cases, GPR need to be moved and perform soundings at various locations to retrieve the image of the underground layers or reflectors. Yet, <phrase>TAPIR</phrase> (Terrestrial And Planetary Imaging Radar) is an innovative stationary HF GPR that allows to image the reflectors in the subsurface through the processing of measured electric and magnetic components of the reflected waves. This instrument was originally developed in the frame of the NetLander project to perform deep soundings of the <phrase>Martian</phrase> subsurface and has been tested during a validation campaign on the <phrase>Antarctic</phrase> <phrase>ice sheet</phrase>. Combining the corresponding observations and numerical simulations of the operation of the instrument we demonstrate its imaging capability and evaluate its performances.
The neural-SIFT <phrase>feature descriptor</phrase> for visual vocabulary <phrase>object recognition</phrase> In computer vision, one area of research which receives a lot of attention is recognizing the semantic content of an image. It's a challenging problem where varying pose, occlusion, scale and differing light conditions affect the ease of recognition. A common approach is to extract local feature descriptors from images and attach object class labels to them, but choosing the best type of feature to use is still an <phrase>open problem</phrase>. Some use <phrase>deep learning</phrase> methods to learn to create features during training. Others apply <phrase>local image descriptors</phrase> to extract features from an image. In most cases these algorithms show good performance, however, the downside of these type of algorithms is that they are not trainable by design. After training there is no <phrase>feedback loop</phrase> to update the type of features to extract, while there possibly could be room for improvement. In this thesis, a continuous <phrase>deep neural network</phrase> feedback system is proposed , which consists of an adaptive <phrase>neural network</phrase> <phrase>feature descriptor</phrase>, the bag of visual words approach, and a neural classifier. Two initialization methods for the neural network <phrase>feature descriptor</phrase> were compared, one where it was trained on the popular <phrase>Scale Invariant Feature Transform</phrase> (SIFT) descriptor output, and one where it was randomly initialized. After initial training, the system propagates the classification error from the neural network classifier through the entire pipeline, updating not only the classifier itself, but also the type of features to extract. The <phrase>feature descriptor</phrase>, before and after additional training, was also applied using a <phrase>support vector machine</phrase> (SVM) classifier to test for generalizability. Results show that for both initialization methods the feedback system increased accuracy substantially when regular training was not able to increase it any further. The proposed neural-SIFT <phrase>feature descriptor</phrase> performs better than the <phrase>SIFT descriptor</phrase> itself even with limited number of training instances. Initializing on an existing <phrase>feature descriptor</phrase> is beneficial when not a lot of <phrase>training samples</phrase> are available. However, when there are a lot of <phrase>training samples</phrase> available the system is able to construct a well-performing <phrase>feature descriptor</phrase> when starting in a random state, solely based on classifier feedback. The improved <phrase>feature descriptor</phrase> did not only show improved performance in the setting in which it was trained, but also while using an <phrase>SVM classifier</phrase>. However, the improvements were small and were only demonstrated with one other classifier. Therefore, more experiments are needed to get a better grip on …
Heuristics For Broad-Coverage <phrase>Natural Language</phrase> Parsing The Slot Grammar system is interesting for <phrase>natural language</phrase> applications because it can deliver parses with deep grammatical information on a reasonably broad scale. The paper describes a numerical scoring system used in Slot Grammar for ambiguity resolution, which not only ranks parses but also contributes to parsing efficiency through a parse space pruning algorithm. Details of the method of computing parse scores are given, and <phrase>test results</phrase> for the English Slot Grammar are presented.
Doppler ultrasound venous mapping of the lower limbs BACKGROUND The study aim was to test the accuracy (intra and interobserver variability), sensitivity, and specificity of a simplified noninvasive ultrasound methodology for mapping superficial and deep veins of the lower limbs.   METHODS 62 consecutive patients, aged 62 ± 11 years, were enrolled. All underwent US-examinations, performed by two different investigators, of both legs, four anatomical parts, and 17 veins, to assess the interobserver variability of evaluation of superficial and deep veins of the lower limbs.   RESULTS Overall the agreement between the second versus the first operator was very high in detecting <phrase>reflux</phrase> (sensitivity 97.9, specificity 99.7, accuracy 99.5; P = 0.80 at McNemar test). The higher CEAP classification stages were significantly associated with <phrase>reflux</phrase> (<phrase>odds ratio</phrase>: 1.778, 95% <phrase>confidence interval</phrase>: 1.552-2.038; P < 0.001) as well as with thrombosis (<phrase>odds ratio</phrase>: 2.765, 95% <phrase>confidence interval</phrase>: 1.741-4.389; P < 0.001). Thus, our findings show a strict association between the symptoms of venous disorders and ultrasound evaluation results for thrombosis or <phrase>reflux</phrase>.   CONCLUSION This study demonstrated that our venous mapping protocol is a reliable method showing a very low interobserver variability, which makes it accurate and reproducible for the assessment of the morphofunctional status of the lower limb veins.
Preferential filtering for <phrase>gravity anomaly</phrase> separation We present the preferential filtering method for <phrase>gravity anomaly</phrase> separation based on <phrase>Green</phrase> equivalent-layer concept and <phrase>Wiener filter</phrase>. Compared to the conventional upward continuation and the preferential continuation, the preferential filtering method has the advantage of no requirement of <phrase>continuation height</phrase>. The method was tested both on the synthetic gravity data of a model of multiple rectangular prisms and on the real gravity data from a <phrase>magnetite</phrase> area in <phrase>Jilin</phrase> <phrase>Province</phrase>, China. The results show that the preferential filtering method produced better separation of <phrase>gravity anomaly</phrase> than both the conventional <phrase>low-pass</phrase> filtering and the upward continuation. The observed gravity anomalies are the sum of gravity effects of density differences at various depths in the subsurface half space. In order to study a specific geological problem using gravity data, the target anomalies must first be separated from the observed gravity anomalies. In the literature, there are a variety of methods proposed for separating gravity anomalies, is that the shallow-source short-wavelength signals and deep-source long-wavelength signals are both simultaneously upward continued and consequently are both attenuated. After subtracting the upward continued regional signals from the observed anomalies, the resultant residual signals still contain parts of regional signals. It means that the anomaly separation is not complete. To solve this problem, Pawlowski (1995) proposed the preferential continuation method based on Green's equivalent layer concepts and Wiener filtering principle. This method attenuates shallow-source short-wavelength signals while minimally attenuating deep-source long-wavelength signals. Later, Xu and Zeng (2000) and Meng et al. (2009) suggested the algorithm of the difference continuation based on the preferential continuation. The algorithm is used to extract certain signals of a given wavelength band, similar to the conventional band-pass filtering. Another problem of the conventional upward continuation, which also occurs in the preferential continuation (Pawlowski, 1995) and its difference continuation algorithm (Xu and Zeng, 2000; Meng et al., 2009), is that the <phrase>continuation height</phrase> must be known. To overcome this problem, Zeng et al. (2008) presented a practical algorithm, based on model studies, to derive an optimum <phrase>continuation height</phrase> by calculating a series of cross-correlations between the upward continuations at two successive heights. The average height of the maximum deflection of these <phrase>cross-correlation</phrase> values yields the optimum <phrase>continuation height</phrase> for regional-residual separation. Guo et al. (2009) and Meng et al. (2009) proposed a similar algorithm for estimating the optimum <phrase>continuation height</phrase> of the preferential upward continuation. In this article we attempted to …
<phrase>Time Series</phrase> Analysis Using Deep <phrase>Feed Forward Neural Networks</phrase> <phrase>Time Series</phrase> Analysis Using Deep <phrase>Feed Forward Neural Networks</phrase> <phrase>Deep neural networks</phrase> can be used for abstraction and as a preprocessing step for other <phrase>machine learning</phrase> classifiers. Our goal was to develop methods for a more accurate automated seizure detection. <phrase>Deep architectures</phrase> have been used for classification of events, and shown in this research to be an effective way of classifying multichannel <phrase>high resolution</phrase> medical data. The medical data used in this thesis was gathered from an electroencephalograph (EEG) used in a hospital setting on seizure patients. To demonstrate the ability of <phrase>deep architectures</phrase> to learn and abstract from input data, the signals from the EEG that contained both seizure and non seizure data were given both as featurized data and raw data to the deep architecture. In addition to the multiple types of data preparation, a patients EEG data was tested not only against their own EEG signal <phrase>training data</phrase> but other patients as well. This study supports the effectiveness of deep <phrase>feed forward neural networks</phrase> for usage in the seizure classification scenario, as well as highlights some of the difficulties associated with training <phrase>deep neural networks</phrase>, as shown through <phrase>experimental results</phrase>. The sooner I'm done with this thing the sooner we can get a kitten. I love you. <phrase>ii Acknowledgments</phrase> I would like to thank all of the people who have helped me, while specifically ensuring that I do so in no particular order, as to not offend someone by accident. To help facilitate this, I generated the list components, and modified their order with a cryptographically secure <phrase>random number generator</phrase> multiple times, while simultaneously poking the <phrase>cat</phrase>. When the <phrase>cat</phrase> moved, I used that configuration of acknowledgements. I would like to thank: • Firstly Andrea. She didn't get randomized, because she's the reason I want to get this thesis done ASAP or sooner. You have heard me complain and stress so much about <phrase>deep learning</phrase> you probably never want to hear it again. Thank you for everything you mean to me, and all the help you gave me in this thesis! I love you. • Secondly, I would like to thank my family. You've supported me through the years, even when there wasn't much of me left to support. If I could beat any of you at <phrase>fantasy</phrase> <phrase>hockey</phrase>, it would be the perfect family. Lab members of <phrase>Adam</phrase> and Tinoosh that we collaborated with getting published at AAAI Spring Symposia and FLAIRS too! We all …
Distinct syndromes of hemineglect. Hemineglect was assessed in 34 patients with right-hemisphere stroke using a letter-cancellation task and a line bisection task. No <phrase>significant correlation</phrase> (r = .39) was found between scores on the two tests. Ten patients who showed neglect on the cancellation task but performed normally on line bisection had frontal or deep lesions. Eleven patients with posterior lesions deviated rightward on line bisection; several of these had minimal or no cancellation deficit. A nonmotor task involving judgment of a bisected line was also performed abnormally by six patients with line bisection shift, suggesting that such shift does not result from a motor response asymmetry. We propose that separable components of the neglect syndrome may be associated with damage to discrete areas of the nondominant hemisphere.
Transducer Development and Characterization for Underwater Acoustic Neutrino Detection Calibration A short bipolar pressure pulse with "<phrase>pancake</phrase>" directivity is produced and propagated when an <phrase>Ultra-High</phrase> Energy (UHE) neutrino interacts with a nucleus in water. Nowadays, acoustic <phrase>sensor networks</phrase> are being deployed in deep seas to detect this phenomenon as a first step toward building a <phrase>neutrino telescope</phrase>. In order to study the feasibility of the method, it is critical to have a calibrator that is able to mimic the neutrino signature. In <phrase>previous works</phrase> the possibility of using the acoustic parametric technique for this aim was proven. In this study, the array is operated at a <phrase>high frequency</phrase> and, by means of the parametric effect, the emission of the <phrase>low-frequency</phrase> acoustic bipolar pulse is generated mimicking the UHE neutrino acoustic pulse. To this end, the development of the transducer to be used in the parametric array is described in all its phases. The transducer <phrase>design process</phrase>, the characterization tests for the bare <phrase>piezoelectric</phrase> <phrase>ceramic</phrase>, and the addition of backing and matching layers are presented. The efficiencies and directivity patterns obtained for both primary and parametric beams confirm that the design of the proposed calibrator meets all the requirements for the emitter.
Optimization of Linear Placements for Wirelength Minimization with Free Sites We study a type of linear placement problem arising in detailed placement optimization of a given cell row in the presence of white-space (extra sites). In this single-row placement problem, the cell order is fixed within the row; all cells in other rows are also fixed. We give the first solutions to the single-row problem: (i) a <phrase>dynamic programming</phrase> technique with time complexity O(m 2) where m is the number of nets incident to cells in the given row, and (ii) an O(m log m) technique that exploits the convexity of the wirelength objective. We also propose an iterative heuristic for improving cell ordering within a row; this can be run optionally before applying either (i) or (ii). Experimental results show an average of 6.5% wirelength improvement on industry <phrase>test cases</phrase> when our methods are applied to the final output of a leading industry placement tool. 1 Introduction The linear placement problem (<phrase>LPP</phrase>) is well-studied in the VLSI <phrase>physical design</phrase> literature, where it has appeared in various guises [15, 10, 11, 6, 2, 9, 13]. Traditionally, linear placement seeks to arrange a number of interconnected circuit modules within a row of locations , to minimize some objective. Applications of <phrase>LPP</phrase> are reported in, e.g., [10, 4, 5]. The <phrase>LPP</phrase> problem is NP-hard [8], and so most <phrase>existing methods</phrase> are heuristics (see [12] for a comprehensive review). In this work, we address a variant of the linear placement problem that arises during detailed placement optimization of <phrase>standard-cell</phrase> layouts. Our context is a " successive-approximation " placement methodology, e.g., (1) mixed analytic (quadratic) and partitioning-based top-down global placement (cf. most leading EDA vendor tools), (2) row balancing and legalization, (3) detailed optimization of row assignments (cf., for example, the TimberWolf placement tool [14]), and (4) detailed routability optimization (" white-space " management and pin alignment) within individual rows. Step (4) in this methodology arises because of routing hot-spots and limited <phrase>porosity</phrase> of the <phrase>cell library</phrase>: even with <phrase>deep-submicron</phrase> multilayer interconnect processes, placers must leave extra space within cell rows so that the layout is routable. Our problem differs from the traditional <phrase>LPP</phrase> formulation in that (i) cells of a given row can be placed with gaps between them, i.e., the number of legal locations is more than the number of cells, and the layout has " white-space " ; and (ii) fixed cells from other rows participate in the net wirelength (cost) …
<phrase>Web Service</phrase> integration platform for <phrase>Polish</phrase> linguistic resources This paper presents a robust linguistic <phrase>Web service</phrase> framework for <phrase>Polish</phrase>, combining several mature offline linguistic tools in a common online platform. The toolset comprise paragraph-, sentence-and token-level segmenter, morphological analyser, disambiguating tagger, shallow and deep parser, named entity recognizer and coreference resolver. Uniform access to processing results is provided by means of a stand-off packaged adaptation of National Corpus of <phrase>Polish</phrase> TEI P5-based representation and <phrase>interchange</phrase> format. A concept of asynchronous handling of requests sent to the implemented <phrase>Web service</phrase> (Multiservice) is introduced to enable processing large amounts of text by setting up <phrase>language processing</phrase> chains of desired complexity. Apart from a dedicated API, a simple Web interface to the service is presented, allowing to compose a chain of annotation services, run it and periodically check for execution results, made available as plain XML or in a simple visualization. Usage examples and results from performance and scalability tests are also included.
Experimental and Numerical Study of Pressure Drop and <phrase>Heat Transfer</phrase> in a Single-phase Micro-channel Heat Sink The pressure drop and <phrase>heat transfer</phrase> characteristics of a single-phase micro-channel heat sink were investigated both experimentally and numerically. The heat sink was fabricated from <phrase>oxygen</phrase>-free copper and fitted with a <phrase>polycarbonate</phrase> plastic cover plate. The heat sink consisted of an array of rectangular micro-channels 231 lm wide and 713 lm deep. Deionized water was employed as the cooling liquid and two <phrase>heat flux</phrase> levels, q 00 eff ¼ 100 W=cm 2 and q 00 eff ¼ 200 W=cm 2 , defined relative to the planform area of the heat sink, were tested. The <phrase>Reynolds number</phrase> ranged from 139 to 1672 for q 00 eff ¼ 100 W =cm 2 , and 385 to 1289 for q 00 eff ¼ 200 W =cm 2. The three-dimensional <phrase>heat transfer</phrase> characteristics of the heat sink were analyzed numerically by solving the conjugate <phrase>heat transfer</phrase> problem involving simultaneous determination of the temperature field in both the solid and liquid regions. Also presented and discussed is a detailed description of the local and average <phrase>heat transfer</phrase> characteristics of the heat sink. The measured pressure drop and temperature distributions show good agreement with the corresponding numerical predictions. These findings demonstrate that the conventional Navier–Stokes and energy equations can adequately predict the <phrase>fluid flow</phrase> and <phrase>heat transfer</phrase> characteristics of micro-channel heat sinks.
<phrase>Wire-length</phrase> prediction using <phrase>statistical techniques</phrase> We address the classic <phrase>wire-length</phrase> estimation problem and propose a new statistical <phrase>wire-length</phrase> estimation approach that captures the probability <phrase>distribution function</phrase> of net lengths after placement and before routing. The <phrase>wire-length</phrase> prediction model was developed using a combination of paramet-ric and non-parametric <phrase>statistical techniques</phrase>. The model predicts not only the length of the net using input parameters extracted from the floorplan of a design, but also <phrase>probability distributions</phrase> that a net with given characteristics obtained after placement will have a particular length. The model is validated using both learn-and-test and resubsti-tution techniques. The model can be used for a variety of purposes, including the generation of a large number of statistically sound and therefore realistic instances of designs. We applied the net models to the probabilistic <phrase>buffer insertion</phrase> problem and obtained substantial improvement in net delay after routing. INTRODUCTION <phrase>Wire-length</phrase> has become one of the most critical metrics in <phrase>physical design</phrase> primarily due to the rise of the deep submi-<phrase>cron</phrase> era. There is a large number of different parameters and constraints, such as the bounding box of the net, number of routing grids and the grid capacity, total number of nets routed in the vicinity of the pertinent net, that are all potentially relevant, but are typically very hard to capture into consistent <phrase>wire-length</phrase> model. Hence, estimating an exact value for the <phrase>wire-length</phrase> is a very hard problem. We have developed a new wirelength model that uses data that can be extracted once the placement of the designs is completed. In order to build the model we used a combination of parametric and non-parametric techniques [2, 3]. Statistical models and prediction methodology can be used in many ways. For example, one can use the prediction information to evaluate the suitability of a particular floorplan for obtaining final routing where nets satisfy a particular user specified condition. For instance, the goal can be to determine which among a number of competing floorplans is most likely to result in a final design with few long nets or overall small sum of wirelengths. They are also a natural component of the overall probabilistic <phrase>design automation</phrase> methodology. One such probabilistic algorithm is [5] which performs <phrase>buffer insertion</phrase> assuming wire-lengths which are estimated as distributions. We used our models in the proba-bilistic <phrase>buffer insertion</phrase> approach of [5] and obtained massive improvements in net delay (∼40%) after routing when compared with traditional bounding box strategies [1].
Deep neural <phrase>heart rate variability</phrase> analysis Despite of the pain and limited accuracy of blood tests for early recognition of <phrase>cardiovascular disease</phrase>, they dominate risk screening and <phrase>triage</phrase>. On the other hand, <phrase>heart rate variability</phrase> is non-invasive and cheap, but not considered accurate enough for clinical practice. Here, we tackle heart beat interval based classification with <phrase>deep learning</phrase>. We introduce an <phrase>end to end</phrase> differentiable <phrase>hybrid architecture</phrase>, consisting of a layer of biological neuron models of <phrase>cardiac</phrase> dynamics (modified FitzHugh Nagumo neurons) and several layers of a standard <phrase>feed-forward neural network</phrase>. The proposed model is evaluated on ECGs from 474 stable at-risk (<phrase>coronary artery disease</phrase>) patients, and 1172 <phrase>chest pain</phrase> patients of an <phrase>emergency department</phrase>. We show that it can significantly outperform models based on traditional <phrase>heart rate variability</phrase> predictors, as well as approaching or in some cases outperforming clinical blood tests, based only on 60 seconds of inter-beat intervals.
Design and Performance Verification of UHPC Piles for Deep Foundations versity is to develop and implement innovative methods, materials, and technologies for improv­ ing transportation efficiency, safety, and reliability while improving the <phrase>learning environment</phrase> of students, faculty, and staff in transportation-related fi elds. The contents of this report reflect the views of the authors, who are responsible for the facts and the accuracy of the information presented herein. The opinions, findings and conclusions expressed in this publication are those of the authors and not necessarily those of the sponsors. The sponsors assume no liability for the contents or use of the information contained in this document. This report does not constitute a standard, specification, or regulation. The sponsors do not endorse products or manufacturers. <phrase>Trademarks</phrase> or manufacturers' names appear in this report only because they are considered essential to the objective of the document. 16. Abstract The strategic plan for bridge engineering issued by <phrase>AASHTO</phrase> in 2005 identified extending the service life and optimizing structural systems of bridges in the <phrase>United States</phrase> as two grand challenges in bridge engineering, with the objective of producing safer bridges that have a minimum service life of 75 years and reduced maintenance cost. Material deterioration was identified as one of the primary challenges to achieving the objective of extended life. In substructural applications (e.g., deep foundations), construction materials such as <phrase>timber</phrase>, steel, and concrete are subjected to deterioration due to environmental impacts. Using innovative and new materials for foundation applications makes the <phrase>AASHTO</phrase> objective of 75 years service life achievable. Ultra <phrase>High Performance</phrase> Concrete (UHPC) with <phrase>compressive strength</phrase> of 180 MPa (26,000 psi) and excellent durability has been used in <phrase>superstructure</phrase> applications but not in geotechnical and foundation applications. This study explores the use of <phrase>precast</phrase>, prestressed UHPC piles in future foundations of bridges and other structures. An H-shaped UHPC section, which is 10-in. (250-mm) deep with weight similar to that of an HP10×57 steel pile, was designed to improve constructability and reduce cost. In this project, instrumented UHPC piles were cast and laboratory and <phrase>field tests</phrase> were conducted. Laboratory tests were used to verify the moment-curvature response of UHPC pile section. In the field, two UHPC piles have been successfully driven in <phrase>glacial till</phrase> <phrase>clay</phrase> soil and load tested under vertical and lateral loads. This report provides a complete set of results for the field investigation conducted on UHPC H-shaped piles. <phrase>Test results</phrase>, durability, drivability, and other material advantages over normal concrete and …
Towards <phrase>Information-Seeking</phrase> Agents We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine <phrase>deep architectures</phrase> with techniques from <phrase>reinforcement learning</phrase> to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic <phrase>rewards</phrase>. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.
On-Line Testing Symposium Issues related to on-line testing are <phrase>increasingly important</phrase> in modern electronic systems. In particular, the huge complexity of electronic systems has led to growth in reliability needs in several application domains as well as pressure for <phrase>low cost</phrase> products. There is a corresponding increasing demand for <phrase>cost-effective</phrase> on-line testing techniques. These needs have increased dramatically with the introduction of very deep submicron and <phrase>nanometer technologies</phrase> which adversely impact <phrase>noise margins</phrase> and process parameters variations and make integrating on-line testing and fault tolerance <phrase>mandatory</phrase> in many modern ICs. The International On-Line Testing Symposium (IOLTS) is an established forum for presenting novel ideas and <phrase>experimental data</phrase> on these areas. The symposium also emphasizes on-line testing in the continuous operation of large applications such as wired, cellular and satellite <phrase>telecommunication</phrase>, as well as in secure chips.
On the computational architecture of the <phrase>neocortex</phrase> This paper is a <phrase>sequel</phrase> to an earlier paper which proposed an active role for the thalamus, integrating multiple hypotheses formed in the cortex via the thalamo-cortical loop. In this paper, I put forward a hypothesis on the role of the reciprocal, topographic pathways between two cortical areas, one often a 'higher' area dealing with more abstract information about the world, the other 'lower', dealing with more concrete data. The higher area attempts to fit its abstractions to the data it receives from lower areas by sending back to them from its deep pyramidal cells a template reconstruction best fitting the lower level view. The lower area attempts to reconcile the reconstruction of its view that it receives from higher areas with what it knows, sending back from its superficial pyramidal cells the features in its data which are not predicted by the higher area. The whole calculation is done with all areas working simultaneously, but with order imposed by synchronous activity in the various top-down, bottom up loops. Evidence for this theory is reviewed and <phrase>experimental tests</phrase> are proposed. A third part of this paper will deal with extensions of these ideas to the <phrase>frontal lobe</phrase>.
The Impact of Diversity on Online Ensemble Learning in the Presence of Concept Drift —On-line <phrase>learning algorithms</phrase> often have to operate in the presence of concept drift (i.e., the concepts to be learnt can change with time). This paper presents a new categorization for concept drift, separating drifts according to different criteria into mutually exclusive and non-heterogeneous categories. Moreover, although ensembles of learning machines have been used to learn in the presence of concept drift, there has been no deep study of why they can be helpful for that and which of their features can contribute or not for that. As diversity is one of these features, we present a diversity analysis in the presence of different types of drift. We show that, before the drift, ensembles with less diversity obtain lower test errors. On the other hand, it is a good strategy to maintain highly diverse ensembles to obtain lower test errors shortly after the drift independent on the type of drift, even though high diversity is more important for more severe drifts. Longer after the drift, high diversity becomes less important. Diversity by itself can help to reduce the initial increase in error caused by a drift, but does not provide a faster recovery from drifts in <phrase>long term</phrase>.
A Randomized Exhaustive Propositionalization Approach for <phrase>Molecule</phrase> Classification <phrase>Drug discovery</phrase> is the process of designing <phrase>compounds</phrase> that have desirable properties, such as activity and non-<phrase>toxicity</phrase>. <phrase>Molecule</phrase> classification techniques are used along this process to predict the properties of the <phrase>compounds</phrase> in order to expedite their testing. Ideally, the classification rules found should be accurate and reveal novel chemical properties, but current <phrase>molecule</phrase> representation techniques lead to less than adequate accuracy and <phrase>knowledge discovery</phrase>. This work extends the propositionalization approach recently proposed for multi-relational <phrase>data mining</phrase> in two ways: it generates expressive attributes exhaustively and it uses <phrase>randomization</phrase> to sample a limited set of complex (" deep ") attributes. Our <phrase>experimental tests</phrase> show that the procedure is able to generate meaningful and interpretable attributes from molecular structural data, and that these features are effective for classification purposes.
Transcribing code-switched bilingual lectures using deep neural networks with unit merging in <phrase>acoustic modeling</phrase> This paper considers the transcription of the widely observed yet less investigated bilingual code-switched speech: the words or phrases of the guest language are inserted within the utterances of the host language, so the languages are switched back and forth within an utterance, and much less data are available for the guest language. Two approaches utilizing the <phrase>deep neural network</phrase> (DNN) were tested and analyzed, including using DNN bottleneck features in HMM/GMM (BF-HMM/GMM) and modeling context-dependent HMM <phrase>senones</phrase> by DNN (<phrase>CD</phrase>-DNN-HMM). In both cases the unit merging (and recovery) techniques in acoustic mod-eling were used to handle the data imbalance problem. Improved recognition accuracies were observed with unit merging (and recovery) for the two approaches under different conditions.
Clustering Structured <phrase>Web Sources</phrase>: A Schema-Based, <phrase>Model-Differentiation</phrase> Approach The Web has been rapidly " deepened " with the prevalence of databases on-line. On this " <phrase>deep Web</phrase>, " numerous sources are structured, providing schema-rich data– Their schemas define the object domain and its query capabilities. This paper proposes clustering sources by their <phrase>query schemas</phrase>, which is critical for enabling both source selection and query mediation, by organizing sources of with similar query capabilities. In abstraction, this problem is essentially clustering categorical data (by viewing each query schema as a transaction). Our approach hypothesizes that " homogeneous sources " are characterized by the same hidden <phrase>generative models</phrase> for their schemas. To find clusters governed by such statistical distributions, we propose a novel <phrase>objective function</phrase>, <phrase>model-differentiation</phrase>, which employs principled <phrase>hypothesis testing</phrase> to maximize statistical heterogeneity among clusters. Our <phrase>evaluation shows</phrase> that, on clustering the Web <phrase>query schemas</phrase>, the <phrase>model-differentiation</phrase> function outperforms existing ones with the hierarchical agglomerative clustering algorithm.
<phrase>Malaria</phrase> control in <phrase>Zambia</phrase> and <phrase>southern Africa</phrase>. The purpose of this article is to review the spectrum of <phrase>image-based</phrase> diagnostic tools used in the investigation of suspected <phrase>deep vein thrombosis</phrase> (DVT). Summary of the experience gained by the author as well as relevant publications, regarding vein <phrase>imaging modalities</phrase> taken from a computerized database, was reviewed. The <phrase>imaging modalities</phrase> reviewed include phlebography, color Doppler duplex <phrase>ultrasonography</phrase> (CDDUS), computerized tomography <phrase>angiography</phrase> (<phrase>CTA</phrase>) and venography (<phrase>CTV</phrase>), <phrase>magnetic resonance</phrase> venography (MRV), and <phrase>radionuclide</phrase> venography (RNV). CDDUS is recommended as the modality of choice for the diagnosis of DVT. A strategy combining clinical score and <phrase>D-dimer</phrase> test refines the selection of patients. Phlebography is reserved for discrepant noninvasive studies.
Expectations and Memory in Link Search Expectations and Memory in Link Search EXPECTATIONS AND MEMORY IN LINK SEARCH 2 ABSTRACT Strategies in searching a link from a web page can rely either on expectations of prototypical locations or on memories of earlier visits to the page. What is the nature of these expectations, how are locations of web objects remembered, and how do the expectations and memories control search? These questions were investigated in an experiment where, in the experimental group, nine experienced users searched links. To obtain information about expectations, users' eye movements were recorded. Memory for locations of web objects was tested immediately afterwards. In the control group, nine matched users had to guess the locations of web objects without seeing the page. Eye-movement data and control group's guesses both indicated a robust expectation of links residing on the left side of the page. Only the location of task-relevant objects could be recollected, indicating that <phrase>deep processing</phrase> is required for memories to become consciously accessible. A comparison between the experimental and control groups revealed that what is represented in memory was not an individual link's location but the approximate locations of link panels. Furthermore, it is argued that practice-related decreases in reaction time reflect savings in <phrase>reprocessing</phrase> caused by priming. Roles for the types of memory in link search are proposed.
<phrase>Soft-Error Rate</phrase> Testing of <phrase>Deep-Submicron</phrase> <phrase>Integrated Circuits</phrase> <phrase>Soft errors</phrase> induced by radiation pose a major challenge for the reliability of complex chips processed in state-of-the-art technologies. This paper reviews <phrase>soft-error rate</phrase> (SER) characterization by real-time system-SER testing and by accelerated testing. Additionally, we present scaling trends, simulation approaches, and improvement techniques. Special attention is given to soft errors in <phrase>combinational logic</phrase>.
From Requirements to Code: Issues and Learning in IS Students' Systems Development Projects Executive Summary The Computing Curricula (2005) place <phrase>Information Systems</phrase> (IS) at the intersection of exact sciences (e.g. General Systems T heory), technology (e.g. <phrase>Computer Science</phrase>), and behavioral sciences (e.g. <phrase>Sociology</phrase>). This presents particular challenges for <phrase>teaching and learning</phrase>, as future IS professionals need to be equipped with a wide range of analytical and <phrase>critical thinking</phrase> skills that will enable them to solve business problems. In addition, they require technical, strong interper-sonal communication, and team skills to contribute to the successful delivery of software products. At the University of Cape T own (<phrase>UCT</phrase>) the capstone course of the IS undergraduate curriculum is structured around three main areas: <phrase>Project Management</phrase>; People Management; and Implementation. T he theoretical parts of this course introduce the student to important aspects of managing projects and people in the Information Communication and T echnology (ICT) Project environment. The practical part comprises a group systems development project, which forms a core part of the course and requires students to apply theoretical skills in a real-world context. Although the impact of the issues relating to <phrase>soft skills</phrase> on student learning is neither underestimated nor ignored in the course, this paper mainly focuses on the technical issues that are experienced during the life of the projects. Students generally experience difficulty in the areas of <phrase>problem-solving</phrase>, coding and testing, all of which are required for successful systems development. IS students are often less technically oriented than their counterparts in the other computing disciplines and their courses involve less technical content. As a result, they may be inadequately prepared for the technical demands of the project. IS professionals must be able to interact with business experts and apply <phrase>problem-solving</phrase> skills in developing possible solutions. It is thus reasonable to argue that the completion of a full life cycle of a project provide IS students with invaluable experience in testing the effectiveness of their proposed solution. A reflective approach has been applied to the course design, resulting in the development of a framework to sufficiently address the issues of <phrase>problem-solving</phrase>, coding, and testing through an action learning cycle. This approach has proved to lead to improved solutions and to encourage <phrase>deep learning</phrase>. It also shows how teaching practices are shaped by looking back reflexively of student learning and the facilitating environments. This paper describes how the course has evolved through four phases, culminating in an approach that guides students Material published as part …
<phrase>Application Layer</phrase> Joint Coding for Image Transmission over <phrase>Deep Space</phrase> Channels —In this paper a method to realize the joint <phrase>application layer</phrase> coding for image transmission over <phrase>deep space</phrase> channels has been presented. In more technical detail, both <phrase>image compression</phrase>, based on algorithms such as JPEG2000 and CCSDS, and encoding techniques, such as LDPC codes, to protect the sent images are simultaneously applied by the proposed mechanism. It acts on the bases of the deep space channel conditions, in terms of <phrase>Bit Error Rate</phrase>, and it is based on the Multi-Attribute <phrase>Decision Making</phrase> theory. In practice, the proposal is aimed at protecting the essential informative contents of images sent through a <phrase>deep space network</phrase> and, at the same time, allows minimizing the load offered (the total amount of data to transmit) by the overall <phrase>application layer</phrase> coding process to the <phrase>deep space network</phrase>. The presented mechanism has been tested through simulations. The obtained results show the effectiveness of the proposal and open the door to further developments of the method in real systems. I. INTRODUCTION The communication impairments in <phrase>Deep Space</phrase> Networks (DSNs) are due to the hostility of the <phrase>space environment</phrase> and, as a consequence, significant efforts to improve the performance of existing <phrase>communication systems</phrase> [1] are required. Nowadays, the technological advances allow connecting heterogeneous terminals separated by thousands of kilometers with satisfactory levels of quality, reliability and flexibility. In fact, by exploiting the transmission capacity of the radio channel, it is possible to achieve radio link for satellite systems (i.e., GEO, MEO and LEO) with performance levels comparable to wired-line technologies.
Removal of <phrase>Impulse Noise</phrase> Using Fuzzy <phrase>Genetic Algorithm</phrase> <phrase>Digital image processing</phrase> plays a key role in <phrase>medical diagnosis</phrase>. Medical images are obtained and analyzed to determine the presence or absence of abnormalities such as tumor, which is vital in understanding the type and magnitude of a disease. Unfortunately, medical images are susceptible to <phrase>impulse noise</phrase> during acquisition, storage and transmission. Hence, image de-noising is a primary precursor for medical <phrase>image analysis</phrase> tasks. Noise removal can be done much more efficiently by a combination of image filters or a composite filter, than by a single image filter. Determining the appropriate filter combination is a difficult task. In this paper, we propose a technique that uses Fuzzy <phrase>Genetic Algorithm</phrase> to find the optimal composite filters for removing all types of <phrase>impulse noise</phrase> from medical images. Here, a Fuzzy Rule Base is used to adaptively change the crossover probability of the <phrase>Genetic Algorithm</phrase> used to determine the optimal composite filters. We use <phrase>Genetic Algorithm</phrase> (GA) to determine composite filters that remove different levels of <phrase>impulse noise</phrase> from an image. In this method, the GA considers a set of possible filter combinations of a particular length, selects the best combinations among them according to a fitness value assigned to each combination based on a <phrase>fitness function</phrase>, and applies genetic operators such as crossover and <phrase>mutation</phrase> on the selected combinations to create the next generation of composite filters. We expect that the results of simulations on a set of standard test images for a wide range of noise <phrase>corruption</phrase> levels will show that the proposed method output performs standard procedures for <phrase>impulse noise</phrase> removal both visually and in terms of performance measures such as PSNR, IQI and Tenengrad values. I.INTRODUCTION <phrase>Image denoising</phrase> refers to the recovery of a <phrase>digital image</phrase> that has been contaminated by additive white <phrase>Gaussian noise</phrase> (AWGN). Many scientific data are contaminated with noise, either because of the <phrase>data acquisition</phrase> process, or because of naturally occurring phenomena. A first pre-processing step in analyzing such data is denoising, that is, estimating the unknown signal of interest from the available <phrase>noisy data</phrase>. There are several different approaches to denoise images. The AWGN channel is a good model for many satellite and deep <phrase>space communication</phrase> links. Since Wideband <phrase>Gaussian noise</phrase> comes from many natural sources, such as the thermal vibrations of <phrase>atoms</phrase> in conductors (referred to as thermal noise or Johnson-Nyquist noise), <phrase>shot noise</phrase>, <phrase>black body</phrase>
Application of Functional <phrase>Delay Tests</phrase> for Testing of <phrase>Transition Faults</phrase> and vice Versa Rapid advances of <phrase>semiconductor technology</phrase> lead to higher circuit integration as well as higher <phrase>operating frequencies</phrase>. The statistical variations of the parameters during the <phrase>manufacturing process</phrase> as well as <phrase>physical defects</phrase> in <phrase>integrated circuits</phrase> can sometimes degrade <phrase>circuit performance</phrase> without altering its logic functionality. These faults are called <phrase>delay faults</phrase>. In this paper we consider the quality of the tests generated for two types of <phrase>delay faults</phrase>, namely, <phrase>functional delay</phrase> and <phrase>transition faults</phrase>. We compared the <phrase>test quality</phrase> of functional <phrase>delay tests</phrase> in regard to <phrase>transition faults</phrase> and vice versa. We have performed various comprehensive experiments with combinational <phrase>benchmark circuits</phrase>. The experiments exhibit that the <phrase>test sets</phrase>, which are generated according to the functional <phrase>delay fault model</phrase>, obtain high fault coverages of <phrase>transition faults</phrase>. However, the functional <phrase>delay fault</phrase> coverages of the <phrase>test sets</phrase> targeted for the <phrase>transition faults</phrase> are low. It is very likely that the <phrase>test vectors</phrase> based on the functional <phrase>delay fault model</phrase> can cover other kinds of the faults. Another advantage of <phrase>test set</phrase> generated at the functional level is that it is independent of and effective for any implementation and, therefore, can be generated at <phrase>early stages</phrase> of the design process. Rapid advances of <phrase>semiconductor technology</phrase> lead to higher circuit integration as well as higher <phrase>operating frequencies</phrase>. Conventional <phrase>fault models</phrase> like the standard single stuck-at model were developed for <phrase>gate-level</phrase> logic circuits. Regardless of <phrase>stuck-at fault</phrase> mo-del's efficiency for several decades, alternative models need to account for <phrase>deep sub-micron</phrase> manufacturing <phrase>process variations</phrase> [1]. Increasing performance requirements of circuits make it difficult to design them with large timing margins. Thus imprecise delay modelling, statistical variations of the parameters during the <phrase>manufacturing process</phrase> as well as <phrase>physical defects</phrase> in <phrase>integrated circuits</phrase> can sometimes degrade <phrase>circuit performance</phrase> without altering its logic func-tionality. These faults are called <phrase>delay faults</phrase>. Ensuring that the designs meet the performance specifications requires application of <phrase>delay tests</phrase>. However, <phrase>delay fault testing</phrase> of <phrase>deep submicron designs</phrase> is a complex task. It requires application of two-vector patterns at the circuit's intended operating speed. The <phrase>test application</phrase> usually requires <phrase>high-speed</phrase> testers or it could also be done through built-in self-test [2]. Two general types of <phrase>delay fault</phrase> models, the gate <phrase>delay fault model</phrase> [3, 4] and the <phrase>path delay fault</phrase> model [5], have been used for modelling <phrase>delay defects</phrase>. Although the <phrase>path delay fault</phrase> model is generally considered to be more realistic and effective in modelling physical …
Center Pivot Design for Effluent <phrase>Irrigation</phrase> of Agricultural <phrase>Forage</phrase> Crops Center Pivot Design for Effluent <phrase>Irrigation</phrase> of Agricultural <phrase>Forage</phrase> Crops Balancing the continuous supply of domestic wastewater from effluent treatment plants with fluctuating crop water demands requires unique <phrase>irrigation</phrase> design strategies. A key design consideration when utilizing disinfected secondary treated city water is to maximize the re-use of effluent in winter months, when <phrase>forage</phrase> crop water demands are low, yet still produce minimal deep <phrase>percolation</phrase>. Twenty-seven center pivots in <phrase>Palmdale</phrase>, California required new custom-designed sprinkler packages to dispose of approximately 7,000 gallons per minute of treated wastewater. Through innovative design efforts, extensive testing and field experimentation, a standardized package has been adopted by the <phrase>County</phrase> <phrase>Sanitation</phrase> <phrase>District</phrase> of <phrase>Los Angeles</phrase> <phrase>County</phrase> that enables a highly efficient application of re-use city wastewater without groundwater degradation throughout the year. Many factors influenced the selection of the sprinkler package Introduction Currently in the <phrase>United States</phrase>, many locations use <phrase>reclaimed water</phrase>. <phrase>Reclaimed water</phrase> is treated effluent which is typically for non-potable uses, such as <phrase>irrigation</phrase>. Historically, treated effluent from <phrase>wastewater treatment</phrase> facilities was discharged directly into a stream, <phrase>river</phrase>, or other natural body of water. However, the continued demand for <phrase>fresh water</phrase> supplies has increased need for reuse of treated wastewater. Using <phrase>reclaimed water</phrase> for non-potable use saves <phrase>potable water</phrase> for drinking, since less <phrase>potable water</phrase> will be used for non-potable uses.
Molecular Event Extraction from Link Grammar <phrase>Parse Trees</phrase> in the BioNLP'09 Shared Task We present an approach for extracting molecular events from literature based on a <phrase>deep parser</phrase>, using in a <phrase>query language</phrase> for <phrase>parse trees</phrase>. Detected events range from <phrase>gene expression</phrase> to protein localization, and cover a multitude of different entity types, including genes/proteins, binding sites, and locations. Furthermore, our approach is capable of recognizing <phrase>negation</phrase> and the <phrase>speculative</phrase> character of extracted statements. We first parse documents using Link Grammar (BioLG) and store the <phrase>parse trees</phrase> in a database. Events are extracted using a <phrase>newly developed</phrase> <phrase>query language</phrase> with traverses the BioLG linkages between trigger terms, arguments, and events. The concrete queries are learnt from an annotated corpus. On the BioNLP Shared Task <phrase>test data</phrase>, we achieve an overall <phrase>F1</phrase>-measure of 32, 29, and 30% for tasks 1, 2, and 3, respectively.
F-structure Transfer-based <phrase>Statistical Machine Translation</phrase> In this paper, we describe a statistical <phrase>deep syntactic</phrase> transfer decoder that is trained fully automatically on parsed bilingual corpora. <phrase>Deep syntactic</phrase> transfer rules are induced automatically from the f-structures of a LFG parsed bitext corpus by automatically <phrase>aligning</phrase> local f-structures, and inducing all rules consistent with the node alignment. The transfer decoder outputs the n-best <phrase>TL</phrase> f-structures given a SL f-structure as input by applying large numbers of transfer rules and searching for the best output using a log-<phrase>linear model</phrase> to combine feature scores. The decoder includes a fully integrated dependency-based tri-gram <phrase>language model</phrase>. We include an experimental evaluation of the decoder using different parsing disambiguation resources for the German data to provide a comparison of how the system performs with different German <phrase>training and test</phrase> parses.
<phrase>Complex network</phrase> perspective on network dynamics: a study on investment network in VC industry in China From the <phrase>complex network</phrase> perspective, using <phrase>social network analysis</phrase> methods, we aim to systematically conduct in-deep research on the emergence and evolution of the investment network in the <phrase>venture capital</phrase> industry in China to show the network dynamics within <phrase>a 12</phrase>-year period. We develop and test 4 alternative logics of attachment - accumulative advantage, follow-the-trend, multiconnectivity and trust-transfer -- to account for both the structure and dynamics of investment in the <phrase>venture capital</phrase> industry. In addition, we map the network dynamics of the field over the years of 1995--2006 and present the results using network visualization. Through systematical analysis of the <phrase>long term</phrase> data in the VC field, we can elaborate the action of <phrase>VCs</phrase> from the network perspective, supplement the existing literature on investment motivation of <phrase>VCs</phrase>, enrich the social meaning of investment behavior and help to better understand the practical condition in this field.
Powerset’s <phrase>Natural Language</phrase> Wikipedia <phrase>Search Engine</phrase> This demonstration shows the capabilities and features of Powerset's <phrase>natural language</phrase> <phrase>search engine</phrase> as applied to the <phrase>English Wikipedia</phrase>. <phrase>Powerset</phrase> has assembled scalable <phrase>document retrieval</phrase> technology to construct a semantic index of the <phrase>World Wide Web</phrase>. In order to develop and test our technology, we have released a search product (at http://www.powerset.com) that incorporates all the information from the <phrase>English Wikipedia</phrase>. The product also integrates community-edited content from Metaweb's <phrase>Freebase</phrase> database of structured information. Users may query the index using keywords, <phrase>natural language</phrase> questions or phrases. Retrieval latency is comparable to standard keyword based consumer <phrase>search engines</phrase>. <phrase>Powerset</phrase> semantic indexing is based on the XLE, <phrase>Natural Language Processing</phrase> technology <phrase>licensed</phrase> from the <phrase>Palo Alto</phrase> <phrase>Research Center</phrase> (PARC). During both indexing and querying, we apply our deep <phrase>natural language</phrase> <phrase>analysis methods</phrase> to extract semantic " facts "-relations and semantic connections between words and concepts-from all the sentences in Wikipedia. At query time, advanced search-engineering technology makes these facts available for retrieval by matching them against facts or partial facts extracted from the query. In this demonstration, we show how retrieved information is presented as conventional <phrase>search results</phrase> with links to relevant Wikipedia pages. We also demonstrate how the distilled <phrase>semantic relations</phrase> are organized in a browsing format that shows relevant subject/relation/object triples related to the user's query. This makes it easy both to find other relevant pages and to use our Search-Within-The-Page feature to localize additional semantic searches to the text of the selected target page. Together these features summarize the facts on a page and allow navigation directly to information of interest to individual users. Looking ahead beyond continuous improvements to core search and scaling to much larger collections of content, Powerset's automatic extraction of semantic facts can be used to create and extend knowledge resources including lexicons, ontologies, and entity profiles. Our system is already deployed as a consumer-search <phrase>web service</phrase>, but we also plan to develop an API that will enable programmatic access to our structured representation of text.
Comparing Low and <phrase>High-Fidelity</phrase> Prototypes in <phrase>Mobile Phone</phrase> Evaluation This study compared <phrase>usability testing</phrase> results found with low-and <phrase>high-fidelity</phrase> prototypes for mobile <phrase>phones</phrase>. The main objective is to obtain deep understanding of usability problems found with different prototyping methods. Three mobile <phrase>phones</phrase> from different manufactures were selected in the experiment. The usability level of the mobile <phrase>phones</phrase> was evaluated by participants who completed a questionnaire consisting of 13 usability factors. Incorporating the task-based complexity of the three mobile <phrase>phones</phrase>, significant differences in the <phrase>usability evaluation</phrase> for each individual factor were found. Suggestions on <phrase>usability testing</phrase> with prototyping technique for mobile <phrase>phones</phrase> were proposed. This study tries to provide new evidence to the field of <phrase>mobile phone</phrase> usability research and develop a feasible way to quantitatively evaluate the prototype usability with novices. The comparisons of paper-based and fully functional prototypes led us to realize how significantly the unique characteristics of different prototypes affect the <phrase>usability evaluation</phrase>. The experiment took product complexity into account and made suggestions on choosing proper prototyping technique for testing particular aspects of <phrase>mobile phone</phrase> usability.
A <phrase>Web-Based</phrase> Agent Challenges Human Experts on Crosswords ames and puzzles reproduce the complexity of the <phrase>real world</phrase> with " the smallest initial structures " (<phrase>Minsky</phrase> 1968), thus making their study important for both theoretical and practical issues. Since the birth of <phrase>artificial intelligence</phrase> (AI), games and puzzles have received much attention. The game that has captured most of the attention of computer scientists is chess. Games play the role of a laboratory where machines can safely be tested by a direct competition with humans. Along with the development of successful game-playing programs, the investigation should also include a methodological discussion on how this performance is achieved. <phrase>Deep Blue</phrase> heavily relied on computational power joined with a <phrase>search algorithm</phrase> based on static evaluation functions to assess the different configurations of the game. Instead of relying on a preprogrammed approach, Tesauro conceived <phrase>TD</phrase>-■ Crosswords are very popular and represent a useful domain of investigation for modern <phrase>artificial intelligence</phrase>. In contrast to solving other celebrated games (such as chess), cracking crosswords requires a <phrase>paradigm shift</phrase> towards the ability to handle tasks for which humans require extensive semantic knowledge. This article introduces WebCrow, an automatic <phrase>crossword</phrase> solver in which the needed knowledge is mined from the web: clues are solved primarily by accessing the web through <phrase>search engines</phrase> and applying <phrase>natural language processing</phrase> techniques. In competitions at the European Conference on <phrase>Artificial Intelligence</phrase> (ECAI) in 2006 and other conferences this <phrase>web-based</phrase> approach enabled WebCrow to out-perform its human challengers. Just as chess was once called " the <phrase>Drosophila</phrase> of <phrase>artificial intelligence</phrase> , " we believe that <phrase>crossword</phrase> systems can be useful <phrase>Drosophila</phrase> of <phrase>web-based</phrase> agents.
<phrase>Syntactic Features</phrase> for <phrase>Protein-Protein</phrase> Interaction Extraction Background: Extracting <phrase>Protein-Protein Interactions</phrase> (PPI) from <phrase>research papers</phrase> is a way of translating information from English to the language used by the databases that store this information. With recent advances in automatic PPI detection, it is now possible to speed up this process considerably. <phrase>Syntactic features</phrase> from different parsers for biomedical English text are readily available, and can be used to improve the performance of such <phrase>PPI extraction</phrase> systems. Results: A complete PPI system was built. It uses a <phrase>deep syntactic</phrase> parser to capture the semantic meaning of the sentences, and a shallow dependency parser to improve the performance further. <phrase>Machine learning</phrase> is used to automatically make rules to extract pairs of interacting proteins from the semantics of the sentences. The results have been evaluated using the AImed corpus, and they are better than earlier <phrase>published results</phrase>. The F-score of the current system is 69.5% for <phrase>cross-validation</phrase> between pairs that may come from the same abstract, and 52.0% when complete abstracts are hidden until final testing. Automatic 10-<phrase>fold cross-validation</phrase> on the entire AImed corpus can be done in less than 45 minutes on a single server. We also present some previously unpublished statistics about the AImed corpus, and a short analysis of the AImed representation language. Conclusions: We present a <phrase>PPI extraction</phrase> system, using different syntactic parsers to extract features for SVM with Tree Kernels, in order to automatically create rules to discover <phrase>protein interactions</phrase> described in the <phrase>molecular biology</phrase> literature. The system performance is better than other published systems, and the implementation is freely available to anyone who is interested in using the system for academic purposes. The system can help researchers quickly discover reported PPIs, and thereby increasing the speed at which databases can be populated and novel <phrase>signaling pathways</phrase> can be constructed.
A hybrid nano-CMOS architecture for defect and fault tolerance As the end of the semiconductor roadmap for CMOS approaches, architectures based on nanoscale molecular devices are attracting attention. Among several alternatives, silicon nanowires and <phrase>carbon nanotubes</phrase> are the two most promising <phrase>nanotechnologies</phrase> according to the ITRS. These technologies may enable scaling deep into the nanometer regime. However, they suffer from very defect-prone manufacturing processes. Although the reconfigurability property of the nanoscale devices can be used to tolerate high defect rates, it may not be possible to locate all defects. With very high device densities, testing each component may not be possible because of time or technology restrictions. This points to a scenario in which even though the devices are tested, the tests are not very comprehensive at locating defects, and hence the shipped chips are still defective. Moreover, the devices in the nanometer range will be susceptible to <phrase>transient faults</phrase> which can produce arbitrary <phrase>soft errors</phrase>. Despite these drawbacks, it is possible to make nanoscale architectures practical and realistic by introducing defect and fault tolerance. In this article, we propose and evaluate a hybrid <phrase>nanowire</phrase>-CMOS architecture that addresses all three problems&#8212;namely high defect rates, unlocated defects, and <phrase>transient faults</phrase>&#8212;at the same time. This goal is achieved by using multiple levels of redundancy and majority voters. A key aspect of the architecture is that it contains a judicious balance of both nanoscale and traditional CMOS components. A companion to the architecture is a compiler with heuristics to quickly determine if logic can be mapped onto partially defective nanoscale elements. The heuristics make it possible to introduce defect-awareness in placement and routing. The architecture and compiler are evaluated by applying the complete <phrase>design flow</phrase> to several benchmarks.
<phrase>Field Testing</phrase> of K10 with HYDRA at <phrase>NASA Ames Research Center</phrase> <phrase>High resolution</phrase> hydrogen surface mapping is essential for locating and characterizing <phrase>water ice</phrase> and other hydrogenous volatile deposits in permanently shadowed lunar <phrase>craters</phrase>. This is especially important for potential in-situ <phrase>resource utilization</phrase>. Although orbital <phrase>remote sensing</phrase> can provide much information, <phrase>prospecting</phrase> for near-subsurface resources can only be performed directly on the surface. The small HYDRA neutron <phrase>spectrometer</phrase> has been successfully integrated onto the K10 Black planetary rover, operated by the Intelligent Robotics Group at <phrase>NASA Ames Research Center</phrase> [1,2]. The system was used to assess hydrogen content in an initial set of <phrase>field tests</phrase> at Ames. During these tests, we successfully detected and mapped targets of various hydrogen contents and <phrase>burial</phrase> depths. The objectives of the exercise were as follows: • Integrate and operate the HYDRA neutron <phrase>spectrometer</phrase> with the K10 Black rover. • Acquire HYDRA data as the rover navigates a grid of GPS waypoints chosen without <phrase>prior knowledge</phrase> of the target locations. • Detect and localize near-surface enhanced hydrogen deposits within the rover test area. The K10/HYDRA rover tests were carried out from <phrase>Sept</phrase>. 18 – 20, 2007 at an unvegetated pad of fill dirt, north of building TA27B at the <phrase>NASA Ames Research Center</phrase>. The area is shown in Figure 1. The extent of the test area was approximately 50 meters in the north-south direction, and 20 meters in the east-west direction. Within this area holes were excavated. One set of targets consisted of 3x3 foot <phrase>polyethylene</phrase> slabs (each 0.5 in thick, stacked 8 deep for an overall thickness of ~10 cm), buried at depths of 0, 5, 15 and 30 cm. Another set of targets consisted of 4x4 ft, 2x2 ft, and 1x1 ft stacks of cut 0.5-inch thick <phrase>drywall</phrase> (<phrase>gypsum</phrase>), each stacked 12 deep for an overall thickness of 15 cm. Finally, some excavations were simply back-filled with the excavated material to act as decoys for the test. The locations of the <phrase>polyethylene</phrase> and <phrase>gypsum</phrase> targets were known only to three personnel involved in the exercise, in order to provide a single-blind test.
Review of "Explorer's Guide to the <phrase>Semantic Web</phrase> by Thomas B. Passin" A book with a dark-gray hat on its cover and the subtitle " How to Break Code " makes a strong statement. It does not disappoint. It covers many form of exploiting software that you never dared to explore. The book approaches its problem from many security disciplines. It takes on the <phrase>reverse-engineering</phrase> angle to break <phrase>copyright protection</phrase> systems or to find software defects. It takes the pentest (<phrase>penetration test</phrase>) view when it explores how to attack server-side software, with local and remote attack options. It describes the botnet (robot network) master's options when it targets client software problems. It shows how to hide <phrase>malware</phrase> (<phrase>malicious software</phrase>) via the <phrase>rootkit</phrase> approach, diving deep—even into <phrase>flash memory</phrase>—and evading forensic analysis. The authors also present more conceptual views, such as the root cause of software security problems, 49 <phrase>attack patterns</phrase>, how to craft malicious input, and buffer overflows in all variations. Each topic includes a tutorial, sample systems or code, and known exploits using these techniques. If the topic is unfamiliar, the tutorial may be insufficient, but links to further information are provided. The sample code is clear enough to allow smarter scripters to elaborate on it. There are not many details on the known exploits, but a simple <phrase>Web search</phrase> on any of the key terms will provide all that are necessary. The book is about 450 pages, and contains eight chapters. The three longest chapters are on <phrase>reverse engineering</phrase> , <phrase>buffer overflow</phrase>, and rootkits. The others are on software, <phrase>attack patterns</phrase>, exploiting server software, and exploiting client software Who should read this book? The authors start by defending why anyone would write such a book. They show that anything they describe has been exploited already. They spell out how it was done, loud and clear. This takes away ignorance. So, if your job is to build secure software systems, to implement license or <phrase>copyright protection</phrase> systems, to pentest systems, or to do forensic analysis, you will benefit from reading the book. Ed Felten, <phrase>Princeton University</phrase> professor of computer science, is quoted on the cover: " It's hard to protect yourself if you don't know what you're up against. " But having that knowledge, after reading this book, may not improve your <phrase>peace</phrase> of mind. —A. Mariën Passin, a principal <phrase>systems engineer</phrase> specializing in <phrase>data modeling</phrase>, <phrase>Web databases</phrase>, and XML projects. Thus, it is not unexpected to find that he covers …
<phrase>Model-based development</phrase>: applications by H.S. Lahman I picked up this book because I am always interested in learning about new <phrase>software design</phrase> methodologies. I learned Structured design by analyzing PL/1 code and <phrase>Object-oriented design</phrase> by creating <phrase>Smalltalk</phrase> and Java applications. This book was my first introduction to <phrase>Model-Based development</phrase>. Fundamentally, <phrase>Model-based development</phrase> (MBD) is not much different than an <phrase>object-oriented</phrase> (OO) approach. In MBD the focus is on <phrase>large-scale</phrase> re-use instead of object re-use. In MBD a deep understanding of the customer's domain is essential in producing a design which will stand the test of time. The developer should look for modeling invariants, groups that are stable in the customer's problem space. The software structure should model the customer's infrastructure. Instead of finding a generalized structure which works across many different domains the goal is to find structures relative to the customer's space being analyzed. Design for re-use during the evolution of an organization rather than re-use among different organizations. Another difference between <phrase>Object-oriented design</phrase> and <phrase>Model-Based development</phrase> is how they handle implementation <phrase>hiding</phrase>. MBD is more restrictive in the accessibility of subsystems than OO applications. I like how MBD approaches this better than OO design. I agree with the author that subsystems should be hidden from one another and only interact through well-defined interfaces. When I develop software I don't like passing around handlers all over my application. This is one part of the book where I think the author really <phrase>hits</phrase> it on the nose. Code should reduce dependencies instead of adding them through implementation <phrase>hiding</phrase>. What I enjoyed about this book is that it is not trying to sell the reader on only <phrase>Model-Based development</phrase>. Instead the book discusses the thought processes behind <phrase>software design</phrase>. There is a lot of theory and not a lot of code. For me this was the only negative aspect of the book. I like to see code accompany the discussion about design. It allows me to apply a new <phrase>design methodology</phrase> faster when I can see how it is used in a <phrase>programming language</phrase>. Although, the vast UML examples allow the reader to quickly take any <phrase>Object-oriented language</phrase> and apply the design principles described in the book. The book is divided into three parts. The first part discusses the history of <phrase>object-oriented design</phrase> principles. There is a Pet Care center example which really highlights the use of MBD and how to identify subsystems. This example helps build …
Parallel-pipeline-based traversal unit for hardware-accelerated <phrase>ray tracing</phrase> In this work, we propose a novel parallel-pipeline traversal unit for <phrase>hardware-based</phrase> <phrase>ray tracing</phrase>, which can reduce latency and increase cache locality. Owing to the high memory bandwidth and computation requirements of <phrase>ray-tracing</phrase> operations such as traversal and intersection tests, <phrase>recent studies</phrase> have focused on the development of a <phrase>hardware-based</phrase> traversal and intersection-test unit[Nah et al. 2011][Lee et al. 2012]. Existing hardware engines are based on a single deep pipeline structure that increases the throughput of ray processing per unit time. However, traversal operations involve <phrase>non-deterministic</phrase> changes in the states of a ray. Therefore, in some cases, the ray may be unnecessarily transferred between pipeline stages, thereby increasing the overall latency. In order to solve this problem, we propose a parallel traversal unit having a pipeline per state. Our results show that the proposed system is up to 30% more efficient than a single-pipeline system because it decreases average latency per ray and increases cache efficiency.
Playfully teaching <phrase>artificial intelligence</phrase> by implementing games to undergraduates Using term-work, practical and project <phrase>based approach</phrase> is a common practice for teaching AI course. Teaching <phrase>Artificial Intelligence</phrase> is always a great task for teachers of technical institutes. It is common practice to use computer games to be used as a tool to help introduce basic <phrase>computer science</phrase> concepts to the students. Digital games could have a much bigger role in learning than just as a motivational tool.  In this paper, it is observed the efforts of teaching <phrase>Artificial Intelligence</phrase> (AI) concepts to <phrase>undergraduate students</phrase> of computer engineering and <phrase>information technology</phrase> with games, because game motivate students, which increase retention and helps to become better computer engineers. Paper describes a case study of all types of games and/or puzzles inculcate in teaching AI concepts and other searching algorithms to the students. Teaching informed searching techniques like generate-and-test, <phrase>hill climbing</phrase>, A-star, <phrase>AO</phrase>-star, <phrase>constraint satisfaction</phrase> problems and means-end analysis is easy, if <phrase>real-life</phrase>-problems or puzzles are used for explanation. Using water-jug problem, eight-<phrase>puzzle</phrase> (sliding tiles), money-<phrase>banana</phrase> problem or block-world puzzles helps in understanding these informed searching techniques. Students really put more efforts in inventing good heuristics functions for above games.  <phrase>Minimax</phrase> algorithm for two player game is complex algorithm which is used in IBM's <phrase>Deep-blue</phrase>, who defeated <phrase>world chess champion</phrase> Gary Kasparov in 1997 is also implemented in lab. Our approach in writing and implementing those games or <phrase>puzzle</phrase> in understanding concepts better.
Prediction of <phrase>Parkinson's Disease</phrase> tremor Onset Using a <phrase>Radial Basis Function</phrase> <phrase>Neural Network</phrase> Based on <phrase>Particle Swarm Optimization</phrase> <phrase>Deep Brain Stimulation</phrase> (DBS) has been successfully used throughout the world for the treatment of <phrase>Parkinson's disease</phrase> symptoms. To control abnormal spontaneous electrical activity in target brain areas DBS utilizes a continuous stimulation signal. This continuous power draw means that its implanted battery power source needs to be replaced every 18-24 months. To prolong the life span of the battery, a technique to accurately recognize and predict the onset of the <phrase>Parkinson's disease</phrase> tremors in human subjects and thus implement an on-demand stimulator is discussed here. The approach is to use a <phrase>radial basis function</phrase> <phrase>neural network</phrase> (RBFNN) based on <phrase>particle swarm optimization</phrase> (PSO) and <phrase>principal component analysis</phrase> (<phrase>PCA</phrase>) with <phrase>Local Field</phrase> Potential (LFP) data recorded via the stimulation electrodes to predict activity related to tremor onset. To test this approach, LFPs from the subthalamic nucleus (STN) obtained through <phrase>deep brain</phrase> electrodes implanted in a <phrase>Parkinson</phrase> patient are used to train the network. To validate the network's performance, electromyographic (<phrase>EMG</phrase>) signals from the patient's <phrase>forearm</phrase> are recorded in parallel with the LFPs to accurately determine occurrences of tremor, and these are compared to the performance of the network. It has been found that detection accuracies of up to 89% are possible. Performance comparisons have also been made between a conventional RBFNN and an RBFNN based on PSO which show a marginal decrease in performance but with notable reduction in computational overhead.
<phrase>MR-Brain</phrase> <phrase>Image Segmentation</phrase> Using Gaussian Multiresolution Analysis and the <phrase>EM Algorithm</phrase> We present a MR <phrase>image segmentation</phrase> algorithm based on the conventional Expectation Maximization (EM) algorithm and the multiresolution analysis of images. Although the <phrase>EM algorithm</phrase> was used in MRI brain segmentation, as well as, <phrase>image segmentation</phrase> in general, it fails to utilize the strong <phrase>spatial correlation</phrase> between neighboring pixels. The multiresolution-based <phrase>image segmentation</phrase> techniques, which have emerged as a powerful method for producing <phrase>high-quality</phrase> segmentation of images, are combined here with the <phrase>EM algorithm</phrase> to overcome its drawbacks and in the same time take its advantage of simplicity. Two <phrase>data sets</phrase> are used to test the performance of the EM and the proposed Gaussian Multiresolution EM, GMEM, algorithm. The results, which proved more accurate segmentation by the GMEM algorithm compared to that of the <phrase>EM algorithm</phrase>, are represented statistically and graphically to give <phrase>deep understanding</phrase>.
Imaging Using <phrase>Ground-penetrating Radar</phrase> Measurements – Much work has been done toward reconstructing the <phrase>electrical parameters</phrase> of an unknown buried object. Many methods, and accompanying results from simulations, have been presented. Most research in the area, however, has been in developing imaging algorithms that have been tested primarily through computer simulation. This paper presents a newly constructed facility for <phrase>ground-penetrating radar</phrase> experiments , as well as the proposed applications for such a facility. The test site consists of a <phrase>sand</phrase>-filled volume approximately 15 feet long, 13 feet wide, and 7 feet deep. The experimental setup consists of transmitters and receivers in several common configurations, such as the offset VRP, cross-<phrase>borehole</phrase>, and surface-to-surface configurations. The motivations for such an experimental site are presented, as well as the imaging algorithms intended for application. These algorithms include migration techniques, SAR processing , Born-<phrase>iterative method</phrase>, and <phrase>diffraction</phrase> tomography.
<phrase>Decision making</phrase> based on cohort scores for <phrase>speaker verification</phrase> —<phrase>Decision making</phrase> is an important component in a <phrase>speaker verification</phrase> system. For the conventional GMM-UBM architecture, the decision is usually conducted based on the log <phrase>likelihood ratio</phrase> of the test utterance against the GMM of the claimed speaker and the UBM. This single-score decision is simple but tends to be sensitive to the complex variations in speech signals (e.g. text content, channel, speaking style, etc.). In this paper, we propose a <phrase>decision making</phrase> approach based on multiple scores derived from a set of cohort GMMs (cohort scores). Importantly, these cohort scores are not simply averaged as in conventional cohort methods; instead, we employ a powerful discriminative model as the decision maker. Experimental results show that the proposed method delivers substantial <phrase>performance improvement</phrase> over the baseline system, especially when a deep neural network (DNN) is used as the decision maker, and the DNN input involves some statistical features derived from the cohort scores. I. INTRODUCTION <phrase>Speaker verification</phrase> aims to verify claimed identities of speakers, and has gained great popularity in a wide range of applications including <phrase>access control</phrase>, forensic evidence provision and <phrase>user authentication</phrase>. After decades of research, lots of popular <phrase>speaker verification</phrase> approaches have been proposed , such as Gaussian <phrase>mixture model</phrase>-universal background model (GMM-UBM) [1], joint <phrase>factor analysis</phrase> (JFA) [2] and its 'simplified' version, the i-vector model [3]. Accompanied with these models, various back-end techniques have also been proposed to promote the discriminative capability for speakers, such as within-class covariance normalization (WCCN) [4], nuisance attribute projection (NAP) [5] and probabilistic <phrase>LDA</phrase> (PLDA) [6], etc. These methods have been demonstrated to be highly successful. Recently, <phrase>deep learning</phrase> has been applied to <phrase>speaker verification</phrase> and gained much interest [7], [8]. Within a <phrase>speaker verification</phrase> system, <phrase>decision making</phrase> is an important component [9]. To make a decision, the verification system first determines a score for the test utterance that reflects the confidence that the utterance is from the claimed speaker, and then compares the score with a predefined threshold. In a typical GMM-UBM system, the score is often computed as the log <phrase>likelihood ratio</phrase> that the test utterance being generated from the GMM of the claimed speaker and
Dense <phrase>Associative Memory</phrase> for <phrase>Pattern Recognition</phrase> A model of <phrase>associative memory</phrase> is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense <phrase>associative memory</phrase> and <phrase>neural networks</phrase> commonly used in <phrase>deep learning</phrase>. On the <phrase>associative memory</phrase> side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of <phrase>pattern recognition</phrase>, and the other one as the prototype regime. On the <phrase>deep learning</phrase> side of the duality, this family corresponds to <phrase>feedforward neural networks</phrase> with one <phrase>hidden layer</phrase> and various activation functions, which transmit the activities of the visible neurons to the <phrase>hidden layer</phrase>. This family of activation functions includes <phrase>logistics</phrase>, rectified linear units, and rectified <phrase>polynomials</phrase> of higher degrees. The proposed duality makes it possible to apply energy-based intuition from <phrase>associative memory</phrase> to analyze computational properties of <phrase>neural networks</phrase> with unusual activation functions – the higher rectified <phrase>polynomials</phrase> which until now have not been used in <phrase>deep learning</phrase>. The utility of the dense memories is illustrated for two <phrase>test cases</phrase>: the logical gate XOR and the recognition of handwritten digits from the MNIST <phrase>data set</phrase>.
" <phrase>Hug Machine</phrase> " -deep Pressure Stimulation —This paper is discussing the effects of <phrase>the Hug Machine</phrase> on people who have autism, mainly with children who have the medical condtion. Studies and experiements have shown that when a patient has regular session with the machine have positive affects and are able to handle their symptoms better. Autism is very preverlent problem in our society today. 1 out of every 88 child in <phrase>America</phrase> is diagnosed with Autism and this disease has seen a tenfold increase over the past 40 years. Who knows how far the disease increase in the future. The disorders vary in many differnet ways from difficuttly <phrase>social interaction</phrase>, verbal and <phrase>nonverbal communication</phrase>, and repetitive behaviors. Some though with Autism <phrase>excel</phrase> at school, music, or art even though their <phrase>social skills</phrase> suffer. <phrase>The Hug Machine</phrase> provides help for these problems that they have to face everyday. Children with Autism are more like to be have a lot of stress and anxeity from their environment stressors This stressors are the real reason we have <phrase>the Hug Machine</phrase> today. <phrase>The Hug Machine</phrase> provides a source of calm and relaxation sesation for those with Autism. The device was invented by <phrase>Temple</phrase> Grandin in 1965. She was an adult fighting the battle of Autism and had to suffer with severe stress and <phrase>anxiety</phrase> problems. She had the need to be held or touch by another person, but the stress and <phrase>anxiety</phrase> when she tried to interact with others was to great for her to handle. Then one day she was observing <phrase>cattle</phrase> being branded on her aunt's <phrase>Ranch</phrase> in <phrase>Arizona</phrase>. She notice once the <phrase>cattle</phrase> was in the sqeeze chute it seem to calm down almost instaneously. From this experience, she came to the conclusion that when deep pressure is applied a relaxing sensation overcomes the one having the pressure being applied too. With this sudden realization the idea for <phrase>the Hug Machine</phrase> was born. A pilot study was performed with <phrase>the Hug Machine</phrase>. This study investigated the effects of deep pressure on <phrase>arousal</phrase> and <phrase>anxiety</phrase> reduction in autism with Gradin's device. They took 12 children with autism were randomly assign to either an experimental group or a <phrase>placebo</phrase> group. The <phrase>placebo</phrase> group would not receive deep pressure but still be in the disengaged Hug Machince. Every test subject received two 20 min sessionsa week over a 6 week period. The results were good. Those in the experimental group …
Estimating Photometric Redshifts Using <phrase>Genetic Algorithms</phrase> Photometry is used as a cheap and easy way to estimate redshifts of <phrase>galaxies</phrase>, which would otherwise require considerable amounts of expensive telescope time. However, the analysis of photometric <phrase>redshift</phrase> datasets is a task where it is sometimes difficult to achieve a high <phrase>classification accuracy</phrase>. This work presents a custom <phrase>Genetic Algorithm</phrase> (GA) for mining the <phrase>Hubble Deep Field</phrase> North (HDF-N) datasets to achieve accurate IF-THEN classification rules. This kind of <phrase>knowledge representation</phrase> has the advantage of being intuitively comprehensible to the user, facilitating <phrase>astronomers</phrase>' interpretation of discovered knowledge. The GA is tested against the state of the art <phrase>decision tree</phrase> algorithm C5.0 [Rulequest, 2005] in two datasets, achieving better <phrase>classification accuracy</phrase> and simpler rule sets in both datasets.
Abnormal deep <phrase>grey matter</phrase> development following <phrase>preterm birth</phrase> detected using deformation-based morphometry. <phrase>Preterm birth</phrase> is a leading <phrase>risk factor</phrase> for neurodevelopmental and cognitive impairment in childhood and <phrase>adolescence</phrase>. The most common known cerebral abnormality among <phrase>preterm infants at</phrase> <phrase>term equivalent</phrase> age is a <phrase>diffuse white matter</phrase> abnormality seen on <phrase>magnetic resonance</phrase> (MR) images. It occurs with a similar prevalence to subsequent impairment, but its effect on developing neural systems is unknown. <phrase>MR images</phrase> were obtained at term equivalent age from 62 infants born at 24-33 completed weeks <phrase>gestation</phrase> and 12 term born controls. Tissue damage was quantified using diffusion-weighted imaging, and deformation-based morphometry was used to make a non-subjective survey of the whole brain to identify significant cerebral morphological alterations associated with <phrase>preterm birth</phrase> and with <phrase>diffuse white matter injury</phrase>. <phrase>Preterm infants at</phrase> <phrase>term equivalent</phrase> age had reduced thalamic and lentiform volumes without evidence of acute injury in these regions (t = 5.81, P < 0.05), and these alterations were more marked with increasing prematurity (t = 7.13, P < 0.05 for infants born at less than 28 weeks) and in infants with <phrase>diffuse white matter injury</phrase> (t = 6.43, P < 0.05). The identification of deep <phrase>grey matter</phrase> growth failure in association with <phrase>diffuse white matter injury</phrase> suggests that <phrase>white matter injury</phrase> is not an isolated phenomenon, but rather, it is associated with the maldevelopment of remote structures. This could be mediated by a disturbance to corticothalamic connectivity during a <phrase>critical period</phrase> in cerebral development. Deformation-based morphometry is a powerful tool for modelling the developing brain in health and disease, and can be used to test putative aetiological factors for injury.
<phrase>Statistical Post-Processing</phrase> at Wafersort - An Alternative to Burn-in and a Manufacturable Solution to Test Limit Setting for <phrase>Sub-micron Technologies</phrase> In <phrase>sub-micron CMOS</phrase> processes, it has become increasingly difficult to identify and separate outliers from the intrinsic distribution at test. This is due to the increasing inadequacy of reliability screens such as burn-in and <phrase>IDDQ testing</phrase>. <phrase>Statistical Post-Processing</phrase> (<phrase>SPP</phrase>) methods have been developed to run off-tester using the raw data generated from <phrase>Automatic Test Equipment</phrase> (ATE) and wafersort maps. <phrase>Post-Processing</phrase> modules include advanced IDDQ tests such as <phrase>Delta IDDQ</phrase> and the Nearest Neighbor Residual (NNR), as well as other non-IDDQ based reliability-focused modules. This work presents the application and results of <phrase>SPP</phrase> at <phrase>LSI</phrase> Logic on 0.18um CMOS products. Challenges of production implementation have been overcome, which include " user definable " adaptive threshold limits, handling multiple data sources, and <phrase>data flow</phrase> management. Burn-in data and customer Defects per Million units (DPM) data show a 30-60% decrease in <phrase>failure rate</phrase> with <phrase>SPP</phrase> implementation with very acceptable <phrase>yield loss</phrase>. The measure of <phrase>quiescent current</phrase> provides extended <phrase>fault coverage</phrase> due to the inherently wide observability of the power supply rails for static CMOS designs. However, the static leakage growth for <phrase>deep sub-micron technologies</phrase> has obscured the defect resolution of <phrase>IDDQ testing</phrase>. In order to use IDDQ effectively, methods of <phrase>variance reduction</phrase> in the IDDQ <phrase>data sets</phrase> are required. Two complementary methods of advanced <phrase>IDDQ testing</phrase> for <phrase>deep sub-micron technologies</phrase> include <phrase>Delta Iddq</phrase> and NNR. <phrase>Delta IDDQ</phrase> and similar <phrase>intra-die</phrase> based vector methods have been described in the literature, some examples can be found in [1,3,5,6]. A calculated delta is compared to a threshold that is less difficult to set than that needed for traditional <phrase>IDDQ testing</phrase> due to the reduction in variance of the delta distributions [2]. Small IDDQ <phrase>test sets</phrase>, however, may cause a defect to escape <phrase>Delta IDDQ</phrase> detection. There must be at least one vector activating and one vector not activating a defect to identify the defect with <phrase>Delta IDDQ</phrase>. Using an inter-die approach, such as NNR, in a complementary fashion allows those defective die with all vectors activating the defect to be screened. NNR is an effective <phrase>variance reduction</phrase> technique that uses the parametric data from neighboring die locations for predicting test outcomes of a die [2, 7]. For this implementation, NNR is used on IDDQ data. An important concept in this work is the complementary method in which <phrase>Delta Iddq</phrase> and NNR are used as an improvement to <phrase>IDDQ testing</phrase>. It is possible that a test set …
Deep Approximation of <phrase>Set Cover Greedy Algorithm</phrase> for <phrase>Test Set</phrase> Cui P, Liu HJ. Deep approximation of <phrase>set cover greedy algorithm</phrase> for <phrase>test set</phrase>. Abstract: <phrase>Test set</phrase> problem is a NP-hard problem with wide applications. <phrase>Set cover greedy algorithm</phrase> is one of the commonly used algorithms for the test set problem. It is an <phrase>open problem</phrase> if the <phrase>approximation ratio</phrase> 2ln n+1 directly derived from the <phrase>set cover problem</phrase> can be improved. The generalization of <phrase>set cover greedy algorithm</phrase> is used to solve the redundant <phrase>test set</phrase> problem arising in <phrase>bioinformatics</phrase>. This paper analyzes the distribution of the times for which the item pairs are differentiated, and proves that the <phrase>approximation ratio</phrase> of the <phrase>set cover greedy algorithm</phrase> for <phrase>test set</phrase> can be 1.5lnn+0.5lnlnn+2 by derandomization method, thus shrinks the gap in the analysis of the <phrase>approximation ratio</phrase> of this algorithm. In addition, this paper shows the tight <phrase>lower bound</phrase> (2−o(1))lnn−Θ(1) of the <phrase>approximation ratio</phrase> of <phrase>set cover greedy algorithm</phrase> for the weighted redundant <phrase>test set</phrase> with redundancy n−1.
Characterization of Variability in Deeply-scaled Fully Depleted Soi Devices Characterization of Variability in Deeply-scaled Fully Depleted Soi Devices Permission to make digital or <phrase>hard copies of</phrase> all or part of this work for personal or classroom use is <phrase>granted without fee provided</phrase> that copies are not made or distributed for <phrase>profit or commercial advantage and</phrase> that <phrase>copies bear</phrase> this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, <phrase>requires prior specific permission</phrase>. Abstract Scaling of <phrase>CMOS technology</phrase> into the deep submicron regime gives rise to process variability, which in turn compromises circuit yield. One of the main sources of variability is random <phrase>dopant</phrase> fluctuation (<phrase>RDF</phrase>) in the channel. Fully Depleted Silicon on Insulator technology has been proposed as a promising alternative to bulk CMOS, due to it's undoped channel which reduces <phrase>RDF</phrase>, as well as due to its better electrostatic control of the channel. A testchip for measurement and analysis of variability in a 22nm FDSOI process has been designed. Among other experiments, the tetstchip features an array of 11x11 tiles with variability measurement structures. Each tile contains circuits to measure IV and CV device characteristics, <phrase>RO</phrase> frequencies and <phrase>resistor</phrase> values. <phrase>Scan chains</phrase> and <phrase>multiplexing</phrase> are employed to enable analysis of a large number of DUTs with a limited pad count. The goal of this testchip is to characterize variability in FDSOI. This will be achieved by extracting systematic and random variation data from specifically designed <phrase>test structures</phrase>. The focus of this testchip is to decou-ple different sources of random variation in order to electrically measure line edge roughness and silicon thickness variation due to <phrase>surface roughness</phrase> , characterize the effects of source and drain doping, and quantify the contribution of ground planes and back-biasing in FDSOI variability.
Scaling Distributed <phrase>Machine Learning</phrase> with System and Algorithm Co-design For a lot of important <phrase>machine learning problems</phrase>, due to the rapid growth of data and the ever increasing model complexity, which often manifests itself in the large number of model parameters, no single machine can solve them fast enough. Therefore, distributed optimization and inference is becoming more and more inevitable for solving <phrase>large scale</phrase> <phrase>machine learning problems</phrase> in both <phrase>academia</phrase> and industry. Obtaining an efficient distributed implementation of an algorithm, however, is far from trivial. Both intensive computational workloads and the volume of <phrase>data communication</phrase> demand careful design of distributed computation systems and distributed <phrase>machine learning algorithms</phrase>. In this thesis, we focus on the co-design of <phrase>distributed computing</phrase> systems and distributed optimization algorithms that are specialized for large <phrase>machine learning problems</phrase>. We propose two <phrase>distributed computing</phrase> frameworks: a parameter server framework which features efficient <phrase>data communication</phrase>, and MXNet, a multi-language library aiming to simplify the development of <phrase>deep neural network</phrase> algorithms. In less than two years, we have witnessed the wide adoption of the proposed systems. We believe that as we continue to develop these systems, they will enable more people to take advantage of the power of <phrase>distributed computing</phrase> to design efficient <phrase>machine learning</phrase> applications to solve <phrase>large-scale</phrase> computational problems. Leveraging the two computing platforms, we examine a number of distributed <phrase>optimization problems</phrase> in <phrase>machine learning</phrase>. We present new methods to accelerate the training process, such as data partitioning with better locality properties, communication friendly optimization methods, and more compact statistical models. We implement the new algorithms on the two systems and test on <phrase>large scale</phrase> real <phrase>data sets</phrase>. We successfully demonstrate that careful co-design of computing systems and <phrase>learning algorithms</phrase> can greatly accelerate <phrase>large scale</phrase> distributed <phrase>machine learning</phrase>.
<phrase>High Performance Computing</phrase> Systems and Applications Group OBJECTIVE A position in the field of <phrase>computer science</phrase> with special interests in <phrase>high performance computing</phrase>, <phrase>performance analysis</phrase>, networking and applying computer technology to new areas. Conducted performance analysis of parallel adaptive mesh refinement software by designing, implementing and testing scalable instrumentation library. Involved FORTRAN90 and C <phrase>finite element</phrase> codes using MPI on <phrase>SGI</phrase>/CrayT3E and <phrase>Beowulf</phrase> systems. <phrase>Avionics</phrase> System Engineering Group Designed improved <phrase>scheduling algorithm</phrase> used in the software simulation of spacecraft control and data systems. Implemented the new algorithm, gained experience in <phrase>POSIX</phrase> threads, C++, <phrase>TCL</phrase>, and concurrency issues of parallel <phrase>discrete event simulation</phrase>. Developed <phrase>mathematical models</phrase> to prove superiority of the new scheduling regimen. Designed and implemented Intelligent <phrase>Front-end</phrase> Annotation (IFA) software used in the design of application specific <phrase>integrated circuits</phrase> (ASICs). Reduced number of design iterations resulting in significant cost savings. IFA subsequently <phrase>copyrighted</phrase> by NASA, published in NASA journal, and made available to companies in the <phrase>private sector</phrase>. Evaluated <phrase>fiber optic</phrase> switching equipment for use in the <phrase>Deep Space Network</phrase>. Developed <phrase>user interface</phrase> used to control <phrase>next generation</phrase> receiver remotely over <phrase>TCP/IP</phrase> network. Defined root configuration data format used during initialization of digital receivers prior to tracking interplanetary spacecraft. Spearheaded committee to provide <phrase>Windows 95</phrase>/98 networking support and documentation for users connecting to <phrase>campus</phrase> network via <phrase>Ethernet</phrase>, <phrase>modem</phrase>, or IBX data line. Participated on committee of senior consultants responsible for interviewing prospective new hires, conducting performance evaluations, designing and administering staff <phrase>training program</phrase> and overseeing maintenance and administration of consulting hardware.
The VITA <phrase>Financial Services</phrase> Sales Support Environment <phrase>Knowledge-based</phrase> recommender technologies support customers and <phrase>sales representatives</phrase> in the identification of appropriate products and services. These technologies are especially useful for complex and high involvement products such as cars, computers, or <phrase>financial services</phrase>. In this paper we present the VITA (Virtualis Tanacsado) <phrase>financial services</phrase> recommendation environment which has been deployed for the Fundamenta building and loan association in <phrase>Hungary</phrase>. On the basis of <phrase>knowledge-based</phrase> recommender technologies, VITA supports sales dialogs between Fundamenta <phrase>sales representatives</phrase> and customers interested in <phrase>financial services</phrase> (e.g., loans). VITA has been developed and is maintained on the basis of an environment which supports automated testing and debugging of <phrase>knowledge bases</phrase> and recommender process definitions. Besides presenting the VITA environment we focus on reporting empirical results which clearly show the payoffs of the deployed application in terms of time savings in the conduction of sales dialogs. Complex and high-involvement products such as <phrase>financial services</phrase> impose increasing challenges on <phrase>sales representatives</phrase> responsible for identifying appropriate recommendations for customers as well as on software developers responsible for the development and maintenance of sales support environments. On the one hand, <phrase>sales representatives</phrase> have to know which services should be recommended in which context, and how those services should be explained. On the other hand, <phrase>software engineering</phrase> departments are facing the challenge of frequent change requests regarding service assortments and the rules specifying how those assortments are presented and offered to a customer. Both aspects are major motivations for the application of <phrase>knowledge-based</phrase> recommender technologies (Burke 2000, Felfernig 2007) which allow a flexible development and maintenance of sales <phrase>knowledge bases</phrase>. <phrase>Knowledge-based</phrase> recommender technologies exploit deep knowledge about the product domain in order to derive recommendations. In contrast to the online selling of products such as books or interactive selling of complex products requires the provision of intelligent technologies such as personalized dialogs, explanations, or repair actions for inconsistent customer requirements. An effective organizational embodiment of these technologies allows significant time savings for both <phrase>sales representatives</phrase> in the preparation and conduction of sales dialogs and software engineers in the development of the underlying sales support software. From the business perspective, <phrase>sales representatives</phrase> must be effectively supported in the preparation, conduction, and completion of sales dialogs. The overall goal of financial <phrase>service providers</phrase> is to improve the performance of <phrase>sales representatives</phrase> in terms of an increasing number of sold products per year. Achieving significant time savings in all those phases is the …
Strategic Technology Directions <phrase>Jet Propulsion Laboratory</phrase> <phrase>National Aeronautics and Space Administration</phrase> Technologies are deemed strategic if they strive to address NASA's and JPL's grand challenges and aspirations. Examples of fundamental challenges that we and our technologies will be called upon to address are: What is the concentration of <phrase>carbon dioxide</phrase> and other <phrase>greenhouse</phrase> gases that the <phrase>atmosphere</phrase> and oceans • can absorb without crossing <phrase>climatic</phrase> tipping points? Are there other habitable environments and life on other bodies in the solar system and beyond? • What are the structures and properties of other planetary systems? • What is the nature of <phrase>dark matter</phrase> and <phrase>dark energy</phrase>, and can energy be harnessed on Earth from them? • Many overarching technical challenges await us beyond the present horizon, if we are to respond to these and other goals: How can we test, place, and operate 10m, 20m, 50m, 100m radar/sub-mm/IR/optical apertures in space? • What is the technology and operations path to provide <phrase>a 10</phrase>-fold bandwidth increase per decade for the <phrase>Deep • Space Network</phrase>? How do we provide <phrase>a 10</phrase>-fold increase in spacecraft power? • How do we conduct missions to return samples to Earth from Mars and other large <phrase>planets</phrase>, which we must • before sending people there? It is difficult to resist a sense of appreciation and pride of what has been achieved in the half-century since the space era began. Technologies developed to support this development, and the value of the science return, have been incalculable. It allowed us to time-<phrase>stamp</phrase> the beginning of the universe and appreciate that the kind of matter we are made of comprises only about 4% of it, to learn more about our solar system than all the knowledge humans had distilled since they began to ponder the sky, to understand more about Earth and the dynamics of global change from both natural causes and <phrase>human activity</phrase>. Strategic technologies identified in this document also represent technology capabilities that the <phrase>Jet Propulsion Laboratory</phrase> believes are essential to continuing progress in pursuit of NASA's and JPL's mission, national goals, and in addressing global concerns. Advancing these technologies, that often push the theoretical limits of performance, is a major challenge, not least because such advances require sustaining an environment of imagination, <phrase>creativity</phrase>, and a <phrase>culture</phrase> of innovation for decades of dedicated effort at a time when support is provided on much-shorter time scales and sometimes not at all. The balance of what is invested on near-term projects vs. …
<phrase>Federated search</phrase> in the wild: the combined power of over a hundred <phrase>search engines</phrase> <phrase>Federated search</phrase> has the potential of improving <phrase>web search</phrase>: the user becomes less dependent on a single search provider and parts of the deep web become available through a unified interface, leading to a wider variety in the retrieved <phrase>search results</phrase>. However, a publicly available dataset for <phrase>federated search</phrase> reflecting an actual web environment has been absent. As a result, it has been difficult to assess whether proposed systems are suitable for the web setting. We introduce a new <phrase>test collection</phrase> containing the results from more than a hundred actual <phrase>search engines</phrase>, ranging from large general <phrase>web search engines</phrase> such as Google and <phrase>Bing</phrase> to small <phrase>domain-specific</phrase> engines. We discuss the design and analyze the effect of several sampling methods. For a set of test queries, we collected relevance judgements for the top 10 results of each <phrase>search engine</phrase>. The dataset is publicly available and is useful for researchers interested in resource selection for <phrase>web search</phrase> collections, result merging and size estimation of uncooperative resources.
Ultra-<phrase>high throughput</phrase> <phrase>string matching</phrase> for <phrase>Deep Packet Inspection</phrase> <phrase>Deep Packet Inspection</phrase> (DPI) involves searching a packet's header and payload against thousands of rules to detect possible attacks. The increase in Internet usage and growing number of attacks which must be searched for has meant <phrase>hardware acceleration</phrase> has become essential in the prevention of DPI becoming a bottleneck to a network if used on an edge or core router. In this paper we present a new multi-<phrase>pattern matching</phrase> algorithm which can search for the fixed strings contained within these rules at a guaranteed rate of one character per cycle independent of the number of strings or their length. Our algorithm is based on the Aho-Corasick <phrase>string matching</phrase> algorithm with our modifications resulting in a memory reduction of over 98% on the strings tested from the Snort ruleset. This allows the search structures needed for matching thousands of strings to be small enough to fit in the on-chip memory of an FPGA. Combined with a simple architecture for hardware, this leads to <phrase>high throughput</phrase> and low <phrase>power consumption</phrase>. Our hardware implementation uses multiple <phrase>string matching</phrase> engines working in parallel to search through packets. It can achieve a throughput of over 40 Gbps (<phrase>OC</phrase>-768) when implemented on a Stratix 3 FPGA and over 10 Gbps (<phrase>OC</phrase>-192) when implemented on the lower power <phrase>Cyclone</phrase> 3 FPGA.
Teaching <phrase>Robot Motion</phrase> Planning <phrase>Robot motion</phrase> planning is a fairly intuitive and engaging topic, yet it is difficult to teach. The material is taught in undergraduate and graduate robotics classes in computer science, <phrase>electrical engineering</phrase>, <phrase>mechanical engineering</phrase> and aeronautical engineering, but at an abstract level. <phrase>Deep learning</phrase> could be achieved by having students implement and test different <phrase>motion planning</phrase> strategies. However, a full implementation of <phrase>motion planning</phrase> algorithms by undergraduates is practically impossible in the context of a single class, even by students proficient in programming. By helping undergraduates grasp <phrase>motion planning</phrase> concepts in series of courses designed for increasing advanced levels, we can open the field to young and enthusiastic talent. This cannot be done by asking students to implement <phrase>motion planning</phrase> algorithms from scratch or access thousands of lines of code and just figure out how things work. We present an ongoing project to develop microworld software and a modeling curriculum that supports undergraduate acquisition of <phrase>motion planning</phrase> knowledge and tool use by <phrase>computer science</phrase> and engineering students.
Deep Self-Taught Learning for <phrase>Handwritten Character Recognition</phrase> Recent theoretical and empirical work in statistical <phrase>machine learning</phrase> has demonstrated the importance of <phrase>learning algorithms</phrase> for <phrase>deep architectures</phrase>, i.e., function classes obtained by <phrase>composing</phrase> multiple non-linear transformations. Self-taught learning (exploiting unla-beled examples or examples from other distributions) has already been applied to deep learners, but mostly to show the advantage of unlabeled examples. Here we explore the advantage brought by out-of-distribution examples. For this purpose we developed a powerful generator of stochastic variations and noise processes for character images, including not only affine transformations but also <phrase>slant</phrase>, local elastic deformations, changes in thickness , background images, grey level changes, contrast, occlusion, and various types of noise. The out-of-distribution examples are obtained from these highly distorted images or by including examples of object classes different from those in the target <phrase>test set</phrase>. We show that deep learners benefit more from out-of-distribution examples than a corresponding shallow learner, at least in the area of <phrase>handwritten character recognition</phrase>. In fact, we show that they beat <phrase>previously published</phrase> results and reach human-level performance on both handwritten digit classification and 62-class <phrase>handwritten character recognition</phrase>.
Systematic Defects in <phrase>Deep Sub-Micron Technologies</phrase> Defects due to process-design interaction have a systematic nature. Therefore they can have a profound impact on yield. The capability to detect (and correct) them is a requirement to continue to follow <phrase>Moore's law</phrase>. Most of the systematic defects are detected during the process development. These defects are detectable with <phrase>test structures</phrase> or visual inspection tools. However some process marginalities will only show-up in the topology of 'real' designs. Moreover, these defects are often not detectable with stuck-at testing. We show two examples of process <phrase>related defects</phrase> which could only be detected with more advanced <phrase>test methods</phrase> such as <phrase>transition fault</phrase> testing and <phrase>low voltage</phrase> testing. To correct systematic problems, however , one should not only have the capability to detect defects but also to identify them. Our examples show that other tests would have been far more sensitive in detecting systematic issues. Therefore the detection of systematic defects gives new requirements to <phrase>test suites</phrase> and can only be achieved with a shift in the position of <phrase>manufacturing test</phrase>.
Autonomic Function Tests: Some Clinical Applications Modern autonomic function tests can non-invasively evaluate the severity and distribution of <phrase>autonomic failure</phrase>. They have sufficient sensitivity to detect even subclinical <phrase>dysautonomia</phrase>. Standard laboratory testing evaluates cardiovagal, sudomotor and <phrase>adrenergic</phrase> autonomic functions. Cardiovagal function is typically evaluated by testing <phrase>heart rate</phrase> response to <phrase>deep breathing</phrase> at a defined rate and to the <phrase>Valsalva maneuver</phrase>. Sudomotor function can be evaluated with the quantitative sudomotor <phrase>axon</phrase> reflex test and the thermoregulatory sweat test. <phrase>Adrenergic</phrase> function is evaluated by the <phrase>blood pressure</phrase> and <phrase>heart rate</phrase> responses to the <phrase>Valsalva maneuver</phrase> and to head-up tilt. Tests are useful in defining the presence of <phrase>autonomic failure</phrase>, their <phrase>natural history</phrase>, and response to treatment. They can also define patterns of <phrase>dysautonomia</phrase> that are useful in helping the clinician diagnose certain autonomic conditions. For example, the tests are useful in the diagnosis of the autonomic neuropathies and distal small fiber <phrase>neuropathy</phrase>. The autonomic neuropathies (such as those due to diabetes or <phrase>amyloidosis</phrase>) are characterized by severe generalized <phrase>autonomic failure</phrase>. Distal small fiber <phrase>neuropathy</phrase> is characterized by an absence of <phrase>autonomic failure</phrase> except for distal sudomotor failure. Selective <phrase>autonomic failure</phrase> (which only one system is affected) can be diagnosed by autonomic testing. An example is chronic idiopathic anhidrosis, where only sudomotor function is affected. Among the synucleinopathies, autonomic function tests can distinguish <phrase>Parkinson's disease</phrase> (PD) from multiple system <phrase>atrophy</phrase> (MSA). There is a gradation of <phrase>autonomic failure</phrase>. PD is characterized by mild <phrase>autonomic failure</phrase> and a length-dependent pattern of sudomotor involvement. MSA and pure <phrase>autonomic failure</phrase> have severe generalized <phrase>autonomic failure</phrase> while DLB is intermediate.
Deep Distance Metric Learning with Data Summarization We present Deep Stochastic Neighbor Compression (DSNC), a framework to compress <phrase>training data</phrase> for instance-<phrase>based methods</phrase> (such as k-nearest neighbors). We accomplish this by inferring a smaller set of pseudo-inputs in a new <phrase>feature space</phrase> learned by a deep neural network. Our framework can equivalently be seen as jointly learning a nonlinear distance metric (induced by the deep <phrase>feature space</phrase>) and learning a compressed version of the training data. In particular , compressing the data in a deep <phrase>feature space</phrase> makes DSNC robust against label noise and issues such as within-class multi-modal distributions. This leads to DSNC yielding better accuracies and faster predictions at test time, as compared to other competing methods. We conduct comprehensive empirical evaluations, on both quantitative and qualitative tasks, and on several benchmark datasets, to show its effectiveness as compared to several baselines.
The role of cue information in the <phrase>outcome-density</phrase> effect: evidence from <phrase>neural network</phrase> simulations and a <phrase>causal learning</phrase> experiment Although normatively irrelevant to the relationship between a cue and an outcome, <phrase>outcome density</phrase> (i.e. its base-rate probability) affects people's estimation of causality. By what process causality is incorrectly estimated is of importance to an integrative theory of <phrase>causal learning</phrase>. A potential explanation may be that this happens because <phrase>outcome density</phrase> induces a judgement bias. An alternative explanation is explored here, following which the incorrect estimation of causality is grounded in the processing of cue–outcome information during learning. A first <phrase>neural network</phrase> simulation shows that, in the absence of a <phrase>deep processing</phrase> of cue information, cue–outcome relationships are acquired but causality is correctly estimated. The second simulation shows how an incorrect estimation of causality may emerge from the active processing of both cue and outcome information. In an experiment inspired by the simulations, the role of a <phrase>deep processing</phrase> of cue information was put to test. In addition to an <phrase>outcome density</phrase> manipulation, a shallow cue manipulation was introduced: cue information was either still displayed (concurrent) or no longer displayed (delayed) when outcome information was given. Behavioural and <phrase>simulation results</phrase> agree: the <phrase>outcome-density</phrase> effect was maximal in the concurrent condition. The results are discussed with respect to the extant explanations of the <phrase>outcome-density</phrase> effect within the <phrase>causal learning</phrase> framework.
Delay/disruption-tolerant Network Testing Using a Leo Satellite Delay/Disruption Tolerant Networking (DTN) " bundles " have been proposed for deep-<phrase>space communication</phrase> in the " Interplanetary Internet. " This paper describes the first DTN bundle protocol testing from space, using the <phrase>United Kingdom</phrase> Disaster Monitoring <phrase>Constellation</phrase> (<phrase>UK</phrase>-DMC) satellite in <phrase>Low Earth Orbit</phrase> (LEO). The mismatch problems between the different conditions of the <phrase>private</phrase> dedicated space-to-ground link and the shared, congested, ground-to-ground links are discussed. DTN, with its ability to transfer files on a hop-by-hop basis across different subnets, is presented as a technology that can be used to alleviate this problem. We describe our operational testing, as well as test configurations, goals and results, and lessons learned. I. INTRODUCTION Delay/Disruption Tolerant Networking (DTN) has been defined as an <phrase>end-to-end</phrase> store-and-forward architecture capable of providing communications in highly-stressed network environments. To provide the store-and-forward service, a " bundle " protocol (BP) sits at the <phrase>application layer</phrase> of some number of constituent internets, forming a store-and-forward <phrase>overlay network</phrase> [1]. Key capabilities of the BP include: • Custody-based retransmission – the ability to take responsibility for a bundle reaching its final destination • Ability to cope with intermittent connectivity. • Ability to cope with long <phrase>propagation delays</phrase>. • Ability to take advantage of scheduled, predicted, and opportunistic connectivity (in addition to continuous connectivity).
List of Publications (publikationsförteckning) <phrase>Ph</phrase>-d Dissertation (doktorsavhandling) Publications 1) ** <phrase>Nilsson</phrase> MH, Törnqvist A.L, Rehncrona S. <phrase>Deep-brain stimulation</phrase> in the subthalamic nuclei improves balance performance in patients with Parkinson's disease, when tested without anti-parkinsonian medication. <phrase>Parkinson's disease</phrase> after <phrase>long-term</phrase> treatment with <phrase>subthalamic nucleus</phrase> <phrase>high-frequency</phrase> stimulation. and fear of falling in patients with Parkinson's disease treated with <phrase>high frequency</phrase> subthalamic stimulation. Fear of falling and <phrase>falls</phrase> in people with <phrase>Parkinson's disease</phrase> treated with <phrase>Deep Brain Stimulation</phrase> in the subthalamic nuclei. In press: Acta Neurol Scand.
A Direct Inversion Scheme for Deep Resistivity Sounding Data Using <phrase>Artificial Neural Networks</phrase> Initialization of model parameters is crucial in the conventional 1D inversion of DC electrical data, since a poor guess may result in undesired parameter estimations. In the present work, we investigate the performance of <phrase>neural networks</phrase> in the direct inversion of DC sounding data, without the need of a priori information. We introduce a two-step network approach where the first network identifies the curve type, followed by the model <phrase>parameter estimation</phrase> using the second network. This approach provides the flexibility to accommodate all the characteristic sounding curve types with a wide range of resistivity and thickness. Here we realize a three layer <phrase>feed-forward neural network</phrase> with fast back propagation <phrase>learning algorithms</phrase> performing well. The basic <phrase>data sets</phrase> for training and testing were simulated on the basis of available deep resistivity sounding (DRS) data from the crystalline terrains of <phrase>south India</phrase>. The optimum network parameters and performance were decided as a function of the testing error convergence with respect to the network training error. On adequate training, the final weights simulate faithfully to recover resistivity and thickness on new data. The small discrepancies noticed, however, are well within the resolvability of resistivity sounding curve interpretations.
<phrase>Deep Semantic</phrase> Embedding We introduce <phrase>Deep Semantic</phrase> Embedding (DSE), a supervised <phrase>learning algorithm</phrase> which computes semantic representation for text documents by respecting their similarity to a given query. Unlike other methods that use <phrase>single-layer</phrase> learning machines, DSE maps word inputs into a low-dimensional semantic space with <phrase>deep neural network</phrase>, and achieves a highly nonlinear embedding to model the <phrase>human perception</phrase> of text semantics. Through discriminative fine-tuning of the <phrase>deep neural network</phrase>, DSE is able to encode the relative similarity between relevant/irrelevant document pairs in <phrase>training data</phrase>, and hence learn a reliable ranking score for a query-document pair. We present <phrase>test results</phrase> on datasets including scientific publications and <phrase>user-generated</phrase> <phrase>knowledge base</phrase>.
Conception of a prototype to validate a maintenance expert system — <phrase>Decision making</phrase> is crucial for the life of many production plants. Good decision need the right information at the right time, and a deep knowledge of the system. This is especially true for older plants that are more prone to failures. Unfortunately, these systems are less rich of monitoring and diagnostics instrumentation, because of their seniority. When the maintenance service is carried out by a group of external companies, they suffer the consequences of backwardness of the equipment, having to deal with problems of poor coordination, delays in interventions and so on. <phrase>Small and medium enterprises</phrase> are often in trouble facing these issues so they need dedicated resources in order to offer a maintenance service compliant to the costumer requirements. In this context a <phrase>research project</phrase> was funded, aiming at aiding the birth of a virtual enterprise network of maintainers, which should provide a maintenance service of the highest level in the field of legacy industrial plants. One of the most difficult steps of the project, was to design a prototype capable of testing the architecture of the virtual network, the expert system and the overall goodness of the proposal for potential customers. In this paper we describe how it was possible to design, produce and operate a prototype which could fit a virtual enterprise network of maintainers. The method requires, among other things, to fulfil a preliminary selective Failure Modes and Effects <phrase>Criticality</phrase> Analysis (FMECA) to be properly used to implement the diagnostic system. Any company has to deal with <phrase>decision making</phrase> activities which are a vital part of their functioning. In this ever-competitive world, it is <phrase>increasingly important</phrase> to facilitate the business of taking decision by managers. To facilitate this process, it is necessary to have the right information at the right time, in order to bridge the gap between the needs and the expectations. To facilitate a better flow of information, there is an increasing need for <phrase>information management</phrase> systems, more and more adapted to the needs. For this reason, it is very important to have a clear understanding of their mode of operation and how they are integrated into a company at all levels of management [1]. Only in this way you will understand how the <phrase>decision support system</phrase> can be useful for making appropriate decisions [2] [3]. An <phrase>information management</phrase> system collects and processes all data and provides a organized summary for managers who …
Pós-graduação Em Ciência Da Computação " Riple-te: a Software Product Lines <phrase>Testing Process</phrase> " Machado, Ivan do Carmo RiPLE-TE: a software product lines <phrase>testing process</phrase> / Ivan do Carmo Machado. To my beloved family. Acknowledgements First and foremost, I would like to thank my greatest teacher of all: <phrase>God</phrase>, for providing me this opportunity and granting me the capability to proceed successfully. I could never have done this without the <phrase>faith</phrase> I have in you. I would like to gratefully acknowledge the supervision of Dr. Eduardo Almeida during this work. He provided me with many helpful suggestions and encouragement during the course of this work as well as the challenging research that lies behind it. I also wish to express my appreciation to Dr. Silvio Meira, for accepting me as M.Sc. Student. Special Gratitude goes to the rest of the teaching staff of the <phrase>CIn</phrase>/UFPE, for their valuable classes. My sincere thanks are extended to Dr. Manoel Mendonça, from DMCC/UFBA, and his students, for their help during the execution of the experimental study. I would like to thank the <phrase>Brazilian</phrase> National Research Council (CNPq). Without their grant, this M.Sc. would not have been possible. I cordially thank my friends and colleagues that I have met during my journey in <phrase>Recife</phrase>. I thank my housemates Bruno, Jonatas, Iuri and Leandro for their good friendship and for being the surrogate family during the years I stayed there. I want to express my deep thanks to my colleague Paulo Anselmo for taking intense academic interest in this study as well as providing valuable suggestions that improved the quality of this study. insightful discussions, offering valuable advice and support. Postgraduates of the RiSE <phrase>Research Group</phrase> are thanked for numerous stimulating discussions. Saturdays will not be the same any longer! I would like to express my heartiest thanks to Edna Telma for her understanding, endless patience and encouragement when it was most required, and for never letting me feel that I was away from my family. My cordial thanks to her family members for their kindness and affection, specially to Francisco Joaquim, for supporting me during these years I lived in <phrase>Recife</phrase>. I also want to thanks to Benedita (Dio) for opening her wonderful home to me and making me feel so comfortable there. She provided me a perfect place to finish my work while I was in Salvador. I cannot finish without thanking my family. I am forever indebted to my parents, Serafim and Joselice, my brother Ivo …
Optimized shielding for space <phrase>radiation protection</phrase>. Future <phrase>deep space</phrase> mission and <phrase>International Space Station</phrase> exposures will be dominated by the high-charge and -energy (HZE) ions of the Galactic <phrase>Cosmic Rays</phrase> (<phrase>GCR</phrase>). A few <phrase>mammalian</phrase> systems have been extensively tested over a broad range of ion types and energies. For example, C3H10T1/2 cells, V79 cells, and Harderian gland tumors have been described by various track-structure dependent response models. The <phrase>attenuation</phrase> of <phrase>GCR</phrase> induced biological effects depends strongly on the biological endpoint, response model used, and material composition. Optimization of space shielding is then driven by the nature of the response model and the transmission characteristics of the given material.
Modelling <phrase>Temporal Information</phrase> Using <phrase>Discrete Fourier Transform</phrase> for Video Classification Recently, video classification attracts intensive research efforts. However, most existing works are based on frame-level visual features, which might fail to model the <phrase>temporal information</phrase>, e.g. characteristics accumulated along time. In order to capture video <phrase>temporal information</phrase>, we propose to analyse features in <phrase>frequency domain</phrase> transformed by <phrase>discrete Fourier transform</phrase> (<phrase>DFT features</phrase>). Frame-level features are firstly extract by a pre-trained <phrase>deep convolutional neural network</phrase> (CNN). Then, time domain features are transformed and interpolated into <phrase>DFT features</phrase>. CNN and <phrase>DFT features</phrase> are further encoded by using different pooling methods and fused for video classification. In this way, static <phrase>image features</phrase> extracted from a pre-trained deep CNN and <phrase>temporal information</phrase> represented by <phrase>DFT features</phrase> are jointly considered for video classification. We test our method for video emotion classification and <phrase>action recognition</phrase>. <phrase>Experimental results demonstrate</phrase> that combining <phrase>DFT features</phrase> can effectively capture <phrase>temporal information</phrase> and therefore improve the performance of both video emotion classification and <phrase>action recognition</phrase>. Our approach has achieved a state-of-the-art performance on the largest video emotion dataset (VideoEmotion-8 dataset) and competitive results on <phrase>UCF</phrase>-101.
<phrase>Deep Architectures</phrase> for Articulatory Inversion We implement two <phrase>deep architectures</phrase> for the acoustic-articulatory inversion mapping problem: a deep neural network and a deep trajectory mixture density network. We find that in both cases, <phrase>deep architectures</phrase> produce more accurate predictions than shallow architectures and that this is due to the higher expressive capability of a deep model and not a consequence of adding more adjustable parameters. We also find that a deep trajectory mixture density network is able to obtain better inversion accuracies than smoothing the results of a deep neural network. Our best model obtained an average root mean square error of 0.885 mm on the MNGU0 test dataset.
Drilling Automation for Subsurface Planetary Exploration Future in-situ lunar/<phrase>martian</phrase> <phrase>resource utilization</phrase> and characterization, as well as the scientific search for life on Mars, will require access to the subsurface and hence drilling. Drilling on Earth is hard – an art form more than an engineering discipline. The limited mass, energy and manpower in planetary drilling situations makes application of terrestrial drilling techniques problematic. The Drilling Automation for Mars Exploration (DAME) project's purpose is to develop and field-test drilling automation and robotics technologies for projected use in missions to the <phrase>Moon</phrase> and Mars in the 2011-15 period. 1. INTRODUCTION Space drilling will require intelligent and autonomous systems for robotic exploration and to support future human exploration, as energy, mass and human presence will be scarce. Unlike rover navigation problems, most planetary drilling will be blind – absent any precursor seismic imaging of <phrase>substrates</phrase>, which is common on Earth prior to drilling for <phrase>hydrocarbons</phrase>. On the <phrase>Moon</phrase>, eventual in-situ <phrase>resource utilization</phrase> (ISRU) will require deep drilling with probable human-tended operation [1] of large-bore drills, but initial lunar subsurface exploration and near-term ISRU will be accomplished with <phrase>lightweight</phrase>, rover-deployable or standalone drills capable of penetrating a few tens of meters in depth. On the <phrase>Moon</phrase> or Mars, drilling will be initially automated, then later human-tended at best. Mass and energy will be scarce. Early development and demonstration of automated drilling technologies is necessary – otherwise, no exploration mission designer will allow a drill on board their spacecraft. The search for evidence of extant microbial life on Mars is expected to require the acquisition of core samples from subsurface depths estimated at hundreds to thousands of meters where, beneath <phrase>permafrost</phrase>, the increasing temperature would be consistent with the presence of interstitial water (as a <phrase>brine</phrase>) in its liquid phase.
Detection of <phrase>Resistive Shorts</phrase> in Deep Sub-micron Technologies Current-based tests are the most effective methods available to detect <phrase>resistive shorts</phrase>. <phrase>Delta I DDQ</phrase> testing is the most sensitive variant and can handle off-state currents of 10-100 mA of a single core. Nevertheless this is not sufficient to handle the next generations of very <phrase>deep sub-micron technologies</phrase>. Moreover <phrase>delay-fault testing</phrase> and very-<phrase>low voltage</phrase> testing are not a real alternative for the detection of <phrase>resistive shorts</phrase>. The main limitation of ∆I DDQ testing is the <phrase>intra-die</phrase> variation of the <phrase>threshold voltage</phrase> which results in variations in the off-state current. Two methods are investigated that improve the detection capabilities of ∆I DDQ testing. The first method reduces the impact of <phrase>intra-die</phrase> variation by reducing the amount of logic that <phrase>switches</phrase> states. This method can handle very large off-state currents although at the cost of a substantial increase in test time. The second method investigates the correct scaling of the <phrase>intra-die</phrase> variations as a function of temperature. We show that both methods improve the detection capabilities of ∆I DDQ testing.
A better way to learn features: technical perspective A TYPICAL <phrase>MACHINE learning</phrase> program uses weighted combinations of features to discriminate between classes or to predict <phrase>real-valued</phrase> outcomes. The art of machine learning is in constructing the features, and a radically new method of creating features constitutes a major advance. In the 1980s, the new method was backpropagation, which uses the <phrase>chain rule</phrase> to backpropagate error derivatives through a multilayer, <phrase>feed-forward</phrase>, <phrase>neural network</phrase> and adjusts the weights between layers by following the gradient of the backpropagated error. This worked well for recognizing simple shapes, such as handwritten digits, especially in <phrase>convolutional neural networks</phrase> that use local feature detectors replicated across the image. 5 For many tasks, however, it proved extremely difficult to optimize <phrase>deep neural nets</phrase> with many layers of non-linear features, and a huge number of labeled training cases was required for large <phrase>neural networks</phrase> to generalize well to test data. In the 1990s, <phrase>Support Vector Machines</phrase> (SVMs) 8 introduced a very different way of creating features: the user defines a kernel function that computes the similarity between two input vectors, then a judiciously chosen subset of the training examples is used to create " landmark " features that measure how similar a <phrase>test case</phrase> is to each training case. SVMs have a clever way of choosing which training cases to use as landmarks and deciding how to weight them. They work remarkably well on many <phrase>machine learning</phrase> tasks even though the selected features are non-adaptive. The success of SVMs dampened the earlier enthusiasm for <phrase>neural networks</phrase>. More recently, however, it has been shown that multiple layers of feature detectors can be learned greedily, one layer at a time, by using <phrase>unsupervised learning</phrase> that does not require <phrase>labeled data</phrase>. The features in each layer are designed to model the statistical structure of the patterns of feature activations in the previous layer. After learning several layers of features this way without paying any attention to the final goal, many of the high-level features will be irrelevant for any particular task, but others will be highly relevant because <phrase>high-order</phrase> correlations are the signature of the data's true underlying causes and the labels are more directly related to these causes than to the raw inputs. A subsequent stage of fine-tuning using backpropagation then yields <phrase>neural networks</phrase> that work much better than those trained by backpropagation alone and better than SVMs for important tasks such as object or <phrase>speech recognition</phrase>. The neural …
<phrase>Temperature-gradient</phrase> based <phrase>test scheduling</phrase> for 3D stacked ICs —Defects that are dependent on <phrase>temperature-gradients</phrase> (e.g., <phrase>delay-faults</phrase>) introduce a challenge for achieving an effective <phrase>test process</phrase>, in particular for 3D ICs. Testing for such defects must be performed when the proper <phrase>temperature gradients</phrase> are enforced on the IC, otherwise these defects may escape the test. In this paper, a technique that efficiently heats up the IC during test so that it complies with the specified <phrase>temperature gradients</phrase> is proposed. The specified <phrase>temperature gradients</phrase> are achieved by applying heating sequences to the cores of the IC under test trough test access mechanism; thus no external heating mechanism is required. The scheduling of the test and heating sequences is based on thermal simulations. The schedule generation is guided by functions derived from the IC's temperature equation. <phrase>Experimental results demonstrate</phrase> that the proposed technique offers considerable test time savings. I. INTRODUCTION A promising technology for fabricating three dimensional <phrase>integrated circuits</phrase> is based on Through-Silicon Vias (TSV) [7, 9, 13]. The ICs fabricated using TSVs are commonly referred to as 3D Stacked IC (3D-SIC) [13]. 3D-SIC and other <phrase>deep submicron technologies</phrase> suffer from a considerably larger number of <phrase>delay faults</phrase> as compared with previous technologies. The causes for these <phrase>delay faults</phrase> include resistive bridges and vias, power droops, and cross-talk noise effects. Therefore, <phrase>delay-fault testing</phrase> is necessary to provide sufficient <phrase>fault coverage</phrase> [4, 10]. A large number of pre-bond TSV defects are resistive in nature and, moreover, the mechanical stress caused by TSVs contributes to <phrase>delay faults</phrase> [7, 9]. Therefore, the expected number of <phrase>delay faults</phrase> for 3D-SIC is larger than that of 2D ICs; thus <phrase>delay-fault test</phrase> is, in particular, important for 3D-SIC. Since temperature has a significant effect on delay, its impact should be taken into account in <phrase>delay-fault test</phrase>. A very important effect of temperature on <phrase>signal integrity</phrase> throughout an IC is its effect on the clock network [6]. <phrase>Delay faults</phrase> usually happen because of increased <phrase>clock skew</phrase> and the major sources of skew for 3D-SICs are induced by <phrase>temperature gradients</phrase> [15]. Since <phrase>propagation delays</phrase> in a clock network depend on temperature, different temperatures at different places on an IC (i.e., <phrase>temperature gradients</phrase>) result in <phrase>clock skew</phrase>. <phrase>Temperature gradients</phrase> in an IC may reach up to 50℃ in adjacent cores for <phrase>normal operation</phrase> and even higher during test [6, 15]. Besides, the <phrase>temperature gradients</phrase> in 3D-SICs are much larger than in 2D ICs [19]. This will exacerbate <phrase>temperature gradient</phrase> related …
MAS <phrase>Meta-models</phrase> on Test: UML vs. OPM in the SODA <phrase>Case Study</phrase> In the AOSE (<phrase>Agent-Oriented</phrase> <phrase>Software Engineering</phrase>) area, several research efforts are underway to develop appropriate <phrase>meta-models</phrase> for <phrase>agent-oriented</phrase> methodologies. <phrase>Meta-models</phrase> are meant to check and verify the completeness and expressiveness of methodologies. In this paper, we put to test the well-established standard Unified Modelling Language (UML), and the emergent Object Process Methodology (OPM), and compare their meta-modelling power. Both UML and OPM are used to express the meta-model of SODA, an <phrase>agent-oriented</phrase> methodology which stresses interaction and social aspects of MASs (<phrase>multi-agent systems</phrase>). Meta-modelling SODA allows us to evaluate the effectiveness of the two approaches over both the structural and dynamics parts. Furthermore , this allow us to find out some desirable features that any effective approach to meta-modelling MAS methodologies should exhibit. The definition of a methodology is an interactive process, in which a core is defined and then extended to include all the needed concepts. Meta-modelling enables checking and verifying the completeness and expressiveness of a methodology by understanding its deep semantics, as well as the relationships among concepts in different languages or methods [1]. According to [2], the process of designing a system (object or <phrase>agent-oriented</phrase>) consists of instantiating the system meta-model that the designers have in their mind in order to fulfil the specific problem requirements. In the agent world this means that the meta-model is the critical element(...) because of the variety of methodology <phrase>meta-models</phrase>. In the context of MASs, a meta-model should be a structural representation of the elements (agents, roles, behaviour, ontology,. . .) that constitute the actual system, along with their <phrase>composing</phrase> relationships. Several <phrase>meta-models</phrase> of AOSE methodologies can be found in
Kasparov versus <phrase>Deep Blue</phrase> - <phrase>computer chess</phrase> comes of age is misidentified as " Andreas Nowatzyk " , someone whom Newborn obviously did not know or take the time to find out about. Also, while the material presented by Newborn is sound as far as it goes, it is very slanted toward technical mishaps and other irrelevant trivia when he could have presented games that show the gradual growth of playing strength. It is significant that a coterie of <phrase>computer chess</phrase> achievers have not understood much about chess itself. It is highly commendable that these individuals have achieved all that they have, based principally on generation innovations tested against older versions. However, when such individuals write about their work, it becomes quite clear that they really do not understand what they have or have not achieved. It is as if someone swam the <phrase>English Channel</phrase> and his report is that he just kept putting one arm in front of the other until he touched shore. True, yes, but not very informative. Related to this schism between the technologists and the chess players is the fact that the 1996 <phrase>Deep Blue</phrase> was dominated by the technologists, and once Kas-parov understood its weaknesses, he had no trouble bringing it to its knees. However, in 1997 chess understanding was invoked in a number of small innovations, and this created an entity that played so well that Kasparov became flustered as his tricks were turned aside, and he eventually went off the " deep " end. The real story of <phrase>Computer Chess</phrase> Comes of Age should be written about what the <phrase>Deep Blue</phrase> team did between the matches of 1996 and 1997. I hope this book will be written some day. In the meantime, I will give Newborn's book <phrase>a 7</phrase> on a scale of 1 to 10 and hope that someone who knows more about chess will revise it. This book was published after the 1996 match between Kasparov and <phrase>Deep Blue</phrase>, but before the 1997 match in which Kasparov was defeated. Given the hype and <phrase>speculation</phrase> following that event, I find it difficult to believe that anything I say in this review will persuade anyone to either buy this book or pass on it. However, here goes. Monty Newborn is a phenomenal raconteur. His stories are truly wonderful, and his introductory <phrase>tale</phrase> of a conversation among a group of <phrase>birds</phrase> watching early attempts at human flight is probably worth one third the …
<phrase>Fuzzy Logic</phrase> <phrase>Trajectory Tracking</phrase> Controller for a Tanker This paper proposes a <phrase>fuzzy logic</phrase> controller for design of <phrase>autopilot</phrase> of a ship. Triangular membership functions have been use for fuzzification and the <phrase>centroid</phrase> method for defuzzification. A nonlinear <phrase>mathematical model</phrase> of an <phrase>oil tanker</phrase> has been considered whose parameters vary with the depth of water. The performance of proposed controller has been tested under both course changing and trajectory keeping mode of operations. It has been demonstrated that the performance is robust in shallow as well as <phrase>deep waters</phrase>. D esign of ship control system has been great challenge since long time, because the dynamics of ship are not only nonlinear but change with <phrase>operating conditions</phrase> (depth of water and speed of vehicle). These are also highly influenced by unpredictable external environmental disturbances like winds, sea currents and wind generated waves. IFAC (International Federation of <phrase>Automatic Control</phrase>) has identified the ship control system as one of the benchmark problems, which are difficult to solve [1]. In 1911 Elmer Sperry constructed first automatic steering mechanism by developing a <phrase>closed loop</phrase> system for a ship [2-3]. Minorsky [4] extended the work of Sperry by giving detailed analysis of position <phrase>feedback control</phrase> and formulated three term control law which is referred to as PID (Proportional Integral Derivative) control. Until the 1960s this type of controller was extensively used but after that other linear autopilots like LQG and H ∞ have been reported [5-7]. <phrase>Nonlinear control</phrase> schemes such as state feedback linearizaton, backstepping, output <phrase>feedback control</phrase>, Lyapunov methods and <phrase>sliding mode control</phrase> [8-12] have also been proposed. Applications of <phrase>nonlinear control</phrase> techniques depend on exact knowledge of plant dynamics to be controlled. Since the marine <phrase>vehicle dynamics</phrase> are highly nonlinear and are coupled with <phrase>hydrodynamics</phrase> therefore it is difficult to obtain exact dynamic model of a marine vehicle. To overcome these difficulties, the model free control strategies like <phrase>Fuzzy Logic</phrase> and <phrase>Neural Networks</phrase> are proved considerably useful. Yang, et. al. [13-14] discussed the application of <phrase>fuzzy control</phrase> for nonlinear systems and applied <phrase>Takagi-Sugeno</phrase> type <phrase>autopilot</phrase> for a ship to maintain its heading. <phrase>Santos</phrase>, et. al. [15] proposed the fuzzy <phrase>autopilot</phrase> along with <phrase>PID controller</phrase> for control of vertical <phrase>acceleration</phrase> for fast moving <phrase>ferries</phrase>. Velagic, et. al. [16] developed ship <phrase>autopilot</phrase> for track keeping by using Sugeno type fuzzy system.
<phrase>Phase Transitions</phrase>: A New Paradigm for Evaluating Complexity in Learning and other Complex Systems The study of the <phrase>computational complexity</phrase> of algorithms takes a central place in <phrase>computer science</phrase>. Since its beginning, scientists have studied problems from the complexity <phrase>point of view</phrase>, categorizing their behaviour into a well-known <phrase>complexity class</phrase> hierarchy. Many interesting problems have proved to be intractable. Among these, a class of problems that are particularly prone to a dramatic increase in <phrase>computational complexity</phrase> with increasing problem size is the class of combinatorial problems, which include <phrase>learning problems</phrase>. Recently, it has been uncovered that computational problems show <phrase>critical phenomena</phrase>, similar to those that emerge in physical many-body systems. In particular, <phrase>learning problems</phrase> have shown to be the subject of the emergence of <phrase>phase transitions</phrase>, which allow a new paradigm for evaluating complexity to be considered. Thus, new paradigm is based on the notion of " typical " complexity instead of " <phrase>worst case</phrase> " complexity. For computational systems, the discovery of a <phrase>phase transition</phrase> has important consequences. First of all, the <phrase>phase transition</phrase> region contains the most difficult <phrase>problem instances</phrase>, for which the <phrase>computational complexity</phrase> increases exponentially when the problem size increases. Then, the <phrase>phase transition</phrase> can be used as a source of " difficult " <phrase>test problems</phrase> for assessing the properties and the power of algorithms, and to compare them on meaningful <phrase>problem instances</phrase> on a parity base. Moreover, very small variations of the problem parameters may induce very large variations in the algorithm's behavior and/or in the types of solution. Then, the knowledge of the critical value allows the user to roughly predict the behavior. Moreover, by exploiting further the analogy with physical systems, it is possible to enter into the <phrase>deep structure</phrase> of the problem and of its solutions; the system's behavior near the <phrase>phase transition</phrase> allows a microscopic view of the solution space. This fact not only offers the possibility of a deeper understanding of algorithms' properties, but opens the way to the introduction of new effective algorithms. In this talk, we will show the far-reaching effects of the presence of <phrase>phase transitions</phrase> in various <phrase>learning approaches</phrase>. Moreover, connections will be established with fundamental problem classes in computer science, such as the satisfiability problem and the <phrase>constraint satisfaction problem</phrase> classes, and with <phrase>statistical physics</phrase> approaches, which offered some very effective algorithms. Finally we will show how <phrase>phase transitions</phrase> are rather ubiquitous in both natural and artificial systems, including <phrase>human vision</phrase> and neural system.
<phrase>Peak-load</phrase> Transmission Pricing for the Ieee <phrase>Reliability Test</phrase> System <phrase>Peak-load</phrase> Transmission Pricing for the Ieee <phrase>Reliability Test</phrase> System In order to facilitate the use of inexpensive generation when the existing transmission system creates obstacle to the optimal power transfer, this thesis analyzes the basic <phrase>trade-off</phrase> between using expensive generation and investing in transmission enhancement. Understanding this <phrase>trade-off</phrase> has taken on a new importance as the <phrase>electric utility</phrase> industry undergoes reconstruction from being a regulated <phrase>monopoly</phrase> into serving competitive generation. This thesis continues the work started by Lecinq [8], that has introduced the basic notions of an optimal transmission system and a <phrase>peak-load</phrase> pricing mechanism capable of recovering the transmission enhancement investments. However, the main contribution of this thesis is an in depth study of transmission provision and <phrase>peak-load</phrase> pricing on a relatively large power system, namely, a 24 bus IEEE <phrase>Reliability Test</phrase> System' (often used as an IEEE test standard). In addition, MATLAB-based software was developed to accomplish the objective of <phrase>economic efficiency</phrase>, by valuing <phrase>trade-offs</phrase> between the cost of expensive generation and the transmission enhancement cost. In order to understand the implications of the <phrase>peak-load</phrase> pricing mechanism, various simulations, regarding the effect of the type of transmission pricing on the overall <phrase>economic efficiency</phrase>, were performed using this software on the IEEE <phrase>RTS</phrase>. Finally, the software developed here could be used as a basic transmission enhancement planning tool. I wish to express my deep gratitude to Marija Ilic, my advisor and foremost supporter. I have learned a lot working with her during my graduate study at MIT, and her advice and guidance were invaluable for this work. The <phrase>financial support</phrase> provided by the M.I.T. <phrase>Consortium</phrase> on Transmission Provision and Pricing is also greatly appreciated. Finally, I owe myself to my family, whose confidence and love were with me all these years. Without them, my personal and academic accomplishments would be meaningless.
Fuzzy <phrase>Adaptive Control</phrase> for <phrase>Trajectory Tracking</phrase> of <phrase>Autonomous Underwater Vehicle</phrase> In this paper, the problem of the position and attitude tracking of an <phrase>autonomous underwater vehicle</phrase> (<phrase>AUV</phrase>) in the horizontal plane, under the presence of <phrase>ocean current</phrase> disturbances is discussed. The effect of the gradual variation of the parameters is taken into account. The effectiveness of the adaptive controller is compared with a feedback <phrase>linearization</phrase> method and fuzzy gain control approach. The proposed strategy has been tested through simulations. Also, the performance of the proposed method is compared with other strategies given in some other studies. The boundedness and asymptotic convergence properties of the <phrase>control algorithm</phrase> and its semi-global stability are analytically proven using <phrase>Lyapunov stability</phrase> theory and Barbalat's lemma. I. Introduction <phrase>Autonomous underwater vehicle</phrase> (<phrase>AUV</phrase>) is a field of increasing interest due to its many interesting applications. <phrase>Underwater vehicles</phrase> are extensively employed in the offshore industry, subaquatic scientific investigations and <phrase>rescue</phrase> operations, finding sunken ships, searching for lost artifacts. As they are untethered, they may operate under ice, opening up vast, largely unexplored <phrase>Arctic</phrase> areas that are inaccessible to any other kind of <phrase>research vessel</phrase>, and operate at depths too deep for tethered vehicles. They are also of military interest (e.g. see [1]). Many control methods for <phrase>underwater vehicles</phrase> have been discussed in the literature in the past 15 years to handle uncertainties related to the dynamics, <phrase>hydrodynamics</phrase> and external disturbances. Especially, for developing advanced control strategies for <phrase>Autonomous Vehicles</phrase>. <phrase>Recent developments</phrase> in this area are well summarized in [2,3] in which different <phrase>motion control</phrase> algorithms have been developed under various hypotheses. Adopting a linearized model, some linear control techniques such as <phrase>PID controller</phrase> [4] and LQR algorithm [5] have been developed with acceptable performance in only special kinds of maneuvering.
The beauty of simplicity I fully agree with Kamp that the decision, conscience or not, was a mistake, and ultimately a very costly one. In my own C programming in the 1970s I found it a frequent source of frustration but believe I understand its motivation: C was designed with the <phrase>PDP-11</phrase> <phrase>instruction set</phrase> very much in mind. The celebrated C one-liner while (*s++ = *t++) ; copies the string at s to the string at t. Elegant indeed! But what may have been lost in the <phrase>fog</phrase> of time is the fact that it compiles into a loop of just two <phrase>PDP-11</phrase> instructions, where register R0 contains the location of s and register R1 contains the location of t: A mov (@R0)++,(@R1)++ bne A test result for nonzero and branch Such concise code was seductive and, as I recall, mentioned often in discussions of C. A preceding length count would have required at least three instructions. But even at this level of coding, the <phrase>economy</phrase> of the code for moving a string was purchased for a high price: having to search for the terminating null in order to determine the length of a string; that price was also paid when concatenating strings. The <phrase>security issues</phrase> resulting from potential buffer overruns could hardly have been anticipated at the time, but, even then, such computational costs were apparent. " X-Rays will prove to be a <phrase>hoax</phrase> " : <phrase>Lord</phrase> <phrase>Kelvin</phrase>, 1883. Even the most brilliant scientists sometimes get it wrong. Beware this fatal instruction? Regarding the article " Postmortem Debugging in Dynamic Environments " by <phrase>David</phrase> Pacheco (Dec. 2011), I have a question regarding the broken code example in the section on A s an admirer of the " artistic flare, nuanced style, and technical prowess that separates good code from great code " explored by <phrase>Robert Green</phrase> and Henry Ledgard in their article " Coding Guidelines: Finding the Art in the Science " (Dec. 2011), I was disappointed by the <phrase>au</phrase>-thors' emphasis on " alignment, naming , use of white space, use of context, <phrase>syntax highlighting</phrase>, and <phrase>IDE</phrase> choice. " As effective as these aspects of beautiful code may be, they are at best only skin deep. Beauty may indeed be in the eye of the beholder, but there is a more compelling beauty in the deeper semantic properties of code than layout and naming. I also include judicious use of abstraction, deftly balancing …
State Pruning for Test <phrase>Vector Generation</phrase> for a Multiprocessor <phrase>Cache Coherence Protocol</phrase> 2 Test <phrase>Vector Generation</phrase> Verification is extremely important in designing digital systems such as a <phrase>cache coherence protocol</phrase>. Generating traces for system verification using a <phrase>model checker</phrase> and then using the traces to drive the RTL logic design simulation is an efficient method for debugging. This approach has been called the <phrase>witness string</phrase> method [2]. Since <phrase>depth first search</phrase> (DFS) can quickly get deep into the state space, the original <phrase>witness string</phrase> method is based on DFS. We investigate a state pruning method that exploits multiple search heuristics in simultaneous DFS searches to choose the most efficient traces. In this state pruning algorithm each DFS uses a different heuristic (i.e., min/max <phrase>Hamming distance</phrase>, min/max cache_score). We distribute the <phrase>hash table</phrase> of the entire <phrase>state space</phrase> among the simultaneous searches so that they cooperate to avoid redundant state exploration. To evaluate this new <phrase>search algorithm</phrase> for the <phrase>witness string</phrase> method, we implant several protocol bugs in the Stanford DASH <phrase>cache coherence protocol</phrase>. Using an IBM <phrase>Power4</phrase> system with the <phrase>Berkeley</phrase> Active Message library, we show an improvement in witness strings generation through the state pruning method over a pure DFS and a guided DFS. 1 Introduction System verification in the pre-silicon state of development is crucial for controlling the budget and the time to market. <phrase>Safety critical</phrase> systems, such as a <phrase>cache coherence protocol</phrase>, are not trivial to verify because there are numerous correctness properties which result in a very large <phrase>state space</phrase>. The large <phrase>state space</phrase> causes the state explosion problem when enumerating states for <phrase>formal verification</phrase>. In order to alleviate this problem, the cache coherency models are usually " down-scaled. " Verification of the " down-scaled " models cannot guarantee the correctness of the designs, but is useful as a debugging tool [1]. The <phrase>witness string</phrase> approach [2] is such a method that captures bug traces from <phrase>depth-first-search</phrase> (DFS) of a <phrase>state space</phrase> to help debug the RTL design of a <phrase>cache coherence protocol</phrase>. The efficiency of the resulting witness strings captured through this DFS process is very important. For instance, in the <phrase>Cray</phrase> X1 <phrase>cache coherence protocol</phrase>, a single <phrase>witness string</phrase> can be on the order of a few thousands states. It takes a considerable amount of time to execute all of the necessary witness strings in the RTL <phrase>logic simulation</phrase> [3]. One method to determine efficient witness strings is through a bug-oriented DFS search of the state space. To enhance the …
Materializing multi-<phrase>relational databases</phrase> from the web using taxonomic queries Recently, much attention has been given to extracting tables from <phrase>Web data</phrase>. In this problem, the column definitions and tuples (such as what "company" is headquartered in what "city,") are extracted from Web text, structured <phrase>Web data</phrase> such as lists, or results of querying the deep Web, creating the table of interest. In this paper, we examine the problem of extracting and discovering <i>multiple</i> tables in a given domain, generating a truly multi-<phrase>relational database</phrase> as output. Beyond discovering the relations that define single tables, our approach discovers and leverages "within column" set membership relations, and discovers relations across the extracted tables (e.g., joins). By leveraging within-column relations our method can extract table instances that are ambiguous or rare, and by discovering joins, our method generates truly multi-relational output. Further, our approach uses taxonomic queries to bootstrap the extraction, rather than the more traditional "seed instances." Creating seeds often requires more <phrase>domain knowledge</phrase> than taxonomic queries, and previous work has shown that extraction methods may be sensitive to which input seeds they are given. We test our approach on two <phrase>real world</phrase> domains: <phrase>NBA</phrase> <phrase>basketball</phrase> and cancer information. Our <phrase>results demonstrate</phrase> that our approach generates databases of relevant tables from disparate Web information, and discovers the relations between them. Further, we show that by leveraging the "within column" relation our approach can identify a significant number of relevant tuples that would be difficult to do so otherwise.
Comparison of Effectiveness of <phrase>Current Ratio</phrase> and <phrase>Delta-IDDQ</phrase> Tests I DDQ test is a valuable test method for semiconductor manufacturers. However, its effectiveness is reduced for <phrase>deep sub-micron technology</phrase> chips due to rising background leakage. Current two <phrase>test methods</phrase> that promise to extend the life of I DDQ test are <phrase>Current Ratio</phrase> and <phrase>Delta-I DDQ</phrase>. Although several studies have been reported on these methods, their effectiveness in detecting defects has not been contrasted. In this work, we compare these two methods using industrial <phrase>test data</phrase>.
Deep Outdoor Illumination Estimation We present a CNN-based technique to estimate high-<phrase>dynamic range</phrase> outdoor illumination from a single low <phrase>dynamic range</phrase> image. To train the CNN, we leverage a large dataset of outdoor panoramas. We fit a low-dimensional physically-based outdoor illumination model to the skies in these panoramas giving us a compact set of parameters (including <phrase>sun</phrase> position, atmospheric conditions, and camera parameters). We extract limited field-of-view images from the panoramas, and train a CNN with this large set of input image–output lighting parameter pairs. Given a test image, this network can be used to infer illumination parameters that can, in turn, be used to reconstruct an outdoor illumination environment map. We demonstrate that our approach allows the recovery of plausible illumination conditions and enables automatic <phrase>photorealistic</phrase> virtual object insertion from a single image. An extensive evaluation on both the <phrase>panorama</phrase> dataset and captured HDR environment maps shows that our technique significantly outper-forms previous solutions to this problem.
Finding <phrase>Peer-to-Peer File-Sharing</phrase> Using Coarse Network Behaviors A user who wants to use a service forbidden by their site's usage policy can masquerade their packets in order to evade detection. One masquerade technique sends prohibited traffic on TCP ports commonly used by permitted services, such as port 80. Users who hide their traffic in this way pose a special challenge, since filtering by port number risks interfering with legitimate services using the same port. We propose a set of tests for identifying masqueraded <phrase>peer-to-peer file-sharing</phrase> based on traffic summaries (flows). Our approach is based on the hypothesis that these applications have <phrase>observable</phrase> behavior that can be differentiated without relying on deep packet examination. We develop tests for these behaviors that, when combined, provide an accurate method for identifying these masqueraded services without relying on payload or port number. We test this approach by demonstrating that our integrated detection mechanism can identify <phrase>BitTorrent</phrase> with a 72% true positive rate and virtually no observed false positives in control services (FTP-Data, HTTP, <phrase>SMTP</phrase>).
Design and Field Experimentation of a Prototype <phrase>Lunar Prospector</phrase> Design and Field Experimentation of a Prototype <phrase>Lunar Prospector</phrase> <phrase>Scarab</phrase> is a prototype rover for Lunar missions to survey resources in polar <phrase>craters</phrase>. It is designed as a prospector that would use a deep coring drill and apply soil analysis instruments to measure the abundance of elements of hydrogen and <phrase>oxygen</phrase> and other <phrase>volatiles</phrase> including water. Scarab's <phrase>chassis</phrase> can adjust the <phrase>wheelbase</phrase> and height to stabilize its drill in contact with the ground and can also adjust posture to better ascend and descend steep slopes. This enables unique control of posture when moving and introduces new planning issues. <phrase>Scarab</phrase> has undergone <phrase>field testing</phrase> at Lunar-analog sites in <phrase>Washington</phrase> and <phrase>Hawaii</phrase> in an effort to quantify and validate its mobility and navigation capabilities. We report on results of the experiments in slope ascent and descent and in autonomous kilometer-distance navigation in darkness.
Automatic computation of electrode trajectories for <phrase>Deep Brain Stimulation</phrase>: a hybrid symbolic and numerical approach PURPOSE The optimal electrode trajectory is needed to assist surgeons in planning <phrase>Deep Brain Stimulation</phrase> (DBS). A method for <phrase>image-based</phrase> <phrase>trajectory planning</phrase> was developed and tested.   METHODS Rules governing the DBS <phrase>surgical procedure</phrase> were defined with geometric constraints. A formal geometric solver using multimodal <phrase>brain images</phrase> and a template built from 15 brain MRI scans were used to identify a space of possible solutions and select the optimal one. For validation, a retrospective study of 30 DBS electrode implantations from 18 patients was performed. A trajectory was computed in each case and compared with the trajectories of the electrodes that were actually implanted.   RESULTS Computed trajectories had an average difference of 6.45° compared with reference trajectories and achieved a better overall score based on satisfaction of geometric constraints. Trajectories were computed in 2 min for each case.   CONCLUSION A rule-based solver using pre-operative <phrase>MR brain images</phrase> can automatically compute relevant and accurate patient-specific DBS electrode trajectories.
RH: A Retro-Hybrid Parser Contemporary parser research is, to a large extent, focused on statistical parsers and deep-unification-based parsers. This paper describes an alternative, <phrase>hybrid architecture</phrase> in which an <phrase>ATN</phrase>-like parser, augmented by many preference tests, builds on the results of a fast chunker. The combination is as efficient as most stochastic parsers, and accuracy is close and continues to improve. These results raise questions about the practicality of deep unification for symbolic parsing.
Analytic Models for Crosstalk Delay and Pulse Analysis Under Non-Ideal Inputs In this paper 1 we develop a general methodology to analyze crosstalk to obtain insight into effects that are likely to cause errors in <phrase>deep submicron</phrase> <phrase>high speed</phrase> circuits. We focus on crosstalk due to <phrase>capacitive coupling</phrase> between a pair of lines. We first consider the case where <phrase>crosstalk noise</phrase> manifests as a pulse and characterize the maximum amplitude, width, energy and timing of this pulse. <phrase>Closed form</phrase> equations quantifying the dependence of these pulse attributes on the values of <phrase>circuit parameters</phrase> and the rise time of the input transition are derived. We also consider how crosstalk causes slowdown (speedup), i.e. increases (decreases) the rise/fall times of signals on coupled lines, when their inputs have transitions in the opposite (same) directions. Expressions relating the slowdown (speedup) to <phrase>circuit parameters</phrase>, the rise/fall times of the input transitions, and the skew between the transitions are derived. We show that <phrase>crosstalk effects</phrase> can be significantly aggravated by variations in the fabrication process. New design corners are identified for validation of designs that have significant <phrase>crosstalk effects</phrase>. Finally, the results of our analysis provide conditions that must be satisfied by a sequence of vectors used for validation of designs as well as post-manufacturing testing of devices in the presence of significant crosstalk. I. Introduction Continuous advancements in the field of VLSI has lead to a decrease in device geometries (<phrase>deep sub-micron technology</phrase>), high device densities, high clock rates and small signal transition times. Due to these changes, <phrase>crosstalk noise</phrase> between adjacent interconnects has become an important concern. If not carefully considered during design validation, crosstalk may cause extra signal delay, logic hazards, and even circuit malfunction. Accurate <phrase>modeling and simulation</phrase> of interconnect delay due to crosstalk thus becomes increasingly important in the design of <phrase>high performance</phrase> <phrase>integrated circuits</phrase>.
Effects of Multi-cycle Sensitization on <phrase>Delay Tests</phrase> India-building the tall, thin VLSI engineer p. 5 Advances in <phrase>VLSI design</phrase> and product development challenges p. 6 <phrase>High level</phrase> modeling and validation methodologies for <phrase>embedded systems</phrase> : bridging the productivity gap p. 9 Design of <phrase>deep sub-micron CMOS</phrase> circuits p. 15 Testing embedded cores and Socs-DFT, ATPG and BIST solutions p. 17 Specification and design of multi-million gate SOCs p. 18 ESD reliability challenges of RF/<phrase>mixed signal</phrase> design and processing p. 20 System support for embedded applications p. 22 Narrow band noise suppression scheme for improving <phrase>signal to noise ratio</phrase> p. 25 A path sensitization technique for testing of switched <phrase>capacitor</phrase> circuits p. 30 A novel RF <phrase>front-end</phrase> <phrase>chipset</phrase> for <phrase>ISM band</phrase> wireless applications p. 36 Development of 2.4 GHz RF transceiver <phrase>front-end</phrase> <phrase>chipset</phrase> in 0.25 [mu]m CMOS p. 42 Comparison of heuristic algorithms for variable partitioning in circuit implementation p. 51 Timing minimization by <phrase>statistical timing</phrase> hMetis-based partitioning p. 58 An efficient practical heuristic for good ration-cut partitioning p. 64 An efficient multi-level partitioning algorithm for <phrase>VLSI circuits</phrase> p. 70 <phrase>Low power</phrase> technology mapping for LUT based FPGA-a genetic algorithm approach p. 79 Routability prediction for field programmable gate arrays with a routing hierarchy p. 85 A fast macro based <phrase>compilation</phrase> methodology for partially reconfigurable FPGA designs p. 91 Detailed analysis of FIBL in MOS transistors with high-k gate dielectrics p. 99 Effect of scaling on the non-quasi-static behaviour of the MOSFET for RF IC's p. 105 Small signal characteristics of thin film single halo SOI MOSFET for mixed mode applications p. 110 A new approach to analyze a <phrase>sub-micron CMOS</phrase> inverter p. 116 Optimization of 1.8V I/O circuits for performance, reliability at the 100nm technology node p. 122 Application of look-up table approach to high-k gate dielectric MOS transistor circuits p. 128 Effects of multi-cycle sensitization on <phrase>delay tests</phrase> p. 137 Exclusive test and its applications to <phrase>fault diagnosis</phrase> p. 143 A fault-independent <phrase>transitive closure</phrase> algorithm for redundancy identification p. 149 Exploiting ghost-FSMs as a BIST structure for sequential machines p. 155 Design of a universal BIST (UBIST) structure p. 161 SPaRe : selective partial replication for concurrent <phrase>fault detection</phrase> in FSMs p. 167 Design of a 2D <phrase>DCT</phrase>/IDCT application specific <phrase>VLIW</phrase> processor supporting scaled and sub-sampled blocks p. 177 Design of a <phrase>high speed</phrase> <phrase>string matching</phrase> co-processor for NLP p. 183
A 60 Ghz High Gain Transformer-coupled Differential Power Amplifier in 65nm Cmos — A fully differential 60 GHz three-stage transformer-coupled amplifier is designed and implemented in 65 nm digital <phrase>CMOS process</phrase>. On-chip <phrase>transformers</phrase> which offer DC biasing for individual stages, extra stabilization mechanisms, and simultaneous input/inter-stage/output matching networks are used to facilitate a compact <phrase>circuit design</phrase>. With a cascoded circuit configuration, the amplifier is tested with a linear gain of 30.5 dB centered at 63.5 GHz and a-40 dB reverse isolation under a 1 V supply. The amplifier delivers 9 <phrase>dBm</phrase> and 13 <phrase>dBm</phrase> output power under 1 V and 1.5 V supplies, respectively and occupies a core chip area of 0.05 mm 2. The measurement results validate a high gain and area efficient power amplifier <phrase>design methodology</phrase> in deep-scaled CMOS for millimeter-wave communication system applications.
Learning from mistakes: a comprehensive study on <phrase>real world</phrase> <phrase>concurrency bug</phrase> characteristics The reality of <phrase>multi-core</phrase> hardware has made <phrase>concurrent programs</phrase> pervasive. Unfortunately, writing correct <phrase>concurrent programs</phrase> is difficult. Addressing this challenge requires advances in multiple directions, including <phrase>concurrency bug</phrase> detection, concurrent program testing, concurrent programming model design, etc. Designing effective techniques in all these directions will significantly benefit from a deep understanding of <phrase>real world</phrase> <phrase>concurrency bug</phrase> characteristics.  This paper provides the first (to the best of our knowledge) comprehensive <phrase>real world</phrase> <phrase>concurrency bug</phrase> characteristic study. Specifically, we have carefully examined <phrase>concurrency bug</phrase> patterns, manifestation, and fix strategies of 105 randomly selected <phrase>real world</phrase> <phrase>concurrency bugs</phrase> from 4 representative server and client <phrase>open-source</phrase> applications (<phrase>MySQL</phrase>, <phrase>Apache</phrase>, <phrase>Mozilla</phrase> and <phrase>OpenOffice</phrase>). Our study reveals several interesting findings and provides useful guidance for <phrase>concurrency bug</phrase> detection, testing, and concurrent <phrase>programming language</phrase> design.  Some of our findings are as follows: (1) Around one third of the examined non-deadlock <phrase>concurrency bugs</phrase> are caused by violation to programmers' order intentions, which may not be easily expressed via synchronization primitives like locks and transactional memories; (2) Around 34% of the examined non-deadlock <phrase>concurrency bugs</phrase> involve multiple variables, which are not well addressed by existing bug detection tools; (3) About 92% of the examined <phrase>concurrency bugs</phrase> canbe reliably triggered by enforcing certain orders among no more than 4 <phrase>memory accesses</phrase>. This indicates that testing <phrase>concurrent programs</phrase> can target at exploring possible orders among every small groups of <phrase>memory accesses</phrase>, instead of among all <phrase>memory accesses</phrase>; (4) About 73% of the examinednon-deadlock <phrase>concurrency bugs</phrase> were not fixed by simply adding or changing locks, and many of the fixes were not correct at the first try, indicating the difficulty of reasoning concurrent execution by programmers.
Guaranteed Avoidance of Unpredictable, Dynamically Constrained Obstacles Using <phrase>Velocity Obstacle</phrase> Sets Guaranteed Avoidance of Unpredictable, Dynamically Constrained Obstacles Using <phrase>Velocity Obstacle</phrase> Sets Dynamic <phrase>obstacle avoidance</phrase> is an important, ubiquitous, and often challenging problem for autonomous <phrase>mobile robots</phrase>. This <phrase>thesis presents</phrase> a new method to guarantee collision avoidance with respect to moving obstacles that have constrained dynamics but move unpredictably. Velocity Obstacles have been widely used to plan trajectories that avoid collisions with obstacles under the assumption that the path of the objects are either known or can be accurately predicted ahead of time. However, for real systems, this predicted path will typically only be accurate over short time-horizons. To achieve safety over longer time periods, the method introduced here instead considers the set of all reachable points by an obstacle assuming that the dynamics fit the <phrase>unicycle</phrase> model, which has known constant forward speed and a maximum turn rate (sometimes called the Dubins car model). This thesis extends the <phrase>Velocity Obstacle</phrase> formulation by using <phrase>reachability</phrase> sets in place of a single " known " trajectory to find matching constraints in velocity space, called <phrase>Velocity Obstacle</phrase> Sets. The <phrase>Velocity Obstacle</phrase> Set for each obstacle is equivalent to the <phrase>union</phrase> of all velocity obstacles corresponding to any dynamically feasible future trajectory, given the obstacle's <phrase>current state</phrase>. This region remains bounded as the time horizon is increased to <phrase>infinity</phrase>, and by choosing control inputs that lie outside of these <phrase>Velocity Obstacle</phrase> Sets, it is guaranteed that the host agent can always actively avoid collisions with the obstacles, even without knowing their exact future paths. It thus follows that, subject to certain initial conditions, an iterative planner under these constraints guarantees safety for all time. Finally, the an iterative planner is repeatedly tested and analyzed in simulation under various conditions. If the time horizon is set to some finite value, the guaranteed collision avoidance is lost, but the planned trajectories generally become more direct. This effect of varying this time scale also depends on the presence of static obstacles in the environment and on the dynamic limitations of the host robot. Acknowledgments First, I owe deep thanks to my advisor, Professor Jonathan How, for his guidance over the last two years. His feedback and insights have been greatly beneficial to my research and to my academic career, and his consistent example of hard work and professionalism has demonstrated what it takes to achieve excellence. I also sincerely thank Dr. Luca Bertucelli for the attentive mentorship he has provided me. I would like to thank the members …
Positivist single <phrase>case study</phrase> research in <phrase>information systems</phrase>: a critical analysis Positivist, single <phrase>case study</phrase> is an important research approach within the <phrase>information systems</phrase> discipline. This paper provides detailed definitions of key concepts in positivist, single <phrase>case study</phrase> research and carefully analyses the conduct and outcomes of the Sarker and Lee study that examined the role of social enablers in <phrase>Enterprise Resource Planning</phrase> (<phrase>ERP</phrase>) systems implementation, and was presented at the International Conference on <phrase>Information Systems</phrase> in 2000. A number of key issues about positivist, single <phrase>case studies</phrase> are identified, including the need for a clear and deep understanding of key concepts including theory, proposition, hypothesis and <phrase>hypothesis testing</phrase>; the need for clearly defined concepts in theories being tested; the need for hypotheses not propositions when undertaking <phrase>empirical research</phrase>; the importance of explicit boundaries for theories; the distinction between single <phrase>case studies</phrase> and single experiments; and the problem of easy refutation of strong hypotheses using specific and unique cases. Despite these issues, positivist, single studies provide a sound and systematic approach for conducting research and are an important component of pluralist research programs within <phrase>information systems</phrase>.
Bistatic Deep Soundings with the Hf Gpr <phrase>Tapir</phrase> in the <phrase>Egyptian</phrase> White <phrase>Desert</phrase> Introduction: In the frame of the NetLander mission , the Centre d'étude des Environnements Terrestre et Planétaires (CETP) has developed an impulse <phrase>ground penetrating radar</phrase> (GPR) operating at very low central frequencies from 2 to 6 MHz [1]. This instrument , named <phrase>TAPIR</phrase> (Terrestrial And Planetary Investigation Radar), aims at exploring the geological features in the deep <phrase>Martian</phrase> subsurface and at the detection of liquid water underground <phrase>reservoirs</phrase>. This GPR was designed to retrieve, from a single fixed location, both the distance and the direction of the reflecting structures. This can be achieved by determining the propagation vector of reflected waves, through measurements of two horizontal electrical and three magnetic components of the reflected waves. This direction information is essential not only to characterize the sub-surface structures but also to discriminate between the echoes coming from the subsurface and those due to the surface clutter, In-situ measurements on well documented soils that are analogues of the expected <phrase>Martian</phrase> soils are crucial to validate the performances of the instrument. In 2004, ground tests were successfully carried out on the <phrase>Antarctic</phrase> <phrase>continent</phrase> with a mono-static GPR prototype [2]. In No-vember 2005, an updated version of the instrument working either in monostatic or in bi-static mode was tested in the <phrase>Egyptian</phrase> White <phrase>Desert</phrase>. Complementary sounding investigations were jointly conducted on the same site by research teams from the LPI (<phrase>Lunar and Planetary Institute</phrase>) [3] and the SwRI (<phrase>Southwest Research Institute</phrase>) [4]. They will provide independent information which will help the interpretation of our GPR data.
Dell Computer Corporation: a Zero-time Organization duplicated without permission. Any use, either electronic or paper-based, must have written permission of Dr. Pearlson or Dr. Yeh. The material contained in this case is from company documents, publicly available sources, and personal interviews held with key executives at Dell. The authors wish to thank the managers and executives at Dell for their time and support of this research. Deep in the heart of Texas lies a <phrase>Fortune 500</phrase> company who exemplifies many of the principles of a Zero Time organization. Dell Computer Corporation has seen extraordinarily growth: a 58% <phrase>revenue</phrase> increase and an 82% profit increase in 1997, an equally extraordinary short period of time. Sales <phrase>rose</phrase> to $12.3 billion in 1997, profits to $944 million in 1997, and the <phrase>stock split</phrase> for the sixth time in 1998. Much of this success is due to management principles and a vision that we describe here. First we provide some background information on the company, and we describe the management principals and philosophies we think make Dell a success. Finally, we describe Dell using the <phrase>lens</phrase> of a Zero Time organization. Company Background Many know the story of <phrase>Michael Dell</phrase>, his college-based business of building <phrase>personal computers</phrase> with available parts, and his build to order strategy. Founded in 1984 as PC's Limited, the name was officially changed worldwide to Dell Computer Corporation when the first stock offering took place, in June 1988. Other key turning points, according to <phrase>Michael Dell</phrase>, were in 1986, when Dell first went outside the US to <phrase>Europe</phrase> and hit $50 million in sales; 1989, when the company when from last to first place in their industry on the management of their inventory; and 1993 when the concept of <phrase>segmenting</phrase> took shape and allowed the management to regain control of customers. At the core of Dell's business was the build-to-order strategy. Customers ordered PCs directly, and their order was routed through a credit check, then directly to the manufacturing floor. The order was then built, tested, and shipped to the customer, who received it 5-7 days after placing their order. This strategy afforded Dell some impressive results. First, Dell eliminated middlemen-the resellers, who were part of the traditional distribution model. As such, Dell not only passed the savings to the customers in the form of lower costs, but was also able to understand customer needs first hand and adapt to market changes faster than competitors. Second, …
<phrase>Geomagnetic</phrase> Excursions: Knowns and Unknowns [1] <phrase>Geomagnetic</phrase> excursions are short-lived episodes when <phrase>Earth's magnetic field</phrase> deviates into an intermediate <phrase>polarity</phrase> state. Understanding the origin, frequency, amplitude, duration, and field behavior associated with excursions is a forefront research area within solid earth <phrase>geophysics</phrase>. Recent advances in excursion research are summarized here, and key further research is suggested to resolve major unanswered questions. Improving the global distribution of excursion records, particularly from the <phrase>southern hemisphere</phrase>, obtaining <phrase>high-resolution</phrase> <phrase>sedimentary</phrase> excursion records with good age control from sites with <phrase>sedimentation</phrase> rates >10 cm/kyr, obtaining <phrase>volcanic</phrase> excursion records coupled with <phrase>high-precision</phrase> <phrase>geochronology</phrase>, and estimating excursion duration with high chronological precision will all facilitate <phrase>hypothesis testing</phrase> concerning the <phrase>deep earth</phrase> dynamics that generate <phrase>geomagnetic</phrase> excursions.
Volumetric Layer Segmentation Using Coupled Surfaces Propagation The problem of <phrase>segmenting</phrase> a volumetric layer of-nite thickness is encountered in several important areas within medical <phrase>image analysis</phrase>. Key examples include the extraction of the cortical <phrase>gray matter</phrase> of the brain and the left ventricle <phrase>myocardium</phrase> of the heart. The coupling between the two bounding surfaces of such a layer provides important information that helps to solve the segmentation problem. Here we propose a new approach of coupled surfaces propagation via <phrase>level set</phrase> methods, which <phrase>takes into account</phrase> coupling as an important constraint. By evolving two embedded surfaces simultaneously, each driven by its own image-derived information while maintaining the coupling , we capture a representation of the two bounding surfaces and achieve automatic segmentation on the layer. Characteristic gray level values, instead of image gradient information alone, are incorporated i n deriving the useful image information to drive the surface propagation, which enables our approach to capture the homogeneity inside the layer. The <phrase>level set</phrase> implementation ooers the advantage of easy initializa-tion, computational eeciency and the ability to capture deep folds of the sulci. As a test example, we apply our approach to unedited 3D Magnetic Reso-nance(MR) <phrase>brain images</phrase>. Our algorithm automatically isolates the brain from non-<phrase>brain structures</phrase> and recovers the cortical <phrase>gray matter</phrase>.
Hybrid parallel tempering and <phrase>simulated annealing</phrase> method In this paper, we propose a new hybrid scheme of parallel tempering and <phrase>simulated annealing</phrase> (<phrase>hybrid PT/SA</phrase>). Within the hybrid PT/SA scheme, a composite system with multiple conformations is evolving in parallel on a temperature ladder with various transition step sizes. The <phrase>simulated annealing</phrase> (SA) process uses a cooling scheme to decrease the temperature values in the temperature ladder to the target temperature. The parallel tempering (PT) scheme is employed to reduce the equilibration relaxation time of the composite system at a particular temperature ladder configuration in the SA process. The hybrid PT/SA method reduces the waiting time in deep <phrase>local minima</phrase> and thus leads to a more efficient sampling capability on <phrase>high-dimensional</phrase> complicated <phrase>objective function</phrase> landscapes. Compared to the approaches PT and parallel SA with the same temperature ladder, transition step sizes, and cooling scheme (parallel SA) configurations, our <phrase>preliminary results</phrase> obtained with the hybrid PT/SA method confirm the expected improvements in simulations of several test objective functions, including the Rosenbrock's function and the ''rug-<phrase>ged</phrase> " funnel-like function, and several instantiations of the traveling <phrase>salesman problem</phrase>. The hybrid PT/SA may have slower convergence than <phrase>genetic algorithms</phrase> (GA) with good cross-over heuristics, but it has the advantage of tolerating ''bad " initial values and displaying robust sampling capability, even in the absence of <phrase>additional information</phrase>. Moreover, the hybrid PT/SA has natural parallelization potential. <phrase>Markov chain Monte Carlo</phrase> (MCMC) methods have long been recognized as effective tools in difficult statistical sampling and <phrase>global optimization</phrase> problems arising from a wide range of applications, including physics, biology, medical science, <phrase>chemistry</phrase>, <phrase>material science</phrase>, <phrase>computer science</phrase>, and <phrase>economics</phrase>. In the original <phrase>Metropolis</phrase>–<phrase>Hastings</phrase>-based MCMC [1,2], a <phrase>Markov process</phrase> is built to sample a target <phrase>probability distribution</phrase>: pðxÞ ¼ Z À1 e ÀEðxÞ ; where E(x) refers to the <phrase>objective function</phrase> and Z denotes the normalization constant (partition function). In this scheme, a new state y is generated from the <phrase>current state</phrase> x of the <phrase>Markov process</phrase> by drawing y from a proposal transition function q(x, y). The new state y is accepted with the probability min(1, r), where r is the <phrase>Metropolis</phrase>–<phrase>Hastings</phrase> ratio: 0096-3003/$-see front matter
A Deep-Cutting-Plane Technique for Reverse <phrase>Convex Optimization</phrase> A large number of problems in engineering design and in many areas of social and physical sciences and technology lend themselves to particular instances of problems studied in this paper. Cutting-plane methods have traditionally been used as an effective tool in devising exact algorithms for solving convex and <phrase>large-scale</phrase> <phrase>combinatorial optimization</phrase> problems. Its utilization in nonconvex optimization has been also promising. A cutting plane, essentially a <phrase>hyperplane</phrase> defined by a linear inequality, can be used to effectively reduce the computational efforts in search of a global solution. Each cut is generated in order to eliminate a large portion of the search domain. Thus, a deep cut is intuitively superior in which it will exclude a larger set of extraneous points from consideration. This paper is concerned with the development of deep-cutting-plane techniques applied to reverse-convex programs. An upper bound and a <phrase>lower bound</phrase> for the optimal value are found, updated, and improved at each iteration. The algorithm terminates when the two bounds collapse or all the generated subdivisions have been fathomed. Finally, computational considerations and numerical results on a set of <phrase>test problems</phrase> are discussed. An illustrative example, walking through the steps of the algorithm and explaining the computational process, is presented.
Title of Thesis: Shape and Reflectance Recovery from Indoor <phrase>Image Sequences</phrase> Shape and Reflectance Recovery from Indoor <phrase>Image Sequences</phrase> The author reserves all other publication and other rights in association with the copyright in the thesis, and except as herein before provided, neither the thesis nor any substantial portion thereof may be printed or otherwise reproduced in any material form whatever without the author's prior written permission. The undersigned certify that they have read, and recommend to the Faculty of Graduate Studies and Research for acceptance, a thesis entitled Shape and Reflectance Recovery from Indoor <phrase>Image Sequences</phrase> submitted by Neil Aylon Charles <phrase>Birkbeck</phrase> in partial fulfillment of the requirements for the degree of Master of Science. Abstract Several successful methods for shape reconstruction from <phrase>image sequences</phrase> have been developed using a variational formulation. In this work we utilize the same framework, which leads to a <phrase>Partial Differential Equation</phrase> (<phrase>PDE</phrase>) describing the motion of an initial surface to a refined surface that is a better match to the input images. Motivated by the primary goals of reconstructing the object and the parameters to a reflectance model, we take advantage of known <phrase>lighting conditions</phrase> in the error measure. In particular, we assume that there is light variation, due to object rotation relative to the light source, allowing the recovery of shape in both <phrase>textured</phrase> and textureless regions. Additionally, we propose a method to filter out <phrase>specular</phrase> highlights, which allows the recovery of surfaces having non-Lambertian reflectance. Following recent work using an explicit surface parameterization, we apply the <phrase>PDE</phrase> refinement to a deformable mesh. We test our method using images obtained from an easy to use capture setup. The setup is accessible to the average PC user, because the only hardware requirements are a camera, a light source, and a glossy white sphere. The capture setup provides images, camera calibration, light calibration, and <phrase>silhouette</phrase> images to be used in the refinement. The visual <phrase>hull</phrase> is used as a <phrase>starting point</phrase> for the <phrase>PDE</phrase> evolution. At the end of the refinement, our method outputs a triangulated mesh and the parameters of a Phong reflectance model represented in texture space. Results on real and synthetic images demonstrate that this method is capable of recovering the geometry of textureless surfaces, and moderately <phrase>textured</phrase> surfaces, but is unstable in the recovery of deep concavities. Several examples on real sequences illustrate the applicability of our models in <phrase>computer graphics</phrase> applications, where the recovered objects are composed and rendered under novel lighting and view conditions.
A Hybrid <phrase>Input-Output</phrase> Approach to Model Metabolic Systems: An Application to Intracellular <phrase>Thiamine</phrase> Kinetics Models of the dynamics of complex metabolic systems offer potential benefits to the deep comprehension of the system under study as well as for the performance of certain tasks. Unfortunately, dynamic modeling of a great deal of metabolic systems may be problematic due to the incompleteness of the available knowledge about the underlying mechanisms and to the lack of an adequate observational <phrase>data set</phrase>. In theory, a valid alternative to classical structural modeling through <phrase>ordinary differential equations</phrase> could be represented by <phrase>input-output</phrase> approaches. But, in practice, such methods, which learn the nonlinear dynamics of the system from <phrase>input-output</phrase> data, fail when the experimental <phrase>data set</phrase> is poor either in size or in quality. Such a situation is not rare in the case of metabolic systems. This paper deals with a hybrid approach which aims at overcoming the problems addressed above. More specifically, it allows us to solve the identification problems of the intracellular <phrase>thiamine</phrase> kinetics in the intestine tissue. The method, which is half way between the structural and <phrase>input-output</phrase> approach, uses the outcomes of the simulation of a qualitative structural model to build a good initialization of a fuzzy system identifier. Such an initialization allows us to efficiently cope with both the incompleteness of knowledge and the inadequacy of the available <phrase>data set</phrase>, and to derive an <phrase>input-output</phrase> model of the intracellular <phrase>thiamine</phrase> kinetics in the intestine tissue. The comparison of the predictions of the intracellular <phrase>thiamine</phrase> kinetics obtained by the application of such a model with those obtained by <phrase>traditional approaches</phrase>, namely compartmental models, <phrase>neural networks</phrase>, and fuzzy systems, highlighted a better performance of our model. As the structural assumptions are relaxed, we obtained a model slightly less informative than a purely structural one but robust enough to be used as a simulator. The paper also discusses the interpretative potential offered by such a model, as tested on <phrase>diabetic</phrase> subjects.
Multimodal Emotion Recognition Using Multimodal <phrase>Deep Learning</phrase> To enhance the performance of affective models and reduce the cost of acquiring physiological signals for <phrase>real-world applications</phrase>, we adopt mul-timodal <phrase>deep learning</phrase> approach to construct af-fective models from multiple physiological signals. For unimodal enhancement task, we indicate that the best recognition accuracy of 82.11% on SEED dataset is achieved with shared representations generated by Deep AutoEncoder (DAE) model. For multimodal facilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE) achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets, respectively , which are much superior to the state-of-the-art approaches. For cross-modal learning task, our <phrase>experimental results demonstrate</phrase> that the mean accuracy of 66.34% is achieved on SEED dataset through shared representations generated by EEG-based DAE as <phrase>training samples</phrase> and shared representations generated by eye-based DAE as testing sample, and vice versa.
Robust <phrase>test generation</phrase> for <phrase>power supply noise</phrase> induced <phrase>path delay</phrase> faults In <phrase>deep sub-micron</phrase> designs, the delay caused by <phrase>power supply noise</phrase> (<phrase>PSN</phrase>) can no longer be ignored. A <phrase>PSN</phrase>-induced <phrase>path delay fault</phrase> (PSNPDF) model is proposed in this paper, and should be tested to enhance chip quality. Based on precise <phrase>timing analysis</phrase>, we also propose a robust <phrase>test generation</phrase> technique for PSNPDF. Concept of timing window is introduced into the PSNPDF model. If two devices in the same feed region simultaneously switch in the same direction, the current waveform of the two devices will have an overlap and excessive <phrase>PSN</phrase> will be produced. Experimental results on ISCAS'89 circuits showed <phrase>test generation</phrase> can be finished in a few seconds.
Far Field Extrapolation from Near Field Interactions and Shielding Influence Investigations Based on a FE-PEEC Coupling Method Regarding standards, it is well established that common mode currents are the main source of far field emitted by <phrase>variable frequency drive</phrase> (VFD)-cable-motor associations. These currents are generated by the combination of floating potentials with stray capacitances between these floating potential tracks and the mechanical parts connected to the earth (the heatsink or cables are usual examples). Nowadays, due to frequency and power increases, the systematic compliance to <phrase>EMC</phrase> (<phrase>ElectroMagnetic Compatibility</phrase>) becomes increasingly difficult and costly for industrials. As a consequence, there is a well-identified need to investigate practical and <phrase>low cost</phrase> solutions to reduce the radiated fields of VFD-cable-motor associations. A well-adapted solution is the shielding of wound components well known as the major source of near <phrase>magnetic field</phrase>. However, this solution is not convenient, it is expensive and may not be efficient regarding far field reduction. Optimizing the components placement could be a better and cheaper solution. As a consequence, dedicated tools have to be developed to efficiently investigate not easy comprehendible phenomena and finally to control <phrase>EMC</phrase> disturbances using component placement, layout geometry, shielding design if needed. However, none of the modeling 81 methods usually used in industry complies with large <phrase>frequency range</phrase> and far field models including magnetic materials, multilayer <phrase>PCBs</phrase>, and shielding. The contribution of this paper is to show that alternatives regarding modeling solutions exist and can be used to get in-deep analysis of such complex structures. It is shown in this paper that near field investigations can give information on far field behavior. It is illustrated by an investigation of near field interactions and shielding influence using a FE-PEEC hybrid method. The <phrase>test case</phrase> combining a common mode filter with the floating potentials tracks of an inverter is based on an industrial and commercialized VFD. The near field interactions between the common mode inductance and the tracks with floating potentials are revealed. Then, the influence of the common mode inductance shielding is analyzed.
Spider Search: An Efficient and Non-Frontier-Based Real-Time <phrase>Search Algorithm</phrase> Real-time <phrase>search algorithms</phrase> are limited to constant-bounded search at each time step. We do not see much difference between standard <phrase>search algorithms</phrase> and good real-time <phrase>search algorithms</phrase> when <phrase>problem sizes</phrase> are small. However , having a good real-time <phrase>search algorithm</phrase> becomes important when <phrase>problem sizes</phrase> are large. In this paper we introduce a simple yet efficient algorithm, Spider Search, which uses very low constant time and space to solve problems when agents need deep (but not exhaustive) path analysis at each step. Moreover, we did some <phrase>experimental tests</phrase> to compare Spider search with other searches. We expect that Spider search is the first in a new class of <phrase>tree-based</phrase> rather than frontier-based <phrase>search algorithms</phrase>.
Using Computers to Describe Style <phrase>uman</phrase> beings have a <phrase>poorly understood</phrase> ability to recognize patterns in their environment. Sometimes this ability i s vital to Hsur vival. Sometimes it i s the enabling ability that mediates the creation and maintenance of a <phrase>culture</phrase>. Thus we have art. But always this ability to organize the visual world of art remains largely a <phrase>mystery</phrase> that motivates our study both of art making and art looking. Despite the mysterious nature of art, people have always attempted to understand both the process and the product of art making. Often they are remarkably successful. They have communicated this understanding in several ways. Volumes have been written in the attempt to explain an art that has become accessible to a <phrase>scholar</phrase> as a consequence of deep and prolonged study. Others have expressed their understanding by practicing an art in an established style. This includes the special case of forgers who have demonstated an understanding adequate to fool a-skeptical audience. form i s communicated in such a way as to make i t difficult for others to build upon the knowledge that has been acquired. Even in those cases where scholarly understanding i s best communicated, it seldom can be used with the ease that ordinary scientific results can be used. i s inherently not communicable in the sense that scientific results are. We do not believe this. Our attempt here i s to show that, at least in one aspect, an understanding of art can be communicated in just the way that scientific results are: it can be tested, validated, and used. The aspect that concerns u s i s style (Schapiro 1953). When we are confronted with a homogeneous collection of art works, the first things that we observe are the formal properties: shape, color, <phrase>arrangement</phrase>, In all these cases, however, the achievement of understanding an art In defense of this limiting communication, i t i s often argued that art
On the <phrase>analysis tools</phrase> of traffic fractality — The deep changes recently undergone by <phrase>network traffic</phrase> have strongly impacted its statistical characterization. The traditional assumptions on traffic characteristics have been proven to be obsolete and a new formalism has been brought about to properly take into account traffic new properties. Accordingly, newly designed <phrase>analysis tools</phrase> have been proposed. Our work is focused on the presentation and the test of these tools of analysis, believing that we have first of all to be confident on the methods used to properly characterize <phrase>network traffic</phrase>.
Deterministic built-in self-test using split linear feedback shift register reseeding for <phrase>low-power</phrase> testing A new <phrase>low-power</phrase> testing methodology to reduce the excessive <phrase>power dissipation</phrase> associated with <phrase>scan-based designs</phrase> in the deterministic <phrase>test pattern</phrase> generated by linear feedback <phrase>shift registers</phrase> (LFSRs) in built-in self-test is proposed. This new method utilises two split LFSRs to reduce the amount of the <phrase>switching activity</phrase>. The original test cubes are partitioned into zero-set and one-set cubes according to specified bits in the test cubes, and the split LFSR generates a zero-set or one-set <phrase>cube</phrase> in the given test <phrase>cube</phrase>. In cases where the current scan shifting value is a do not care bit <phrase>accounting</phrase> for the output values of the LFSRs, the last value shifted into the <phrase>scan chain</phrase> is repeatedly shifted into the <phrase>scan chain</phrase> and no transition is produced. <phrase>Experimental results</phrase> for the largest ISCAS'89 <phrase>benchmark circuits</phrase> show that the proposed scheme can reduce the <phrase>switching activity</phrase> by 50% with little <phrase>hardware overhead</phrase> compared with previous schemes. 1 Introduction Highly developed <phrase>deep sub-micron technology</phrase> has enabled the implementation of a large system-on-a-chip (SoC). The large <phrase>SoC design</phrase> includes several <phrase>intellectual property</phrase> (IP) cores such as processor cores, <phrase>embedded memories</phrase> and other peripheral cores. Traditional <phrase>test methods</phrase> using the <phrase>automatic test equipment</phrase> (ATE) have become unsuitable for testing of these large SoCs. This is because the more <phrase>IP cores</phrase> are used in one SoC, the larger <phrase>test data</phrase> volumes are required. Therefore in order to apply this large volume of test patterns to the SoC, the ATE requires large memory and this increases the test cost of the SoC. Also, since the number of external I/O pins of the SoC and the number of ATE channels are limited, the SoC testing is very time consuming. Built-in self-test (BIST) is widely known as a good solution for testing the individual <phrase>IP cores</phrase> [1 – 6]. As a test <phrase>pattern generator</phrase> of BIST, an LFSR (linear feedback shift register) is widely adopted to generate a <phrase>pseudo-random</phrase> <phrase>test pattern</phrase>. However, in cases that produce many random pattern resistant (<phrase>RPR</phrase>) faults in the circuit under test, it is very difficult to get high <phrase>fault coverage</phrase> with a <phrase>pseudo-random</phrase> <phrase>test pattern</phrase>. Several attractive solutions for this problem have been proposed [7 – 21]. The weighted random test was presented to reduce the test set size and to guarantee high <phrase>fault coverage</phrase> [7 – 9]. The weighted random test adds the additional hardware necessary to change the probability of the logic value 1 …
Comparison of GLR and invariant detectors under structured clutter covariance This paper addresses a target detection problem in <phrase>radar imaging</phrase> for which the <phrase>covariance matrix</phrase> of unknown Gaussian clutter has block diagonal structure. This block diagonal structure is the consequence of a target lying along a boundary between two statistically independent clutter regions. Here, we design adaptive detection algorithms using both the generalized <phrase>likelihood ratio</phrase> (GLR) and the invariance principles. There has been considerable interest in applying invariant <phrase>hypothesis testing</phrase> as an alternative to the GLR test. This interest has been motivated by several attractive properties of invariant tests including: exact robustness to variation of nuisance parameters and possible <phrase>finite-sample</phrase> min-max optimality. However, in our deep-hide target detection problem, there are regimes for which neither the GLR nor the invariant tests uniformly outperforms the other. We discuss the relative advantages of GLR and invariance procedures in the context of this <phrase>radar imaging</phrase> and target detection application.
MetaCluster: unsupervised binning of environmental genomic fragments and taxonomic annotation Limited by the laboratory technique, traditional <phrase>microorganism</phrase> research usually focuses on one single individual species. This significantly limits the deep analysis of intricate biological processes among complex <phrase>microorganism</phrase> communities. With the rapid development of <phrase>genome sequencing</phrase> techniques, the traditional <phrase>research methods</phrase> of <phrase>microorganisms</phrase> based on the isolation and cultivation are gradually replaced by <phrase>metagenomics</phrase>, also known as environmental <phrase>genomics</phrase>. The first step, which is also the <phrase>major bottleneck</phrase> of metagenomic <phrase>data analysis</phrase>, is the identification and taxonomic characterization of the DNA fragments (reads) resulting from sequencing a sample of mixed species. This step is usually referred as "binning".  Existing binning methods based on sequence similarity and sequence composition markers rely heavily on the reference <phrase>genomes</phrase> of known <phrase>microorganisms</phrase> and <phrase>phylogenetic</phrase> markers. Due to the limited availability of reference <phrase>genomes</phrase> and the bias and unstableness of markers, these methods may not be applicable in all cases. Not much unsupervised binning methods are reported, but the unsupervised nature of these methods makes them extremely difficult to annotate the clusters with taxonomic labels. In this paper, we present MetaCluster 2.0, an unsupervised binning method which could bin metagenomic sequencing datasets with <phrase>high accuracy</phrase>, and also identify unknown <phrase>genomes</phrase> and annotate them with proper taxonomic labels. The running time of MetaCluster 2.0 is at least 30 times faster than existing binning algorithms.  MetaCluster 2.0, and all the test datasets mentioned in this paper are available at http://i.cs.hku.hk/~alse/MetaCluster/.
Modular SRAM-Based Binary Content-Addressable Memories —Binary Content Addressable Memories (BCAMs), also known as associative memories, are <phrase>hardware-based</phrase> <phrase>search engines</phrase>. BCAMs employ a massively parallel exhaustive search of the entire memory space, and are capable of matching a specific data within a single cycle. Networking, <phrase>memory management</phrase>, <phrase>pattern matching</phrase>, <phrase>data compression</phrase>, DSP, and other applications utilize <phrase>CAMs</phrase> as single-cycle associative search <phrase>accelerators</phrase>. Due to the increasing amount of processed information, modern BCAM applications demand a deep searching space. However, traditional BCAM approaches in FPGAs suffer from storage inefficiency. In this paper, a novel, efficient and modular technique for constructing BCAMs out of standard SRAM blocks in FPGAs is proposed. Hierarchical search is employed to achieve high storage efficiency. Previous hierarchical search approaches cannot be cascaded since they provide a single matching address; this incurs an exponential increase of <phrase>RAM</phrase> consumption as pattern width increases. Our approach, however, efficiently regenerates a match indicator for every single address by storing indirect indices for address match indicators. Hence, the proposed method can be cascaded and <phrase>exponential growth</phrase> is alleviated into linear. Our method exhibits high storage efficiency and is capable of implementing up to 9 times wider BCAMs compared to other approaches. A fully parameterized Verilog implementation is being released as an <phrase>open source</phrase> library. The library has been extensively tested using Altera's Quartus and ModelSim.
Hardware <phrase>Trojan</phrase> <phrase>horse</phrase> detection using <phrase>gate-level</phrase> characterization Hardware <phrase>Trojan</phrase> <phrase>horses</phrase> (HTHs) are the malicious altering of hardware specification or implementation in such a way that its functionality is altered under a set of conditions defined by the attacker. There are numerous HTHs sources including <phrase>untrusted</phrase> foundries, synthesis tools and libraries, testing and verification tools, and configuration scripts. HTH attacks can greatly comprise <phrase>security and privacy</phrase> of hardware users either directly or through interaction with pertinent systems and <phrase>application software</phrase> or with data. However, while there has been a huge <phrase>research and development</phrase> effort for detecting software <phrase>Trojan</phrase> <phrase>horses</phrase>, surprisingly, HTHs are rarely addressed. HTH detection is a particularly <phrase>difficult task</phrase> in modern and pending <phrase>deep submicron technologies</phrase> due to intrinsic manufacturing variability.  Our goal is to provide an impetus for HTH research by creating a generic and easily applicable set of techniques and tools for HTH detection. We start by introducing a technique for recovery of characteristics of gates in terms of <phrase>leakage current</phrase>, switching power, and delay, which utilizes <phrase>linear programming</phrase> to solve a system of equations created using non-destructive measurements of power or delays. This technique is combined with constraint manipulation techniques to detect embedded HTHs. The effectiveness of the approach is demonstrated on a number of standard benchmarks.
Ambulatory Monitoring of Activities and <phrase>Motor Symptoms</phrase> in <phrase>Parkinson's Disease</phrase> Ambulatory monitoring of <phrase>motor symptoms</phrase> in Parkinsons disease (PD) can improve our therapeutic strategies, especially in patients with motor fluctuations. <phrase>Previously published</phrase> monitors usually assess only one or a few basic aspects of the <phrase>cardinal</phrase> <phrase>motor symptoms</phrase> in a laboratory setting. We developed a novel ambulatory monitoring system that provides a complete motor assessment by simultaneously analyzing current motor activity of the patient (e.g. sitting, walking) and the severity of many aspects related to tremor, bradykinesia, and hypokinesia. The monitor consists of a set of four <phrase>inertial sensors</phrase>. Validity of our monitor was established in seven healthy controls and six <phrase>PD patients</phrase> treated with <phrase>deep brain stimulation</phrase> (DBS) of the subthalamic nucleus. Patients were tested at three different levels of DBS treatment. Subjects were monitored while performing different tasks, including motor tests of the Unified Parkinsons Disease Rating Scale (UPDRS). Output of the monitor was compared to simultaneously recorded videos. The monitor proved very accurate in discriminating between several motor activities. Monitor output correlated well with blinded UPDRS ratings during different DBS levels. The combined analysis of motor activity and symptom severity by our PD monitor brings true ambulatory monitoring of a wide variety of <phrase>motor symptoms</phrase> one step closer..
Anthro Arm: the Design of a Seven <phrase>Degree of Freedom</phrase> Arm with Human Attributes Anthro Arm: the Design of a Seven <phrase>Degree of Freedom</phrase> Arm with Human Attributes The author hereby grants to MIT permission to reproduce and to distribute publicly paper and electronic copies of this thesis document in whole or in part in any medium now known or hereafter created. ABSTRACT Studying biological systems has given robotics researchers valuable insight into designing complex systems. This thesis explores one such application of a <phrase>biomimetic</phrase> robotic system designed around a human arm. The design of an <phrase>anthropomorphic</phrase> arm, an arm that is similar to that of a human's, requires deep insight into the <phrase>kinematics</phrase> and <phrase>physiology</phrase> of the biological system. Investigated here is the design and completion of an arm with 7 degrees of freedom and human-like range of motion in each joint. The comparison of actuation schemes and the determination of proper <phrase>kinematics</phrase> enable the arm to be built at a low cost while maintaining <phrase>high performance</phrase> and similarity to the biological analog. Complex parts are built by dividing structures into interlocking 2d shapes that can easily be cut out using a waterjet and then welded together with high reliability. The resulting arm will become part of a bionic system when combined with an existing bionic hand platform that is being developed in the Intelligent Machines Laboratory at MIT. With a well thought out <phrase>modular design</phrase>, the system will be used as a <phrase>test bed</phrase> for <phrase>future research</phrase> involving data simplification and neurological control. The completion of the <phrase>anthropomorphic</phrase> arm reveals that is indeed feasible to use simple DC motors and quick fabrication techniques. The final result is a reliable, modularized, and <phrase>anthropomorphic</phrase> arm.
<phrase>Voltage-drop</phrase>-constrained optimization of <phrase>power distribution</phrase> network based on reliable maximum current estimates – The problem of optimum design of tree-shaped <phrase>power distribution</phrase> networks with respect to the <phrase>voltage drop</phrase> effect is addressed in this paper. An approach for the width adjustment of the power lines supplying the circuit's major functional blocks is formulated, so that the network occupies the minimum possible area under specific <phrase>voltage drop</phrase> constraints at all blocks. The optimization approach is based on precise maximum current estimates derived by statistical means from recent advances in the field of extreme value theory. <phrase>Experimental tests</phrase> include the design of <phrase>power grid</phrase> for a choice of different <phrase>topologies</phrase> and <phrase>voltage drop</phrase> tolerances in a typical benchmark circuit. I. INTRODUCTION The massive <phrase>power distribution</phrase> networks of modern <phrase>deep-submicron VLSI</phrase> circuits are particularly susceptible to a number of reliability problems, the biggest one of which is the well-known <phrase>voltage drop</phrase> or IR-drop problem [1]-[2]. This effect characterizes the lowering of the effective voltage level supplied on the active devices of the circuit due to the finite resistance of the power and ground wires, and can have an adverse effect on circuit speed and <phrase>noise margins</phrase> (Fig.1), degrading performance (at best) or causing faulty logic signals and circuit malfunction [3]. Traditionally, in order to avoid significant drop of the voltage level the <phrase>power distribution</phrase> lines have been made excessively wide to reduce their resistance. However, the <phrase>voltage drop</phrase> problem becomes even more pronounced with each new generation of <phrase>integrated circuits</phrase>, as the increase in the number of devices and <phrase>operating frequency</phrase> combined with the decrease in feature size induce an increase in the currents and the effective resistance respectively, while the decrease in the <phrase>supply voltage</phrase> lowers its acceptable drop along the <phrase>power distribution</phrase> lines. Therefore simply increasing line width without any restriction cannot be maintained since this would result in a significant occupation of valuable <phrase>silicon area</phrase> (which must also accommodate the interconnection lines of the circuit). A method for designing power networks to satisfy certain constraints on IR-drop while occupying the minimum possible <phrase>silicon area</phrase> is required, and this became an important research direction lately. A good deal of work has been done previously for this purpose [4]-[9]. Yet, such a design method necessitates the knowledge of the maximum possible currents flowing at any time through the lines of the network, which in all of <phrase>previous works</phrase> were considered as given. However, their specific value is hard to obtain since the instantaneous current …
Preservation of microelectrode recordings with non-<phrase>GABAergic</phrase> drugs during <phrase>deep brain</phrase> stimulator placement in children. OBJECT <phrase>Deep brain stimulation</phrase> (DBS) has become accepted therapy for intractable <phrase>dystonia</phrase> and other movement disorders. The accurate placement of DBS electrodes into the <phrase>globus</phrase> pallidus internus is assisted by unimpaired microelectrode recordings (MERs). Many <phrase>anesthetic</phrase> and <phrase>sedative</phrase> drugs interfere with MERs, requiring the patient to be awake for target localization and neurological testing during the procedure. In this study, a novel <phrase>anesthetic</phrase> technique was investigated in pediatric DBS to preserve MERs.   METHODS In this paper, the authors describe a <phrase>sedative</phrase>/<phrase>anesthetic</phrase> technique using <phrase>ketamine</phrase>, remifentanil, dexmedetomidine, and nicardipine in 6 pediatric patients, in whom the avoidance of <phrase>GABAergic</phrase> stimulating drugs permitted excellent surgical conditions with no detrimental effects on intraoperative MERs. The quality of the MERs, and the frequency of its use in making electrode placement decisions, was reviewed.   RESULTS All 6 patients had good-quality MERs. The data were of sufficient quality to make a total of 9 trajectory adjustments.   CONCLUSIONS Microelectrode recordings in pediatric DBS can be preserved with a combination of dexmedetomidine and <phrase>ketamine</phrase>, remifentanil, and nicardipine. This preservation of MERs is particularly crucial in electrode placement in children.
Something of a <phrase>Potemkin</phrase> <phrase>Village</phrase>? Acid2 and Mozilla's Efforts to Comply with HTML4 The real point here is that the Acid3 test isn't a broad-spectrum standards-support test. It's a showpiece, and something of a <phrase>Potemkin</phrase> <phrase>village</phrase> at that. Which is a shame, because what's really needed right now is exhaustive <phrase>test suites</phrase> for specifications— <phrase>XHTML</phrase>, <phrase>CSS</phrase>, DOM, SVG.[2] Acid3 is the third of three benchmark tests that have been devised to challenge <phrase>browsers</phrase> to comply with Internet standards [6]. While <phrase>Firefox</phrase> developers at <phrase>Mozilla</phrase> had fully embraced the predecessor to Acid3, Acid2, they showed themselves much more reticent this time around. As the quote above indicates they had come to feel that Acid3 would divert attention from the real issues and might actually make it more difficult to achieve " deep compliance " as developers would scramble to come up with quick fixes just to pass the benchmark test. But were these fears justified? To find out, we retrieved the bug reports for bugs in Mozilla's <phrase>Bugzilla</phrase> bug tracker concerning compliance with the HTML4 standard and tried to analyze the differences in the process of bug resolution between bugs that were linked to Acid2 and bugs that were not. In <phrase>Bugzilla</phrase>, the bug resolution process passes a number of well-defined stages. Based on the transition rates that we observe we conclude that the process of bug resolution is markedly different for bugs associated with Acid2. In particular, bug resolution appears to be much more chaotic in case of Acid2. This might be symptomatic for " scrambling " , which would explain why developers were not so keen to repeat the experience when Acid3 came around. Further investigations, however, are needed to corroborate this hypothesis. Bugs reports in <phrase>Bugzilla</phrase> are often part of Bug Report Networks [3]. That is, they are part of a network of dependencies as bugs can be declared to depend on, block, or duplicate other bugs. Note that the dependencies between bugs are not always purely technical. In fact, an important type of bugs in <phrase>Bugzilla</phrase> is the " meta-bug " , also known as the " tracker bug " , which is a bug at the root of a <phrase>dependency tree</phrase> whose <phrase>leafs</phrase> are bugs that are related to the issue that the meta-bug is trying to address. For instance, meta-bug 7954 is the bug that tracks issues related to the implementation of the HTML4 standard and the meta-bug 289480 tracks the issues related to Acid2. For our investigation …
Deep Speech: Scaling up <phrase>end-to-end</phrase> <phrase>speech recognition</phrase> We present a state-of-the-art speech <phrase>recognition system</phrase> developed using <phrase>end-to-end</phrase> <phrase>deep learning</phrase>. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, <phrase>reverberation</phrase>, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a " phoneme. " Key to our approach is a well-optimized RNN training system that uses multiple <phrase>GPUs</phrase>, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms <phrase>previously published</phrase> results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full <phrase>test set</phrase>. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.
Interacting with the flow <phrase>Mobile devices</phrase> offer challenges for UI design. Limited screen space leads to deep menus, complex navigation and loss of position. We introduce a new <phrase>user interface</phrase> concept that reverses the traditional navigation paradigm. By utilizing <phrase>context awareness</phrase> and allowing the user to control the UI via filters, objects of interest navigate past the user instead of the user navigating to the object. The user operates on a <phrase>single view</phrase> without the need for deep menu navigation. The new UI is also easy to configure. We implemented the concept on the <phrase>Nokia</phrase> <phrase>S60</phrase> 5th edition touch platform and conducted user testing with 16 users. Initially, users felt confused because of new ways of accessing things. However, after a short period of usage, majority of the users found it easy to use. Most of the users felt the system to be fun and playful.
Lattice basis reduction: Improved practical algorithms and solving subset sum problems We report on improved practical algorithms for lattice basis reduction. We propose a practical oating point v e r s i o n o f t h e L 3 {algorithm of Lenstra, Lenstra, Lovv asz (1982). We present a v ariant o f t h e L 3 { algorithm with \deep insertions" and a practical algorithm for block Korkin{Zolotarev reduction, a concept introduced by S c hnorr (1987). Empirical tests show that the strongest of these algorithms solves almost all subset sum problems with up to 66 random weights of arbitrary bit length within at most a few hours on a <phrase>UNISYS</phrase> 6000/70 or within a couple of minutes on a <phrase>SPARC</phrase> 1+ computer.
<phrase>Evolutionary Computation</phrase> Approach for <phrase>Shape from Shading</phrase> A new approach for recovering a 3D shape of an object from a shaded image using a genetic algorithm (GA) is proposed. The 3D-shape is recovered through the analysis of the gray levels in a single image of the scene. This problem is ill-posed except if some additional assumptions are made. In the proposed method, <phrase>shape from shading</phrase> is addressed as an energy minimization problem. The traditional deterministic approach provides efficient algorithms to solve this problem in terms of time but reaches its limits since the energy associated with <phrase>shape from shading</phrase> can contain multiple deep <phrase>local minima</phrase>. <phrase>Genetic Algorithm</phrase> is used as an alternative approach which is efficient at exploring the entire <phrase>search space</phrase>. The Algorithm is tested in both synthetic and real image and is found to perform accurate and efficient results.. 1. INTRODUCTION Surface shape recovery of observed objects is one of the main goals in the field of three-dimensional (3-D) <phrase>computer vision</phrase>. <phrase>Marr</phrase> identified <phrase>shape-from shading</phrase> (<phrase>SFS</phrase>) as providing one of the key routes to understanding 3D surface structure via the 2½D sketch [1]. The process has been an active area of research for over two decades. It is concerned with recovering 3D surface <phrase>shape from shading</phrase> patterns. The subject has been tackled in a variety of ways since the pioneering work of <phrase>Horn</phrase> and his coworker's in the 1970s [2]. They are classified into direct and indirect methods. Direct or active methods recover depth information directly using range finder and structure light. Indirect methods determine the relative depth by cues extracted from gray level images of observed object. This class contains <phrase>shape from shading</phrase>. Since the direct or active techniques usually involve many complex external component setup, they may not be suitable for instance in object shape estimation. Hence many researchers have focused on the latter class of method. The <phrase>shape from shading</phrase> methods make use of the change in image brightness of an object to determine the relative surface depth and can be effectively applied to smooth surface regions. Most of the shading techniques employ variational approaches which impose constraints such as <phrase>smoothness</phrase>, imaging equation and light conditions. As a result these techniques can only be applied to the object surface that satisfies the constraints. Also the <phrase>computational complexity</phrase> of the resulting algorithms is high and increases as the number of these constraints increases. Useful alternatives for variational shading are local shading and linear …
IC Outlier Identification Using Multiple Test Metrics WITH ADVANCES in <phrase>semiconductor manufacturing</phrase> processes, ICs become faster, more powerful, and more densely packed. This in turn makes testing difficult, especially for <phrase>deep-submicron</phrase> chips (http://public.itrs.net). Engineers use parametric tests to evaluate different IC parameters like speed or <phrase>leakage current</phrase> (<phrase>I DDQ</phrase>). If a parameter is outside a predetermined limit or threshold, the chip is considered defective. For example, a chip can operate too slowly (a delay failure) or consume excessive <phrase>I DDQ</phrase>. Engineers usually determine the <phrase>pass/fail</phrase> threshold by taking a sample of chips from the initial production for characterization. Alternatively, they can build device models and determine a threshold through simulation. Chips that fail only parametric tests do not meet their specifications completely, but are functional. However, manufacturers can reject such chips for reliability reasons. With the reduction in transistor geometries, precise <phrase>process control</phrase> of manufacturing becomes more difficult. This leads to variation in transistor parameters, both within a chip and within a wafer (across chips). This results in a large spread in <phrase>fault-free</phrase> parameter values. For example, <phrase>I DDQ</phrase> increases exponentially as you scale down transistors. As a result, setting the <phrase>pass/fail</phrase> threshold becomes difficult because the overlap between <phrase>fault-free</phrase> and faulty distributions invariably results in <phrase>yield loss</phrase> and/or test escapes. If you plot a distribution of test parameters for different chips, the defective chips appear as outliers in the tail of the distribution. Screening these chips is therefore, in principle , similar to statistical <phrase>outlier rejection</phrase>. Since statistical <phrase>outlier rejection</phrase> is a well-researched topic, this opens up a wide variety of <phrase>statistical methods</phrase> for application to VLSI testing. This article presents an overview of the application of outlier identification methodology for VLSI test, using industrial <phrase>test data</phrase>. Variability is inevitable in <phrase>semiconductor manufacturing</phrase>. Variations occur because of changes in processing conditions such as temperature, implant dose, and gas <phrase>turbulence</phrase>. Transistor parameters such as effective channel length and width, and gate oxide thickness change from chip to chip. Generally, test engineers observe four levels of variations: within chip, within wafer, between wafers, and between lots, in order of increasing magnitude. These variations have a tremendous impact on test parameters. For example, <phrase>Figure 1 shows</phrase> 20 <phrase>I DDQ</phrase> measurements for each of five chips. Chips A and B are adjacent to each other on a wafer. Chip C is from a different region of the same wafer. Chip D is from a different wafer from the same lot …
On Optimizing Scan Testing Power and Routing Cost in <phrase>Scan Chain</phrase> Design With advanced VLSI manufacturing technology in <phrase>deep submicron</phrase> (DSM) regime, we can integrate entire electronic systems on a single chip (SoC). Due to the complexity in <phrase>SoC design</phrase>, circuit testability becomes one of the most challenging works. Without careful design in scan cell placement and chain ordering, circuits consume much more power in <phrase>test mode</phrase> operation than that in normal functional mode. This elevated testing power may cause problems including overall yield lost and instant circuit damage. In this paper, we present an approach to simultaneously minimizing power and routing cost in <phrase>scan chain</phrase> reordering after cell placement. We formulate the problem as a Traveling <phrase>Salesman Problem</phrase> (TSP), different cost evaluation from [3], [5], and apply an efficient heuristic to solve it. The experimental results are encouraging. Compared with a recent result in [3], which uses the approach with clustering overhead, we obtain up to 10% average <phrase>power saving</phrase> under the same low routing cost. Furthermore, we obtain 57% routing cost improvement under the same test <phrase>power consumption</phrase> in s9234, one of ISCAS'89 benchmarks. We collaborate multiple <phrase>scan chains</phrase> architecture with our methodology and obtain good results as well.
<phrase>Multi-objective</phrase> AI Planning: Evaluating DaE YAHSP on a Tunable Benchmark All standard Artifical Intelligence (AI) <phrase>planners</phrase> to-date can only handle a single objective, and the only way for them to take into account multiple objectives is by aggregation of the objectives. Furthermore , and in deep contrast with the single objective case, there exists no benchmark problems on which to test the algorithms for <phrase>multi-objective</phrase> planning. Divide-and-Evolve (DaE) is an evolutionary planner that won the (single-objective) deterministic temporal <phrase>satisficing</phrase> track in the last International Planning Competition. Even though it uses intensively the classical (and hence single-objective) planner YAHSP (Yet Another Heuristic Search Planner), it is possible to turn DaEYAHSP into a <phrase>multi-objective</phrase> evolutionary planner. A tunable benchmark suite for <phrase>multi-objective</phrase> planning is first proposed, and the performances of several variants of <phrase>multi-objective</phrase> DaEYAHSP are compared on different instances of this benchmark, hopefully paving the road to further <phrase>multi-objective</phrase> competitions in AI planning.
Processing Wearable Sensor Data to Optimize <phrase>Deep-Brain Stimulation</phrase> R ecent advances in miniature-sensor technology have enabled the development of wearable systems 1 that physicians could use to monitor motor behavior in individuals with <phrase>Parkinson's disease</phrase>. Parkin-son's disease occurs when neurons in a part of the <phrase>midbrain</phrase> referred to as <phrase>substantia nigra</phrase> pars compacta degenerate and stop producing dopamine, a <phrase>neurotransmitter</phrase> involved in motor and cognitive functions. Symptoms include tremors, bradykinesia (slowed movement), rigidity, and impaired balance. Therapy is based on augmenting or replacing dopamine using drugs that activate dopamine receptors. These therapies are often successful for some time, but patients eventually develop motor complications such as wearing off (that is, the benefits wear off within a few hours of taking the medication) and dyskinesias (involuntary and sometimes violent writhing movements). When pharmacological interventions can't sufficiently manage symptoms , stimulating the subthalamic nucleus—or <phrase>deep-brain stimulation</phrase>— can help. Physicians determine the optimal settings for <phrase>deep-brain stimulation</phrase> by clinically testing different combinations of various stimulation parameters. However, target symptoms respond to parameter changes at different times—some within seconds, others not for days or weeks. Another complication is how various medications work when combined with <phrase>deep-brain stimulation</phrase>. Choosing optimal stimulator parameters is thus very challenging. Gathering <phrase>accelerometer</phrase> data from patient-worn sensors following the adjustment of stimulation settings could be key in optimizing <phrase>deep-brain stimulation</phrase>. Here, we present the results of a pilot study we conducted to evaluate this approach. Our work is a first step toward implementing advanced strategies for optimizing clinical outcomes using systematic data capture and analysis. <phrase>Data collection</phrase> anD analysis <phrase>Deep-brain stimulation</phrase> requires quadripolar electrodes, extension cables, and an internal pulse generator. Stimulators deliver pulses through cylindrical electrode contacts that are 1.27 mm in diameter and 1.5 mm long. The relevant stimulation parameters, which the physicians control using an external <phrase>console</phrase>, are electrode <phrase>polarity</phrase> , contact location, and the pulse amplitude, duration, and frequency. Generally, the pulse amplitude ranges from 1 to 3.5 volts, the pulse width from 60 to 90 µs, and the frequency from 110 to 150 Hz. The stimulation across the quadripolar electrode can be either monopolar or bipolar. 2 Clearly, numerous combinations of stimulation parameter values exist. Our goal was to determine whether <phrase>accelerometer</phrase> data could help physicians predict clinical scores for the severity of tremor, bradykinesia, and <phrase>dyskinesia</phrase>; identify distinct movement patterns that mark transitory behaviors once stimulation has stopped; and estimate the rate of change in the • • • This is a …
<phrase>Believable</phrase> Social and Emotional Agents One of the key steps in creating quality interactive <phrase>drama</phrase> is the ability to create quality interactive characters (or <phrase>believable</phrase> agents). Two important aspects of such characters will be that they appear emotional and that they can engage in <phrase>social interactions</phrase>. My basic approach to these problems has been to use a broad agent architecture and minimal amounts of modeling of other agent in the environment. This approach is based on an understanding of the artistic nature of the problem. To enable agent-builders (artists) to create emotional agents, I provide a general framework for building emotional agents, default emotion-processing rules, and discussion about how to create quality, emotional characters. My framework gets a lot of its power from being part of a broad agent architecture. The concept is simple: the agent will be emotionally richer if there are more things to have emotions about and more ways to express them. This reliance on breadth has also meant that I have been able to create simple emotion models that rely on perception and motivation instead of deep modeling of other agents and complex cognitive processing. To enable agent builders to create social behaviors for <phrase>believable</phrase> agents, I have designed a methodology that provides heuristics for incorporating personality into social behaviors and suggests how to model other agents in the environment. I propose an approach to modeling other agents that calls for limiting the amount of modeling of other agents to that which is sufficient to create the desired behavior. Using this technique, I have been able to build robust social behaviors that use surprisingly little representation. I have used this methodology to build a number of social behaviors, like <phrase>negotiation</phrase> and making friends. I have built three simulations containing seven agents to drive and test this work. I have also conducted user studies to demonstrate that these agents appear to be emotional and can engage in non-trivial <phrase>social interactions</phrase> while also being good characters with distinct personalities.
Evaluation of residue-residue <phrase>contact prediction</phrase> in CASP10. We present the results of the assessment of the intramolecular residue-residue contact predictions from 26 prediction groups participating in the 10th round of the <phrase>CASP</phrase> experiment. The most <phrase>recently developed</phrase> direct coupling <phrase>analysis methods</phrase> did not take part in the experiment likely because they require a very deep <phrase>sequence alignment</phrase> not available for any of the 114 CASP10 targets. The performance of <phrase>contact prediction</phrase> methods was evaluated with the measures used in previous CASPs (i.e., prediction accuracy and the difference between the distribution of the predicted contacts and that of all pairs of residues in the target protein), as well as new measures, such as the Matthews <phrase>correlation coefficient</phrase>, the area under the precision-recall curve and the ranks of the first correctly and incorrectly predicted contact. We also evaluated the ability to detect interdomain contacts and tested whether the difficulty of predicting contacts depends upon the protein length and the depth of the family <phrase>sequence alignment</phrase>. The analyses were carried out on the target domains for which structural homologs did not exist or were difficult to identify. The evaluation was performed for all types of contacts (short, medium, and <phrase>long-range</phrase>), with emphasis placed on <phrase>long-range</phrase> contacts, i.e. those involving residues separated by at least 24 residues along the sequence. The assessment suggests that the best CASP10 <phrase>contact prediction</phrase> methods perform at approximately the same level, and comparably to those participating in CASP9.
A 2-d Processor Array for Massively Parallel <phrase>Image Processing</phrase> The concept of introducing <phrase>image processing</phrase> logic within the spatial gaps of an array of photodiodes is the key factor behind the presented work. A two-dimensional massively parallel <phrase>image processing</phrase> paradigm based on 8×8 pixel <phrase>neighborhood</phrase> digital processors has been designed. A <phrase>low complexity</phrase> processor array architecture along with its <phrase>instruction set</phrase> has been designed and fully verified on a FPGA platform. Various <phrase>image processing</phrase> tests have been run on the FPGA platform to demonstrate the functionality of a design that uses 12 parallel processors. The test results indicate that the architecture is scalable to support high frame rates while allowing for flexible processing due to inherent pro-grammability at a high level. The gate level <phrase>logic synthesis</phrase> results of the processor targeting a 0.13 µm <phrase>CMOS technology</phrase> indicates a low <phrase>silicon area</phrase> complexity, allowing for <phrase>image sensor</phrase> integration. iii DEDICATION This thesis is dedicated to my beloved parents Jagdish Nelliparthi and <phrase>Radha</phrase> Rani Nelliparthi. A special dedication to my maternal great-grandmother late Srimathi Durgamma Thirumareddy, my paternal great-grandfather late Sri Lak-shmiah Naidu Nelliparthi, my maternal grandfather late Sri Appa Rao Thiru-mareddy, my paternal grandfather Sri Lakshmiah Venkata Rao Nelliparthi and my sister late Kumari Swapna Rani Chirugudu. iv ACKNOWLEDGMENTS Firstly, I would like to express my deep gratitude to my adviser Dr. Sina Balkir and I am indebted for his continuous support, valuable guidance and encouragement throughout my Masters program. Also, I am indebted to my co-adviser Dr. Michael W. Hoffman for his continued guidance, support and invaluable inputs throughout my Thesis work. I would like to thank committee member Dr. Khalid Sayood for providing critical review and recommendations for my thesis. A special thanks to Mr. Nathan Schemm and Mr. Daniel J. White for their valuable assistance for this work. Also, I would like to acknowledge the support of the faculty and staff of <phrase>Electrical Engineering</phrase> Department. Finally, I am grateful to my parents, friends and well-wishers for their love, support, patience and encouragement they provided me with, in achieving my Masters degree.
Algorithm 825: A deep-cut bisection envelope algorithm for fixed points We present the BEDFix (Bisection Envelope Deep-cut <phrase>Fixed point</phrase>) algorithm for the problem of approximating a <phrase>fixed point</phrase> of a function of two variables. The function must be Lipschitz continuous with constant 1 with respect to the <phrase>infinity</phrase> norm; such functions are commonly found in <phrase>economics</phrase> and <phrase>game theory</phrase>. The computed approximation satisfies a residual criterion given a specified error tolerance. The BEDFix algorithm improves the BEFix algorithm presented in Shellman and <phrase>Sikorski</phrase> [2002] by utilizing "deep cuts," that is, eliminating additional segments of the feasible domain which cannot contain a <phrase>fixed point</phrase>. The upper bound on the number of required function evaluations is the same for BEDFix and BEFix, but our numerical tests indicate that BEDFix significantly improves the average-case performance. In addition, we show how BEDFix may be used to solve the absolute criterion <phrase>fixed point</phrase> problem with significantly better performance than the simple iteration method, when the Lipschitz constant is less than but close to 1. BEDFix is highly efficient when used to compute residual solutions for bivariate functions, having a bound on function evaluations that is twice the <phrase>logarithm</phrase> of the reciprocal of the tolerance. In the tests described in this article, the number of evaluations performed by the method averaged 31 percent of this <phrase>worst-case</phrase> bound. BEDFix works for nonsmooth continuous functions, unlike methods that require gradient information; also, it handles functions with minimum Lipschitz constants equal to 1, whereas the complexity of simple iteration approaches <phrase>infinity</phrase> as the minimum Lipschitz constant approaches 1. When BEDFix is used to compute absolute criterion solutions, the worst-case complexity depends on the <phrase>logarithm</phrase> of the reciprocal of 1-q, where q is the Lipschitz constant, as well as on the <phrase>logarithm</phrase> of the reciprocal of the tolerance.
The Deep Versus the Shallow: Effects of Co-Speech Gestures in Learning From Discourse This study concerned the role of gestures that accompany discourse in <phrase>deep learning</phrase> processes. We assumed that co-speech gestures favor the construction of a complete <phrase>mental representation</phrase> of the discourse content, and we tested the predictions that a discourse accompanied by gestures, as compared with a discourse not accompanied by gestures, should result in better recollection of conceptual information, a greater number of discourse-based inferences drawn from the information explicitly stated in the discourse, and poorer recognition of verbatim of the discourse. The results of three experiments confirmed these predictions.
A General and Multi-lingual Phrase Chunking Model Based on Masking Method Several phrase chunkers have been proposed over the past few years. Some state-of-the-art chunkers achieved better performance via integrating external resources, e.g., parsers and additional <phrase>training data</phrase>, or combining multiple learners. However, in many languages and domains, such external materials are not easily available and the combination of multiple learners will increase the cost of <phrase>training and testing</phrase>. In this paper, we propose a mask method to improve the chunking accuracy. The experimental results show that our chunker achieves better performance in comparison with other deep parsers and chunkers. For CoNLL-2000 <phrase>data set</phrase>, our system achieves 94.12 in F rate. For the base-chunking task, our system reaches 92.95 in F rate. When <phrase>porting</phrase> to Chinese, the performance of the base-chunking task is 92.36 in F rate. Also, our chunker is quite efficient. The complete chunking time of a 50K words document is about 50 seconds. 1 Introduction Automatic text chunking aims to determine non-overlap phrases structures (chunks) in a given sentence. These phrases are non-recursive, i.e., they cannot be included in other chunks [1]. Generally speaking, there are two phrase chunking tasks, including text chunking (shallow parsing) [15], and <phrase>noun phrase</phrase> (NP) chuncking [16]. The former aims to find the chunks that perform partial analysis of the syntactic structures in texts [15], while the later aims to identify the initial portions of non-recursive <phrase>noun phrase</phrase>, i.e., the first level <phrase>noun phrase</phrase> structures of the parsing trees [17] [19]. In this paper, we extend the NP chunking task to arbitrary phrase chunking, i.e., base-chunking. In comparison, shallow parsing extracts not only the first level but also the other level phrase structures of the parsing tree into the flat non-overlap chunks. Chunk information of a sentence is usually used to present syntactic relations in texts. In many <phrase>Natural Language Processing</phrase> (NLP) areas, e.g., chunking-based full parsing [1] [17] [24], clause identification [3] [19], <phrase>semantic role</phrase> labeling (SRL) [4], <phrase>text categorization</phrase> [15] and <phrase>machine translation</phrase>, the phrase structures provide downstream <phrase>syntactic features</phrase> for further analysis. In many cases, an efficient and PDF created with pdfFactory Pro trial version www.pdffactory.com
Modelling and Identification of Yaw Motion of an Open-frame Underwater Robot A semi-autonomous <phrase>unmanned</phrase> <phrase>underwater vehicle</phrase> (UUV), named VSOR, is being developed at the Laboratory of Sensors and Actuators at the University of <phrase>Sao Paulo</phrase>. The vehicle has been designed to provide inspection and intervention capabilities in specific missions in <phrase>deep water</phrase> oil fields. With the aim of identifying the <phrase>hydrodynamic</phrase> <phrase>motion parameters</phrase>, such as, <phrase>drag coefficient</phrase> and added mass <phrase>inertia</phrase>, experimental trials with the vehicle prototype in a test tank have been performed. The methodology is based on the utilisation of an uncoupled 1-DOF (<phrase>degree of freedom</phrase>) dynamic system equation of an <phrase>underwater vehicle</phrase> and the application of the integral method which is the classical <phrase>least squares</phrase> algorithm applied to the integral form of the system dynamic equations. An assessment of the feasibility of the method is presented.
Unexpected Universality in Disordered Systems and Modeling <phrase>Perpendicular</phrase> <phrase>Recording Media</phrase> In this thesis, I study the <phrase>random-field</phrase> <phrase>Ising model</phrase> (RFIM) in both theoretical and applied perspectives. For theoretical interests, I compare the avalanche behavior in equilibrium and non-equilibrium and find an unexpected universality. The application part focuses on the <phrase>reliability test</phrase> of the ∆H(M, ∆M) methodology , which has been used to measure microscopic properties of magnetic <phrase>recording media</phrase>. Based on RFIM, an interacting random hysteron model has been developed and used to systematically test the reliability of the ∆H(M, ∆M) methodology. Avalanche behavior in response to slowly changing external conditions is ubiquitous in a remarkably wide variety of <phrase>dynamical systems</phrase>. When driven far from equilibrium, those systems display impulsive <phrase>cascade</phrase> of dynamic <phrase>avalanches</phrase> spanning a broad range of sizes. Independent of their microscopic details, many non-equilibrium systems have been shown to have exactly the same dynamic avalanche behavior on many scales. This fact is called universality. So far, non-equilibrium systems were believed to be completely different from equilibrium ones. However, here we show that the zero-temperature RFIM exhibits surprisingly similar avalanche behavior in equilibrium and out of equilibrium. This finding solves a highly controversial question, i.e. whether the equilibrium and non-equilibrium disorder-induced <phrase>phase transitions</phrase> of RFIM belong to the same universality class. Our finding also indicates that generally equilibrium systems and their non-equilibrium counterparts may have deep connections. In state-of-the-art storage applications such as <phrase>hard disk</phrase> drives, the intrinsic switching field distribution of the media grains is one of the most crucial properties defining the recording quality. However, this piece of microscopic information is very hard to measure macroscopically, especially for the <phrase>perpendicular</phrase> <phrase>recording media</phrase>. Using the interacting random hysteron model, we have studied the reliability of the <phrase>recently developed</phrase> ∆H(M, ∆M) method. We demonstrated that this method does have several advantages over comparable methods. First, it has a well-defined reliability range and it allows for self-consistency checks. Second, the presence of dipolar interactions in the range of typical <phrase>recording media</phrase> substantially enhances the reliability of this method. Third, it is robust even in the presence of randomness in exchange or <phrase>magneto</phrase>-static coupling within the range of a typical <phrase>recording media</phrase>.
Evidence for asymmetric intra <phrase>substantia nigra</phrase> functional connectivity - application to <phrase>basal ganglia</phrase> processing The growing uses of <phrase>deep brain stimulation</phrase> for various <phrase>basal ganglia</phrase> (BG) abnormalities have reinforced the need to better understand its functional circuitry and organization. Here we focus on cortico-<phrase>basal-ganglia</phrase> pathways to test the "parallel, segregated" versus "funneling, integrated" theories. Using manganese-enhanced MRI (MEMRI) together with <phrase>principal component</phrase> spatiotemporal analysis, we previously described two patterns of caudomedial <phrase>striatum</phrase> efferent connectivity to the <phrase>substantia nigra</phrase> pars reticulata (SNr) that were hypothesized to represent the coexistence of integrated and segregated processes. These patterns corresponded to a direct mono-synaptic projection to the dorsolateral core of the SN and to a di-synaptic projection covering the entire nucleus. In the current study, MEMRI of the rostrolateral <phrase>striatum</phrase> was carried out to test whether this coexistence remains in the mirror pathway, by measuring rostrolateral <phrase>striatum</phrase> efferent connectivity that is known to connect to the ventromedial SNr. Only one spatiotemporal pattern of manganese accumulation, corresponding to projections from the <phrase>striatum</phrase>, was observed. It corresponds to a mono-synaptic projection to the ventromedial SNr covering SNr laminas, but no manganese was observed at the dorsolateral SNr core. Together with our previous findings, this suggests functional asymmetry along the SNr which is consistent with the known anatomical organization of <phrase>dendrite</phrase> and axonal 3D arborization. Consequently, the polarized connectivity along the dorsolateral-ventromedial axis implies that funneling and integration occur in the core (dorsolateral SNr) to the lamina (ventromedial SNr) direction, whereas in the other direction, and within other parts of the SNr, <phrase>segregation</phrase> predominates.
<phrase>Constraint-based</phrase> <phrase>Type Inference</phrase> for <phrase>Guarded Algebraic Data Types</phrase> <phrase>Constraint-based</phrase> <phrase>Type Inference</phrase> for <phrase>Guarded Algebraic Data Types</phrase> <phrase>Guarded algebraic data types</phrase> subsume the concepts known in the literature as indexed types, guarded recursive <phrase>datatype</phrase> constructors, and first-class phantom types, and are <phrase>closely related</phrase> to inductive types. They have the distinguishing feature that, when typechecking a function defined by cases, every branch may be checked under different assumptions about the type variables in scope. This mechanism allows exploiting the presence of dynamic tests in the code to produce extra static type information. We propose an extension of the <phrase>constraint-based</phrase> type system HM(X) with deep <phrase>pattern matching</phrase> , <phrase>guarded algebraic data types</phrase>, and polymorphic <phrase>recursion</phrase>. We prove that the type system is sound and that, provided recursive function definitions carry a type annotation, <phrase>type inference</phrase> may be reduced to constraint solving. Then, because solving arbitrary constraints is expensive, we further restrict the form of type annotations and prove that this allows producing so-called tractable constraints. Last, in the specific setting of equality, we explain how to solve tractable constraints. To the best of our knowledge, this is the first generic and comprehensive account of <phrase>type inference</phrase> in the presence of <phrase>guarded algebraic data types</phrase>. Inférence de typesà base de contraintes pour les types de données algébriques gardés <phrase>Résumé</phrase> : Les types de données algébriques gardés généralisent les concepts connus dans la littérature sous les noms de types indexés, constructeurs de types de données récursifs gardés et types fantômes depremì ere classe, et sont intimement liés aux types inductifs. <phrase>Ils</phrase> ont pour trait caractéristique le fait que, lors du typage d'une fonction définie par cas, chaque branche peutêtre typée sous des hypothèses différentesà propos des variables de types connues. Ce mécanisme permet d'exploiter la présence de tests dynamiques dans le code pour produire une information de typage statique supplémentaire. Nous proposons une extension du système de typesà base de contraintes HM(X) avec filtrage profond, types de données algébriques gardés et récursivité polymorphe. Nous démontrons que ce système de types est sûr et que, pourvu que les définitions de fonctions récursives soient annotées par un schéma de types, l'inférence de types se réduità la résolution de contraintes. Ensuite, parce que la résolution de contraintes arbitraires est coûteuse, nous restreignons la forme des annotations autorisées et démontrons que cela permet de produire des contraintes dites gérables. Enfin, dans le cas particulier de l'´ egalité, nous expliquons comment résoudre les contraintes gérables. ` A notre connaissance, ceci est le premier traité générique et …
Modeling the habitat suitability for <phrase>deep-water</phrase> <phrase>gorgonian</phrase> <phrase>corals</phrase> based on terrain variables a r t i c l e i n f o The <phrase>coral</phrase> species Paragorgia <phrase>arborea</phrase> and Primnoa resedaeformis are abundant and widely distributed gorgonians in <phrase>North Atlantic</phrase> waters. Both species add significant habitat complexity to the benthic environment, and support a host of <phrase>invertebrate</phrase> species. Mapping their distribution is an essential step in conservation and resource management , but challenging as a result of their remoteness. In this study, three predictive models — <phrase>Ecological</phrase> Niche <phrase>Factor Analysis</phrase>, <phrase>Genetic Algorithm</phrase> for Rule-set Production and Maximum Entropy modeling (MaxEnt) were applied to predict the distribution of species' suitable habitat across a region of <phrase>Røst</phrase> <phrase>Reef</phrase> (<phrase>Norwegian</phrase> margin) based on multiscale terrain variables. All three models were successful in predicting the habitat suitability for both <phrase>gorgonian</phrase> species across the study area, and the MaxEnt predictions were shown to outperform other predictions. All three models predicted the most suitable habitats for both species to mainly occur along the ridges and on the upper section of the large <phrase>slide</phrase>, suggesting both species preferentially colonize topographic highs. Jackknife tests for MaxEnt predictions highlighted the <phrase>seabed</phrase> aspect in relation to P. <phrase>arborea</phrase> distribution, and the <phrase>seabed</phrase> relative position (curvature) in relation to the distribution of both species. Given the vulnerability of <phrase>deep-water</phrase> <phrase>corals</phrase> to anthropogenic impacts, further <phrase>comparative study</phrase> over a wider study area would be particularly beneficial for the management of the species. Paragorgia <phrase>arborea</phrase> and Primnoa resedaeformis are abundant and widely distributed <phrase>gorgonian</phrase> <phrase>coral</phrase> species observed in <phrase>North Atlantic</phrase> waters (Mortensen and Buhl-Mortensen, 2004; Tendal, 1992). The two species are among the largest <phrase>deep-sea</phrase> gorgonians, with P. <phrase>arborea</phrase> colonies reaching heights above the seafloor of 0.5–2.5 m and P. resedaeformis colony lengths of 0.5–1 m, with both providing complex biotic habitats for numerous <phrase>invertebrate</phrase> species (Buhl-Mortensen et al., 2010; Mortensen et al., 2008; Roberts et al., 2009). The <phrase>deep-water</phrase> gorgonians are long lived and slow growing species, and are fragile and vulnerable to damage which may result from anthropogenic activity such as bottom <phrase>trawling</phrase> and oil exploration (Mortensen and Buhl-Mortensen, 2005). Mapping the distribution of <phrase>deep-water</phrase> <phrase>coral</phrase> species is fundamental in assessing the potential risks to these <phrase>ecosystems</phrase> posed by these activities, and for developing management plans, particularly as species protection <phrase>legislation</phrase> such as the EC <phrase>Habitats Directive</phrase> stipulates protection of these habitats for all member states. However, such distribution mapping is challenging given the remoteness of these habitats, and predictive modeling techniques address …
Generalization of Figure-Ground Segmentation from <phrase>Binocular</phrase> to <phrase>Monocular</phrase> Vision in an Embodied Biological Brain Model Humans have the remarkable ability to generalize from <phrase>binocular</phrase> to <phrase>monocular</phrase> figure-ground segmentation of <phrase>complex scenes</phrase>. This is clearly evident anytime we look at a <phrase>photograph</phrase>, computer monitor or simply close one eye. We hypothesized that this skill is due to of the ability of our brains to use rich embodied signals, such as disparity, to train up <phrase>depth perception</phrase> when only the information from one eye is available. In order to test this hypothesis we enhanced our virtual robot, Emer, who is already capable of performing robust, state-of-the-art, invariant 3D <phrase>object recognition</phrase> [1], with the ability to learn figure-ground segmen-tation, allowing him to recognize objects against complex backgrounds. Continued development of this skill holds <phrase>great promise</phrase> for efforts, like Emer, that aim to create an <phrase>Artificial General Intelligence</phrase> (AGI). For example, it promises to unlock vast sets of <phrase>training data</phrase>, such as <phrase>Google Images</phrase>, which have previously been inaccessible to AGI models due to their lack of embodied, <phrase>deep learning</phrase>. More immediately practical implications , such as achieving human performance on the Caltech101 <phrase>object recognition</phrase> dataset [2], are discussed.ment is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. <phrase>Disclaimer</phrase>: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements , either expressed or implied,
A logic base tool set for real-time Ada <phrase>software development</phrase> This is a report on work conducted privately that explores the use of <phrase>predicate logic</phrase> to reason about real-time design. Safety and reliability issues associated with embedded real-time systems make greater demands on development engineers than non-real-time systems whose functional complexity is of similar degree. The effort undertaken here attempts to provide <phrase>methods and tools</phrase> for dealing with both the functional and temporal aspects of <phrase>software engineering</phrase>. The goal is to improve the effectiveness of engineering and thus the productivity of a development team. The problem was approached using a variant of the IDEFO (SADT) modeling methodology, an adaptation equipped with primitives for expressing data types, time constraints, and process activation rules. The SADT   methodology serves as the interface between an engineer and a &#8220;<phrase>deep structure</phrase>&#8221; representation of a system expressed using logic predicates. The SADT framework supports the modeling of a system with an arbitrary number of processing resources using hierarchical specifications of time constraints and <phrase>resource utilization</phrase> limitations. The concepts of <phrase>resource utilization</phrase>, period, as well as &#8220;hard&#8221; and &#8220;soft&#8221; deadline are built into the approach. This particular report describes a tool set that has been built using these concepts. The tools illustrate that the logic based models are sufficient for conduction control path analysis and schedulability analysis. An example is provided of tool capability in which schedulable entities are   identified in the SADt model assigned priorities, and then are shown to be schedulable by application of the Critical Instant test. The associated <phrase>PDL</phrase>, annotated with time constraints is produced as well. All steps are automatic. Rate monotonic prioritization is selected for the example. Experience with the tools indicates that this is a practical <phrase>cost effective</phrase> approach to real-time <phrase>systems engineering</phrase>.
Impact of Wireless Channel Temporal Variation on MAC Design for Body Area Networks We investigate the impact of wireless channel temporal variations on the design of medium <phrase>access control</phrase> (MAC) protocols for body area networks (BANs). Our measurements-based channel model captures large and small time-scale signal correlations, giving an accurate picture of the signal variation, specifically, the deep fades which are the features that mostly affect the behavior of the MAC. We test the effect of the channel model on the performance of the 802.15.4 MAC both in contention access mode and <phrase>TDMA</phrase> access mode. We show that there are considerable differences in the performance of the MAC compared to simulations that do not model channel temporal variation. Furthermore, explaining the behavior of the MAC under a temporal varying channel, we can suggest specific design choices for the emerging BAN MAC standard.
Real-Time SLAM with <phrase>Octree</phrase> Evidence Grids for Exploration in Underwater <phrase>Tunnels</phrase> At 110m in diameter and over 350m in depth, the <phrase>cenote</phrase> Zacatón in central Mexico is a unique flooded <phrase>sinkhole</phrase>. A platform for conducting preliminary <phrase>sonar</phrase> tests is tethered in place. Abstract We describe a <phrase>Simultaneous Localization and Mapping</phrase> (SLAM) method for a hovering <phrase>underwater vehicle</phrase> that will explore underwater <phrase>caves</phrase> and <phrase>tunnels</phrase>, a true three dimensional (3D) environment. Our method consists of a Rao-Blackwellized <phrase>particle filter</phrase> with a 3D evidence grid map representation. We describe a procedure for dynamically adjusting the number of particles to provide real-time performance. We also describe how we adjust the <phrase>particle filter</phrase> prediction step to accommodate sensor degradation or failure. We present an efficient <phrase>oc</phrase>-tree <phrase>data structure</phrase> which makes it feasible to maintain the hundreds of maps needed by the <phrase>particle filter</phrase> to accurately model large environments. This <phrase>octree</phrase> structure can exploit spatial locality and temporal shared ancestry between particles to reduce the processing and storage requirements. To test our SLAM method, we utilize <phrase>data collected</phrase> with manually-deployed <phrase>sonar</phrase> mapping vehicles in the <phrase>Wakulla Springs</phrase> <phrase>cave</phrase> system in <phrase>Florida</phrase> and the Sistema Zacatón in Mexico, as well as data collected by the DEPTHX vehicle in the test tank at the <phrase>Austin</phrase> Applied <phrase>Research Laboratory</phrase>. We demonstrate our mapping and localization approach with these <phrase>real-world</phrase> datasets. Zacatón is a flooded <phrase>cenote</phrase> (<phrase>sinkhole</phrase>) in <phrase>Tamaulipas</phrase>, Mexico, that has been measured at over 350m deep (Gary 2002). The depths of the <phrase>cenote</phrase> remain unexplored, but during a preliminary expedition in May 2005, we discovered that the upper 200m of Zacatón is roughly a cylinder 110m wide that tapers slightly with depth (Figure 1). Zacatón is the deepest of a series of similar water-filled formations, which are thought to have formed as hydrothermal groundwater dissolved through a layer of <phrase>limestone</phrase> (Gary 2002). Zacatón has a small <phrase>river</phrase> flowing out through a tunnel near the surface, which indicates that water is flowing in from somewhere below the mapped regions, perhaps through navigable <phrase>tunnels</phrase>. The <phrase>mineral</phrase>-rich water in Zacatón supports colorful microbial mats in the <phrase>photic zone</phrase> and has exotic geo-chemical features which make it an excellent match 1
Automated <phrase>Micro-architectural</phrase> <phrase>Test Generation</phrase> for Validation of Modern Processors — <phrase>Design complexity</phrase> of todays microprocessors is increasing at an alarming rate to cope up with the required <phrase>performance improvement</phrase> by adopting complicated <phrase>micro-architectural</phrase> features such as deep pipelines, dynamic scheduling, out-of-order and <phrase>superscalar</phrase> execution, and dynamic <phrase>speculation</phrase>. Since verification complexity is directly proportional to the <phrase>design complexity</phrase>, considerable amount of time and resources are spent on design validation. In the current industrial practice, billions of random test programs generated at <phrase>instruction set architecture</phrase> (ISA) level are used during <phrase>simulation-based</phrase> validation. However, architectural <phrase>test generation</phrase> techniques have limitations in terms of exercising intricate <phrase>micro-architectural</phrase> artifacts. Therefore, it is necessary to use <phrase>micro-architectural</phrase> details during <phrase>test generation</phrase>. Furthermore, there is a lack of automated techniques for directed <phrase>test generation</phrase> targeting <phrase>micro-architectural</phrase> faults. To address these challenges, we present a directed <phrase>test generation</phrase> technique at <phrase>micro-architectural</phrase> level for functional validation of microprocessors. A processor model is described in a temporal <phrase>specification language</phrase> at micro-architecture level. The desired behaviors of micro-architecture mechanisms are expressed as <phrase>temporal logic</phrase> properties. We use decompositional <phrase>model checking</phrase> for systematic <phrase>test generation</phrase>. Our experiments using a processor based on the <phrase>Power Architecture</phrase> T M Technology 1 shows very <phrase>promising results</phrase> in terms of <phrase>test generation</phrase> time as well as <phrase>test program</phrase> length.
<phrase>Test Scheduling</phrase> for Circuits in Micron to <phrase>Deep Submicron Technologies</phrase> —We discuss the test scheduling problem in this paper. We first provide a historical perspective of the original <phrase>test scheduling</phrase> formulation that dealt only with resource conflicts, followed by the consideration of power constraint <phrase>test scheduling</phrase>. We then move on to the recent formulations which include dealing with thermal constraint. We explain solutions, their limitations and the challenges that remain. With the emergence of on-chip sensors, in future it may be possible to leverage the use of such sensors to arrive at more efficient schedules. The paper explains these new opportunities and suggests research directions. This paper also contains an exhaustive list of references that may help the researchers and practitioners dealing with this problem.
Dependability analysis of a very large volume <phrase>neutrino telescope</phrase> The SRISL performed the dependability analysis of a very large volume <phrase>neutrino telescope</phrase> in the KM3NeT project (EU FP6 KM3NeT Design Study, Contract no. 011937), as part of the work group on «<phrase>Risk assessment</phrase> and <phrase>quality assurance</phrase>». The KM3NeT is an over M€300 European project involving 40 institutes or university groups from 10 countries, to design, install and operate a deep‐sea research infrastructure hosting a <phrase>neutrino telescope</phrase> with a volume of at least one cubic kilometer at the bottom of the <phrase>Mediterranean Sea</phrase>. The neutrino detector will consist of several thousand optical sensor modules placed on mechanical structures connecting them in vertical assemblies. The sensors will be inter‐connected via watertight and pressure‐resistant connectors. Their measured data will travel through an underwater network of specially designed multiplexed passive optical or active electronic equipment and <phrase>optical fibers</phrase> to the onshore base. Challenges in the availability features of this system rise from the extreme deployment, operation and maintenance conditions at depths of 3500 to 5500 meters (depending on the site location). The dependability analysis performed in the SRISL was a first order <phrase>steady state</phrase> approximation and consisted of the following steps:  Treat the <phrase>neutrino telescope</phrase> as a complex system; identify the system components and their operational interdependencies, and the required function of the telescope system.  Develop an appropriate <phrase>mathematical model</phrase> to estimate the telescope unavailability based on the unavailabilities of its components and a set of <phrase>steady state</phrase> unavailability evaluation correlations depending on possible component repair/test characteristics.  Obtain results for a variety of alternative detector network configurations, and distances of the detector from the onshore facilities. The final study presented ranges and combinations of possible component unavailability values that satisfied a fixed unavailability requirement for the telescope system. It also developed dependability requirements for major components and/or subsystems consistent with an overall system performance target. The results depicted the dependence of the system unavailability on the number of optical modules and the alternative <phrase>deep sea</phrase> infrastructure configurations for transferring the measured signals. This work has been presented as a detailed report on the dependability of the telescope, and will be included in the telescope technical design report which is still in preparatory phase. Further research is under way to establish a realistic perspective on the practically attainable component reliability ranges, and develop suitable cost‐reliability correlations to depict the trade‐offs between component cost and reliability. These correlations will collaborate with the …
<phrase>Leakage Current</phrase> in Sub-Quarter Micron MOSFET: A Perspective on Stressed <phrase>Delta IDDQ</phrase> Testing The effectiveness of single threshold <phrase>I DDQ</phrase> measurement for <phrase>defect detection</phrase> is eroded owing to higher and more variable background <phrase>leakage current</phrase> in modern VLSIs. <phrase>Delta I DDQ</phrase> is identified as one alternative for <phrase>deep submicron</phrase> current measurements. Often <phrase>delta I DDQ</phrase> is coupled with voltage and thermal stress in order to accelerate the <phrase>failure mechanisms</phrase>. A major concern is the I DDQ limit setting under normal and stressed conditions. In this article, we investigate the impact of voltage and thermal stress on the background leakage. We calculate <phrase>I DDQ</phrase> limits for normal and stressed <phrase>operating conditions</phrase> of 0.18 µm n-MOSFETs using a device simulator. Intrinsic <phrase>leakage current</phrase> components of transistor are analyzed and the impact of technology scaling on effectiveness of stressed I DDQ testing is also investigated.
Analysis of MOS cross-coupled LC-tank oscillators using short-channel device equations — New analytical techniques for estimating the large-signal periodic <phrase>steady-state</phrase> solution of MOS LC-tank oscillators using short-channel device equations are presented. These techniques allow us to make quantitative estimates of the oscillator <phrase>steady-state</phrase> performance without the need for time-consuming transient simulations using simulators such as SPICE. Further, our engineering techniques provide insight and quantitative understanding on the design of current-day, <phrase>deep-submicron</phrase> MOS LC-tank os-cillators and serve as a <phrase>starting point</phrase> in a design strategy that includes complete <phrase>phase noise</phrase>/timing <phrase>jitter</phrase> analysis and optimization. Our analytical results for a cross-coupled LC-tank oscillator that was previously fabricated and tested are in good agreement with simulations using HSPICE.
A scalable built-in self-recovery (BISR) VLSI architecture and <phrase>design methodology</phrase> for 2D-mesh based on-<phrase>chip networks</phrase> On-Chip Networks (OCNs) have been proposed to solve the complex on-chip communication problems. In Very <phrase>Deep-Submicron era</phrase>, <phrase>OCN</phrase> will also be affected by faults in chip due to technologies shrinking. Many researches focused on fault detection and diagnosis in <phrase>OCN</phrase> systems. However, these approaches didn't consider faulty <phrase>OCN</phrase> system recovery. This paper proposes a scalable built-in self-recovery (BISR) <phrase>design methodology</phrase> and corresponding Surrounding Test Ring (STR) architecture for 2D-mesh based OCNs to extend the work of diagnosis. The BISR <phrase>design methodology</phrase> consists of STR architecture generation, faulty system recovery, and system correctness maintenance. For an n × n mesh, STR architecture contains one controller and 4n test modules which are formed as a ring-like connection surrounding the <phrase>OCN</phrase>. Moreover, these test modules generate test patterns for <phrase>fault diagnosis</phrase> during warm-up time. According to these diagnosis results, the faulty system is recovered. Finally, this paper proposes a fault-tolerant routing algorithm, Through-Path <phrase>Fault-Tolerant</phrase> (TP-FT) routing, to maintain the correctness of this faulty system. In our experiments, the proposed approach can reduce 68.33∼79.31% unreachable packets and 4.86∼23.6% latency in comparison with traditional approach with 8.48∼13.3% <phrase>area overhead</phrase> .
<phrase>Phase space</phrase> and power spectral approaches for EEG-based automatic sleep-wake classification in humans: A <phrase>comparative study</phrase> using short and standard epoch lengths <phrase>Sleep disorders</phrase> in humans have become a <phrase>public health</phrase> issue in recent years. Sleep can be analysed by studying the electroencephalogram (EEG) recorded during a night's sleep. Alternating between sleep-wake stages gives information related to the sleep quality and quantity since this alternating pattern is highly affected during <phrase>sleep disorders</phrase>. Spectral composition of <phrase>EEG signals</phrase> varies according to <phrase>sleep stages</phrase>, alternating phases of high energy associated to <phrase>low frequency</phrase> (<phrase>deep sleep</phrase>) with periods of low energy associated to <phrase>high frequency</phrase> (wake and light sleep). The analysis of sleep in humans is usually made on periods (epochs) of 30-s length according to the original Rechtschaffen and Kales sleep scoring manual. In this work, we propose a new <phrase>phase space</phrase>-based (mainly based on <phrase>Poincaré</phrase> plot) algorithm for <phrase>automatic classification</phrase> of sleep-wake states in humans using EEG data gathered over relatively short-time periods. The effectiveness of our approach is demonstrated through a series of experiments involving EEG data from seven healthy adult female subjects and was tested on epoch lengths ranging from 3-s to 30-s. The performance of our <phrase>phase space</phrase> approach was compared to a 2-dimensional <phrase>state space</phrase> approach using the power spectral (PS) in two selected human-specific frequency bands. These powers were calculated by dividing integrated spectral amplitudes at selected human-specific frequency bands. The comparison demonstrated that the <phrase>phase space</phrase> approach gives better performance in the case of short as well as standard 30-s epoch lengths.
An Application of Nonlinear Resistive Networks in <phrase>Computer Vision</phrase> an Application of Nonlinear Resistive Networks in <phrase>Computer Vision</phrase> In this thesis, an improved <phrase>optical flow</phrase> algorithm is presented, as well as a hardware called "Tanh" component. The new approach performs an optimization that reduces the error at spatial discontinuities, and increases the computational speed using analog circuit implementation. Simple simulation of this design is tested using HSPICE. We also build a simulator for a complicated circuit using C, which focuses more on speed, and less on transient. For image smoothing and segmentation problems and <phrase>optical flow</phrase> problems, a series of test images are fed to both the <phrase>resistor</phrase> network and the so-called "TANH" network to determine how effective the Tanh network is in <phrase>image analysis</phrase>. Acknowledgments I would like to express my deepest gratitude to my supervisor, Professor Berthold K. <phrase>Horn</phrase>, for giving me the opportunity and support to work on this thesis, for providing me with crucial guidance and encouragement throughout years, and for serving as a model of an enthusiastic teacher and a dedicated scientist. I am also grateful to Dr. Richard Lanza, my thesis co-supervisor, for providing invaluable insight, ideas, and support. I have learned much from their expertise and am grateful for the time they have devoted to me. This work has benefited from many discussions with Professor John L. Wyatt about non-linear networks, Professor Hae-Seung Lee about analog <phrase>circuit design</phrase>, Professor Rahul Sarpeshkar about non-linear analog <phrase>circuit design</phrase> and Dr. Christopher Terman about non-linear <phrase>circuit simulation</phrase>. I thank all of them for their enthusiasm, patience, and advice. Love and Yajun Fang, for their help and invaluable advice in the past three years. I would also like to thank all my friends in MIT and elsewhere for the great moment we have shared in the past years, especially thank Benjamin Walter, for his advice and help in thesis revising. I would like to express my deep love to my family, especially my parents, who have granted me unconditional love throughout my life; my sisters Bizhou and Jin, my brothers Bihui and Juhui, who have given me all kinds of support in both my career and my life. Finally and most importantly, I would like to thank my girl friend, Xiang Xian, for her love and encourage.
<phrase>Artificial Intelligence</phrase> as the year 2000 approaches Approaches as a human being and defying an interrogator to I am aware that I have acquired a <phrase>reputation</phrase> for being critical of the claims made for <phrase>artificial intelligence</phrase> (AI). It is true that I am repelled by some of the hype that I hear and by the lack of <phrase>self-criticism</phrase> that it implies. However, the underlying philosophical questions have long been of deep interest to me. I can even claim to be the first AI professional; my definition of a professional being someone who performs a service and accepts a fee in return. I had read Alan Turing's paper "Calculating Machinery and Intelligence" when it appeared in Mind in 1950, and was deeply impressed. Without attempting to emulate Turing's erudition-or the wit in which he clothed it-I wrote a short article on the same subject and offered it to the <phrase>editor</phrase> of The <phrase>Spectator</phrase>, a <phrase>British</phrase> journal devoted to contemporary affairs. The <phrase>editor</phrase> accepted it and I duly received a fee. Apart from his mathematical papers, I still consider the paper in o ~ Mind to be the best thing Turing o ever wrote. He began with the z question "Can machines think?" In z that form, he found the question to be unsatisfactorily formulated. An attempt to extract the essential o underlying point of interest led him-to propose the famous <phrase>Turing test</phrase>. p-Instead of asking whether a partic-~ ular machine could think, he suggested that one should instead ask whether it could pass this test. The test involved the machine posing determine whether it was a man or a woman. Turing admitted that he ]c strong arguments to put form favor of the view that a digita puter would one day be able his test, although he was incli believe that it would. He ma interesting suggestion that t/c chine might be equipped learning program and then like a child. The way he put that the machine might <phrase>bq</phrase> grammed to simulate a child'! rather than an adult's brain brings out the point that th is intimately connected learning. Turing was quite aware that the education of a machine would, like that of a child, be a long, drawn-out process. He also saw practical difficulties. <phrase>TI</phrase> computer would not have 1(and could not be sent to s& like a normal child. Even ifth ciency could be overcome by ' engineering" he was afrai other children might make sive …
Performance analysis of CCSDS File Delivery Protocol and erasure coding techniques in <phrase>deep space</phrase> environments The rising demand for multimedia services even in hazardous environments, such as <phrase>space missions</phrase> and military <phrase>theatres</phrase> , and the consequent need of proper <phrase>internetworking</phrase> technologies have revealed the performance limits experienced by TCP protocol over long-delay and <phrase>lossy</phrase> links and highlighted the importance of the communication features provided by the protocol architectures proposed by the Consultative Committee for Space Data Systems (CCSDS). This paper proposes a CCSDS File Delivery Protocol (CFDP) extension, based on the implementation of erasure coding schemes, within the CFDP itself, in order to assure high reliability to the <phrase>data communication</phrase> even in presence of very critical conditions, such as hard shadowing, deep-fading periods and intermittent links. Different encoding techniques are considered and various channel conditions, in terms of Bit Error Ratio and bandwidth values, are tested.
Transformations to automate model change evolution As models are elevated to first-class artifacts within the <phrase>software development</phrase> lifecycle, new approaches are needed to address the accidental complexities associated with current modeling practice (e.g., manually evolving the <phrase>deep hierarchical</phrase> structures of large system models can be error prone and labor intensive). This research abstract presents a model transformation approach to automate model evolution and testing tools to improve the quality of model transformation.
Automated <phrase>Legislative</phrase> Drafting: Generating Paraphrases of <phrase>Legislation</phrase> In this paper, we describe which roles <phrase>deep structures</phrase> of law play in drafting <phrase>legislation</phrase>. <phrase>Deep structures</phrase> contain a formal description of the intended <phrase>normative</phrase> eeects of a new regulation. We will discuss mechanisms that can be used to generate diierent paraphrases of regulations automatically. Since it is possible to test the paraphrases on legal <phrase>knowledge based systems</phrase>, we have provided two extra design steps in <phrase>legislative</phrase> drafting which can be supported by automated tools. <phrase>Deep structures</phrase> are straightforward descriptions of the <phrase>normative</phrase> eeects of regulations. Each <phrase>deep structure</phrase> distinguishes desired and undesired behaviour, and has no further internal structure such as paragraphs or exception structures. This paper describes methods to translate a <phrase>deep structure</phrase> to representations of diierent types of codes, i.e. paraphrases. Each representation of a code has a diierent structure, according to the choice we make during the translation regarding: a) the initial assumptions of the regulation, i.e. modelling from desired or undesired behaviour, b) the <phrase>level of abstraction</phrase>, c) the viewpoint of the law, i.e. the category of norm subjects, and d) the type of deontic modalities the regulation largely uses. All paraphrases have the same eeects as the <phrase>deep structure</phrase>, but with diierent features, and are suitable for diierent goals.
RVAB: Rational Varied-Depth Search in Siguo Game Game playing is one of the classic problems of <phrase>artificial intelligence</phrase>. The Siguo game is an emerging field of research in the area of game-playing programs. It provides a new <phrase>test bed</phrase> for <phrase>artificial intelligence</phrase> with imperfect information. To improve search efficiency for Siguo with more branches and the uncertain payoff in the <phrase>game tree</phrase>, this paper presents a modified <phrase>Alpha-Beta</phrase> <phrase>Search algorithm</phrase>, which is called rational varied-depth <phrase>Alpha-Beta</phrase> (RVAB). The RVAB The basic ideas of RVAB algorithm is : if player get much information about opponent during playing game, they can do more deep thinking about strategies of game. If player can only get few, very uncertain information about opponent during playing game, they don't think more deep about their strategy because it is worthless for player to speculate the strategies of game under very uncertain. Experiments show that RVAB achieves the goals of the improvability of visited nodes efficiency, although it costs a little more memory.
Influence of Manufacturing Variations in IDDQ Measurements: A New Test Criterion This work presents a new § © ¨ ¨-based test criterion supported by the characteristics of a set of experimental testing measurements realized over different samples of industrial ICs and by the definition of the corresponding simulation model. Comparing the current consumptions of a specific circuit a significant correlation between measurements can be observed. The current behaviour can be divided into two parts: (1) a circuit dependent one, which has a major contribution, and affects equally all the devices in a given die, and (2) a smaller die dependent fraction due to variations , defective and non-defective, of each of the devices of a specific die. In this paper, a current model is defined, introducing the effects of manufacturing variations in the basic equations of the sub-threshold current to explain that double behaviour. The results show how it is possible to obtain a lot of information from § ¨ ¨ measurements and how other test selection criteria can be applied to increase the § ¨ ¨ testing sensitivity and quality. testing is performed by measuring the <phrase>quiescent current</phrase> of the power supply and comparing this obtained value with a fixed limit. In the last years this technique has proven to be very useful and has been an important contribution to improve the quality of <phrase>CMOS ICs</phrase> [1]. The selection of the current limit is one of the open and key questions in the utilization of § ¨ ¨ testing. The selection criterion is based on a <phrase>pass/fail</phrase> limit and it is just efficient with failures which provoke high increments of consumption. However, the sensitivity of the § ¨ ¨ is insufficient in order to determine the correctness of ICs with consumptions close to the current limit. The current limit has to be fixed as a tradeoff between the yield and the required quality, that is, it must be selected to reduce the cost impact of <phrase>yield loss</phrase>, without imposing a penalty on <phrase>defect detection</phrase> [2]. Some approaches have been proposed for a better limit selection: current estimation methodologies [3], <phrase>statistical analysis</phrase> of § © ¨ ¨ data [4][5] or the consideration of global <phrase>process variations</phrase> [6]. In addition, the forecast for <phrase>deep sub-micron technologies</phrase> shows an abrupt increment of the background current in several <phrase>orders of magnitude</phrase> and a decrement of some defect effects [7]. The separation between defective and non-defective currents and, therefore, the § ¨ ¨ sensitivity will …
Microprocessor <phrase>Software- Based Self</phrase>-testing Sbst in the Microprocessor <phrase>Test Process</phrase> Microprocessor Test and Validation EVER-INCREASING MARKET DEMANDS for higher computational performance at lower cost and <phrase>power consumption</phrase> continually drive processor vendors to develop new microprocessor generations. Every new generation incorporates technology innovations from different research domains, such as <phrase>microelectronics</phrase>, <phrase>digital-circuit</phrase> design, and computer architecture. All these technology advancements, however, impose new challenges on microprocessor testing. As device geometries shrink, <phrase>deep-submicron</phrase> <phrase>delay defects</phrase> become more prominent, thereby increasing the need for at-speed tests. Also, increases in the core <phrase>operating frequency</phrase> and speed of I/O interfaces necessitate more-expensive external <phrase>test equipment</phrase>. In addition, as multicore processor architectures become more popular, the time needed to test the chip scales with the number of cores, unless the inherent execution parallelism is exploited during testing. These test technology challenges prompted the semiconductor industry during the past decade to consider new <phrase>testing methods</phrase> that can be incorporated in an established microprocessor test flow. The purpose of such methods is to achieve the target DPPM (defective parts per million) rate that <phrase>high-quality</phrase> product development demands, but without imposing excessive overhead in the test budget or interfering with the well-optimized, <phrase>high-performance</phrase> processor design. Such a testing method, which was introduced in the 1980s and has garnered renewed interest in the past decade, is functional self-testing, 1 also called instruction-based self-testing, or <phrase>software-based self</phrase>-testing. SBST has become more accepted for microprocessor testing, and it already forms an integral part of the <phrase>manufacturing test</phrase> flow for major processor vendors. 2,3 This article is the first attempt at classifying SBST approaches according to the way they generate self-test programs. Hence, we propose a taxonomy for the most representative SBST approaches according to their <phrase>test program</phrase> development philosophy. We also summarize research approaches for optimizing other key SBST aspects, and we highlight the potential of SBST for detecting and diagnosing faults at different stages of the microprocessor test and validation process. The key idea of SBST is to exploit on-chip program-mable resources to run normal programs that test the processor itself. The processor generates and applies functional-<phrase>test patterns</phrase> using its native <phrase>instruction set</phrase>, virtually eliminating the need for additional test-specific hardware. (Test-specific hardware such as <phrase>scan chains</phrase> might exist in the chip, but SBST normally does not use this hardware.) Also, the test is applied at the processor's actual <phrase>operating frequency</phrase>. A typical flow for an SBST application in a microprocessor chip comprises the following three steps (see Figure 1): 1. Test code and <phrase>test data</phrase> …
Validation of Experts versus Atlas-based and Automatic Registration Methods for <phrase>Subthalamic Nucleus</phrase> Targeting on MRI Objects In functional stereotactic neurosurgery, one of the cornerstones upon which the success and the operating time depends is an accurate targeting. The subthalamic nucleus (STN) is the usual target involved when applying <phrase>deep brain stimulation</phrase> for <phrase>Parkinson's disease</phrase> (PD). Unfortunately , STN is usually not clearly visible in common medical <phrase>imaging modalities</phrase>, which justifies the use of atlas-based segmentation techniques to infer <phrase>the STN location</phrase>. Materials and methods Eight bilaterally implanted <phrase>PD patients</phrase> were included in this study. A three-dimensional <phrase>T1-weighted</phrase> sequence and inversion recovery T2-weighted <phrase>coronal</phrase> slices were acquired pre-operatively. We propose a methodology for the construction of a <phrase>ground truth</phrase> of <phrase>the STN location</phrase> and a scheme that allows both, to perform a comparison between different <phrase>non-rigid registration</phrase> algorithms and to evaluate their usability to locate the STN automatically. Results The intra-expert variability in identifying <phrase>the STN location</phrase> is 1.06 ± 0.61 mm while the best <phrase>non-rigid registration</phrase> method gives an error of 1.80±0.62 mm. On the other hand, <phrase>statistical tests</phrase> show that an affine registration with only 12 degrees of freedom is not enough for this application. Conclusions Using our validation–evaluation scheme, we demonstrate that automatic STN localization is possible and accurate with <phrase>non-rigid registration</phrase> algorithms.
Auto-gopher -a Wire-line Rotary-<phrase>hammer</phrase> Ultrasonic Drill Developing technologies that would enable NASA to sample rock, soil, and ice by coring, drilling or abrading at a significant depth is of great importance for a large number of in-situ exploration missions as well as for earth applications. Proven techniques to sample Mars subsurface will be critical for future NASA <phrase>astrobiology</phrase> missions that will search for records of past and present life on the <phrase>planet</phrase>, as well as the search of water and other resources. A deep corer, called Auto-Gopher, is currently being developed as a joint effort of the JPL's NDEAA laboratory and <phrase>Honeybee</phrase> Robotics Corp. The Auto-Gopher is a wire-line rotary-<phrase>hammer</phrase> drill that combines rock breaking by hammering using an ultrasonic <phrase>actuator</phrase> and cuttings removal by rotating a fluted bit. The hammering mechanism is based on the Ultrasonic/Sonic Drill/Corer (USDC) that has been developed as an adaptable tool for many of drilling and coring applications. The USDC uses an intermediate free-flying mass to transform the <phrase>high frequency</phrase> vibrations of the <phrase>horn</phrase> tip into a sonic hammering of a <phrase>drill bit</phrase>. The USDC concept was used in a previous task to develop an Ultrasonic/Sonic Ice Gopher. The lessons learned from testing the ice gopher were implemented into the design of the Auto-Gopher by inducing a rotary motion onto the fluted coring bit. A wire-line version of such a system would allow penetration of significant depth without a large increase in mass. A laboratory version of the corer was developed in the NDEAA lab to determine the design and drive parameters of the integrated system. The design configuration lab version of the design and fabrication and preliminary testing results are presented in this paper.
Wordless Sounds: Robust <phrase>Speaker Diarization</phrase> Using Privacy-Preserving Audio Representations —This paper investigates robust <phrase>privacy-sensitive audio features</phrase> for speaker diarization in multiparty conversations: ie., a set of <phrase>audio features</phrase> having low <phrase>linguistic information</phrase> for speaker diarization in a single and multiple distant microphone scenarios. We systematically investigate <phrase>Linear Prediction</phrase> (LP) residual. Issues such as prediction order and choice of representation of LP residual are studied. Additionally, we explore the combination of LP residual with subband information from 2.5 kHz to 3.5 kHz and spectral slope. Next, we propose a supervised framework using deep neural architecture for deriving <phrase>privacy-sensitive audio features</phrase>. We benchmark these approaches against the traditional Mel Frequency Cepstral Coefficients (MFCC) features for speaker diarization in both the microphone scenarios. Experiments on the RT07 evaluation dataset show that the proposed approaches yield diarization performance close to the MFCC features on the single distant microphone dataset. To objectively evaluate the notion of privacy in terms of <phrase>linguistic information</phrase> , we perform human and <phrase>automatic speech recognition</phrase> tests, showing that the proposed approaches to <phrase>privacy-sensitive audio features</phrase> yield much lower recognition accuracies compared to MFCC features.
Nonlinear <phrase>low-dimensional</phrase> regression using <phrase>auxiliary</phrase> coordinates When doing regression with inputs and outputs that are <phrase>high-dimensional</phrase>, it often makes sense to reduce the dimensionality of the inputs before mapping to the outputs. Much work in statistics and <phrase>machine learning</phrase>, such as reduced-rank regression, sliced inverse regression and their variants, has focused on linear <phrase>dimensionality reduction</phrase>, or on estimating the <phrase>dimensionality reduction</phrase> first and then the mapping. We propose a method where both the <phrase>dimensionality reduction</phrase> and the mapping can be nonlinear and are estimated jointly. Our key idea is to define an <phrase>objective function</phrase> where the <phrase>low-dimensional</phrase> coordinates are free parameters, in addition to the <phrase>dimensionality reduction</phrase> and the mapping. This has the effect of decoupling many groups of parameters from each other, affording a far more effective optimization than if using a deep network with nested mappings, and to use a good initialization from sliced inverse regression or spectral methods. Our experiments with image and robot applications show our approach to improve over direct regression and various existing approaches. We consider the problem of <phrase>low-dimensional</phrase> regression , where we want to estimate a mapping between inputs x ∈ R Dx and outputs y ∈ R Dy that are both continuous and <phrase>high-dimensional</phrase> (such as images, or control commands for a robot), but going through a low-dimensional, or latent, space z ∈ R Dz : y = g(F(x)), where z = F(x), y = g(z) and D z < D x , D y. In some situations, this can be preferable to a direct (full-dimensional) regression y = G(x), for example if, in addition to the regression, we are interested in obtaining a low-dimensional representation of x for its own <phrase>sake</phrase> (e.g. visualization or <phrase>feature extraction</phrase>). Even when the true mapping G is not <phrase>low-dimensional</phrase>, using a direct regression requires many parameters (D x D y in <phrase>linear regression</phrase>) and their estimation may be unreliable with small sample sizes. Using a low-dimensional composite mapping g • F with fewer parameters can be seen as a form of regularization and lead to better generalization with <phrase>test data</phrase>. Finally, a common practical approach is to reduce the dimension of x independently of y, say with <phrase>principal component analysis</phrase> (<phrase>PCA</phrase>), and then solve the regression. However , the latent coordinates z obtained in this way do not necessarily preserve the information that is needed to predict y. This is the same reason why one would use <phrase>linear discriminant analysis</phrase> …
Measuring Cortical Thickness Using An Image Domain Local Surface Model And Topology Preserving Segmentation We present a measure of <phrase>gray matter</phrase> (GM) thickness based on local surface models in the image domain. Thickness is measured by integrating GM probability maps along the <phrase>white matter</phrase> (WM) surface normal direction. The method is simple to implement and allows <phrase>statistical tests</phrase> to be performed in the <phrase>gray matter</phrase> volume. A novel <phrase>topol</phrase>-ogy preserving segmentation method is introduced that is able to accurately recover GM in deep sulci. We apply this methodology to a <phrase>longitudinal study</phrase> of <phrase>gray matter</phrase> <phrase>atrophy</phrase> in a patient cohort diagnosed with frontotemporal demen-tia (FTD) spectrum disorders. Following <phrase>image-based</phrase> nor-malization of GM thickness maps, results show significant reduction in cortical thickness in several Brodmann areas spanning temporal, <phrase>parietal</phrase> and frontal lobes across subjects .
Digital Modeling and Testing Research on Digging Mechanism of <phrase>Deep Rootstalk Crops</phrase> The digital model of the laboratory bench parts of digging <phrase>deep rootstalk crops</phrase> were established through adopting the parametric model technology based on feature. The virtual assembly of the laboratory bench of digging <phrase>deep rootstalk crops</phrase> was done and the digital model of the laboratory bench parts of digging <phrase>deep rootstalk crops</phrase> was gained. The vibrospade, which is the key part of the laboratory bench of digging <phrase>deep rootstalk crops</phrase> was simulated and the movement parametric curves of <phrase>spear</phrase> on the vibrospade were obtained. The results show that the <phrase>spear</phrase> was accorded with design requirements. It is propitious to the deep rootstalk.
Synthesizing geometry constructions In this paper, we study the problem of automatically solving ruler/<phrase>compass</phrase> based geometry construction problems. We first introduce a logic and a <phrase>programming language</phrase> for describing such constructions and then phrase the automation problem as a program synthesis problem. We then describe a new program synthesis technique based on three key insights: (i) reduction of symbolic reasoning to concrete reasoning (based on a deep theoretical result that reduces verification to <phrase>random testing</phrase>), (ii) extending the <phrase>instruction set</phrase> of the <phrase>programming language</phrase> with <phrase>higher level</phrase> primitives (representing basic constructions found in <phrase>textbook</phrase> chapters, inspired by how humans use their experience and knowledge gained from chapters to perform complicated constructions), and (iii) pruning the forward exhaustive search using a <phrase>goal-directed</phrase> heuristic (simulating backward reasoning performed by humans). Our tool can successfully synthesize constructions for various geometry problems picked up from <phrase>high-school</phrase> textbooks and examination papers in a reasonable amount of time. This opens up an amazing set of possibilities in the context of making classroom teaching interactive.
A Self-Applicable Supercompiler A supercompiler is a program which can perform a deep transformation of programs using a principle which is similar to <phrase>partial evaluation</phrase>, and can be referred to as metacomputation. Supercompilers that have been in existence up to now (see 12], 13]) were not self-applicable: this is a more diicult problem than self-application of a partial evaluator, because of the more intricate logic of supercompilation. In the present paper we describe the rst self-applicable model of a supercompiler and present some tests. Three features distinguish it from the previous models and make self-application possible: (1) The input language is a subset of Refal which w e refer to as at Refal. (2) The process of driving is performed as a transformation of <phrase>pattern-matching</phrase> graphs. (3) Metasystem jumps are implemented, which a l l o ws the supercom-piler to avoid interpretation whenever direct computation is possible.
<phrase>Efficient Methods</phrase> for Unsupervised Learning of <phrase>Probabilistic Models</phrase> <phrase>Efficient Methods</phrase> for Unsupervised Learning of <phrase>Probabilistic Models</phrase> <phrase>High dimensional</phrase> <phrase>probabilistic models</phrase> are used for many modern scientific and engineering <phrase>data analysis</phrase> tasks. Interpreting neural spike trains, compressing video, identifying features in DNA microarrays, and recognizing particles in <phrase>high energy physics</phrase> all rely upon the ability to find and model complex structure in a <phrase>high dimensional space</phrase>. Despite their <phrase>great promise</phrase>, <phrase>high dimensional</phrase> <phrase>probabilistic models</phrase> are frequently computationally intractable to work with in practice. In this thesis I develop solutions to overcome this intractability, primarily in the context of energy based models. A common cause of intractability is that model distributions cannot be analytically normalized. Probabilities can only be computed up to a constant, making training exceedingly difficult. To solve this problem I propose 'minimum probability flow learning', a variational technique for <phrase>parameter estimation</phrase> in such models. The utility of this training technique is demonstrated in the case of an <phrase>Ising model</phrase>, a Hopfield auto-<phrase>associative memory</phrase>, an <phrase>independent component analysis</phrase> model of <phrase>natural images</phrase>, and a <phrase>deep belief network</phrase>. A second common difficulty in training <phrase>probabilistic models</phrase> arises when the parameter space is ill-conditioned. This makes <phrase>gradient descent</phrase> optimization slow and impractical, but can be alleviated using the natural gradient. I show here that the natural gradient can be related to signal whitening, and provide specific prescriptions for applying it to <phrase>learning problems</phrase>. It is also difficult to evaluate the performance of models that cannot be analytically normalized, providing a particular challenge to <phrase>hypothesis testing</phrase> and model comparison. To overcome this, I introduce a method termed '<phrase>Hamiltonian</phrase> annealed <phrase>importance sampling</phrase>,' which more efficiently estimates the normalization constant of non-analytically-normalizable models. This method is then used to calculate and compare the log likelihoods of several state of the art <phrase>probabilistic models</phrase> of natural image patches. 1 Finally, many tasks performed with a trained probabilistic model (for instance, <phrase>image denoising</phrase> or inpainting and <phrase>speech recognition</phrase>) involve generating samples from the model distribution, which is typically a very computationally expensive process. I introduce a modification to <phrase>Hamiltonian</phrase> <phrase>Monte Carlo</phrase> sampling that reduces the tendency of sampling trajectories to double back on themselves, and enables statistically independent samples to be generated more rapidly. Taken together, it is my hope that these contributions will help scientists and engineers to build and manipulate <phrase>probabilistic models</phrase>. 2 Acknowledgements
Symbolic <phrase>model checking</phrase> <phrase>composite Web services</phrase> using operational and control behaviors This paper addresses the issue of verifying if <phrase>composite Web services</phrase> design meets some desirable properties in terms of deadlock freedom, safety (something bad never happens), and <phrase>reachability</phrase> (something good will eventually happen). <phrase>Composite Web services</phrase> are modeled based on a separation of concerns between business and control aspects of <phrase>Web services</phrase>. This separation is achieved through the design of an operational behavior, which defines the composition functioning according to the <phrase>Web services</phrase>' <phrase>business logic</phrase>, and a control behavior, which identifies the valid sequences of actions that the operational behavior should follow. These two behaviors are formally defined using automata-<phrase>based techniques</phrase>. The proposed approach is <phrase>model checking</phrase>-based where the operational behavior is the model to be checked against properties defined in the control behavior. The paper proves that the proposed technique allows checking the soundness and completeness of the design model with respect to the operational and control behaviors. Moreover, automatic translation procedures from the design models to the NuSMV model checker's code and a verification tool are reported in the paper. <phrase>Web services</phrase> are widely used for developing <phrase>Business-to-Business</phrase> applications whose performance spreads by default over orga-nizations' boundaries. Indeed, <phrase>Web services</phrase> rely on a set of platform-independent and vendor-neutral specifications that offer the necessary means for their description, discovery, <phrase>invocation</phrase>, and mainly composition (Yang, Zhang, & <phrase>Lan</phrase>, 2007). Despite the great interest of the research and industry communities in <phrase>Web services</phrase>, some challenging issues remain pending and hence, hinder the adoption of <phrase>Web services</phrase> to develop robust, dynamic, and safe business applications. Examples of issues include verifica-The severity of these issues intensifies when several component <phrase>Web services</phrase> are put together to form <phrase>composite Web services</phrase> (Bensli-In this paper, we address the ''thorny'' issue of verifying the design of <phrase>composite Web services</phrase>. Developing business applications that <phrase>end-users</phrase> trust requires a deep investigation of the different and independent operations and behaviors that the component <phrase>Web services</phrase> in a composition execute and exhibit, respectively. For instance, having a deadlock in a <phrase>Web service</phrase>-based business transaction is a simply disaster for all stakeholders. Although software vendors can guarantee the safety of their <phrase>Web services</phrase>, the development, testing, and verification of these <phrase>Web services</phrase> are done independently from other vendors' peers, which means a serious lack of how these <phrase>Web services</phrase> behave when put together. To tackle this problem, we use <phrase>model checking</phrase>, a powerful formal and fully automatic technique for the verification of …
<phrase>Protein-protein</phrase> <phrase>binding affinity</phrase> prediction from <phrase>amino acid</phrase> sequence MOTIVATION <phrase>Protein-protein interactions</phrase> play crucial roles in many biological processes and are responsible for smooth functioning of the machinery in <phrase>living organisms</phrase>. Predicting the <phrase>binding affinity</phrase> of <phrase>protein-protein</phrase> complexes provides deep insights to understand the recognition mechanism and identify the strong binding partners in <phrase>protein-protein</phrase> interaction networks.   RESULTS In this work, we have collected the experimental <phrase>binding affinity</phrase> data for a set of 135 <phrase>protein-protein</phrase> complexes and analyzed the relationship between <phrase>binding affinity</phrase> and 642 properties obtained from <phrase>amino acid</phrase> sequence. We noticed that the overall correlation is poor, and the factors influencing affinity depends on the type of the complex based on their function, molecular weight and <phrase>binding site</phrase> residues. Based on the results, we have developed a novel methodology for predicting the <phrase>binding affinity</phrase> of <phrase>protein-protein</phrase> complexes using sequence-based features by classifying the complexes with respect to their function and predicted percentage of <phrase>binding site</phrase> residues. We have developed regression models for the complexes belonging to different classes with three to five properties, which showed a correlation in the range of 0.739-0.992 using jack-knife test. We suggest that our approach adds a new aspect of biological significance in terms of classifying the <phrase>protein-protein</phrase> complexes for affinity prediction.
Syntactically Annotated Corpora of Estonian Syntactically annotated corpora are needed 1) to train and test parsers and various language technological products-grammar checkers, information retrievers and extractors, machine translators etc; 2) to check the agreement of existing linguistic theories with the real language usage. The corpora can be annotated on different levels of depth. In shallow syntactically annotated corpora a syntactic function is determined for every wordform; in deep syntactically annotated corpora (treebanks) also the dependency structure is determined for every sentence (graphically represented as a tree). There exists a Constraint Grammar shallow syntactic parser for Estonian, developed by K. Müürisep and T. Puolakainen. To train the parser, we have annotated texts of written Estonian (20 000 words of <phrase>fiction</phrase>, 6 000 words of legal text and 10 000 words of <phrase>newspaper</phrase> texts). By now we have extended the size of the corpus up to 200 000 words. We have also started to build two versions of Estonian treebank. A <phrase>parallel corpus</phrase> of 50 sentences from J.Gaarder's novel "<phrase>Sophie's world</phrase>" has been annotated and aligned and a Constraint Grammar plus <phrase>phrase structure</phrase> hybrid treebank is being developed, currently consisting of 2400 <phrase>automatically generated</phrase> trees, 149 of them manually revised.
Infaunal Marsh Foraminifera From the <phrase>Outer Banks</phrase>, <phrase>North Carolina</phrase>, <phrase>USA</phrase> The distribution and abundance of live (<phrase>rose</phrase> <phrase>Bengal</phrase> stained) and dead, shallow infaunal (0–<phrase>1 cm depth</phrase>) and <phrase>deep infaunal</phrase> (><phrase>1 cm depth</phrase>) benthic foraminifera have been documented at three locations representing different salinity settings on the fringing <phrase>marshes</phrase> along the <phrase>Pamlico Sound</phrase> and <phrase>Currituck Sound</phrase> coasts of North Carolina's <phrase>Outer Banks</phrase>. Two cores taken at each site represent the lower and higher marsh. Twenty-two taxa were recorded as live. Of these, eight taxa were found only at shallow infaunal depths; the other 14 taxa occur at <phrase>deep infaunal</phrase> depths in one or more cores. Only Jadammina macrescens and Tiphotrocha comprimata were recorded as living in all six cores. The distributions of the other taxa were restricted by combinations of infaunal depth, salinity regime and location on the marsh. The tests of infaunal foraminifera were generally more likely to be preserved in the lower marsh than the higher marsh at low-and intermediate-salinity sites. The opposite pattern was evident at the high-salinity site but this may be due to the low numbers of <phrase>deep infaunal</phrase> specimens recovered. Arenoparrella mexicana, Haplophragmoides wilberti, Jadammina macrescens and Trochammina inflata are the most resistant taxa, whereas Miliammina fusca is the species whose tests are most likely to be lost to post-mortem degradation. In five of the six cores, foraminiferal assemblages and populations do not differ significantly with depth which suggests that the foraminifera of the 0–<phrase>1 cm depth</phrase> interval provide an adequate model upon which paleoenvironmental (including former <phrase>sea level</phrase>) reconstructions can be based. AIBSTRACT <phrase>North Carolina</phrase>. The deepest records of living marsh fora-The distribution and abundance of live (<phrase>rose</phrase> Ben[,-lI stained) and dead. shallow infaunal (0-<phrase>1 cm depth</phrase>) and <phrase>deep infaunal</phrase> (><phrase>1 cm depth</phrase>) benthic foraminifera have been documented ;rt three locations representing different salinity settings on the fringing <phrase>marshes</phrase> along the <phrase>Pamlico sound</phrase>' and <phrase>Currituck Sound</phrase> coasts of North Carolina's <phrase>Outer Banks</phrase>. Two cores taken a t each site represent the lowcr. and higher marsh. Twenty-two <phrase>tax</phrase>;! were recorded as live. Of these, eight h x a were found only a t shallow infaunal depths; the other 14 taxa occur at <phrase>deep infaunal</phrase> depths in one or more cores. Only Jadammina ntacrescerts and Tipholro-cha compriinata were recorded as living in all six cores. The distributions of the other taxa were restricted by combinations of irrfaunal depth, salinity regime and location on the marsh. The tests of infar~nal foraminifera were generally more likely to be …
Using interactive multimedia for <phrase>teaching and learning</phrase> <phrase>object oriented</phrase> <phrase>software design</phrase> <phrase>Object Oriented</phrase> (OO) design and programming is an abstract and complex domain, and students have problems with understanding the concepts and applying them to the design of Software systems. At <phrase>Napier University</phrase>, approximately 400 <phrase>undergraduate students</phrase> per year take <phrase>Object Oriented</phrase> <phrase>Software Design</phrase> (OOSD). There is a growing need to find a way to support students' learning. The question was what we could do to support large number o f students with an abstract domain. The solution we came up with was using Interactive Multimedia (IMM) for learning and teaching the subject. Key strengths of IMM are interactivity and visualisation. IMM can help students develop clear understanding of OO concepts such as objects, classes, and <phrase>message passing</phrase> through interactivity and visualisation. Learning requires active thinking. Although the IMM materials will be initiated from a lecture or a tutorial, they are aimed to be self-directed learning materials. The materials should be able to encourage students to think actively in order to promote <phrase>deep learning</phrase>. Hyperlinks have been used to prompt internal question and reflection. Graphical representation is used to visualise OO <phrase>design process</phrase> from <phrase>real world</phrase> physical objects to software system built. <phrase>Research into</phrase> students' learning using these features is needed in order to explore new design aspects with IMM to improve learning in <phrase>higher education</phrase>. Two different types of learning materials have been developed to support this research. One is a resource-oriented material, which is similar to primary courseware [1] and will be initiated by a <phrase>lecturer</phrase> in a lecture. The other is a task-oriented material with embedded hyperlinks to the resource-oriented ones, and will be used in a tutorial. To investigate the effectiveness of hyperlinks in promoting cognitive interactivity, we test three types of hyperlinks, which are no <phrase>hyperlink</phrase>, static <phrase>hyperlink</phrase> presented as default and dynamic <phrase>hyperlink</phrase> appearing with tips when there is a mistake or incorrect answer made. This <phrase>poster</phrase> will describe trials conducted at Bmnel and Napier <phrase>universities</phrase>. The results and comparison made from the trials in terms of students' attitudes to IMM assisted learning and their performance will be presented along with findings about hyperlinks and visualisation in learning. Ada is still a popular language for introductory programming courses in many university <phrase>Computer Science</phrase> departments. It is often used as a 'super Pascal' for teaching basic algorithmic constructs before moving on to 'big picture' <phrase>object-oriented</phrase> languages. It is a good educational language because of its clear, …
Novel <phrase>speech processing</phrase> mechanism derived from auditory neocortical circuit analysis Analysis of the prominent anatomical and physiological features of auditory thalamus and <phrase>neocortex</phrase> has enabled construction of models designed to identify functionality emergent from these biological circuits. These models have recently been shown to provide powerful computational mechanisms for processing of continuous time-varying sequences such as speech; testing on speech databases has yielded positive initial results that are reported here. The model constitutes a novel hypothesis of underlying functions of auditory <phrase>neocortex</phrase>, and also represents a novel approach to <phrase>speech processing</phrase>. Research in our laboratory has been concentrating on the phenomenon of <phrase>long-term potentiation</phrase> (LTP) [3], which is the most likely candidate for a substrate of neocortical learning and memory. A set of simple learning rules was formulated based on physiological properties of LTP-i) synaptic weight can only increase, ii) every increase is small fixed change, and iii) low saturation threshold permits only 5-10 weight increases over the whole period of training [2] [10]. A series of models were constructed based on the known anatomical cortical features-sparse-random connec-tivity in the superficial cortical layers, emergence of the cortical patches defined by the radius of the local inhibition, and feedback inhibition and masking. The above variety of neocortical features specify a biologically constrained class of microcircuits, which typically perform <phrase>pattern recognition</phrase> or classification via <phrase>competitive learning</phrase> and <phrase>lateral inhibition</phrase> [6] [5]. Simulations of those circuits lead to efficient hardware implementations, with a proven utility for <phrase>pattern recognition</phrase> via efficient approximation of statistical <phrase>pattern recognition</phrase> methods (e.g. <phrase>Bayes</phrase> classifiers) [5]. Key anatomical properties of the auditory model being reviewed in [16] (see also Table 1) include i) topographic (MGv) versus broadly-tuned (<phrase>MGm</phrase>) thalamic nuclei, convergently projecting to primary <phrase>auditory cortex</phrase>; local cortical circuits composed of roughly 100:1 <phrase>excitatory</phrase> to inhibitory cells with <phrase>lateral inhibition</phrase>; and iii) vertical columnar organization projecting from middle to superficial to deep layers. Key physiological properties include: i) plastic (<phrase>NMDA</phrase>-dependent) <phrase>synapses</phrase> from broadly tuned <phrase>MGm</phrase> afferents versus non-plastic <phrase>synapses</phrase> from topographic MGv affer-ents; ii) plasticity via <phrase>long-term potentiation</phrase> (LTP); and iii) time courses for excitation versus inhibition of roughly 1:100. Learning in the model is based on the physiological induction and expression rules for synaptic <phrase>long-term potentiation</phrase> or LTP [12] [11] which have been shown in previous modeling efforts to give rise to useful computational properties [2] [10] [9] [5] [7] [8]. Superficial Cortical Layer Broadly tuned afferents from non-specific thalamic nucleus Sparse connectivity (p ~ .1) LTP of …
Boussinesq–<phrase>Green</phrase>–Naghdi rotational water wave theory a r t i c l e i n f o Using Boussinesq scaling for water waves while imposing no constraints on rotationality, we derive and test model equations for nonlinear water wave transformation over varying depth. These use polynomial basis functions to create velocity profiles which are inserted into the basic equations of motion keeping terms up to the desired Boussinesq scaling order, and solved in a weighted residual sense. The models show rapid convergence to exact solutions for linear dispersion, shoaling, and orbital velocities; however, properties may be substantially improved for a given order of approximation using asymptotic rearrangements. This improvement is accomplished using the large numbers of degrees of freedom inherent in the definitions of the polynomial basis functions either to match additional terms in a <phrase>Taylor series</phrase>, or to minimize errors over a range. Explicit coefficients are given at O(μ 2) and O(μ 4), while more generalized basis functions are given at <phrase>higher order</phrase>. Nonlinear performance is somewhat more limited as, for reasons of complexity, we only provide explicitly lower order nonlinear terms. Still, second order <phrase>harmonics</phrase> may remain good to kh ≈ 10 for O(μ 4) equations. Numerical tests for wave transformation over a <phrase>shoal</phrase> show good agreement with experiments. Future work will <phrase>harness</phrase> the full rotational performance of these systems by incorporating turbulent and <phrase>viscous</phrase> stresses into the equations, making them into <phrase>surf zone</phrase> models. Modern Boussinesq water wave theory began in the 1960s as moderate computing power became more available to researchers. Papers by <phrase>Peregrine</phrase> (1967), and Madsen and Mei (1969) extended the <phrase>shallow water equations</phrase> asymptotically into deeper water to arrive at <phrase>inviscid</phrase>, nonlinear, wave evolution equations with leading order dispersive effects. These were confined to relatively <phrase>shallow water</phrase>, with μ ≡ k 0 h 0 b 1.5, where k 0 is a typical <phrase>wavenumber</phrase> and h 0 is a typical <phrase>water depth</phrase> and so had a limited range of application. However, even at these <phrase>early stages</phrase> it was realized that entire families of equations could be developed that were asymptotically identical but had differing properties. With exceptions (Witting, 1984), this finding was largely ignored until the early 1990s when several groups of researchers (Madsen and Sørensen, 1992; Madsen et al., 1991; Nwogu 1993) used various methods of asymptotic rearrangement to improve properties of Boussinesq equations so that dispersion relations were accurate to the nominal <phrase>deep water</phrase> limit of k 0 h …
Beyond the Short Answer Question with <phrase>Research Methods</phrase> Tutor <phrase>Research Methods</phrase> Tutor is a new <phrase>intelligent tutoring system</phrase> created by <phrase>porting</phrase> the existing implementation of the AutoTutor system to a new domain, <phrase>Research Methods</phrase> in <phrase>Behavioural Sciences</phrase>, which allows more interactive dialogues. The procedure of <phrase>porting</phrase> allowed for an evaluation of the domain independence of the AutoTutor framework and for the identification of domain related requirements. Specific recommendations for the development of other dialogue-based tutors were derived from our experience. Recent advances in <phrase>Intelligent Tutoring System</phrase> technology focus on developing dialogue-based tutors, which act as conversational partners in learning. AutoTutor [5, 7], one of the prevalent systems in this field, claims to simulate naturalistic tutoring sessions in the domain of computer <phrase>literacy</phrase>. An innovative characteristic of AutoTutor is the use of a talking head as the primary interface with the user. The system is also claimed to be domain independent and to be capable of supporting deep reasoning in the tutorial dialogue. One goal of the current project is to test these claims. Another motivation was the fact that the domain of AutoTutor, computer <phrase>literacy</phrase>, provides limited potential for activating deep reasoning mechanisms. By <phrase>porting</phrase> the tutor to a new domain, which requires in-depth qualitative reasoning, we can address issues of domain independence and framework usability in a concrete manner. The new tutor, based on the AutoTutor framework, is built on the domain of <phrase>Research Methods</phrase> in <phrase>Behavioural Sciences</phrase> and thus was named <phrase>Research Methods</phrase> Tutor (<phrase>RMT</phrase>).
ACM SRC <phrase>poster</phrase>: gem: a formal dynamic environment for HPC pedagogy Computing is undergoing a dramatic shift from sequential to <phrase>parallel processing</phrase>. With this shift comes new challenges: how to debug code with multiple processes and threads, and how to effectively teach these programming concepts. Traditional testing tools are ineffective and inefficient when it comes to detecting deep seated logical bugs in parallel code, and often lack a GUI in popular IDE's. No support exists for teaching actual courses based on these tools either. In previous work, my <phrase>research group</phrase> provided an MPI testing tool called <phrase>ISP</phrase> and integrated it into Eclipse's PTP via the GEM plug-in. I expand on these with enhanced graphical interactions, <phrase>interception</phrase> of threaded behavior, and providing a tool for HPC Pedagogy.
A Perspective on Regional and Global Strategies of <phrase>Multinational</phrase> Enterprises <phrase>Multinational</phrase> enterprises (MNEs) are the key drivers of globalization, as they foster increased economic interdependence among national markets. The ultimate test to assess whether these MNEs are global themselves is their actual penetration level of markets across the globe, especially in the broad 'triad' markets of <phrase>NAFTA</phrase>, the <phrase>European Union</phrase> and <phrase>Asia</phrase>. Yet, data on the activities of the 500 largest MNEs reveal that very few are successful globally. For 320 of the 380 firms for which geographic sales data are available, an average of 80.3% of total sales are in their home region of the triad. This means that many of the world's largest firms are not global but regionally based, in terms of breadth and depth of market coverage. Globalization, in terms of a balanced geographic distribution of sales across the triad, thus reflects a special, and rather unusual, outcome of doing international business (<phrase>IB</phrase>). The regional concentration of sales has important implications for various strands of mainstream <phrase>IB</phrase> research, as well as for the broader managerial debate on the design of optimal strategies and governance structures for MNEs. Introduction Globalization, in the sense of increased economic interdependence among nations, is a <phrase>poorly understood</phrase> phenomenon. In this paper, we focus on the key <phrase>actors</phrase> in the globalization process, namely the firms that drive this process. A relatively small set of <phrase>multinational</phrase> enterprises (MNEs) accounts for most of the world's trade and investment. Indeed, the largest 500 MNEs account for over 90% of the world's stock of <phrase>foreign direct investment</phrase> (<phrase>FDI</phrase>) and they, themselves, conduct about half the world's trade (Rugman, 2000). Yet, this paper demonstrates that most of these firms are not 'global' companies, in the sense of having a broad and deep penetration of foreign markets across the world. Instead, most of them have the vast majority of their sales within their home leg of the 'triad', namely in <phrase>North America</phrase>, the <phrase>European Union</phrase> (EU) or <phrase>Asia</phrase>. This new view on 'globalization' is very different from the conventional, mainstream perspective. The latter perspective focuses primarily on macro-level growth patterns in trade and <phrase>FDI</phrase>, and compares these data with national GDP growth rates, but without ever analyzing the equivalent micro-level growth data for the MNEs responsible for the trade and <phrase>FDI</phrase> flows (<phrase>United Nations</phrase>, 2002).
Automatic Verification of Hybrid Systems with Large Discrete <phrase>State Space</phrase> We address the problem of <phrase>model checking</phrase> hybrid systems which exhibit nontrivial discrete behavior and thus cannot be treated by considering the discrete states one by one, as most currently available verification tools do. Our procedure relies on a deep integration of several techniques and tools. A first-order extension of AND-Inverter-Graphs (AIGs) serves as a compact representation format for sets of configurations which are composed of continuous regions and discrete states. <phrase>Boolean</phrase> reasoning on the AIGs is complemented by first-order reasoning in various forms and on various levels. These include subsumption checks for simple constraints, test <phrase>vector generation</phrase> for fast inequality checks of <phrase>boolean</phrase> combinations of constraints, and an exact implication check. These techniques are integrated within a <phrase>model checker</phrase> for universal CTL. Technically, it deals with <phrase>discrete-time</phrase> hybrid systems with linear differentials. The paper presents the approach, the architecture of a prototype implementation, and first <phrase>experimental data</phrase>.
Situated Simplification Testing satisfaction of guards is the essential operation of concurrent <phrase>constraint programming</phrase> (<phrase>CCP</phrase>) systems. We present and prove correct, for the first time, an incremental algorithm for the simultaneous tests of entailment and disentailment of rational tree constraints to be used in <phrase>CCP</phrase> systems with deep guards (e.g., in AKL or in Oz). The algorithm is presented as the simplz+ztion of the constraints which form the (possibly deep) guards and which are situated at different nodes in a tree (of arbitrary depth). The nodes correspond to local computation spaces. In this algorithm, a variable may have multiple bindings (which each represent a constraint on that same variable in a different node). These may be realized in various ways. We give a simple <phrase>fixed-point</phrase> algorithm and use it for proving that the tests implemented by another, practical algorithm are correct and complete for entailment and disentailment. We formulate the results in this paper for rational tree constraints; they can be adapted to finite and feature trees.
<phrase>Delay Testing</phrase> Considering <phrase>Power Supply Noise</phrase> Eeects We propose a new delay <phrase>test generation</phrase> technique that can take into account the impact of the <phrase>power supply noise</phrase> on the signal <phrase>propagation delays</phrase>. This is diierent from existing <phrase>delay fault</phrase> models and <phrase>test generation</phrase> techniques that ignore the dependence of <phrase>path delays</phrase> on the applied <phrase>test patterns</phrase> and cannot capture the worst-case timing scenarios in <phrase>deep submicron designs</phrase>. In addition to sensitizing the fault and propagating the fault eeects to the primary outputs , our new tests also produce the worst-case <phrase>power supply noise</phrase> on the nodes in the target path. Thus, the tests also cause the worst-case <phrase>propagation delay</phrase> for the nodes along the target path. Our experimental results on <phrase>benchmark circuits</phrase> show that the new <phrase>delay tests</phrase> produce signiicantly longer delays on the tested paths compared to the tests derived using existing <phrase>delay testing</phrase> methods.
Layer-wise learning of deep <phrase>generative models</phrase> When using deep, multi-layered architectures to build <phrase>generative models</phrase> of data, it is difficult to train all layers at once. We propose a layer-wise training procedure admitting a performance guarantee compared to the global optimum. It is based on an optimistic proxy of future performance, the best latent marginal. We interpret auto-encoders in this setting as <phrase>generative models</phrase>, by showing that they train a <phrase>lower bound</phrase> of this criterion. We test the new learning procedure against a state of the art method (stacked RBMs), and find it to improve performance. Both theory and experiments highlight the importance, when training <phrase>deep architectures</phrase>, of using an inference model (from data to hidden variables) richer than the generative model (from hidden variables to data).
<phrase>Parkinson's disease</phrase>: a <phrase>motor control</phrase> study using a wrist robot —<phrase>Deep brain stimulation</phrase> (DBS) is the most common <phrase>surgical procedure</phrase> for <phrase>patients with Parkinson's disease</phrase> (PD). DBS has been shown to have a positive effect on PD symptoms; however, its specific effects on <phrase>motor control</phrase> are not yet understood. We introduce the novel use of a wrist robot in studying the effects of stimulation on motor performance and learning. We present results from patients performing reaching movements in a null field and in a force field with and without stimulation. We discuss special cases where robotic testing reveals otherwise undiagnosed impairments, and where clinical scores and robot-based scores display opposing trends.
Gmdh Algorithm for Optimal Model Choice by the External Error Criterion with the Extension of Definition by Model Bias and Its Applications to the Committees and <phrase>Neural Networks</phrase> —In the case of substantial noise, i.e., for inaccurate and incomplete data, the use of the Group Method of Data Handling (GMDH) algorithm leads to sharp and rather deep minimums of dependency of external criterion of accuracy measured on testing sample on the complexity of model structure. This minimum indicates the optimal model. In practice, however, if the noise is just noticeable, i.e., if data are accurate, the minimum becomes indefinite or does not exist at all. In this case, an extension of definition is needed based on a new criterion such as, e.g., the value of a model bias measured on the two identical data samples. The combi-natorial GMDH algorithm with an extension of definition by the model bias can be used as a neuron in committees and in repeatedly multilayered neural networks for solving the problems of medical monitoring.
Onboard classifiers for science event detection on a <phrase>remote sensing</phrase> spacecraft Typically, data collected by a spacecraft is downlinked to Earth and preprocessed before any analysis is performed. We have developed classifiers that can be used onboard a spacecraft to identify high priority data for downlink to Earth, providing a method for maximizing the use of a potentially bandwidth limited downlink channel. Onboard analysis can also enable rapid reaction to dynamic events, such as flooding, <phrase>volcanic eruptions</phrase> or <phrase>sea ice</phrase> break-up.Four classifiers were developed to identify <phrase>cryosphere</phrase> events using <phrase>hyperspectral</phrase> images. These classifiers include a manually constructed classifier, a <phrase>Support Vector Machine</phrase> (SVM), a <phrase>Decision Tree</phrase> and a classifier derived by searching over combinations of thresholded band ratios. Each of the classifiers was designed to run in the computationally constrained operating environment of the spacecraft. A set of scenes was hand-labeled to provide <phrase>training and testing</phrase> data. Performance results on the test data indicate that the SVM and manual classifiers outperformed the <phrase>Decision Tree</phrase> and band-ratio classifiers with the SVM yielding slightly better classifications than the manual classifier.The manual and <phrase>SVM classifiers</phrase> have been uploaded to the EO-1 spacecraft and have been running onboard the spacecraft for over a year. Results of the onboard analysis are used by the Autonomous Sciencecraft Experiment (ASE) of NASA's New <phrase>Millennium</phrase> Program onboard EO-1 to automatically target the spacecraft to collect follow-on imagery. The software demonstrates the potential for future deep <phrase>space missions</phrase> to use onboard <phrase>decision making</phrase> to capture short-lived science events.
Autonomous <phrase>Mobile Robot</phrase> Research at <phrase>Louisiana State</phrase> University's Robotics <phrase>Research Laboratory</phrase> need for specific capabilities in AMRs, including rapid multimodal sensing and integration, real-time response, real-time interruptability, and fault tolerance. <phrase>Research Laboratory</phrase> (RRL) is conducted with the help of a specially modified Denning MRV (<phrase>mobile robot</phrase> vehicle) III research robot (figure 1). MRV III is an excellent <phrase>test bed</phrase> for robotics research: It has a straightforward <phrase>user interface</phrase>, it is easily modified to accept new features, and it s The Department of <phrase>Computer Science</phrase> at <phrase>Louisiana State University</phrase> (LSU) has been involved in robotics research since 1985 when the Robotics <phrase>Research Laboratory</phrase> (RRL) was established as a research and teaching program specializing in autonomous <phrase>mobile robots</phrase> (AMRS). Researchers at RRL are conducting <phrase>high-quality</phrase> research in amrs with the goal of identifying the computational problems and the types of knowledge that are fundamental to the design and implementation of <phrase>autonomous mobile</phrase> robotic systems. In this article, we overview the projects that are currently under way at LSU's RRL. Autonomous <phrase>mobile robots</phrase> (AMRs) are synthetic operational systems that can govern themselves while they accomplish given objectives and simultaneously manage their resources and maintain their integrity. The practical importance of research in AMRs has steadily increased over the last several years because of developments in technology that have brought such systems closer to reality. Already, industrial robots are being used in applications that involve monotonous or tedious tasks as well as applications in hazardous environments such as <phrase>nuclear power plants</phrase>. <phrase>Next-generation</phrase> robots are being planned for applications such as <phrase>deep-sea</phrase> mining and salvage operations , servicing-assembly tasks in space, and maintenance activities in toxic environments such as nuclear plants. Continued research in this area is certain to pay large dividends in the near future. The various unstructured environments in which AMRs are expected to operate can change rapidly. AMR can unexpectedly encounter environmental threats that are hazardous to itself or its environment (for example , the outbreak of a fire or the failure of one of the robot's internal systems). Dynamic and complex environments such as these impose a also be operated as a slave to another computer through its external ports. Aside from MRV III, the lab is equipped with various robotic manipulators, several <phrase>Macintosh</phrase> IIs and PCs, and a <phrase>HERO</phrase> robot. RRL is continually upgrading its facilities to expand research capabilities. This upgrading is especially true of the Denning MRV III robot. RRL is currently adding several features to MRV III, most …
Novel <phrase>Design Methodology</phrase> for <phrase>High-Performance</phrase> <phrase>XOR-XNOR</phrase> <phrase>Circuit Design</phrase> As we scale down to <phrase>deep submicron</phrase> (DSM) technology, noise is becoming a metric of equal importance as power, speed, and area. Smaller <phrase>feature sizes</phrase>, <phrase>low voltage</phrase>, and <phrase>high frequency</phrase> are some of the characteristics for DSM circuits. A novel design methodology for the design of <phrase>energy-efficient</phrase> noise-tolerant <phrase>XOR-XNOR</phrase> circuits that can operate at low voltages is proposed. The proposed circuits are characterized and compared with <phrase>previously published</phrase> circuits for reliability and energy efficiency. To test their driving capability, the proposed gates are implanted in an existing 5-2 compressor design and is shown to provide superior performance. The average noise threshold energy is used for quantifying the noise immunity. Simulation results show that the proposed circuits are more noise-immune and displays better <phrase>power consumption</phrase> results as well as power-delay product characteristics. Also, the circuits prove to be faster and successfully works at all ranges of <phrase>supply voltage</phrase> starting from 0.6V to 3.3V.
Type checking XML transformations Acknowledgments I wish to thank Helmut Seidl for inspiring me to conduct my research in the field of XML transformations, and I feel deep gratitude towards him for guiding me like <phrase>Virgil</phrase> through the circles of tree transducers. He employed me on a project position providing ideal conditions for conducting my research. It was funded by the " Deutsche Forschungs Gemeinschaft " under the headword " MttCheck ". I am grateful to many colleagues who in one way or another contributed to this thesis. Special thanks to Thomas Gawlitza, Ingo Scholtes, and Peter Ziewer for careful reading of the <phrase>manuscript</phrase> and for their helpful <phrase>comments and suggestions</phrase>. Last but not least, I want to thank Jennifer for her love and <phrase>constant support</phrase> on the stony path of writing this work. Abstract <phrase>XML documents</phrase> are often generated by some application in order to be processed by a another program. For a correct exchange of information it has to be guaranteed that for correct inputs only correct outputs are produced. The shape of correct documents, i.e., their type, is usually specified by means of schema languages. This thesis is concerned with methods for statically guaranteeing that transformations are correct with respect to <phrase>pre-defined</phrase> types. Therefore, we consider the XML transformation language <phrase>TL</phrase> abstracting the <phrase>key features</phrase> of common XML transformation languages. We show that each <phrase>TL</phrase> program can be decomposed into at most three stay macro tree transducers. By means of classical results for stay macro tree transducers, this decomposition allows to formulate a type checking algorithm for <phrase>TL</phrase>. This method, however, has even for small programs an exorbi-tant <phrase>run-time</phrase>. Therefore, we develop an alternative approach, which allows at least for a large class of transformations that they can be type checked in polynomial time. The developed algorithms have been implemented and tested for practical examples. 1 Introduction When <phrase>Axel Thue</phrase> published his article about logical problems [Thu10; ST00] in 1910, he certainly would not have expected that his ideas of trees and their rewriting will become as popular <phrase>as 90</phrase> years later with the introduction of the <phrase>Extensible Markup Language</phrase> (XML). The <phrase>W3C</phrase> 1 , an international <phrase>consortium</phrase> to develop <phrase>Web standards</phrase>, describes this language as " a simple, very flexible text format derived from <phrase>SGML</phrase>. Originally designed to meet the challenges of <phrase>large-scale</phrase> <phrase>electronic publishing</phrase>, XML is also playing an increasingly important role in the exchange of the variety of …
Web-Prospector - An Automatic, Site-Wide <phrase>Wrapper Induction</phrase> Approach for Scientific Deep-<phrase>Web Databases</phrase> <phrase>Wrapper induction</phrase> techniques traditionally focus on learning wrappers based on examples from one class of <phrase>Web pages</phrase>, i.e. from <phrase>Web pages</phrase> that are all similar in structure and content. Thereby, traditional <phrase>wrapper induction</phrase> targets the understanding of <phrase>Web pages</phrase> generated from a database using the same generation template as observed in the example set. Applying such techniques to <phrase>Web sites</phrase> generated from biological databases, however, we found that there is a need for wrapping of structurally diverse <phrase>web pages</phrase> from multiple classes making the problem more challenging. Furthermore, we observed that such scientific <phrase>web sites</phrase> do not just provide mere data, but they also tend to provide schema information in terms of data labels – giving further cues for solving the <phrase>web site</phrase> wrapping task. In this paper we present a novel approach to automatic <phrase>information extraction</phrase> from whole <phrase>Web sites</phrase> that considers the novel challenge and takes advantage of the additional clues commonly available in scientific deep <phrase>Web databases</phrase>. The solution consists of a sequence of steps: 1. classification of similar <phrase>Web pages</phrase> into classes, 2. discovery of these classes and 3. <phrase>wrapper induction</phrase> for each class. Our approach thus allows us to perform unsupervised <phrase>information retrieval</phrase> from across an entire <phrase>Web site</phrase>. We test our algorithm against three <phrase>real-world</phrase> biochemical deep <phrase>Web sources</phrase> and report our <phrase>preliminary results</phrase>, which are very promising.
Brain functional integration decreases during <phrase>propofol</phrase>-induced loss of consciousness Consciousness has been related to the amount of integrated information that the brain is able to generate. In this paper, we tested the hypothesis that the loss of consciousness caused by <phrase>propofol</phrase> anesthesia is associated with a significant reduction in the capacity of the brain to integrate information. To assess the functional structure of the whole brain, functional integration and partial correlations were computed from fMRI data acquired from 18 healthy volunteers during resting wakefulness and <phrase>propofol</phrase>-induced deep <phrase>sedation</phrase>. Total integration was significantly reduced from wakefulness to deep <phrase>sedation</phrase> in the whole brain as well as within and between its constituent networks (or systems). Integration was systematically reduced within each system (i.e., brain or networks), as well as between networks. However, the ventral attentional network maintained interactions with most other networks during deep <phrase>sedation</phrase>. Partial correlations further suggested that functional connectivity was particularly affected between <phrase>parietal</phrase> areas and frontal or temporal regions during deep <phrase>sedation</phrase>. Our <phrase>findings suggest</phrase> that the breakdown in brain integration is the neural correlate of the loss of consciousness induced by <phrase>propofol</phrase>. They stress the <phrase>important role</phrase> played by <phrase>parietal</phrase> and frontal areas in the generation of consciousness.
Equivalent circuit modeling of <phrase>guard ring</phrase> structures for evaluation of substrate crosstalk isolation A <phrase>substrate-coupling</phrase> equivalent circuit can be derived for an arbitrary <phrase>guard ring</phrase> test structure by way of F-matrix computation. The derived <phrase>netlist</phrase> represents a unified impedance network among multiple sites on a chip surface and allows <phrase>circuit simulation</phrase> for evaluation of isolation effects provided by guard rings. Geometry dependency of <phrase>guard ring</phrase> effects attributes to layout patterns of a test structure, including such as area of a <phrase>guard ring</phrase> as well as location distance from the circuit to be isolated by the <phrase>guard ring</phrase>. In addition, structural dependency arises from vertical impurity concentrations such as p<sup>+</sup>, n<sup>+</sup>, and deep n-well, which are generally available in a <phrase>deep-submicron</phrase> <phrase>CMOS technology</phrase>. The proposed <phrase>simulation based</phrase> prototyping technique of <phrase>guard ring</phrase> structures can include all these dependences and thus can be strongly helpful to establish isolation strategy against <phrase>substrate coupling</phrase> in a given technology, in an early stage of SoC developments.
I Ic Diagnosis Using Multiple Supply Pad <phrase>I Ddq</phrase> S DDQ has been used extensively as a reliability screen for shorting defects in digital <phrase>integrated circuits</phrase>. Unfortunately, single-threshold <phrase>I DDQ</phrase> methods applied to devices fabricated in <phrase>deep-submicron technologies</phrase> result in unacceptably <phrase>high levels</phrase> of <phrase>yield loss</phrase>. The significant increase in subthresh-old <phrase>leakage currents</phrase> in these technologies makes it difficult to set an absolute <phrase>pass/fail</phrase> threshold to fail only defective devices. 1 There have been proposed solutions to the subthreshold <phrase>leakage current</phrase> problem and, more recently, to <phrase>process variation</phrase> issues. Current signatures, <phrase>delta-I DDQ</phrase> and ratio <phrase>I DDQ</phrase> are based on a self-relative analysis, in which the average <phrase>I DDQ</phrase> of each device is factored into the <phrase>pass/fail</phrase> threshold value. 2–4 We refer to the technology dependency of subthreshold <phrase>leakage current</phrase> as a technology-related variation effect to contrast it with the chip-to-chip variation effects caused by changes in process parameters (<phrase>process variation</phrase>). We base our approach on a previous V DDT-based <phrase>method called</phrase> <phrase>transient signal analysis</phrase> (TSA). 5 TSA uses <phrase>regression analysis</phrase> to calibrate for process and technology-related variation effects by cross-correlating multiple supply pin <phrase>transient signals</phrase> measured under each <phrase>test sequence</phrase>. The I DDQ signal analysis, or QSA, method proposed uses a set of <phrase>I DDQ</phrase> measurements instead, each obtained from the individual supply pins of the device under test (DUT). The process and technology calibration procedure used in QSA is based on a <phrase>regression analysis</phrase> procedure similar to TSA. In TSA, we referred to signal variation resulting from defects as regional variation, to contrast it with the global variations introduced by process and technology-related effects. For <phrase>transient signals</phrase>, the supply rail's resistance-capacitance (RC) network modifies signal characteristics , such as phase and magnitude, at different supply pin test points. In QSA, only the resistive component of the supply rail network introduces variation in the I DDQ values at different supply pins. In either case, the position of the defect in the layout with respect to any given <phrase>power supply</phrase> pin is related to the amount of regional defect variation observed at that pin. For QSA, the variation is directly related to the resistance between the defect site and the pin. For example, a larger value of <phrase>I DDQ</phrase> is expected on supply pins closer to the defect site because of the smaller resistance. Therefore, the multiple <phrase>I DDQ</phrase> measurements can be used to detect the defect as well as triangulate the physical position of the defect in the layout. …
For Problems Sufficiently Hard . . . Ai Needs Cogsci the View Sketched Is <phrase>cognitive science</phrase> relevant to AI problems? Yes — but only when these problems are sufficiently hard. When they qualify as such, the best move for the clever AI researcher is to turn not to yet another faster machine bestowed by <phrase>Moore's Law</phrase>, and not to some souped-up version of an instrument in the AI toolbox, but rather to the human mind's approach to the problem in question. Despite Turing's (Turing 1950) prediction, made over half a century ago, that by now his test (the Tur-ing test, of course) would be passed by machines, the best conversational computer can't out-debate a sharp toddler. The mind is still the most powerful thinking thing in the known universe; a brute fact, this. But what's a " sufficiently hard " problem? Well, one possibility is that it's a problem in a set of those which are Turing-solvable, but which takes a lot of time to solve. As you know, this set can be analyzed into various subsets; complexity theorists do that for us. Some of these subsets contain only problems that most would say are very hard. For example, most would say that an <phrase>NP-complete</phrase> problem is very hard. But is it sufficiently hard, in our sense? No. Let P be such a problem, a <phrase>decision problem</phrase> for F associated with some finite <phrase>alphabet</phrase> A, say. We have an algorithm A that solves P. 1 And odds are, A doesn't correspond to <phrase>human cognition</phrase>. The best way to proceed in an attempt to get a computer to decide particular members of A is to rely on computational <phrase>horsepower</phrase>, and some form of pruning to allow decisions to be returned in relatively short order. What we have just described structurally, maps with surprising accuracy onto what was done in AI specifically for the problem of chess. In his famous " 20 Questions " paper, written in the very early days of AI and CogSci (and arguably at the very dawn of a sub-field very relevant, for reasons touched upon later, to issues dealt with herein: computational cognitive model-ing and cognitive architectures), <phrase>Newell</phrase> (<phrase>Newell</phrase> 1973) suggested that perhaps the nature of <phrase>human cognition</phrase> could be revealed by building a machine able to play good chess. But <phrase>Deep Blue</phrase> was assuredly not what <phrase>Newell</phrase> had in mind. <phrase>Deep Blue</phrase> was an experiment in harnessing <phrase>horsepower</phrase> to muscle through a Turing-1 I.e., for every u ∈ A …
<phrase>Magnetic-Field</phrase>-Based 3D ETREE Modelling for Multi-Frequency <phrase>Eddy Current</phrase> Inspection More and more <phrase>solid-state magnetic</phrase> field sensors such as Hall devices are used in <phrase>eddy current</phrase> inspection (EC) to acquire <phrase>magnetic field</phrase> signals. This work extends the previous analytical model, i.e. 2D Extended Truncated Region <phrase>Eigenfunction</phrase> Expansion (ETREE) of EC, and focuses on establishment of 3D ETREE of multi-frequency <phrase>eddy current</phrase> inspection (MFEC) on stratified conductors, while taking into account the <phrase>solid-state magnetic</phrase> field sensors for field quantification and rectangular coils for field excitation. 3D <phrase>Finite Element</phrase> Modelling (FEM) and a hybrid modelling are adopted for verification of the established model. It has been noticed that the 3D ETREE implements the <phrase>fast and accurate</phrase> computation of <phrase>magnetic field</phrase> signals of MFEC. Following that, the directional characteristics of EC with rectangular excitation coils are investigated, which reveals that the coil width contributes more to the measurement sensitivity than the coil length, and benefits the evaluation of defects and <phrase>anisotropic</phrase> conductivity profile of conductors in the follow-up study. 1 Introduction To evaluate the integrity and structural health of metallic structures such as stratified conductors, Electromagnetic Non-destructive Evaluation (ENDE) techniques, especially <phrase>Eddy Current</phrase> (EC) and transient <phrase>eddy current</phrase>, are preferred and used in real-time inspections [1]. <phrase>Eddy-current</phrase> testing traditionally relies on the detection of impedance changes in a <phrase>pickup</phrase> coil as it moves across the inspected specimen. Accordingly, the theoretical modelling for EC previously was implemented merely to predict: (1) the impedance signals from the stranded induction coils for time-<phrase>harmonic</phrase> field [2], [3]; and (2) the <phrase>electromotive force</phrase> (EMF) signals from coils for transient field [4]. However, it is formidable for traditional EC to detect deep flaws in conductive materials, because
Whanausip: a Secure <phrase>Peer-to-peer</phrase> Communications Platform Whanausip: a Secure <phrase>Peer-to-peer</phrase> Communications Platform This <phrase>thesis presents</phrase> a novel mechanism for achieving secure and reliable <phrase>peer-to-peer</phrase> communications on the Internet. WhanauSIP merges a Sybil-proof <phrase>distributed hash table</phrase> with mature SIP technology to enable <phrase>instant messaging</phrase>, audio chat, and <phrase>video conferencing</phrase> that is resilient to censoring, <phrase>eavesdropping</phrase>, and <phrase>forgery</phrase>. Performance and security evaluations performed on the PlanetLab network demonstrate that the majority of resource lookups return within 5 seconds. These results indicate that WhanauSIP delivers practical performance with respect to call session initialization latency for <phrase>VoIP</phrase> <phrase>telephony</phrase>. Furthermore, the tests demonstrated that lookup performance was minimally affected during a Sybil cluster <phrase>ID</phrase> attack, illustrating the network's resilience to malicious adversaries. The thesis delivers three software packages for public use: a general Whanau <phrase>distributed hash table</phrase> implementation, a WhanauSIP <phrase>gateway</phrase>, and a desktop IM/<phrase>VoIP</phrase> client. Acknowledgments This project began a few <phrase>years ago</phrase>, when I had a " there must be a better way " moment. The truth is that we live in a digital world where there is a growing ability and willingness to invade privacy. Additionally, countries and organizations can use their leverage over centralized services to monitor and censor their citizens. I knew information should be free, and I knew people had a right to <phrase>free speech</phrase>, but I didn't know how to make it so. It was really in the world of PDOS that I gained the skills I needed to fulfill my vision. I owe a great amount of gratitude to Chris Lesniewski-Laas for his deep insights, novel ideas, and continuing support. Every meeting with him not only brought clarity to the project, but also clarity to my vision. You've helped me grow tremendously and I am lucky to have had you as an advisor. I would also like to thank Professor Kaashoek. You have a magical ability to turn complex ideas into a flowing story. Your <phrase>comments and suggestions</phrase> have been wonderfully useful and it has truly been an honor to work with you. Last but not least, I need to thank my <phrase>pea</phrase>. <phrase>Serena</phrase>, you have helped me stay focused. You have helped me with my writing and presentation skills. You have inspired, motivated, and kept me excited. You have supported me day in and day out, and I could not thank you enough for just being by my side.
Subthalamic <phrase>deep brain stimulation</phrase> and impulse control in <phrase>Parkinson's disease</phrase>. BACKGROUND AND PURPOSE Experimental studies suggest that <phrase>deep brain stimulation</phrase> (DBS) of the subthalamic nucleus (STN) induces impulsivity in patients with Parkinson's disease (PD). The purpose of this study was to assess various measures of impulse control in <phrase>PD patients</phrase> with <phrase>STN DBS</phrase> in comparison to patients receiving medical therapy.   METHODS In a cross-sectional evaluation, 53 consecutively eligible patients were assessed for impulsivity with the Barratt Impulsiveness Scale, for impulse control disorders (ICDs) using the <phrase>Minnesota</phrase> Impulsive Disorders Interview, and for obsessive-compulsive symptoms using the Maudsley Obsessional-Compulsive Inventory.   RESULTS Independent samples t-tests revealed that compulsivity scores were not different between DBS patients and patients without DBS. However, impulsivity scores were <phrase>significantly higher</phrase> in DBS patients. Additionally, ICDs were observed in 3 of 16 (19%) DBS patients and in 3 of 37 (8%) medically treated patients. No association was found between the use of dopamine <phrase>agonists</phrase> and impulsivity in DBS patients.   CONCLUSIONS Our data suggest that screening for impulsivity and ICDs should be performed prior to DBS, and that patients should be monitored for these problems during follow-up. Prospective trials are needed to confirm the findings of this exploratory study and to elucidate the reasons of a possible induction of impulsivity by <phrase>STN DBS</phrase>.
<phrase>Heart rate</phrase> analysis in 24 patients treated with 150 mg <phrase>amitriptyline</phrase> per day. Twenty-four patients treated with 150 mg <phrase>amitriptyline</phrase> per day for an episode of <phrase>major depression</phrase> underwent a standardized <phrase>heart rate</phrase> analysis (HRA) before therapy and after 14 days. The battery of cardiovascular reflex tests included the determination of the coefficient of variation (CV) while resting and during deep respiration, a spectral analysis of <phrase>heart rate</phrase>, the <phrase>heart rate</phrase> response to standing, and the Valsalva manoeuvre. The results of the initial HRA did not differ from a group of 24 normal control subjects matched for age and sex. On day 14 of treatment the patients showed significantly reduced values of <phrase>heart rate variability</phrase> in all tests (P < 0.0001), probably due to the <phrase>anticholinergic</phrase> side effects of <phrase>amitriptyline</phrase>. <phrase>Heart rate</phrase> increased from 78.1 to 93.6 <phrase>bpm</phrase> on average (P < 0.0001). Abnormal CV at rest was registered in 96% of the patients; during deep respiration 29% showed abnormal CV results. An abnormal spectral analysis was found in 100% of the cases (<phrase>low frequency</phrase> peak: 42%, mid-frequency peak: 100%, <phrase>high frequency</phrase> peak: 79%). The <phrase>heart rate</phrase> response to standing was abnormal in 75% and the Valsalva test in 33% of the cases. Eighty-eight percent of the patients fulfilled the criteria of a cardiovascular autonomic <phrase>neuropathy</phrase> under the conditions of <phrase>amitriptyline</phrase> therapy. As yet, the consequences of these changes for the patients have not been sufficiently elucidated.
Thermo-mechanical Behaviour of a Compacted Swelling <phrase>Clay</phrase> Compacted unsaturated swelling <phrase>clay</phrase> is often considered as a possible buffer material for deep <phrase>nuclear waste</phrase> disposal. An <phrase>isotropic</phrase> cell permitting simultaneous control of suction, temperature and pressure was used to study the thermo-mechanical behaviour of this <phrase>clay</phrase>. Tests were performed at total suctions ranging from 9 to 110 MPa, temperature from 25 to 80 <phrase>°C</phrase>, <phrase>isotropic</phrase> pressure from 0.1 to 60 MPa. It was observed that heating at constant suction and pressure induces either swelling or contraction. The results from compression tests at constant suction and temperature evidenced that at lower suction, the yield pressure was lower, the elastic compressibility parameter and the plastic compressibility parameter were higher. On the other hand, at a similar suction, the yield pressure was slightly influenced by the temperature; and the compressibility parameters were insensitive to temperature changes. The thermal hardening phenomenon was equally evidenced by following a thermo-mechanical path of loading–heating-cooling-reloading.
<phrase>Eigenvector</phrase> <phrase>Centrality</phrase> in Industrial SAT Instances Despite the success of modern SAT solvers on industrial instances, most of the progress relies on intensive experimental testing of improvements or new ideas. In most cases, the behavior of CDCL solvers cannot be predicted and even small changes may have a dramatic positive or negative effect. In this paper, we do not try to improve the performance of SAT solvers, but rather try to improve our understanding of their behavior. More precisely, we identify an essential structural property of industrial instances, based on the <phrase>Eigenvector</phrase> <phrase>centrality</phrase> of a graphical representation of the formula. We show how this static value, computed only once over the initial formula casts new light on the behavior of CDCL solvers. We also <phrase>advocate</phrase> for a better partitionning of industrial problems. Our experiments clearly suggest deep discrepancies among the families of benchmarks used in the last SAT competitions.
Investigations on linear transformations for speaker adaptation and normalization Diese Dissertation ist auf den Internetseiten der Hochschulbibliothek online verfügbar. Zwei Dinge sind zu unserer Arbeit nötig: Unermüdliche Ausdauer und die Bereitschaft, etwas, in das man viel Zeit und Arbeit gesteckt hat, wieder wegzuwerfen. Acknowledgments First I would like to thank my supervisor Prof. Dr.-Ing. Hermann <phrase>Ney</phrase>, head of the Lehrstuhl für Informatik VI at the <phrase>RWTH Aachen</phrase>, for the opportunity to realize this work as part of your team. You introduced me to the exciting field of <phrase>pattern recognition</phrase> in general and <phrase>speech recognition</phrase> in particular. You allowed me great <phrase>latitude</phrase> to pursue my ideas and followed them with great interest. I would also like to thank you for the numerous interesting and enlightening discussions we had. I am also grateful to my second supervisor Prof. <phrase>Christian</phrase> Wellekens, who is with the Multimedia Communications Department of Institut Eurécom, <phrase>France</phrase>, for your interest in my work, the in-depth reading of this thesis and the valuable comments. Stephan Kanthak, you have been an enormous help in many computer problems and difficult debugging sessions. I always admired your deep insight in computer technology, Linux and C++. Besides that, we had many funny talks about the world and his brother. Ralf Schlüter, I am grateful for our discussions and numerous sessions at the <phrase>whiteboard</phrase>, which gave me a deeper insight into <phrase>speech recognition</phrase> and helped to solve a couple of problems. you kept the computers running and patiently dealt with all my requests. I always enjoyed very much the relaxing time at lunch and <phrase>coffee</phrase> breaks with the To all current and former colleagues of the Lehrstuhl für Informatik VI for the motivating <phrase>atmosphere</phrase>, many interesting discussions and also many laughter. I want to express a very special thank to my girlfriend Beate. You had an important part in the success of this thesis. Without you, life would be less wonderful. Nicht zuletzt möchte ich besonders meinen Eltern danken. Ihr habt meinen Weg immer verfolgt, mich ermutigt und unterstützt. Abstract This thesis deals with linear transformations at various stages of the <phrase>automatic speech recognition</phrase> process. In <phrase>current state</phrase>-of-the-art <phrase>speech recognition</phrase> systems linear transformations are widely used to care for a potential mismatch of the <phrase>training and testing</phrase> data and thus enhance the recognition performance. A large number of approaches has been proposed in literature, though the connections between them have been disregarded so far. By developing a unified mathematical framework, close relationships between …
Stand Diameter Distribution Modelling and Prediction Based on Richards Function The objective of this study was to introduce application of the Richards equation on modelling and prediction of stand diameter distribution. The <phrase>long-term</phrase> repeated measurement <phrase>data sets</phrase>, consisted of 309 diameter frequency distributions from Chinese fir (Cunninghamia lanceolata) <phrase>plantations</phrase> in the southern China, were used. Also, 150 stands were used as fitting data, the other 159 stands were used for testing. Nonlinear regression method (<phrase>NRM</phrase>) or <phrase>maximum likelihood</phrase> estimates method (MLEM) were applied to estimate the parameters of models, and the parameter prediction method (PPM) and parameter recovery method (PRM) were used to predict the diameter distributions of unknown stands. Four main conclusions were obtained: (1) R distribution presented a more accurate simulation than three-parametric Weibull function; (2) the parameters p, q and r of R distribution proved to be its scale, location and shape parameters, and have a deep relationship with stand characteristics, which means the parameters of R distribution have good theoretical interpretation; (3) the ordinate of <phrase>inflection point</phrase> of R distribution has significant <phrase>relativity</phrase> with its <phrase>skewness</phrase> and <phrase>kurtosis</phrase>, and the fitted main distribution range for the cumulative diameter distribution of Chinese fir <phrase>plantations</phrase> was 0.4∼0.6; (4) the goodness-of-fit test showed diameter distributions of unknown stands can be well estimated by applying R distribution based on PRM or the combination of PPM and PRM under the condition that only quadratic mean <phrase>DBH</phrase> or plus stand age are known, and the non-rejection rates were near 80%, which are higher than the 72.33% non-rejection rate of three-parametric Weibull function based on the combination of PPM and PRM.
Latent <phrase>feature learning</phrase> in <phrase>social media</phrase> network The current trend in <phrase>social media</phrase> analysis and application is to use the <phrase>pre-defined</phrase> features and devoted to the later model development modules to meet the end tasks. In this work, we claim that representation is critical to the end tasks and contributes much to the model development module. We provide evidence that specially learned feature well addresses the diverse, heterogeneous and collective characteristics of <phrase>social media</phrase> data. Therefore, we propose to transfer the focus from the model development to latent <phrase>feature learning</phrase>, and present a general <phrase>feature learning</phrase> framework based on the popular deep architecture. In particular, following the proposed framework, we design a novel relational generative <phrase>deep learning</phrase> model to test the idea on <phrase>link analysis</phrase> tasks in the <phrase>social media</phrase> networks. We show that the derived latent features well embed both the media content and their observed links, leading to improvement in <phrase>social media</phrase> tasks of user recommendation and social <phrase>image annotation</phrase>.
General formulas for capacity of classical-quantum channels Coding problems of classical-quantum channels are considered in the most general setting, where no structural assumptions such as the stationary mem-oryless property are made on a channel. The <phrase>channel capacity</phrase> as well as the characterization of the strong converse property is given just in parallel with the corresponding classical results of Verdú-Han based on the so-called information-spectrum method. The general results are applied to the stationary memoryless case with or without cost constraint, whereby a deep relation between the channel <phrase>coding theory</phrase> and the <phrase>hypothesis testing</phrase> for two quantum states is elucidated.
Model predictive control of remotely operated <phrase>underwater vehicles</phrase> — This paper describes the implementation of a model predictive controller novel in an underwater robot vehicle. This work also shows the development of an <phrase>underwater vehicle</phrase> model that accounts for physical, <phrase>hydrodynamic</phrase> and restorative effects, while the <phrase>damping</phrase> coefficients are neglected in the prediction of the vehicle position and orientation. The vehicle kinematic and dynamic models are linearized and arranged into the state space form inside the predictive controller. The model helps to determine the future position and orientation of the vehicle to track a predefined underwater trajectory in an optimal way. The results show that the predictive controller offered significant benefits compared to PID controllers by reducing the MSE and RMS by 40% and 76% respectively. I. INTRODUCTION A Remote Operated Vehicle (<phrase>ROV</phrase>) is a type of <phrase>Unmanned</phrase> <phrase>Underwater Vehicle</phrase> (UUV) connected to the surface through a cable or <phrase>umbilical</phrase> line. ROVs can perform important underwater tasks that include assisting the offshore exploration and production of oil and gas [1] and studying marine life and collecting <phrase>deep water</phrase> samples [2]. Improving ROVs involves not only researching their design, but also the reliability of their operation and maneuverability [3]. The design, implementation and testing of the guidance and control systems for ROVs have been addressed by several researchers during the last decade [4],[5]. The design of <phrase>robust tracking</phrase> controllers using proportional and derivative action with nonlinear compensation has proved to be stable, converging the tracking error exponentially [6]. <phrase>Model-based</phrase> <phrase>closed loop</phrase> <phrase>trajectory tracking</phrase> control has been successfully deployed in several ROVs in the <phrase>United States</phrase> and the <phrase>United Kingdom</phrase> [7]. Soylu, Buckham and Podhorodesky have proposed the use of Chattering-free <phrase>sliding mode</phrase> and l ∞ controllers for the trajectory control of ROVs to incorporate the thruster saturation limits as part of the controller design [8]. The control techniques mentioned above have significantly improved the operation reliability and task accuracy of ROVs [5]. Nevertheless, these control algorithms do not consider the effect of forecast perturbation and out-coming tracking maneuvers that can be well predicted by a dynamic model. Model Predictive Control (MPC) [9], [10] is a <phrase>model-based</phrase> <phrase>control algorithm</phrase> that solves a <phrase>finite horizon</phrase> optimal
Arabic <phrase>Goal-oriented</phrase> Conversational Agent Based on <phrase>Pattern Matching</phrase> and Knowledge Trees Conversational Agents (CA's) are computer agents used in applications to converse with humans using <phrase>natural language</phrase> dialogues. They are widely used in different fields like industry, education, <phrase>marketing</phrase>, health, and other services. <phrase>Goal Oriented</phrase> Conversational Agents (GO-CAs) are agents having a deep strategic purpose which enables them to direct conversations to achieve a certain goal using a specific domain. Typically (CA's) are programmed to have a set of rules that guide the conversation with the user. One technique used to script CA's is through <phrase>pattern matching</phrase> algorithms. Such algorithms are used to match the user's dialogue and instigate the conversation through writing a series of scripts that contains the rules and patterns relevant to the domain. Throughout the conversation, values can be extracted from the user's dialogue which allows the CA to respond with the correct answer. CA's have been mainly developed for the <phrase>English language</phrase> and very limited work has been carried out in Arabic. This is mainly due to the complexity of the language and the lack of resources supporting the <phrase>Arabic language</phrase>. This paper proposes a new CA architecture based on a <phrase>pattern matching</phrase> algorithm for the development of a goal orientated Arabic Conversational Agents (ACA). The ACA incorporates a new <phrase>scripting language</phrase> and <phrase>knowledge engineering</phrase> is used to construct the domain. A prototype ACA was developed and the <phrase>Iraqi</phrase> <phrase>passport</phrase> system was used as a domain to evaluate the new ACA. The ACA was tested and evaluated by experts within the <phrase>Iraq</phrase> Consulate with encouraging results and received <phrase>positive feedback</phrase>.
Validation of Tissue Modelization and Classification Techniques in <phrase>T1-Weighted</phrase> <phrase>MR Brain Images</phrase> We present a deep study of the performance of tissue modelization and classification techniques for <phrase>T1-weighted</phrase> <phrase>MR images</phrase>. It is assumed that only <phrase>T1-weighted</phrase> MR image modality is available. The methods presented here were selected to represent the whole range of <phrase>prior information</phrase> that can be used in the classification, i.e. intensity, spatial and anatomical priors. First, we consider the finite Gaussian <phrase>mixture model</phrase> (A-FGMM) with a <phrase>Bayes</phrase> classification. The second method is <phrase>closely related</phrase> to A-FGMM but also considers a hidden <phrase>Markov random field</phrase> (HMRF) model to account for spatial <phrase>prior information</phrase>. For this model, the maximum a posteriori (MAP) criterion is used as the classification decision rule. Third, we study a tissue model that assumes the mixture tissues to be probablistically modeled by the <phrase>linear combination</phrase> of their correspondent pure Gaussian tissue densities (C-GPV). Here again, <phrase>Bayes</phrase> classification is used for the final classification. The fourth method, D-GPV-HMRF, uses the same image model as method C-GPV, but it also encodes spatial information by a hidden <phrase>Markov random field</phrase> as done in method B-HMRF. The fifth algorithm does not model the tissue classes by parametric probability densities, but rather by non-parametric models. As a result, the probabilistic tissue model and the classification criterion can not be distinguished anymore, but are directly interdependent. The resulting algorithm minimizes an information theortic quantity, called the error probability (E-<phrase>EP</phrase>). The final method is also non-parametric, but again adds a HMRF to model spatial <phrase>prior information</phrase> (F-NPHMRF). All methods have been tested on Digital Brain Phantom images for which the classification ground truths were known. Noise and intensity non-uniformities were added to simulate real imaging conditions. No enhancement of the image quality is considered either before or during the classification process. This way robustness and accuracy of the methods is tested in front of the image artifacts. <phrase>Results demonstrate</phrase> clearly that methods relying on both, intensity and spatial information, are in more robust to noise and field inhomogeneities. We demonstrate also that partial volume (PV) is still not perfectly modeled, even though methods that account for mixture classes outperform methods that just consider pure classes.
On Comparison of <phrase>NCR</phrase> Effectiveness with a Reduced <phrase>I{DDQ</phrase>} Vector Set I DDQ test-based <phrase>outlier rejection</phrase> becomes difficult for <phrase>deep sub-micron technology</phrase> chips due to increased leakage and <phrase>process variations</phrase>. The use of Neighbor <phrase>Current Ratio</phrase> (<phrase>NCR</phrase>) that uses <phrase>wafer-level</phrase> <phrase>spatial correlation</phrase> for identifying outlier chips has been proposed earlier as a means of coping with these issues. Due to the slow speed of I DDQ test, there is a strong motivation to reduce the number of <phrase>test vectors</phrase> without compromising the fault coverage. In this paper, we examine the effectiveness of Neighbor <phrase>Current Ratio</phrase> using a reduced <phrase>I DDQ</phrase> vector set and industrial <phrase>test data</phrase>.
FLASH: fast length adjustment of short reads to improve genome assemblies MOTIVATION <phrase>Next-generation</phrase> sequencing technologies generate very large numbers of short reads. Even with very deep genome coverage, short read lengths cause problems in de novo assemblies. The use of paired-end libraries with a fragment size shorter than twice the read length provides an opportunity to generate much longer reads by overlapping and merging read pairs before assembling a genome.   RESULTS We present FLASH, a fast computational tool to extend the length of short reads by overlapping paired-end reads from fragment libraries that are sufficiently short. We tested the correctness of the tool on one million simulated read pairs, and we then applied it as a pre-processor for genome assemblies of Illumina reads from the <phrase>bacterium</phrase> <phrase>Staphylococcus</phrase> aureus and human <phrase>chromosome</phrase> 14. FLASH correctly extended and merged reads >99% of the time on simulated reads with an <phrase>error rate</phrase> of <1%. With adequately set parameters, FLASH correctly merged reads over 90% of the time even when the reads contained up to 5% errors. When FLASH was used to extend reads prior to assembly, the resulting assemblies had substantially greater N50 lengths for both contigs and scaffolds.   AVAILABILITY AND IMPLEMENTATION The FLASH system is implemented in C and is freely available as open-<phrase>source code</phrase> at http://www.cbcb.umd.edu/software/flash.   CONTACT t.magoc@gmail.com.
Patterns in Benthic Biodiversity Link Lake Trophic Status to Structure and Potential Function of Three Large, Deep Lakes Relative to their scarcity, large, deep lakes support a large proportion of the world's <phrase>freshwater</phrase> species. This biodiversity is threatened by <phrase>human development</phrase> and is in need of conservation. Direct comparison of biodiversity is the basis of biological monitoring for conservation but is difficult to conduct between large, insular <phrase>ecosystems</phrase>. The objective of our study was to conduct such a comparison of benthic biodiversity between three of the world's largest lakes: <phrase>Lake Tahoe</phrase>, <phrase>USA</phrase>; <phrase>Lake Hövsgöl</phrase>, <phrase>Mongolia</phrase>; and <phrase>Crater Lake</phrase>, <phrase>USA</phrase>. We examined biodiversity of common benthic <phrase>organism</phrase>, the non-biting <phrase>midges</phrase> (<phrase>Chironomidae</phrase>) and determined lake trophic status using chironomid-based lake typology, tested whether <phrase>community structure</phrase> was similar between the three lakes despite geographic distance; and tested whether chironomid diversity would show significant variation within and between lakes. Typology analysis indicated that <phrase>Lake Hövsgöl</phrase> was ultra-<phrase>oligotrophic</phrase>, <phrase>Crater Lake</phrase> was <phrase>oligotrophic</phrase>, and <phrase>Lake Tahoe</phrase> was borderline <phrase>oligotrophic</phrase>/mesotrophic. These results were similar to traditional <phrase>pelagic</phrase> measures of lake trophic status for <phrase>Lake Hövsgöl</phrase> and <phrase>Crater Lake</phrase> but differed for <phrase>Lake Tahoe</phrase>, which has been designated as ultra-<phrase>oligotrophic</phrase> by traditional <phrase>pelagic</phrase> measures such as transparency found in the literature. Analysis of similarity showed that <phrase>Lake Tahoe</phrase> and <phrase>Lake Hövsgöl</phrase> <phrase>chironomid communities</phrase> were more similar to each other than either was to <phrase>Crater Lake</phrase> communities. Diversity varied between the three lakes and spatially within each lake. This research shows that <phrase>chironomid communities</phrase> from these large lakes were sensitive to trophic conditions. <phrase>Chironomid communities</phrase> were similar between the deep environments of <phrase>Lake Hövsgöl</phrase> and <phrase>Lake Tahoe</phrase>, indicating that <phrase>chironomid communities</phrase> from these lakes may be useful in comparing trophic state changes in large lakes. Spatial variation in Lake Tahoe's diversity is indicative of differential response of <phrase>chironomid communities</phrase> to <phrase>nutrient</phrase> enrichment which may be an indication of changes in trophic state within and across habitats.
Efficient <phrase>Monte Carlo</phrase> Device Modeling —A single-particle approach to full-band <phrase>Monte Carlo</phrase> device simulation is presented which allows an efficient computation of drain, substrate and gate currents in <phrase>deep submicron</phrase> MOSFETs. In this approach, <phrase>phase-space</phrase> elements are visited according to the distribution of real <phrase>electrons</phrase>. This scheme is well adapted to a test-function evaluation of the drain current, which emphasizes regions with large drift velocities (i.e., in the inversion channel), a substrate current evaluation via the impact <phrase>ionization</phrase> generation rate (i.e., in the LDD region with relatively high electron temperature and density) and a computation of the gate current in the dominant direct-tunneling regime caused by relatively cold <phrase>electrons</phrase> (i.e., directly under the gate at the source well of the inversion channel). Other important features are an efficient treatment of impurity <phrase>scattering</phrase>, a <phrase>phase-space</phrase> steplike propagation of the electron allowing to minimize self-<phrase>scattering</phrase>, just-before-<phrase>scattering</phrase> gathering of statistics, and the use of a frozen <phrase>electric field</phrase> obtained from a drift-diffusion simulation. As an example an 0.1-m n-MOSFET is simulated where typically 30 minutes of CPU time are necessary per bias point for practically sufficient accuracy.
Power Flow Analysis on <phrase>Cuda</phrase>-based Gpu This major qualifying project investigates the algorithm and the performance of using the <phrase>CUDA</phrase>-based <phrase>Graphics Processing Unit</phrase> for power flow analysis. The accomplished work includes the design, implementation and testing of the power flow solver. Comprehensive analysis shows that the execution time of the <phrase>parallel algorithm</phrase> outperforms that of the sequential algorithm by several factors. iii Acknowledgements I would like to express my deep and sincere gratitude to my academic and MQP advisor, Professor Xinming Huang, for giving me the professional and insightful <phrase>comments and suggestions</phrase> on this project.
Deep Read: A <phrase>Reading Comprehension</phrase> System This paper describes initial work on Deep Read, an automated <phrase>reading comprehension</phrase> system that accepts arbitrary text input (a story) and answers questions about it. We have acquired a corpus of 60 development and 60 test stories of 3 rd to 6 th grade material; each story is followed by short-answer questions (an answer key was also provided). We used these to construct and evaluate a baseline system that uses <phrase>pattern matching</phrase> (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution). This simple system retrieves the sentence containing the answer 30–40% of the time.
Wheels around the world: <phrase>windows live</phrase> mobile <phrase>interface design</phrase> We present a unique <phrase>interface design</phrase> for <phrase>mobile devices</phrase> that addresses major user pain points with deep menu systems and page scrolling. Using a series of 1-5 wheels of content, arranged in a <phrase>combination-lock</phrase> style on a single mobile screen, this design enables a user to consume a multitude of personalized internet and <phrase>web content</phrase> without ever scrolling up/down or selecting from a menu. Additionally, the wheels are easily mapped to a personalized PC experience such as those from My <phrase>MSN</phrase>, live.com, and myYahoo!, enabling users to access their PC content from anywhere. Results from iterative testing across US, Japan, and China show the model to be an effective and desirable mode of consuming personal and internet content on the <phrase>mobile device</phrase>, despite very different navigation paradigms and cultural expectations in each of the countries.
Improved Reconstruction of 4D-<phrase>MR Images</phrase> by Motion Predictions The reconstruction of 4D images from 2D <phrase>navigator</phrase> and data slices requires sufficient observations per motion state to avoid blurred images and motion artifacts between slices. Especially images from rare motion states, like deep inhalations during free-breathing, suffer from too few observations. To address this problem, we propose to actively generate more suitable images instead of only selecting from the available images. The method is based on learning the relationship between <phrase>navigator</phrase> and data-slice motion by <phrase>linear regression</phrase> after <phrase>dimensionality reduction</phrase>. This can then be used to predict new data slices for a given <phrase>navigator</phrase> by <phrase>warping</phrase> existing data slices by their predicted displacement field. The method was evaluated for 4D-MRIs of the <phrase>liver</phrase> under free-breathing, where sliding boundaries pose an additional challenge for <phrase>image registration</phrase>. Leave-one-out tests for five short sequences of ten volunteers showed that the proposed prediction method improved on average the residual mean (95%) motion between the <phrase>ground truth</phrase> and predicted data slice from 0.9mm (1.9mm) to 0.8mm (1.6mm) in comparison to the best selection method. The approach was particularly suited for unusual motion states, where the mean error was reduced by 40% (2.2mm vs. 1.3mm).
Dynamic Analysis of an Electrostatic <phrase>Energy Harvesting</phrase> System Dynamic Analysis of an Electrostatic <phrase>Energy Harvesting</phrase> System Traditional <phrase>small-scale</phrase> <phrase>vibration</phrase> energy harvesters have typically low efficiency of <phrase>energy harvesting</phrase> from <phrase>low frequency</phrase> vibrations. Several <phrase>recent studies</phrase> have indicated that introduction of nonlinearity can significantly improve the efficiency of such systems. Motivated by these observations we have studied the nonlinear electrostatic energy harvester using a combination of analytical and numerical approaches. The analytical approach was based on the normal <phrase>vibration</phrase> mode analysis around an <phrase>equilibrium point</phrase>. The numerical model was implemented and tested using Modelica language. It was found that the efficiency of energy transfer strongly depends on three parameters: the ratio between the maximal electrical and mechanical energies in the system and ratio of natural frequencies of electric and mechanical modes, and finally the <phrase>dimensionless</phrase> degree of nonlinearity in the system. The dependence of the transfer factor on these three parameters was studied and characterized both theoretically and numerically. It was found that the transfer factor Tr has a sharply pronounced peak as a function of e providing a possibility of efficient energy conversion between modes with highly different normal frequencies. 3 4 Acknowledgments I would like to extend my gratitude to the many people who helped to bring this <phrase>research project</phrase> to fruition. First, I would like to thank Professor Kostya Turitsyn for providing me the opportunity of taking part in this research. I am so deeply grateful for his help, professionalism, valuable guidance and <phrase>financial support</phrase> throughout this project and through my entire program of study that I do not have enough words to express my deep and sincere appreciation. am gratefully indebted to him for his valuable comments for this thesis. I would also like to thank Mr. Petr Vorobev and my roommate Daniela Miao, who have willingly proof read my thesis. Finally, I must express my very profound gratitude to my parents and my friends Sha Miao and Xin Xu for providing me with unfailing support and continuous encouragement throughout my years of study and through the process of researching and writing this thesis. This accomplishment would not have been possible without them. Thank you.
Automated Synthesis of Distributed Controllers Synthesis is a particularly challenging problem for <phrase>concurrent programs</phrase>. At the same time it is a very promising approach, since <phrase>concurrent programs</phrase> are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized runtime monitoring. Modern computing systems are increasingly distributed and heterogeneous. Software needs to be able to exploit these advances, providing means for applications to be more performant. Traditional concurrent programming paradigms, as in Java, are based on threads, <phrase>shared-memory</phrase>, and locking mechanisms that guard access to common data. More recent paradigms like the reactive programming model of <phrase>Erlang</phrase> [4] and Scala [35,36] replace <phrase>shared memory</phrase> by asynchronous <phrase>message passing</phrase>, where sending a message is non-blocking. In all these concurrent frameworks, writing reliable software is a serious challenge. Programmers tend to think about code mostly in a sequential way, and it is hard to grasp all possible schedulings of events in a concurrent execution. For similar reasons, verification and analysis of <phrase>concurrent programs</phrase> is a difficult task. Testing, which is still the main method for <phrase>error detection</phrase> in software, has low coverage for <phrase>concurrent programs</phrase>. The reason is that bugs in such programs are difficult to reproduce: they may happen under very specific thread schedules and the likelihood of taking such corner-case schedules is very low. Automated verification, such as <phrase>model-checking</phrase> and other traditional exploration techniques, can handle very limited instances of <phrase>concurrent programs</phrase>, mostly because of the very large number of possible states and of possible interleavings of executions. <phrase>Formal analysis</phrase> of programs requires as a prerequisite a clean <phrase>mathematical model</phrase> for programs. Verification of sequential programs starts usually with an abstraction step – reducing the value domains of variables to finite domains, viewing conditional branching as non-<phrase>determinism</phrase>, etc. Another major simplification consists in disallowing <phrase>recursion</phrase>. This leads to a very robust computational model, namely finite-state automata and regular languages. Regular languages of words (and trees) are particularly well understood notions. The deep connections between logic and automata revealed by the foundational work of Büchi, <phrase>Rabin</phrase>, and others, are the main ingredients in automata-based verification .
IITP: A Supervised Approach for Disorder Mention Detection and Disambiguation In this paper we briefly describe our supervised <phrase>machine learning</phrase> approach for disorder mention detection system that we submitted as part of our participation in the SemEval-2014 Shared task. The main goal of this task is to build a system that automatically identifies mentions of clinical conditions from the clinical texts. The main challenge lies due in the fact that the same mention of concept may be represented in many surface forms. We develop the system based on the supervised <phrase>machine learning algorithms</phrase>, namely <phrase>Conditional Random Field</phrase> and <phrase>Support Vector Machine</phrase>. One appealing characteristics of our system is that most of the features for learning are extracted automatically from the given training or test datasets without using deep <phrase>domain specific</phrase> resources and/or tools. We submitted three runs, and best performing system is based on <phrase>Conditional Random Field</phrase>. For task A, it shows the precision, recall and F-measure values of 50.00%, 47.90% and 48.90%, respectively under the strict matching criterion. When the matching criterion is relaxed, it shows the precision, recall and F-measure of 81.50%, 79.70% and 80.60%, respectively. For task B, we obtain the accuracies of 33.30% and 69.60% for the relaxed and strict matches, respectively.
Effects of <phrase>subthalamic nucleus</phrase> (STN) stimulation on <phrase>motor cortex</phrase> excitability. BACKGROUND <phrase>Deep brain stimulation</phrase> of the internal global pallidus (GPi) and the subthalamic nucleus (STN) has become a treatment alternative in advanced PD. Although the effects of GPi stimulation have been examined recently, little is known about STN stimulation effects on <phrase>motor cortex</phrase> excitability.   METHODS The effects of STN stimulation were studied in eight patients with advanced PD using paired-pulse <phrase>transcranial magnetic stimulation</phrase> (<phrase>TMS</phrase>) in comparison with healthy control subjects. Motor evoked potentials following paired-pulse <phrase>TMS</phrase> (interstimulus interval 3 ms to test for corticocortical inhibition vs 13 ms for facilitation) have been recorded from the extensor carpi radialis and its functional <phrase>antagonist</phrase>, the flexor carpi radialis muscle. <phrase>Silent</phrase> period (<phrase>SP</phrase>) was also determined. Patients were examined under four conditions: medication "off"/stimulator "off" vs medication "on"/stimulator "off" vs medication "off"/stimulator "on" vs medication "on"/stimulator "on."   RESULTS Although the mean values for intracortical inhibition (<phrase>ICI</phrase>) were not significantly different, data variation was smaller and levels of significance higher with the STN stimulator switched "on," suggesting that <phrase>ICI</phrase> was more consistent. <phrase>SP</phrase> during stimulator "on"/medication "on" was longer than during stimulator "off"/medication "off." Motor performance as indicated by a finger-tapping test and Unified PD Rating Scale III was significantly better with dopaminergic medication and further improved with stimulator "on."   CONCLUSIONS <phrase>Results suggest</phrase> an effect of <phrase>subthalamic nucleus</phrase> stimulation on intracortical inhibitory mechanisms. This hypothesis could at least partially explain a more consistent depression of motor evoked potentials following inhibiting paired-pulse <phrase>transcranial magnetic stimulation</phrase>, a longer <phrase>silent</phrase> period (under stimulator "on"/medication "on"), and a reduction of akinesia and rigidity leading to a better motor performance in <phrase>subthalamic nucleus</phrase>-stimulated patients.
Automatic Data Aggregation for Software <phrase>Distributed Shared Memory</phrase> Systems Automatic Data Aggregation for Software <phrase>Distributed Shared Memory</phrase> Systems Software <phrase>Distributed Shared Memory</phrase> (DSM) provides a <phrase>shared-memory</phrase> abstraction on <phrase>distributed memory</phrase> hardware, making a parallel programmer's task easier. Unfortunately, <phrase>software DSM</phrase> is less eecient than the direct use of the underlying <phrase>message-passing</phrase> hardware. The chief reason for this is that hand-coded and compiler-generated <phrase>message-passing</phrase> programs typically achieve better data aggregation in their messages than programs using <phrase>software DSM</phrase>. <phrase>Software DSM</phrase> has poorer data aggre-gation because the system lacks the knowledge of the application's behavior that a <phrase>programmer</phrase> or compiler analysis can provide. We propose four new techniques to perform automatic data aggregation in <phrase>software DSM</phrase>. Our techniques use <phrase>run-time</phrase> analysis of past data-fetch accesses made by a processor, to <phrase>aggregate data</phrase> movement for future accesses. They do not need any additional compiler support. We implemented our techniques in the TreadMarks <phrase>software DSM</phrase> system. We used a test suite of four applications-3D-FFT, Barnes-Hut, Ilink and Shallow. For these applications we obtained 40% to 66% reduction in message counts which resulted in 6% to 19% improvement in execution times. ACKNOWLEDGEMENTS I express my deep gratitude to Dr. <phrase>Alan Cox</phrase> for his continuous guidance and advise without which I could not have completed this thesis. I thank the other members of my committee Dr. Willy Zwaenepoel, Dr. Sarita Adve and Dr. Peter Druschel for their invaluable <phrase>comments and suggestions</phrase>. I also thank my friends Ramakrishnan Rajamony and Parthasarathy Ranganathan for their continuous help throughout the course of my work. Contents Abstract <phrase>ii Acknowledgments</phrase> iii List of Tables vi List of Illustrations vii 1 Introduction 1
The (Non)Utility of <phrase>Predicate-Argument</phrase> Frequencies for Pronoun Interpretation State-of-the-art pronoun interpretation systems rely predominantly on morphosyntac-tic contextual features. While the use of <phrase>deep knowledge</phrase> and inference to improve these models would appear technically in-feasible, previous work has suggested that <phrase>predicate-argument</phrase> statistics mined from naturally-occurring data could provide a useful approximation to such knowledge. We test this idea in several system configurations , and conclude from our results and subsequent error analysis that such statistics offer little or no predictive information above that provided by morphosyntax.
3d Positioning Issues in <phrase>Virtual Reality</phrase> Environments In last few years a big deal of work has been done to introduce <phrase>Virtual Reality</phrase> tools in Engineering Design, Mechanical and Aeronautical Assembly and many other fields, like professional training for civil and military purpose. Many interfaces has been developed and used particularly in assembly environment, where 3D part positioning can be considered the most important issue. Bi-dimensional grids and snaps (or object-snap) have been replaced with " magnetic " handles, 3D grids or assembly <phrase>wizards</phrase> without a deep study and evaluation of interface impact and results, either in terms of modeling and components assembly speed, either in terms of positioning precision. In this paper a original test and comparison between different positioning methods in <phrase>Virtual Reality</phrase> has been provided in order to increase user-handling capabilities and interactivity. Furthermore, a <phrase>Virtual Reality</phrase> desk for semi-immersive engineering applications (VRDD) has been designed and manufactured, targeting a more comfortable and user-friendly environment for modeling and assembly of 3D components. A Open <phrase>Inventor</phrase> VR interface with Polhemus tracking system and shuttered <phrase>stereo vision</phrase> on a Virtual Desk has been used to test several 3D positioning solutions.
Collocational Knowledge versus General <phrase>Linguistic Knowledge</phrase> among <phrase>Iranian</phrase> Efl Learners This study has a twofold purpose. The first and foremost is to see whether there exists any correlation between the collocational knowledge and general <phrase>linguistic knowledge</phrase> of EFL learners. The second is to reveal which type(s) of <phrase>collocation</phrase> is or are more difficult for EFL learners. To this end, 35 subjects, screened by a proficiency test, were given a 90-item <phrase>multiple-choice</phrase> test including lexical collocations (noun+noun, noun+verb, verb+noun, and <phrase>adjective</phrase>+noun), and grammatical collocations (noun+<phrase>preposition</phrase> and <phrase>preposition</phrase>+noun). A native speaker checked the final version of the data and necessary corrections were made. The <phrase>results showed</phrase> that a) there was no <phrase>significant correlation</phrase> between general <phrase>linguistic knowledge</phrase> and collocational knowledge of EFL learners, and b) the grammatical collocations were more difficult than the lexical collocations for learners and from among all subcategories, noun+<phrase>preposition</phrase> was the most difficult and noun+verb was the easiest. Introduction One of the most important aspects of learning a language is learning the vocabulary of that language and its appropriate use. Since traditional techniques of learning vocabulary–the learning of individual words or memorizing bilingual vocabulary lists–appeared to be no longer tenable, researchers suggested ways for learning multiword phrases, chunks as well as association between lexical items. Anderson and Nagy (1991), for instance, accentuate the importance of deep meanings including collocational properties in words. Students need to know which words go with which other words, how words go together normally, and how we can manipulate these
A yield improvement methodology using pre- and post-silicon statistical <phrase>clock scheduling</phrase> — In deep sub-micron technologies, <phrase>process variations</phrase> can cause significant <phrase>path delay</phrase> and <phrase>clock skew</phrase> uncertainties thereby lead to timing failure and <phrase>yield loss</phrase>. In this paper, we propose a comprehensive <phrase>clock scheduling</phrase> methodology that improves timing and yield through both pre-silicon <phrase>clock scheduling</phrase> and post-silicon clock tuning. First, an optimal <phrase>clock scheduling</phrase> algorithm has been developed to allocate the slack for each path according to its timing uncertainty. To balance the skew that can be caused by <phrase>process variations</phrase>, programmable delay elements are inserted at the clock inputs of a small set of <phrase>flip-flops</phrase> on the timing critical paths. A <phrase>delay-fault testing</phrase> scheme combined with <phrase>linear programming</phrase> is used to identify and eliminate timing violations in the manufactured chips. Experimental results show that our methodology achieves substantial yield improvement over a traditional <phrase>clock scheduling</phrase> algorithm in many of the ISCAS89 <phrase>benchmark circuits</phrase>, and obtain an average yield improvement of 13.6%. I. INTRODUCTION The performance and yield of <phrase>sequential circuits</phrase> can be improved significantly by carefully assigning clock arrival times to each flip-flop. This technique is usually called <phrase>clock scheduling</phrase> [1], [2] or <phrase>clock skew</phrase> optimization [3], [4], [5], [6]. Because of <phrase>process variations</phrase>, the <phrase>path delays</phrase> in each manufactured chip are different, and each chip requires a different clock schedule to achieve its best performance. However , the clock trees in existing designs usually do not have tuning capabilities. Moreover, <phrase>process variations</phrase> also change clock arrival times and cause unintentional clock skews, which aggravate the timing yield problem. <phrase>Previous works</phrase> attempt to improve the timing yield of a sequential circuit by finding a good clock schedule that maximizes the slacks for all paths in different ways. The authors in [5], [6] formulate the <phrase>clock skew</phrase> <phrase>optimization problem</phrase> as a least square error problem: where the error is defined as the difference between the assigned skew and the middle point of the permissible range. Held et al. [1] and Albrecht et al [2] adopt the minimum balance algorithm [7] to find the clock schedule that yields a lexicographically maximum slack vector when the slacks are sorted in nondecreasing order. Both approaches do not take into consideration the statistical behavior of <phrase>process variation</phrase>. Recent clock tree designs have started to incorporate tuning capabilities in order to remove unintentional clock skews or speedup timing convergence. Intel's <phrase>Itanium</phrase> T M processors
On-chip traffic modeling and synthesis for <phrase>MPEG-2</phrase> video applications —The objective of this paper is to introduce <phrase>self-similarity</phrase> as a fundamental property exhibited by the bursty traffic between on-chip modules in typical <phrase>MPEG-2</phrase> video applications. <phrase>Statistical tests</phrase> performed on relevant traces extracted from common video clips establish unequivocally the existence of <phrase>self-similarity</phrase> in video traffic. Using a generic tile-based communication architecture, we discuss the implications of our findings on on-chip buffer space allocation and present quantitative evaluations for typical video streams. We also describe a technique for synthetically generating traces having statistical properties similar to those obtained from real video clips. Our proposed technique speeds up buffer simulations, allows media system designers to explore architectures rapidly and use large media data benchmarks more efficiently. We believe that our findings open new directions of research with <phrase>deep implications</phrase> on some fundamental issues in on-chip networks design for multimedia applications.
Vision-aided Imu for Handheld Pedestrian Navigation He has been working with GNSS and integrated systems for over 10 years. Tom Botterill obtained his MA degree in Maths and <phrase>Computer Science</phrase> at <phrase>Cambridge University</phrase> in 2005 and is and is currently studying towards a PhD with the He has over 18 years <phrase>R&D</phrase> experience in the area of hardware and <phrase>software development</phrase> in <phrase>signal processing</phrase> and sensor integration. He is currently holding the position of Senior Research Scientist at the Geospatial Research Centre in New <phrase>Zealand</phrase>, where his main area of research is on <phrase>signal processing</phrase> methods to enable GNSS positioning in indoor environments, together with general Electronics and Communication development. ABSTRACT <phrase>Low cost</phrase> <phrase>inertial sensors</phrase> are often promoted as the solution to indoor navigation. However, in reality, the quality of the measurements is poor, and as a result, the sensors can only be used to navigate for a few seconds at a time before the drift becomes too large to be useful. Therefore, it is necessary to regularly update the sensors with measurements from external systems such as GPS or other sensors useful for navigation. One such sensor is provided by the <phrase>computer vision</phrase> community where a camera can be used to obtain information about the relative translation and rotation between successive images. This paper describes the use of a camera attached to a low cost IMU for navigation in areas where GPS is unavailable such as indoors or deep <phrase>urban canyons</phrase>. It is assumed that a pedestrian user is walking with the <phrase>mobile device</phrase> held out in front of them with the camera pointing approximately towards the ground. Features are matched between successive frames, and the robust RANSAC framework is used to identify which of these lie on the <phrase>ground plane</phrase>, while estimating the camera's orientation and 3 dimensional body frame translation relative to its previous position. This information is used to aid the IMU using a <phrase>Kalman filter</phrase> to reduce the position drift. This paper describes the implementation of the combined <phrase>computer vision</phrase> and <phrase>inertial navigation</phrase> approach. A tactical grade IMU is used for initial testing since it provides more reliable measurements and enables us to provide a reference by which to compare the measurements obtained from the <phrase>computer vision</phrase> algorithm. It is demonstrated that even with a good quality IMU, the algorithm is able to significantly improve the performance of INS navigation when GPS measurements are unavailable.
The benefits of using Guyton's model in a hypotensive control system In order to improve the intraoperative applications, this paper presents the advantages of using Guyton's model in hypotensive control system development. In this system, the mean arterial pressure is decreased and maintained at a <phrase>low level</phrase> during <phrase>anaesthesia</phrase> by controlling <phrase>sodium</phrase> nitroprusside infusion rate. The key of the study is to develop a physiological model of cardiovascular dynamics to present the mean arterial pressure response to <phrase>sodium</phrase> nitroprusside, which was considered as a <phrase>linear model</phrase> in most of known <phrase>blood pressure</phrase> control systems. Being linear, the previous models cannot accurately mimic a physiological system of human circulation, especially at deep hypotensive control with strong reaction of the body. The enhanced model in this study was modified based on Guyton's model of human circulation. It is useful to design a <phrase>PID controller</phrase>, which allows studying and handling the wide range of the body sensitivities. This model is also helpful for studying the behaviors of patients under <phrase>anaesthesia</phrase> conditions, such as the <phrase>perfusion</phrase> of organs and the reaction of the body at hypotensive state. A fuzzy gain scheduler and a supervising algorithm were also developed for online tuning the controller to handle the behavior of the body. The control system was tested on 25 experiments on seven pigs in the animal laboratory. Simulation and <phrase>experiment results</phrase> proved the usefulness of Guyton's model in control system design which can present the dynamical response of <phrase>blood pressure</phrase> in the circulation under and after hypotensive control. The results also indicated the safety and stability of the controller.
The physiological microphone (PMIC): A competitive alternative for speaker assessment in stress detection and <phrase>speaker verification</phrase> Interactive speech system scenarios exist which require the user to perform tasks which exert limitations on <phrase>speech production</phrase>, thereby causing speaker variability and reduced speech performance. In noisy stressful scenarios, even if noise could be completely eliminated , the production variability brought on by stress, including <phrase>Lombard</phrase> effect, has a more pronounced impact on speech system performance. Thus, in this study we focus on the use of a <phrase>silent</phrase> speech interface (PMIC), with a corresponding experimental assessment to illustrate its utility in the tasks of stress detection and <phrase>speaker verification</phrase>. This study focuses on the suitability of PMIC versus close-talk microphone (CTM), and reports that the PMIC achieves as good performance as CTM or better for a number of test conditions. PMIC reflects both stress-related information and speaker-dependent information to a far greater extent than the CTM. For stress detection performance (which is reported in % accuracy), PMIC performs at least on par or about 2% better than the CTM-based system. For a <phrase>speaker verification</phrase> application, the PMIC outperforms CTM for all matched <phrase>stress conditions</phrase>. The performance reported in terms of %EER is 0.91% (as compared to 1.69%), 0.45% (as compared to 1.49%), and 1.42% (as compared to 1.80%) for PMIC. This indicates that PMIC reflects speaker-dependent information. Also, another advantage of the PMIC is its ability to record the user <phrase>physiology</phrase> traits/state. Our experiments illustrate that PMIC can be an attractive alternative for stress detection as well as <phrase>speaker verification</phrase> tasks along with an advantage of its ability to record physiological information, in situations where the use of CTM may hinder operations (<phrase>deep sea</phrase> divers, fire-fighters in <phrase>rescue</phrase> operations, etc.).
Experimental Study of <phrase>Cognitive Radio</phrase> <phrase>Test-bed</phrase> Using Usrp Acknowledgments First of all, I would like to express sincere gratitude to my adviser Dr. Shuangqing <phrase>Wei</phrase> for his <phrase>constant support</phrase> in building my thesis. I would like to thank him for introducing me to the field of <phrase>Wireless Communication</phrase> and <phrase>Cognitive Radio</phrase>. His throughout guidance and motivation in solving complex problems has provided encouragement, enthusiasm and support , while the knowledge acquired by working with him has provided deep understanding of issues that are apart from classroom work. Also, I would like to thank my Co-adviser Dr. Rajgopal Kannan, whose support has provided valuable experience. I would thank Dr. Xue-bin Liang for being a member of thesis committee. Apart from all, my parents and elder sister have provided the emotional and <phrase>financial support</phrase> to build my confidence and without which this wouldn't be possible. I would like to thank my friends and roommates for their support and <phrase>morale</phrase> boost during the hard times. Also, I would like to mention my lab-mates for giving their resources and time in enabling to complete my thesis.
Performance-impact limited area fill synthesis Chemical-mechanical planarization (CMP) and other manufacturing steps in very deep-submicron VLSI have varying effects on device and interconnect features, depending on the local layout density. To improve manufacturability and performance predictability, area fill features are inserted into the layout to improve uniformity with respect to density criteria. However, the performance impact of area fill insertion is not considered by any fill method in the literature. In this paper, we first review and develop estimates for capacitance and timing overhead of area fill insertions. We then give the first formulations of the Performance Impact Limited Fill (<phrase>PIL-Fill</phrase>) problem with the objective of either minimizing total delay impact (MDFC) or maximizing the minimum slack of all nets (MSFC), subject to inserting a given prescribed amount of fill. For the MDFC <phrase>PIL-Fill</phrase> problem, we describe three practical solution approaches based on Integer <phrase>Linear Programming</phrase> (ILP-I and ILP-II) and the Greedy method. For the MSFC <phrase>PIL-Fill</phrase> problem, we describe an iterated greedy method that integrates call to an industry <phrase>static timing analysis</phrase> tool. We test our methods on layout testcases obtained from industry. Compared with the normal fill method [3], our ILP-II method for MDFC <phrase>PIL-Fill</phrase> problem achieves between 25-% and 90% reduction in terms of total <i>weighted edge delay</i> (roughly, a measure of sum of node slacks) impact while maintaining identical quality of the layout density control; and our iterated greedy method for MSFC <phrase>PIL-Fill</phrase> problem also shows significant advantage with respect to the minimum slack of nets on post-fill layout.
An efficient design-for-verification technique for HDLs Due to the high complexity of modern circuit designs, verification has become the <phrase>major bottleneck</phrase> of the entire <phrase>design process</phrase>. There is an emerging need for a practical solution to reduce the verification time. In <phrase>manufacturing test</phrase>, a well-known technique, "<phrase>design-for-testability</phrase>", is often used to reduce the testing time. By inserting some extra      circuits on the hard-to-test points, the testability can be improved and the testing time can be reduced. In this paper, we apply the similar idea to functional verification and propose an efficient "design-for-verification" (<phrase>DFV</phrase>) technique to help users reduce the verification time. The conditions for hard-to-control (<phrase>HTC</phrase>) codes in a <phrase>HDL</phrase> design are clearly defined, and an efficient algorithm to detect them automatically is proposed. Besides the <phrase>HTC</phrase> detection, we also propose an algorithm that can eliminate those <phrase>HTC</phrase> points with minimum number of <phrase>DFV</phrase> points. By the help of those <phrase>DFV</phrase> points, the number of required <phrase>test patterns</phrase> to reach the same coverage can be greatly reduced especially for deep-sequential designs.
Design of Asynchronous Controllers with Delay Insensitive Interface <phrase>Deep submicron technology</phrase> calls for new design techniques, in which wire and gate delays are accounted to have equal or nearly equal effect on circuit behavior. Asynchronous speed-independent (SI) circuits, whose behavior is only robust to gate <phrase>delay variations</phrase>, may be too optimistic. On the other hand, building circuits totally delay-insensitive (DI), for both gates and wires, is impractical. The paper presents a new approach for synthesis of globally DI and locally SI circuits suggested in [7]. The method starts from a speed-independent implementation and locally modifies gate functions to ensure their independence from delays in communication wires. The suggested approach was successfully tested on a set of benchmarks.
PhySIC_IST: cleaning <phrase>source trees</phrase> to infer more informative supertrees BACKGROUND Supertree methods combine phylogenies with overlapping sets of taxa into a larger one. Topological conflicts frequently arise among <phrase>source trees</phrase> for methodological or biological reasons, such as <phrase>long branch</phrase> attraction, lateral gene transfers, <phrase>gene duplication</phrase>/loss or deep gene coalescence. When topological conflicts occur among <phrase>source trees</phrase>, liberal methods infer supertrees containing the most frequent alternative, while <phrase>veto</phrase> methods infer supertrees not contradicting any source tree, i.e. discard all conflicting resolutions. When the source trees host a significant number of topological conflicts or have a small <phrase>taxon</phrase> overlap, supertree methods of both kinds can propose poorly resolved, hence uninformative, supertrees.   RESULTS To overcome this problem, we propose to infer non-plenary supertrees, i.e. supertrees that do not necessarily contain all the taxa present in the source trees, discarding those whose position greatly differs among <phrase>source trees</phrase> or for which insufficient information is provided. We detail a variant of the PhySIC <phrase>veto</phrase> <phrase>method called</phrase> PhySIC_IST that can infer non-plenary supertrees. PhySIC_IST aims at inferring supertrees that satisfy the same appealing theoretical properties as with PhySIC, while being as informative as possible under this constraint. The informativeness of a supertree is estimated using a variation of the CIC (<phrase>Cladistic</phrase> Information Content) criterion, that <phrase>takes into account</phrase> both the presence of multifurcations and the absence of some taxa. Additionally, we propose a statistical preprocessing step called STC (<phrase>Source Trees</phrase> Correction) to correct the source trees prior to the supertree inference. STC is a liberal step that removes the parts of each source tree that significantly conflict with other <phrase>source trees</phrase>. Combining STC with a <phrase>veto</phrase> method allows an explicit <phrase>trade-off</phrase> between <phrase>veto</phrase> and liberal approaches, tuned by a single parameter.Performing <phrase>large-scale</phrase> simulations, we observe that STC+PhySIC_IST infers much more informative supertrees than PhySIC, while preserving low type I error compared to the well-known <phrase>MRP</phrase> method. Two biological <phrase>case studies</phrase> on animals confirm that the STC preprocess successfully detects anomalies in the source trees while STC+PhySIC_IST provides well-resolved supertrees agreeing with current knowledge in <phrase>systematics</phrase>.   CONCLUSION The <phrase>paper introduces</phrase> and tests two new methodologies, PhySIC_IST and STC, that demonstrate the interest in inferring non-plenary supertrees as well as preprocessing the source trees. An implementation of the methods is available at: http://www.atgc-montpellier.fr/physic_ist/.
The New Face of Design for Manufacturability 232 <phrase>Deep-submicron</phrase> Beol Yield Challenges Infrastructure for Successful Beol <phrase>Yield Ramp</phrase>, Transfer to Manufacturing, and <phrase>Dfm</phrase> Characterization at 65 Nm and Below The challenges presented by <phrase>deep-submicron</phrase> interconnect back-end-of-line (BEOL) integration continue to grow in number, complexity, and required resolution at 90 nm and 65 nm. These challenges are causing industry-wide delays in technology deployment as well as low and often unstable yields. The historically observed improvements in time to successful <phrase>yield ramp</phrase> and final <phrase>manufacturing yield</phrase> as the industry deploys new <phrase>technology nodes</phrase> disappeared at 90 nm. Such improvements have been significant factors in fueling the semiconductor industry's growth. In this article, we describe an infrastructure developed to specifically address BEOL <phrase>deep-submicron</phrase> <phrase>yield-learning</phrase> needs. These include the need to reduce the overall time to results and to provide information that manufacturers can successfully use in process and yield debug, and in higher-<phrase>level design</phrase> models. This infrastructure establishes a needed foundation for <phrase>deep-submicron</phrase> <phrase>technology nodes</phrase> where design and manufacturing share yield entitlement. By building on this foundation , manufacturers can accelerate yield issue detection and correction, and realize yield-aware design flows. The <phrase>International Technology Roadmap for Semiconductors</phrase> 1 provides an excellent summary of the <phrase>confounding</phrase> combination of increased process complexity , reduced <phrase>yield-learning</phrase> cycles, and reduced defect visibility that confronts the industry. Accordingly, a <phrase>yield ramp</phrase> infrastructure must be available that can provide the following characteristics: ■ parts-per-billion sensitivity to key yield-limiting <phrase>topologies</phrase> (not just to purely random defects); ■ identification of nonvisual defects, such as defects in high-<phrase>aspect-ratio</phrase> features and interfacial defects; ■ direct identification of defect location and layer; ■ ability to generate vastly more data than has been possible with traditional technology characterization vehicles, to provide sufficient coverage of ever-expanding design rule sets, interactions with resolution enhancement technologies (RETs), and so on; Editor's Note: Optimized <phrase>test structures</phrase> are necessary to measure and analyze the causes for systematic <phrase>yield loss</phrase>. This article introduces a novel test structure for BEOL—an <phrase>infrastructure IP</phrase> for process monitoring. It also describes a method for characterizing and measuring <phrase>yield ramp</phrase> issues and solutions for improving <phrase>silicon debug</phrase> and <phrase>DFM</phrase>.
Goodness of Fit Tests for Generalized Linear <phrase>Mixed Models</phrase> Goodness of Fit Tests for Generalized Linear <phrase>Mixed Models</phrase> Generalized Linear <phrase>mixed models</phrase> (GLMMs) are widely used for <phrase>regression analysis</phrase> of data, continuous or discrete, that are assumed to be clustered or correlated. Assessing model fit is important for valid inference. We therefore propose a class of <phrase>chi-squared</phrase> goodness-of-fit tests for GLMMs. Our <phrase>test statistic</phrase> is a <phrase>quadratic form</phrase> in the differences between observed values and the values expected under the estimated model in cells defined by a partition of the covariate space. We show that this <phrase>test statistic</phrase> has an asymptotic <phrase>chi-squared distribution</phrase>. We study the power of the test through simulations for two special cases of GLMMs, linear <phrase>mixed models</phrase> (LMMs) and logistic <phrase>mixed models</phrase>. For LMMs, we further derive the analytical power of the test under contiguous local alternatives and compare it with simulated empirical power. Three examples are used to illustrate the proposed test. Dedication To my parents and Jun. <phrase>ii Acknowledgments</phrase> It is a great honor for me to thank all the people who made this thesis possible. First and foremost, I owe my deepest gratitude to my two coadvisors, Slud has been a significant presence in my life. His perpetual energy and his rigor and enthusiasm in research have been motivating me through my whole doctoral study. His insights have strengthened this study significantly. I will always be thankful for his wisdom, knowledge, deep concern and constant encouragement. I would like to show my greatest gratitude and sincere thanks to Dr. Ruth M. <phrase>Pfeiffer</phrase> who offered me the predoctoral training at <phrase>National Cancer Institute</phrase> (<phrase>NCI</phrase>) which led me to the world of <phrase>Biostatistics</phrase> and cancer research. Her rigor and passion on research influenced me a lot. I am deeply indebted to her for her being always ready to help and her precious time on guiding me through my research. I could not have finished this thesis within four years without her invaluable help. She has also made available her support in a number of other ways, such as revising my CV, giving suggestions to my job talk rehearsal. She is the best mentor I have ever met. It has been an honor for me to work with her.
<phrase>Cold Regions</phrase> Issues for <phrase>Off-road Autonomous Vehicles</phrase> <phrase>Cold Regions</phrase> Issues for <phrase>Off-road Autonomous Vehicles</phrase> <phrase>Cold Regions</phrase> Issues for <phrase>Off-road Autonomous Vehicles</phrase> Front cover: Regions of the world considered severely cold (A) and moderately cold (B). Lines are based on Bates and Bilello (1966). World map from DI <phrase>Cartography</phrase> Center (1999). ABSTRACT About half of Earth's land mass experiences mean temperatures below 0<phrase>°C</phrase> during the coldest month. Attendant conditions pose major challenges to the operation of <phrase>off-road autonomous vehicles</phrase>. Low-temperature effects on <phrase>lubricants</phrase>, materials, and batteries can impair a robot's ability to operate at all. Cold starting will be a serious problem if missions require long periods of engine shutdown. Deep <phrase>snow</phrase> can easily immobilize vehicles on terrain that would otherwise pose no problems. Blowing <phrase>snow</phrase> and icing can also degrade the performance of sensors needed for navigation and target detection. Winter operation of passenger vehicles and construction equipment provides guidance to surmount <phrase>cold-regions</phrase> effects on robotic vehicles. This report identifies problems likely to be encountered, simple preventative measures, and references for <phrase>additional information</phrase>. Conditions are sufficiently demanding that <phrase>off-road autonomous vehicles</phrase> must be designed for and tested in <phrase>cold regions</phrase> if they are expected to operate there successfully. <phrase>DISCLAIMER</phrase>: The contents of this report are not to be used for <phrase>advertising</phrase>, publication, or promotional purposes. Citation of trade names does not constitute an official endorsement or approval of the use of such commercial products. All product names and <phrase>trademarks</phrase> <phrase>cited</phrase> are the property of their respective owners. The findings of this report are not to be construed as an official Department of the <phrase>Army</phrase> position unless so designated by other authorized documents.
<phrase>Collaborative Learning</phrase> with the <phrase>Cognitive Tutor</phrase> Algebra 1 <phrase>Collaborative Learning</phrase> with the <phrase>Cognitive Tutor</phrase> Algebra. an Experimental Classroom Study. <phrase>Collaborative Learning</phrase> with the <phrase>Cognitive Tutor</phrase> Algebra 2 Interest in developing improved methods for mathematics instruction has increased since TIMSS and <phrase>PISA</phrase>. In our project, we investigate a new way to promote learning in mathematics: we enhanced the <phrase>Cognitive Tutor</phrase> Algebra, a computer-based <phrase>intelligent tutoring system</phrase> for mathematics at the <phrase>high school</phrase> level, to a <phrase>collaborative learning</phrase> setting. Although the Algebra Tutor has shown to increase learning substantially, there are also several shortcomings. For instance, learning with the Algebra Tutor places emphasis on improving students' <phrase>problem solving</phrase> skills, yet a deep understanding of underlying mathematical concepts is not necessarily achieved. To reduce these shortcomings, we extended the <phrase>learning environment</phrase> to a dyadic setting, thus adding new learning opportunities such as the possibility to mutually elaborate on the learning content. A script was developed to guide students' interaction and to ensure that students profit from these new learning opportunities. Our script followed a general <phrase>jigsaw</phrase>-scheme, i.e. it distributed expertise for the problem between partners and allowed them to prepare individually for the following interaction, thus laying the grounds for effective collaboration. During the collaboration, the script provided additional support: it prompted fruitful interaction and adaptively supported students as they encountered difficulties. Following each problem, it guided the dyads to reflect on their interaction in order to improve subsequent collaboration. In an experimental classroom study taking place over the course of one week, we compared three conditions to evaluate the impact of collaboration in a cognitive tutoring setting: individual learning, unscripted <phrase>collaborative learning</phrase>, and scripted <phrase>collaborative learning</phrase>. After a two-day learning phase, several post tests were administered to assess learning on three levels: reproduction, transfer and future learning. In the collaborative reproduction test, we found a higher need for <phrase>cognitive tutor</phrase> assistance in scripted students with low <phrase>prior knowledge</phrase> (note that this was the first <phrase>post test</phrase> after script support had been removed). In the subsequent individual reproduction test, however, differences between conditions were no longer observed. In fact, the <phrase>learning outcomes</phrase> of students in the two collaborative conditions were comparable to those in the individual condition even though collaborative students had solved fewer problems, i.e. had had less practice, during the learning phase. Analysis of the transfer test did not reveal differences between conditions. Finally, scripted collaboration better prepared students for future <phrase>collaborative learning</phrase> situations as compared to the unscripted collaboration condition. In addition to presenting the study and its results, this <phrase>paper discusses</phrase> the advantages and methodological challenges …
Bacterioplankton Distribution and Production in the Bathypelagic Ocean: Directly Coupled to Particulate Organic Carbon <phrase>Export</phrase>? A recently published evaluation of bacterioplankton abundance and productivity in the bathypelagic <phrase>North Pacific</phrase> suggests that these properties are generally coupled with particulate organic carbon (POC) fluxes. In that analysis, bacterial <phrase>biomass</phrase> and productivity were several-fold greater in <phrase>subarctic</phrase> than subtropical waters, consistent with the basin-scale distribution of POC flux and suggestive of a sinking POC-e DOC-X <phrase>bacteria</phrase> transformation of the carbon. To test this hypothesis, we sought to determine whether the very strong spatial and temporal gradients in POC flux in the <phrase>Arabian Sea</phrase> would force similar deep-ocean gradients in bacterial variables. On both a within-and between-cruise basis, there was variability in <phrase>bacterial abundance</phrase> and <phrase>thymidine</phrase> incorporation in the deep <phrase>Arabian Sea</phrase>, but correspondence was equivocal between these variables and several correlates to <phrase>export</phrase>: flux of <phrase>biogenic</phrase> carbon from the euphotic zone, state of the <phrase>monsoon</phrase>, and proximity to productive coastal <phrase>upwelling</phrase> zones. However, when annual mean <phrase>bacterial abundance</phrase> at 2,000 m was compared with annual POC flux at that depth, a strong correspondence emerged: high annual flux supported high <phrase>bacterial abundance</phrase> (such a correspondence was not found for bacterial productivity). This finding suggests that bathypelagic <phrase>bacterial abundance</phrase> responds to the <phrase>long-term</phrase> mean input of <phrase>organic matter</phrase> and less to episodic inputs. A comparative evaluation of the <phrase>North Pacific</phrase> revealed that although the bathypelagic <phrase>bacteria</phrase> there showed correspondence to deep POC flux, that variable alone would not account for the wide meridional variations in <phrase>bacterial abundance</phrase> that have been reported. The <phrase>nutrition</phrase> and sustenance of organisms in the deep sea has been a classic problem in <phrase>oceanography</phrase> since the discovery of life on the <phrase>sea floor</phrase> by the <phrase>Challenger Expedition</phrase> in 1872-1876 (Mills 1983). With the advent of deep-ocean <phrase>sediment</phrase> traps, <phrase>sedimentation</phrase> of particulate <phrase>organic matter</phrase> became recognized as the principal agent of <phrase>deep-sea</phrase> food supply (<phrase>Moseley</phrase> 1880; Tyler 1988). Subsidy with dissolved <phrase>organic matter</phrase> has long been invoked as a potential nutritional and energy source (J0rgensen 1976). <phrase>Bacteria</phrase> dominate the <phrase>metabolism</phrase> of <phrase>deep waters</phrase> below the euphotic zone (Pomeroy and Johannes 1968), but the mechanism of their <phrase>nutrition</phrase> is not clear. The proximal sources of <phrase>organic matter</phrase> for <phrase>bacteria</phrase> are dissolved, low-molecular-weight substances because bacterial uptake systems are restricted to transporting <phrase>molecules</phrase> <500 Da across cell membranes (<phrase>Williams</phrase> 2000). However, the ultimate sources of the substances actually transported into bacterial cells are not well characterized , even at a crude operational level. Karl et al. (1988) showed that …
Micro <phrase>Gas Bearings</phrase> Fabricated by Deep <phrase>X-ray</phrase> Lithography Micro bearing systems for Micro Electrome-chanical Systems (MEMS) have drawn attention for several decades as critical components for micro rotating machinery. Ideally, frictionless bearings are needed, and in practice, micro <phrase>gas bearings</phrase> approach the ideal. Typically, bearings function as a separate component, assembled onto sliding counterparts. However, in micro scale devices, assembly procedures are known to be very tedious and time consuming. This leads to the pursuit of single material monolithic structures. Critical issues arising from these approaches include: limitation of materials, friction, and reliability, among others. In this paper, new approaches have been pursued. Micro <phrase>gas bearings</phrase> were fabricated as a single component through <phrase>X-ray</phrase> lithography. A <phrase>stainless steel</phrase> gauge pin, machined to ultra precision , was used as a journal shaft. Simple and very easy assembly processes using self-<phrase>aligning</phrase> concepts were developed as an alternative method to conventional assembly. This article presents the design, fabrication, assembly, and testing of micro <phrase>gas bearings</phrase>. 1 Introduction Micro fabrication technology based on <phrase>Integrated Circuit</phrase>-processes is mature in the various fields of MEMS such as optical devices, mechanical RF circuit, sensors, micro fluidics, and bio-devices. However, micro actuators fabricated through conventional micro <phrase>machining</phrase> processes have several limitations: sparse selection of materials, limited functionality, reliability, only a few processes, and structures that are principally 2-D [1–2]. For micro rotating devices such as micro motors, micro engines, and micro <phrase>turbines</phrase> to provide or generate meaningful power to external systems, their structures should be truly 3-dimensional. In addition, <phrase>long-term</phrase> reliability and assembly/packaging are critical issues. L.G. Frechette, et al. [3] demonstrated an electrostatic <phrase>induction motor</phrase> supported by externally pressurized hydrostatic <phrase>gas bearings</phrase>. However, due to limitations arising from <phrase>deep reactive ion etching</phrase> (DRIE) to thick silicon wafers, the nominal gas film thickness and the diameter to length ratio were out of normal design ranges for typical macro scale <phrase>gas bearings</phrase>. This article presents a deep <phrase>X-ray</phrase> lithography <phrase>fabrication process</phrase> for <phrase>newly developed</phrase> <phrase>micro gas</phrase> journal bearings. Using the process, bearings with diameter (500 lm) to length (320 lm) ratio of 0.64 were fabricated. A <phrase>stainless steel</phrase> gauge pin machined to ultra <phrase>high precision</phrase> was the journal shaft; the <phrase>high precision</phrase> maintained a nominal bearing clearance of 1 lm. The journal bearings have several evenly distributed recesses of 2 lm deep along the circumferential direction, to overcome an inherent instability in the <phrase>hydrodynamic</phrase> <phrase>gas bearings</phrase> [4]. <phrase>Thrust</phrase> bearings with 4 <phrase>thrust</phrase> pads, 3 lm high with …
Generating optimization-based <phrase>decision support systems</phrase> This <phrase>paper discusses</phrase> the implementation of opti-mizaiion based DSSs. A n approach is proposed that will enable OR/MS analysts t o develop this kind of system much more eociently than is possible today. The aim is to develop systems with modern GUIs, that interact with DBMSs, and that can be built with little programming effort. Many of the tools required for this approach were developed to support SML, which was chosen to represent the models because it has ample expressive power, completely specified syntax and semantics, total structure/data independence, and lends itself to the surface/<phrase>deep structure</phrase> disiinc-tion, which makes the approach feasible. The approach is being implemented in a <phrase>research project</phrase> funded by the <phrase>Chilean</phrase> Government and several firms and is being tested on problems taken from these firms.
DT - An Automated Theorem Prover for Multiple-Valued First-Order Predicate Logics We describe the automated theorem prover \Deep Thought" (d DT). The prover can be used for arbitrary multiple-valued rst-order logics, provided the connec-tives can be deened by truth tables and the quantiiers are generalizations of the classical universal resp. ex-istential quantiiers. d DT has been tested with many interesting multiple-valued logics as well as classical rst-order <phrase>predicate logic</phrase>. d DT uses a free-variable semantic tableau <phrase>calculus</phrase> with generalized signs. For the existential tableau-rules two liberalized versions are implemented. The system utilizes a static index to control the application of <phrase>axioms</phrase> as wells as the search for applicable rules. A dynamic lemma generation strategy and various heuristics to control the tableau expansion and branch closure are integrated into d DT. Theoretically, contradiction sets of arbitrary size can be discovered to close a branch.
On the performance of <phrase>evolutionary algorithms</phrase> in biomedical keyword clustering In the field of <phrase>life sciences</phrase> it often turns out to be a challenge to quickly find the desired information due to the huge amount of available data. The research area of <phrase>information retrieval</phrase> (IR) addresses this problem and tries to provide suitable solutions. One of the approaches used in IR is query extension based on keyword or document clusters.  In this paper we present a deep analysis of a keyword clustering approach using four different kinds of <phrase>evolutionary algorithms</phrase>, namely <phrase>evolution strategy</phrase> (ES), <phrase>genetic algorithm</phrase> (GA), <phrase>genetic algorithm</phrase> with strict offspring selection (OSGA), and the <phrase>multi-objective</phrase> elitist non-dominated sorting <phrase>genetic algorithm</phrase> (NSGA-II).  We have identified features that characterize solution candidates for the keyword clustering problem, e.g., the number of documents covered and how well the identified clusters of keywords match with the occurrence of keywords in the given set of documents. The use of these features and how <phrase>evolutionary algorithms</phrase> can be used to solve the optimization of keyword clusters is shown in this paper.  To test the here presented approach we used a real world <phrase>data set</phrase> provided within the TREC-9 conference; this <phrase>data collection</phrase> includes information about approximately 36,000 documents collected from the <phrase>PubMed</phrase> database.  In the results section we compare the performance of the here tested <phrase>evolutionary algorithms</phrase> and see that especially ES and NSGA-II produce meaningful results for this documents collection. This approach based on <phrase>evolutionary algorithms</phrase> shall be used further on in automated query extension for biomedical <phrase>information retrieval</phrase> in <phrase>PubMed</phrase>.
Quantification via Probability <phrase>Estimators</phrase> —Quantification is the name given to a novel <phrase>machine learning</phrase> task which deals with correctly estimating the number of elements of one class in a set of examples. The output of a quantifier is a real value; since training instances are the same as a classification problem, a natural approach is to train a classifier and to derive a quantifier from it. Some <phrase>previous works</phrase> have shown that just classifying the instances and counting the examples belonging to the class of interest (classify & count) typically yields bad quantifiers, especially when the class distribution may vary between <phrase>training and test</phrase>. Hence, adjusted versions of classify & count have been developed by using modified thresholds. However, <phrase>previous works</phrase> have explicitly discarded (without a deep analysis) any possible approach based on the probability estimations of the classifier. In this paper, we present a method based on averaging the probability estimations of a classifier with a very simple scaling that does perform reasonably well, showing that probability <phrase>estimators</phrase> for quantification capture a richer view of the problem than methods based on a threshold.
Peer Knowledge Modeling in <phrase>Computer Supported</phrase> <phrase>Collaborative Learning</phrase> Learners benefit from collaboration because it triggers effective interaction processes such as externalization, elicitation and <phrase>negotiation</phrase> of knowledge. In order to communicate effectively, learners need to have a certain representation of their peers' knowledge. We refer to the process of building and maintaining a representation of the peers' knowledge as peer knowledge modeling. The present thesis aim at making contributions to the fields of computer support <phrase>collaborative learning</phrase> and work (CSCL, CSCW) on three main levels. First, as an empirical contribution, we investigate the process of peer knowledge modeling in the context of CSCL. Our main research question inquires the effects of a socio-cognitive support, providing co-learners with cues about their peer's <phrase>prior knowledge</phrase>, on <phrase>collaborative learning</phrase> outcomes and processes. In an empirical study (the KAT experiment), university students (N=64) participated in a remote computer-mediated dyadic learning scenario. They were provided (or not) with a visual representation of their partner's <phrase>prior-knowledge</phrase> level through a Knowledge Awareness Tool (KAT). <phrase>Results showed</phrase> that the KAT enhances co-learners' <phrase>collaborative learning</phrase> gain. This effect appears to be mediated by the positive effect of the KAT on participants' accuracy in estimating their peer's knowledge. Analyses on the process level showed that participants of the KAT condition produce more elaborated utterances. KAT condition dyads' interactions are more focused on knowledge <phrase>negotiation</phrase>, whereas the control condition dyads are mainly focused on task completion. The KAT seems to provide a sensitizing metacognitive support, structuring and regulating the collaboration by helping co-learners to cope with their knowledge gaps and discrepancies. Second, as a methodological contribution, we examine the <phrase>affordance</phrase> of dual <phrase>eye tracking</phrase> techniques as an innovative methodology to investigate, on a deeper level, the socio-<phrase>cognitive processes</phrase> underlying collaboration. We introduce <phrase>DUET</phrase> (DUal <phrase>Eye-Tracking</phrase>), a method using a multimodal technique to collect rich data featuring peers' synchronized gaze patterns, verbal interaction and potentially activities. We examine the main research applications of <phrase>DUET</phrase> and exemplify them with analyses conducted in the context of the KAT experiment. <phrase>DUET</phrase> method appears to be a promising technique to investigate collaborative processes on a <phrase>deep level</phrase>. Finally, as a third computational contribution, we built upon micro-level analyses of the verbal referencing process to introduce and test REGARD, a computational model allowing to automatically detect verbal references and locate the specific object of reference. The results of the test show a reasonably good accuracy of the REGARD algorithm to detect and associate verbal references to …
Some new results in <phrase>model integration</phrase> <phrase>Model Integration</phrase> has been an active field of research. Ideally one would like to apply the same results from <phrase>software engineering</phrase> to construct new models from previously defined and tested ones. As in <phrase>software engineering</phrase> many algorithms and/or procedures written in diflerent languages are assembled together, so there are many models expressed in diflerent modeling languages which we would like to integmte when necessary. Our aim is to show some new results in <phrase>model integration</phrase>, achieved in the framework of Structured Modeling (SM), by loo&g only at the model scheme and to inuestigate the possibility of constructing a unified fmmeworh to integrate algebmic and Structured Models. 1 Introduction In a recent survey paper Geoffrion, [17], states that: " <phrase>model integration</phrase> means difierent things to difier-ent people ". We would like to add that " it means at least one common thing for many people " , that is, a methodology to construct new models by assembling correlated sub-models or to extract sub-models from models already defined. Clearly, there is a strict similarity between <phrase>model integration</phrase> and one of the aspects from <phrase>software engineering</phrase> concerning the production of new code: <phrase>software programs</phrase> are often obtained by assembling well tested routines written in different <phrase>programming languages</phrase>. The computer <phrase>programmer</phrase> has to know how the <phrase>input/output</phrase> communication works for the parameters of the routines, how the <phrase>data structures</phrase> are stored, the compilers' options, etc. However sometimes he doea not need to call the whole routine, because it computes more than what he needs, but only a part of it. Turning to <phrase>Model Integration</phrase>, we can say that a model builder would like to follow more or less the same steps taken by a computer <phrase>programmer</phrase>. In fact, he would like to assemble models defined in the same or different definitional frameworks. Many authors have distinguished between two types of integration. In this paper we adopt the convention proposed by Geoffrion in [17], and call " deep " integration that for which the models to integrate and their results have to belong to the same definitional framework; " functional " integration is when such a constraint does not hold. Examples and results for " deep " integration can be found in Geoffrion and Tsai [15], [25]. In a previous paper, [9], we attempted to see how " deep " integration for Structured Models could be automated. In our approach there is only one concern: …
A C/C++-Based Functional Verification Framework Using the <phrase>SystemC</phrase> Verification Library This paper describes SoCBase-VL, which is a C/C++ based integrated framework for SoC functional verification. It has a layered architecture which provides easier test-bench description, automatic verification of bus interfaces and seamless testbench migration. This framework does not require verification engineers to learn other verification languages as long as they have sufficient knowledge on both C/C++ and <phrase>SystemC</phrase>. We have confirmed its usefulness by applying it to a <phrase>TFT-LCD</phrase> Controller verification. 1 Introduction In the dynamic verification, a set of stimuli is applied to a design and then, its responses are compared to the corresponding correct outputs to check its <phrase>equivalence</phrase> or cor-rectness. This verification approach requires a testbench that generates stimuli and checks correct outputs. Thus, the quality of verification depends on the quality of the test-bench. As the designs are getting more complex, however, the difficulty of authoring the testbenches is continuously growing even more rapidly. The difficulties related to the testbench design can be summarized as follows:  As the number of the state in a component increases linearly, the number of <phrase>test cases</phrase> increases exponentially. Therefore, manual enumeration of each <phrase>test case</phrase> is not feasible.  Several models for a component may be required at different abstraction levels. A testbench for each model should be redesigned to verify the model.  Describing a testbench often requires a deep and thorough understanding on <phrase>domain-specific</phrase> knowledge. e.g. Bus Specification.  A quantitative measure of the quality of verification is needed. Otherwise, the quality of verification tends to depend on that of verification engineers. To alleviate those problems, many researchers and EDA vendors offer tools for testbench authoring [1-5]. The Sys-temC Verification Library (SCV) is an extension of Sys-temC for easier testbench authoring which provides constrained <phrase>randomization</phrase> and transaction level tracing[1]. SoCBase-VL is another extension of the SCV, which additionally provides a layered architecture for easier test-bench description, seamless testbench migration, and an automatic verification of bus interfaces. It also provides the Coverage Monitor Modeling Library (<phrase>CML</phrase>) for functional coverage monitoring. In this paper, we explain our layered testbench architecture in Section 2 and the <phrase>CML</phrase> in Section 3. In Section 4, we briefly introduce how to use our framework through a practical example. The summary and future works are given in Section 5. A H/W component (or a system) can have several abstraction level models: transaction level model, <phrase>RT-level</phrase> model, FPGA prototype and Silicon. We propose a layered …
Npoess <phrase>Soil Moisture</phrase> Satellite <phrase>Data Assimilation</phrase>: Using Windsat Data A four–dimensional coupled atmospheric/land <phrase>data assimilation</phrase> framework is developed using the Regional Atmospheric <phrase>Mesoscale</phrase> <phrase>Data Assimilation</phrase> System (RAMDAS) to retrieve deep <phrase>soil moisture</phrase> profiles. Passive microwave data from <phrase>CORIOLIS</phrase> WindSat is used as a surrogate for future National Polar-orbiting Operational Environmental Satellite System (NPOESS) microwave sensors. Current efforts are focused on the use of the system for a case study occurring in September 2003. The polarization ratio was found to be the most useful function for the observational <phrase>soil moisture</phrase> <phrase>sensitivity analysis</phrase>. Simple <phrase>brightness temperature</phrase> differences indicated <phrase>radiative transfer</phrase> model biases on the order of 5-8 K. Additional <phrase>radiative transfer</phrase> model debiasing studies are thus needed; however, WindSat polarization ratio results are able to demonstrate a strong <phrase>soil moisture</phrase> signal. The calibration and validation approaches for both the output <phrase>soil moisture</phrase> product and related model input <phrase>data sets</phrase> were also tested using the WindSat <phrase>data sets</phrase>. In particular, model output from the <phrase>USAF</phrase> Agricultural Meteorological Model (AGRMET) was analyzed and compared to in situ and satellite <phrase>data sets</phrase>. The performance characteristics of AGRMET were determined both temporally and spatially. Objective methods for performing <phrase>quality control</phrase> have been developed to assure that the first-guess information used within the satellite <phrase>data assimilation</phrase> system is of the highest possible quality. The outcome of this work will be to extend satellite <phrase>soil moisture</phrase> information from the surface to deeper soil levels to more accurately determine its effect upon <phrase>DoD</phrase>-related trafficability, off-road mobility, counter-mine operations, and <phrase>hydrological</phrase> streamflow estimation.
Underwater <phrase>Photogrammetry</phrase> and Object Modeling: A Case Study of Xlendi Wreck in <phrase>Malta</phrase> In this paper we present a <phrase>photogrammetry</phrase>-based approach for <phrase>deep-sea</phrase> underwater surveys conducted from a submarine and guided by <phrase>knowledge-representation</phrase> combined with a logical approach (ontology). Two major issues are discussed in this paper. The first concerns <phrase>deep-sea</phrase> surveys using <phrase>photogrammetry</phrase> from a submarine. Here the goal was to obtain a set of images that completely covered the selected site. Subsequently and based on these images, a <phrase>low-resolution</phrase> 3D model is obtained in real-time, followed by a very <phrase>high-resolution</phrase> model produced back in the laboratory. The second issue involves the extraction of known artefacts present on the site. This aspect of the research is based on an a priori representation of the knowledge involved using systematic reasoning. Two parallel processes were developed to represent the <phrase>photogrammetric</phrase> process used for <phrase>surveying</phrase> as well as for identifying <phrase>archaeological</phrase> artefacts visible on the <phrase>sea floor</phrase>. Mapping involved the use of the CIDOC-<phrase>CRM</phrase> system (International Committee for Documentation (CIDOC)-Conceptual Reference Model)-This is a system that has been previously utilised to in the heritage sector and is largely available to the established scientific community. The proposed theoretical representation is based on procedural attachment; moreover, a strong link is maintained between the <phrase>ontological</phrase> description of the modelled concepts and the <phrase>Java programming language</phrase> which permitted 3D structure estimation and modelling based on a set of oriented images. A very recently discovered <phrase>shipwreck</phrase> acted as a testing ground for this project; the Xelendi <phrase>Phoenician</phrase> <phrase>shipwreck</phrase>, found off the <phrase>Maltese</phrase> coast, is probably the oldest known <phrase>shipwreck</phrase> in the <phrase>western Mediterranean</phrase>. The approach presented in this paper was developed in the scope of the GROPLAN project (Généralisation du Relevé, avec Ontologies et Photogrammétrie, pour l'Archéologie Navale et Sous-marine). Financed by the <phrase>French</phrase> National Research Agency (<phrase>ANR</phrase>) for four years, this project associates two <phrase>French</phrase> research laboratories, an industrial partner, the University of <phrase>Malta</phrase>, and Texas A &amp; M University.
A Kind of <phrase>Low-cost</phrase> Non-intrusive Autonomous Fault Emulation System SRAM-Filed Programmable Gate Arrays (FPGA) have become one of the most important carriers of digital electronic system because of its many inborn advantages. However, as manufacture of <phrase>Integrated Circuit</phrase> evolves towards Very <phrase>Deep Sub-Micron technology</phrase>, FPGA designers must be careful of circuit's <phrase>Single Event Upset</phrase> (SEU) susceptibility when used in hostile environment, such as <phrase>avionics</phrase> and space applications where reliability is vital. We proposed a SEU-fault emulation platform to evaluate circuit's SEU mitigation performance. The platform does not need any external circuit or micro controller to manage fault emulation process compared with existing approach. Source codes of <phrase>Circuit Under Test</phrase> (CUT) do not need to be modified or intruded with any component. It is a non-intrusive testing. Communication between host-computer and emulation board is minimized to accelerate <phrase>fault injection</phrase> speed. Experimental result shows that a single fault injecting (including Multi-Bits-Upset) only costs 29us. A circuit state reloading technology is exploited to increase emulation efficiency. Moreover, in the field of evolvable hardware, genetic operations can be reconfigured and its fitness can be evaluated on-line using the proposed fast dynamic reconfiguration method, which is useful for implementing self-repair and self-evolutionary hardware.
A <phrase>Situation Awareness</phrase> Assistant for Human <phrase>Deep Space Exploration</phrase> This paper presents the <phrase>development and testing</phrase> of a Virtual Camera (VC) system to improve <phrase>astronaut</phrase> and mission operations <phrase>situation awareness</phrase> while exploring other planetary bodies. In this embodiment, the VC is implemented using a <phrase>tablet</phrase>-based computer system to navigate through interactive database application. It is claimed that the advanced interaction media capability of the VC can improve <phrase>situation awareness</phrase> as the distribution of human <phrase>space exploration</phrase> roles change in <phrase>deep space exploration</phrase>. The VC is being developed and tested for usability and capability to improve <phrase>situation awareness</phrase>. Work completed thus far as well as what is needed to complete the project will be described. Planned testing will also be described. 1 INTRODUCTION The Virtual Camera (VC) for human <phrase>deep space exploration</phrase> is a virtual assistant that is being developed for use by <phrase>astronauts</phrase> and other associated mission operations personnel as they use human-piloted rovers to explore the surface of other <phrase>planets</phrase> [1]. The VC concept is based on incremental upgrading of a suboptimal 3-D geographical and geological database. It is an interactive window on the world as we know it at the time it is being used. Such an interactive window enables the user to maintain better situational awareness, to navigate and further explore space. It can be used for training , <phrase>data analysis</phrase> and augmentation of actual surface exploration. The VC for human <phrase>space exploration</phrase> was originally described for use by <phrase>astronauts</phrase> navigating a surface exploration rover on a remote body such as the <phrase>Moon</phrase> [1]. Further analysis has indicated that such an interactive window to data in multiple dimensions has many other applications in domains such as <phrase>aviation</phrase>, medicine, control systems and <phrase>finance</phrase>. This paper presents a <phrase>tablet</phrase> version of the VC for <phrase>deep space exploration</phrase>
<phrase>Model Checking</phrase> Large <phrase>Network Protocol</phrase> Implementations Network protocols must work. The effects of protocol specification or implementation errors range from reduced performance, to security breaches, to bringing down entire networks. However, network protocols are difficult to test due to the exponential size of the state space they define. Ideally, a protocol implementation must be validated against all possible events (packet arrivals, packet losses, timeouts, etc.) in all possible protocol states. Conventional means of testing can explore only a minute fraction of these possible combinations. This paper focuses on how to effectively find errors in large <phrase>network protocol</phrase> implementations using <phrase>model checking</phrase>, a <phrase>formal verification</phrase> technique. <phrase>Model checking</phrase> involves a systematic exploration of the possible states of a system, and is well-suited to finding intricate errors lurking deep in exponential <phrase>state spaces</phrase>. Its primary limitation has been the effort needed to use it on software. The primary contribution of this paper are novel techniques that allow us to model check complex, <phrase>real-world</phrase>, well-tested protocol implementations with reasonable effort. We have implemented these techniques in CMC, a C <phrase>model checker</phrase> [30] and applied the result to the Linux <phrase>TCP/IP</phrase> implementation, finding four errors in the protocol implementation.
Transformation-Invariant Convolutional Jungles Many <phrase>Computer Vision</phrase> problems arise from <phrase>information processing</phrase> of data sources with nuisance variances like scale, orientation, contrast, perspective foreshortening or – in <phrase>medical imaging</phrase> – <phrase>staining</phrase> and local <phrase>warping</phrase>. In most cases these variances can be stated a priori and can be used to improve the generalization of recognition algorithms. We propose a novel supervised <phrase>feature learning</phrase> approach, which efficiently extracts information from these constraints to produce interpretable, transformation-<phrase>invariant features</phrase>. The proposed method can incorporate a large class of transformations , e.g., shifts, rotations, change of scale, morphological operations, non-linear distortions, photometric transformations, etc. These features boost the discrimination power of a novel <phrase>image classification</phrase> and segmentation method, which we call Transformation-Invariant Convolu-tional Jungles (TICJ). We test the algorithm on two benchmarks in <phrase>face recognition</phrase> and <phrase>medical imaging</phrase>, where it achieves state of the art results, while being computation-ally significantly more efficient than <phrase>Deep Neural Networks</phrase>.
Meeting medical terminology needs-the ontology-enhanced Medical Concept Mapper This paper describes the <phrase>development and testing</phrase> of the Medical Concept Mapper, a tool designed to facilitate access to online medical information sources by providing users with appropriate medical search terms for their personal queries. Our system is valuable for patients whose knowledge of medical vocabularies is inadequate to find the desired information, and for medical experts who search for information outside their field of expertise. The Medical Concept Mapper maps <phrase>synonyms</phrase> and semantically related concepts to a user's query. The system is unique because it integrates our <phrase>natural language processing</phrase> tool, i.e., the <phrase>Arizona</phrase> (<phrase>AZ</phrase>) Noun Phraser, with human-created ontologies, the Unified Medical Language System (<phrase>UMLS</phrase>) and <phrase>WordNet</phrase>, and our computer generated Concept Space, into one system. Our unique contribution results from combining the <phrase>UMLS</phrase> Semantic Net with Concept Space in our <phrase>deep semantic</phrase> parsing (DSP) algorithm. This algorithm establishes a medical query context based on the <phrase>UMLS</phrase> Semantic Net, which allows Concept Space terms to be filtered so as to isolate related terms relevant to the query. We performed two user studies in which Medical Concept Mapper terms were compared against human experts' terms. We conclude that the <phrase>AZ</phrase> Noun Phraser is well suited to extract medical phrases from user queries, that <phrase>WordNet</phrase> is not well suited to provide strictly medical <phrase>synonyms</phrase>, that the <phrase>UMLS</phrase> Metathesaurus is well suited to provide medical <phrase>synonyms</phrase>, and that Concept Space is well suited to provide related medical terms, especially when these terms are limited by our DSP algorithm.
Dependable Computing in the Context of Mobility, Nomadicity, Ubiquity, and Pervasiveness Why do software projects fail? : reasons and a solution using a Bayesian classifier to predict potential risk (PDF) p. 4 Sigma : a fault-tolerant <phrase>mutual exclusion</phrase> algorithm in dynamic <phrase>distributed systems</phrase> subject to process crashes and memory losses p. 7 Intersecting sets : a basic abstraction for asynchronous agreement problems p. 15 Decision optimal early-stopping k-set agreement in synchronous systems prone to send omission failures p. 23 Privacy-preserving <phrase>Bayesian network</phrase> structure learning on <phrase>distributed heterogeneous</phrase> data p. 31 Simultaneous simulation of alternative system configurations <phrase>Shravan</phrase> Gaonkar p. 41 Availability assessment of <phrase>SunOS</phrase>/<phrase>Solaris</phrase> Unix systems based on Syslogd and wtmpx log files : a case study p. 49 On-chip debugging-based fault emulation for robustness evaluation of <phrase>embedded software</phrase> components p. 57 <phrase>Bayesian networks</phrase> modeling for software inspection effectiveness p. 65 Application-based metrics for strategic placement of detectors p. 75 A hardware approach to concurrent <phrase>error detection</phrase> capability enhancement in <phrase>COTS</phrase> processors p. 83 Optimal <phrase>fault-tolerant</phrase> routing scheme for generalized <phrase>hypercube</phrase> p. 91 <phrase>High-order</phrase> syndrome testing for <phrase>VLSI circuits</phrase> p. 101 A new BIST solution for system-on-chip p. 109 A failure-aware model for estimating and analyzing the efficiency of <phrase>Web services</phrase> compositions p. 114 Code design and decoding methods for burst error locating codes p. 125 An evaluation of the virtual router redundancy protocol extension with <phrase>load balancing</phrase> p. 133 Formal development of software for tolerating <phrase>transient faults</phrase> p. 140 On the fully-informed communication-induced checkpointing protocol p. 151 Optimal choice of checkpointing interval for <phrase>high availability</phrase> p. 159 An improved scheme of index-based checkpointing p. 167 Compression/scan co-design for reducing <phrase>test data volume</phrase>, scan-in <phrase>power dissipation</phrase> and test application time p. 175 Partitioned cache shadowing for <phrase>deep sub-micron</phrase> (DSM) regime p. 183 Proxy <phrase>cryptography</phrase> for secure inter-domain information exchanges p. 193 <phrase>Anomaly detection</phrase> with high deviations for system security p. 200 A multi-faceted approach towards spam-resistible <phrase>mail</phrase> p. 208 A virtual modeling and a fast algorithm for grid service reliability p. 219 Research on architecture and design principles of <phrase>COTS</phrase> components based generic <phrase>fault-tolerant</phrase> computer p. 227 Bi-objective model for <phrase>test-suite</phrase> reduction based on modified condition/decision coverage p. 235 A reliable routing algorithm based on fuzzy applicability of F sets in <phrase>MANET</phrase> p. 245 An efficient approach to tolerating route errors in <phrase>mobile ad hoc networks</phrase> p. 250 A novel approach to kernel construction of China Bridge CA p. 258
Test-<phrase>pattern Selection</phrase> for Screening <phrase>Small-delay Defects</phrase> in Very-deep Submicron <phrase>Integrated Circuits</phrase> † —<phrase>Timing-related defects</phrase> are major contributors to test escapes and in-field reliability problems for very-deep submicron <phrase>integrated circuits</phrase>. Small <phrase>delay variations</phrase> induced by crosstalk, <phrase>process variations</phrase>, <phrase>power-supply noise</phrase>, as well as <phrase>resistive opens</phrase> and shorts can potentially cause <phrase>timing failures</phrase> in a design, thereby leading to quality and reliability concerns. We present a test-grading technique that uses the method of output deviations for screening <phrase>small-delay defects</phrase> (SDDs). A new gate-delay defect <phrase>probability measure</phrase> is defined to model <phrase>delay variations</phrase> for <phrase>nanometer technologies</phrase>. The proposed technique intelligently selects the best set of patterns for SDD detection from an n-detect pattern set generated using timing-unaware <phrase>automatic test-pattern generation</phrase> (ATPG). It offers significantly lower <phrase>computational complexity</phrase> and it excites a larger number of long paths compared to a commercial <phrase>timing-aware ATPG</phrase> tool. Our results also show that, for the same <phrase>pattern count</phrase>, the selected patterns provide more effective coverage ramp-up than <phrase>timing-aware ATPG</phrase> and a recent <phrase>pattern-selection</phrase> method for random <phrase>small-delay defects</phrase> potentially caused by <phrase>resistive shorts</phrase>, <phrase>resistive opens</phrase>, and <phrase>process variations</phrase>.
VIP - an input <phrase>pattern generator</phrase> for indentifying critical <phrase>voltage drop</phrase> for <phrase>deep sub-micron</phrase> designs We present a novel input <phrase>pattern generator</phrase> for dynamic power network simulation. The obtained patterns successfully identia critical <phrase>voltage drop</phrase> areas for a set of industrial designs, which are dificult to be found using functional vectors. The <phrase>search engine</phrase> of the <phrase>pattern generator</phrase> for <phrase>worst-case</phrase> IR <phrase>voltage drop</phrase> is based on the <phrase>multi-objective</phrase> <phrase>genetic algorithm</phrase>. To achieve high coverage for critical <phrase>voltage drop</phrase> cells, we propose to model the search criteria into the maximum weighted matching of a <phrase>bipartite graph</phrase>, and guide the search direction according to the matching results. Experimental results show that, compared with the other approaches, our patterns give a higher coverage of critical <phrase>voltage drop</phrase> cells. 1. Introduction For designers of today's <phrase>high performance</phrase> and complexity ICs, the accurate and efficient analysis for power net <phrase>voltage drop</phrase> is very important. Excessive <phrase>voltage drop</phrase> increases the transistor and gate delays, which results in unpredictable performance or performance failing to meet original design goal. To identify this problem, dynamic simulation is needed for providing the profile of the <phrase>voltage drop</phrase>. Therefore, generating <phrase>high-quality</phrase> input patterns for simulation of the voltage drops has become a necessary step in the entire <phrase>design cycle</phrase>. Recently, several <phrase>Genetic-Algorithm</phrase>-based (GA-based) techniques have been proposed to generate the input patterns for identifying the maximum instantaneous current [4], maximum <phrase>power dissipation</phrase> [ 11, and maximum <phrase>voltage drop</phrase> [5]. Through iteratively generating the new test patterns for simulation based on the " good " property of the current patterns, they produce tight lower bounds for these problems. In such a way, however, certain functional blocks whose current has little contribution to the maximum total current or maximum
Evaluating <phrase>remotely sensed</phrase> rainfall estimates using nonlinear <phrase>mixed models</phrase> and geographically weighted regression This article evaluates an infrared-based satellite algorithm for rainfall estimation, the <phrase>Convective</phrase> <phrase>Strat</phrase>-iform technique, over Mediterranean. Unlike a large number of works that evaluate <phrase>remotely sensed</phrase> estimates concentrating on global measures of accuracy, this work examines the relationship between <phrase>ground truth</phrase> and satellit0e derived data in a local scale. Hence, we examine the fit of <phrase>ground truth</phrase> and <phrase>remotely sensed</phrase> data on a widely adopted <phrase>probability distribution</phrase> for rainfall totals – the mixed <phrase>log-normal distribution</phrase> – per measurement location. Moreover, we test for spatial nonstationarity in the relationship between in situ observed and satellite-estimated rainfall totals. The former investigation takes place via using recent algorithms that estimate nonlinear <phrase>mixed models</phrase> whereas the latter uses geographically weighted regression. <phrase>Remotely sensed</phrase> information from satellites, having a high spatial coverage and high temporal sampling, can play a key role in monitoring precipitation in <phrase>flood</phrase>-prone regions, sea precipitation, and other <phrase>extreme weather</phrase> events. Such information is of particular importance in areas with sparse <phrase>rain gauge</phrase> and precipitation radar networks from which reliable real time assessments of precipitation can be obtained. An area where the latter argument applies is the <phrase>Mediterranean basin</phrase>; its climate is known for its variety and variability, due to the surrounding <phrase>orography</phrase>, to the relative <phrase>high temperature</phrase> of the sea and to the different origin and <phrase>physical characteristics</phrase> of the air masses. There is a paucity of dense <phrase>rain gauge</phrase> networks or precipitation radar networks due to the presence of the <phrase>Mediterranean Sea</phrase> and to the complex <phrase>topography</phrase>. In addition, <phrase>rain gauge</phrase> observations are not generally available in real time in many regions, and missing reports and grossly erroneous reports occur in cases of extremely heavy rainfall and in regions of steep terrain. A number of techniques have been developed to indirectly estimate rainfall using visible (VIS) and infrared (IR) satellite data. Most of these methods are based on the notion that deep convec-tive clouds might produce more rain and on operational findings, which show that regions of rainfall tend to be correlated with bright (VIS), cold (IR) clouds. When IR satellite data of high spatial and <phrase>temporal resolution</phrase> became available, precipitation was correlated to the cloud-top temperature (CTT) and the relationships used in the satellite techniques were redefined as a function of CTT. Numerous new precipitation estimation algorithms have been developed that use IR data as the only data input; i. One disadvantage of VIS/IR techniques for estimating precipitation rates …
On the Complexity of Nonrecursive XQuery and Functional Query Languages on Complex Values This article studies the complexity of evaluating functional query languages for complex values such as <phrase>monad algebra</phrase> and the <phrase>recursion</phrase>-free fragment of XQuery. We show that <phrase>monad algebra</phrase>, with equality restricted to atomic values, is complete for the class TA[2<sup><i>O</i>(<i>n</i>)</sup>, <i>O</i>(<i>n</i>)] of problems solvable in linear exponential time with a linear number of alternations if the query is assumed to be part of the input. The monotone fragment of <phrase>monad algebra</phrase> with atomic value equality but without <phrase>negation</phrase> is NEXPTIME-complete. For <phrase>monad algebra</phrase> with deep value equality, that is, equality of complex values, we establish TA[2<sup><i>O</i>(<i>n</i>)</sup>, <i>O</i>(<i>n</i>)] lower and exponential-space upper bounds. We also study a fragment of XQuery, <phrase>Core XQuery</phrase>, that seems to incorporate all the features of a <phrase>query language</phrase> on complex values that are traditionally deemed essential. A close connection between <phrase>monad algebra</phrase> on lists and <phrase>Core XQuery</phrase> (with &#8220;child&#8221; as the only axis) is exhibited. The two languages are shown expressively equivalent up to representation issues. We show that <phrase>Core XQuery</phrase> is just as hard as <phrase>monad algebra</phrase> with respect to query and combined complexity. As <phrase>Core XQuery</phrase> is NEXPTIME-hard, the best-known techniques for processing such problems require exponential amounts of <phrase>working memory</phrase> and doubly exponential time in the worst case. We present a property of queries---the lack of a certain form of composition---that virtually all <phrase>real-world</phrase> XQueries have and that allows for query evaluation in <phrase>PSPACE</phrase> and thus singly exponential time. Still, we are able to show for an important special case---<phrase>Core XQuery</phrase> with equality testing restricted to atomic values---that the composition-free language is just as expressive as the language with composition. Thus, under widely-held complexity-theoretic assumptions, the language with composition is an exponentially more succinct version of the composition-free language.
<phrase>Ion Thruster</phrase> Plume Plasma Interactions in the Solar Wind the <phrase>Ion Thruster</phrase> Plasma Near-spacecraft Plume Environment 3. Far-field Plume-<phrase>solar Wind</phrase> Interactions the Solar Wind Plasma Formulation and Approach Ion propulsion will be used for the first time on an in-terplanetary spacecraft, <phrase>Deep Space</phrase> One (DS1), scheduled for launch in July 1998. A primary objective of New <phrase>Millennium</phrase> DS 1 is to flight validate solar electric propulsion (SEP) for interplanetary missions. The cruise phase of the mission will characterize the life and performance of a 30 cm <phrase>xenon</phrase> <phrase>ion thruster</phrase> and determine how its operation may affect spacecraft payloads and critical subsystem... Effects introduced by the operation of the <phrase>ion thruster</phrase> have long raised both technology and science concerns. The technology concerns include plutne backflow contamination and spacecraft interactions with the induced plasma environment. Backflow contamination can lead to effluent deposition that can affect thermal control surfaces, optical sensors, solar arrays, science instru-mentation, and communications. The induced plasma environment will modify spacecraft charging characteristics , and can lead to plasma interactions with the solar array. The science concerns relate to plasma nleasrrre-rnents. The plume will modify the properties of the solar wind flowing around the spacecraft and may contaminate measurements of the <phrase>ambient</phrase> plasma and <phrase>magnetic fields</phrase>. As ion thrusters are designed to operate for long periods of time, these effects need to be carefully assessed. The interactions induced by <phrase>ion thruster</phrase> plumes have been studied for some time. Due to the complexity of the problem, the difhculty of matching space conditions in a laboratory, and the lack of opportunities to flight test ion thrusters, computer particle simulations have recently become the best means to study this problem. Samania Roy et aL[1996a,1996b] used hybrid PIC sinl-ulations to model the far-downstream region and study charge exchange ion backflow. Wang and Brophy[1995] developed full particle and hybrid PIC-<phrase>MCC</phrase> models of single and multiple thruster plumes and studied the effects of <phrase>ambient</phrase> environment on plasma plumes. Wang et al. [1996] have carried out 3-D simulations of <phrase>ion thruster</phrase> plume environments using parameters similar to those of the NS'I'AR (NASA Solar-Electric Propulsion Technology Application Readiness) thruster to be used on DS1. All studies on this subject so far have concentrated on charge-exchange ion interactions near the spacecraft. There have been no studies concerning other aspects of plume interactions, such as <phrase>ion thruster</phrase> operation in the solar wind environment and plume-<phrase>solar wind</phrase> interactions. For an interplanetary spacecraft such w the DS-1, the <phrase>ion thruster</phrase> operates in the solar wind, which is a tenuous, relatively hot plasma with a high flow speed and a frozen-in …
Decoding the perioperative process breakdowns: A theoretical model and implications for system design BACKGROUND Breakdowns in communication and coordination are situations of mismatch between actual and expected conditions in joint activities. Breakdowns have been identified as the leading cause of adverse events in healthcare, especially in the Operating Room environment. As a result, researchers have started to examine breakdowns in healthcare as emergent dynamics of teamwork. However, the occurrence and consequences of breakdowns related to inter-team processes are yet to be addressed at a fine level of detail. In this paper we seek understanding of breakdowns at the systemic level, and its relevance to design.   OBJECTIVES The objective of this study is to bring forward an in-depth understanding of the impact of breakdowns on the surgical process by expanding the focus of analysis beyond teamwork dynamics, to the level of hospital system processes. This study also aims to examine the implications of such understanding of breakdowns for the design of clinical systems.   METHODS Properties of breakdowns and repairs were inductively derived, and developed into a formal coding scheme, which was applied over a set of observed breakdowns from an elective surgery unit in a <phrase>North American</phrase> hospital. Systematic <phrase>content analysis</phrase> was employed to quantify qualitative data spanning 79 h of observations, followed by statistical hypotheses testing for relationships between variables of breakdowns and repairs.   MEASURES Breakdown type, theme, tangibility, coordination scale, breakdown lifetime, repair strategy, and repair cost.   RESULTS The <phrase>results reveal</phrase> that properties of breakdowns determine properties of repairs. The majority of breakdowns were outside the scope of teamwork--at the inter-team coordination level. The results also demonstrate that breakdowns usually propagate downstream in the surgical process, affecting the work of multiple teams, and the longer they propagate the higher the communication cost associated with the respective repair. The implications are two-fold: in terms of theory we develop a <phrase>conceptual framework</phrase> of breakdowns in perioperative work, and in terms of system design we propose a design framework informed by the acquired understanding of breakdowns.   CONCLUSIONS This study achieved an initial understanding of the deep features of breakdowns from a process-oriented perspective, which allowed us to build the groundwork for a theoretical model of breakdowns in perioperative activities and to propose a design approach that tackles breakdowns during <phrase>early stages</phrase> of system development. The direct association between breakdowns and repairs can be exploited in both IT-system design and organizational design. The patterns of repair work can inform design so as to provide clinicians with the types of information that will prevent breakdowns from occurring or to mitigate the impact of breakdowns. The <phrase>results reveal</phrase> that preventing breakdown propagation should be a prime target in surgical applications design.
Experience in <phrase>critical path selection</phrase> for <phrase>deep sub-micron</phrase> delay test and timing validation <phrase>Critical path selection</phrase> is an indispensable step for AC delay test and timing validation. Traditionally, this step relies on the construction of a set of worse-case paths based upon discrete <phrase>timing models</phrase>. However, the assumption of discrete <phrase>timing models</phrase> can be invalidated by <phrase>timing defects</phrase> and <phrase>process variation</phrase> in the deep sub-micron domain, which are often continuous in nature. As a result, critical paths defined in a traditional <phrase>timing analysis</phrase> approach may not be truly critical in reality. In this paper, we propose using a statistical delay evaluation framework for estimating the quality of a path set. Based upon the new framework, we demonstrate how the traditional definition of a <phrase>critical path</phrase> set may deviate from the true <phrase>critical path</phrase> set in the deep sub-micron domain. To remedy the problem, we discuss improvements to the existing <phrase>path selection</phrase> strategies by including new objectives. We then compare statistical approaches with <phrase>traditional approaches</phrase> based upon experimental analysis of both <phrase>defect-free</phrase> and defect-injected cases.
[Feeling for affective <phrase>psychosis</phrase>]. The paper presents new research results on the <phrase>pathogenesis</phrase> of depression and mania and future perspectives on diagnosis and treatment. On the one hand, we know that depression most often develops in the <phrase>interplay</phrase> between genes and the environment; and, on the other hand, that depression and mania may be induced suddenly by <phrase>deep brain stimulation</phrase> of the subthalamic areas. Ongoing studies aim to identify <phrase>biomarkers</phrase> for depression and mania, and results from <phrase>recent research</phrase> suggest that <phrase>neuroticism</phrase>, abnormal response to the DEX-<phrase>CRH</phrase> test, increased frontotemporal serotonine 2A binding, abnormal emotional processing and deceased <phrase>executive function</phrase> may be candidates for such <phrase>biomarkers</phrase>. These <phrase>biomarkers</phrase> may help to improve diagnosis and treatment in the future.
<phrase>Experimental Evaluation</phrase> of Automatic Hint Generation for a Logic Tutor We have augmented the Deep Thought logic tutor with a Hint <phrase>Factory</phrase> that generates data-driven, context-specific hints for an existing <phrase>computer aided</phrase> instructional tool. We investigate the impact of the Hint Factory's <phrase>automatically generated</phrase> hints on educational outcomes in a switching replications experiment that shows that hints help students persist in a <phrase>deductive</phrase> logic proofs tutor. Three instructors taught two semester-long courses, each teaching one semester using a logic tutor with hints, and one semester using the tutor without hints, controlling for the impact of different instructors on course outcomes. Our results show that students in the courses using a logic tutor augmented with <phrase>automatically generated</phrase> hints attempted and completed significantly more logic proof problems, were less likely to abandon the tutor, performed significantly better on a <phrase>post-test</phrase> implemented within the tutor, and achieved higher grades in the course.
A 40 nm 0.32 V 3.5 MHz 11T single-ended bit-interleaving subthreshold SRAM with data-aware write-assist This paper presents a new bit-interleaving 11T subthreshold SRAM cell with Data-Aware Power-Cutoff (DAPC) Write-assist to mitigate the leakage and variation and improve the Write-ability in deep sub-100nm technology. Measurement results from a 4 Kb <phrase>test chip</phrase> implemented in 40 nm <phrase>General Purpose</phrase> (40GP) <phrase>CMOS technology</phrase> operates for V<sub>DD</sub> down to 0.32 V (~0.69X of <phrase>threshold voltage</phrase>) with V<sub>DDMIN</sub> limited by Read operation. The measured maximum operation frequency is 3.5 MHz (16.5 MHz) at 0.32 V (0.38 V) with total <phrase>power consumption</phrase> of 15.2 <phrase>&mu</phrase>;W (27.2 <phrase>&mu</phrase>;W) at 25 &#176;C.
Teaching <phrase>data structures</phrase> with BeSocratic (abstract only) <phrase>Data structures</phrase> are one of the fundamental concepts that all computer scientist students must learn if they are to succeed in their careers. Therefore, it is important to develop and assess questions targeted at improving the teaching of <phrase>data structures</phrase>. Unfortunately, research suggests that <phrase>multiple choice</phrase> or matching questions cannot be used to properly assess <phrase>deep knowledge</phrase> on a subject [1,2,3,4]. Students can often guess their way to the correct answer. We believe that students must construct these structures instead of simply identifying them. However, analyzing many hand-drawn <phrase>data structures</phrase> is time-consuming for large class sizes. This <phrase>poster</phrase> describes a <phrase>web-based</phrase> software tool, <i>BeSocratic</i>, designed to facilitate interactivity in a <phrase>data structures</phrase> course. <i>BeSocratic</i> allows students to build <phrase>data structures</phrase> intuitively using a combination of <phrase>handwriting recognition</phrase> and gestures. Using <i>BeSocratic</i>, instructors can create intelligent tutors that teach students to construct various <phrase>data structures</phrase>. These tutors are able to identify problems and provide multi-tiered feedback to students. Furthermore, <i>BeSocratic</i> records each action a student makes, so it may be replayed and visualized to gain deeper insights into how students construct <phrase>data structures</phrase> and complete algorithms. We have created and pilot-tested a <i>BeSocratic</i> activity, which teaches students how to construct splay trees.
Automated segmentation of <phrase>basal ganglia</phrase> and <phrase>deep brain structures</phrase> in MRI of <phrase>Parkinson's disease</phrase> PURPOSE <phrase>Template-based</phrase> segmentation techniques have been developed to facilitate the accurate targeting of <phrase>deep brain structures</phrase> in patients with movement disorders. Three <phrase>template-based</phrase> brain MRI segmentation techniques were compared to determine the best strategy for <phrase>segmenting</phrase> the <phrase>deep brain structures</phrase> of <phrase>patients with Parkinson's disease</phrase>.   METHODS <phrase>T1-weighted</phrase> and T2-weighted <phrase>magnetic resonance</phrase> (MR) image templates were created by averaging <phrase>MR images</phrase> of 57 <phrase>patients with Parkinson's disease</phrase>. Twenty-four <phrase>deep brain structures</phrase> were manually segmented on the templates. To validate the <phrase>template-based</phrase> segmentation, 14 of the 24 <phrase>deep brain structures</phrase> from the templates were manually segmented on 10 MR scans of Parkinson's patients as a <phrase>gold standard</phrase>. We compared the manual segmentations with three methods of automated segmentation: two registration-<phrase>based approaches</phrase>, automatic nonlinear <phrase>image matching</phrase> and anatomical labeling (ANIMAL) and symmetric image normalization (SyN), and one patch-label fusion technique. The automated labels were then compared with the manual labels using a <phrase>Dice</phrase>-kappa metric and center of gravity. A Friedman test was used to compare the <phrase>Dice</phrase>-kappa values and paired t tests for the center of gravity.   RESULTS The Friedman test showed a significant difference between the three methods for both thalami (p < 0.05) and not for the subthalamic nuclei. Registration with ANIMAL was better than with SyN for the left thalamus and was better than the patch-<phrase>based method</phrase> for the right thalamus.   CONCLUSION Although <phrase>template-based</phrase> approaches are the most used techniques to segment <phrase>basal ganglia</phrase> by <phrase>warping</phrase> onto <phrase>MR images</phrase>, we found that the patch-<phrase>based method</phrase> provided similar results and was less time-consuming. Patch-<phrase>based method</phrase> may be preferable for the subthalamic nucleus segmentation in patients with Parkinson's disease.
Deep cerebellar neurons mirror the <phrase>spinal</phrase> cord's gain to implement an inverse controller Smooth and <phrase>coordinated</phrase> motion requires precisely timed muscle <phrase>activation patterns</phrase>, which due to biophysical limitations, must be predictive and executed in a <phrase>feed-forward</phrase> manner. In a previous study, we tested Kawato's original proposition, that the <phrase>cerebellum</phrase> implements an inverse controller, by mapping a multizonal microcomplex's (MZMC) <phrase>biophysics</phrase> to a joint's inverse <phrase>transfer function</phrase> and showing that inferior olivary neuron may use their intrinsic oscillations to mirror a joint's oscillatory dynamics. Here, to continue to validate our mapping, we propose that climbing fiber input into the deep cerebellar nucleus (DCN) triggers rebounds, primed by <phrase>Purkinje cell</phrase> inhibition, implementing gain on IO's signal to mirror the <phrase>spinal cord</phrase> reflex's gain thereby achieving inverse control. We used biophysical modeling to show that <phrase>Purkinje cell</phrase> inhibition and climbing fiber excitation interact in a multiplicative fashion to set DCN's rebound strength; where the former <phrase>primes</phrase> the cell for rebound by deinactivating its T-type <phrase>Ca2</phrase>(+) channels and the latter triggers the channels by rapidly depolarizing the cell. We combined this result with our <phrase>control theory</phrase> mapping to predict how experimentally injecting current into DCN will affect overall motor output performance, and found that injecting current will proportionally scale the output and unmask the joint's natural response as observed by motor output ringing at the joint's <phrase>natural frequency</phrase>. Experimental verification of this prediction will lend support to a MZMC as a joint's inverse controller and the role we assigned underlying biophysical principles that enable it.
<phrase>Modeling Language</phrase> Vagueness in Privacy Policies Using Deep Neural Networks Website privacy policies are too long to read and difficult to understand. The over-sophisticated language undermines the effectiveness of privacy notices. People become less willing to share their <phrase>personal information</phrase> when they perceive the <phrase>privacy policy</phrase> as vague. The goal of this paper is to decode vagueness from a <phrase>natural language processing</phrase> perspective. While thoroughly identifying the vague terms and their linguistic scope remains an elusive challenge, in this work we seek to learn vector representations of words in privacy policies using deep neural networks. The vector representations are fed to an interactive visualization tool (LSTMVis) to test on their ability to discover syntactically and semantically related terms. The approach holds promise for modeling and understanding language vagueness.
<phrase>Ocean Drilling Program</phrase> Leg 191 Scientific Prospectus Northwest Pacific Seismic <phrase>Observatory</phrase> and <phrase>Hammer</phrase> Drill Engineering Tests Material in this publication may be copied without restraint for library, abstract service, educational, or personal research purposes; however, republication of any portion requires the written consent of the <phrase>Director</phrase>, Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the <phrase>National Science Foundation</phrase>, the participating agencies, This Scientific Prospectus is based on precruise JOIDES panel discussions and scientific input from the designated Co-Chief Scientists on behalf of the drilling proponents. The operational plans within reflect JOIDES Planning Committee and thematic panel priorities. During the course of the cruise, actual site operations may indicate to the Co-Chief Scientists and the Operations Manager that it would be scientifically or operationally advantageous to amend the plan detailed in this prospectus. It should be understood that any proposed changes to the plan presented here are contingent upon approval of the <phrase>Director</phrase> of the <phrase>Ocean Drilling Program</phrase> in consultation with the Science and Operations Committees (successors to the Planning Committee) and the Pollution Prevention and Safety Panel. ABSTRACT <phrase>Ocean Drilling Program</phrase> Leg 191 consists of two parts: (1) a science segment devoted to drilling and casing a hole on the northwest Pacific abyssal seafloor (Site WP-2) coupled with the installation of a broadband <phrase>seismometer</phrase> for a <phrase>long-term</phrase> sub-seafloor <phrase>borehole</phrase> <phrase>observatory</phrase> and (2) engineering tests of the <phrase>hard rock</phrase> reentry system (HRRS) and other equipment. The seismic <phrase>observatory</phrase> is an important component of the International Ocean Network <phrase>seismometer</phrase> net. By filling a large gap in the global station grid, it will help increase the resolution of global tomographic studies, which have revolutionized understanding of mantle dynamics and structure. Moreover, it will allow more precise studies of the seismic structure of old <phrase>Pacific Ocean</phrase> crust and <phrase>lithosphere</phrase>, as well as better resolution of earthquake locations and mechanisms in the northwest Pacific <phrase>subduction zone</phrase>. Approximately 400 m of <phrase>sediments</phrase> and 100 m of <phrase>basalt</phrase> will be cored at Site WP-2. Studies of these cores will add to existing knowledge of <phrase>Cretaceous</phrase> Pacific <phrase>mid-ocean ridge</phrase> <phrase>basalt</phrase> <phrase>chemistry</phrase>, construction of the ocean crust, paleolatitude, the age of magnetic lineations, <phrase>basalt</phrase> physical properties, and the deep <phrase>biosphere</phrase>. Engineering tests will be conducted to test the performance of hammers and bits for the HRRS that will eventually allow <phrase>hard-rock</phrase> spudding of holes at mid-ocean ridges and other locales where holes must be started on hard outcrops. The …
Excavation-parallel laser scanning of a <phrase>medieval</phrase> cesspit in the <phrase>archaeological</phrase> zone <phrase>cologne</phrase>, <phrase>germany</phrase> During the construction of an underground museum in the historic city center of <phrase>Cologne</phrase>, <phrase>Germany</phrase>, large parts of the <phrase>Roman</phrase> and <phrase>medieval</phrase> city are being excavated. The newly excavated remains as well as remains of the <phrase>Roman</phrase> city, which had already been excavated in 1954, exhibit structural damages. While at first deficiencies in the construction were assumed to be the cause of the damages, in 2003 a seismogenic origin was suggested. To further test this hypothesis of seismically induced slope movements and other possible causes, a multidisciplinary project was started. One step in this project is the documentation of the damages using a 3D laser scanner, followed by a quantitative damage analysis. This article presents the 3D documentation and the quantitative damage analysis of a recently excavated <phrase>medieval</phrase> cesspit. The 8.3m-deep cesspit was mapped during 11 campaigns using a phase-based 3D laser scanner. Due to the static conditions of the cesspit, the structure could not be excavated in its entirety. After the excavation of every 1-2m-section, <phrase>restoration</phrase> work had to be done to avoid a collapse of the construction. The laser scanning technique offered the possibility of working parallel to the excavation so the original conditions of each section could be documented before the <phrase>restoration</phrase>. The resulting models were used to identify, classify, and quantify the structural damages of the cesspit.
3d Reconstruction of Subsurface <phrase>Geological Bodies</phrase>: Methods and Applications An original approach for the 3D visualisation and modeling of buried <phrase>deep and shallow</phrase> subsurface <phrase>geological bodies</phrase> by means of GOCAD is presented in this paper. Cartographic data and structural surface observations have been used, establishing a link between the <phrase>Geographic Information Systems</phrase>, where the data are stored, and the GOCAD environment. Four main sources of information are needed for the development of a 3D structural model: 1-topographic data represented by contour lines and quoted points; 2-geological, <phrase>geomorphological</phrase> and tectonic boundaries consisting of 2D linear elements; 3-mesoscopic structural measurements including attitude of planar and linear elements (bedding, thrusts, strike-slip, normal faults, lineations, etc.); 4-geological <phrase>cross-section</phrase> reconstructed through the analysis of surface geological data; Other sources of geological information as wells data, seismic sections, etc, can be also introduced into the model. The analysed <phrase>geological bodies</phrase> consist of a deep <phrase>landslide</phrase> developed in the <phrase>sedimentary</phrase> cover of the <phrase>Southern Alps</phrase> (<phrase>Lombardia</phrase>, <phrase>Northern Italy</phrase>), and of the <phrase>sedimentary</phrase> successions of the Sant'Arcangelo basin, a recent <phrase>piggy</phrase>-back basin located in the Southern <phrase>Apennines</phrase> (<phrase>Basilicata</phrase>, <phrase>Southern Italy</phrase>). The geometric features of the reconstructed <phrase>geological bodies</phrase> can be used to design preliminary monitoring plans or subsurface investigations through seismic surveys and drilling. The characterisation of the shallow subsurface is important for <phrase>civil engineering</phrase> and environmental applications that depend upon precise definitions of the geometrical, geomechanical and <phrase>hydrological</phrase> properties of rock bodies. 1. Introduction This paper describes a complete framework for the 3D representation and modeling of <phrase>deep and shallow</phrase> subsurface <phrase>geological bodies</phrase>. Three dimensional reconstruction is developed in the GOCAD ® environment, basing on digital information stored in <phrase>Geographic Information Systems</phrase> (GIS) and related databases. These procedures intend to visualise geological information and to establish topological relationships among the analysed objects, coupling the <phrase>data processing</phrase> capabilities of GIS (ArcINFO ® , ArcView ®) with 3D modeling in GOCAD. A simple link among different software is established through a set of conversion programs that make the information, stored in a GIS and related <phrase>database management</phrase> system, available on demand. The proposed procedures have been tested on different geological problems exploiting the same types of data, consisting of geological and structural surface observations directly surveyed in the field. Two examples of 3D reconstruction are here shortly
Spanish FreeLing <phrase>Dependency Grammar</phrase> This paper presents the development of an <phrase>open-source</phrase> Spanish <phrase>Dependency Grammar</phrase> implemented in FreeLing environment. This grammar was designed as a resource for NLP applications that require a step further in <phrase>natural language</phrase> automatic analysis, as is the case of Spanish-to-<phrase>Basque</phrase> translation. The development of wide-coverage <phrase>rule-based</phrase> grammars using <phrase>linguistic knowledge</phrase> contributes to extend the existing Spanish deep parsers collection, which sometimes is limited. Spanish FreeLing <phrase>Dependency Grammar</phrase>, named EsTxala, provides deep and robust <phrase>parse trees</phrase>, solving attachments for any structure and assigning syntactic functions to dependencies. These steps are dealt with hand–written rules based on <phrase>linguistic knowledge</phrase>. As a result, FreeLing Dependency Parser gives a unique analysis as a <phrase>dependency tree</phrase> for each sentence analyzed. Since it is a resource open to the scientific community, exhaustive grammar evaluation is being done to determine its accuracy as well as strategies for its manteinance and improvement. In this paper, we show the results of an <phrase>experimental evaluation</phrase> carried out over EsTxala in order to test our evaluation methodology.
An IC <phrase>manufacturing yield</phrase> model considering <phrase>intra-die</phrase> variations In <phrase>deep submicron</phrase> <phrase>feature sizes</phrase> continue to shrink aggressively beyond the natural capabilities of the 193 nm lithography used to produce those features thanks to all the innovations in the field of resolution enhancement techniques (RET). With reduced <phrase>feature sizes</phrase> and tighter pitches die level variations become an increasingly dominant factor in determining <phrase>manufacturing yield</phrase>. Thus a prediction of design-specific features that impact <phrase>intra-die</phrase> variability and correspondingly its yield is extremely valuable as it allows for altering such features in a manner that reduces <phrase>intra-die</phrase> variability and improves yield. In this paper, a <phrase>manufacturing yield</phrase> model which <phrase>takes into account</phrase> both physical layout features and manufacturing fluctuations is proposed. The <phrase>intra-die</phrase> systematic variations are evaluated using a physics-<phrase>based model</phrase> as a function of a design's physical layout. The random variations and their across-die spatial correlations are obtained from data harvested from manufactured <phrase>test structures</phrase>. An efficient algorithm is proposed to reduce the order of the <phrase>numerical integration</phrase> in the yield model. The model can be used to (i) predict manufacturing yields at the design stage and (ii) enhance the layout of a design for higher <phrase>manufacturing yield</phrase>.
Global Refinement of Random Forest Y X <phrase>Training Samples</phrase> Rf 2k <phrase>Leaf Nodes</phrase> Rf 10k <phrase>Leaf Nodes</phrase> Ours 2k <phrase>Leaf Nodes</phrase> Rf 100k <phrase>Leaf Nodes</phrase> <phrase>Random forest</phrase> is well known as one of the best learning methods. In spite of its great success, it also has certain drawbacks: the heuristic learning rule does not effectively minimize the global training loss; the model size is usually too large for many real applications. To address the issues, we propose two techniques, global refinement and global pruning, to improve a pre-trained <phrase>random forest</phrase>. The proposed global refinement jointly relearns the <phrase>leaf nodes</phrase> of all trees under a global <phrase>objective function</phrase> so that the complementary information between multiple trees is well exploited. In this way, the fitting power of the forest is significantly enhanced. The global pruning is developed to reduce the model size as well as the over-fitting risk. The refined model has better performance and smaller storage cost, as verified in extensive experiments. <phrase>Random forest</phrase> [2] is one of the most popular learning methods and has many ideal properties: 1) it is simple to understand and implement; 2) it is strong in handling non-linearity and outliers; 3) it is friendly to parallel training and large data; and 4) it is fast in testing. Recently, it has proven extremely successful on important applications in <phrase>data mining</phrase> [12] and <phrase>computer vision</phrase> [6, 11]. In spite of its great success, <phrase>random forest</phrase> has certain insufficiency from both theoretical and practical viewpoints. Theoretically, the heuristic learning of random forest is suboptimal in terms of minimizing training error. Specifically, each individual tree is learnt independently and greedily. Such learning does not fully utilize complementary information among different trees. Practically, for complex real problems [3, 4, 6, 11], deep trees are usually required to fit the training data well. This results in high storage cost, which is a serious issue especially for <phrase>embedded devices</phrase> such as <phrase>mobile phone</phrase> or <phrase>Kinect</phrase>. While tree pruning can reduce the tree size, <phrase>existing methods</phrase> [7, 8] are independently performed on individual trees and could degrade the performance of <phrase>random forest</phrase>. To address the above problems, we propose a simple and effective method to refine a pre-trained <phrase>random forest</phrase>. We notice that the learning and prediction of <phrase>random forest</phrase> is inconsistent: the learning of individual trees is independent but the prediction averages all trees' outputs. The loss functions implied from these two processes are actually different. This limits the fitting power of random forest. To alleviate such inconsistency, we discard the old values stored in all tree <phrase>leaves</phrase> …
Design and analysis of a head-mounted parallel kinematic device for skull surgery PURPOSE Precision skull surgery requires specialized instrumentation to satisfy demanding requirements in cochlear array implantation, <phrase>deep brain stimulation</phrase> electrode placement, and related applications. A miniaturized reconfigurable parallel kinematic mechanism which can be directly mounted on a patient's skull was designed, built, and tested for precision skull surgery.   METHODS A Stewart-Gough platform is attached to a patient's skull so no optical tracking affecting the overall accuracy in keyhole surgery is required. Six bone anchors comprising the mechanism base joints are implanted at positions with sufficient skull thickness. Since no fixation frame is required, intervention planning flexibility is increased. The centers of the spherical shaped bone anchors can be localized accurately in the image space. An implicit registration to the physical space is performed by assembling the <phrase>kinematics</phrase>. Registration error is minimized compared to common optical tracker-<phrase>based approaches</phrase>. Due to the reconfigurable mechanism, an optimization of the hexapod's configuration is needed to maximize accuracy and mechanical stability during the incision. Mathematical simulation was conducted to estimate system performance.   RESULTS <phrase>Simulation results</phrase> revealed significant improvement in accuracy and stability when exploiting the redundant degrees of freedom and the implemented reconfigurability. Inaccurate localization of base points in the image <phrase>data set</phrase> was identified as the main source of error. A first prototype with passive prismatic actuators, i.e. micrometer calipers, was successfully built.   CONCLUSIONS A head-mounted parallel kinematic device for <phrase>high precision</phrase> skull surgery was developed that provides submillimetric accuracy in straight-line incisions. The system offers enhanced flexibility due to the absence of a rigid fixation frame.
An Efficient Algorithm for Calculating the Worst-case Delay due to Crosstalk Analyzing the effect of crosstalk on delay is critical for <phrase>high performance</phrase> circuits. The <phrase>major bottleneck</phrase> in performing crosstalk-induced delay analysis is the high computational cost of simulating the coupled interconnect and the nonlinear drivers. In this work, we propose an efficient iterative algorithm that avoids time-consuming nonlinear driver simulations and performs node-specific crosstalk delay analysis. The proposed algorithm has been tested over circuits in two <phrase>deep submicron technologies</phrase> with varying driver sizes, interconnect parasitics, signal transition times and it has been found to predict the worst-case delay to within 10 % of the actual delay. 1 Introduction Due to scaling in <phrase>process technology</phrase>, <phrase>coupling capacitance</phrase> has become dominant and crosstalk issues have become highly critical. Crosstalk leads to two significant problems. Firstly, coupling effects may inject noise into a circuit leading to discharge of the capacitance at the output of a gate, and thereby altering functionality. This effect has been extensively Secondly, crosstalk-induced delay can critically affect <phrase>circuit performance</phrase>. This problem is more serious and this paper is directed towards analyzing this effect. The coupling between interconnect lines makes it difficult to consider the effect of different aggressor drivers in an independent fashion. The traditional method of interconnect analysis that considers only one line at a time is no longer valid since the behavior of each line can depend on that of its transitive neighbors. This implies that either a large number of lines must be concurrently simulated, or that an intelligent iterative approach must be used, simulating only one line at a time. Moreover, nonlinear driver simulation greatly increases the computation time needed for analysis in either of these scenarios. Ignoring nonlinear drivers completely or modeling them with a simple linear resistance is generally known to give large errors. Early approaches to incorporate the effects of coupling in calculating delays made use of a switch factor of [0,2] or [-1,3] for the <phrase>coupling capacitance</phrase> [Kah00], [Che00], [Sap00]. The worst-case bound has been found to predict overly pessimistic delay values for some cases and underestimate others [Dar97]. The exact value of the switch factor is dependent on signal <phrase>polarity</phrase>, driver strengths, interconnect parasitics, slew rates and arrival times and cannot be determined a priori. Switch factor methods do not consider many of these parameters and their accuracy for <phrase>deep submicron designs</phrase> is questionable. A relative window <phrase>based approach</phrase> is proposed in [Sas00]. A look-up table is used to capture …
Two Discourse Driven Language Models for Semantics <phrase>Natural language understanding</phrase> often requires <phrase>deep semantic</phrase> knowledge. Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a <phrase>language model</phrase> if done at an appropriate <phrase>level of abstraction</phrase>. We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities. For each model, we investigate four implementations: a " standard " <phrase>N-gram</phrase> <phrase>language model</phrase> and three discriminatively trained " neural " language models that generate embeddings for semantic frames. The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a <phrase>narrative</phrase> cloze test and extrinsically – we show that our SemLM helps improve performance on semantic <phrase>natural language processing</phrase> tasks such as co-reference resolution and discourse parsing.
Assistance to planning in <phrase>deep brain stimulation</phrase>: <phrase>data fusion</phrase> method for locating anatomical targets in MRI. Symptoms of <phrase>Parkinson's disease</phrase> can be relieved through <phrase>deep brain stimulation</phrase>. This <phrase>neurosurgical</phrase> technique relies on <phrase>high precision</phrase> positioning of electrodes in specific areas of the <phrase>basal ganglia</phrase> and the thalamus. In order to identify these anatomical targets, which are located deep within the brain, we developed a <phrase>semi-automated</phrase> method of <phrase>image analysis</phrase>, based on <phrase>data fusion</phrase>. Information provided by both anatomical <phrase>magnetic resonance images</phrase> and expert knowledge is managed in a common possibilistic frame, using a <phrase>fuzzy logic</phrase> approach. More specifically, a <phrase>graph-based</phrase> virtual atlas modeling theoretical anatomical knowledge is matched to the <phrase>image data</phrase> from each patient, through a research algorithm (or strategy) which simultaneously computes an estimation of the location of every structures, thus assisting the <phrase>neurosurgeon</phrase> in defining the optimal target. The method was tested on 10 images, with <phrase>promising results</phrase>. Location and segmentation results were statistically assessed, opening perspectives for enhancements.
<phrase>Fault-coverage</phrase> analysis techniques of crosstalk in chip interconnects —This paper addresses the problem of evaluating the effectiveness of <phrase>test sets</phrase> to detect <phrase>crosstalk defects</phrase> in system-level interconnects and buses of <phrase>deep submicron</phrase> (DSM) chips. The <phrase>fast and accurate</phrase> estimation technique will enable: 1) evaluation of different existing tests, like functional, scan, logic built-in self-test (BIST), and <phrase>delay tests</phrase>, for effective testing of <phrase>crosstalk defects</phrase> in core-to-core interconnects and 2) development of crosstalk tests if the existing tests are not sufficient, thereby minimizing the cost of interconnect testing. Based on a covering relationship we distinguish between transition tests in detecting <phrase>crosstalk defects</phrase> and develop an abstract <phrase>crosstalk fault</phrase> model for chip interconnects. With this <phrase>fault model</phrase> and the covering relationship, we develop a fast and efficient method to estimate the fault coverage of any general <phrase>test set</phrase>. We also develop a <phrase>simulation-based</phrase> technique to calculate the probability of occurrence of the defects corresponding to each fault, which enables the fault-coverage analysis technique to produce accurate estimates of the actual <phrase>crosstalk defect coverage</phrase> of a given <phrase>test set</phrase>. The crosstalk test and fault properties, as well as the accuracy of the proposed crosstalk coverage analysis techniques, have been validated through extensive simulation experiments. The experiments also demonstrate that the proposed crosstalk techniques are <phrase>orders of magnitude</phrase> faster than the alternative method of SPICE-level simulation. Finally, we demonstrate the practical applicability of the proposed <phrase>fault-coverage</phrase> analysis technique by using it to evaluate the crosstalk <phrase>fault coverage</phrase> of <phrase>logic BIST</phrase> tests for the system-level interconnects and buses in a <phrase>digital signal processor</phrase> core.
<phrase>Social Computing</phrase> for Educational <phrase>Knowledge Building</phrase> <phrase>Research and development</phrase> activities in <phrase>social computing</phrase>, although still in exploratory stages, have already inspired a wide array of undertakings aimed at fostering the collective engagement of small groups and larger collectivities. <phrase>Social computing</phrase> research combines theory building and design work; experiments supporting small social groups or large <phrase>online communities</phrase> create new interaction settings that allow testing and expanding theoretical perspectives. We are interested in harnessing <phrase>social computing</phrase> for educational purposes, facilitating the building of knowledge by distributed small groups of students working collaboratively. We want to understand how <phrase>knowledge-building</phrase> social activity can be accomplished and how effective <phrase>social computing</phrase> environments can be designed and deployed to promote <phrase>collaborative learning</phrase>. Our project, the Virtual Math Teams (VMT) project at mathforum.org, is an attempt to develop a <phrase>social computing</phrase> environment that promotes scaffolded discourse about mathematics among teens and to study the forms of collective interaction that take place there. The VMT project is based upon an evolving theory of group cognition. This theory hypothesizes that " small groups are the engines of <phrase>knowledge building</phrase>. The knowing that groups build up in <phrase>manifold</phrase> forms is what becomes internalized by their members as individual learning and externalized in their communities as certifiable knowledge " (Stahl, 2006, p. 16). The VMT project is an ongoing effort to catalyze and promote a math discourse community. Starting very simply in 2003 from a successful online math problem-of-the-week service (mathforum.org/<phrase>pow</phrase>/) and taking advantage of popular off-the-shelf chat software to make it collaborative, we have since then gradually evolved a more sophisticated environment involving carefully scripted pedagogical interventions, open-ended math issues and custom software—guided by extensive analysis of student behaviors through cycles of trials. We now want to strengthen its <phrase>social networking</phrase> supports to facilitate wider adoption and to further our research agenda. While the ubiquity of networked computers connected through the Internet from homes and schools creates an exciting opportunity for students around the world to explore math together, the practical difficulties are enormous. We are interested in facilitating the development of <phrase>high-level</phrase> thinking skills and the <phrase>deep understanding</phrase> that comes from engaging in effective dialog and merging personal perspectives, but we find that students are accustomed to using text chat and the Internet for superficial socializing. Furthermore, their habits of learning are overwhelmingly skewed toward passive acquisition of knowledge from authority sources like teachers and books, rather than from self-regulated or collaborative inquiry. Finally, attempts …
A New Methodology for Concurrent <phrase>Technology Development</phrase> and <phrase>Cell Library</phrase> Optimization To minimize the time to market and cost of new sub 0.25um <phrase>process technologies</phrase> and products, PDF Solutions, Inc., has developed a new comprehensive approach based on the use of predictive simulation tools combined with highly efficient experimental design techniques and special <phrase>test structures</phrase>. This paper focuses on our approach for concurrent development of new technologies and optimization of cell libraries for these technologies. We present a software system called Circuit Surfer which performs this library optimization in a highly automated fashion and with guaranteed correctness in silicon. We demonstrate several examples of Circuit Surfer applications to <phrase>cell library</phrase> design to optimize such objective functions as performance, cell area or yield. 1. Introduction With each new generation of technology and products , <phrase>semiconductor manufacturing</phrase> becomes more complex. The increase in IC functionality has been made possible by a continuous drive towards smaller <phrase>feature sizes</phrase>. This decrease in dimensions of semiconductor structures has given rise to a new set of problems as manufacturing sensitivity to critical design and processing parameters has risen dramatically. While IC manufacturing becomes more complex, market windows for new products are shrinking. Success in today's <phrase>marketplace</phrase> requires effective technology integration as dictated by consumer demand. Against the backdrop of changing market conditions, the overall <phrase>design cycle</phrase> time and <phrase>yield ramp</phrase> have become the key drivers for product profitability. Technology independent <phrase>design methodology</phrase>, popularized by Mead and Conway[14] and used ever since to address the growing complexity problem, no longer applies to <phrase>deep submicron designs</phrase>. Unfortunately, this failure is happening at a time when it is more crucial than ever to design products concurrently with new <phrase>technology development</phrase> and its transfer to high volume manufacturing These changes require a redefinition of the interfaces between design, test and manufacturing. In the following section, we present a comprehensive view of the yield problem and a " holistic " yield improvement methodology specifically designed to overcome yield detrac-tors in state-of-the-art technologies. While elimination of systematic and <phrase>design/process</phrase> matching issues is critical to <phrase>yield ramp</phrase>, and hence profitability, there exist organizational barriers that reinforce <phrase>traditional approaches</phrase>. One barrier present in virtually all companies , even <phrase>vertically integrated</phrase> manufacturers (<phrase>IDMs</phrase>), exists between the design and manufacturing groups.
<phrase>Educational Technology</phrase> in Introductory College Physics <phrase>Teaching and Learning</phrase>: The Importance of Students’ Perception and Performance E Ed du uc ca at <phrase>ti</phrase> io on na al l T Te ec ch hn no ol lo og gy y i in n I In <phrase>nt</phrase> tr <phrase>ro</phrase> od du uc ct to or ry y C Co ol ll le eg <phrase>ge</phrase> e P <phrase>Ph</phrase> hy <phrase>ys</phrase> si ic cs s T Te <phrase>ea</phrase> ac ch hi in ng g a an nd d L Le <phrase>ea</phrase> ar <phrase>rn</phrase> ni in ng g: : T Th he e I Im mp po or rt ta an <phrase>nc</phrase> ce e o of f S St tu ud de en <phrase>nt</phrase> ts s' ' P Pe er rc ce <phrase>ep</phrase> pt <phrase>ti</phrase> io on n a an nd d P Pe er rf fo or rm ma an <phrase>nc</phrase> ce e Abstract In this paper the researcher explored how introductory physics student perceptions about learning physics and their perspectives about physics instructors' presentational formats might be developed. Within a constructivist framework, it is of fundamental importance that the educators understand and address student expectations about effective instructional methods and <phrase>educational technology</phrase>-integrated curricula in particular. The researcher also investigated the likely impact of student expectations of <phrase>learning outcomes</phrase> as part of the implications of improving the approach towards teaching. Introduction Physics is a science composed of well-founded expectations of how the natural world should behave, and it uses the tool of mathematics to describe these behaviors (Foster, 2000). Physics learning therefore involves observing phenomena, quantifying the observations, and synthesizing the results into theories (<phrase>Williams</phrase>, 1999). The traditional approach to instruction is to teach physics through <phrase>solving problems</phrase>. Students of physics are expected to learn both the descriptive or conceptual side of physics and its predictive, <phrase>problem-solving</phrase>, and logical reasoning aspects. Because learning logical thinking is difficult, many students try to memorize formulas and recall results from the homework problems at test time. <phrase>Hammer</phrase> (1994) reported that many <phrase>students learn</phrase> by rote because they have a naive conception of what it means to understand physics. Formulas and equations are important for physics because physical quantities have to be calculated by using them. However, if students cannot understand the physics behind the formulas, they usually will not be able to solve the problem. Elby (1999) focused on another cause of these study habits because many students believed that a deep understanding of physics is not necessary to obtain high grades in …
Formal Software Analysis Emerging Trends in Software <phrase>Model Checking</phrase> Formal Software Analysis Emerging Trends in Software <phrase>Model Checking</phrase> * 1995 through 2004. He worked for six years as a Senior Engineer with Intermetrics Inc. developing compilers and software for <phrase>safety-critical</phrase> <phrase>embedded systems</phrase> before returning to graduate school at the University of <phrase>Massachusetts</phrase> at <phrase>Amherst</phrase> where he completed his PhD in 1995. His interests cover a wide range of topics in software dependability including: specification methods, <phrase>static analysis</phrase> and verification, <phrase>run-time</phrase> monitoring and testing. He is program co-chair of FASE'07 and <phrase>ICSE</phrase>'08 and currently serves as <phrase>Secretary</phrase> and <phrase>Treasurer</phrase> of ACM SIGSOFT. <phrase>interests include</phrase> <phrase>formal methods</phrase>, software specifications, <phrase>model checking</phrase>, <phrase>static analysis</phrase>, language-based security, and <phrase>software architecture</phrase>. Together with colleagues in the Laboratory for <phrase>Static Analysis</phrase> and Transformation of Software (<phrase>SAnToS</phrase>) at <phrase>Kansas State</phrase>, he has developed several <phrase>formal analysis</phrase> tools including the <phrase>Indus</phrase> <phrase>static analysis</phrase> and slicing tool and the <phrase>Bandera</phrase> and <phrase>Bogor</phrase> <phrase>model checking</phrase> frameworks. His current efforts are focused on transferring these tools into industrial use in collaboration with engineers from <phrase>Lockheed Martin</phrase> and <phrase>Rockwell Collins</phrase>. He has served on a number of program committees for major meetings in the <phrase>formal analysis</phrase> area including FSE, POPL, PLDI, CAV and TACAS. where he completed his Ph.D. in 2004. He received the <phrase>National Science Foundation</phrase> Faculty Early CAREER Award in 2007 for his research on an integrated, <phrase>formal analysis</phrase> framework for modular <phrase>reasoning about</phrase> <phrase>deep semantic</phrase> properties of open <phrase>object-oriented</phrase> systems. He leads the development of the <phrase>Bogor</phrase> software <phrase>model checking</phrase> framework, which have been supported by <phrase>Lockheed Martin</phrase> and IBM. His <phrase>collaborative work</phrase> on high-assurance <phrase>multiagent systems</phrase> has been funded by the <phrase>Air Force</phrase> Office of Scientific Research. His broad <phrase>interests include</phrase> software specification and verification using various forms of static and dynamic analyses. interests are in using abstraction and symbolic techniques as a base for efficient software verification. Her other <phrase>interests include</phrase> automated assume-guarantee style compositional verification and <phrase>programming language</phrase> design. She received her PhD degree from <phrase>Kansas State University</phrase>, the Department of Computing and Information Sciences. She has served on program committees for many of the major meetings in the <phrase>formal analysis</phrase> area including Center. During his time at NASA he was one of the pioneers of the software <phrase>model checking</phrase> field and the lead for the Java PathFinder <phrase>model checker</phrase> project. His <phrase>interests include</phrase> advanced testing and <phrase>static analysis</phrase> techniques using <phrase>symbolic execution</phrase>. He received his PhD from the University of <phrase>Manchester</phrase> in 1998. Abstract The study of methodologies and techniques to produce correct software …
A Usability Study on the Use of Multi-Context Visualization This material is posted here with permission of the IEEE. Such ermission of the IEEE does not in any way imply IEEE endorsement of any of the University of Technology, Sydney's products or services. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for <phrase>advertising</phrase> or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to <phrase>pubs</phrase>-permissions@ieee.org. By choosing to view this document, you agree to all provisions of the copyright laws protecting it Abstract Graph visualization has been widely used in <phrase>real-world applications</phrase>, as it provides better presentation of overall <phrase>data structure</phrase>. However, there are navigation problems existing in deep and large relational datasets. To address these challenges, a new technique called multi-context visualization, which provides users with rich contextual information, has been proposed as the solution to the navigation in <phrase>large scale</phrase> datasets. This paper evaluates the multi-context visualization by conducting an experiment-based user study. To answer whether the more contextual information positively assist in making more accurate and easier decisions, it aims to evaluate the effectiveness and efficiency of the multi-context visualization, by measuring the user performance. Specifically, this usability test was designed to test if the use of multiple context views can improve navigation problems for deep and large relational <phrase>data sets</phrase>.
Optimized hybrid verification of <phrase>embedded software</phrase> —The verification of <phrase>embedded software</phrase> has become an important subject over the last years. However, neither stand-alone verification approaches, like <phrase>simulation-based</phrase> or <phrase>formal verification</phrase>, nor state-of-the-art hybrid/semiformal verification approaches are able to verify large and complex <phrase>embedded software</phrase> with hardware dependencies. This work presents an optimized scalable hybrid verification approach for the verification of <phrase>embedded software</phrase> with hardware dependencies using a mixed bottom-up/top-down algorithm with optimized static parameter assignment (SPA). These algorithms and methodologies like SPA and <phrase>counterexample</phrase> guided simulation are used to combine <phrase>simulation-based</phrase> and <phrase>formal verification</phrase> in a new way. SPA offers a way to interact between dynamic and static verification approaches based on an automated ranking heuristic of possible function parameters according to the impact on the model size. Furthermore, SPA inserts initialization code for specific function parameters into the <phrase>source code</phrase> under test and supports model building and optimization algorithms to reduce the state space. We have <phrase>successfully applied</phrase> this optimized hybrid verification methodology to an <phrase>embedded software</phrase> application: Motorola's Powerstone Benchmark suite. The results show that our approach scales better than stand-alone software <phrase>model checkers</phrase> to reach <phrase>deep state</phrase> spaces. I. INTRODUCTION <phrase>Embedded software</phrase> (ESW) is omnipresent in our daily life. It plays a key role in overcoming the time-to-market pressure and providing new functionalities. Therefore, a high number of users are dependent on its functionality [1]. ESW is often used in <phrase>safety critical</phrase> applications (automotive, medical, avionic) where correctness is of fundamental importance. Thus, verification and validation approaches are an important part of the development process. The most commonly used approaches to verify <phrase>embedded software</phrase> are based on simulation or <phrase>formal verification</phrase> (FV) approaches. Testing, co-debugging and/or co-simulation techniques result in a tremendous effort to create <phrase>test vectors</phrase>. Furthermore, critical corner case scenarios might remain un-noticed. An extension of simulation is the assertion-based ver-ication (<phrase>ABV</phrase>) methodology that captures a design's intended
Special Issue on <phrase>Recent Advances</phrase> on Brain-inspired Innovative Computing Brain requires the most important scientific endeavor of the 21st century, and the International Conference on Neural <phrase>Information Processing</phrase> (ICONIP2007) looks for a clue to reveal and implement the brain functions with an integrated approach. To make an archive on the innovative results presented and discussed at ICONIP2007, this special issue includes <phrase>extended versions</phrase> of six selected papers on " <phrase>Recent Advances</phrase> on Brain-inspired Innovative Computing " from ICONIP2007, held in <phrase>Kitakyushu</phrase>, Japan, 13-16 November, 2007. The papers in this issue have been thoroughly reviewed and revised to give the readers a variety of recent findings on the brain-inspired innovative computing. 1. Introduction. This special issue is composed of six excellent papers which present new insight and models based on the <phrase>brain function</phrase> and verify the effectiveness with a variety of interesting real problems. The first three papers mainly focus on the modeling part and tend to be rather theoretical, and the remaining three papers highlight the challenging applications of the new models, ranging from task segmentation in a <phrase>mobile robot</phrase> to every day load forecasting for an electric power company. In the paper entitled Bifurcation-<phrase>based Model</phrase> Construction of a <phrase>Pyramidal Cell</phrase> of the <phrase>Primary Visual Cortex</phrase>, Ishiki et al. proposed a physiologically plausible <phrase>pyramidal cell</phrase> model of the <phrase>primary visual cortex</phrase> which contains many regulating systems of the intra-cellular calcium ions concentration, and shows the usefulness of the nonlinear dynamical system theory such as global bifurcation and slow/fast decomposition analyses for a construction of the neuron model. In the paper, Classifying <phrase>Deep Brain</phrase> Neuronal Activities by Bursting Parameters, Chao et al. developed a method for classifying neuronal activities from the <phrase>deep brain</phrase> nuclei, <phrase>subthalamic nucleus</phrase> (STN) and subtantia nigra (SNr). Analyzing 54 trials of data from Parkinson's patients with seven bursting relevant parameters inducing a classifier of <phrase>support vector machine</phrase> with <phrase>principal component analysis</phrase> that improves the <phrase>classification accuracy</phrase> for 22% on average. In the work Novel Models for Hourly Solar Radiation Using a 2-D Approach, Hocaoglu et al. analyzed one year hourly solar radiation data and modeled the general behavior of the solar radiation in a year using a 2-dimensional surface fitting approach. Gaussian surface model with proper model parameters is found to be the most accurate model among the tested analytical models for data characterization.
Teaching Patterns and <phrase>Software Design</phrase> In this paper we describe our experiences with reengi-neering an undergraduate course in <phrase>software design</phrase>. The course's <phrase>learning outcomes</phrase> require that students can model, design and implement software. These are inherently practical skills and rely on functioning knowledge. To facilitate a <phrase>learning environment</phrase> in which students can acquire the necessary <phrase>deep level</phrase> of understanding, we have designed the course by applying the educational theory of constructive alignment and a number of proven techniques for teaching, learning, and assessment. Fundamentally, we have embraced the <phrase>active learning</phrase> paradigm that recog-nises that student activity is critical to the <phrase>learning process</phrase>. In this paper, we describe several <phrase>active learning</phrase> techniques that we have used including role play, <phrase>problem solving</phrase> and peer learning. We also describe two novel assessment techniques we have developed , holistic assessment and formative examination. In addition we describe how students work with JU-<phrase>nit</phrase>, a popular <phrase>unit testing</phrase> tool, not as users but as developers by applying <phrase>design patterns</phrase> to extend it with new functionality. Finally, we report on student assessment results and relay student feedback.
Filtering Out <phrase>Deep Brain Stimulation</phrase> Artifacts Using a Nonlinear Oscillatory Model This letter is devoted to the suppression of spurious signals (artifacts) in records of neural activity during <phrase>deep brain stimulation</phrase>. An approach based on nonlinear adaptive model with self-oscillations is proposed. We developed an algorithm of adaptive filtering based on this approach. The proposed algorithm was tested using recordings collected from patients during the stimulation. This was then compared to <phrase>existing methods</phrase> and showed the best performance.
Attribute based lattice rescoring in spontaneous <phrase>speech recognition</phrase> In this paper we extend attribute-based lattice rescoring to spontaneous <phrase>speech recognition</phrase>. This technique is based on two <phrase>key features</phrase>: (i) an attribute-based frontend, which consists of a bank of speech attribute detectors followed up by an evidence merger that generates confidence scores (e.g., sub-word posterior probabilities), and (ii) a rescoring module that integrates information generated by the frontend into an existing ASR engine through lattice rescoring. The speech attributes used in this work are phonetic features, such as frication and palatalization. Experimental results on the Switchboard part of the <phrase>NIST</phrase> 2000 Hub5 <phrase>data set</phrase> demonstrate that the proposed approach outperforms LVCSR systems based on Gaussian <phrase>mixture model</phrase>/ <phrase>hidden Markov model</phrase> (GMM/HMM) that does not use attribute related information. Furthermore, a small yet promising improvement is also observed when rescoring word-lattices generated by a state-of-the-art ASR system using deep neural networks. Different frontend configuration are investigated and tested.
<phrase>Neural network-based</phrase> symbol recognition using a few labeled samples The recognition of pen-based visual patterns such as sketched symbols is amenable to supervised <phrase>machine learning</phrase> models such as <phrase>neural networks</phrase>. However, a sizable, labeled training corpus is often required to learn the high variations of freehand sketches. To circumvent the costs associated with creating a large training corpus, improve the recognition accuracy with only a limited amount of <phrase>training samples</phrase> and accelerate the development of sketch <phrase>recognition system</phrase> for novel sketch domains, we present a neural network training protocol that consists of three steps. First, a large pool of unlabeled, synthetic samples are generated from a small set of existing, labeled <phrase>training samples</phrase>. Then, a <phrase>Deep Belief Network</phrase> (DBN) is <phrase>pre-trained</phrase> with those synthetic, unlabeled samples. Finally, the <phrase>pre-trained</phrase> DBN is fine-tuned using the limited amount of labeled samples for classification. The training protocol is evaluated against supervised baseline approaches such as the nearest neighbor classifier and the neural network classifier. The benchmark <phrase>data sets</phrase> used are partitioned such that there are only a few labeled samples for training, yet a large number of labeled <phrase>test cases</phrase> featuring rich variations. <phrase>Results suggest</phrase> that our training protocol leads to a significant error reduction compared to the baseline approaches. Sketch understanding [1,2] aims to enable the computers to interpret man-made, freehand sketches and extract the intended information underlying the input strokes. Fig. 1 shows two exemplary sketches depicting two engineering systems and the corresponding engineering model. If successful, sketch understanding could provide a natural human-computer interface for scenarios in which physical, pen-and-paper sketches have been routinely used, such as the early ideation process or the classroom instruction. Moreover, sketch understanding could automate the mining, organization, search and critique of the information embedded in freehand sketches, potentially resulting in a myriad of <phrase>intelligent agents</phrase>, such as a web spider that crawls through the drawings in online textbooks and lecture notes to learn the design rules of electrical systems, an archiver that indexes <phrase>brainstorming</phrase> sketches for later retrieval and reuse, and a computer grader for the free-body diagrams that students draw in their <phrase>statics</phrase> homework. One of the core problems in sketch understanding is to devise a symbol recognizer to compute a categorical label for each segment of the input sketch. Used in conjunction with a sketch parser that divides the input sketch into segments and possibly a post-processor that ensures the consistency of the recognition, an interpretation of the input sketch can …
A Built-in I Ddq Testing Circuit * Although I DDQ testing has become a widely accepted <phrase>defect detection</phrase> technique for <phrase>CMOS ICs</phrase>, its effectiveness in very <phrase>deep submicron technologies</phrase> is threatened by the increased transistor <phrase>leakage current</phrase>. In this paper, a built-in I DDQ testing circuit is presented, that aims to extend the viability of I DDQ testing in future technologies and first <phrase>experimental results</phrase> are discussed.
Transparent-<phrase>Test Methodologies</phrase> for <phrase>Random Access Memories</phrase> Without/With ECC An extended tanh law <phrase>MOSFET model</phrase> for <phrase>high temperature</phrase> <phrase>circuit simulation</phrase>, " J. Alpha-<phrase>power law</phrase> <phrase>MOSFET model</phrase> and its application to CMOS inverter delay and other formulas, " J. Experimental characterization and mod-eling of electron saturation velocity in MOSFET's inversion layer from 90 to 350 K, " IEEE Electron Device Lett. A temperature dependent model for the saturation velocity in semiconductor materials, " Mater.age current mechanisms and leakage reduction techniques in deep-submicrometer <phrase>CMOS circuits</phrase>, " paradigm of predictive MOSFET and interconnect modeling for early <phrase>circuit design</phrase>, " in [15] C. <phrase>Park</phrase> et al., " Reversal of <phrase>temperature dependence</phrase> of <phrase>integrated circuits</phrase> operating at very low voltages, " in IEDM Tech. <phrase>Supply voltage</phrase> scaling for temperature insensitive CMOS circuit operation, " IEEE Trans. Design impact of positive <phrase>temperature dependence</phrase> on drain current in sub-1-V CMOS VLSIs, " in Analysis of substrate thermal gradient effects on optimal <phrase>buffer insertion</phrase>, " in Proc. ICCAD, 2001, pp. 44–48. [21] R. Dennard et al., " Design of ion-implanted MOSFETs with very small physical dimension, " in J. Full chip leakage estimation considering <phrase>power supply</phrase> and temperature variations, " in Proc. Abstract—This paper presents a systematic procedure for transforming a bit-oriented March test into a transparent word-oriented March test. The test-time complexity of the transparent word-oriented March tests converted by the proposed method is only (P + 5 log 2 B + 2)N for an N × B-bit <phrase>Random Access Memory</phrase>, and the test-time complexity of the corresponding signature-prediction test is QN. Here, P and Q denote the number of total Read/Write and Read test operations of the original bit-oriented March test. A transparent-<phrase>test methodology</phrase> for memories with error-correction code (ECC) is also proposed. This methodology can test and locate faulty cells, and no signature prediction is needed. The test-time complexity of the proposed transparent-<phrase>test methodology</phrase> for memories with ECC is only (P + 5 log 2 B + 2)N.
Implementation and Evaluation of Mpi-based Parallel Md Program The <phrase>message-passing interface</phrase> (MPI)-based <phrase>object-oriented</phrase> particle–particle interactions (PPI) library is implemented and evaluated. The library can be used in the n-particle simulation algorithm designed for a ring of p interconnected processors. The parallel simulation is scalable with the number of processors, and has the time requirement proportional to n 2 /p if n/p is large enough, which guarantees optimal speedup. In a certain range of <phrase>problem sizes</phrase>, the speedup becomes superlinear because enough cache memory is available in the system. The library is used in a simple way by any potential user, even with no deep programming knowledge. Different simulations using particles can be implemented on a wide spectrum of different computer platforms. The main purpose of this article is to test the PPI library on well-known methods, e.g., the parallel <phrase>molecular dynamics</phrase> (MD) simulation of the monoatomic system by the second-order leapfrog Verlet algorithm. The performances of the parallel simulation program implemented with the proposed library are competitive with a custom-designed simulation code. Also, the implementation of the split integration <phrase>symplectic</phrase> method, based on the analytical calculation of the <phrase>harmonic</phrase> part of the particle interactions, is shown, and its expected performances are predicted.
Learning to remove <phrase>multipath distortions</phrase> in Time-of-Flight range images for a <phrase>robotic arm</phrase> setup — Range images captured by Time-of-Flight (ToF) cameras are corrupted with <phrase>multipath distortions</phrase> due to interaction between modulated light signals and scenes. The interaction is often complicated, which makes a <phrase>model-based</phrase> solution elusive. We propose a learning-based approach for removing the <phrase>multipath distortions</phrase> for a ToF camera in a <phrase>robotic arm</phrase> setup. Our approach is based on <phrase>deep learning</phrase>. We use the <phrase>robotic arm</phrase> to automatically collect a large amount of ToF range images containing various <phrase>multipath distortions</phrase>. The training images are automatically labeled by leveraging a <phrase>high precision</phrase> structured light sensor available only in the training time. In the test time, we apply the learned model to remove the <phrase>multipath distortions</phrase>. This allows our <phrase>robotic arm</phrase> setup to enjoy the speed and compact form of the ToF camera without compromising with its range measurement errors. We conduct extensive experimental validations and compare the proposed method to several baseline algorithms. The experiment results show that our method achieves 55% error reduction in range estimation and largely outperforms the baseline algorithms.
Mapping <phrase>Water Constituents</phrase> in <phrase>Lake Constance</phrase> Using Chris/proba CHRIS-PROBA data were acquired at 3 days in 2003 and one day in 2004 at the Eastern part of <phrase>Lake Constance</phrase>. Field campaigns were organised at these days in order to measure optical parameters of the water and the <phrase>atmosphere</phrase> and concentrations of <phrase>water constituents</phrase> from ship for validation. Five <phrase>deep water</phrase> sampling stations were chosen. The modular inversion program (MIP) developed by Heege, Miksa and Kisselev was used for the coupled retrieval of <phrase>water constituents</phrase> and aerosol concentrations, for atmospheric and water surface corrections and for optical closure and sensor calibration tests. Different viewing angles of CHRIS images of 2003 were used to test the atmospheric correction algorithm. Spectra were calculated and adjusted to the measured ones by fitting the concentrations of 3 aerosol types (rural, maritime, urban) and 3 <phrase>water constituents</phrase> (suspended matter, <phrase>chlorophyll</phrase>, Gelbstoff). The resulting mean relative error for suspended matter is 17%, for <phrase>chlorophyll</phrase> it is 28% which is within the error margin of the measurements. Maps of <phrase>chlorophyll</phrase>, suspended matter and Gelbstoff were generated for all viewing angles of CHRIS/PROBA where sunglint contamination is absent. By comparing these maps the accuracies of the resulting <phrase>water constituents</phrase> are determined. Looking at one pixel from different angles, the water constituent concentration is supposed to be the same. The mean relative error for each sampling station is 37% for <phrase>chlorophyll</phrase> and 24% for suspended matter. A more detailed <phrase>statistical analysis</phrase> is in preparation.
Chinese <phrase>Text Retrieval</phrase> Without Using a Dictionary It is generafly believed that words, rather than characters, should be the smallest indexing unit for Chinese <phrase>text retrieval</phrase> systems, and that it is essential to have a comprehensive <phrase>Chinese dictionary</phrase> or lexicon for Chhmse <phrase>text retrieval</phrase> systems to do well. <phrase>Chinese text</phrase> has no delimiters to mark woni boundaries. As a result, any <phrase>text retrieval</phrase> systems that build word-based indexes need to segment text into words. We implemented several statistical and dictionary-hazed <phrase>word segmentation</phrase> methods to study the effect on <phrase>retrieval effectiveness</phrase> of different segmentation methods using the TREC-S Chinese <phrase>test collection</phrase> and topics. The results show that, for all three sets of queries, the simple bigram indexing and the purely statistical <phrase>word segmentation</phrase> perform better than the popular dictionary-based maximum matching method with a dictionary of 138,955 entries. 1 Introduction The <phrase>written Chinese</phrase> text has no delimitem to mark word boundaries , it consists of a string of characters and <phrase>punctuation</phrase>. The first step toward word-based indexing is to break a sequence of characters into words. The process of breaking a string of character into words is called <phrase>word segmentation</phrase>. <phrase>Word segmentation</phrase> is known to be a difficult task because accurate segmentation of <phrase>written Chinese</phrase> text may require deep analysis of the sentences. Even Chhese speakers may d~ree over how a sentence should be segmented because of the lack of a clear-cut definition on what constitutes a Chinese word. Some practical and popular <phrase>word segmentation</phrase> methods use dictionaries (lexicon), the simplest one being just a list of Chinese words. The dictionary coverage of words can have a significant impact on the accuracy of <phrase>word segmentation</phrase>. It is virtually impossible to list all the Chinese words in a dictionary because the set of words is open-ended. The construction of a comprehensive dictionary is itself a difficult task. Another group of <phrase>word segmentation</phrase> methuds uses the lexical statistics of the <phrase>Chinese characters</phrase> in corpora to mark the word boundaries. The lexical statistics may include the occurrence frequency of a character in text corpora, and the co-occurrence frequency of two or more charectem in text corpora. What makes the statistical <phrase>word segmentation</phrase> approaches appealing is that they do not require a comprehensive dictionary to mark word boundaries. It is generally believed that a comprehensive <phrase>Chinese dictionary</phrase> or lexicon is needed for a <phrase>Chinese text</phrase> <phrase>retrieval system</phrase> to perform well. We want to know if Chinese <phrase>text retrieval</phrase> sys-Permisaionto make digitallhardcopies of …
People detection through quantified fuzzy temporal rules The knowledge about the position and movement of people is of great importance in <phrase>mobile robotics</phrase> for implementing tasks such as navigation, mapping, localization, or human–<phrase>robot interaction</phrase>. This knowledge enhances the robustness, reliability and performance of the robot control architecture. In this paper, a pattern classifier system for the detection of people using <phrase>laser range</phrase> finders data is presented. The approach is based on the quantified fuzzy temporal rules (QFTRs) <phrase>knowledge representation and reasoning</phrase> paradigm, that is able to analyze the spatio-temporal patterns that are associated to people. The pattern classifier system is a <phrase>knowledge base</phrase> made up of QFTRs that were learned with an <phrase>evolutionary algorithm</phrase> based on the <phrase>cooperative</phrase>-competitive approach together with token competition. A deep experimental study with a Pioneer II robot involving a five-<phrase>fold cross-validation</phrase> and several runs of the <phrase>genetic algorithm</phrase> has been done, showing a classification rate over 80%. Moreover, the characteristics of the tests represent complex and realistic conditions (people moving in groups, the robot moving in part of the experiments, and the existence of static and moving people). The operation of <phrase>mobile robots</phrase> in real environments, like <phrase>supermarkets</phrase>, <phrase>railway</phrase> stations, <phrase>hospitals</phrase>, etc., is generally characterized by the existence of people and <phrase>moving objects</phrase> in the surrounding. This fact needs to be considered when implementing tasks such as mapping or <phrase>path planning</phrase>, since discarding <phrase>moving objects</phrase> usually leads to errors and poor performance. The detection of people is particularly important for <phrase>service robots</phrase> and, fundamentally, for human–<phrase>robot interaction</phrase> , where both moving people and also static people have to be detected. The detection of people is highly influenced by the type of sensor being used. The two types of sensors most widely employed for this purpose are cameras [1–4] and range finders (generally, <phrase>laser range</phrase> finders) [5–7]. The advantages of <phrase>laser range</phrase> finders are that they can directly measure objects geometry, distances information is accurate, the field of view is large, and information of the probability of occupancy of each area of the environment can be easily obtained. On the contrary, the quantity of information that can be extracted is lower than with a camera and, therefore, distinguishing among objects with similar geometric properties becomes much more difficult. Several proposals have been done for the detection of people with <phrase>laser range</phrase> finders. They can be grouped into three categories: those that are based on the difference of occupancy between consecutive range scans [7–16] …
<phrase>Parallel Algorithms</phrase> for Neuronal <phrase>Spike Sorting</phrase> State-of-the-art algorithms will be studied for sorting spike data recorded via tetrodes from living rats. <phrase>Parallel algorithms</phrase> will be implemented on state-of-the-art parallel CPU hardware in order to improve performance of existing algorithms, and increase the range of tractable algorithms. Emphasis will be placed on tuning the algorithms to achieve maximum performance from the hardware platform, and grasping a deep understanding of programming practices needed to achieve high performance on parallel hardware. Detailed analysis will be performed to understand the trade-offs and quality of the algorithms, with regards to clustering quality and execution time. The project should culminate in a working application with a graphical user interface which will be used to perform <phrase>spike sorting</phrase>, and to experiment with the different implemented algorithms. Abstract Neurons communicate through <phrase>electrophysiological</phrase> signals, which may be recorded using electrodes inserted into living tissue. When a neuron emits a signal, it is referred to as a spike, and an electrode can detect these from multiple neurons. Neuronal <phrase>spike sorting</phrase> is the process of classifying the spike activity based on which neuron each spike signal is emitted from. Advances in technology have introduced better recording equipment, which allows the recording of many neurons at the same time. However, clustering software is lagging behind. Currently, <phrase>spike sorting</phrase> is often performed semi-manually by experts, with computer assistance, in a drastically reduced <phrase>feature space</phrase>. This makes the clustering prone to <phrase>subjectivity</phrase>. Automating the process will make classification much more efficient, and may produce better results. Implementing accurate and efficient <phrase>spike sorting</phrase> algorithms is therefore <phrase>increasingly important</phrase>. We have developed parallel implementations of superparamagnetic clustering , a novel clustering algorithm, as well as <phrase>k-means clustering</phrase>, serving as a useful comparison. Several <phrase>feature extraction</phrase> methods have been implemented to test various input distributions with the clustering algorithms. To assess the quality of the results from the algorithms, we have also implemented different cluster quality algorithms. Our implementations have been benchmarked, and found to scale well both with increased <phrase>problem sizes</phrase> and when run on <phrase>multi-core</phrase> processors. The results from our cluster quality measurements are inconclusive, and we identify this as a problem related to the <phrase>subjectivity</phrase> in the manually classified datasets. To better assess the utility of the algorithms, comparisons with intracellular recordings should be performed. iii iv Acknowledgements This thesis was written during spring 2011, as part of the course TDT4900 – Computer and <phrase>Information science</phrase>, master thesis. The project …
<phrase>COMA</phrase>-boost: co-operative <phrase>multi agent</phrase> AdaBoost Multi <phrase>feature space</phrase> representation is a common practise in computer vision applications. Traditional features such as HOG, SIFT, SURF etc., individually encapsulates certain discriminative cues for visual classification. On the other hand, each layer of a deep neural network generates multi ordered representations. In this paper we present a novel approach for such multi feature representation learning using Adaptive Boosting (AdaBoost). General practise in AdaBoost [8] is to concatenate components of feature spaces and train base learners to classify examples as correctly/incorrectly classified. We posit that multi <phrase>feature space</phrase> learning should be viewed as a derivative of <phrase>cooperative</phrase> <phrase>multi agent</phrase> learning. To this end, we propose a mathematical framework to leverage performance of base learners over each <phrase>feature space</phrase>, gauge a measure of "difficulty" of training space and finally make soft weight updates rather than strict binary weight updates prevalent in regular AdaBoost. This is made possible by periodically sharing of response states by our learner agents in the boosting framework. Theoretically, such soft weight update policy allows infinite combinations of weight updates on training space compared to only two possibilities in AdaBoost. This opens up the opportunity to identify 'more difficult' examples compared to 'less difficult' examples. We test our model on traditional multi feature representation of MNIST <phrase>handwritten character</phrase> dataset and 100-<phrase>Leaves</phrase> classification challenge. We consistently outperform traditional and variants of multi view boosting in terms of accuracy while margin analysis reveals that <phrase>proposed method</phrase> fosters formation of more confident ensemble of learner agents. As an application of using our model in conjecture with <phrase>deep neural network</phrase>, we test our model on the challenging task of retinal <phrase>blood vessel</phrase> segmentation from fundus images of DRIVE dataset by using kernel dictionaries from layers of unsupervised trained stacked autoencoder network. Our work opens a new avenue of research for combining a popular statistical <phrase>machine learning</phrase> paradigm with deep network architectures.
Editorial: <phrase>Alan Turing</phrase> and Artiicial Intelligence His Turing's] point was that we should not be species-chauvinistic, or <phrase>anthropocentric</phrase>, about the insides of an intelligent being, for there might be inhuman ways of being intelligent. {Daniel C. Dennett Alan Mathison Turing (23 June 1912{7 June 1954) was one of the most eminent scientists of the 20th century (Figure 1). His research was a central <phrase>catalyst</phrase> of the computer <phrase>revolution</phrase>. The concept of a <phrase>Turing machine</phrase>, which he developed in the 1930s, is still one of the most widely used models of computation in theoretical <phrase>computer science</phrase> , but this monumental contribution was only the rst of many. and maintainer of the \<phrase>Alan Turing</phrase> <phrase>Home Page</phrase>" 1 | divides Turing's publications into ve areas: <phrase>mathematical logic</phrase>, mechanical intelligence, <phrase>pure mathematics</phrase>, <phrase>morphogenesis</phrase>, and <phrase>crypt</phrase>-analysis. Moreover, in Sir Roger Penrose's words, Turing was also \a deep and innuential <phrase>philosopher</phrase> in addition to his having made contributions to mathematics, technology and code-breaking that profoundly contribute to our present-day well being" (A. Hodges, 1998). In a landmark article published in October 1950 in the philosophy journal Mind (Figure 2) Turing made a famous assertion. He predicted that by the year 2000 it would be feasible to write a program that would, after ve minutes of questioning, have at least a 30% chance of fooling an average conversational partner into believing it was a human being (Turing, 1950). 2 As Charniak and McDermott (1985, p. 10) remark \Actually, the Mind] paper makes it sound as if Turing had in mind the computer pretending to be a woman in the man/woman game, but the point is not completely clear, and most have assumed that he intended the test to be a person/computer one, and not woman/computer." See (Saygin et al., 1999) for an attempt at clariication.
A New Maximal Diagnosis Algorithm for Bus-structured Systems Complex interconnects in highly integrated system chips are implemented with the bus structure. From testing <phrase>point of view</phrase>, the bus structure system needs more complicated consideration than simple wiring networks since a bus line is received data from many drivers. Therefore, some faults are detected all the time and others are detected only at the particular time. We propose a new <phrase>interconnect test</phrase> algorithm for the bus structure. The MD+ algorithm supports maximal diagnosis for the bus-structured system and its test period is shorter than the previous algorithms. Moreover, the MD+ algorithm is easy to apply since it is based on the complete diagnosis algorithm for wiring networks. The effectiveness of the MD+ algorithm is confirmed by comparing the test length with previous bus based interconnect test algorithms. 1. Introduction <phrase>Deep submicron technology</phrase> makes it possible to integrate a system in a single chip, called SOC (System on a Chip). The various modules are integrated in a chip and their complex interconnects are implemented with the bus-structured system. As a <phrase>point of view</phrase> for SOC testing, the defects on interconnects are shown as defects of a system because the <phrase>interconnect test</phrase> is only done through the I/O pins of the SOC. As IEEE 1149.1[1] supports the testing environments for the board level test, IEEE P1500 does the same role for SOC. The <phrase>interconnect test</phrase> procedure is executed by applying various interconnect test algorithms through P1500 interface. Many researches [2]-[12] have been made for the diagnosis of interconnect. They assume the three general <phrase>fault models</phrase> named <phrase>stuck-at fault</phrase>, stuck-open fault, and short fault. They assume the multiple faults on a net but it does not mean the complete diagnosis. It's because the complete diagnosis is physically impossible, so we focus on the maximal diagnosis. Most of the previous
A Rule-Based <phrase>Question Answering</phrase> System For <phrase>Reading Comprehension Tests</phrase> We have developed a rule-based system, Quarc, that can reada <phrase>short story</phrase> and find the sentence in the story that best answers a given question. Quarc uses heuristic rules that look for lexical and semantic clues in the question and the story. We have tested Quarc on <phrase>reading comprehension tests</phrase> typically given to children in grades 3-6. Overall, Quarc found the correct sentence 40% of the time, which is encouraging given the simplicity of its rules. 1 Introduction In the <phrase>United States</phrase>, we evaluate the reading ability of children by giving them <phrase>reading comprehension tests</phrase>. These test typically consist of a <phrase>short story</phrase> followed by questions. Presumably , the tests are designed so that the reader must understand important aspects of the story to answer the questions correctly. For this reason, we believe that <phrase>reading comprehension tests</phrase> can be a valuable tool to assess the state of the art in <phrase>natural language understanding</phrase>. These tests are especially challenging because they can discuss virtually any topic. Consequently , broad-coverage <phrase>natural language processing</phrase> (NLP) techniques must be used. But the <phrase>reading comprehension tests</phrase> also require semantic understanding, which is difficult to achieve with broad-coverage techniques. We have developed a system called Quarc that "takes" <phrase>reading comprehension tests</phrase>. Given a story and a question, Quarc finds the sentence in the story that best answers the question. Quarc does not use deep <phrase>language understanding</phrase> or sophisticated techniques, yet it achieved 40% accuracy in our experiments. Quarc uses hand-crafted heuristic rules that look for lexical and semantic clues in the question and the story. In the next section, we de
Improving Structural Testing of <phrase>Object-Oriented Programs</phrase> via Integrating <phrase>Evolutionary Testing</phrase> and <phrase>Symbolic Execution</phrase> —<phrase>Achieving high</phrase> structural coverage such as <phrase>branch coverage</phrase> in <phrase>object-oriented programs</phrase> is an important and yet challenging goal due to two main challenges. First, some branches involve complex program logics and generating tests to cover them requires deep knowledge of the program structure and semantics. Second, covering some branches requires special method sequences to lead the receiver object or non-primitive arguments to specific desirable states. Previous work has developed the <phrase>symbolic execution</phrase> technique and the <phrase>evolutionary testing</phrase> technique to address these two challenges, respectively. However, neither technique was designed to address both challenges at the same time. To address the respective weaknesses of these two previous techniques, we propose a novel framework called Evacon that integrates <phrase>evolutionary testing</phrase> (used to search for desirable method sequences) and <phrase>symbolic execution</phrase> (used to generate desirable method arguments). We have implemented our framework and applied it to test 13 classes previously used in evaluating white-box <phrase>test generation</phrase> tools. The experimental results show that the tests generated using our framework can achieve higher <phrase>branch coverage</phrase> than the ones generated by <phrase>evolutionary testing</phrase>, <phrase>symbolic execution</phrase>, or <phrase>random testing</phrase> within the same amount of time.
Efficacy of a New charge-Balanced Biphasic Electrical Stimulus in the isolated <phrase>Sciatic nerve</phrase> and the hippocampal Slice Most <phrase>deep brain</phrase> stimulators apply rectangular monophasic voltage pulses. By modifying the stimulus shape, it is possible to optimize stimulus efficacy and find the best compromise between clinical effect, minimal side effects and <phrase>power consumption</phrase> of the stimulus generator. In this study, we compared the efficacy of three types of charge-balanced biphasic pulses (CBBPs, nominal duration 100 μs) in isolated sciatic nerves and in in vitro hippocampal brain slices of the rat. Using these two models, we tested the efficacy of several stimulus shapes exclusively on <phrase>axons</phrase> (in the <phrase>sciatic nerve</phrase>) and compared the effect with that of stimuli in the more complex neuronal network of the hippocampal slice by considering the stimulus-response relation. We showed that (i) adding an <phrase>interphase</phrase> gap (IPG, range 100-500 μs) to the CBBP enhances stimulus efficacy in the rat <phrase>sciatic nerve</phrase> and (ii) that this type of stimuli (CBBP with IPG) is also more effective in hippocampal slices. This benefit was similar for both models of voltage and current stimulation. In our two models, asymmetric CBBPs were less beneficial. Therefore, CBBPs with IPG appear to be well suited for application to DBS, since they enhance efficacy, extend battery life and potentially reduce harmful side effects.
miRDeep-P: a computational tool for analyzing the <phrase>microRNA</phrase> <phrase>transcriptome</phrase> in plants MOTIVATION Ultra-deep sampling of <phrase>small RNA</phrase> libraries by <phrase>next-generation</phrase> sequencing has provided rich information on the <phrase>microRNA</phrase> (miRNA) <phrase>transcriptome</phrase> of various plant species. However, few computational tools have been developed to effectively deconvolute the complex information.   RESULTS We sought to employ the signature distribution of <phrase>small RNA</phrase> reads along the miRNA precursor as a model in plants to profile expression of known miRNA genes and to identify novel ones. A freely available package, miRDeep-P, was developed by modifying miRDeep, which is based on a probabilistic model of miRNA <phrase>biogenesis</phrase> in animals, with a plant-specific scoring system and filtering criteria. We have tested miRDeep-P on eight <phrase>small RNA</phrase> libraries derived from three plants. Our <phrase>results demonstrate</phrase> miRDeep-P as an effective and easy-to-use tool for characterizing the miRNA <phrase>transcriptome</phrase> in plants.   AVAILABILITY http://faculty.virginia.edu/lilab/miRDP/ CONTACT: ll4jn@virginia.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at <phrase>Bioinformatics</phrase> online.
Fast simulated diffusion: an optimization algorithm for multiminimum problems and its application to <phrase>MOSFET model</phrase> parameter extraction A new algorithm, namely a fast simulated diffusion (FSD), is proposed to solve a multiminimal <phrase>optimization problem</phrase> on multidimensional continuous space. The algorithm performs a greedy search and a random search alternately and can find the global minimum with a practical success rate. A new, efficient hill-decending method employed as the greedy search in the FSD is proposed. When the FSD is applied to a set of standard test functions, it shows an <phrase>order of magnitude</phrase> faster speed than the conventional simulated diffusion. Some of the <phrase>optimization problems</phrase> encountered in system and <phrase>VLSI designs</phrase> are classified into multioptimal problems. A MOSFET parameter extraction problem is one of them and the proposed FSD is <phrase>successfully applied</phrase> to the problem with a <phrase>deep submicron</phrase> MOSFET.
An Old Friend Revisited: Countable Models of <phrase>ω-Stable Theories</phrase> We work in the context of <phrase>ω-stable theories</phrase>. We obtain a natural, algebraic equivalent of <phrase>ENI</phrase>-NDOP and discuss recent joint proofs with S. <phrase>Shelah</phrase> that if an <phrase>ω-stable</phrase> theory has either <phrase>ENI</phrase>-DOP or is <phrase>ENI</phrase>-NDOP and is <phrase>ENI</phrase>-deep, then the set of models of T with universe ω is Borel complete. In 1983 <phrase>Shelah</phrase>, Harrington, and Makkai proved Vaught's conjecture for <phrase>ω-stable theories</phrase> [11]. In that paper they determined which <phrase>ω-stable theories</phrase> have fewer than 2 ℵ 0 countable models and proved a strong structure theorem for models of such a theory. As in most verifications of Vaught's conjecture for specific classes, little attention was paid to countable models of <phrase>ω-stable theories</phrase> have 'many' models. It is curious that following the publication of [11] in 1984, the investigation of the class of countable models of an arbitrary <phrase>ω-stable</phrase> theory lay <phrase>fallow</phrase> for many years. 1 One explanation for this hiatus may have been a lack of test questions. How could one describe the complexity of a class of countable structures beyond asserting that there are 2 ℵ 0 nonisomorphic ones? A remedy was provided by the collective 1 We understand that recently Martin Koerwien has been working independently on similar problems.
<phrase>Deep Sub-micron</phrase> Sram Design for <phrase>Ultra-low</phrase> Leakage Standby Operation <phrase>Deep Sub-micron</phrase> Sram Design for <phrase>Ultra-low</phrase> Leakage Standby Operation Contents Permission to make digital or <phrase>hard copies of</phrase> all or part of this work for personal or classroom use is <phrase>granted without fee provided</phrase> that copies are not made or distributed for <phrase>profit or commercial advantage and</phrase> that <phrase>copies bear</phrase> this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, <phrase>requires prior specific permission</phrase>. Suppressing the standby current in memories is critical in <phrase>low-power design</phrase>. By lowering the <phrase>supply voltage</phrase> (<phrase>V DD</phrase>) to its standby limit, the <phrase>data retention</phrase> voltage (DRV), SRAM <phrase>leakage power</phrase> can be reduced substantially. The DRV theoretical limit is derived to be 52mV for a 90nm technology at <phrase>room temperature</phrase>. The DRV increases with transistor mismatches. Based on sub-threshold circuit analysis, a practical DRV model is developed and verified with measurement data from several test chips in 130nm and 90nm technologies. By reducing the standby <phrase>V DD</phrase> of a 32K-bit 130nm industrial IP SRAM module to 490 mV (390 mV <phrase>worst-case</phrase> DRV + 100 mV electrical-noise guard-band), an 85% leakage <phrase>power saving</phrase> is measured, compared to the <phrase>standby power</phrase> at 1V. Since the DRV is a strong function of both process and design parameters, the SRAM cell can be optimized to reduce DRV. It is shown that the body bias and device channel length are the most effective knobs in minimizing DRV. This is confirmed with measurement data from a 90nm SRAM <phrase>test chip</phrase>. Building on these results, feasibility of a 270mV standby <phrase>V DD</phrase> is demonstrated for an optimized 4K-bit SRAM in a 90nm technology, leading to a 97% <phrase>leakage power</phrase> reduction. By dynamically configuring the body bias during read and write operations, the active operation <phrase>noise margins</phrase> and data access speed are also improved according to <phrase>simulation results</phrase>. Correcting the <phrase>low-voltage</phrase> retention errors with error correction code (ECC) provides another opportunity to further reduce the SRAM standby <phrase>V DD</phrase>. To establish a power-per-bit metric, the SRAM <phrase>leakage power</phrase> is modeled as a function of the ECC parameters, DRV distribution and the standby <phrase>V DD</phrase>. This power metric is optimized based on ECC theory to obtain fundamental bounds of the <phrase>power saving</phrase> enabled by error-tolerant design. Taking into account the practical design requirements, an error-tolerant SRAM design with a (31, 26) <phrase>Hamming code</phrase> is proposed introducing a further <phrase>power reduction</phrase> of 33%. Both the circuit optimization and the error-tolerant architecture are …
<phrase>High-resolution</phrase> <phrase>Hurricane</phrase> Forecasts Problem Overview Widely varying scales of atmospheric motion make it extremely difficult to predict <phrase>hurricane</phrase> intensity, even after decades of research. A new model capable of resolving a hurricane's deep <phrase>convection</phrase> motions was tested on a large sample of Atlantic <phrase>tropical cyclones</phrase>. Results show that using finer resolution can improve storm intensity predictions. P redicting a hurricane's intensity remains a daunting challenge even after four decades of research. The intrinsic difficulties lie in the vast range of spatial and temporal scales of atmospheric motions that affect <phrase>tropical cyclone</phrase> intensity. The range of spatial scales is literally millimeters to 1,000 kilometers or more. Atmospheric dynamical models must account for all these scales simultaneously. Being a non-linear system, these scales can interact. Although forecasters must make approximations to keep computations finite, there's a continued push for finer resolution to capture as many of these scales as possible. From the present standpoint of computational feasibility, a model's minimum grid lengths that still allow modeling of the storm and its near environment are a few hundred horizontal meters and approximately 50 to 100 vertical meters. Most current weather prediction uses grid-based rather than spectral-based models (such as Fourier or some other <phrase>basis function</phrase>). 1 <phrase>Statistical analysis</phrase> of energy spectra reveal that motions with scales smaller than approximately six to seven grid points aren't well resolved. 2 Therefore, the minimum resolvable physical length scales are nearly 1 km horizontally and perhaps 300 m vertically. Given current computing capability, however, timely numerical forecasts must be run on much coarser grids. What does this mean for <phrase>hurricane</phrase> forecasts? We believe that it's important to resolve clouds— at least the largest <phrase>cumulonimbus</phrase>-producing <phrase>thunderstorms</phrase>. These clouds have a horizontal scale of at most a few kilometers and thus can be resolved only with a 1 km or less horizontal grid spacing. These clouds span the troposphere's vertical extent—12 to 16 km—and so are relatively easy to resolve in the vertical if 30 to 40 layers are used. Nevertheless, to cover the region affected by a <phrase>hurricane</phrase> in a five-day forecast requires a horizontal domain of perhaps 5,000 km. A volume with a grid increment of 500 m horizontally and 250 m vertically over a domain of depth 25 km contains roughly 10 10 grid points. Because the equations of motion are first-order in time, knowing the model state at one time lets us, in principle, predict the state a short time …
Extending the Embedded System E-TDDunit <phrase>Test Driven Development</phrase> Tool for Development of a Real Time Video Security System Prototype Despite the existence of 75 " different " <phrase>xUNIT</phrase> frameworks, their domain of application differs only in the <phrase>programming language</phrase>, compiler or operating system supported. If one is working in the embedded world, <phrase>unit testing</phrase> is still needed, but now our " testing requirements " differ significantly from the testing framework needed for the desktop world. <phrase>Embedded systems</phrase> often have significant non-<phrase>functional requirements</phrase> , which demand validation at the unit level. In addition, they interact intimately with hardware resources and often have only very limited <phrase>input/output</phrase> capabilities – imagine a <phrase>xUNIT</phrase> framework where <phrase>printing</phrase> to the screen is a technical challenge! There have been a number of notable efforts in migrating Agile ideas into the embedded environment but only one or two intrepid practitioners have braved this new domain deep down towards and into the " <phrase>plumbing</phrase> " layer of small <phrase>embedded systems</phrase>. The purpose of this abstract is to demonstrate extensions of an embedded system <phrase>test driven development</phrase> tool (E-TDDUnit [1]) to permit the development of a real-time security system prototype (Fig. 1) around an <phrase>Analog Devices</phrase> ADSP-BF533 <phrase>Blackfin</phrase> Processor EZ-Kit Lite evaluation board. This initial solution and tests were successfully ported to a newly available BF537 system (with both video and Internet connection); demonstrating the practicality of the approach. E-TDDUnit is a custom-ized CPPUnitLite version [2] adopted and modified so that the tests could run on the real embedded system where the timing relationships were not just seen through a simulated environment. The code development for this project can be recognized as having two distinct stages, commonly found in embedded applications involving video and <phrase>telecommunications</phrase>. An attempt to run the same set of E-TDDUnit tests for (1) the <phrase>double precision</phrase> <phrase>floating-point</phrase> MATLAB prototyping phase (develop and test <phrase>signal processing</phrase> algorithms) and (2) a code migration phase, where the algorithm implementation must meet the time and precision requirements of running on a <phrase>fixed point</phrase> processor, was abandoned because of the impractical C++ / MATLAB interface.
Fast <phrase>simulation based</phrase> testing of anti-tearing mechanisms for small <phrase>embedded systems</phrase> —Small <phrase>embedded systems</phrase> are often powered by unreliable power supplies like <phrase>energy harvesting</phrase> systems (e.g., for sensor nodes) or external power supplies (for <phrase>smart cards</phrase>). For secure <phrase>embedded systems</phrase> a sudden loss of power can violate <phrase>data integrity</phrase>. The power has just to drop when data is written to <phrase>non-volatile memory</phrase>. Thinking about a <phrase>byte</phrase> array in a <phrase>smart card</phrase> representing some digital money of an e-purse, this becomes obvious. In order to guarantee <phrase>data integrity</phrase> a secure embedded system has to provide an anti-tearing mechanism. Testing this mechanism is very difficult, extensive, and requires <phrase>deep inside</phrase> knowledge into its <phrase>implementation details</phrase>. In this work we show how a simulation of an embedded system can be used to test the anti-tearing mechanism. <phrase>High-level</phrase> <phrase>test cases</phrase> are used to generate <phrase>test vectors</phrase> automatically. The proposed approach allows a fast and comprehensive test of the anti-tearing mechanism. We 1 explain our proposed mechanism on the basis of a case study of a <phrase>smart card</phrase> system. However, the mechanism is general enough to be used for secure <phrase>embedded systems</phrase> of any kind.
Characterization of <phrase>Optical Interconnects</phrase> Characterization of <phrase>Optical Interconnects</phrase> Title: Associate Professor of <phrase>Electrical Engineering</phrase> and Computer Sciences Interconnect has become a major issue in <phrase>deep sub-micron technology</phrase>. Even with copper and low-k dielectrics, parasitic eeects of interconnects will eventually impede advances in integrated electronics. One technique that has the potential to provide a <phrase>paradigm shift</phrase> is <phrase>optics</phrase>. This project evaluates the feasibility of <phrase>optical interconnects</phrase> for distributing data and clock signals. In adopting this scheme, variation is introduced by the detector, the <phrase>waveguides</phrase>, and the <phrase>optoelectronic</phrase> circuit, which includes device, <phrase>power supply</phrase> and temperature variations. We attempt to characterize the eeects of the aforementioned sources of variation by designing a baseline <phrase>optoelectronic</phrase> circuitry and fabricating a test chip which consists of the circuitry and detectors. Simulations are also performed to supplement the eeort. The results are compared with the performance of traditional metal interconnects. The feasibility of <phrase>optical interconnects</phrase> is found to besensitive to the <phrase>optoelectronic</phrase> circuitry used. Variation eeects from the devices and <phrase>operating conditions</phrase> have profound impact on the performance of <phrase>optical interconnects</phrase> since they introduce substantial skew and delay in the otherwise ideal system. 2 3 Acknowledgments I would like to thank Professor Boning and Professor Chandrakasan for their time and eeort in guiding and motivating this work. Their encouragement and trust were crucial to the completion of the project. I w ould also like to thank Kush for his willingness and sellessness in providing insights and enlightening suggestions whenever I encountered problems along the way. I am grateful to Desmond and Andy for imparting their knowledge of device physics and for assisting in the design and testing of the detector. In addition, I want to thank Paul-Peter for his assistance in building the prototype. Heartfelt appreciation is extended to Jim and Illiana for oering invaluable help during the layout of the chip. I w ant to thank Rong-<phrase>Wei</phrase> for his <phrase>constant support</phrase>, be it emotional or technical, and for his belief in my ability, m o r e s o than myself. Lastly, I would like to thank my family for their love, support, and sacriices. My father, for ooering insights and inspirationss my mother, for the loving care and gentle encouragementss my little brother, for the mysteriously timed phone calls and laughter that we share.
A 2D unknown contour recovery method immune to system non-linear effects — A method to recover general 2D a priori unknown contours using a kind of special optic sensor is described. Contour recovery is an important task for exploratory operations in unknown environments as well as for more practical applications such as grinding or deburring. It is not an easy task since the recovered contour (generally obtained using encoder data) is severely distorted due to errors in the kinematic model of the robot and to the non-linearities of its actuators. Some <phrase>mathematical models</phrase> have been presented to partially compensate for those effects, but they require a deep knowledge of both the robot and sensor models which are difficult to obtain accurately, and normally imply an adaptive non-linear control to estimate some of the unknown parameters of the model. Our approach, in despite of its simplicity, is intrinsically immune to non-linearities, which allows us to eliminate most of the distortions added to the sensor data. A simple algorithm to follow unknown planar contours is presented and used to test the performance of this approach in comparison to the one using encoder data. <phrase>Experimental results</phrase> and practical problems are also discussed.
A method-based ahead-of-time compiler for android applications The execution environment of Android system is based on a <phrase>virtual machine</phrase> called Dalvik <phrase>virtual machine</phrase> (DVM) in which the execution of an application program is in interpret-mode. To reduce the interpretation overhead of DVM, Google has included a trace-based just-in-time compiler (JITC) in the latest version of Android. Due to limited resources and the requirement for reasonable response time, the JITC is unable to apply deep optimizations to generate <phrase>high quality</phrase> code. In this paper, we propose a method-based ahead-of-time compiler (AOTC), called Icing, to speed up the execution of Android applications without the modification of any components of Android framework. The main idea of Icing is to convert the hot methods of an application program from DEX code to C code and uses the <phrase>GCC</phrase> compiler to translate the C code to the corresponding native code. With the <phrase>Java Native Interface</phrase> (<phrase>JNI</phrase>) library, the translated native code can be called by DVM. Both AOTC and JITC have their strength and weakness. In order to combine the strength and avoid the weakness of AOTC and JITC, in Icing, we have proposed a cost model to determine whether a method should be handled by AOTC or JITC during profiling. To evaluate the performance of Icing, four benchmarks used by Google JITC are used as <phrase>test cases</phrase>. The performance results show that, with Icing, the execution time of an application is two to three times faster than that without JITC, and 25% to 110% faster than that with JITC.
Effects of sequential and discrete <phrase>rapid naming</phrase> on reading in Japanese children with reading difficulty. To clarify whether <phrase>rapid naming</phrase> ability itself is a main underpinning factor of rapid automatized naming tests (RAN) and how deep an influence the discrete decoding process has on reading, we performed <phrase>discrete naming</phrase> tasks and discrete <phrase>hiragana</phrase> reading tasks as well as sequential naming tasks and sequential <phrase>hiragana</phrase> reading tasks with 38 Japanese schoolchildren with reading difficulty. There were high correlations between both discrete and sequential <phrase>hiragana</phrase> reading and sentence reading, suggesting that some mechanism which automatizes <phrase>hiragana</phrase> reading makes sentence reading fluent. In object and color tasks, there were moderate correlations between sentence reading and sequential naming, and between sequential naming and discrete naming. But no correlation was found between reading tasks and discrete naming tasks. The influence of <phrase>rapid naming</phrase> ability of objects and colors upon reading seemed relatively small, and multi-item processing may work in relation to these. In contrast, in the digit naming task there was moderate correlation between sentence reading and discrete naming, while no correlation was seen between sequential naming and discrete naming. There was moderate correlation between reading tasks and sequential digit naming tasks. Digit <phrase>rapid naming</phrase> ability has more direct effect on reading while its effect on RAN is relatively limited. The ratio of how <phrase>rapid naming</phrase> ability influences RAN and reading seems to vary according to kind of the stimuli used. An assumption about components in RAN which influence reading is discussed in the context of both sequential processing and discrete naming speed.
<phrase>Deep Neural Networks</phrase> Segment Neuronal Membranes in <phrase>Electron Microscopy</phrase> Images We address a central problem of <phrase>neuroanatomy</phrase>, namely, the automatic segmen-tation of neuronal structures depicted in stacks of <phrase>electron microscopy</phrase> (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep <phrase>artificial neural network</phrase> as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and <phrase>max-pooling</phrase> layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain <phrase>gradient descent</phrase> on a 512 × 512 × 30 stack with known <phrase>ground truth</phrase>, and tested on a stack of the same size (<phrase>ground truth</phrase> unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific <phrase>post-processing</phrase>, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, <phrase>warping</phrase> error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.
Early voltage and saturation voltage improvement in <phrase>deep sub-micron technologies</phrase> using associations of transistors The design of analog <phrase>integrated circuits</phrase> together with <phrase>mixed-signal</phrase> applications in deep sub-micron technologies is a difficult task, since state-of-the-art technologies and minimum channel length transistors, suitable for <phrase>digital circuits</phrase>, are very rarely optimized for analog <phrase>block design</phrase>. Non-desired effects are present shortest transistors, leading mainly to a high output conductance, which is disadvantageous for gain in the stages. In this work, we present measurement results supporting the associations of transistors concept to be used in such applications: the T-Shaped Transistor (TST). The main characteristic of this association is its <phrase>trapezoidal</phrase> nature, with no limit on the sizes of the unit composite transistors, providing lower output conductance and saturation voltage in comparison to regular configurations. Such electrical characteristics are demonstrated by means of electrical simulations and electrical measurements of a test chip fabricated by MOSIS in an IBM 0.18&#956;m <phrase>CMOS process</phrase>.
Modeling <phrase>Freeway</phrase> <phrase>Lane Changing</phrase> Behavior Modeling <phrase>Freeway</phrase> <phrase>Lane Changing</phrase> Behavior Drivers continuously evaluate the surrounding traffic and the roadway environment, and make decisions about lanes and travel speed. The objective of this thesis is to develop a <phrase>lane changing</phrase> model that can be used in microscopic traffic simulation models to capture drivers' <phrase>lane changing</phrase> behavior. <phrase>Lane change</phrase> is modeled as a sequence of four steps: decision to consider a <phrase>lane change</phrase>, choice of left or right lane, search for an acceptable gap to execute the decision, and performing the <phrase>lane change</phrase> maneuver. First, a decision is made whether a driver will consider <phrase>changing lanes</phrase>. If a decision to consider <phrase>changing lanes</phrase> is made, a lane is chosen from the alternatives. Finally, the gap acceptance model determines whether the available gap in the target lane is sufficient for a safe merging and <phrase>lane change</phrase> can be completed. A discrete choice framework is used to model the <phrase>lane changing</phrase> behavior. The framework allows for modeling the impact of different elements of the traffic and roadway environment on driver behavior. The model is applied in the special case of merging from an on-ramp. Results from the estimation of the parameters show that in addition to the gap length, other important factors that affect drivers gap acceptance behavior are relative speed, distance remaining to the point at which <phrase>lane change</phrase> must be complete, and delay in completing merging. Finally, the estimated model is tested in a micro-simulation environment. ACKNOWLEDGEMENT I acknowledge with deep gratitude the guidance and constant inspiration provided by my was a privilege to work with them. I have learned a lot from them during the course of this research. My special thanks goes to <phrase>Qi</phrase> Yang-an extremely helpful and always encouraging friend. Sometimes he came out of his way to help me that made me feel guilty lot of times. I wish him success in life. grateful to the Center for Transportation Studies at MIT for awarding me admission with financial aid without which MIT would have been a dream place to me. I would like to thank all my friends that made my stay at MIT an enjoyable experience Department for the friendship and their support. And finally, I wish I knew the exact words to express my indebtedness to Lubna, my wife, for her <phrase>constant support</phrase>, care, and endless love, to my parents for having <phrase>faith</phrase> in me and their love, encouragement, and constant inspiration that helped me outgrow …
Multiple-Description Wavelet based <phrase>Image Coding</phrase> We consider the problem of coding images for transmission over error-prone channels. The impairments we target are transient channel shutdowns, as would occur in a packet network when a packet is lost, or in a wireless system during a deep fade: when data is delivered it is assumed to be error-free, but some of the data may never reach the receiver. The proposed algorithms are based on a combination of multiple description scalar quantizers with techniques <phrase>successfully applied</phrase> to the construction of some of the most efficient subband coders. A given image is encoded into multiple independent packets of roughly equal length. When packets are lost, the quality of the approximation computed at the receiver depends only on the number of packets received, but does not depend on exactly which packets are actually received. When compared with previously reported results on the performance of robust image coders based on multiple descriptions, on standard test images, our coders attain similar PSNR values using typically about 50-60% of the <phrase>bit rate</phrase> required by these other state-of-the-art coders, while at the same time providing significantly more freedom in the mechanism for allocation of redundancy among descriptions.
A Parallel Circuit-Partitioned Algorithm for Timing Driven Cell Placement <phrase>Simulated annealing</phrase> based <phrase>standard cell</phrase> placement for <phrase>VLSI designs</phrase> has long been acknowledged as a compute-intensive process. All previous work in parallel <phrase>simulated annealing</phrase> based placement has minimized area, but with <phrase>deep submicron</phrase> design, minimizing wirelength delay is also needed. The algorithm discussed in this paper is the first <phrase>parallel algorithm</phrase> for timing driven placement. We have used a very accurate Elmore delay model which is more compute intensive and hence the need for parallel placement is more apparent. Parallel placement is also needed for very large circuits that may not fit in the memory of a single processor. Therefore, our algorithm is circuit partitioned and can handle arbitrary large circuits on <phrase>distributed memory</phrase> multiprocessors. The algorithm, called mpiPLACE, has been tested on several large benchmarks on a variety of parallel architectures.
<phrase>Edge detection</phrase> of noisy images based on cellular <phrase>neural networks</phrase> This paper studies a technique employing both cellular <phrase>neural networks</phrase> (CNNs) and linear matrix inequality (LMI) for <phrase>edge detection</phrase> of noisy images. Our main work focuses on training templates of <phrase>noise reduction</phrase> and <phrase>edge detection</phrase> CNNs. Based on the <phrase>Lyapunov stability</phrase> theorem, we derive a criterion for global asymptotical stability of a unique equilibrium of the <phrase>noise reduction</phrase> CNN. Then we design an approach to train <phrase>edge detection</phrase> templates, and this approach can detect the edge precisely and efficiently, i.e., by only one iteration. Finally, we illustrate performance of the proposed methodology from the aspect of peak <phrase>signal to noise ratio</phrase> (PSNR) through computer simulations. Moreover, some comparisons are also given to prove that our method outperforms classical operators in gray image <phrase>edge detection</phrase>. It is well known that the Hopfield <phrase>neural network</phrase> (HNN) requires <phrase>fully connected</phrase> and grows exponentially with the size of the array. Thus it is very difficult to implement, even in modest array sizes, as VSLI circuits [1,2]. A novel class of <phrase>information processing</phrase> system called cellular <phrase>neural network</phrase> (CNN) was proposed by Chua and Yang in 1988, which came from the HNN and <phrase>cellular automata</phrase> as an effective combination of both characteristics [3,4]. Moreover, the CNN has two prominent features: real-time <phrase>signal processing</phrase> capability and local connection. On one hand, the characteristic of real-time <phrase>signal processing</phrase> has been extensively exploited in various applications such as parallel <phrase>signal processing</phrase>, image <phrase>edge detection</phrase>, connected component detection and various morphology operations (dilation, <phrase>erosion</phrase> and hole filling, etc.). On the other hand, the characteristic of local connection makes it applicable to VLSI implementation and allows to operate at a very <phrase>high speed</phrase> in real time. With <phrase>deep submicron technology</phrase> (0.25 um-0.33 um), an array of 100 Â 100 large analog processors array can be implemented on a single chip, whose theoretical computation speed can be at least a thousand times faster than the current digital processor [5]. Some smaller operational test chips have also been designed [6–8]. As a result of this rapid development, the CNNs have been widely studied for practical applications in image and video <phrase>signal processing</phrase>, robotic and biological visions and higher brain functions [9–12]. The most important key point of CNNs applications is how to find the satisfactory feedback template ''A'', control template ''B'' and bias ''I''. In recent years, the problem of CNN design for <phrase>image processing</phrase> has attracted considerable attention [13–20] and the …
<phrase>Pattern Matching</phrase> Using Layered Strifa for <phrase>Intrusion Detection</phrase> With the advent and explosive growth of the global Internet adaptive/automatic network intrusion and <phrase>anomaly detection</phrase> in wide area data networks is fast gaining critical research and practical importance. In order to detect intrusions in a network, need efficient IDS. <phrase>Deep packet inspection</phrase> (DPI) has the ability to inspect both packet headers and payloads to identify the attack signatures in order to protect Internet systems. <phrase>Regular expression</phrase> matching, despite its flexibility and efficiency in attack detection, brings high computation and storage complexities to NIDSs, making packet processing a bottleneck. Stride <phrase>finite automata</phrase> (StriFA), a new family of <phrase>finite automata</phrase>, to accelerate both <phrase>string matching</phrase> and <phrase>regular expression</phrase> matching with reduced memory consumption. To increase the efficiency of StriFA, a layered approach of attack detection by using KDD 99 <phrase>DARPA</phrase> dataset is integrated with StriFA. We demonstrate that attack detection accuracy can be achieved by using StriFA and high efficiency by implementing the Layered Approach. I. INTRODUCTION Intrusions are the abnormal events happening in the computer system or network which attempts to compromise the confidentiality and availability of data or a system or a network. Intrusions are caused by attackers who seek to gain extra prerogatives by getting at a system from the internet; however they may be unauthorized user or the authorized users misusing their prerogatives. <phrase>Intrusion detection</phrase> is the mechanism of supervising events occurring in the networks to detect the abnormal <phrase>behaviours</phrase> of events i.e. intrusions. The most common approaches in <phrase>intrusion detection system</phrase> are <phrase>anomaly detection</phrase> and misuse detection. <phrase>Anomaly detection</phrase> can identify the activities that vary from the common behaviour, and thus have the potential to detect novel attacks. An approach for detecting intrusions is to conceptualize both the normal and the known <phrase>attack patterns</phrase> for training a system, then performing classification of the test data. It integrates the advantages of both the signature-based and the anomaly-based detections, known as the Hybrid System. Hybrid systems are effective, subject to the categorization method used. They can be used to classify the unseen or new instances when occur, and then they assign one of the known classes to every test instance, because during training the system learns patterns and features from all the classes. But the only problem with the hybrid systems is the availability of labelled data. However, data requirement is also a concern for the signature, and the anomaly-based
Challenges and Opportunities for FPGA Programmable System Platforms <phrase>Process technology</phrase> and architecture innovation are the two engines that have fueled a spectacular advancement in FPGAs over the past 10 years. During this period, the price of FPGAs has been reduced with 2 <phrase>orders of magnitude</phrase>, the logic capacity of FPGAs has been increased with 2 <phrase>orders of magnitude</phrase> and the performance has been increased with one <phrase>order of magnitude</phrase>. Whereas ASICs buck the <phrase>tide</phrase> of processing technology, FPGAs ride the <phrase>tide</phrase>. <phrase>Deep submicron</phrase> effects are breaking the traditional <phrase>modular design</phrase> flow of traditional SOC architectures. A growing portion of the design time is spent on dealing with <phrase>deep submicron</phrase> effects, at the expense of the creative process of design authoring. Surveys show that, today, less than 20% of the design time, for complex SOCs, is spent on design authoring. Programmable FPGA platforms give designers the benefits of <phrase>deep submicron</phrase> but rather than focusing on getting the silicon to work, you can focus on getting the design to work. Today, several million <phrase>logic gates</phrase> can be implemented in FPGAs, as such covering the sweet spot of the ASIC market. By combining the programmable nature of the FPGA with advanced testing methodologies, the fitness of a specific piece of silicon can be guaranteed for a given application. The increased yield associated with the custom testing approach allows to further reduce price of large FPGAs with more than 80%. As a consequence, FPGAs are being used for ever higher product volumes. FPGAs, through their regular, parallel architecture and <phrase>distributed memory</phrase> organization can continue to take benefit of scaling dimensions by adding more parallel hardware and <phrase>distributed memory</phrase>. <phrase>Von Neumann</phrase> architectures, originating from the days when <phrase>silicon area</phrase> was very limited, have been dictating sequential programming models. Spatial computing, exploiting massive resources of parallel hardware will change the way we program future system platforms. We predict that FPGAs will become the heart of most systems over the next 5 years, replacing ASICs and processors as the fabric of choice.
Pooling Faces: <phrase>Template Based</phrase> <phrase>Face Recognition</phrase> with Pooled <phrase>Face Images</phrase> We propose a novel approach to <phrase>template based</phrase> <phrase>face recognition</phrase>. Our dual goal is to both increase recognition accuracy and reduce the computational and storage costs of template matching. To do this, we leverage on an approach which was proven effective in many other domains, but, to our knowledge, never fully explored for <phrase>face images</phrase>: average pooling of face photos. We show how (and why!) the space of a template's images can be partitioned and then pooled based on image quality and head pose and the effect this has on accuracy and template size. We perform <phrase>extensive tests</phrase> on the IJB-A and <phrase>Janus</phrase> CS2 <phrase>template based</phrase> <phrase>face identification</phrase> and verification benchmarks. These show that not only does our approach outperform published state of the art despite requiring far fewer cross template comparisons , but also, surprisingly, that image pooling performs on par with deep feature pooling.
Understanding the Salience of <phrase>Cognitive Diversity</phrase> in Face-to-Face and Computer-Mediated Teams In the group <phrase>decision-making</phrase> literature, the effects of diversity on group interaction and performance have also been well investigated. However, with the increasing reliance of organizations on <phrase>collaborative technology</phrase>, it is not clear whether <phrase>cognitive diversity</phrase> affects the <phrase>decision-making</phrase> processes and group performance across cultural and geographic boundaries in a similar fashion as it does in traditional face-to-face teams. Our goal of this study, thus, is to compare the effects of <phrase>cognitive diversity</phrase> on group interaction and decision outcomes in traditional teams and computer-mediated teams. Considering the two dimensions of <phrase>cognitive diversity</phrase>, we suggest that in traditional teams, surface-level <phrase>cognitive diversity</phrase> reduces team members' satisfaction with group interaction; while <phrase>deep-level</phrase> <phrase>cognitive diversity</phrase> improves team members' satisfaction with group decisions. In comparison, the negative effect of surface-level <phrase>cognitive diversity</phrase> will be mitigated in <phrase>virtual teams</phrase>, whereas the positive effect of <phrase>deep-level</phrase> <phrase>cognitive diversity</phrase> will be strengthened. The study is empirically tested using an intellective <phrase>decision-making</phrase> task. Results provide partial support for our hypotheses, and shed light on both research and practice involving technology-mediated teams.
<phrase>Neural Network-based</phrase> Underwater <phrase>Image Classification</phrase> for Autonomous <phrase>Underwater Vehicles</phrase> <phrase>Image processing</phrase> has been one of hot issues for <phrase>real world</phrase> robot applications such as navigation and visual servoing. In case of underwater robot application, however, conventional optical camera-based images have many limitations for real application due to visibility in <phrase>turbid</phrase> water, image saturation under underwater light in the <phrase>deep water</phrase>, and short visible range in the water. Thus, most of underwater image applications use <phrase>high frequency</phrase> <phrase>sonar</phrase> to get precise acoustic image. There have been some approaches to apply optical <phrase>image processing</phrase> methods to acoustic image, but performance is still not good enough for <phrase>automatic classification</phrase>/recognition. In this paper, a <phrase>neural network-based</phrase> <phrase>image processing</phrase> algorithm is proposed for acoustic <phrase>image classification</phrase>. Especially, shadow of an acoustic object is mainly used as a cue of the classification. The neural network classifies a pre-taught image from noisy and/or occlude object images. In order to get fast learning and retrieving, a Bidirectional <phrase>Associative Memory</phrase> (BAM) is used. It is remarked that the BAM doesn't need many learning trials, but just simple <phrase>multiplication</phrase> of two vectors for generating a correlation matrix. However, because of the simple calculation, it is not guaranteed to learn and recall all <phrase>data set</phrase>. Thus, it is needed to modify the BAM for improving its performance. In this paper, <phrase>complement</phrase> <phrase>data set</phrase> and weighted learning factor are used to improve the BAM performance. The test results show that the proposed method successfully classified 4 pre-taught object images from various underwater object images with up to 50% of B/W noise.
<phrase>Oracle Big Data Connectors</phrase> <phrase>Big Data</phrase> for the Enterprise <phrase>Key Features</phrase>  Tight Integration with <phrase>Oracle Database</phrase> <phrase>Oracle Big Data Connectors</phrase> Oracle <phrase>Sql</phrase> Connector for Hadoop Distributed File System  Leverage Hadoop compute resources for data in HDFS  Enable Oracle <phrase>SQL</phrase> to access and load Hadoop data  Fast and very efficient load from Hadoop into <phrase>Oracle Database</phrase>  Partition pruning of Hive tables during load and query  <phrase>Graphical user interfaces</phrase> of Oracle Data Integrator drive <phrase>data transformation</phrase> workflows on <phrase>Hadoop </phrase> Automatically transform R programs into Hadoop jobs  Process large volumes of XML files in parallel and load XQuery results into the database  Access data in HDFS securely with Kerberos <phrase>authentication</phrase> KEY BENEFITS  Quickly deliver data discovery applications to business users Query data in-place in Hadoop with Oracle <phrase>SQL</phrase>  Extremely fast data loading between Hadoop and <phrase>Oracle Database</phrase> while minimizing database CPU utilization during load  Enable data scientists to use R on data in Hadoop and combine with advanced analytics in the database  Process extremely large volumes of <phrase>XML data</phrase> in <phrase>Hadoop </phrase> Reduce the complexities of Hadoop through graphical tooling  Integrated and tested on <phrase>Big Data</phrase> Appliance  Easy-to-use for Hadoop and Oracle Developers <phrase>Oracle Big Data Connectors</phrase> is a <phrase>software suite</phrase> that integrates processing in Hadoop with operations in a <phrase>data warehouse</phrase>. Designed to leverage the latest features of <phrase>Apache Hadoop</phrase>, <phrase>Big Data Connectors</phrase> connect Hadoop clusters with database infrastructure to <phrase>harness</phrase> massive volumes of structured and <phrase>unstructured data</phrase> for critical business insights. <phrase>Big Data Connectors</phrase> greatly simplify development and are optimized for efficient connectivity and <phrase>high-performance</phrase> between <phrase>Oracle Big Data</phrase> Appliance and Oracle Exadata. <phrase>Oracle Big Data Connectors</phrase> 3.0 delivers a rich set of new features, increased connectivity, enhanced performance, and security for <phrase>Big Data</phrase> applications. Large volumes of data are increasingly collected and processed in Hadoop, while enterprise IT systems are centered on relational <phrase>data warehouses</phrase>. <phrase>Oracle Big Data Connectors</phrase> bridges <phrase>data processing</phrase> in Hadoop with <phrase>Oracle Database</phrase>, providing the crucial ability to unify data across these systems. Combining pre-processing of large data volumes of raw and <phrase>unstructured data</phrase> in Hadoop with the advanced analytics, complex <phrase>data management</phrase>, and real-time query capabilities of <phrase>Oracle Database</phrase>, <phrase>Oracle Big Data Connectors</phrase> deliver features that support information discovery, deep analytics and fast integration of all data in the enterprise. The components of this <phrase>software suite</phrase> are: <phrase> Oracle</phrase> <phrase>SQL</phrase> Connector for Hadoop Distributed File System <phrase> Oracle</phrase> Loader for Hadoop <phrase> Oracle</phrase> Data Integrator Application Adapter for Hadoop <phrase> Oracle</phrase> R Advanced Analytics for Hadoop <phrase> Oracle</phrase> XQuery …
Exploratory Engineering in Ai We regularly see examples of new <phrase>artificial intelligence</phrase> (AI) capabilities. Google's self-driving car has safely traversed thousands of miles. Watson beat the <phrase>Jeopardy</phrase>! champions, and <phrase>Deep Blue</phrase> beat the chess champion. <phrase>Boston</phrase> Dynamics' Big <phrase>Dog</phrase> can walk over uneven terrain and right itself when it <phrase>falls</phrase> over. From many angles, software can recognize faces as well as people can. As their capabilities improve, AI systems will become increasingly independent of humans. We will be no more able to monitor their decisions than we are now able to check all the math done by today's computers. No doubt such automation will produce tremendous economic value, but will we be able to trust these advanced autonomous systems with so much capability? For example, consider the autonomous trading programs which lost <phrase>Knight</phrase> Capital $440 million (pre-<phrase>tax</phrase>) on August 1st, 2012, requiring the <phrase>firm</phrase> to quickly raise $400 million to avoid <phrase>bankruptcy</phrase>. 1 This event undermines a common view that AI systems cannot cause much harm because they will only ever be tools of human masters. Autonomous trading programs make millions of trading decisions per day, and they were given sufficient capability to nearly <phrase>bankrupt</phrase> one of the largest traders in U.S. equities. Today, AI <phrase>safety engineering</phrase> mostly consists in a combination of <phrase>formal methods</phrase> and testing. Though powerful, these methods lack foresight: they can be applied only to particular extant systems. We describe a third, complementary approach which aims to predict the (potentially hazardous) properties and behaviors of broad classes of future AI agents, based on their <phrase>mathematical structure</phrase> (e.g. <phrase>reinforcement learning</phrase>). Such projects hope to discover methods "for determining whether the behavior of learning agents [will remain] within the bounds of pre-specified constraints... after learning." 2 We call this approach "exploratory engineering in AI."
Development of an Improved GUI Automation Test System Based on Event-Flow Graph A more automated graphic <phrase>user interface</phrase> (GUI) test model, which is based on the event-flow graph, is proposed. In the model, a <phrase>user interface</phrase> automation API tool is first used to carry out <phrase>reverse engineering</phrase> for a <phrase>GUI test</phrase> sample so as to obtain the event-flow graph. Then two approaches are adopted to create <phrase>GUI test</phrase> sample cases. That is to say, an improved <phrase>ant colony optimization</phrase> (<phrase>ACO</phrase>) algorithm is employed to establish a sequence of testing cases in the course of the daily smoke test. The sequence goes through all object event points in the event-flow graph. On the other hand, the <phrase>spanning tree</phrase> obtained by deep <phrase>breadth-first search</phrase> (BFS) approach is utilized to obtain the testing cases from goal point to outset point in the course of the deep regression test. Finally, these cases are applied to test the new GUI. Moreover, according to the above-mentioned model, a corresponding prototype system based on Microsoft UI automation framework is developed, thus giving a more effective way to improve the GUI automation test in Windows OS.
<phrase>Web-based</phrase> Personalised System of Instruction: an Effective Approach for Diverse Cohorts with Virtual Learning Environments? <phrase>Computer-mediated Communication</phrase> Improving Classroom Teaching Media in Education Post-<phrase>secondary Education</phrase> Teaching/learning Strategies The Personalised System of Instruction is a form of mastery learning which, though it has been proven to be educationally effective, has never seriously challenged the dominant lecture-tutorial teaching method in <phrase>higher education</phrase> and has largely fallen into disuse. An <phrase>information and communications technology</phrase> assisted version of the Personalised System of Instruction using a <phrase>virtual learning environment</phrase> is promoted here based on the authors " longitudinal design <phrase>research into</phrase> this pedagogy. The particular elements of the <phrase>virtual learning environment</phrase> which are promoted are short video clips, online formative tests and an assessment management system. The authors present their experiences of developing and deploying this pedagogy for the teaching of introductory <phrase>discrete mathematics</phrase> to large classes of <phrase>Computer Science</phrase> students at two <phrase>UK</phrase> <phrase>higher education</phrase> institutions both with whole cohorts and " at risk " groups of students. In particular, this method is promoted as particularly helpful to students who do not adopt a deep approach to learning as many students fail to do. Moreover " at risk " students using this method (n = 71) demonstrated an average Glass <phrase>effect size</phrase> of 0.83 compared with other " at risk " students who did not (n = 35). Based on these experiences, this pedagogy is promoted as an effective approach to teaching in <phrase>higher education</phrase>, especially the teaching of cognitive skills to diverse cohorts of students on foundation level modules.
The use of ICT in the <phrase>public sector</phrase> and its influence on communication with citizens in <phrase>Slovenia</phrase> The internet has become one of the most important means of communication in all areas of our life. In the paper we focused on central and local government bodies and their attitude towards information and communication technology. By analysing <phrase>web pages</phrase>, inquiring public servants and testing the responses on citizens' questions we tried to discover influences of Internet on better informing of citizens, their participation in making decisions of public interest and communication between citizens and central and local government bodies. 1. INTRODUCTION Over the last few years the Internet has become one of the most important means of communication in all social areas and the <phrase>public sector</phrase> is no exception. However, the expectations of experts from the administrative field and those who are engaged in <phrase>public sector</phrase> from a more organisational, <phrase>sociological</phrase> or <phrase>political</phrase> perspective are enormous [Schalken, 2000]. Taking into account the nature of the <phrase>public sector</phrase> where efficient collection, processing, storing, distribution and exchange of information between administrative bodies and between administrative bodies and citizens is one of the fundamental activities, expectations of deep changes and advantages being ushered in by the use of the Internet are completely legitimate. Furthermore, the more the number of Internet users in the population of an area approaches the degree of penetration of other <phrase>mass media</phrase> (particularly <phrase>television</phrase>), the more realistic considerations about the use of Internet in <phrase>political</phrase> processes and <phrase>enforcement</phrase> of <phrase>democracy</phrase> will become. The influence (and use) of the Internet on <phrase>democratic</phrase> processes in a certain environment can be direct or indirect. By direct influence we can understand the use of Internet for the realisation of public opinion polling, different <phrase>referenda</phrase> and in finally, even elections. Literature on the topic reveals a series of pilot projects and experiments with Internet usage for <phrase>direct democracy</phrase> introduction (the city of <phrase>Amsterdam</phrase>, American elections, etc). Unfortunately in <phrase>Slovenia</phrase> the <phrase>political</phrase> elites would not think in this way yet, and also the penetration of Internet is not so high (the latest information suggests 15% of households have access to Internet [RIS, 1999]) that would allow similar experiments. However, Internet and its intensive use in <phrase>public sector</phrase> can indirectly influence on the democratisation of public life and <phrase>democratic</phrase> processes in a particular area. Intensive, creative and stimulating use of the Internet in <phrase>public sector</phrase> can essentially contribute to better communication between <phrase>political</phrase> and administrative bodies and citizens. It can also contribute to better-924 …
Is optical imaging spectroscopy a viable measurement technique for the investigation of the negative BOLD phenomenon? A concurrent optical imaging spectroscopy and fMRI study at high field (7 T) Traditionally <phrase>functional magnetic resonance imaging</phrase> (fMRI) has been used to map activity in the <phrase>human brain</phrase> by measuring increases in the Blood Oxygenation Level Dependent (BOLD) signal. Often accompanying positive BOLD fMRI signal changes are sustained negative signal changes. <phrase>Previous studies</phrase> investigating the neurovascular coupling mechanisms of the negative BOLD phenomenon have used concurrent 2D-optical imaging spectroscopy (2D-OIS) and <phrase>electrophysiology</phrase> (Boorman et al., 2010). These experiments suggested that the negative BOLD signal in response to whisker stimulation was a result of an increase in deoxy-<phrase>haemoglobin</phrase> and reduced multi-unit activity in the deep cortical layers. However, Boorman et al. (2010) did not measure the BOLD and haemodynamic response concurrently and so could not quantitatively compare either the spatial maps or the 2D-OIS and fMRI <phrase>time series</phrase> directly. Furthermore their study utilised a homogeneous tissue model in which is predominantly sensitive to haemodynamic changes in more superficial layers. Here we test whether the 2D-OIS technique is appropriate for studies of <phrase>negative BOLD</phrase>. We used concurrent fMRI with 2D-OIS techniques for the investigation of the haemodynamics underlying the negative BOLD at 7 <phrase>Tesla</phrase>. We investigated whether optical methods could be used to accurately map and measure the negative BOLD phenomenon by using 2D-OIS haemodynamic data to derive predictions from a biophysical model of <phrase>BOLD signal changes</phrase>. We showed that despite the deep cortical origin of the negative BOLD response, if an appropriate heterogeneous tissue model is used in the <phrase>spectroscopic</phrase> analysis then 2D-OIS can be used to investigate the negative BOLD phenomenon.
Question-answer Approach to <phrase>Human-computer Interaction</phrase> in Collaborative Designing Question-answer Approach to <phrase>Human-computer Interaction</phrase> in Collaborative Designing Background of Qa-approach INTRODUCTION One of problematic kinds of a human-computer activity is a collective creating of Software Intensive Systems (SISs) in any of which the software plays an essential role in the system functionality, cost, development risk, and development time (Software, 2006). A very low degree of success (about 35%) in the activity of such type indicates that the problem of failures is connected with an absence of very important means accessible to both developers and users of the SIS. From the general <phrase>point of view</phrase>, the unsuccess-fulness of the SIS development is being discovered via users interactions with the SIS that essentially differ from reactions expected by users. Similar events indicate that corresponding units of the programmed behavior have not been tested by developers or were being understood incorrectly. Usually any definite unit of the behavior has not been tested when this unit was not qualified by developer as an essential case. ABSTRACT The chapter presents a question-answer approach to the programming of Human-Computer Interactions (HCI) during collaborative development of software intensive systems. Efficiency of the general work can be essentially increased if the human part of the work is informed by precedents and executed with a special kind of pseudo-program by " intellectual processors. " The role of any processor of such type is fulfilled by a designer. The suggested approach was investigated and evolved until an <phrase>instrumental</phrase> system providing the pseudo-code programming of intellectual processors combined with computer processors. Interactions between processors are based on question-answer reasoning. Pseudo-code programs and their corresponding <phrase>instrumental</phrase> means can be combined easily with traditional HCI. Therefore, the developers of SISs need the effective means for adequate defining of the essential behavior units, their modeling for achieving the necessary understanding and also for testing the units in appropriate conditions of designing and using. First of all, the essential units are to be distinguished and such actions can be fulfilled experimentally by interacting with the developing system in real time of designing. Let us notice that interactions used in experiments with the chosen behavior unit can play for this unit the integrative and others helpful roles. On a <phrase>deep belief</phrase> of the author, the named behavioral units are to be distinguished, defined, modeled, understood, coded, and tested as precedents. " Precedents are actions or decisions that have already happened in the past and which can be referred to and justified as an example that can …
Assessment of GPS Signal Quality <phrase>in Urban Environments</phrase> Using Deeply Integrated GPS/IMU research <phrase>interests include</phrase> various aspects of <phrase>inertial navigation</phrase>, GPS/INS integration, GPS software receiver development, GPS <phrase>carrier phase</phrase> positioning, <phrase>digital signal processing</phrase>, Laser Radar (LADAR) localization technologies, and joint time-frequency <phrase>data analysis</phrase>. He received the ION Early Achievement Award in 2006. research has included the Local Area Augmentation System (LAAS), with emphasis on the analysis of GPS signal anomalies, and the use of integrated GPS systems for vehicle navigation <phrase>in urban environments</phrase>. ABSTRACT This research evaluates the quality of GPS signals and their usability for localization <phrase>in urban environments</phrase> using GPS <phrase>data collected</phrase> in <phrase>urban canyons</phrase>. GPS signals collected on a <phrase>Software Defined Radio</phrase> (SDR) platform in <phrase>urban canyons</phrase> in <phrase>downtown</phrase> <phrase>Athens</phrase>, <phrase>Ohio</phrase> and <phrase>Columbus</phrase>, <phrase>Ohio</phrase> are processed using a deeply integrated GPS/INS scheme. This <phrase>deep integration</phrase> architecture allows for coherent signal integration over time intervals as long as 1 second. The <phrase>deep integration</phrase> mode that provides continuous <phrase>carrier phase</phrase> tracking is used herein. Performance of the <phrase>deep integration</phrase> scheme in <phrase>urban canyons</phrase> is compared to performance characteristics of commercially available low-sensitivity GPS receivers. Results characterize the received signals <phrase>in urban environments</phrase> in terms of <phrase>signal strength</phrase>, tracking continuity and multipath influence on signal tracking performance. Results attained show that signals from 5 to 6 Space Vehicles (SVs) are available for processing, even in dense <phrase>urban canyons</phrase>. Deep GPS/INS integration enables continuous <phrase>carrier phase</phrase> tracking, thus allowing for accuracies on the cm/s level in velocity <phrase>in urban environments</phrase>. In contrast, velocity performance of the commercial low-sensitivity GPS receivers considered yielded errors on the level of 1 m/s. Additionally, the <phrase>results demonstrate</phrase> that continuous <phrase>carrier phase</phrase> tracking is possible, even for those cases where buildings block the satellite Line Of Sight (LOS). Further, consistent <phrase>carrier phase</phrase> tracking is performed for at least 2 SVs for a test scenario where all LOS vectors are blocked by buildings, and up to 6 SVs for all other urban <phrase>canyon</phrase> scenarios. Tracking remains consistent for weak signals with Carrier-to-Noise Ratios (<phrase>CNRs</phrase>) down to 12 dB-Hz.
Tri-Scan: A Novel DFT Technique for CMOS <phrase>Path Delay Fault Testing</phrase> We propose a novel <phrase>Design for Testability</phrase> technique to apply two <phrase>pattern tests</phrase> for <phrase>path delay fault testing</phrase>. Due to stringent timing requirements of deep-submicron VLSI chips, <phrase>design-for-test</phrase> schemes have to be tailored for detecting stuck-at as well as <phrase>delay faults</phrase> quickly and efficiently. Existing techniques such as enhanced scan add substantial <phrase>hardware overhead</phrase>, whereas techniques such as scan-shifting or functional justification make the <phrase>test generation</phrase> process complex and produce lower coverage for <phrase>scan based designs</phrase> as compared to non-scan designs. We exploit the characteristics of CMOS circuitry to enable the application of two-<phrase>pattern tests</phrase>. The proposed technique reduces the problem of <phrase>path delay fault testing</phrase> for <phrase>scan based designs</phrase> to that of <phrase>path delay fault testing</phrase> with complete accessibility to the <phrase>combinational logic</phrase>, and has minimal <phrase>area overhead</phrase>. The scheme also provides significant reduction in power during scan operation.
More than skin deep: measuring effects of the underlying model on <phrase>access-control</phrase> system usability In <phrase>access-control</phrase> systems, policy rules <i>conflict</i> when they prescribe different decisions (allow or deny) for the same access. We present the results of a user study that demonstrates the significant impact of <phrase>conflict-resolution</phrase> method on policy-authoring usability. In our study of 54 participants, varying the <phrase>conflict-resolution</phrase> method yielded <phrase>statistically significant</phrase> differences in accuracy in five of the six tasks we tested, including differences in accuracy rates of up to 78%. Our <phrase>results suggest</phrase> that a <phrase>conflict-resolution</phrase> method favoring rules of smaller scope over rules of larger scope is more usable than the <phrase>Microsoft Windows</phrase> operating system's method of favoring deny rules over allow rules. Perhaps more importantly, our <phrase>results demonstrate</phrase> that even seemingly small changes to a system's semantics can fundamentally affect the system's usability in ways that are beyond the power of <phrase>user interfaces</phrase> to correct.
Swat-based Streamflow and Embayment Modeling of Karst-affected <phrase>Chapel</phrase> Branch Watershed, <phrase>South Carolina</phrase> SWAT is a GIS-based basin-<phrase>scale model</phrase> widely used for the characterization of <phrase>hydrology</phrase> and <phrase>water quality</phrase> of large, complex watersheds; however, SWAT has not been fully tested in watersheds with karst <phrase>geomorphology</phrase> and downstream <phrase>reservoir</phrase>-like embayment. In this study, SWAT was applied to test its ability to predict monthly streamflow dynamics for a 1,555 <phrase>ha</phrase> karst watershed, <phrase>Chapel</phrase> Branch Creek, which drains to a large embayment and is comprised of highly diverse land uses. SWAT was able to accurately simulate the monthly streamflow at a <phrase>cave</phrase> spring (CS) outlet draining mostly agricultural and forested lands and a <phrase>golf</phrase> course plus an unknown groundwater discharging area, only after adding known monthly subsurface inputs as a <phrase>point source</phrase> at that location. Monthly streamflows at two other locations, both with multiple land uses, were overpredicted when lower lake levels were prevalent as a result of <phrase>surface water</phrase> flow to groundwater (losing streams). The model underpredicted the flows during rising lake levels, likely due to high conductivity and also a deep <phrase>percolation</phrase> coefficient representing flow lost to shallow and deep groundwater. At the main watershed outlet, a wide section performing as a <phrase>reservoir</phrase> embayment (RE), the model was able to more accurately simulate the measured mean monthly outflows. The RE storage was estimated by using a daily <phrase>water balance</phrase> approach with upstream inflows, rainfall, and PET as inputs and using parameters obtained by <phrase>bathymetric</phrase> survey, LiDAR, and downstream lake level data. Results demonstrated the substantial influence of the karst features in the <phrase>water balance</phrase>, with conduit and diffuse flow as an explanation for the missing upstream flows appearing via subsurface conveyance to the downstream <phrase>cave</phrase> spring, thus providing a more accurate simulation at the embayment outlet. Results also highlighted the influences of downstream lake levels and karst voids/conduits on the watershed hydrologic balance. Simulation performance of <phrase>hydrology</phrase> could be improved with more accurate DEMs obtained from LiDAR for karst feature identification and related modification of SWAT parameters. This SWAT modeling effort may have implications on <phrase>nutrient</phrase> and <phrase>sediment</phrase> loading estimates for TMDL development and implementation in karst watersheds with large downstream embayments that have significant changes in water level due to adjoining lakes. nderstanding watershed <phrase>hydrology</phrase> is critical, as it is often a primary driving force for <phrase>nutrient</phrase> cycl‐ ing and loading dynamics and subsequent down‐ stream <phrase>water quality</phrase> impacts as a result of rapid <phrase>urbanization</phrase> and other land use changes. For this purpose, …
<phrase>Model Checking</phrase> for Autonomic Systems Specified with ASSL Autonomic computing augurs <phrase>great promise</phrase> for <phrase>deep space exploration</phrase> missions, bringing on-board intelligence and less reliance on control links. As part of our research on the ASSL (Autonomic System <phrase>Specification Language</phrase>) framework, we have successfully specified autonomic properties, verified their consistency, and generated prototype models for both the NASA <phrase>ANTS</phrase> (Autonomous Nano-Technology Swarm) concept mission and the NASA <phrase>Voyager</phrase> mission. The first release of ASSL provides built-in consistency checking and <phrase>functional testing</phrase> as the only means of software verification. We discuss our work on <phrase>model checking</phrase> autonomic systems specified with ASSL. In our approach, an ASSL specification is translated into a state-transition model, over which <phrase>model checking</phrase> is performed to verify whether the ASSL specification satisfies correctness properties. The latter are expressed as <phrase>temporal logic</phrase> formulae expressed over sets of ASSL constructs. We also discuss possible solutions to the state-explosion problem in terms of state graph abstraction and probability weights assigned to states. Moreover, we present an example <phrase>case study</phrase> involving checking liveness properties of autonomic systems with ASSL.
Pervasive <phrase>parallel computing</phrase>: an historic opportunity for innovation in programming and architecture <phrase>Parallel programming</phrase> has been the subject of deep research for decades -- and renowned in the software community as a difficult challenge to the degree that many companies have teams of parallelism and concurrency experts. Further, many ISV's explicitly design their <phrase>software architectures</phrase> so as to ensure that the majority of the development effort, including of course debug and test, can be done without consideration of parallelism. What makes parallelism so difficult, are the knotty and coupled problems of correctness, performance -- particularly data locality, and software modularity.  In Terascale (manycore) chip-level multiprocessors, we are facing a pervasive and critical <phrase>parallel programming</phrase> challenge. Core counts on a single chip are expected to increase rapidly, progressing with <phrase>Moore's law</phrase>, and quad-core systems are already available today in mainstream volume client and server platforms. To continue the rapid performance scaling to which we have become accustomed, applications will need to exhibit ample parallelism (and increasing amounts of it) for successive generations of hardware. Further, because the move to multiple-core parallelism as the primary basis for <phrase>performance improvement</phrase> is pervasive, this requirement <phrase>falls</phrase> on a wide range of applications including traditional <phrase>large-scale</phrase> commercial and HPC server, desktop, <phrase>laptop</phrase>, and even those running on small <phrase>mobile devices</phrase>. That breadth has numerous implications for the types of solutions that are required. We will discuss some of the requirements for Terascale <phrase>parallel programming</phrase> solutions, and point out several potentially fruitful directions. A number of these solutions will build on mainstream programming approaches (objects, modularity, imperative), particularly introducing parallelism with modest disruption to both <phrase>large-scale</phrase> and local-scale program structure. However, there is an opportunity for radically different approaches to take hold in the mainstream (e.g. functional).  On the hardware front, there are several reasons why the <phrase>parallel programming</phrase> problem for Terascale (manycore) systems is easier than previous generations of multiprocessors (and can be much easier). The basic hardware characteristics of chip-multiprocessors provide much greater opportunity for efficient coupling and coordination, and a tightly-coupled memory system, simplifying a wealth of sophisticated scheduling and sharing structures. Further, the diminishing performance returns for larger single cores releases innovation to support both <phrase>parallel programming</phrase>, and <phrase>higher-level</phrase> programming in general. This is a huge opportunity to pioneer new approaches and solutions that are radically better than those widely-used today.  We will close with some <phrase>speculation</phrase> on the rate of progress of <phrase>parallel programming</phrase> into the mainstream software community and some implications of such proliferation.
<phrase>Graph-based</phrase> <phrase>Path Planning</phrase> for <phrase>Mobile Robots</phrase> <phrase>Graph-based</phrase> <phrase>Path Planning</phrase> for <phrase>Mobile Robots</phrase> For <phrase>mobile robots</phrase>, whose capricious perceptions are such a bother. iii ACKNOWLEDGEMENTS When I entered the graduate ECE program at <phrase>Georgia</phrase> Tech in 2003, I had accepted a position working on correlating gene interaction based on <phrase>time-series</phrase> expression data. An unexpected set of events later led me to work on compressing digital signals for low-bandwidth communication, ostensibly for image transmission between <phrase>mobile robots</phrase>. Then, before I knew what hit me, I found myself <phrase>neck deep</phrase> in a multi-million dollar competitive robotics project funded by <phrase>DARPA</phrase>. I consider myself extremely lucky to have ended up in a position that allows me such extraordinary experience with field robotics, especially given the precious little programming and practical experience I had when I started. The <phrase>GRITS</phrase> lab has been a stimulating place, and having the trio of <phrase>iRobot</phrase> <phrase>Magellan</phrase> " trashcans " , the NREC Lagr pair of robots, and the <phrase>Porsche</phrase> <phrase>Cayenne</phrase> " <phrase>sting</phrase> " robot has left me unusually spoiled. There aren't many places where there a fresh young student can dive right into top-of-the-line robot hardware. My advisor, Professor Magnus Egerstedt, has been a motivating and ever <phrase>animated</phrase> mentor, and to whom I offer my deepest thanks. He gave me the freedom to explore whatever kinds of solutions intrigued me and was an <phrase>irreplaceable</phrase> support in guiding me along the way. I cannot say enough good things about these past few years in his lab. In a similar way, Professor Tucker Balch has been a great pleasure to work with (and to work for). He has made a lasting influence on me, particularly in how I look at field robotics and tackle complex projects. The white board discussions and the <phrase>field tests</phrase> have certainly helped shape this thesis and contributed enormously to my understanding of robotics. I would also like to thank all my cohorts in the <phrase>GRITS</phrase> and BORG labs, especially Matt Powers, without whose partnership on the LAGR project much of the practical application of this thesis might literally not have ever gotten rolling. Our discussions – which ranged from basic programming to esoteric control architectures to how to make <phrase>cheesecake</phrase> from scratch – have been very valuable to me, and his suggestions have greatly improved the iv quality of the implemented versions of the work presented below. Much of my work is a direct result of problems that needed to be solved for the LAGR project, so I …
Prediction of novel precursor miRNAs using a context-sensitive <phrase>hidden Markov model</phrase> (CSHMM) BACKGROUND It has been apparent in the last few years that small non coding <phrase>RNAs</phrase> (<phrase>ncRNA</phrase>) play a very significant role in biological regulation. Among these microRNAs (miRNAs), 22-23 <phrase>nucleotide</phrase> small regulatory <phrase>RNAs</phrase>, have been a major object of study as these have been found to be involved in some basic biological processes. So far about 706 miRNAs have been identified in humans alone. However, it is expected that there may be many more miRNAs encoded in the <phrase>human genome</phrase>. In this report, a "context-sensitive" <phrase>Hidden Markov Model</phrase> (CSHMM) to represent miRNA structures has been proposed and tested extensively. We also demonstrate how this model can be used in conjunction with filters as an <phrase>ab initio</phrase> method for miRNA identification.   RESULTS The probabilities of the CSHMM model were estimated using known human miRNA sequences. A classifier for miRNAs based on the likelihood score of this "trained" CSHMM was evaluated by: (a) <phrase>cross-validation</phrase> estimates using known human sequences, (b) predictions on a dataset of known miRNAs, and (c) prediction on a dataset of non coding <phrase>RNAs</phrase>. The CSHMM is compared with two <phrase>recently developed</phrase> methods, miPred and CID-miRNA. The results suggest that the CSHMM performs better than these methods. In addition, the CSHMM was used in a pipeline that includes filters that check for the presence of EST matches and the presence of Drosha cutting sites. This pipeline was used to scan and identify potential miRNAs from the human <phrase>chromosome</phrase> 19. It was also used to identify novel miRNAs from <phrase>small RNA</phrase> sequences of human normal <phrase>leukocytes</phrase> obtained by the <phrase>Deep sequencing</phrase> (Solexa) methodology. A total of 49 and 308 novel miRNAs were predicted from <phrase>chromosome</phrase> 19 and from the <phrase>small RNA</phrase> sequences respectively.   CONCLUSION The results suggest that the CSHMM is likely to be a useful tool for miRNA discovery either for analysis of individual sequences or for genome scan. Our pipeline, consisting of a CSHMM and filters to reduce false positives shows promise as an approach for <phrase>ab initio</phrase> identification of novel miRNAs.
The influence of modality on <phrase>deep-reasoning questions</phrase> This study investigated the influence that modality (print versus spoken text) had on learning with <phrase>deep reasoning questions</phrase>. Half the participants were randomly assigned to receive <phrase>deep-reasoning questions</phrase> during the learning session. The other half received the same information in the absence of <phrase>deep-reasoning questions</phrase>. The participants who received <phrase>deep reasoning questions</phrase> were randomly assigned to one of two different groups. One group received <phrase>deep reasoning questions</phrase> as on-screen printed text while the other group received <phrase>deep reasoning questions</phrase> in a spoken modality via a text to speech engine. Participants who received <phrase>deep reasoning questions</phrase> had higher <phrase>post-test</phrase> scores than those who did not, a finding that replicated <phrase>previous research</phrase>. Additionally, learning was better for the learners who received printed text than spoken messages, a finding that is not compatible with a number of theoretical and empirical claims in the literature. (2010) 'The influence of modality on <phrase>deep-reasoning questions</phrase>', Int. <phrase>interests include</phrase> <phrase>intelligent tutoring</phrase> environments, student generated questions, and emotions. Scotty D. Craig is a Research Scientist in the Institute for <phrase>Intelligent Systems</phrase> located at the University of <phrase>Memphis</phrase>. To date, he has worked on projects in such areas as affect and learning, discourse processing, mechanical reasoning, multimedia learning, vicarious learning environments and <phrase>intelligent tutoring systems</phrase>.
<phrase>Haptic</phrase> discrimination of object shape in humans: two-dimensional angle discrimination. The human ability to recognize objects on the basis of their shape, as defined by active exploratory movements, is dependent on sensory feedback from mechanoreceptors located both in the skin and in <phrase>deep structures</phrase> ( <phrase>haptic</phrase> feedback). Surprisingly, we have little information about the mechanisms for integrating these different signals into a single sensory <phrase>percept</phrase>. With the eventual aim of studying the underlying central neural mechanisms, we developed a shape discrimination test that required active exploration of objects, but was restricted to one component of shape, two-dimensional (2D) angles. The angles were machined from 1-cm-thick <phrase>Plexiglas</phrase>, and consisted of two 8-cm-long <phrase>arms</phrase> that met to form an angle of 90 degrees (standard) or 91 degrees to 103 degrees (comparison angles). Subjects scanned pairs of angles with the <phrase>index finger</phrase> of the outstretched arm and identified the larger angle of each pair explored. Discrimination threshold (75% correct) was 4.7 degrees (range 0.7 degrees to 12.1 degrees), giving a precision of 5.2% (0.8-13.4%: difference/standard). Repeated blocks of trials, either in the same session or on different days, had no effect on discrimination threshold. In contrast, the motor strategy was partly modified: scanning speed increased but dwell-time at the intersection did not change. Finally, 2D angle discrimination was not significantly modified by rotating the orientation of one of the angles in the pair (0 degrees, 4 degrees or 8 degrees rotation towards the midline, in the vertical plane), providing evidence that subjects evaluated each angle independently in each trial. Subject reports indicated that they relied on cutaneous feedback from the exploring digit (amount of compression of the finger at the angle) and mental images of the angles, most likely arising from <phrase>proprioceptive</phrase> information (from the shoulder) generated during the to-and-fro scans over the angle. In terms of shoulder angles, the mean discrimination threshold here was 0.54 degrees (range 0.08 degrees to 1.36 degrees). These values are lower than previous estimates of position sense at the shoulder. In light of the subjects' strategies, it therefore seems likely that both cutaneous and <phrase>proprioceptive</phrase> (including both dynamic and static position-related signals) feedback contributed to the <phrase>haptic</phrase> discrimination of 2D angles.
<phrase>Deep Learning</phrase> Design for Sustainable Innovation within Shifting Learning Landscapes Changes in the underpinning technologies for TEL is occurring at a pace that we have never before experienced, and this is unlikely to slow down. This necessitates a broader and more profound understanding of design that needs to be more future-proof than relying on the latest or emerging technologies and yet embraces the collaborative, multimodal and ubiquitous nature of learning in 21C. In addressing this challenge this article develops, exemplifies and tests the approach of <phrase>Deep Learning</phrase> Design (DLD), which has led to relatively <phrase>large-scale</phrase> and sustainable innovations and also outlined clear directions for near-future developments. Specifically, in this article we: justify why DLD is necessary and describe its key principles; exemplify these principles through four TEL initiatives; and, draw some implications and conclusions from across these projects about DLD and future learning.
Hydroacoustic Signal Classification Using <phrase>Support Vector Machines</phrase> Kernel-based algorithms such as <phrase>support vector machines</phrase> (SVMs) are state-of-the-art in <phrase>machine learning</phrase> for <phrase>pattern recognition</phrase>. This chapter introduces SVMs and describes a specific application to hydroacoustic signal classification. <phrase>Long-range</phrase>, passive-acoustic monitoring in the oceans is facilitated by propagation properties for underwater sound. In particular, the deep sound (SOFAR, Sound Fixing and Ranging) channel can act as a <phrase>waveguide</phrase> for underwater signals. In this chapter , SVMs are employed for classifying hydroacoustic signals recorded by the <phrase>sensor network</phrase> for verification of the <phrase>Comprehensive Nuclear-Test-Ban Treaty</phrase>. Constraints in the early <phrase>signal processing</phrase> chain and limited data require tailored kernel functions and careful SVM <phrase>model selection</phrase>. We demonstrate how problem-specific kernel functions can increase classifier performance when combined with efficient <phrase>gradient-based</phrase> approaches for optimizing kernel and SVM regularization parameters.
IDDQ <phrase>data analysis</phrase> using neighbor <phrase>current ratios</phrase> I DDQ test loses its effectiveness for <phrase>deep sub-micron</phrase> chips since it cannot distinguish between faulty and <phrase>fault-free</phrase> currents. The concept of <phrase>current ratios</phrase>, in which the ratio of maximum to minimum <phrase>I DDQ</phrase> is used to screen faulty chips, has been <phrase>previously proposed</phrase>. However, it is incapable of screening some defects. The neighboring chips on a wafer have similar <phrase>fault-free</phrase> properties and are correlated. In this paper, the use of <phrase>spatial correlation</phrase> in combination with <phrase>current ratios</phrase> is investigated. By differentiating chips based on their non-conformance to local <phrase>I DDQ</phrase> variation, outliers are identified. The analysis of SEMATECH <phrase>test data</phrase> is presented.
Modeling Bus Load on Can Modeling Bus Load on Can Title: Modeling Bus Load on Can II Preface This titled master thesis has been done at the <phrase>Research and Development</phrase>/System I take this opportunity to thank several people who gave me their support during my Thesis preparation days at <phrase>Södertälje</phrase> and during my Post graduation at <phrase>Halmstad</phrase>. Deep and special thanks go first to Jan Lindman, my supervisor at <phrase>Scania</phrase>, for his intensive comments, instructions, guidance, testing and tracking. To Prof. Tony Larsson, my supervisor and examiner at <phrase>Halmstad</phrase> University for his patience and unlimited explanations, valuable instructions and support. To <phrase>David</phrase> Holmgren, the RESA Group Manager for his instructions and comments. To my father, mother and wife, I have received intensive and unlimited support from them. Abstract The existence of high load and latency in the CAN bus network would indeed lead to a situation where a given message crosses its deadline; this situation would disturb the continuity of the required service as well as activating fault codes due to delay of message delivery, which might lead to system failure. The outcome and goal of this thesis is to research and formulate methods to determine and model busload and latencies, by determining parameters such as alpha and breakdown utilization, which are considered as indications to the start of network breakdown when a given message in a dataset start to introduce latency by crossing its deadline which are totally prohibited in critical real time communications. The final goal of this master thesis is to develop a TOOL for calculating, modeling, determining and visualizing <phrase>worst case</phrase> busload, throughput, networks' breakdown points and <phrase>worst case</phrase> latency in <phrase>Scania</phrase> CAN bus networks which is based on the J1939 protocol. SCANLA (The developed CAN busload analyzer tool in this thesis) is running as an executable application and uses a Graphical User Interface as a human-computer interface (i.e., a way for humans to interact with the tool) that uses windows, <phrase>icons</phrase> and menus and which can be manipulated by a mouse. associated databases and accompanying documents, such as this thesis report. This Tool is hoped that it will be useful but without any <phrase>warranty</phrase>. For a description and instructions on how to use the tool, read intensively the report. Please let the <phrase>producer</phrase> of the mentioned tool know of any improvements, changes or modifications that might be made.
<phrase>Mastering</phrase> use cases: capturing <phrase>functional requirements</phrase> for interactive applications Use cases were introduced in the early 90s by Jacobson. He defined a use case as a "specific way of using the system by using some part of the functionality." Use case modeling is making its way into mainstream practice as a key activity in the <phrase>software development process</phrase> (e.g., Unified Process). There is accumulating evidence of significant benefits to customers and developers. The use case model is the artifact of choice for capturing <phrase>functional requirements</phrase> and as such, serves as a contract of the envisioned system behavior between stakeholders. It drives the architecture of the application, it can be used to generate functional <phrase>test cases</phrase> and often serves as a reference point for maintenance and documentation purposes. Writing effective and well-structured use cases is a difficult task which requires a deep understanding of the surrounding techniques and best practices. Current practice has shown that it is easy to misuse them or make mistakes that can unintentionally turn them into "abuse cases".
Multiprocessor Implementation of <phrase>Transitive Closure</phrase> The matter embodied herein has not been submitted to any other University for the award of any other degree. 8023119 This is to certify that above statement made by the candidate is correct and true to best of my knowledge. Acknowledgement To discover, analyze and to present something new is to <phrase>venture</phrase> on an untrodden path towards an unexplored destination is an arduous <phrase>adventure</phrase> unless one gets a true torchbearer to show the way. This enlightening guidance, I found in my revered guide Mrs patronization it was never possible to give final shape to this thesis. I express my heartfelt gratitude towards her for her valuable guidance, encouragement, constant involvement, inspiration and the enthusiasm with which she solved my difficulties. I shall be failing in my duties if I do not express my deep sense of gratitude (Deemed University), <phrase>Patiala</phrase>, for his valuable advices and suggestions. I would like to thank all the faculty and staff members of <phrase>Computer Science</phrase> & Engineering Department for providing me all the facilities required for the completion of this work. Last but not the least, I express my heartfelt thanks to my parents, my friends for cooperation , which they were always ready to extend. Abstract <phrase>Graph theoretic</phrase> algorithms are found quite effective for solving complex <phrase>real life</phrase> problems. Computing the <phrase>transitive closure</phrase> in directed graphs is a fundamental graph problem. <phrase>Transitive closure</phrase> can be thought of as establishing a <phrase>data structure</phrase> that makes it possible to solve <phrase>reachability</phrase> questions (can I get to x from y?) efficiently. After the preprocessing of constructing the <phrase>transitive closure</phrase>, all <phrase>reachability</phrase> queries can be answered in constant time by simply reporting a matrix entry. <phrase>Transitive closure</phrase> is fundamental in propagating the consequences of modified attributes of a graph G. For efficient system utilization and fast response to the user, it is necessary to use <phrase>parallel algorithms</phrase> for solving a single problem. <phrase>Transitive closure</phrase> is a highly parallelizable problem; it belongs to the class <phrase>NC</phrase> of problems that can be solved in polylogarithmic time (i.e. O(log c n) for some constant c) with polynomial number of processors. This thesis proposes an efficient scalable multiprocessor algorithm for <phrase>transitive closure</phrase> computation in a <phrase>distributed computing</phrase> environment. It focuses on the role of <phrase>transitive closure</phrase> in the <phrase>Graph Theory</phrase>, Very Large Databases, Relational <phrase>Database Management Systems</phrase>, and VLSI <phrase>Test Generation</phrase>. Serious efforts have been made in investing new parallel and /or …
Open <phrase>Object-oriented</phrase> Modelling and Validation Framework for Modular <phrase>Industrial Automation</phrase> Systems This paper introduces a framework for formal modelling and validation of automation systems intended to be used by control engineers. The framework is based on a graphical, modular, and typed formalism of Net Condition/Event Systems. This allows for modelling of realistic hierarchically organized <phrase>industrial automation</phrase> systems in a <phrase>closed loop</phrase>. The framework consists of methodologies and tools which enable <phrase>formal analysis</phrase> of automation systems. The framework will be used to improve safety, reliability and robustness of automation systems predicting potential faults and deadlocks. 1 INTRODUCTION Modern <phrase>production systems</phrase> need to be more flexible and re-configurable. For this reason they are built from standardized processing modules. Their software is also organized in a <phrase>modular form</phrase> and is executed on distributed control devices. When new configurations of <phrase>production systems</phrase> are formed from the modular components, the testing becomes a bottleneck for quick commissioning. Formal validation can reduce the time-consuming testing and commissioning phases of system's development and deployment. As the functionality of such systems is determined by cooperation of entities of heterogeneous domains, e.g. mechanical, electric, automation hardware and software, the validation has to take into account the relevant properties from all these domains. Formal modelling of automation systems proved to be helpful for validation of automation systems by simulation or by <phrase>formal verification</phrase> of static and dynamic properties. In automation systems the software represents a variable part, while the models of equipment can be reused through the engineering cycle. Once developed by a machine vendor, the models may follow the equipment, enabling the machine users (e.g. system integrators) to validate new configurations of the machines re-using the models of their components. This vision, however, requires a more systematic approach to the modelling, than that can be seen now. Models in most of the formalisms such as <phrase>Petri nets</phrase> or <phrase>finite automata</phrase> lack integrating capabilities: while they may cope well with the modelling of a particular process, building the overall model of a system comprising several processes is difficult. An opposite example make the modelling techniques based on the Unified Modelling Language (UML). The UML is getting increasingly popular also in automation for its ability to describe systems in <phrase>object-oriented</phrase> form. However, the UML lacks a formal background and can hardly be used for deep analysis of the systems. This paper tries to sketch another approach for systematic modelling of systems by means of a modular modelling formalism. The paper is organized …
Impact of Well Edge Proximity Effect on Timing — This paper studies impact of the well edge proximity effect on <phrase>digital circuit</phrase> delay, based on model parameters extracted from <phrase>test structures</phrase> in an industrial 65nm wafer process. The experimental results show that up to 10% of delay increase arises by the well edge proximity effect in the 65nm technology, and it depends on interconnect length. Furthermore, due to asymmetric increase in pMOS and nMOS threshold voltages, delay may decrease in spite of the <phrase>threshold voltage</phrase> increase. From these results, we conclude that considering WPE is indispensable to cell characterization in the 65nm technology. I. INTRODUCTION At <phrase>front-end</phrase> stages of CMOS wafer process, P and N types of ions are implanted to form wells. During the implantations, lateral <phrase>scattering</phrase> of the ions nearby edges of the photo-resist causes well doping concentration, as <phrase>shown in Fig</phrase>. 1 [1][2][3]. The well doping concentration mainly drifts the <phrase>threshold voltage</phrase> of MOS transistors. We call it " well edge proximity effect " (WPE). As advanced deep well implants with high-energy implanters are introduced to suppress parasitic bipolar gain for latch-up protection, WPE becomes severer. In <phrase>circuit design</phrase>, WPE can be suppressed by making a large separation between gate poly and enclosing well edge. On the other hand, as <phrase>shown in Fig</phrase>. 2, common cell based designs intrinsically involve WPE, whereas analog <phrase>circuit design</phrase> can make use of the large separation. The figure depicts a part of standard cells placement. Since wells are continuous along the cell rows and then dummy cells are placed at the both ends of the cell rows for lithographical reasons, horizontal proximity between gate and well edges can be negligible. As for the vertical direction, the inner and the outer spacing shown in the figure are uniquely determined for cell by cell. Therefore, we should take care of WPE only for the vertical direction in the digital cell based design. During <phrase>circuit design</phrase>, a lot of effort is put into timing convergence[4]. From a standpoint of <phrase>circuit design</phrase>, delay variation due to WPE is a matter of utmost concern. We evaluate the impact of WPE on circuit timing as a case study in an industrial 65nm technology, and demonstrate the delay increase quantitatively. We also point out that WPE decreases cell delay in some conditions of input waveform and output load.
PGO: A parallel <phrase>computing platform</phrase> for <phrase>global optimization</phrase> based on <phrase>genetic algorithm</phrase> This paper presents the design, architecture and implementation of a general parallel <phrase>computing platform</phrase>, termed PGO, based on the <phrase>Genetic Algorithm</phrase> for <phrase>global optimization</phrase>. PGO provides an efficient and easy-to-use framework for paralleliz-ing the <phrase>global optimization</phrase> procedure for general scientific <phrase>modeling and simulation</phrase> processes. Along with a core optimization kernel built on a Genetic Algorithm, PGO also includes a general input generator and an output extractor that can facilitate its easy integration with various scientific computing tasks. In this paper, we demonstrate the efficiency and versatility of PGO with two different applications: (1) the parallelization of a large <phrase>scale parameter</phrase> estimation problem associated with mod-eling water flow in a heterogeneous deep <phrase>vadose zone</phrase>; (2) the parallelization of a complex simulation-optimization procedure for searching for an optimal groundwater remediation design. PGO is developed as an open <phrase>source code</phrase>, and is independent of the computer operating system. It has been tested in a heterogeneous computing environment consisting of <phrase>Solaris</phrase> 9, <phrase>Fedora</phrase> <phrase>Core 2</phrase> Linux, and <phrase>Microsoft Windows</phrase> machines, and is freely available for download from
<phrase>Problem-Based Learning</phrase> in Mathematics - A tool for Developing Students' Conceptual Knowledge Mathematics teachers must teach students not only to solve problems but also to learn about mathematics through <phrase>problem solving</phrase>. 1 While " many students may develop procedural fluency … they often lack the deep conceptual understanding necessary to solve new problems or make connections between mathematical ideas. " 2 This presents a challenge for teachers: <phrase>problem-based learning</phrase> (PBL) provides opportunities for teachers to meet this challenge. PBL exists as a teaching method grounded in the ideals of <phrase>constructivism</phrase> and student-centred learning. When using PBL, teachers help students to focus on <phrase>solving problems</phrase> within a <phrase>real-life</phrase> context, encouraging them to consider the situation in which the problem exists when trying to find solutions. 3 The majority of research examining PBL focuses on its use in <phrase>medical schools</phrase>, with the <phrase>key features</phrase> being (a) the use of collaborative small-group work, (b) a student-centred approach, (c) the teacher as facilitator and (d) the use of <phrase>real-life</phrase> problems as the organizing focus. 4 In the medical <phrase>arena</phrase>, groups of students are given a set of realistic patient symptoms and expected to research possible diagnoses and courses of treatment; groups work independently, developing and answering their own questions. If, during this diagnostic phase, a group is unsuccessful in addressing key issues, the instructor notes this on their assessment but does not provide the solution. 4 In the classroom setting, it is this aspect of PBL which presents the most signifcant challenge, requiring teachers to shift from direct instruction to supporting students organize their own learning. 5 What kind of mathematics problems help students develop deep, conceptual understanding? Research Tells Us ● Many students lack a deep understanding of mathematical concepts. ● Classroom teachers find it difficult both to develop a <phrase>real-life</phrase> hook for students and to allow students to work through <phrase>problem solving</phrase> independently. ● PBL is a promising approach not only to build mathematics understanding but also to test students' conceptual knowledge. ● PBL requires teachers to present students with multifaceted, <phrase>real-life</phrase> problems and to act as facilitators supporting students in organizing their own learning.
Induction of Integrated View for <phrase>XML Data</phrase> with Heterogeneous DTDs This paper proposes a novel approach to integrating heterogeneous XML DTDs. With this approach, an information agent can be easily extended to integrate heterogeneous XML-based contents and perform <phrase>federated search</phrase>. Based on a tree grammar inference technique, this approach derives an integrated view of XML DTDs in an information integration framework. The derivation takes advantages of naming and structural similarities among DTDs in similar domains. The complete approach consists of three main steps. (1) <i><phrase>DTD</phrase> clustering</i> clusters DTDs in similar domains into classes. (2) <i>Schema learning</i> applies a tree grammar inference technique to generate a set of tree grammar rules from the DTDs in a class from the previous step. (3) <i>Minimization</i> optimizes the rules generated in the previous step and transforms them into an integrated view. We have implemented the proposed approach into a system called <i>DEEP</i> and tested the system on artificial and real domains. The experimental results reveal that this system can effectively and efficiently integrate radically different DTDs.
A development Environment for an MTT-Based Sentence Generator With the rising standard of the state of the art in text generation and the increase of the number of practical generation applications, it becomes more and more important t o p r o vide means for the maintenance of the generator, i.e. its extension , modiication, and monitoring by gram-marians who are not familiar with its internals. However, only a few sentence and text generators developed to date actually provide these means. One of these generators is kpml (Bate-man, 1997). Kpml comes with a Development Environment and there is no doubt about the contribution of this environment to the popularity of the systemic approach in generation. In the generation project at <phrase>Stuttgart</phrase>, the realization of a <phrase>high quality</phrase> d e v elopment e n-vironment (henceforth, de) has been a central topic from the beginning. The de provides support to the user with respect to writing, modifying , testing, and debugging of (i) grammar rules, (ii) lexical information, and (iii) linguistic structures at diierent l e v els of abstraction. Furthermore, it automatically generalizes the organization of the lexica and the grammar. In what follows, we brieey describe de's main features. The theoretical linguistic background of the de is the Meaning-Text Theory (Mel' cuk, 19888 Polgu ere, 1998). However, its introduction is beyond the scope of this notee the interested reader is asked to consult the above r e f-erences as well as further literature on the use of mtt in text generation|for instance, (Ior-In mtt, seven levels (or strata) of linguistic description are distinguished, of which ve are relevant for generation: semantic (Sem), <phrase>deep-syntactic</phrase> (DSynt), surface-syntactic (SSynt), deep-morphological (DMorph) and surface-morphological (SMorph). In order to be able to generate starting from the data in a data base, we i n troduce an additional, the conceptual (Con) <phrase>stratum</phrase>. The input structure to de is thus a conceptual structure (ConStr) derived from the data in the db. The generation process consists of a series of structure mappings between adjacent s t r a t a u n til the SMorph <phrase>stratum</phrase> is reached. At the SMorph <phrase>stratum</phrase>, the structure is a string of linearized word forms. The central module of the de is a compiler that maps a structure speciied at one of the ve rst of the above strata on a structure at the adjacent <phrase>stratum</phrase>. To support the user in the examination of the internal information gathered during the …
Interactive dirt: increasing mobile work performance with a wearable projector-camera system Mobile teamwork requires people to maintain good situational awareness (SA) about their <phrase>real world</phrase> environments. Current <phrase>mobile devices</phrase> are highly portable, but their <phrase>user interfaces</phrase> (UIs) require too deep of focus of attention to allow their users to use them and simultaneously maintain SA. As a result, some mobile practitioners have little or no access to useful computer-based interactive services. Inspired by existing projector-camera systems, this paper studies the feasibility of developing a wearable projector-camera system that enables users to access <phrase>human-computer interaction</phrase> (HCI) services without negatively affecting their SA. A functional prototype of the "Interactive Dirt" system was developed using inexpensive <phrase>commercial off-the-shelf</phrase> technologies. A field experiment was conducted as a formative evaluation to test the utility of the prototype under extreme mobile teamwork requirements for SA--military stability and support operations (SASO). Results show strong potential to increase performance of mobile teams.
The Role of Diversity and Technology in Global <phrase>Virtual Teams</phrase> This study is an attempt to develop and test a comprehensive model for <phrase>global virtual</phrase> team (GVT) effectiveness based on development of collaborative partnership among diverse team members and the moderating role of <phrase>collaborative technology</phrase> and task. The research is an ongoing dissertation work. The <phrase>conceptual model</phrase> is based on traditional I-P-O framework for understanding GVT effectiveness. Team diversity in terms of surface level, functional, and <phrase>deep level</phrase> are treated as the central tenet of team inputs. Collaborative partnership elements are at the process level, moderated by task and <phrase>collaborative technology</phrase>. At the outcome level, this study is more interested in GVT effectiveness as measured by team performance and individual team member satisfaction.
Road-testing the English Resource Grammar Over the <phrase>British National Corpus</phrase> This paper addresses two questions: (1) when a large <phrase>deep processing</phrase> resource developed for relatively closed domains is run over open text, what coverage does it have, and (2) what are the most effective and time-efficient ways of consolidating gaps in the coverage of such as resource?
An efficient method to detect <phrase>periodic behavior</phrase> in botnet traffic by analyzing control plane traffic Botnets are large networks of bots (compromised machines) that are under the control of a small number of bot masters. They pose a significant threat to Internet's communications and applications. A botnet relies on command and control (C2) communications channels traffic between its members for its attack execution. <phrase>C2 traffic</phrase> occurs prior to any attack; hence, the detection of botnet's <phrase>C2 traffic</phrase> enables the detection of members of the botnet before any real harm happens. We analyze <phrase>C2 traffic</phrase> and find that it exhibits a <phrase>periodic behavior</phrase>. This is due to the pre-programmed behavior of bots that check for updates to download them every T seconds. We exploit this <phrase>periodic behavior</phrase> to detect <phrase>C2 traffic</phrase>. The detection involves evaluating the periodogram of the monitored traffic. Then applying Walker's large sample test to the periodogram's maximum ordinate in order to determine if it is due to a periodic component or not. If the periodogram of the monitored traffic contains a periodic component, then it is highly likely that it is due to a bot's <phrase>C2 traffic</phrase>. The test looks only at aggregate control plane traffic behavior, which makes it more scalable than techniques that involve <phrase>deep packet inspection</phrase> (DPI) or tracking the communication flows of different <phrase>hosts</phrase>. We apply the test to two types of botnet, tinyP2P and <phrase>IRC</phrase> that are generated by SLINGbot. We verify the <phrase>periodic behavior</phrase> of their <phrase>C2 traffic</phrase> and compare it to the results we get on real traffic that is obtained from a secured enterprise network. We further study the characteristics of the test in the presence of injected HTTP background traffic and the effect of the <phrase>duty cycle</phrase> on the <phrase>periodic behavior</phrase>.
Digital Techniques for <phrase>Etruscan</phrase> Graves: the Etruscanning Project Etruscanning is a project founded by the <phrase>European Commission</phrase> and it focuses on the investigation of new digitization and presentation techniques, in order to recreate the original context of the <phrase>Etruscan</phrase> graves. Several digital techniques have been applied for the stages of digitization, virtual <phrase>restoration</phrase> and reconstruction and communication. The possibility of working on two different tombs allows us to deep two specific approaches and to diversify the final real-time applications. This project represents an interesting opportunity to create a concrete link between research and communication in the field of virtual <phrase>museums</phrase>, testing the effective impact in terms of cultural transmission, learning and appreciation both in non-linear <phrase>narrative</phrase> plots conception and in novel metaphors of interaction. From a technological <phrase>point of view</phrase> the most innovative result of the project is the implementation of natural interaction interfaces, allowing the public to move and interact with objects inside the <phrase>virtual environment</phrase>.
Responsive Neuromodulators Based on <phrase>Artificial Neural Networks</phrase> Used to Control Seizure-like Events in a Computational Model of Epilepsy <phrase>Deep brain stimulation</phrase> (DBS) has been noted for its potential to suppress <phrase>epileptic</phrase> seizures. To date, DBS has achieved mixed results as a therapeutic approach to seizure control. Using a computational model, we demonstrate that high-complexity, biologically-inspired responsive neuromodulation is superior to periodic forms of neuromodulation (responsive and non-responsive) such as those implemented in DBS, as well as neuromodulation using random and random repetitive-interval stimulation. We configured <phrase>radial basis function</phrase> (RBF) networks to generate outputs modeling interictal <phrase>time series</phrase> recorded from <phrase>rodent</phrase> hippocampal slices that were perfused with low Mg²⁺/high K⁺ solution. We then compared the performance of RBF-based interictal modulation, periodic biphasic-pulse modulation, random modulation and random repetitive modulation on a cognitive <phrase>rhythm</phrase> generator (CRG) model of spontaneous seizure-like events (SLEs), testing efficacy of <phrase>SLE</phrase> control. A <phrase>statistically significant</phrase> improvement in <phrase>SLE</phrase> mitigation for the RBF interictal modulation case versus the periodic and random cases was observed, suggesting that the use of biologically-inspired neuromodulators may achieve better results for the purpose of electrical control of seizures in a clinical setting.
Development of Multi-stack Process on Wafer-on-wafer (wow) The multi-stack process on wafer-on-wafer (WOW) has been developed. In order to realize the multi-stacked wafer with ultra thinned wafer of less than 10μm with adhesive <phrase>polymer</phrase>, several processes have been optimized. The wafer thickness after back-grinding was controlled within the total thickness variation (TTV) of 1.2μm on <phrase>wafer-level</phrase> of 8inch. For the side wall of though silicon vias (TSV), <phrase>SiN</phrase> film with low deposition temperature of 150 <phrase>°C</phrase> has been developed and applied for TSV process without degradation for electrical characteristics. The uniformity of Cu electro-plating has been improved that the overburdened Cu from the sueface was decreased from 13.3 μm to 0.7 μm by optimizing plating solution. The CMP process following Cu electro-plating has been customized for the high rate of 5 μm/min. Finally, the stacked wafer has been evaluated for thermal cycle test (TCT) of 100 cycles with-65 to 150 <phrase>°C</phrase>. The result showed that there was no degradation for packaging process. Introduction Although there are various methods reported for three dimensional integration (3DI) of semiconductor devices, the production cost remains as a big issue. Therefore, it is necessary to use not the state-of-the art technology but the conventional facilities and technology. In addition, the production yield must be so high that technological difficulty for production processes has to be set as low as possible. In order to reduce the issues shown above, the aspect ratio of TSV has to be as small as possible. In this paper, the thickness of stack-wafer was thinned down to less than 10 μm. The thinned wafer was stacked on a base wafer with an adhesive <phrase>polymer</phrase> following TSV formation with Cu filling for interconnection. The TSV was formed by <phrase>Deep Reactive Ion Etching</phrase> (DRIE) process and the via diameter was 10μm. The aspect ratio of the TSV was just 1.5 that was not difficult for production. The side-wall dielectric film was deposited and formed by PE-CVD and RIE respectively. The Cu filling and redistribution layer (RDL) were formed simultaneously by damascene process. This TSV formation process was very close to BEOL process, which contributed to both production cost and yield. We call the process " Wafer-on-Wafer (WOW) ". By repeating the WOW process, multi-wafer-stack was finally achieved, which lead to realize <phrase>high-density</phrase> 3D integration with production worthy process [1-3]. The key technologies for WOW process were <phrase>categorized</phrase> into four parts; 1) wafer-thinning, 2) wafer stacking [4], 3) TSV formation with …
Cell-level temperature distributions in <phrase>skeletal muscle</phrase> post <phrase>spinal cord injury</phrase> as related to deep tissue injury Deep tissue injury (DTI) is a severe pressure <phrase>ulcer</phrase>, which initiates in <phrase>skeletal muscle</phrase> tissue under intact skin. Patients with <phrase>spinal cord injury</phrase> (SCI) are especially vulnerable to DTI, due to their impaired motosensory capacities. The underlying mechanisms that lead to DTI are, however, still <phrase>poorly understood</phrase>. This study focuses on cell-level temperature distributions in muscles of patients with SCI, which typically contain thinner <phrase>muscle fibers</phrase> and fewer <phrase>capillaries</phrase>. It has been shown previously by our group that <phrase>ischemic</phrase> muscles of rat models of DTI cool down mildly and locally, which is very likely to slow the diffusivity of <phrase>metabolites</phrase> in the <phrase>ischemic</phrase> regions. However, it is unclear how these temperature decreases affect diffusivity at the scale of individual <phrase>muscle cells</phrase> in the microanatomy of SCI patients. We hypothesize that a 2 degrees C drop in the temperature of inflowing <phrase>capillary</phrase> blood, as shown in our animal studies, has a substantial effect on lowering the diffusivity of <phrase>metabolites</phrase> in <phrase>skeletal muscle</phrase>, but the pathological microanatomy in the chronic phase of SCI is less dominant in affecting the local temperatures in and around <phrase>muscle cells</phrase>. In order to test this hypothesis, two-dimensional <phrase>finite element</phrase> (FE) models of cross sections through the microanatomy of <phrase>muscle tissue</phrase> were developed using COMSOL Multiphysics software for normal and SCI muscles. The models included <phrase>muscle cells</phrase>, <phrase>extracellular matrix</phrase> (ECM), and <phrase>capillaries</phrase>, each with its own geometrical, thermal, and heat production properties. The SCI model configuration specifically included reduced <phrase>cross section</phrase> of myofibrils in favor of more ECM, less <phrase>capillaries</phrase>, and decreased blood inflow rate. After <phrase>a 20</phrase>-s <phrase>heat transfer</phrase> simulation, it was found that temperatures around the cells of the SCI muscle were approximately 2 degrees C lower than that in the normal muscle, that is, heat production from the muscle cell <phrase>metabolism</phrase> did not compensate for the lower inflowing blood temperature in the SCI model. We conclude that the temperature and rate of inflowing <phrase>capillary</phrase> blood are the dominant factors determining the localized temperatures in the <phrase>microarchitecture</phrase> of an <phrase>ischemic</phrase> SCI <phrase>muscle tissue</phrase>. The altered SCI microanatomy was shown to be less influential. Taken together with the Stokes-<phrase>Einstein</phrase> theory, our results indicate that diffusivity of <phrase>metabolites</phrase> would be approximately 50% less around the cells of SCI muscle due to local cooling, which is yet another factor compromising tissue viability in the patients with SCI.
An Adaptive and Predictive Respiratory Motion Model for Image-Guided Interventions: Theory and First Clinical Application This paper describes a predictive and adaptive single parameter motion model for updating roadmaps to correct for respiratory motion in image-guided interventions. The model can adapt its motion estimates to respond to changes in breathing pattern, such as deep or fast breathing, which normally would result in a decrease in the accuracy of the motion estimates. The adaptation is made possible by <phrase>interpolating</phrase> between the motion estimates of multiple submodels, each of which describes the motion of the target <phrase>organ</phrase> during cycles of different amplitudes. We describe a predictive technique which can predict the amplitude of a breathing cycle before it has finished. The predicted amplitude is used to <phrase>interpolate</phrase> between the motion estimates of the submodels to tune the adaptive model to the current breathing pattern. The proposed technique is validated on affine motion models formed from <phrase>cardiac</phrase> <phrase>magnetic resonance imaging</phrase> (MRI) datasets acquired from seven volunteers and one patient. The amplitude prediction technique showed errors of 1.9-6.5 mm. The combined predictive and adaptive technique showed 3-D motion prediction errors of 1.0-2.8 mm, which represents an improvement in modelling performance of up to 40% over a standard nonadaptive single parameter motion model. We also applied the combined technique in a clinical setting to test the feasibility of using it for respiratory motion correction of roadmaps in image-guided <phrase>cardiac</phrase> catheterisations. In this clinical case we show that 2-D registration errors due to respiratory motion are reduced from 7.7 to 2.8 mm using the proposed technique.
Decision-theoretic Reenement Planning: Principles and Application We present a general theory of action abstraction for reducing the complexity of decision-theoretic planning. We develop projection rules for abstract actions and prove our abstraction techniques to be correct. We present a planning algorithm that uses the abstraction theory to eeciently explore the space of possible plans by eliminating suboptimal classes of plans without explicitly examining all plans in those classes. An instance of the algorithm has been implemented as the drips decision-theoretic reenement <phrase>planning system</phrase>. We apply the planner to the problem of selecting the optimal test/treat strategy for managing patients suspected of having <phrase>deep-vein thrombosis</phrase> of the lower extremities. We show that drips signiicantly outperforms a standard <phrase>branch-and-bound</phrase> <phrase>decision tree</phrase> evaluation algorithm on this domain. We would like to thank Charles <phrase>Kahn</phrase> for pointing us to the DVT application.
An inexact <phrase>Newton method</phrase> combined with Hestenes multipliers' scheme for the solution of Karush-Kuhn-Tucker systems In this work a Newton interior–point method for the solution of Karush– Kuhn–Tucker systems is presented. A crucial feature of this <phrase>iterative method</phrase> is the solution, at each iteration, of the inner subproblem. This subproblem is a linear–<phrase>quadratic programming</phrase> problem, that can solved approximately by an inner <phrase>iterative method</phrase> such as the Hestenes multipliers' method. A deep analysis on the choices of the parameters of the method (perturbation and <phrase>damping</phrase> parameters) has been done. The global convergence of the Newton interior–point method is proved when it is viewed as an inexact <phrase>Newton method</phrase> for the solution of non-linear systems with restriction on the sign of some variables. The Newton interior–point method is numerically evaluated on <phrase>large scale</phrase> <phrase>test problems</phrase> arising from elliptic <phrase>optimal control</phrase> problems which show the effectiveness of the approach.
<phrase>Web-based</phrase> Medical Teaching using a <phrase>Multi-Agent System</phrase> <phrase>Web-based</phrase> teaching via <phrase>Intelligent Tutoring Systems</phrase> (ITSs) is considered as one of the most successful enterprises in <phrase>artificial intelligence</phrase>. Indeed, there is a long list of ITSs that have been tested on humans and have proven to facilitate learning, among which we may find the well-tested and known tutors of algebra, geometry, and computer languages. These ITSs use a variety of computational paradigms, as <phrase>production systems</phrase>, <phrase>Bayesian networks</phrase>, schema-templates, theorem proving, and explanatory reasoning. The next generation of ITSs are expected to go one step further by adopting not only more intelligent interfaces but will focus on integration. This article will describe some particularities of a tutoring system that we are developing to simulate conversational dialogue in the area of Medicine, that enables the integration of highly heterogeneous sources of information into a coherent <phrase>knowledge base</phrase>, either from the tutor's <phrase>point of view</phrase> or the development of the discipline in itself, i.e. the system's content is created automatically by the physicians as their daily work goes on. This will encourage students to articulate lengthier answers that exhibit deep reasoning, rather than to deliver straight tips of shallow knowledge. The goal is to take advantage of the normal functioning of the <phrase>health care</phrase> units to build on the fly a <phrase>knowledge base</phrase> of cases and data for teaching and research purposes.
<phrase>Monte Carlo</phrase> <phrase>Tree Search</phrase>: <phrase>Long-term</phrase> versus <phrase>short-term</phrase> planning —In this paper we investigate the use of <phrase>Monte Carlo</phrase> <phrase>Tree Search</phrase> (MCTS) on the Physical <phrase>Travelling Salesman Problem</phrase> (PTSP), a real-time game where the player navigates a ship across a map full of obstacles in order to visit a series of waypoints as quickly as possible. In particular, we assess the algorithm's ability to plan ahead and subsequently solve the two major constituents of the PTSP: the order of waypoints (<phrase>long-term</phrase> planning) and driving the ship (<phrase>short-term</phrase> planning). We show that MCTS can provide better results when these problems are treated separately: the optimal order of cities is found using Branch & Bound and the ship is navigated to collect the waypoints using MCTS. We also demonstrate that the physics of the PTSP game impose a challenge regarding the optimal order of cities and propose a solution that obtains better results than following the TSP route of minimum <phrase>Euclidean distance</phrase>. I. INTRODUCTION Games haven always been a popular benchmark for testing new techniques in <phrase>computational intelligence</phrase>. Real-time (video) games have become increasingly popular in recent years and many competitions are held at international conferences every year where competitors from different areas of research compete to be the best. <phrase>Video games</phrase> tend to be very complex and players must solve a wide range of problems, often in very little time, to make progress. To better understand these requirements, it is useful to examine some of the characteristics of such games in a simplified framework. In this paper we focus on a simple single-player real-time game: the Physical <phrase>Travelling Salesman Problem</phrase> (PTSP) requires the player to navigate a ship in real-time across a map filled with obstacles to collect a series of waypoints as quickly as possible. Despite its simplicity, the PTSP is representative of the numerous challenges a player faces in more complex <phrase>video games</phrase>, such as real-time constraints, continuous <phrase>state spaces</phrase> and open-endedness. The PTSP lacks the presence of an opponent and hence one is able to plan ahead without having to worry about the actions carried out by the adversary. However, this does not make it trivial: the PTSP is a real-time game where the action to execute must be chosen quickly. Hence, it is usually not possible to plan the entire gameplay at the <phrase>early stages</phrase> of the game. Furthermore, the <phrase>search space</phrase> may simply be too big to perform deep searches. In many cases one has …
<phrase>Ship Squat</phrase> in Non-uniform <phrase>Water Depth</phrase> The problem of predicting <phrase>ship squat</phrase> in non-uniform <phrase>water depth</phrase> is studied in this paper. For transverse depth variations, calculations are done using slender-body <phrase>shallow-water</phrase> theory, as implemented in the code " ShallowFlow ". Examples are given for realistic ships transiting dredged channels, and the effect of channel width on <phrase>ship squat</phrase> is discussed. Further examples are given for ships transiting canals such as the new <phrase>Panama</phrase> <phrase>Canal</phrase>. It is found that in a typical dredged channel, midship squat can be in the order of 20% larger than in open water of the same depth, while dynamic trim is essentially unchanged. In canals such as the new <phrase>Panama</phrase> <phrase>canal</phrase>, midship sinkage can be 100% larger than in open water of the same depth. 1. Introduction Many port approaches utilize dredged channels, with <phrase>shallow water</phrase> either side of a deep channel. This has implications for ship under-<phrase>keel</phrase> clearance, as transverse depth restrictions tend to increase <phrase>ship squat</phrase> over its uniform-depth value. The effect is magnified further in wall-sided canals. As an example, model tests of Guliev [1] showed an increase of 20% for squat in a dredged channel, and an increase of 150% for squat in a <phrase>canal</phrase>. In this article, we discuss the mechanisms of increased <phrase>ship squat</phrase> in confined water, and give example calculations for realistic <phrase>test cases</phrase>. 2. Theory The theory used here is based on the slender-body <phrase>shallow water</phrase> theory of Tuck [2] for open water; Tuck [3] for canals and <phrase>Beck</phrase> et al. [4] for dredged channels. Minor changes to these theories have been made to make them more applicable to modern transom-<phrase>stern</phrase> ships, and the methods have been extended to cater to arbitrary transverse <phrase>bathymetry</phrase>, as described in [5]. The ship inputs are simply the <phrase>waterline</phrase> breadth and section area curve, so there is no need to mesh the <phrase>hull</phrase>. For commercial ships whose lines plan is <phrase>confidential</phrase>, the required inputs may be estimated based on representative standard series ships, modified based on information from the ship's Trim and Stability Book, as described in [6]. Validation has been done using containership model tests in rectangular and non-rectangular canals [7]; <phrase>bulk carrier</phrase> model tests in wide canals [8]; containerships at full-scale [9],[10]; and bulk carriers at full-scale [11]. The available <phrase>experimental data</phrase> has been used to develop empirical corrections to the theoretical methods, following the ICORELS procedure adopted by PIANC [12]. The resulting methods are implemented …
Identifying and Discriminating Seismic Patterns Leading ¯ank Eruptions at Mt. <phrase>Etna</phrase> <phrase>Volcano</phrase> during 1981±1996 <phrase>Seismicity</phrase> affecting Mt. <phrase>Etna</phrase> <phrase>Volcano</phrase> (Italy) has been investigated in order to identify and discriminate seismic patterns precursory to ¯ank eruptions. An intense period (1981±1996) of <phrase>seismicity</phrase> and <phrase>volcanism</phrase>, during which eight ¯ank eruptions occurred has been considered. Two <phrase>statistical methods</phrase> are used: mean <phrase>hypothesis testing</phrase> and <phrase>entropic</phrase> <phrase>decision trees</phrase>. The results of the two methods are consistent and reveal a pattern of`deep' and`<phrase>western</phrase>' events, prior to the ¯ank eruptions that can be used as a predictive tool as well as a physical modeling constraint.
A <phrase>Task-specific</phrase> Approach for Crawling the Deep Web There is a great amount of valuable information on the web that cannot be accessed by conventional crawler engines. This portion of the web is usually known as the Deep Web or the <phrase>Hidden Web</phrase>. Most probably, the information of highest value contained in the deep web, is that behind web forms. In this paper, we describe a prototype hidden-<phrase>web crawler</phrase> able to access such content. Our approach is based on providing the crawler with a set of domain definitions, each one describing a specific data-collecting task. The crawler uses these descriptions to identify relevant query forms and to learn to execute queries on them. We have tested our techniques for several <phrase>real world</phrase> tasks, obtaining a high degree of effectiveness. I. INTRODUCTION Crawlers are <phrase>software programs</phrase> that automatically traverse the web, retrieving pages to build a searchable index of their content. Conventional crawlers receive as input a set of "seed" pages and recursively obtain new ones by locating and traversing their outbound links. Crawling techniques have led the construction of highly successful commercial <phrase>web search engines</phrase>. Nevertheless, conventional web crawlers cannot access to a significant fraction of the web, which is usually called the " <phrase>hidden web</phrase> " or the " <phrase>deep web</phrase> ". The problem of crawling the " <phrase>hidden web</phrase> " can be divided into two challenges:-Crawling the " server-side " <phrase>hidden web</phrase>. Many websites offer query forms to access the contents of an underlying database. Conventional crawlers cannot access these pages because they do not know how to execute queries on those forms.-Crawling the " <phrase>client-side</phrase> " <phrase>hidden web</phrase>. Many websites use techniques such as <phrase>client-side</phrase> <phrase>scripting</phrase> languages and session maintenance mechanisms. Most conventional crawlers are unable to handle this kind of pages. Several works have tried to characterize the <phrase>hidden web</phrase> [4],
A Comparison of Broad Versus Deep Auditory Menu Structures OBJECTIVE The primary purpose of this experiment was to gain a greater understanding of the utilization of <phrase>working memory</phrase> when interacting with a speech-enabled <phrase>interactive voice response</phrase> (IVR) system.   BACKGROUND A widely promoted guideline advises limiting IVR menus to five or fewer items because of constraints of the human memory system, commonly citing Miller's (1956) paper. The authors argue that Miller's paper does not, in fact, support this guideline. Furthermore, applying modern theories of working memory leads to the opposite conclusion--that reducing menu length by creating a deeper structure is actually more demanding of users' working memories and leads to poorer performance and satisfaction.   METHOD Participants took a <phrase>working memory</phrase> capacity test and then attempted to complete a series of e-<phrase>mail</phrase> tasks using one of two IVR designs (functionally equivalent, but one with a broad menu structure and the other with a <phrase>deep structure</phrase>).   RESULTS Users of the broad-structure IVR performed better and were more satisfied than users of the <phrase>deep-structure</phrase> IVR. Furthermore, this effect was more pronounced for those with low <phrase>working memory</phrase> capacity.   CONCLUSION Results indicate that creating a deeper structure is more demanding of working memory resource than the alternative of longer, shallower menus.   APPLICATION This experiment has important practical implications for all systems with auditory menus (particularly IVRs) because it provides <phrase>empirical evidence</phrase> refuting a widely promoted design practice.
Precision Passive Alignment of Wafers Precision Passive Alignment of Wafers Several macro-scale bench level experiments were carried out to evaluate the alignment <phrase>repeatability</phrase> that can be obtained through the elastic averaging principle. Based on these results, a precision passive alignment technique for <phrase>wafer bonding</phrase> application was developed. Wafer integral features that allow two stacked wafers to self-align were designed, fabricated and tested for wafer alignment <phrase>repeatability</phrase> and accuracy. Testing has demonstrated sub-micrometer <phrase>repeatability</phrase> and accuracy can be held using the proposed technique on 4 inch wafers. Passive alignment of the wafers is achieved when convex pyramids, supported on flexural cantlievers, and concave v-grooves patterned on the edges of the wafer engage and are preloaded. A silicon <phrase>cantilever</phrase> beam flexure between one of the wafers and the <phrase>pyramid</phrase> provides compliance to the coupling to avoid strain on the wafers and allows the surfaces of the wafers to mate. Both the concave coupling features and the convex coupling features are bulk microma-chined through wet <phrase>anisotropic</phrase> etch (KOH). The convex features are then release etched through a backside <phrase>deep reactive ion</phrase> etch (DRIE). As part of the fabrication process development, tests were performed to optimize the convex corner compensating mask structures needed to create the <phrase>pyramid</phrase> shaped convex coupling structures. Testing has shown that patterning two pairs of features on each of the four sides of the wafer is enough to achieve sub-micrometer <phrase>repeatability</phrase>.
Building Predictive Models on Complex Symbolic Sequences with a Second-Order Recurrent BCM Network with <phrase>Lateral Inhibition</phrase> <phrase>Activation patterns</phrase> across recurrent units in <phrase>recurrent neural networks</phrase> (RNNs) can be thought of as spatial codes of the history of inputs seen so far. When trained on symbolic sequences to perform the next-symbol prediction, RNNs tend to organize their <phrase>state space</phrase> so that \close" recurrent activation vectors correspond to histories of symbols yielding similar next-symbol distributions 1]. This leads to simple nite-context predictive models built on top of recurrent activations by grouping close <phrase>activation patterns</phrase> via a <phrase>vector quantization</phrase>. In this paper we investigate an unsu-pervised alternative to the state space organization. In particular, we use a recurrent version of the Bienenstock, Cooper and Munro (BCM) network with <phrase>lateral inhibition</phrase> 2] to map histories of symbols into activations of the recurrent layer. Recurrent BCM networks perform a kind of time-conditional projection pursuit. We compare the nite-context models built on top of BCM recurrent activations with those constructed on top of RNN recurrent activation vectors. As a <phrase>test bed</phrase> we use two complex symbolic sequences with rather deep memory structures. Surprisingly, the BCM-<phrase>based model</phrase> has a comparable or better performance than its RNN-based counterpart. This can be explained by the familiar information latching problem in <phrase>recurrent networks</phrase> when longer time spans are to be latched 3, 4].
Bio-inspired grasp control in a robotic hand with massive sensorial input The capability of grasping and lifting an object in a suitable, stable and controlled way is an outstanding feature for a robot, and thus far, one of the major problems to be solved in robotics. No robotic tools able to perform an advanced control of the grasp as, for instance, the human hand does, have been demonstrated to date. Due to its capital importance in science and in many applications, namely from biomedics to manufacturing, the issue has been matter of deep scientific investigations in both the field of <phrase>neurophysiology</phrase> and robotics. While the former is contributing with a profound understanding of the dynamics of real-time control of the slippage and grasp force in the human hand, the latter tries more and more to reproduce, or take inspiration by, the nature's approach, by means of hardware and software technology. On this regard, one of the major constraints robotics has to overcome is the real-time processing of a large amounts of data generated by the tactile sensors while grasping, which poses serious problems to the available computational power. In this paper a bio-inspired approach to tactile <phrase>data processing</phrase> has been followed in order to design and test a hardware-software robotic architecture that works on the <phrase>parallel processing</phrase> of a large amount of <phrase>tactile sensing</phrase> signals. The working principle of the architecture bases on the cellular nonlinear/<phrase>neural network</phrase> (CNN) paradigm, while using both hand shape and spatial-temporal features obtained from an array of microfabricated force sensors, in order to control the sensory-<phrase>motor coordination</phrase> of the robotic system. Prototypical grasping tasks were selected to measure the system performances applied to a computer-interfaced robotic hand. Successful grasps of several objects, completely unknown to the robot, e.g. soft and deformable objects like plastic bottles, soft balls, and Japanese <phrase>tofu</phrase>, have been demonstrated.
Diagnostics and a qualitative model First generation <phrase>expert systems</phrase> were using shallow knowledge based on heuristic information to solve a diagnostic problem. This approach has many disadvantages, which can be avoided by using <phrase>deep knowledge</phrase>. Diagnostic reasoning based on <phrase>deep knowledge</phrase> is called <phrase>model-based</phrase> diagnostics. Recently, the use of qualitative modeling in relation to <phrase>deep knowledge</phrase> in <phrase>expert systems</phrase> has become <phrase>increasingly important</phrase>. The main purpose of our contribution is to present the <phrase>model-based</phrase> diagnostic approach at a formal level. The originality of the presented formalization is the concept of the diagnostic space, the characterization of the minimal diagnoses, and the measurement. The formalization serves as the theoretical background to prove our view to the design of qualitative system models and to establish the diagnostic architecture called DISY. The qualitative system model in our diagnostic approach needs not to be specially adopted for use in the diagnostic domain. The only requirement is that it must simulate the system behavior expressed by normal or abnormal functioning of its components. Proposed DISY architecture is not complex and simply takes into an account the previous diagnostic result to obtain a new one from the additional observation-measurement (medical tests or examinations) of the system.
An Extended Metastability <phrase>Simulation Method</phrase> for Synchronizer Characterization Synchronizers play a key role in multi-clock domain systems on chip. Designing reliable synchronizers requires estimating and evaluating synchro-nizer parameters (resolution time constant) and (metastability window). Typically, evaluation of these parameters has been done by empirical rules of thumb or simple circuit simulations to ensure that the synchronizer <phrase>MTBF</phrase> is sufficiently long. This paper shows that those rules of thumb and some common <phrase>simulation method</phrase> are unable to predict correct synchronizer parameters in deep sub-micron technologies. We propose an extended <phrase>simulation method</phrase> to estimate synchronizer characteristics more reliably and compare the <phrase>results obtained</phrase> with other state-of-the-art simulation methods and with measurements of a 65nm LP CMOS <phrase>test-chip</phrase>. 1 Introduction Multiple-clock domain System on Chip (SoC) designs require synchronization when transferring signals and data among clock domains and when receiving asynchronous inputs. Such synchronizations are susceptible to metastability effects which can cause malfunction in a receiving circuit. In critical designs, this risk must be mitigated. To assess the risk and to design reliable synchronizers, models describing the <phrase>failure mechanisms</phrase> for latches and <phrase>flip-flops</phrase> have been developed [ 1][ 2]. Most models express the risk of not resolving metastability in terms of the meantime between failures (<phrase>MTBF</phrase>) of the circuit, Eq. (1), where S is the time allotted for resolution, and are the receiver and sender clock frequencies, respectively, is the resolution time constant, and is a parameter related to the effective setup-and-hold time window during which the synchronizer is vulnerable to metastability. Over the years, techniques have been developed for obtaining an arbitrarily long <phrase>MTBF</phrase>. These techniques have been translated into convenient rules of thumb for designers. As <phrase>digital circuits</phrase> have become more complex, denser and faster with
The Impact of Organization, Project and Governance Variables on <phrase>Software Quality</phrase> and Project Success — In this paper we present a statistically tested evidence about how quality and success rate are correlated with variables reflecting the organization and aspects of its project's governance, namely retrospectives and metrics. The results presented in this paper are based on the Agile Projects Governance Survey that collected 129 responses. This paper discuss the deep analysis of this survey, and the main <phrase>findings suggest</phrase> that when applying <phrase>agile software development</phrase>, the quality of software improves as the organization measures <phrase>customer satisfaction</phrase> more frequently, and as the impact of retrospective increases. Project success improves as quality, frequency of measuring <phrase>customer satisfaction</phrase>, organization experience in agile development, retrospective impact, the team participation in retrospective and the team contribution to retrospective, increases.
Tracking real-time <phrase>user experience</phrase> (TRUE): a comprehensive instrumentation solution for complex systems Automatic recording of <phrase>user behavior</phrase> within a system (instrumentation) to develop and test theories has a rich history in psychology and system design. Often, researchers analyze instrumented behavior in isolation from other data. The problem with collecting instrumented behaviors without attitudinal, demographic, and contextual data is that researchers have no way to answer the 'why' behind the 'what'. We have combined the collection and analysis of behavioral instrumentation with other HCI methods to develop a system for Tracking Real-Time <phrase>User Experience</phrase> (TRUE). Using two <phrase>case studies</phrase> as examples, we demonstrate how we have evolved instrumentation methodology and analysis to extensively improve the design of <phrase>video games</phrase>. It is our hope that TRUE is adopted and adapted by the broader HCI community, becoming a useful tool for gaining deep insights into <phrase>user behavior</phrase> and improvement of design for other complex systems.
Deformations of IC Structure in Test and <phrase>Yield Learning</phrase> This paper argues that the existing approaches to modeling and characterization of IC malfunctions are inadequate for test and <phrase>yield learning</phrase> of <phrase>Deep Sub-Micron</phrase> (DSM) products. Traditional notions of a spot defect and local and global <phrase>process variations</phrase> are analyzed and their shortcomings are exposed. A detailed taxonomy of process-induced deformations of DSM IC structures, enabling modeling and characterization of IC malfunctions, is proposed. The blueprint of a roadmap enabling such a characterization is suggested.
Partition Based SoC <phrase>Test Scheduling</phrase> with Thermal and Power Constraints under <phrase>Deep Submicron Technologies</phrase> For core-based System-on-Chip (SoC) testing, conventional <phrase>power-constrained</phrase> <phrase>test scheduling</phrase> methods do not guarantee a thermal-safe solution. Also, most of the test scheduling schemes make poor assumptions about <phrase>power consumption</phrase>. In <phrase>deep submicron era</phrase>, <phrase>leakage power</phrase> and wake-up <phrase>power consumption</phrase> can not be neglected. In this paper, we propose a partition based thermal-aware <phrase>test scheduling algorithm</phrase> with more realistic assumptions of recent SoCs. In our <phrase>test scheduling algorithm</phrase>, each test is partitioned and the earliest starting time of each partition is searched. To reduce the execution time of thermal simulation, we also exploit <phrase>superposition principle</phrase> to compute the power and thermal profile rapidly and accurately. We apply our <phrase>test scheduling algorithm</phrase> to <phrase>ITC</phrase>'02 SoC benchmarks and the results show improvements in the total test time over scheduling schemes without partitioning.
Local Optimization Method with Global Multidimensional Search for Descent This paper presents a new method for solving <phrase>global optimization</phrase> problems. We use a local technique based on the notion of discrete gradients for finding a cone of descent directions and then we use a global cutting angle algorithm for finding global minimum within the intersection of the cone and the feasible region. We present results of numerical experiments with well-known <phrase>test problems</phrase> and with the so-called cluster function. These results confirm that the proposed algorithm allows one to find a global minimizer or at least a deep local minimizer of a function with a huge amount of shallow <phrase>local minima</phrase>.
<phrase>Delay testing</phrase> considering <phrase>power supply noise</phrase> effects We propose a new delay <phrase>test generation</phrase> technique that can take into account the impact of the <phrase>power supply noise</phrase> on the signal <phrase>propagation delays</phrase>. This is diierent from existing <phrase>delay fault</phrase> models and <phrase>test generation</phrase> techniques that ignore the dependence of <phrase>path delays</phrase> on the applied <phrase>test patterns</phrase> and cannot capture the worst-case timing scenarios in <phrase>deep submicron designs</phrase>. In addition to sensitizing the fault and propagating the fault eeects to the primary outputs , our new tests also produce the worst-case <phrase>power supply noise</phrase> on the nodes in the target path. Thus, the tests also cause the worst-case <phrase>propagation delay</phrase> for the nodes along the target path. Our experimental results on <phrase>benchmark circuits</phrase> show that the new <phrase>delay tests</phrase> produce signiicantly longer delays on the tested paths compared to the tests derived using existing <phrase>delay testing</phrase> methods.
Deciding Confluence of Ground Term Rewrite Systems in Cubic Time It is well known that the confluence property of ground term rewrite systems (ground TRSs) is <phrase>decidable</phrase> in polynomial time. For an efficient implementation, the degree of this polynomial is of great interest. The best complexity bound in the literature is given by Comon, Godoy and Nieuwenhuis (2001), who describe an O(n 5) algorithm, where n is the size of the ground TRS. In this paper we improve this bound to O(n 3). The algorithm has been implemented in the confluence tool <phrase>CSI</phrase>. 1 Introduction It is well known that confluence of ground TRSs can be decided in polynomial time. In this paper, we are interested in the degree of the associated polynomial. To derive a polynomial time decision procedure for confluence of ground TRSs, Comon et al. [3] use an approach based on a transformation by Plaisted [9] that flattens the TRS. Then they test deep joinability of sides of rules. The authors sketch an implementation with complexity O(n 5), where n is the size of the given TRS. Tiwari [10] and Godoy et al. [6] base their approach on a rewrite closure that constructs tree transducers—the given TRS R is converted into two TRSs F and B such that F and B −1 are left-flat, right-constant, F is terminating, and → * R = → * F · → * B. They then consider top-stabilizable terms to derive conditions for confluence. Tiwari obtains a bound of O(n 9) (but a more careful implementation would end up with O(n 6)), while Godoy et al. obtain a bound of O(n 6). The algorithm of [3] is limited to ground TRSs, but [10] extends the algorithm to certain shallow, linear systems, and [5] treats shallow, linear systems in full generality. 1 In these extensions, however, the exponent depends on the maximum <phrase>arity</phrase> of the function symbols of the given TRS. In our work we combine ideas from [3, 10, 6] in order to improve the complexity bound to O(n 3). The key ingredients are a Plaisted-style rewrite closure, which results in TRSs F and B of only quadratic size, and top-stabilizability, which is cheaper to test than deep joinability. 1 The same claim can be found in [6]. However, rule splitting, a key step in the proof of their Lemma 3.1, only works if left-hand side and right-hand side variables are disjoint for every rule.
A Novel Algorithm for <phrase>Color Constancy</phrase> Introduction ' specific clients, is ht Clearance Center lid directly to CCC or general distribu-<phrase>ale</phrase>. Abstract <phrase>Color constancy</phrase> is the skill by which it is possible to tell the color of an object even under a colored light. I interpret the color of an object as its color under a fixed canonical light, rather than as a surface reflectance function. This leads to an analysis that shows two distinct sets of circumstances under which <phrase>color constancy</phrase> is possible. In this framework, <phrase>color constancy</phrase> requires estimating the illuminant under which the image was taken. The estimate is then used to choose one of a set of linear maps, which is applied to the image to yield a color descriptor at each point. This set of maps is computed in advance. The illuminant can be estimated using image measurements alone, because, given a number of weak assumptions detailed in the text, the color of the illuminant is constrained by the colors observed in the image. This constraint arises from the fact that surfaces can reflect no more light than is cast on them. For example, if one observes a patch that excites the red receptor strongly, the illuminant cannot have been <phrase>deep blue</phrase>. Two algorithms are possible using this constraint, corresponding to different assumptions about the world. The first algorithm, Crule will work for any surface reflectance. Crule corresponds to a form of coefficient rule, but obtains the coefficients by using constraints on illuminant color. The set of illuminants for which Crule will be successful depends strongly on the choice of <phrase>photoreceptors</phrase>: for narrowband <phrase>photoreceptors</phrase>, Crule will work in an unrestricted world. The second algorithm, Mwext, requires that both surface reflectances and illuminants be chosen from finite dimensional spaces; but under these restrictive conditions it can recover a large number of parameters in the illuminant, and is not an attractive model of human <phrase>color constancy</phrase>. Crule has been tested on real images of Mondriaans, and works well. I show results for Crule and for the Retinex algorithm of Land (Land 1971; Land 1983; Land 1985) operating on a number of real images. The experimental work shows that for good constancy, a <phrase>color constancy</phrase> system will need to adjust the gain of the receptors it employs in a fashion analagous to adaptation in humans. People experience color as a surface property that is largely unaffected by the color of the illuminating light. This phenomenon …
Recent Advances in AI Planning The past five years have seen dramatic advances in <phrase>planning algorithms</phrase>, with an emphasis on propo-sitional methods such as GRAPHPLAN and compilers that convert <phrase>planning problems</phrase> into <phrase>propositional</phrase> <phrase>conjunctive normal form</phrase> formulas for solution using systematic or stochastic SAT methods. Related work, in the context of spacecraft control, advances our understanding of interleaved planning and execution. In this survey, I explain the latest techniques and suggest areas for <phrase>future research</phrase>. T he field of AI planning seeks to build control algorithms that enable an agent to synthesize a course of action that will achieve its goals. Although researchers have studied planning since the early days of AI, <phrase>recent developments</phrase> have revolutionized the field. Two approaches, in particular, have attracted much attention: (1) the two-phase GRAPHPLAN (Blum and Furst 1997) planning algorithm and (2) methods for compiling <phrase>planning problems</phrase> into <phrase>propositional</phrase> formulas for solution using the latest, speedy systematic and stochastic SAT algorithms. These approaches have much in common, and both are affected by recent progress in <phrase>constraint satisfaction</phrase> and search technology. The current level of performance is quite impressive , with several <phrase>planners</phrase> quickly <phrase>solving problems</phrase> that are <phrase>orders of magnitude</phrase> harder than the test pieces of only two <phrase>years ago</phrase>. As a single, representative example, the BLACKBOX planner (Kautz and Selman 1998a) requires only 6 minutes to find a 105-action <phrase>logistics</phrase> plan in a world with 10 16 possible states. Furthermore, work on <phrase>propositional</phrase> planning is <phrase>closely related</phrase> to the algorithms used in the autonomous controller for the <phrase>National Aeronautics and Space Administration</phrase> (NASA) <phrase>Deep Space</phrase> One spacecraft, launched in October 1998. As a result, our understanding of inter-leaved planning and execution has advanced as well as the speed with which we can solve classical <phrase>planning problems</phrase>. The goal of this survey is to explain these <phrase>recent advances</phrase> and suggest new directions for research. Because this article requires minimal AI background (for example, simple logic and basic <phrase>search algorithms</phrase>), it's suitable for a wide audience, but my treatment is not exhaustive because I don't have the space to discuss every active topic of planning research. 1 I progress as follows: The remainder of the introduction defines the planning problem and surveys freely <phrase>downloadable</phrase> planner implementations. The next sections discuss GRAPHPLAN, SAT <phrase>compilation</phrase> , and interleaved planning and execution. I conclude by quickly mentioning other <phrase>recent advances</phrase> and suggestion topics for <phrase>future research</phrase>. Preliminaries A simple " classical " formulation …
TriBiCa: <phrase>Trie</phrase> <phrase>Bitmap</phrase> Content Analyzer for <phrase>High-Speed</phrase> Network <phrase>Intrusion Detection</phrase> —<phrase>Deep packet inspection</phrase> (DPI) is often used in network <phrase>intrusion detection</phrase> and prevention systems (NIDPS), where incoming packet payloads are compared against known attack signatures. Processing every single <phrase>byte</phrase> in the incoming packet payload has a very stringent time constraint, e.g., 200 ps for a 40-Gbps line. Traditional DPI systems either need a large memory space or use special memory such as <phrase>ternary</phrase> <phrase>content addressable memory</phrase> (TCAM), limiting parallelism, or yielding high cost/<phrase>power consumption</phrase>. In this paper, we present a <phrase>high-speed</phrase>, <phrase>single-chip</phrase> DPI scheme that is scalable and configurable through memory updates. The scheme is based on a novel <phrase>data structure</phrase> called TriBiCa (<phrase>Trie</phrase> <phrase>Bitmap</phrase> Content Analyzer), which provides minimal perfect <phrase>hashing</phrase> functionality. It uses a <phrase>trie</phrase> structure with a <phrase>hash function</phrase> performed at each layer. Branching is determined by the <phrase>hashing</phrase> results with an objective to evenly partition attack signatures into multiple groups at each layer. During a query, as an input traverses the <phrase>trie</phrase>, an address to a table in the memory that stores all attack signatures is formed and is used to access the signature for an exact match. Due to the small space required, multiple copies of TriBiCa can be implemented on a single chip to perform pipelining and parallelism simultaneously, thus achieving <phrase>high throughput</phrase>. We have designed the TriBiCa on a modest FPGA chip, <phrase>Xilinx</phrase> Virtex II Pro, achieving 10-Gbps throughput without using any external memory. A proof-of-concept design is implemented and tested with 1-Gbps packet streams. By using today's state-of-the-art FPGAs, a throughput of 40 Gbps is believed to be achievable.
Automating defects simulation and <phrase>fault modeling</phrase> for SRAMs —The continuos improvement in <phrase>manufacturing process</phrase> density for Very <phrase>Deep Sub Micron technologies</phrase> constantly leads to new classes of defects in memory devices. Exploring the effect of fabrication defects in future technologies, and identifying new classes of realistic functional <phrase>fault models</phrase> with their corresponding <phrase>test sequences</phrase>, is a time consuming task up to now mainly performed by hand. This paper proposes a new approach to automate this procedure. The proposed method exploits the capabilities of <phrase>evolutionary algorithms</phrase> to automatically identify faulty behaviors into defective memories and to define the corresponding <phrase>fault models</phrase> and relevant <phrase>test sequences</phrase>. Target defects are modeled at the electrical level in order to optimize the results to the specific technology and memory architecture.
Measurement and Analysis of Variability in <phrase>Cmos Circuits</phrase> Measurement and Analysis of Variability in <phrase>Cmos Circuits</phrase> Measurement and Analysis of Variability in <phrase>Cmos Circuits</phrase> Permission to make digital or <phrase>hard copies of</phrase> all or part of this work for personal or classroom use is <phrase>granted without fee provided</phrase> that copies are not made or distributed for <phrase>profit or commercial advantage and</phrase> that <phrase>copies bear</phrase> this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, <phrase>requires prior specific permission</phrase>. Abstract Measurement and Analysis of Variability in <phrase>CMOS circuits</phrase> The scaling of <phrase>CMOS technology</phrase> into the deep sub-micron regime has resulted in increased impact of process variability on circuits, to the point where it is considered a <phrase>major bottleneck</phrase> to further scaling. In order to continue scaling, there is a need to reduce margins in the design by classifying <phrase>process variations</phrase> as systematic or random. In this work, a methodology to characterize variability through measurement and analysis has been developed. Systematic and random, die-to-die (D2D) and within-die (WID) components of variability are quantified and corresponding sources of variability are identified. This methodology was developed for an early 90nm <phrase>CMOS process</phrase> and further refined for an early 45nm <phrase>CMOS process</phrase>. Test-chips have been designed to study the effects of layout, and characterize variability of delay and <phrase>leakage current</phrase> using an array of <phrase>test-structures</phrase>. Delay is obtained through the measurement of <phrase>ring oscillator</phrase> frequencies, and transistor <phrase>leakage current</phrase> is measured by an on-chip <phrase>analog-to-digital converter</phrase> (ADC). 2 In 90nm, it has been found that transistor performance depends strongly on polysil-<phrase>icon</phrase> (poly-Si) gate density and that <phrase>spatial correlation</phrase> depends on gate orientation and the direction of gate spacing. WID variation is small with three standard deviations over mean (3σ/μ) ≈ 3.5%, whereas D2D and systematic layout-induced variations are significant, with 3σ/μ D2D variation of ≈ 15% and a maximum layout-induced frequency shift of 10%. In 45nm, a process which features <phrase>immersion lithography</phrase>, strained-Si and more restrictive design rules for gate spacing, it has been found that systematic layout-induced variability has decreased. However, new sources of variability due to the dependence of stress on layout were found. WID has increased to 3σ/μ ≈ 6.6% and can be attributed to a smaller transistor area whereas D2D variation has remained at 3σ/μ ≈ 15%. This methodology is effective in characterizing variability. It improves the accuracy of statistical models and allows process corners to be set up for WID or D2D variations. In addition, sources of systematic variations are …
<phrase>Learning styles</phrase> and approaches to learning among medical undergraduates and postgraduates BACKGROUND The challenge of imparting a large amount of knowledge within a limited time period in a way it is retained, remembered and effectively interpreted by a student is considerable. This has resulted in crucial changes in the field of medical education, with a shift from didactic teacher centered and subject based teaching to the use of interactive, problem based, student centered learning. This study tested the hypothesis that <phrase>learning styles</phrase> (visual, auditory, read/write and kinesthetic) and approaches to learning (deep, strategic and superficial) differ among first and final year undergraduate medical students, and postgraduates medical trainees.   METHODS We used self administered VARK and ASSIST questionnaires to assess the differences in <phrase>learning styles</phrase> and approaches to learning among medical undergraduates of the University of <phrase>Colombo</phrase> and postgraduate trainees of the Postgraduate Institute of Medicine, <phrase>Colombo</phrase>.   RESULTS A total of 147 participated: 73 (49.7%) first year students, 40 (27.2%) final year students and 34(23.1%) postgraduate students. The majority (69.9%) of first year students had multimodal <phrase>learning styles</phrase>. Among final year students, the majority (67.5%) had multimodal <phrase>learning styles</phrase>, and among postgraduates, the majority were unimodal (52.9%) learners.Among all three groups, the predominant approach to learning was strategic. Postgraduates had significant higher mean scores for deep and strategic approaches than first years or final years (p < 0.05). Mean scores for the superficial approach did not differ significantly between groups.   CONCLUSIONS The <phrase>learning approaches</phrase> suggest a positive shift towards deep and strategic learning in postgraduate students. However a similar difference was not observed in <phrase>undergraduate students</phrase> from first year to final year, suggesting that their curriculum may not have influenced learning methodology over a five year period.
Minimally Supervised Domain-Adaptive Parse Reranking for Relation Extraction The paper demonstrates how the generic parser of a minimally supervised <phrase>information extraction</phrase> framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic <phrase>deep-linguistic</phrase> parser was employed that works with a largely hand-crafted <phrase>head-driven phrase structure grammar</phrase> (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank. It will be shown how the estimated confidence of RE rules learned from the n best parses can be exploited for parse reranking. The acquired rerank-ing model improves the performance of RE in both <phrase>training and test</phrase> phases with the new first parses. The obtained significant boost of recall does not come from an overall gain in parsing performance but from an application-driven selection of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for <phrase>task-specific</phrase> parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task.
Hybrid <phrase>Renewable</phrase> Energy Systems for a <phrase>Dynamically Positioned</phrase> Buoy Hybrid <phrase>Renewable</phrase> Energy Systems for a <phrase>Dynamically Positioned</phrase> Buoy Hybrid <phrase>Renewable</phrase> Energy Systems for a <phrase>Dynamically Positioned</phrase> Buoy  ii  We the undersigned committee hereby approve the attached thesis To ease the burdens associated with deep ocean buoy moorings, a relatively recent technological development known as <phrase>Dynamic Positioning</phrase> (DP) could be employed. This method, which is being used on some oil drilling ships and <phrase>semi-submersible</phrase> platforms, provides pinpoint positioning for precision drilling and other operations with the use of multiple, multi-axis, thrusters below the <phrase>waterline</phrase> of a vessel to counter the effects of winds and ocean currents. This eliminates the need for anchoring in deep oceans, but depending on the characteristics of the vessel and environmental conditions, power requirements for DP tend to be quite substantial and costly. A theoretical design of a hybrid wind and <phrase>solar energy</phrase> system on an ocean surface buoy is made for the purpose of powering a low cost, simple, <phrase>dynamic positioning</phrase> system. This system was implemented on a dynamically positioned buoy (DPB) intended for sea keeping using <phrase>renewable energy sources</phrase>.  iv  Some prototypes of autonomous surface vehicles have experimented with <phrase>renewable</phrase> energies as a source of supplemental power, but these vehicles are typically designed as transient surface vehicles with station keeping capability as a secondary function. A combination of design requirements set forth by 2004 <phrase>Defense Advanced Research Projects Agency</phrase> (<phrase>DARPA</phrase>) solicitation number BAA04-33 and 2008 solicitation number DARPASN08-45 are used as a basis for DPB design parameters. The aims of the DPB design thesis are to develop and test a low cost, <phrase>dynamic positioning</phrase> system that will continuously maintain a 250 m <phrase>watch</phrase> radius and to present a theoretical hybrid <phrase>renewable</phrase> energy system to power it, thereby improving on the station keeping buoy (SKB) energy balance problem. Global Positioning System (GPS) technology, combined with an 8-bit embedded microcontroller and circuitry provide sufficient autonomous control signals to independent thrusters below the <phrase>waterline</phrase>. These correct for position offsets caused by sea and air currents in the open ocean. The results of the system in a 2.5 m s-1 wind validated the feasibility of mounting a horizontal axis <phrase>wind turbine</phrase> on a buoy without a necessary counter balancing device. A hybrid wind and solar <phrase>renewable</phrase> energy system was designed using [54]. The 100% power load of about 1280 total <phrase>watts</phrase> proved too substantial to warrant the practicality of this <phrase>renewable</phrase> energy system in three out of four simulations. One optimization, however, was able to produce an annual capacity  v  …
<phrase>Bootstrapping</phrase> from <phrase>Game Tree</phrase> Search In this paper we introduce a new algorithm for updating the parameters of a heuris-tic evaluation function, by updating the heuristic towards the values computed by an <phrase>alpha-beta</phrase> search. Our algorithm differs from previous approaches to learning from search, such as Samuel's checkers player and the <phrase>TD</phrase>-Leaf algorithm, in two key ways. First, we update all nodes in the search tree, rather than a single node. Second, we use the outcome of a deep search, instead of the outcome of a subsequent search, as the training signal for the evaluation function. We implemented our algorithm in a chess program Meep, using a linear heuristic function. After initialising its weight vector to small random values, Meep was able to learn <phrase>high quality</phrase> weights from self-play alone. When tested online against human opponents , Meep played at a master level, the best performance of any chess program with a heuristic learned entirely from self-play.
Modeling and Analysis of <phrase>Crosstalk Coupling</phrase> Effect on the Victim Interconnect Using the ABCD Network Model The paper describes an ABCD modeling approach of a victim interconnect that <phrase>takes into account</phrase> the <phrase>crosstalk coupling</phrase> effect due to aggressor line in deep sub-micron chips. After the order reduction the crosstalk model is utilized for the analysis of <phrase>crosstalk coupling</phrase> effect on the victim's output signal. Various timing issues related to signal waveform such as, delay time, overshoot and undershoot occurrence time etc., that in effect help to ensure in prior the desired <phrase>signal integrity</phrase> (SI) and performance reliability of the SoCs, can be estimated analytically using the reduced order crosstalk model. It has been observed that the <phrase>crosstalk coupling</phrase> effect introduces the delay in the victim's output signal which can be significant enough or even unacceptable if many aggressors simultaneously couple energy to the victim line, or the line spacing between the aggressor and victim is reduced due to under-etching or even, length of the victim interconnect is increased because of improper layouts / routing. Influences of other interconnect parasitics on the victim´s output signal can also be tested using the same model. <phrase>Simulation results</phrase> obtained with our reduced order model is found to be quite good and comparable to the accuracy of the PSPICE simulation.
CPA and CCA-Secure Encryption Systems that are not 2-Circular Secure Traditional definitions of encryption guarantee security for plaintexts which can be derived by the adversary. In some settings, such as <phrase>anonymous</phrase> <phrase>credential</phrase> or disk encryption systems, one may need to reason about the security of messages potentially unknown to the adversary, such as secret keys encrypted in a self-loop or a cycle. A <phrase>public-key</phrase> cryptosystem is n-circular secure if it remains secure when the ciphertexts E are revealed, for independent key pairs. A natural question to ask is what does it take to realize circular security in the <phrase>standard model</phrase>? Are all CPA-secure (or CCA-secure) <phrase>cryptosystems</phrase> also n-circular secure for n > 1? One way to resolve this question is to produce a CPA-secure (or CCA-secure) cryptosystem which is demonstrably insecure for key cycles larger than self-loops. Recently and independently, Acar, Belenkiy, Bellare and Cash provided a CPA-secure cryptosystem, under the SXDH assumption, that is not 2-circular secure. In this paper, we present a different CPA-secure <phrase>counterexample</phrase> (under SXDH) as well as the first CCA-secure <phrase>counterexample</phrase> (under SXDH and the existence of certain NIZK proof systems) for n > 1. Moreover, our 2-circular attacks recover the secret keys of both <phrase>parties</phrase> and thus exhibit a <phrase>catastrophic failure</phrase> of the system whereas the attack in Acar et al. provides a test whereby the adversary can distinguish whether it is given a 2-cycle or two random ciphertexts. These negative results are an important step in answering deep questions about which attacks are prevented by commonly-used definitions and systems of encryption.
System Test Evaluation and Review Technique Stochastic Modeling and <phrase>Optimal Control</phrase> for System Testing of Software System Test Execution Process Test Manager System Test Execution Model System Test Evaluation and Review Technique|3 System Test Evaluation and Review Technique|4 ' H τ µ * µ [ ()] arg max lim i t E c t t µ µ →∞   −     System Test Evaluation and Review Technique|2 Preface This work is part of my <phrase>Master's degree</phrase> program at VU University, where each student is required to perform a research regarding a specific problem motivated by practice. I would like to extend my gratitude to dr. Sandjai Bhulai whose course in <phrase>Stochastic Optimization</phrase> instilled in me the belief that it was mathematically feasible to build a model for System Test Execution and Test Management. His research, ideas, efforts and enthusiasm have helped me enormously in writing this paper. I am grateful to Shunji Osaki and Hisashi Mine for their paper on Semi-<phrase>Markov Decision Processes</phrase> which helped me gain deep insight into the subject. Also, I would like to extend my gratitude to <phrase>Salah</phrase> E Elmaghraby for his work on GERT and SMP's which was enormously helpful for writing this paper. Finally, to James A Whittaker who pioneered the work on <phrase>Markov Chain</phrase> Modeling of <phrase>Software Testing</phrase> which in turn became the foundation on which I could perform my research in this area. This paper is my humble effort to extend the research in the field of Modeling of <phrase>Software Testing</phrase> and Test Management. Executive Summary System testing of software is defined as the " investigation conducted to evaluate whether a complete and integrated software system complies with its specified requirements " 1. Thus system testing is a process that requires creation of <phrase>test cases</phrase> for every function point of the software and execution of the <phrase>test cases</phrase> to validate whether the function point conforms to the specified requirements. In case of failure of a test, a defect is logged that is fixed by the development team and again re-tested. The system <phrase>testing process</phrase> can therefore be described by the following steps: (a) <phrase>Requirements analysis</phrase>, (b) Test estimation and strategy (c) Test planning (d) Creation of test scripts based on requirements (e) Execution of test scripts on the software product (f) Reporting of defects (g) Retesting of fixed defects and (h) Test Closure The system <phrase>testing process</phrase> described in steps (e)-(g) is a cyclic process and requires allocation of resources (software testers) to complete testing in allocated time. However, as <phrase>businesses</phrase> increasingly tend to reduce the time to market of their products and services, coupled with …
Analytical Semi-Empirical Model for SER Sensitivity Estimation of <phrase>Deep-Submicron</phrase> <phrase>CMOS Circuits</phrase> An analytical expression is proposed for the estimation of the <phrase>soft-error rate</phrase> (SER) sensitivity of circuits designed in deep-submicron CMOS technologies. The model parameters for a given technology and for a specific radiation type have been determined by combining experimental accelerated SER <phrase>test results</phrase> with critical charge data obtained from circuit simulations. The resulting analytical models are discussed for the cases of the alpha-induced SER of a 0.18 m process and for both the alpha-and neutron-induced SER of a 0.13 m process. The results indicate that the approach provides an efficient means to predict the contributions of individual nodes to the SER of a circuit. The method is shown to be effective in the evaluation of the impact of design modifications on the circuit SER.
I for <phrase>Cosmic Ray</phrase> Soft Errors in Semiconductor memories This paper presents a review of experiments performed by IBM to investigate the causes of soft errors in <phrase>semiconductor memory</phrase> chips under field test conditions. The effects of <phrase>alpha-particles</phrase> and <phrase>cosmic rays</phrase> are separated by comparing multiple measurements of the <phrase>soft-error rate</phrase> (SER) of samples of memory chips deep underground and at various altitudes above the earth. The results of <phrase>case studies</phrase> on four different memory chips show that <phrase>cosmic rays</phrase> are an important source of the <phrase>ionizing radiation</phrase> that causes <phrase>soft errors</phrase>. The results of <phrase>field testing</phrase> are used to confirm the accuracy of the modeling and the accelerated testing of chips.
On-chip cache hierarchy-aware <phrase>tile scheduling</phrase> for multicore machines Iteration space tiling and scheduling is an important technique for optimizing loops that constitute a large fraction of execution times in computation kernels of both scientific codes and embedded applications. While tiling has been studied extensively in the context of both uniprocessor and multiprocessor platforms, prior research has paid less attention to <phrase>tile scheduling</phrase>, especially when targeting multicore machines with deep on-chip cache hierarchies. In this paper, we propose a cache hierarchy-aware <phrase>tile scheduling</phrase> algorithm for multicore machines, with the purpose of maximizing both horizontal and vertical data reuses in on-chip caches, and balancing the workloads across different cores. This <phrase>scheduling algorithm</phrase> is one of the key components in a source-to-source translation tool that we developed for automatic loop parallelization and multithreaded code generation from sequential codes. To the best of our knowledge, this is the first effort that develops a fully-automated <phrase>tile scheduling</phrase> strategy customized for on-chip cache <phrase>topologies</phrase> of multicore machines. The experimental results collected by executing twelve application programs on three commercial Intel machines (Nehalem, <phrase>Dunnington</phrase>, and Harpertown) reveal that our cache-aware <phrase>tile scheduling</phrase> brings about 27.9% reduction in cache misses, and on average, 13.5% improvement in execution times over an alternate method tested.
Prediction of pharmacologically induced baroreflex sensitivity from local time and <phrase>frequency domain</phrase> indices of R-R interval and <phrase>systolic blood pressure</phrase> signals obtained during <phrase>deep breathing</phrase> Pharmacological measurement of baroreflex sensitivity (BRS) is widely accepted and used in clinical practice. Following the introduction of pharmacologically induced BRS (p-BRS), alternative assessment methods eliminating the use of drugs were in the center of interest of the cardiovascular <phrase>research community</phrase>. In this study we investigated whether p-BRS using <phrase>phenylephrine</phrase> injection can be predicted from non-pharmacological time and <phrase>frequency domain</phrase> indices computed from <phrase>electrocardiogram</phrase> (ECG) and <phrase>blood pressure</phrase> (BP) data acquired during <phrase>deep breathing</phrase>. In this scheme, ECG and BP data were recorded from 16 subjects in a two-phase experiment. In the first phase the subjects performed irregular deep breaths and in the second phase the subjects received <phrase>phenylephrine</phrase> injection. From the first phase of the experiment, a large pool of predictors describing the local characteristic of beat-to-beat interval tachogram (RR) and <phrase>systolic blood pressure</phrase> (SBP) were extracted in time and frequency domains. A subset of these indices was selected using twelve subjects with an exhaustive search fused with a leave one subject out <phrase>cross validation</phrase> procedure. The selected indices were used to predict the p-BRS on the remaining four test subjects. A multivariate regression was used in all prediction steps. The algorithm achieved best prediction accuracy with only two features extracted from the <phrase>deep breathing</phrase> data, one from the frequency and the other from the time domain. The normalized L2-norm error was computed as 22.9% and the <phrase>correlation coefficient</phrase> was 0.97 (p=0.03). These <phrase>results suggest</phrase> that the p-BRS can be estimated from non-pharmacological indices computed from ECG and invasive BP data related to <phrase>deep breathing</phrase>.
Memory Testing Under Different <phrase>Stress Conditions</phrase>: An Industrial Evaluation This paper presents the effectiveness of various <phrase>stress conditions</phrase> (mainly voltage and frequency) on detecting the <phrase>resistive shorts</phrase> and open defects in <phrase>deep sub-micron</phrase> <phrase>embedded memories</phrase> in an industrial environment. Simulation studies on very-<phrase>low voltage</phrase>, high voltage and at-speed testing show the need of the <phrase>stress conditions</phrase> for <phrase>high quality</phrase> products; i.e., low defect-per-million (DPM) level, which is driving the semiconductor market today.The above test conditions have been validated to screen out bad devices on real silicon (a test-chip) built on CMOS 0.18 um technology.IFA (inductive fault analysis) based simulation technique leads to an efficient fault coverage and DPM estimator, which helps the customers upfront to make decisions on test algorithm implementations under different <phrase>stress conditions</phrase> in order to reduce the number of test escapes.
<phrase>Recurrent Neural Networks</phrase> With Limited Numerical Precision <phrase>Recurrent Neural Networks</phrase> (RNNs) produce state-of-art performance on many <phrase>machine learning</phrase> tasks but their demand on resources in terms of memory and computational power are often high. Therefore, there is a great interest in optimizing the computations performed with these models especially when considering development of specialized <phrase>low-power</phrase> hardware for <phrase>deep networks</phrase>. One way of reducing the computational needs is to limit the numerical precision of the network weights and biases, and this will be addressed for the case of RNNs. We present results from the use of different stochastic and deterministic reduced precision training methods applied to two major RNN types, which are then tested on three datasets. The results show that the stochastic and deterministic ternarization, pow2-ternarization, and exponential quantization methods gave rise to low-precision RNNs that produce similar and even <phrase>higher accuracy</phrase> on certain datasets, therefore providing a path towards training more efficient implementations of RNNs in specialized hardware.
Scan Design and Ac Test We propose a novel <phrase>Design for Testability</phrase> technique to apply two <phrase>pattern tests</phrase> for <phrase>path delay fault testing</phrase>. Due to stringent timing requirements of deep-submicron VLSI chips, <phrase>design-for-test</phrase> schemes have to be tailored for detecting stuck-at as well as <phrase>delay faults</phrase> quickly and efficiently. Existing techniques such as enhanced scan add substantial <phrase>hardware overhead</phrase>, whereas techniques such as scan-shifting or functional justification make the <phrase>test generation</phrase> process complex and produce lower coverage for <phrase>scan based designs</phrase> as compared to non-scan designs. We exploit the characteristics of CMOS circuitry to enable the application of two-<phrase>pattern tests</phrase>. The proposed technique reduces the problem of <phrase>path delay fault testing</phrase> for <phrase>scan based designs</phrase> to that of <phrase>path delay fault testing</phrase> with complete accessibility to the <phrase>combinational logic</phrase>, and has minimal <phrase>area overhead</phrase>. The scheme also provides significant reduction in power during scan operation.
<phrase>SVD</phrase>-Based Ghost Circuitry Detection Ghost circuitry (GC) insertion is the malicious addition of hardware in the specification and/or implementation of an IC by an attacker intending to change circuit functionality. There are numerous GC insertion sources, including <phrase>untrusted</phrase> foundries, synthesis tools and libraries, testing and verification tools, and configuration scripts. Moreover, GC attacks can greatly compromise the <phrase>security and privacy</phrase> of hardware users, either directly or through interaction with pertinent systems, <phrase>application software</phrase>, or with data. GC detection is a particularly <phrase>difficult task</phrase> in modern and pending <phrase>deep submicron technologies</phrase> due to intrinsic manufacturing variability. Here, we provide algebraic and statistical approaches for the detection of ghost circuitry. A singular value decomposition (<phrase>SVD</phrase>)-based technique for gate characteristic recovery is applied to solve a system of equations created using fast and non-destructive measurements of <phrase>leakage power</phrase> and/or delay. This is then combined with statistical constraint manipulation techniques to detect embedded ghost circuitry. The effectiveness of the approach is demonstrated on the ISCAS 85 benchmarks.
Website Identification: Revisiting the Online Consumer Purchasing Intent Research Every business dreams of having committed, loyal and enthusiastic customers. However, many challenges stand in the way. This is especially true of electronic vendors who must not only grapple with the traditional product issues but also technology issues related to competition in the <phrase>digital economy</phrase>. Prior studies in <phrase>information systems</phrase> have emphasized technology and relational factors such as trust and stickiness in their modeling. This study proposes website identification as a concept that can help turn customers into " super customers " and provide lasting and deep relationships between the electronic vendor and the customer leading to creation of economic value. A structural equation model with website identification as a mediating variable was tested using a sample of 406 individuals. The results support the model hypotheses.
Modeling Users' <phrase>Powertrain</phrase> Preferences Leslie Pack Kaelbling Professor Thesis Supervisor Modeling Users' <phrase>Powertrain</phrase> Preferences Our goal is to construct a system that can determine a drivers preferences and goals and perform appropriate actions to aid the driver achieving his goals and improve the quality of his road behavior. Because the recommendation problem could be achieved effectively once we know the driver's intention, in this thesis, we are going to solve the problem to determine the driver's preferences. A <phrase>supervised learning</phrase> approach has already been applied to this problem. However , because the approach locally classify a small interval at a time and is memory-less, the <phrase>supervised learning</phrase> does not perform well on our goal. Instead, we need to introduce new approach which has following characteristics. First, it should consider the entire stream of measurements. Second, it should be tolerant to the environment. Third, it should be able to distinguish various intentions. In this thesis, two different approaches, Bayesian <phrase>hypothesis testing</phrase> and inverse <phrase>reinforcement learning</phrase>, will be used to classify and estimate the user's preferences. Bayesian <phrase>hypothesis testing</phrase> classifies the driver as one of several <phrase>driving types</phrase>. Assuming that the <phrase>probability distributions</phrase> of the features (i.e. average, <phrase>standard deviation</phrase>) for a short period of measurement are different among the <phrase>driving types</phrase>, Bayesian <phrase>hypothesis testing</phrase> classifies the driver as one of <phrase>driving types</phrase> by maintaining a belief distribution for each driving type and updating it online as more measurements are available. On the other hand, inverse <phrase>reinforcement learning</phrase> estimates the users' preferences as a <phrase>linear combination</phrase> of <phrase>driving types</phrase>. The inverse <phrase>reinforcement learning</phrase> approach assumes that the driver maximizes a reward function while driving, and his reward function is a <phrase>linear combination</phrase> of raw / expert features. Based on the observed tra-jectories of representative drivers, <phrase>apprenticeship</phrase> learning first calculates the reward function of each driving type with raw features, and these reward functions serve as expert features. After, with observed trajectories of a new driver, the same algorithm calculates the reward function of him, not with raw features, but with expert features, and estimates the preferences of any driver in a space of <phrase>driving types</phrase>. Acknowledgments I owe my deepest gratitude to Professor Leslie Pack Kaelbling, my academic, UROP, and thesis advisor, for her unending support, guidance, and inspiration, which have enabled me to develop a deep understanding in the field of <phrase>computer science</phrase> and <phrase>machine learning</phrase>. I would also like to thank Professor Tomas Lozano-Perez, for his guidance and insightful comments that enabled me to …
Malacoda: towards <phrase>high-level</phrase> <phrase>compilation</phrase> of <phrase>network security</phrase> applications on reconfigurable hardware While the use of <phrase>reconfigurable computing</phrase> for tasks such as packet header processing or <phrase>deep packet-inspection</phrase> in <phrase>high-speed</phrase> networks has been widely studied, efforts to extend the technology to application-level processing have only recently been made. One issue that has prevented wider use of reconfigurable platforms in that context is the unfamiliar programming environment: Such systems commonly require expertise in computer architecture and digital logic design generally foreign to networking experts. To make the technology more accessible to potential users, we present the high-level <phrase>domain-specific language</phrase> Malacoda for application-level network processing and an associated compiler that automatically translates Malacoda descriptions into <phrase>high-performance</phrase> hardware blocks for insertion into an FPGA-based processing platform. We evaluate our approach on the use-case of a hardware-accelerated secure honeypot-in-a-box, programmed in Malacoda, and implemented on the NetFPGA 10G board. Results from a live-test of the system connected to a 10G Internet uplink complete the evaluation.
<phrase>Reasoning about</phrase> Complexity of <phrase>Object-Oriented Programs</phrase> Modern imperative <phrase>object-oriented design</phrase> methods and languages take a rigorous approach to compatibility and reusability mainly from an interface and speciication <phrase>point of view</phrase> | if at all. Beside functional speciication, however, users select classes from libraries based on performance characteristics, too. This report develops an appropriate fundamental approach towards performance estimation, measurement and metering in OO approaches. We use examples written in the Sather language to demonstrate the concepts of so-called OO-machines, which lend themselves to performance metrics, and a <phrase>calculus</phrase> for <phrase>reasoning about</phrase> performance. A <phrase>language binding</phrase> of these concepts is then sketched in the form of cost annotations that allow programmers to le classes in libraries well-documented with cost related speciica-tions. These annotations can optionally be used for instrumenting code that meters cost and checks whether the taken measurements are consistent with the given speciication. In this way programmers can beneet from cost annotations by means of documentation and rigorous testing without requiring a deep familiarity with the theoretical underpinnings.
<phrase>Deep Linguistic</phrase> Processing for <phrase>Spoken Dialogue</phrase> Systems We describe a framework for <phrase>deep linguistic</phrase> processing for <phrase>natural language understanding</phrase> in task-oriented <phrase>spoken dialogue</phrase> systems. The goal is to create domain-general processing techniques that can be shared across all domains and dialogue tasks, combined with <phrase>domain-specific</phrase> optimization based on an ontology mapping from the generic LF to the application on-tology. This framework has been tested in six domains that involve tasks such as interactive planning, coordination operations, tutoring, and learning.
The impact of aerosols on cloud and precipitation processes: cloud-resolving model simulations 1. INTRODUCTION Aerosols and especially their effect on clouds are one of the key components of the climate system and the <phrase>hydrological</phrase> cycle [Ramanathan et al., 2001]. Yet, the aerosol effect on clouds remains largely unknown and the processes involved not well understood. A recent report published by the <phrase>National Academy of Science</phrase> states "The greatest uncertainty about the aerosol climate forcing-indeed, the largest of all the uncertainties about global climate forcing-is probably the indirect effect of aerosols on clouds [<phrase>NRC</phrase>, 2001]." The aerosol effect on clouds is often <phrase>categorized</phrase> into the traditional "first indirect (i.e., Twomey)" effect on the cloud droplet sizes for a constant liquid water path [Twomey, 1977] and the "semi-direct" effect on cloud coverage [e.g., Ackerman et al., 2000]. Enhanced aerosol concentrations can also suppress warm rain processes by producing a narrow droplet spectrum that inhibits collision and coalescence processes [e. The aerosol effect on precipitation processes, also known as the second type of aerosol indirect effect [Albrecht, 1989], is even more complex, especially for mixed-phase <phrase>convective</phrase> clouds. Table 1 summarizes the key observational studies identifying the microphysical properties, cloud characteristics, <phrase>thermodynamics</phrase> and dynamics associated with cloud systems from high-aerosol continental environments. For example, atmospheric aerosol concentrations can influence cloud droplet size distributions, warm-rain process, cold-rain process, cloud-top height, the depth of the mixed phase region, and occurrence of <phrase>lightning</phrase>. In addition, high aerosol concentrations <phrase>in urban environments</phrase> could affect precipitation variability by providing an enhanced source of <phrase>cloud condensation nuclei</phrase> (CCN). Hypotheses have been developed to explain the effect of urban regions on <phrase>convection</phrase> and precipitation [van den Heever and <phrase>Cotton</phrase>, 2007 and Shepherd, 2005]. Please see Tao et al. (2007) for more detailed description on aerosol impact on precipitation. Recently, a detailed spectral-bin microphysical scheme was implemented into the Goddard <phrase>Cumulus</phrase> Ensemble (<phrase>GCE</phrase>) model. Atmospheric aerosols are also described using <phrase>number density</phrase> size-distribution functions. A spectral-bin microphysical model is very expensive from a computational <phrase>point of view</phrase> and has only been implemented into the 2D <phrase>_____</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase> version of the <phrase>GCE</phrase> at the present time. The model is tested by studying the evolution of deep tropical clouds in the west Pacific warm pool region and summertime <phrase>convection</phrase> over a mid-<phrase>latitude</phrase> <phrase>continent</phrase> with different concentrations of CCN: a low "clean" concentration and a high "dirty" concentration. The impact of atmospheric aerosol concentration on cloud and precipitation will be investigated.
The " Global Drifter Program " Drifter Measurements of Surface Velocity, Sst, Sss, Winds and <phrase>Atmospheric Pressure</phrase> 1. PROJECT SUMMARY 1.1. Rationale The principal scientific questions of the role of the ocean in <phrase>climate change</phrase> are how well can we describe or model the <phrase>ocean circulation</phrase> today and how well can these descriptions or models predict the evolution of future climates. Climate time scale changes in the <phrase>sea surface temperature</phrase> (SST) directly force changes in the <phrase>air temperature</phrase> and <phrase>habitability</phrase> conditions very large parts of the globe. On these interannual time scales SST depends on <phrase>ocean circulation</phrase> as well as air-sea interaction. A global array of drifters provide the operational <phrase>instrumental</phrase> <phrase>data sets</phrase> describing SST and ocean near surface circulation and evolution and these data are used for testing climate models and enhancing <phrase>long-range</phrase> weather prediction and interannual <phrase>climate change</phrase>. Sensors that measure <phrase>sea surface</phrase> salinity (SSS) are now added to drifters and these SSS data are critical to determining the oceans' <phrase>fresh water</phrase> cycle and onset of <phrase>deep-water</phrase> renewals. Air pressures measured on drifters are assimilated into weather prediction models and are used by operational meteorological agencies to discern <phrase>severe weather</phrase> conditions over the oceans. Drifter pressure data also contribute significantly to the calculation of the inverted <phrase>barometer</phrase> effect on global <phrase>sea level rise</phrase> as measured from altimeters. Wind sensor and subsurface temperature chain data are used to improve prediction of <phrase>tropical storms</phrase> and <phrase>hurricanes</phrase>. Drifters designed and built within the " Global Drifter Program " (GDP) have proven to be reliable, autonomous platforms for obtaining climate and operational weather data from the global oceans. 1.2. Objectives of the " Global Drifter Program " The " Global Drifter Program " (GDP) is the principal international component of the <phrase>Joint Commission</phrase> of Marine Measurements (JCOMM) " Global Surface Drifting Buoy Array ". It is a " Scientific Project " of the Data Buoy Cooperation Panel (DBCP) of <phrase>World Meteorological Organization</phrase> (<phrase>WMO</phrase>)/International Ocean Commission (<phrase>IOC</phrase>). It is a near-operational ocean-observing network that, through the <phrase>Argos</phrase> satellite system and the Global <phrase>Telecommunication</phrase> System (GTS), returns real time data on ocean near-surface currents, SST and air pressure (and winds, subsurface temperature-T(z), and SSS) and provides a <phrase>data processing</phrase> system for scientific utilization of these data. In addition to GDP, drifters are deployed by operational <phrase>oceanographic</phrase> and meteorological agencies and individual scientific research projects, whose data are utilized by GDP. In turn, GDP data are made available to operational users and scientists <phrase>at large</phrase>. Wind
System for deep <phrase>venous thrombosis</phrase> detection using objective compression measures A system for objective vessel compression assessment for deep <phrase>venous thrombosis</phrase> characterization using ultrasound <phrase>image data</phrase> and a sensorized ultrasound probe is presented. Two new objective measures calculated from applied force and transverse vessel area are also presented and used to describe vessel compressibility. A modified star-Kalman algorithm is used for feature detection in acquired ultrasound images, and objective measures of vessel compressibility are calculated from the detected features and acquired force and location data from the sensorized probe. A <phrase>three-dimensional shape</phrase> model of the examined vessel that includes compressibility measures mapped as colors to its surface is presented on the <phrase>user interface</phrase>, as well as a virtual representation of the image plane. The compressibility measures were validated using expert segmentation of healthy and diseased vessels and compared using paired t-tests, which showed a significant difference between healthy and diseased cases for both measures. 100% sensitivity and specificity were obtained for both measures. The system was implemented in real-time (16 Hz) and evaluated using a tissue phantom and on healthy human subjects. Sensitivity was 100% and 60%, while specificity was 97% for both measures when implemented. The initial results for the system and its components are promising.
A Test Suite for Inference Involving <phrase>Adjectives</phrase> Recently, most of the research in NLP has concentrated on the creation of applications coping with textual entailment. However, there still exist very few resources for the evaluation of such applications. We argue that the reason for this resides not only in the novelty of the research field but also and mainly in the difficulty of defining the linguistic phenomena which are responsible for inference. As the TSNLP project has shown <phrase>test suites</phrase> provide optimal diagnostic and evaluation tools for NLP applications, as contrary to text corpora they provide a deep insight in the linguistic phenomena allowing control over the data. Thus in this paper, we present a test suite specifically developed for studying inference problems shown by English <phrase>adjectives</phrase>. The construction of the <phrase>test suite</phrase> is based on the <phrase>deep linguistic</phrase> analysis and following classification of entailment patterns of <phrase>adjectives</phrase> and follows the TSNLP guidelines on linguistic databases providing a clear coverage, systematic annotation of inference tasks, large reusability and simple maintenance. With the design of this <phrase>test suite</phrase> we aim at creating a resource supporting the evaluation of computational systems handling <phrase>natural language</phrase> inference and in particular at providing a benchmark against which to evaluate and compare existing semantic analysers.
Effects of link annotations on search performance in layered and unlayered hierarchically organized information spaces The effects of link annotations on user search performance in hypertext environments having deep (layered) and shallow link structures were investigated in this study. Four environments were tested—layered-annotated , layered-unannotated, shallow-annotated, and shallow-unannotated. A single document was divided into 48 sections, and layered and unlayered versions were created. Additional versions were created by adding annotations to the links in the layered and unlayered versions. Subjects were given three queries of varying difficulty and then asked to find the answers to the queries that were contained within the hypertext environment to which they were randomly assigned. Correspondence between the wording links and queries was used to define difficulty level. The results of the study confirmed <phrase>previous research</phrase> that shallow link structures are better than deep (layered) link structures. Annotations had virtually no effect on the search performance of the subjects. The subjects performed similarly in the annotated and unannotated environments, regardless of whether the link structures were shallow or deep. An analysis of question difficulty suggests that the wording in links has primacy over the wording in annotations in influencing user search behavior. Introduction The Internet and the <phrase>World Wide Web</phrase> have grown to the point where they have become a major resource for access-ing information. Web-accessible information is composed largely of hypertext documents. These documents have links to subunits of information within them, or to information in other documents. The earliest hypertext systems predated the <phrase>World Wide Web</phrase>, and one problem with hypertext since its inception has been navigation and way finding (<phrase>Hammond</phrase>, 1988; Kerr, 1986). Because information is presented in interlinked fragments that can be accessed nonlinearly, many users become disoriented (Marchionini & Shneiderman, 1993). The problem becomes magnified when users attempt to find information in a global hypertext environment as vast as the Web. The way information is represented and linked in hypertext can either alleviate or exacerbate the way finding problem. This study builds on <phrase>previous research</phrase> related to information linking and representation by testing the effects of link annotations on user ability to search hierarchically organized hypertext under conditions where the hierarchical structure has intervening link layers and conditions where it does not.
Practical Detection of Spammers and Content Promoters in Online Video Sharing Systems A number of online video sharing systems, out of which YouTube is the most popular, provide features that allow users to post a video as a response to a discussion topic. These features open opportunities for users to introduce polluted content, or simply pollution, into the system. For instance, spammers may post an unrelated video as response to a popular one, aiming at increasing the likelihood of the response being viewed by a larger number of users. Moreover, content promoters may try to gain visibility to a specific video by posting a large number of (potentially unrelated) responses to boost the rank of the responded video, making it appear in the top lists maintained by the system. Content pollution may jeopardize the trust of users on the system, thus compromising its success in promoting <phrase>social interactions</phrase>. In spite of that, the available literature is very limited in providing a deep understanding of this problem. In this paper, we address the issue of detecting video <phrase>spammers and promoters</phrase>. Towards that end, we first manually build a <phrase>test collection</phrase> of real YouTube users, classifying them as spammers, promoters, and legitimate users. Using our <phrase>test collection</phrase>, we provide a characterization of content, individual, and social attributes that help distinguish each user class. We then investigate the feasibility of using supervised classification algorithms to automatically detect <phrase>spammers and promoters</phrase>, and assess their effectiveness in our <phrase>test collection</phrase>. While our classification approach succeeds at separating <phrase>spammers and promoters</phrase> from legitimate users, the high cost of manually labeling vast amounts of examples compromises its full potential in realistic scenarios. For this reason, we further propose an <phrase>active learning</phrase> approach that automatically chooses a set of examples to label, which is likely to provide the highest amount of information, drastically reducing the amount of required <phrase>training data</phrase> while maintaining comparable classification effectiveness.
Licklider transmission protocol (LTP)-based DTN for cislunar communications Delay/disruption-tolerant networking (DTN) technology offers a new solution to highly stressed communications in space environments, especially those with long link delay and frequent link disruptions in deep-<phrase>space missions</phrase>. To date, little work has been done in evaluating the performance of the available "convergence layer" protocols of DTN, especially the Licklider Transmission Protocol (LTP), when they are applied to an interplanetary Internet (<phrase>IPN</phrase>). In this paper, we present an experimental evaluation of the Bundle Protocol (BP) running over various "convergence layer" protocols in a simulated cislunar communications environment characterized by varying degrees of signal <phrase>propagation delay</phrase> and <phrase>data loss</phrase>. We focus on the LTP convergence layer (LTPCL) adapter running on top of UDP/IP (i.e., BP/LTPCL/UDP/IP). The performance of BP/LTPCL/UDP/IP in realistic file transfers over a PC-based network <phrase>test bed</phrase> is compared to that of two other DTN <phrase>protocol stack</phrase> options, BP/TCPCL/<phrase>TCP/IP</phrase> and BP/UDPCL/UDP/IP. A statistical method of <i>t</i>-test is also used for analysis of the experimental results. The <phrase>experiment results</phrase> showthat LTPCL has a significant performance advantage over <phrase>Transmission Control Protocol</phrase> convergence layer (TCPCL) for link delays longer than 4000 ms regardless of the <phrase>bit error rate</phrase> (BER). For a very <phrase>lossy</phrase> channel with a BER of around 10<sup>-5</sup>, LTPCL has a significant goodput advantage over TCPCL at all the link delay levels studied, with an advantage of around 3000 B/s for delays longer than 1500 ms. LTPCL has a consistently significant goodput advantage over UDPCL, around 2500-3000 B/s, at all levels of link delays and BERs.
Neural mechanisms for <phrase>voice recognition</phrase> We investigated neural mechanisms that support <phrase>voice recognition</phrase> in a training paradigm with fMRI. The same listeners were trained on different weeks to categorize the mid-regions of voice-morph continua as an individual's voice. Stimuli implicitly defined a voice-acoustics space, and training explicitly defined a voice-identity space. The <phrase>pre-defined</phrase> centre of the voice category was shifted from the acoustic centre each week in opposite directions, so the same stimuli had different training histories on different tests. Cortical sensitivity to voice similarity appeared over different time-scales and at different representational stages. First, there were <phrase>short-term</phrase> adaptation effects: increasing acoustic similarity to the directly preceding stimulus led to haemodynamic response reduction in the middle/posterior STS and in right ventrolateral <phrase>prefrontal</phrase> regions. Second, there were longer-term effects: response reduction was found in the orbital/<phrase>insular cortex</phrase> for stimuli that were most versus least similar to the acoustic mean of all preceding stimuli, and, in the anterior temporal pole, the deep posterior STS and the <phrase>amygdala</phrase>, for stimuli that were most versus least similar to the trained voice-identity category mean. These findings are interpreted as effects of neural sharpening of <phrase>long-term</phrase> stored typical acoustic and category-internal values. The analyses also reveal anatomically separable voice representations: one in a voice-acoustics space and one in a voice-identity space. Voice-identity representations flexibly followed the trained identity shift, and listeners with a greater identity effect were more accurate at recognizing familiar voices. <phrase>Voice recognition</phrase> is thus supported by neural voice spaces that are organized around flexible 'mean voice' representations.
<phrase>Test-Quality</phrase>/Cost Optimization Using Output-Deviation-Based Reordering of <phrase>Test Patterns</phrase> —At-speed <phrase>functional testing</phrase>, <phrase>delay testing</phrase>, and n-detection <phrase>test sets</phrase> are being used today to detect deep submi-crometer defects. However, the resulting <phrase>test data</phrase> volumes are too high; the 2005 International <phrase>Roadmap for Semiconductors</phrase> predicts that <phrase>test-application</phrase> times will be 30 times larger in 2010 than they are today. In addition, many new types of defects cannot be accurately modeled using existing <phrase>fault models</phrase>. Therefore, there is a need to model the quality of <phrase>test patterns</phrase> such that they can be quickly assessed for defect screening. Test selection is required to choose the most effective pattern sequences from large <phrase>test sets</phrase>. Current industry practice for test selection is based on fault grading, which is computationally expensive and must also be repeated for every <phrase>fault model</phrase>. Moreover, although <phrase>efficient methods</phrase> exist today, for fault-oriented <phrase>test generation</phrase>, there is a lack of understanding on how best to combine the <phrase>test sets</phrase> thus obtained, i.e., derive the most effective <phrase>union</phrase> of the individual <phrase>test sets</phrase> without simply taking all the patterns for each <phrase>fault model</phrase>. This paper presents the use of the output deviation as a surrogate coverage-metric for pattern modeling and test grading. A flexible, but general, probabilistic-<phrase>fault model</phrase> is used to generate a probability map for the circuit, which can subsequently be used for <phrase>test-pattern</phrase> reordering. The output deviations resulting from the probability map(s) are used as a coverage-metric to model <phrase>test patterns</phrase>; the higher the deviation, the better the quality of the <phrase>test pattern</phrase>. We show that, for the ISCAS <phrase>benchmark circuits</phrase> and as compared to other reordering methods, the proposed method provides " steeper " coverage curves for different <phrase>fault models</phrase>.
ImageNet Classification with Deep <phrase>Convolutional Neural Networks</phrase> We trained a large, <phrase>deep convolutional neural network</phrase> to classify the 1.2 million <phrase>high-resolution</phrase> images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 <phrase>error rates</phrase> of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by <phrase>max-pooling</phrase> layers, and three <phrase>fully-connected</phrase> layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the <phrase>fully-connected</phrase> layers we employed a <phrase>recently-developed</phrase> regularization <phrase>method called</phrase> " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test <phrase>error rate</phrase> of 15.3%, compared to 26.2% achieved by the second-best entry.
A U T H O R S <phrase>Shopping Orientations</phrase>, Product Types and Internet Shopping Intentions Background and Research Questions A b s t r a c t <phrase>Shopping orientations</phrase> are useful in the study of <phrase>patronage</phrase> behaviour including store loyalty, <phrase>brand</phrase> loyalty, in-<phrase>home shopping</phrase>, and out-shopping. This paper describes an empirical study that examined the relationship between <phrase>shopping orientations</phrase>, product types, and consumer intentions to use the Internet for shopping. Analyses of data collected from over 750 survey respondents reveal that home, economic, and local <phrase>shopping orientations</phrase> are related to <phrase>online shopping</phrase> intentions. Product types, based on cost and tangibility, do not have a moderating influence on the relationship between <phrase>shopping orientations</phrase> and intentions to <phrase>shop</phrase> using the Internet, but do have a direct effect on the latter. And, the incremental contribution of demographic indicators in predicting <phrase>online shopping</phrase> intentions is minimal. Implications of the findings and the association between <phrase>shopping orientations</phrase> and the more easily ascertainable demographic indicators are discussed. His research examines the adoption, use, and consquences of systems that facilitate <phrase>electronic commerce</phrase>. INTRODUCTION Although the grandiose predictions for business-to-consumer Internet commerce are yet to be realized, the continued success of a few online merchants such as <phrase>Amazon</phrase> and <phrase>eBay</phrase> offer evidence of the virtual medium's potential as a <phrase>retail</phrase> channel. Therefore, in spite of the rapid decline in the fortunes of dot-com companies that has justifiably triggered deep scepticism about its future, dismissing the digital channel as a fad may be premature. Haste to stake a claim on the digital frontier may be largely to blame for the recent spate of failures among online commercial ventures. It is not surprising that ill-conceived <phrase>business models</phrase> that bypassed careful scrutiny from overly eager investors are proving to be unsustainable. However, rashness alone may not explain the rapid exit of many online <phrase>businesses</phrase>. Even companies that have proven their viability with respectable sales are discovering that there are some major roadblocks to harnessing the power of the Internet to serve as a channel for communications, transactions and distribution. Managing the unpredictability of technological glitches, solving the logistical <phrase>puzzle</phrase> of order fulfilment and delivery, allaying consumer fears about <phrase>security and privacy</phrase> breaches, and building and nurturing trust are all major problems facing online retailers. Even more of a challenge is identifying, attracting and retaining consumers who would embrace the new medium as a shopping channel. Research to address the above issues in electronic retailing is becoming increasingly sophisticated. Moving beyond demographics, researchers are building and testing more complex models to …
Climate Quality Broadband and Narrowband Solar Reflected <phrase>Radiance</phrase> Calibration Between Sensors in Orbit As the potential impacts of <phrase>global climate change</phrase> become more clear [1], the need to determine the accuracy of climate prediction over decade-to-century time scales has become an urgent and critical challenge. The most critical tests of <phrase>climate model</phrase> predictions will occur using observations of decadal changes in climate forcing, response, and feedback variables. Many of these key climate variables are observed by remotely sensing the global distribution of reflected solar spectral and broadband <phrase>radiance</phrase>. These "reflected solar" variables include aerosols, clouds, radiative fluxes, <phrase>snow</phrase>, ice, <phrase>vegetation</phrase>, ocean color, and land cover. Achieving sufficient satellite instrument accuracy, stability, and overlap to rigorously observe decadal change signals has proven very difficult in most cases and has not yet been achieved in others [2]. One of the earliest efforts to make climate quality observations was for Earth Radiation Budget: Nimbus 6/7 in the late 1970s, ERBE in the 1980s/90s, and CERES in 2000s are examples of the most complete global records. The recent CERES data products have carried out the most extensive intercomparisons because if the need to merge data from up to 11 instruments (CERES, <phrase>MODIS</phrase>, <phrase>geostationary</phrase> imagers) on 7 spacecraft (Terra, Aqua, and 5 <phrase>geostationary</phrase>) for any given month. In order to achieve climate calibration for cloud feedbacks, the radiative effect of clear-sky, all-sky, and cloud radiative effect must all be made with very high stability and accuracy. For <phrase>shortwave</phrase> solar reflected flux, even the 1% CERES broadband absolute accuracy (1-σ confidence bound) is not sufficient to allow gaps in the radiation record for decadal <phrase>climate change</phrase>. Typical absolute accuracy for the best narrowband sensors like <phrase>SeaWiFS</phrase>, MISR, and <phrase>MODIS</phrase> range from 2 to 4% (1-σ). <phrase>IPCC</phrase> <phrase>greenhouse gas</phrase> <phrase>radiative forcing</phrase> is ~ 0.6 Wm-2 per decade or 0.6% of the global mean <phrase>shortwave</phrase> reflected flux, so that a 50% cloud feedback would change the global reflected flux by ~ 0.3 Wm-2 or 0.3% per decade in broadband SW calibration change. Recent results comparing CERES reflected flux changes with <phrase>MODIS</phrase>, MISR, and <phrase>SeaWiFS</phrase> narrowband changes concluded that only <phrase>SeaWiFS</phrase> and CERES were approaching sufficient stability in calibration for decadal <phrase>climate change</phrase> [3]. Results using deep <phrase>convective</phrase> clouds in the optically thick limit as a stability target may prove very effective for improving past <phrase>data sets</phrase> like ISCCP. Results for intercalibration of <phrase>geostationary</phrase> imagers to CERES using an entire month of regional nearly coincident data demonstrates new approaches to constraining the …
Estimating Depth of Anesthesia from <phrase>Eeg Signals</phrase> Using <phrase>Wavelet Transform</phrase> Electroencephalogram (EEG) is the brain signal containing valuable information about the conscious and unconscious states of the brain, which may provide a useful tool to measure depth of anesthesia. However, raw <phrase>EEG signals</phrase> received in various states of consciousness cannot be distinguished visually. In this paper an approach is presented to find out difference between <phrase>EEG signals</phrase> in fully awake and in <phrase>deep sleep</phrase> conditions with respect to the coefficients of <phrase>wavelet transform</phrase>. Continuous <phrase>wavelet transform</phrase> of the raw EEG signal obtained at different conscious state of a human subject have been performed. Statistical analyses were then performed on coefficient values to determine the differences between the sleep state and the awake state. From statistical t-test analysis significant difference of the two state of consciousness was found.
Wishart Deep Stacking Network for Fast <phrase>POLSAR Image Classification</phrase> Inspired by the popular <phrase>deep learning</phrase> architecture - Deep Stacking Network (DSN), a specific deep model for polarimetric <phrase>synthetic aperture radar</phrase> (POLSAR) <phrase>image classification</phrase> is proposed in this paper, which is named as Wishart Deep Stacking Network (W-DSN). First of all, a fast implementation of Wishart distance is achieved by a special <phrase>linear transformation</phrase>, which speeds up the classification of <phrase>POLSAR image</phrase> and makes it possible to use this polarimetric information in the following <phrase>Neural Network</phrase> (NN). Then a single-<phrase>hidden-layer</phrase> <phrase>neural network</phrase> based on the fast Wishart distance is defined for <phrase>POLSAR image classification</phrase>, which is named as Wishart Network (WN) and improves the <phrase>classification accuracy</phrase>. Finally, a <phrase>multi-layer</phrase> <phrase>neural network</phrase> is formed by stacking WNs, which is in fact the proposed <phrase>deep learning</phrase> architecture W-DSN for <phrase>POLSAR image classification</phrase> and improves the <phrase>classification accuracy</phrase> further. In addition, the structure of WN can be expanded in a straightforward way by adding <phrase>hidden units</phrase> if necessary, as well as the structure of the W-DSN. As a preliminary exploration on formulating specific <phrase>deep learning</phrase> architecture for <phrase>POLSAR image classification</phrase>, the proposed methods may establish a simple but clever connection between <phrase>POLSAR image</phrase> interpretation and <phrase>deep learning</phrase>. The <phrase>experiment results</phrase> tested on real <phrase>POLSAR image</phrase> show that the fast implementation of Wishart distance is very efficient (a <phrase>POLSAR image</phrase> with 768000 pixels can be classified in 0.53s), and both the single-<phrase>hidden-layer</phrase> architecture WN and the <phrase>deep learning</phrase> architecture W-DSN for <phrase>POLSAR image classification</phrase> perform well and work efficiently.
Diagnosis for <phrase>scan-based</phrase> BIST: reaching deep into the signatures For partitioning-based diagnosis in a <phrase>scan-based</phrase> BIST environment, an exact analysis scheme, capable of identifying all <phrase>scan cells</phrase> that receive incorrect data, is proposed. In contrast to previously suggested approaches, the scheme we propose identifies all failing <phrase>scan cells</phrase> with no ambiguity whatsoever. Not only do we resolve failing <phrase>scan cells</phrase> unambiguously, but we do so at the earliest possible instance through <phrase>reexamination</phrase> of already computed signatures. Intensive utilization of this highly precise diagnostic state information leads to prognostic information regarding the usefulness of running upcoming tests which in turn leads to reductions in diagnosis time in excess of 30% compared to previous approaches.
Characterizing Pedophile Conversations on the Internet using Online Grooming —Cyber-<phrase>crime</phrase> targeting children such as online pe-dophile activity are a major and a growing concern to the society. A deep understanding of <phrase>predatory</phrase> chat conversations on the Internet has implications in designing effective solutions to automatically identify malicious conversations from regular conversations. We believe that a deeper understanding of the pedophile conversation can result in more sophisticated and robust <phrase>surveillance systems</phrase> than majority of the current systems relying only on shallow processing such as simple word-counting or keyword spotting. In this paper, we study pedophile conversations from the perspective of online grooming theory and perform a series of linguistic-based empirical analysis on several pedophile chat conversations to gain useful insights and patterns. We manually annotated 75 pedophile chat conversations with six stages of online grooming and test several hypothesis on it. The results of our experiments reveal that relationship forming is the most dominant online grooming stage in contrast to the sexual stage. We use a widely used word-counting program (LIWC) to create psycho-linguistic profiles for each of the six online grooming stages to discover interesting textual patterns useful to improve our understanding of the online pedophile phenomenon. Furthermore , we present empirical results that throw light on various aspects of a pedophile conversation such as probability of state transitions from one stage to another, distribution of a pedophile chat conversation across various online grooming stages and correlations between <phrase>pre-defined</phrase> word categories and online grooming stages.
A <phrase>Path Planning</phrase> and <phrase>Obstacle Avoidance</phrase> Algorithm for an Autonomous Robotic Vehicle Abstract Sharayu Yogesh Ghangrekar. a <phrase>Path Planning</phrase> and <phrase>Obstacle Avoidance</phrase> <phrase>Path planning</phrase> in robotics is concerned with developing the logic for navigation of a robot. <phrase>Path planning</phrase> still has a long way to go considering its deep impact on any robot's functionality. Various <phrase>path planning</phrase> techniques have been tried and tested earlier, including probabilistic, integral and genetic approaches. The <phrase>implementation details</phrase> of most of these algorithms are proprietary to specific organizations. The requirement of a customized strategy for collision free and concerted navigation of an <phrase>All-Terrain Vehicle</phrase> (ATV) led to the activities of this research. As a part of this research an algorithm has been developed and simulated to give a visual effect. The algorithm presented is evolutionary and capable of <phrase>path planning</phrase> for <phrase>ATVs</phrase> in the presence of completely known and newly-discovered obstacles. This algorithm helps the ATV to maneuver in an open field in a specific pattern and avoid the obstacles, if any, along its path. As part of the research the actual algorithm is implemented and simulated using C and WINAPI. As a result, given the data of known obstacles and the field, the ATV can maneuver in a systematic and optimum manner towards its goal by avoiding all the obstacles in its path. This algorithm can also be deployed on an ATV using real time data from LIDAR and GPS. The logic of the algorithm can be extended for <phrase>path planning</phrase> in a completely dynamic environment. iv ACKNOWLEDGMENTS
<phrase>LI-BIST</phrase>: A Low-Cost Self-Test Scheme for SoC Logic Cores and Interconnects For system-on-chips (SoC) using <phrase>deep submicron</phrase> (DSM) technologies, interconnects are becoming critical determinants for performance, reliability and power. Buses and long interconnects being susceptible to <phrase>crosstalk noise</phrase>, may lead to functional and <phrase>timing failures</phrase>. Existing at-speed interconnect crosstalk <phrase>test methods</phrase> propose inserting dedicated interconnect self-<phrase>test structures</phrase> in the SoC to generate vectors which have high <phrase>crosstalk defect coverage</phrase>. However, these methods may have a prohibitively high <phrase>area overhead</phrase>. To reduce this overhead, existing <phrase>logic BIST</phrase> structures like LFSRs could be reused to deliver interconnect tests. But, as shown by our experiments, use of LFSR tests achieve poor <phrase>crosstalk defect coverage</phrase>. Additionally, it has been shown that the power consumed during testing can potentially become a significant concern. In this paper, we present Logic-Interconnect BIST (<phrase>LI-BIST</phrase>), a comprehensive self-test solution for both the logic of the cores and the SoC interconnects. <phrase>LI-BIST</phrase> reuses existing <phrase>logic BIST</phrase> structures but generates <phrase>high-quality</phrase> tests for interconnect <phrase>crosstalk defects</phrase>, while minimizing the <phrase>area overhead</phrase> and interconnect <phrase>power consumption</phrase>. The application of the <phrase>LI-BIST</phrase> methodology on example SoCs indicates that <phrase>LI-BIST</phrase> is a viable, <phrase>low-cost</phrase>, yet comprehensive solution for testing SoCs.
Fast <phrase>Regular Expression</phrase> Matching Using Small TCAMs for Network <phrase>Intrusion Detection</phrase> and Prevention Systems <phrase>Regular expression</phrase> (RE) matching is a core component of <phrase>deep packet inspection</phrase> in modern networking and security devices. In this paper, we propose the first <phrase>hardware-based</phrase> RE matching approach that uses <phrase>Ternary</phrase> Content Addressable Memories (TCAMs), which are off-the-shelf chips and have been widely deployed in modern networking devices for packet classification. We propose three novel techniques to reduce TCAM space and improve RE matching speed: transition sharing, table consolidation, and variable striding. We tested our techniques on 8 <phrase>real-world</phrase> RE sets, and our results show that small TCAMs can be used to store large DFAs and achieve potentially high RE matching throughtput. For space, we were able to store each of the corresponding 8 DFAs with as many as 25,000 states in a 0.59Mb TCAM chip where the number of TCAM bits required per <phrase>DFA</phrase> a different TCAM encoding scheme that facilitates processing multiple characters per transition, we were able to achieve potential RE matching throughputs of between 10 and 19 Gbps for each of the 8 DFAs using only a single 2.36 Mb TCAM chip.
A theory of defocus via <phrase>Fourier analysis</phrase> In this paper we present a novel theory to analyze defo-cused images of a volume density by exploiting well-known results in <phrase>Fourier analysis</phrase> and the singular value decomposition. This analysis is fundamental in two respects: First, it gives a deep insight into the basic mechanisms of image formation of defocused images, and second, it shows how to incorporate additional a-priori knowledge about the geometry and photometry of the scene in <phrase>restoration</phrase> algorithms. For instance, we show that the case of a scene made of a single surface results in a simple constraint in the Fourier domain. We derive two basic types of algorithms for vol-umetric reconstruction: One based on a dense set of defo-cused images, and one based on a sparse set of defocused images. While the first one excels in simplicity, the second one is of more practical use. Both algorithms are tested on real and synthetic data.
A hybrid finite <phrase>automaton</phrase> for practical <phrase>deep packet inspection</phrase> Deterministic <phrase>finite automata</phrase> (DFAs) are widely used to perform <phrase>regular expression</phrase> matching in linear time. Several techniques have been proposed to compress DFAs in order to reduce memory requirements. Unfortunately, many <phrase>real-world</phrase> IDS <phrase>regular expressions</phrase> include complex terms that result in an exponential increase in number of <phrase>DFA</phrase> states. Since all recent proposals use an initial <phrase>DFA</phrase> as a <phrase>starting-point</phrase>, they cannot be used as comprehensive <phrase>regular expression</phrase> representations in an IDS.  In this work we propose a hybrid <phrase>automaton</phrase> which addresses this issue by combining the benefits of deterministic and <phrase>non-deterministic</phrase> <phrase>finite automata</phrase>. We test our proposal on Snort rule-sets and we validate it on real traffic traces. Finally, we address and analyze the worst case behavior of our scheme and compare it to traditional ones.
Phototaxic Foraging of the Archaepaddler, a Hypothetical <phrase>Deep-Sea</phrase> Species An <phrase>autonomous agent</phrase> (animat, hypothetical animal), called the (archae) paddler, is simulated in sufficient detail to regard its simulated <phrase>aquatic</phrase> locomotion (paddling) as physically possible. The paddler is supposed to be a model of an animal that might exist, although it is perfectly possible to view it as a model of a robot that might be built. The agent is assumed to navigate in a simulated <phrase>deep-sea</phrase> environment, where it forages for autoluminescent prey. It uses a biologically inspired phototaxic foraging strategy, while paddling in a layer just above the bottom. The advantage of this living space is that the navigation problem--and hence our model--is essentially two-dimensional. Moreover, the deep-sea environment is physically simple (and hence easy to simulate): no significant currents, constant temperature, completely dark. A foraging performance metric is developed that circumvents the necessity to solve the traveling <phrase>salesman problem</phrase>. A parametric simulation study then quantifies the influence of habitat factors, such as the density of prey, and body geometry (e.g., placement, direction and directional selectivity of the eyes) on foraging success. Adequate performance proves to require a specific body geometry adapted to the habitat characteristics. In general, performance degrades gracefully for modest changes of the geometric and habitat parameters, indicating that we work in a stable region of "design space." The parameters have to strike a compromise between, on the one hand, to "see" as many targets at the same time as possible. One important conclusion is that simple reflex-based navigation can be surprisingly efficient. Additionally, performance in a global task (foraging) depends strongly on local parameters such as visual direction tuning, position of the eyes and paddles, and so forth. Behavior and habitat "<phrase>mold</phrase>" the body, and the body geometry strongly influences performance. The resulting platform enables further testing of foraging strategies or vision and locomotion theories stemming either from biology or from robotics.
Self-X RAN: Autonomous self organizing radio access networks Current situation for radio access <phrase>network management</phrase> Deployment and maintenance become more and more complex and cost extensive Trend to smaller cells, multi-band operation, heterogeneous <phrase>mobile networks</phrase> High manual intervention for configuration, capacity upgrade or in failure cases required High effort required for optimisation of system performance Deep system expertise required High effort necessary for measurement campaigns (drive tests) Different tools for planning, configuration, measurement/KPI acquisition and optimisation involved increasing effort for <phrase>network management</phrase> and optimisation new concepts for simplified network operation required
Comparing <phrase>Bacterial Communities</phrase> Inferred from <phrase>16s Rrna Gene</phrase> Sequencing and <phrase>Shotgun</phrase> <phrase>Metagenomics</phrase> <phrase>16S rRNA gene</phrase> sequencing has been widely used for probing the species structure of a variety of environmental <phrase>bacterial communities</phrase>. Alternatively, <phrase>16S rRNA gene</phrase> fragments can be retrieved from <phrase>shotgun</phrase> metagenomic sequences (metagenomes) and used for species profiling. Both approaches have their limitations-<phrase>16S rRNA</phrase> sequencing may be biased because of unequal amplification of species' <phrase>16S rRNA</phrase> genes, whereas <phrase>shotgun</phrase> metagenomic sequencing may not be deep enough to detect the <phrase>16S rRNA</phrase> genes of rare species in a community. However, <phrase>previous studies</phrase> showed that these two approaches give largely similar species profiles for a few <phrase>bacterial communities</phrase>. To investigate this problem in greater detail, we conducted a systematic comparison of these two approaches. We developed PHYLOSHOP, a pipeline that predicts <phrase>16S rRNA gene</phrase> fragments in metagenomes, reports the taxonomic assignment of these fragments, and visualizes their taxonomy distribution. Using PHYLOSHOP, we analyzed 33 metagenomic datasets of human-associated <phrase>bacterial communities</phrase>, and compared the bacterial community structures derived from these metagenomic datasets with the <phrase>community structure</phrase> derived from <phrase>16S rRNA gene</phrase> sequencing (71 datasets). Based on several <phrase>statistical tests</phrase> (including a statistical test proposed here that takes into consideration differences in <phrase>sample size</phrase>), we observed that these two approaches give significantly different community structures for nearly all the <phrase>bacterial communities</phrase> collected from different locations on and in <phrase>human body</phrase>, and that these differences cannot be be explained by differences in <phrase>sample size</phrase> and are likely to be attributed by experimental method.
Improving the Accuracy of RF <phrase>Alternate Test</phrase> Using Multi-VDD Conditions: Application to Envelope-Based Test of LNAs —This work demonstrates that multi-VDD conditions may be used to improve the accuracy of <phrase>machine learning</phrase> models , significantly decreasing the prediction error. The proposed technique has been <phrase>successfully applied</phrase> to a previous <phrase>alternate test</phrase> strategy for LNAs based on response envelope detection. A prototype has been developed to show its feasibility. The prototype consists of a low-power 2.4GHz LNA and a simple envelope detector, integrated in a 90nm <phrase>CMOS technology</phrase>. Post-layout <phrase>simulation results</phrase> are provided to verify the functionality of the approach. I. INTRODUCTION Nowadays, advances in RF <phrase>CMOS technologies</phrase> have enabled the integration of complete transceivers in a single chip, which provides a significant reduction in production cost. However there is a simultaneous increase in the cost of testing and diagnosing these devices. Their diverse specifications and high <phrase>operating frequency</phrase>, as well as the large impact of <phrase>process variations</phrase> in current <phrase>deep submicron technologies</phrase>, make necessary <phrase>extensive tests</phrase> and dedicated <phrase>high-frequency</phrase> <phrase>test equipment</phrase>. RF testing exhibits the same difficulties present in analog testing, but adding the problem of handling <phrase>high-frequency</phrase> signals. That is, RF testing is based on functional characterization, while <phrase>fault-model</phrase>-based tests, very successful in the digital test domain, are difficult to standardize in the RF field since each circuit type demands its own custom <phrase>fault model</phrase>. Reducing RF test complexity and cost is still an open research topic that has been addressed in a number of different approaches. Recent work in this area includes defect modeling and failure diagnosis [1], [2], <phrase>alternate test</phrase> [2]–[5], DfT and <phrase>BIST techniques</phrase> [6]–[8], etc. In particular, the combination of <phrase>BIST techniques</phrase> with the <phrase>statistical analysis</phrase> of <phrase>alternate test</phrase> seems to be a promising solution to mitigate most RF test drawbacks. On one hand, moving some of the testing functions to the device under test (DUT) would reduce <phrase>test equipment</phrase> cost, and eliminate the problem of transporting <phrase>high-frequency</phrase> test signals. On the other hand, <phrase>alternate test</phrase> strategies take advantage of advanced statistical tools to find correlations between a reduced number of <phrase>observables</phrase> (signatures), and the diverse DUT specifications, thus reducing the number of necessary test measurements and configurations.
A Logic for Vagueness In Dummett's important paper [1] on the <phrase>sorites paradox</phrase> it is suggested that the vagueness of observational predicates such as '.. .is red' or more obviously '.. .looks red' generates an apparent incoherence: their use resembles a game governed by inconsistent rules. A similar incoherence is seen by Wright [18, 17] as a real and serious threat to very ordinary ideas of how language works. Wright argues not that the use of vague predicates is incoherent but that it would be if the use of language were a practice in which the admissibility of moves were determined by rules whose general properties are discoverable by <phrase>appeal</phrase> to non-behavioural notions. But unless we do move from anecdotes about behaviour to just such rules, how are we to reason at all? The incoherence in question is an outcome of the vagueness or tolerance of observational locutions, and would seem if established for them to spread to non-observational vague expressions like '.. .is water' or '.. .is a <phrase>test tube</phrase>', 1 thus vitiating almost all of our attempts to use language consistently, even in science. On the face of it, vagueness is everywhere, whence such deep-seated incoherence would upset even such fragile understanding of semantics as we have gleaned from a century's work. The argument connecting vagueness to incoherence, therefore, strikes at the heart of logic: every philosophical logi-cian is called upon to respond to it. The <phrase>sorites paradox</phrase>, the " <phrase>slippery slope</phrase> argument " or the " <phrase>paradox</phrase> of the heap " , is old and famous and wears an air of sophistry. One feels that the problem will vanish on exposure of the trivial trick involved. What is surprising is that it is so deep and difficult after all. An example or two will help focus the discussion. 1 How long and thin must a piece of glassware be, or how polluted may a liquid be, before it no longer counts as a <phrase>test tube</phrase> or as water?
Complete IOCO <phrase>test cases</phrase>: a case study <phrase>Input/Output</phrase> Transition Systems (IOTSs) have been widely used as test models in <phrase>model-based testing</phrase>. Traditionally, <phrase>input output</phrase> conformance testing (IOCO) has been used to generate random <phrase>test cases</phrase> from IOTSs. A recent <phrase>test case</phrase> generation method for IOTSs, called Complete IOCO, applies <phrase>fault models</phrase> to obtain complete <phrase>test suites</phrase> with guaranteed <phrase>fault coverage</phrase> for IOTSs. This paper measures the efficiency of Complete IOCO in comparison with the traditional IOCO <phrase>test case</phrase> generation implemented in the JTorX tool. To this end, we use a case study involving five specification models from the automotive and the <phrase>railway</phrase> domains. Faulty <phrase>mutations</phrase> of the specifications were produced in order to compare the efficiency of both <phrase>test generation</phrase> methods in killing them. The results indicate that Complete IOCO is more efficient in detecting deep faults in large <phrase>state spaces</phrase> while IOCO is more efficient in detecting shallow faults in small <phrase>state spaces</phrase>.
Conference Paper Ggscrs: Ggnmos Triggered Silicon Controlled Rectifiers for <phrase>Esd Protection</phrase> in <phrase>Deep Sub- Micron Cmos</phrase> Processes Ggscrs: Ggnmos Triggered Silicon Controlled Rectifiers for <phrase>Esd Protection</phrase> in Deep Sub-micron <phrase>Cmos Processes</phrase> <phrase>EOS</phrase>/ESD symposium 2001 In this paper, design aspects, operation, protection capability and applications of SCRs in deep submicron CMOS are addressed. A novel Grounded‐Gate Nmos Triggered SCR device (GGSCR) is introduced and compared to the LVTSCR. Experimental verification, including endurance testing, demonstrates that GGSCRs can fulfill all <phrase>ESD protection</phrase> requirements for today's IC applications in different 0.25um, 0.18um and 0.13um <phrase>CMOS processes</phrase>. Abstract – In this paper, design aspects, operation, protection capability and applications of SCRs in <phrase>deep sub-micron CMOS</phrase> are addressed. A novel Grounded-Gate Nmos Triggered SCR device (GGSCR) is introduced and compared to the LVTSCR. Experimental verification, including endurance testing, demonstrates that GGSCRs can fulfill all <phrase>ESD protection</phrase> requirements for todays IC applications in different 0.25um, 0.18um and 0.13um <phrase>CMOS processes</phrase>.
A Reflective Framework for Configurable Workflow Processes and Tools 1 Abstract This paper describes some key issues in implementing a reflective <phrase>object-oriented</phrase> framework. We use the framework to build applications in administrative environments. These applications share a common <phrase>business model</phrase>, and require a mix of database, <phrase>document management</phrase> and workflow functionality. So far, we have focused our development efforts on the central administration of schools in the <phrase>Flemish Community</phrase> of <phrase>Belgium</phrase>, with increasing emphasis on access to the central applications from within the schools and local boards through the Internet. In addition we implemented a few <phrase>test cases</phrase> to verify the validity of our approach. We rewrote a rather large application of the <phrase>Belgian</phrase> <phrase>Police</phrase> Department in a few weeks; this includes training the developer. The framework uses a repository to store meta-information about <phrase>end-user</phrase> applications. This includes object model, object behavior, constraints, specifications of application environments, query screens, layout definitions of overview lists and forms, <phrase>authorization</phrase> rules, workflow process templates and event-condition-action rules. Fully operational <phrase>end-user</phrase> and <phrase>development tools</phrase> consult this meta-information at <phrase>run-time</phrase>, and adapt themselves dynamically to the application specifications in the database. Thus we separate specifications of a particular organization's <phrase>business model</phrase> from the generic functionality of the tools. Rather than coding or generating code, we develop <phrase>end-user</phrase> applications and most of the <phrase>development tools</phrase> themselves by building increasingly complete specifications of the <phrase>business model</phrase>. These specifications are available for immediate execution. One of the main objectives of the framework is a high-degree of <phrase>end-user</phrase> configurability. Configuration by <phrase>end-users</phrase> is often just skin-deep. In our case it involves all aspects of <phrase>end-user</phrase> application development: knowledgeable users adapt the business rules and workflow processes, whereas regular <phrase>end-users</phrase> adapt the form and overview list layouts and query screens to their own needs. The business rules ensure consistency in all cases, because their specifications are de-coupled from the application functionality. Users are becoming increasingly aware that change is a constant factor and that applications are never truly finished. Thus we give them the necessary tools to build and configure their applications. <phrase>End-user</phrase> application developers are also users of the system, with there own requirements. These may change over time, or depend on the kind of applications they are building. Given the generic functionality of the <phrase>end-user</phrase> tools and the configuration options, another major goal of our framework consists of replacing as many dedicated <phrase>development tools</phrase> as possible by applications configured in the system itself. Thus any enhancement of …
How much can <phrase>part-of-speech tagging</phrase> help parsing? <phrase>Folk</phrase> wisdom holds that incorporating a part-of-speech tagger into a system that performs <phrase>deep linguistic</phrase> analysis will improve the speed and accuracy of the system. <phrase>Previous studies</phrase> of tagging have tested this belief by incorporating an existing tagger into a parsing system and observing the effect on the speed of the parser and accuracy of the results. However, not much work has been done to determine in a <phrase>fine-grained</phrase> manner exactly how much tagging can help to disambiguate or reduce ambiguity in parser output. We take a new approach to this issue by examining the full parse-forest output of a large-scale LFG-based <phrase>English grammar</phrase> (Riezler et al., 2002) running on the XLE grammar development platform (Maxwell and Kaplan, 1993, 1996), and partitioning the parse outputs into <phrase>equivalence</phrase> classes based on the tag sequences for each parse. If we find a large number of tag-sequence <phrase>equivalence</phrase> classes for each sentence, we can conclude that different parses tend to be distinguished by their tags; a small number means that tagging would not help much in reducing ambiguity. In this way, we can determine how much tagging would help us in the best case, if we had the " perfect tagger " to give us the correct tag sequence for each sentence. We show that if a perfect tagger were available, a reduction in ambiguity of about 50% would be available. Somewhat surprisingly, about 30% of the sentences in the corpus that was examined would not be disambiguated, even by the perfect tagger, since all of the parses for these sentences shared the same tag sequence. Our study also helps to inform research on tagging by providing a targeted determination of exactly which tags can help the most in disambiguation.
Tiled <phrase>convolutional neural networks</phrase> <phrase>deep belief networks</phrase> for scalable unsupervised learning of hierarchical representations. In ICML, 2009. What is the best multi-stage architecture for <phrase>object recognition</phrase>? In ICCV, 2009. [4] A. Hyvarinen and P. Hoyer. Topographic <phrase>independent component analysis</phrase> as a model of V1 organization and receptive fields. Algorithm <phrase>Convolutional neural networks</phrase> [1] work well for many recognition tasks:-Local receptive fields for computational reasons-Weight sharing gives translational invariance However, weight sharing can be restrictive because it prevents us from learning other kinds of invariances. Abstract <phrase>Convolutional neural networks</phrase> (CNNs) have been <phrase>successfully applied</phrase> to many tasks such as digit and <phrase>object recognition</phrase>. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled <phrase>convolutional neural networks</phrase> (Tiled CNNs), which use a regular " tiled " pattern of tied weights that does not require that adjacent <phrase>hidden units</phrase> share identical weights, but instead requires only that <phrase>hidden units</phrase> k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs' advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex <phrase>invariant features</phrase> allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets. TICA <phrase>network architecture</phrase> Evaluating benefits of convolutional training Training on 8x8 samples and using these weights in a Tiled CNN obtains only 51.54% on the test set compared to 58.66% using our <phrase>proposed method</phrase>. Networks learn concepts like edge detectors, corner detectors Invariant to translation, rotation and scaling • Algorithms for pretraining <phrase>convolutional neural networks</phrase> [2,3] do not use untied weights to learn invariances. • TICA can be used to pretrain Tiled CNNs because it can learn invariances even when trained only on <phrase>unlabeled data</phrase> [4, 5]. Tiled CNNs are more flexible and usually better than fully <phrase>convolutional neural networks</phrase>. Pretraining with TICA finds invariant and discriminative features and works well with finetuning.
Hierarchical <phrase>Extreme Learning</phrase> Machine for unsupervised representation learning —Learning representations from massive unlabelled data is a topic for <phrase>high-level</phrase> tasks in many applications. The recent great improvements on benchmark <phrase>data sets</phrase>, which are achieved by increasingly complex <phrase>unsupervised learning</phrase> methods and <phrase>deep learning</phrase> models with many parameters, usually requiring many tedious tricks and much expertise to tune. Additionally, the filters learned by these complex architectures are quite similar to standard hand-crafted visual features, and training to fine-tune the weights of <phrase>deep architectures</phrase> requires a long time. In this paper, the <phrase>Extreme Learning</phrase> Machine-Auto Encoder (ELM-AE) is employed as the learning unit to learn local receptive fields at each layer, and the lower layer responses are transferred to the last layer (trans-layer) to form a more complete representation to retain more information. In addition, some beneficial methods in <phrase>deep learning</phrase> architectures such as local contrast normalization and whitening are added to the implemented hierarchical <phrase>Extreme Learning</phrase> Machine networks to further boost the performance. The resulting trans-layer representations are processed into block histograms with binary <phrase>hashing</phrase> to produce translation and <phrase>rotation invariant</phrase> representations, which are utilized to do <phrase>high-level</phrase> tasks such as recognition and detection. The proposed trans-layer representation method with ELM-AE based learning of local receptive filters was tested on the MNIST digit recognition <phrase>data set</phrase>, including MNIST variations, and on the <phrase>Caltech</phrase> 101 <phrase>object recognition</phrase> database. Compared to traditional <phrase>deep learning</phrase> methods, the proposed ELM-AE based system has a much faster learning speed and attains 65.97% accuracy on the <phrase>Caltech</phrase> 101 task (15 samples per class) and 99.45% on the standard MNIST <phrase>data set</phrase>.
<phrase>Laminar</phrase> patterns of local <phrase>excitatory</phrase> input to <phrase>layer 5 neurons</phrase> in <phrase>macaque</phrase> <phrase>primary visual cortex</phrase>. <phrase>Layer 5 neurons</phrase> in <phrase>primary visual cortex</phrase> make putative reciprocal feedback connections to the superficial layers. To test this hypothesis, we employed scanning laser photostimulation combined with intracellular <phrase>dye</phrase> injection to examine local functional <phrase>excitatory</phrase> inputs to and axonal projections from individual <phrase>layer 5 neurons</phrase> in brain slices from <phrase>monkey</phrase> V1. In contrast with <phrase>previous studies</phrase> of other V1 neurons, <phrase>layer 5 neurons</phrase> received significant input from nearly all of the cortical layers, suggesting individual layer 5 cells integrate information from a broad range of input sources. Nevertheless relative strengths of <phrase>laminar</phrase> inputs varied across neurons. <phrase>Cluster analysis</phrase> of relative strength of <phrase>laminar</phrase> inputs to individual <phrase>layer 5 neurons</phrase> revealed four discrete clusters representing recurring input patterns; each cluster included both <phrase>excitatory</phrase> and inhibitory neurons. Twenty-five of 40 <phrase>layer 5 neurons</phrase> <phrase>fell</phrase> into two clusters, both characterized by very strong input from superficial layers. These input patterns are consistent with <phrase>layer 5 neurons</phrase> providing feedback to superficial layers. The remaining 15 neurons received stronger input from deep layers. Differences in input from layer 4Calpha versus 4Cbeta also suggest specific associations of the magnocellular and parvocellular visual pathways, with populations receiving stronger input from deep versus superficial cortical layers.
Convolutional-Recursive <phrase>Deep Learning</phrase> for 3D <phrase>Object Classification</phrase> Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve <phrase>object recognition</phrase>. Most current methods rely on very well-designed features for this new 3D modality. We introduce a model based on a combination of convolutional and recursive <phrase>neural networks</phrase> (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns <phrase>low-level</phrase> translationally <phrase>invariant features</phrase> which are then given as inputs to multiple, fixed-tree RNNs in order to compose <phrase>higher order</phrase> features. RNNs can be seen as combining convolution and pooling into one efficient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during <phrase>training and testing</phrase> than comparable architectures such as two-layer CNNs.
Fifteen compilers in fifteen days <phrase>Traditional approaches</phrase> to semester-long projects in compiler courses force students to implement the <phrase>early stages</phrase> of a compiler in depth; since many students fall behind, they have little opportunity to implement the back end. Consequently, students have a deep knowledge of early material and no knowledge of latter material. We propose an approach based on incremental development and <phrase>test-driven development</phrase>; this approach solves the emphasis problem, provides experience with useful tools, and allows for such a course to be taught in a three or four weeks.
On "deep" knowledge extraction from documents SYNDIKATE comprises a family of <phrase>natural language understanding</phrase> systems for automatically acquiring knowledge from <phrase>real-world</phrase> texts (e.g., <phrase>information technology</phrase> test reports, medical finding reports), and for transferring their content to formal representation structures which constitute a corresponding text <phrase>knowledge base</phrase>. We present a general system architecture which integrates requirements from the analysis of single sentences, as well as those of referentially linked sentences forming <phrase>cohesive</phrase> texts. Properly <phrase>accounting</phrase> for text cohesion phenomena is a prerequisite for the soundness and validity of the generated text representation structures. It is also crucial for any information system application making use of <phrase>automatically generated</phrase> text <phrase>knowledge bases</phrase> in a reliable way.
Analytical models for crosstalk excitation and propagation in <phrase>VLSI circuits</phrase> We develop a general methodology to analyze <phrase>crosstalk effects</phrase> that are likely to cause errors in <phrase>deep submicron</phrase> <phrase>high speed</phrase> circuits. We focus on crosstalk due to <phrase>capacitive coupling</phrase> between a pair of lines. <phrase>Closed form</phrase> equations are derived that quantify the severity of these effects and describe qualitatively the dependence of these effects on the values of <phrase>circuit parameters</phrase>, the rise/fall times of the input transitions, and the skew between the transitions. For noise propagation, we present a new way for predicting the output waveform produced by an inverter due to a non-<phrase>square wave</phrase> pulse at its input. To expedite the computation of the response of a <phrase>logic gate</phrase> to an input pulse, we have developed a novel way of modeling such gates by an equivalent inverter. The results of our analysis provide conditions that must be satisfied by a sequence of vectors used for validation of designs as well as post-manufacturing testing of devices in the presence of significant crosstalk. We present data to demonstrate accuracy of our results, including example runs of a test generator that uses these results.
A Multi-Mode Power Gating Structure for <phrase>Low-Voltage</phrase> <phrase>Deep-Submicron</phrase> <phrase>CMOS ICs</phrase> —Most existing power gating structures provide only one <phrase>power-saving</phrase> mode. We propose a novel power gating structure that supports both a cutoff mode and an intermediate <phrase>power-saving</phrase> and data-retaining mode. Experiments with <phrase>test structures</phrase> fabricated in 0.13-m CMOS bulk technology show that our power gating structure yields an expanded design space with more power-performance tradeoff alternatives.
A Semantic-modal View on Ramsey's Test We present a <phrase>semantic analysis</phrase> of the Ramsey test, pointing out its deep underlying flaw: the tension between the " static " nature of AGM revision (which was originally tailored for revision of only purely <phrase>ontic</phrase> beliefs, and can be applied to <phrase>higher-order</phrase> beliefs only if given a " backwards-looking " interpretation) and the fact that, semantically speaking, any Ramsey conditional must be a modal operator (more precisely, a dynamic-<phrase>epistemic</phrase> one). Thus, a belief about a Ramsey conditional is in fact a <phrase>higher-order</phrase> belief, hence the AGM revision postulates are not applicable to it, except in their " backwards-looking " interpretation. But that interpretation is consistent only with a restricted (weak) version of Ramsey's test (in-applicable to already revised theories). The solution out of the conundrum is twofold: either accept only the weak Ramsey test; or replace the AGM revision operator * by a truly " dynamic " revision operator ⊗, which will not satisfy the AGM <phrase>axioms</phrase>, but will do something better: it will " keep up with reality " , correctly describing revision with <phrase>higher-order</phrase> beliefs.
Countermeasures Against <phrase>High-Order</phrase> <phrase>Fault-Injection</phrase> Attacks on <phrase>CRT</phrase>-<phrase>RSA</phrase> In this paper we study the existing <phrase>CRT</phrase>-<phrase>RSA</phrase> countermeasures against <phrase>fault-injection</phrase> attacks. In an attempt to classify them we get to achieve deep understanding of how they work. We show that the many countermeasures that we study (and their variations) actually share a number of common features, but optimize them in different ways. We also show that there is no conceptual distinction between test-based and infective countermeasures and how either one can be transformed into the other. Furthermore, we show that faults on the code (skipping instructions) can be captured by considering only faults on the data. These intermediate results allow us to improve the state of the art in several ways: (a) we fix an existing and that was known to be broken <phrase>countermeasure</phrase> (namely the one from Shamir); (b) we drastically optimize an existing <phrase>countermeasure</phrase> (namely the one from Vigilant) which we reduce to 3 tests instead of 9 in its original version, and prove that it resists not only one fault but also an arbitrary number of randomizing faults; (c) we also show how to upgrade countermeasures to resist any given number of faults: given a correct first-order <phrase>countermeasure</phrase>, we present a way to design a prov-able <phrase>high-order</phrase> <phrase>countermeasure</phrase> (for a well-defined and reasonable <phrase>fault model</phrase>). Finally, we pave the way for a generic approach against <phrase>fault attacks</phrase> for any <phrase>modular arithmetic</phrase> computations, and thus for the automatic insertion of countermeasures.
A Methodology for Deep Sub-0.25 M <phrase>Cmos Technology</phrase> Prediction —We present a novel methodology for characterization of sub-quartermicron <phrase>CMOS technologies</phrase>. It involves process calibration, device calibration employing two-dimensional device simulation and automated Technology <phrase>Computer Aided Design</phrase> (TCAD) optimization and, finally, transient mixed-mode de-vice/<phrase>circuit simulation</phrase>. The proposed methodology was tested on 0.25 m technology and applied to 0.13 m technology in order to estimate <phrase>ring oscillator</phrase> speed. The simulation results show an excellent agreement with available <phrase>experimental data</phrase>.
<phrase>Speech Synthesis</phrase> for Portable Devices A Text-To-Speech (TTS) <phrase>synthesiser</phrase> is a computer-based system that should be able to read any text aloud, whether it was directly introduced in the computer by an operator or scanned and submitted to an <phrase>Optical Character Recognition</phrase> (<phrase>OCR</phrase>) system. This project presents the design and implementation of a speech <phrase>synthesiser</phrase> for portable devices. The TTS implementation was developed for J2ME. This development involved the conversion of a desktop TTS program by <phrase>Sun</phrase> Microsystems Inc. called FreeTTS. The implementation also involved the partial development of Java Sound API's for J2ME on <phrase>Windows CE</phrase>. The TTS implementation on a <phrase>PDA</phrase>, apart from being platform independent produces the same sound quality with a far less powerful processor than its desktop counterparts. The implementation is written for J2ME, and is also compatible with JDK 1.3 on desktop computers. The program has been tested on a Dell <phrase>Axim</phrase> <phrase>X5</phrase> <phrase>PDA</phrase> running <phrase>Windows CE</phrase>, and has been tested on a wide range of desktop computers and <phrase>operating systems</phrase> including: <phrase>Windows 2000</phrase>, <phrase>Windows XP</phrase>, <phrase>FreeBSD</phrase> and Linux. i Acknowledgements " ...very little do we have and enclose which we can call our own in the deep sense of the word. We all have to accept and learn, either from our predecessors or from our contemporaries. Even the greatest genius would not have achieved much if he had wished to extract everything from inside himself. But there are many good people, who do not understand this, and spend half their lives wondering in darkness with their dreams of originality. I have known artists who were proud of not having followed any teacher and of owing everything only to their own genius. Such fools! " [<phrase>Goethe</phrase>, Conversations with Eckermann, 17 Feb 1832] First of all, I would like to thank all of the staff in DIT Kevin Street, in particular Dr. Fred Mtenzi, the project supervisor for all the guidance he provided throughout this work. I appreciate the input of the <phrase>computer science</phrase> classes, FT228, FT225 and DT226 for their continued discussion, support and assisting with <phrase>software testing</phrase>. Thanks to NSIcom who issued a license for the use of their J2ME <phrase>virtual machine</phrase> in this project, and also for answering questions relating to the hardware and processing limitations of portable devices. Great thanks are expressed to both <phrase>Donald Knuth</phrase> [31], the author of T E X, and also to <phrase>Leslie Lamport</phrase> [34] for writing L A T E X, …
Advanced <phrase>code coverage</phrase> analysis using <phrase>substring</phrase> holes <phrase>Code coverage</phrase> is a common aid in the <phrase>testing process</phrase>. It is generally used for marking the <phrase>source code</phrase> segments that were executed and, more importantly, those that were not executed.  Many <phrase>code coverage</phrase> tools exist, supporting a variety of languages and <phrase>operating systems</phrase>. Unfortunately, these tools provide little or no assistance when <phrase>code coverage</phrase> data is voluminous. Such quantities are typical of system tests and even for earlier testing phases. Drill-down capabilities that look at different granularities of the data, starting with directories and going through files to functions and lines of <phrase>source code</phrase>, are insufficient. Such capabilities make the assumption that the coverage issues themselves follow the code hierarchy. We argue that this is not the case for much of the uncovered code. Two notable examples are error handling code and platform-specific constructs. Both tend to be spread throughout the source in many files, even though the related coverage, or lack thereof, is highly dependent.  To make the task more manageable, and therefore more likely to be performed by users, we developed a hole analysis algorithm and tool that is based on common substrings in the names of functions. We tested its effectiveness using two large IBM software systems. In both of them, we asked domain experts to <phrase>judge</phrase> the results of several hole-ranking heuristics. They found that 57% - 87% of the 30 top-ranked holes identified by the effective heuristics are relevant. Moreover, these holes are often unexpected. This is especially impressive because <phrase>substring</phrase> hole analysis relies only on the names of functions, whereas domain experts have a broad and deep understanding of the system.  We grounded our results in a theoretical framework that states desirable mathematical properties of hole ranking heuristics. The empirical results show that heuristics with these properties tend to perform better, and do so more consistently, than heuristics lacking them.
International Journal of <phrase>Computer Science</phrase> and <phrase>Mobile Computing</phrase> <phrase>High Speed</phrase> <phrase>Fsm-based</phrase> Programmable Memory Built-in Self-test (mbist) Controller — This paper proposed a <phrase>High speed</phrase> <phrase>FSM-based</phrase> controller for programmable memory built-in self-test for testing memory devices. This technique is popular because of its flexibility of new test algorithms. The architecture of controller is designed to implement a new test algorithm has less number of operations and this algorithm emphasis testing of <phrase>high density</phrase> memory ICs either faulty or good .The components of controller is studied and designed using Verilog <phrase>HDL</phrase>. The analysis of the timing, logic area usage and speed are presented. I. INTRODUCTION Memories are the most universal component today almost all system chips contain some type of <phrase>embedded memory</phrase>, such as ROM, SRAM, <phrase>DRAM</phrase>, and <phrase>flash memory</phrase>. In the embedded domain, embedded <phrase>RAMs</phrase> of the StrongArmSA110 occupy 90% of the total area. The projection is, by 2014, memory will represent more than 94% of the chip area in average SOC environment, according to the <phrase>International Technology Roadmap for Semiconductors</phrase> 2007[7] the percentage of chip area occupied by memories in a design and the increasing trend predicted for the next decade, with the advent of deep-submicron VLSI technology, the memory density and capacity is growing but the clock frequency is never higher. The dominant use of embedded <phrase>memory cores</phrase> along with emerging new architectures and technologies make providing a low <phrase>area overhead</phrase> and <phrase>high speed</phrase> test solution for these on-chip memories a very challenging task. Built-in self-test (BIST) [5] has been proven to be one of the most <phrase>cost-effective</phrase> and widely used solutions for memory testing because the tests can run at circuit speed to yield a more realistic test time, no external <phrase>test equipment</phrase>, reduced development efforts and on-chip <phrase>test pattern generation</phrase> to provide higher controllability and observability. There are several <phrase>FSM-based</phrase> controllers proposed in [1-10].In <phrase>FSM-based</phrase> <phrase>memory BIST</phrase> controller, counters are the key component especially in <phrase>FSM-based</phrase> <phrase>memory BIST</phrase> controller but some <phrase>FSM-based</phrase> <phrase>BIST controller</phrase> [2] excluded counter from its design. This type of architecture has optimum <phrase>area overhead</phrase> however less flexible to allow any changes in the test algorithm. Usually, different counters [3], [4] are used to generate the address, <phrase>test data</phrase> and read/write sequences. Two types of <phrase>FSM-based</phrase> <phrase>BIST controller</phrase> architectures are proposed in [5]. Both are designed by using a counter for the test <phrase>pattern generator</phrase> and test controller but one is using MISR which is a part of the <phrase>BIST controller</phrase> block for output response analyzer (ORA) while another one is …
On-line leakage-aware energy minimization scheduling for hard real-time systems As the <phrase>semiconductor technology</phrase> proceeds into the deep sub-micron era, leakage and its dependency with the temperature become critical in dealing with the power/energy minimization problem. In this paper, we develop an analytical method to estimate energy consumption on-line with the leakage/temperature dependency taken into consideration. Based on this method, we develop an on-line <phrase>scheduling algorithm</phrase> to reduce the overall energy consumption for a hard real-time system scheduled according to the Earliest Deadline First (<phrase>EDF</phrase>) policy. Our experimental results show that the proposed energy estimation method can achieve up to 210X speedup compared with an existing approach while still maintaining <phrase>high accuracy</phrase>. In addition, with a large number of different <phrase>test cases</phrase>, the proposed energy saving scheduling method consistently outperforms two <phrase>closely related</phrase> researches in average by 10% and 14% respectively.
Exploring continued <phrase>online service</phrase> usage behavior: The roles of <phrase>self-image congruity</phrase> and regret The expectation–confirmation model (ECM) of continued <phrase>information systems</phrase> (IS) use has proven to be successful across <phrase>online service</phrase> contexts. <phrase>Previous studies</phrase> based on ECM have focused on a <phrase>referent</phrase> (i.e., comparison standard) that is centered on the target IS (i.e., target <phrase>online service</phrase>). The effect of this ref-erent, captured through confirmation, has been strongly demonstrated. Yet, few studies have explored the saliency of two additional reference effects, captured through <phrase>self-image congruity</phrase> and regret, in <phrase>online service</phrase> continuance. To fill this knowledge gap, this paper attempts to develop a research model that extends the ECM perspective in view of the additional contributions of regret and self-image congru-ity on two post-adoption beliefs (perceived usefulness and perceived enjoyment) and continuance intention. For this extension, we synthesized the extant literature on continued IS use, <phrase>self-image congruity</phrase>, and regret. The model was empirically tested within the context of a <phrase>social network service</phrase>. Our analysis result shows that <phrase>self-image congruity</phrase> plays a key role in forming the two post-adoption beliefs. It is also found that the absolute effect of regret on continuance intention is larger than the effects of other antecedents identified in IS. Overall, this study preliminarily confirms the salience of <phrase>self-image congruity</phrase> and regret in <phrase>online service</phrase> continuance. Although initial use is an important measure of <phrase>online service</phrase> success, it does not necessarily result in the desired managerial performance unless the use continues (Bhattacherjee, 2001b). Specifically , how to promote continued <phrase>online service</phrase> usage or, alternatively , how to prevent discontinuance is a critical issue for online <phrase>service providers</phrase> to consider (Parthasarathy & Bhattacherjee, 1998). Deep down, <phrase>online service</phrase> managers know that achieving strong and sustained customers is crucial. Therefore, <phrase>research into</phrase> this <phrase>online service</phrase> continuance has recently emerged as an <phrase>important issue</phrase> in the IS literature Individuals' <phrase>information systems</phrase> (IS) continuous usage decisions are congruent with consumers' repeat purchase decisions. The expectancy-confirmation paradigm has been strongly confirmed across a wide range of product repurchase and service con-tinuance contexts (e. from the paradigm, Bhattacherjee (2001b) developed the expecta-tion–confirmation model (ECM) of continued IS use. The model explicitly focuses on a user's psychological motivations that emerge after initial adoption of IS. Furthermore, it has proven to be successful across consumer-oriented <phrase>online service</phrase> contexts What are the distinguishing characteristics that influence people to continue IS usage in a consistent fashion? The original ECM hypothesizes that an individual's intention to continue IS usage depends on three variables: …
A Computationally Efficient Stereo Algorithm for Adaptive <phrase>Cruise Control</phrase> Vision a Computationally Efficient <phrase>Stereo Vision</phrase> Algorithm for Adaptive <phrase>Cruise Control</phrase> A major obstacle in the application of <phrase>stereo vision</phrase> to <phrase>intelligent transportation systems</phrase> is high computational cost. In this thesis, we present an edge based, subpixel stereo algorithm which is adapted to permit accurate distance measurements to objects in the field of view in real-time using a compact camera assembly. Once computed , the 3-D scene information may be directly applied to a number of in-vehicle applications, such as adaptive <phrase>cruise control</phrase>, <phrase>obstacle detection</phrase>, and lane tracking. On-road applications, such as vehicle counting and incident detection, are also possible. A PC based three-camera <phrase>stereo vision</phrase> system, constructed with off-the-shelf components, was built to serve as a tool for testing and refining algorithms which approach real-time performance. In-vehicle road trial results are presented and discussed. Acknowledgments I wish to thank Professor Berthold <phrase>Horn</phrase> for agreeing to supervise this thesis, and for generously sharing his expertise in <phrase>machine vision</phrase> over the course of the <phrase>research project</phrase>. I also extend my deep gratitude to Dr. Ichiro Masaki for proposing this graduate work, and for his continued support and advice as the project evolved. The students of the <phrase>Artificial Intelligence</phrase> Lab provided lots of technical advice and equally abundant friendship. I wish to extend special thanks to Marcelo Mizuki for his unbounded generosity in helping out at various phases of the project. He safely piloted the test vehicle during numerous road trials while I provided a steady stream of distractions from the passenger's seat. Thanks to Gideon Stein for helping me get started with this project, and for a number of stimulating discussions. Many thanks The staff members in the AI Lab were also very helpful. I'd especially like to thank Inglese for helping me to navigate the <phrase>labyrinth</phrase> of purchasing, and Ron Wiken for his help with miscellaneous supplies and for giving me a crash course on the <phrase>milling machine</phrase>. I'd like to thank Eric <phrase>Hopkins</phrase> at Metrovideo in <phrase>Wellesley</phrase>, MA, for his assistance in technical matters regarding video and <phrase>image processing</phrase> hardware. Kudos to the staff at Imaging Technology Inc. in <phrase>Bedford</phrase>, MA, for their patient technical support. Above all, I thank my family for their love and support in all that I have done, from the beginning.
Voice Based Biometric Security System Project Details Acknowledgment We express our gratitude and deep-felt thanks towards our esteemed guide, Dr. Sudip Sanyal for his able guidance, which enabled us to complete our project. We are extremely grateful to Prof. ASR Murthy for providing us with valuable suggestions and feedback. We take this opportunity to thank our colleagues who provided us with valuable suggestions, and also <phrase>lent</phrase> their voices for analysis and testing during the course of the project, thus resulting in the successful completion. AIM: Develop a Biometric Security System, which used the <phrase>human voice</phrase> as a distinguishing feature between various users.
An Effective Scheduler for IP Routers At the Communications and <phrase>Telematics</phrase> Laboratory of the University of <phrase>Coimbra</phrase> is being developed a router prototype with the aim to provide QoS to different traffic classes. One of the most important mechanisms of this router is the IP packet scheduler. It is well known that the common scheduling discipline of current routers (first come first serve) turns them useless when QoS is needed-a different type of scheduler must be used. Our first idea to overcome this problem was to use a simple, open, and available scheduler, easy to adapt to the system we wanted to implement. We thought of the WFQ discipline, and, as we are using a <phrase>testbed</phrase> of Intel machines running <phrase>FreeBSD</phrase> OS, we admitted that the WFQ/ALTQ implementation would be an interesting choice. Nevertheless, a broad set of tests carried out at our laboratory proved the contrary. Most important, these tests guided us to a deep knowledge about the problems, and causes, that can weaken the effectiveness of IP schedulers. Given the importance of that surplus information, we decided to implement our own scheduler. The idea was to take advantage of a most pragmatic view of scheduling activities to construct a scheduler with the best possible characteristics, but also very simple, thus, able to reach very good performance levels. This paper presents the scheduler that resulted from our attempts. The proposed scheduler was subject to a set of tests that proved its ability to effectively differentiate traffic classes. The results of these tests are also presented and analyzed.
Cicerobot, a Cognitive Robot for Museum <phrase>Tours</phrase> The paper describes CiceRobot, a robot based on a <phrase>cognitive architecture</phrase> for robot vision and action. The aim of the architecture is to integrate <phrase>visual perception</phrase> and actions with <phrase>knowledge representation</phrase>, in order to let the robot to generate a deep inner understanding of its environment. The principled integration of perception, action and of symbolic knowledge is based on the introduction of an intermediate representation based on Gärdenfors conceptual spaces. The architecture has been tested on a RWI B21 <phrase>autonomous robot</phrase> on tasks related with guided <phrase>tours</phrase> in the <phrase>Archaeological</phrase> Museum of <phrase>Agrigento</phrase>. <phrase>Experimental results</phrase> are presented.
Faster parameterized algorithms for <phrase>minor containment</phrase> The theory of Graph Minors by Robertson and Seymour is one of the deepest and significant theories in modern Combinatorics. This theory has also a strong impact on the recent development of Algorithms , and several areas, like Parameterized Complexity, have roots in Graph Minors. Until very recently it was a common belief that Graph Minors Theory is mainly of theoretical importance. However, it appears that many deep results from Robertson and Seymour's theory can be also used in the design of practical algorithms. <phrase>Minor containment</phrase> testing is one of algorithmically most important and technical parts of the theory, and minor containment in graphs of bounded branchwidth is a basic ingredient of this algorithm. In order to implement <phrase>minor containment</phrase> testing on graphs of bounded branchwidth, Hicks [NETWORKS 04] described an algorithm, that in time O(3 k 2 · (h + k − 1)! · m) decides if a graph G with m edges and branchwidth k, contains a fixed graph H on h vertices as a minor. That algorithm follows the ideas introduced by Robertson and Seymour in [J'CTSB 95]. In this work we improve the dependence on k of Hicks' result by showing that checking if H is a minor of G can be done in time O(2 (2k+1)·log k · h 2k · 2 2h 2 · m). Our approach is based on a combinatorial object called rooted packing, which captures the properties of the potential models of subgraphs of H that we seek in our <phrase>dynamic programming</phrase> algorithm. This formulation with rooted packings allows us to speed up the algorithm when G is embedded in a fixed surface, obtaining the first single-exponential algorithm for <phrase>minor containment</phrase> testing. Namely, it runs in time 2 O(k) · h 2k · 2 O(h) · n, with n = |V (G)|. Finally, we show that slight modifications of our algorithm permit to solve some related problems within the same time bounds, like induced minor or contraction <phrase>minor containment</phrase>.
Learning through inquiry: student difficulties with online course-based Material This study investigates the case-based learning experience of 133 undergraduate <phrase>veterinarian</phrase> science students. Using qualitative methodologies from relational Student Learning Research , variation in the quality of the learning experience was identified, ranging from coherent , deep, quality experiences of the cases, to experiences that separated significant aspects, such as the online <phrase>case histories</phrase>, laboratory <phrase>test results</phrase>, and annotated images emphasizing symptoms, from the meaning of the experience. A key outcome of this study was that a significant percentage of the students surveyed adopted a poor approach to learning with online resources in a blended experience even when their overall learning experience was related to <phrase>cohesive</phrase> conceptions of <phrase>veterinary</phrase> science, and that the difference was even more marked for less successful students. The outcomes from the study suggest that many students are unsure of how to approach the use of online resources in ways that are likely to maximise benefits for learning in blended experiences, and that the benefits from case-based learning such as authenticity and <phrase>active learning</phrase> can be threatened if issues closely associated with qualitative variation arising from incoherence in the experience are not addressed.
Hidden Conditional Neural Fields for Continuous Phoneme <phrase>Speech Recognition</phrase> SUMMARY In this paper, we propose Hidden Conditional Neural Fields (HCNF) for continuous phoneme <phrase>speech recognition</phrase>, which are a combination of Hidden Conditional Random Fields (HCRF) and a <phrase>Multi-Layer</phrase> <phrase>Perceptron</phrase> (MLP), and inherit their merits, namely, the discrimina-tive property for sequences from HCRF and the ability to extract non-linear features from an MLP. HCNF can incorporate many types of features from which non-linear features can be extracted, and is trained by sequential criteria. We first present the formulation of HCNF and then examine three methods to further improve <phrase>automatic speech recognition</phrase> using HCNF, which is an <phrase>objective function</phrase> that explicitly considers training errors, provides a hierarchical <phrase>tandem</phrase>-style feature and includes a deep non-linear feature extractor for the observation function. We show that HCNF can be trained realistically without any initial model and outperforms HCRF and the triphone <phrase>hidden Markov model</phrase> trained by the minimum phone error (MPE) manner using <phrase>experimental results</phrase> for continuous English phoneme recognition on the TIMIT core <phrase>test set</phrase> and Japanese phoneme recognition on the <phrase>IPA</phrase> 100 <phrase>test set</phrase>.
Czech <phrase>Syntactic Analysis</phrase> <phrase>Constraint-based</phrase> - XDG: One Possible Start This article describes an attempt to implement a <phrase>constraint-based</phrase> <phrase>dependency grammar</phrase> for Czech, a language with rich morphology and free <phrase>word order</phrase>, in the formalism Extensible <phrase>Dependency Grammar</phrase> (XDG). The grammar rules are automatically inferred from the <phrase>Prague</phrase> Dependency Treebank (PDT) and constrain dependency relations, modification frames and <phrase>word order</phrase>, including non-projectivity. Although these simple constraints are adequate from the linguistic <phrase>point of view</phrase>, their combination is still too weak and allows an exponential number of solutions for a sentence of n words. 1 Motivation Current approaches to <phrase>syntactic analysis</phrase> of Czech and other languages with a high degree of free <phrase>word order</phrase> have limitations that are important from the theoretical <phrase>point of view</phrase>. First, all the available parsers are restricted to the surface <phrase>syntactic analysis</phrase> and there is no simple way of extending them to include a <phrase>deep syntactic</phrase> (for instance tectogrammatical) level of representation. Second, the available statistical parsers produce only one solution for a given sentence, ignoring the possibility of the syntactic ambiguity of a sentence. And last but not least, the available parsers 1 are by nature statistical and do not contribute to the explanation of syntactic phenomena very much. Several declarative (relational) approaches to syntax analysis overcoming these problems are available , including well known formalisms such as HPSG or LFG, or the robust <phrase>constraint-based</phrase> dependency parsing by (Foth, Menzel, and <phrase>Schröder</phrase>, 2004). Another promising formalism is the Extensible <phrase>Dependency Grammar</phrase> 2 (XDG, (Debusmann et al., 2004)). None of these approaches has ever been tested on a language with rich morphology and freer <phrase>word order</phrase> in a large scale. <phrase>Dependency Grammar</phrase> is a formalism that excellently fits our needs: • XDG is dependency based, as FGD is. • XDG distinguishes between immediate dominance (<phrase>ID</phrase>, dependency) relations and linear precedence (LP); constraints are allowed to speak about these two dimensions separately as well as simultaneously and the dimensions are mutually constraining each other. It is easy to handle non-<phrase>projective</phrase> constructions in XDG. Both these issues are important with respect to the relatively free <phrase>word order</phrase> of Czech. • XDG allows for handling such dimensions of language description as the <phrase>deep syntactic</phrase> (tec-togrammatical) level. FGD's main objective is deep <phrase>syntactic structure</phrase>. 1 Rare exceptions include an unpublished parser for Czech by Zdeněk Žabokrtský. 2 The term grammar is used here in the sense of a set of rules underlying syntactic or syntactico-<phrase>semantic analysis</phrase>.
Need For Undergraduate And Graduate-Level Education In Testing Of Microelectronic Circuits And Systems As <phrase>deep-sub-micron</phrase> and beyond technology emerges, <phrase>quality assurance</phrase> of microelectronic circuits and systems becomes more important than ever. Consequentially, (1) a strong need for well-educated microelectronic circuits and systems test engineers is desired by the industry, (2) graduate-level research efforts are also called to overcome numerous micro-electronic circuits and systems test issues. This paper is to address issues related to increasing impact of the electronic circuits and systems test field on education in electrical and computer engineering and to propose suitable educational topics for undergraduate and graduate-level electrical and computer engineering courses.
<phrase>Deep architectures</phrase> for protein contact map prediction MOTIVATION Residue-residue <phrase>contact prediction</phrase> is important for <phrase>protein structure prediction</phrase> and other applications. However, the accuracy of current contact predictors often barely exceeds 20% on <phrase>long-range</phrase> contacts, falling short of the level required for <phrase>ab initio</phrase> structure prediction.   RESULTS Here, we develop a novel <phrase>machine learning</phrase> approach for contact map prediction using three steps of increasing resolution. First, we use 2D recursive <phrase>neural networks</phrase> to predict coarse contacts and orientations between <phrase>secondary structure</phrase> elements. Second, we use an energy-<phrase>based method</phrase> to align <phrase>secondary structure</phrase> elements and predict contact probabilities between residues in contacting <phrase>alpha-helices</phrase> or strands. Third, we use a deep neural network architecture to organize and progressively refine the prediction of contacts, integrating information over both space and time. We train the architecture on a large set of non-redundant proteins and test it on a large set of non-homologous domains, as well as on the set of <phrase>protein domains</phrase> used for <phrase>contact prediction</phrase> in the two most recent CASP8 and CASP9 experiments. For <phrase>long-range</phrase> contacts, the accuracy of the new CMAPpro predictor is close to 30%, a significant increase over existing approaches.   AVAILABILITY CMAPpro is available as part of the SCRATCH suite at http://scratch.proteomics.ics.uci.edu/.   CONTACT pfbaldi@uci.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at <phrase>Bioinformatics</phrase> online.
<phrase>High-Performance</phrase> Traffic Workload Architecture for Testing DPI Systems — Traffic identification and classification are essential tasks performed by <phrase>Internet Service Providers</phrase> (ISPs) administrators. <phrase>Deep Packet Inspection</phrase> (DPI) is currently playing a key role in traffic identification and classification due to its increased expressive power. To allow <phrase>fair</phrase> comparison among different DPI techniques and system, workload generators should have the following characteristics: (i) synthetic packets with meaningful payloads; (ii) TCP and UDP traffic generation; (iii) configurable <phrase>network traffic</phrase> profile, and (iv) <phrase>high-speed</phrase> sending rate. Filling this niche of interest, this paper proposes a workload generator framework which inherits all of the above characteristics. <phrase>Performance evaluation</phrase> shows that our flexible workload generator system achieves very high sending rates over a 10Gbps network, using a <phrase>commodity</phrase> Linux machine. Additionally, we have configured and tested our workload generator following a real application traffic profile. We then have analyzed its results within a DPI system, proving its accuracy and efficiency.
A radiation tolerant <phrase>Phase Locked Loop</phrase> design for <phrase>digital electronics</phrase> — With decreasing <phrase>feature sizes</phrase>, lowered <phrase>supply voltages</phrase> and increasing <phrase>operating frequencies</phrase>, the radiation tolerance of <phrase>digital circuits</phrase> is becoming an <phrase>increasingly important</phrase> problem. Many <phrase>radiation hardening</phrase> techniques have been presented in the literature for combinational as well as <phrase>sequential logic</phrase>. However, the radiation tolerance of clock generation circuitry has received scant attention to date. Recently, it has been shown that in the deep submicron regime, the clock network contributes significantly to the chip level <phrase>Soft Error Rate</phrase> (SER). The on-chip <phrase>Phase Locked Loop</phrase> (PLL) is particularly vulnerable to radiation strikes. In this paper, we present a radiation hardened PLL design. Each of the components of this design – the <phrase>voltage controlled oscillator</phrase> (VCO), the phase frequency detector (PFD) and the loop filter are designed in a radiation tolerant manner. Whenever possible, the circuit elements used in our PLL exploit the fact that if a gate is implemented using only PMOS (NMOS) transistors then a radiation particle strike can result only in a logic 0 to 1 (1 to 0) flip. By separating the PMOS and NMOS devices, and splitting the gate output into two signals, extreme <phrase>high levels</phrase> of radiation tolerance are obtained. Our PLL is tested for radiation immunity for critical charge values up to 250fC. Our <phrase>results demonstrate</phrase> that over a large number of radiation strikes on a number of sensitive nodes in our design, the worst case <phrase>jitter</phrase> is just 18%. In the worst case, our PLL returns to the locked state in 16 cycles of the VCO clock, after a radiation strike. I. INTRODUCTION With relentless device scaling, lowered <phrase>supply voltages</phrase> and higher <phrase>operating frequencies</phrase>, the <phrase>noise margins</phrase> of <phrase>VLSI designs</phrase> are reducing. Thus <phrase>VLSI circuits</phrase> are becoming more vulnerable to noise due to crosstalk, <phrase>power supply</phrase> variations and <phrase>single event effects</phrase> (SEE) or <phrase>soft errors</phrase>. SEEs are caused when radiation particles such as <phrase>protons</phrase>, <phrase>neutrons</phrase>, <phrase>alpha particles</phrase>, or heavy ions strike sensitive diffusion regions in <phrase>VLSI designs</phrase>. These radiation particle strikes can deposit a charge, resulting in a voltage <phrase>glitch</phrase> on the affected node. This is particularly problematic for memories, since it can directly flip the stored state of a memory element, resulting in a <phrase>Single Event Upset</phrase> (SEU) [1], [2]. Although SEU induced errors in sequential elements continue to be problematic, it is expected that soft errors in <phrase>combinational logic</phrase> will become problematic in future technologies [3], [4], [5]. In a combinational circuit, …
Inference of <phrase>Low-Dimensional</phrase> Latent Structure in <phrase>High-Dimensional Data</phrase> (EE) Abstract The problem of learning a latent model for sparse or <phrase>low-dimensional</phrase> representation of <phrase>high-dimensional data</phrase> has attracted significant attention for many years. This thesis focuses on learning latent models for sparse or <phrase>low-dimensional</phrase> representation of images, dynamic data, and documents with Bayesian nonparametrics. The thesis consists of three parts. First, nonparametric Bayesian methods are considered for recovery of imagery based upon compressive measurements. A truncated beta-<phrase>Bernoulli process</phrase> is employed to infer an appropriate dictionary for the test data, and also for image recovery. In the context of compressive sensing, significant improvements in image recovery are manifested using learned dictionaries, relative to using standard orthonormal image expansions. The compressive-measurement projections are also optimized for the learned dictionary. Spatial interrelationships within imagery are exploited through use of the <phrase>Dirichlet</phrase> and <phrase>probit</phrase> stick-breaking processes. Several example results are presented, with comparisons to other state-of-the-art methods in the literature. Second, hierarchical Bayesian methods are employed to learn a reversible statistical embedding. The proposed embedding procedure is connected to spectral embedding methods (e.g., diffusion maps and Isomap), yielding a new statistical spectral framework. The proposed approach allows one to discard the training data when embedding new data, allows synthesis of <phrase>high-dimensional data</phrase> from the embedding space, and provides accurate estimation of the latent-space dimensionality. Hier-iv archical Bayesian methods are also developed to learn a nonlinear dynamic model in the <phrase>low-dimensional</phrase> embedding space, allowing joint analysis of multiple types of dynamic data, sharing strength and inferring interrelationships. In addition to analyzing dynamic data, the learned model also yields effective synthesis. Example results are presented for statistical embedding, latent-space dimensionality estimation , and analysis and synthesis of <phrase>high-dimensional</phrase> (dynamic) <phrase>motion-capture</phrase> data. Third, a new hierarchical <phrase>tree-based</phrase> topic model is developed, based on nonpara-metric Bayesian techniques. The model has two unique attributes: (i) a child node in the tree may have more than one parent, with the goal of eliminating redundant sub-topics deep in the tree; and (ii) parsimonious sub-topics are manifested, by removing redundant usage of words at multiple scales. The depth and width of the tree are unbounded within the prior, with a retrospective sampler employed to adap-tively infer the appropriate tree size based upon the corpus under study. Excellent quantitative results are manifested on five standard <phrase>data sets</phrase>, and the inferred <phrase>tree structure</phrase> is also found to be highly interpretable.
Bilingual Generation of Job from Quasi-Conceptual Descriptions Forms The EXCLASS system (Expert Job Evaluation Assistant) is intended to provide intelligent support for job description and classification in the <phrase>Canadian</phrase> Public Service. The Job Description Module (JDM) of EXCLASS is used to create conceptual representations of <phrase>job descriptions</phrase>, which are used for job evaluation and bilingual generation of textual <phrase>job descriptions</phrase>. The design of these representations was subject to two opposing constIaints: (1) that they be deep enough to resolve the ambiguities present in textual <phrase>job descriptions</phrase> , and (2) that they be close enough to surface linguistic forms that they can be conveniently manipulated by users with little specialized training. The close correspondence of concepts to surface words and phrases, as well as properties of the job description sublanguage, permit a simplified generator design, whereby phrases are prepackaged with a certain amount of linguistic structure, and combined according to a small set of mostly language-independent rules. Text planning, consisting mainly of grouping and ordering of conjoined phrases, is performed manually by the user, and composition of conceptual forms is supported by a "continuous text feedback" function. 1. Goals of EXCLASS The EXCLASS system (described on a more general level in Korelsky & Caldwell 1993) is intended to provide intelligent support for the process of describing and evaluating jobs in the <phrase>Canadian</phrase> Public Service. The Job Description Module (JDM) of EXCLASS, developed by CoGenTex for the <phrase>Canadian</phrase> <phrase>Treasury Board</phrase>, provides resources for the user to compose conceptual representations of <phrase>job descriptions</phrase>. The JDM generates textual <phrase>job descriptions</phrase> in both English and <phrase>French</phrase> from these representations; a Job Evaluation Module (JEM) also reasons on them to produce a classification and rating of a job, according to the government's evolving Universal Classification Standard. The first phase of the EXCLASS project resulted in a proof-of-concept prototype, based on a sample of some 30 <phrase>job descriptions</phrase> in the domain of procurement and <phrase>asset management</phrase>, in which the JDM and JEM are linked through a common <phrase>graphical interface</phrase>. The second phase, concluded in the spring of 1994, involved <phrase>R&D</phrase> in preparation for <phrase>fielding</phrase> and site testing of the system in a selected government department. EXCLASS is intended to eventually be used by thousands of managers across <phrase>Canada</phrase>, thus decreasing reliance on classification experts, while at the same time increasing the standardization, objectivity and comparability of job classifications across diverse occupational and organizational groupings. The principal task of the JDM is to produce an <phrase>unam</phrase>-biguous …
An <phrase>Energy-efficient</phrase> <phrase>Triple-channel Uwb-based Cognitive Radio</phrase> an <phrase>Energy-efficient</phrase> <phrase>Triple-channel Uwb-based Cognitive Radio</phrase> an <phrase>Energy-efficient</phrase> <phrase>Triple-channel Uwb-based Cognitive Radio</phrase> an <phrase>Energy-efficient</phrase> <phrase>Triple-channel Uwb-based Cognitive Radio</phrase> Permission to make digital or <phrase>hard copies of</phrase> all or part of this work for personal or classroom use is <phrase>granted without fee provided</phrase> that copies are not made or distributed for <phrase>profit or commercial advantage and</phrase> that <phrase>copies bear</phrase> this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, <phrase>requires prior specific permission</phrase>. Acknowledgement Working and studying with brilliant people in the superb environment at <phrase>UC Berkeley</phrase> was one of the most fortunate and honorable opportunities in my whole life. Most of all my gratitude to my research advisor, Prof. Jan M. Rabaey is so sincere and deep that I can't even find proper words to express it. Without the dedication, insight, and tolerance he showed me during my stay, I could not have reached this point, much less conducted proper research. I also would like to convey my deepest thanks to Prof. <phrase>Ali</phrase> M. Ninejad for the discussion and his devotion to the field of research. It was also my honor to have Prof. Paul K. Wright as my qualification committee. The proposed <phrase>triple-channel UWB-based cognitive radio</phrase> exploits spectral crowding and coexistence of other wireless devices as the number of sensors and wearable computing devices increases in 3.1GHz to 10.6GHz <phrase>ISM band</phrase> to achieve <phrase>energy efficient</phrase> 1Gb/s short-range <phrase>wireless communication</phrase>. A dual-resolution analog wavelet-based spectrum performs bandwidth-and frequency-agile <phrase>band pass filter</phrase> (BPF) to detect narrowband and wideband interferers with low <phrase>power consumption</phrase>. A charge-<phrase>pump</phrase>-based triangular waveform generators and a source follower type <phrase>low pass filter</phrase> (<phrase>LPF</phrase>) generates <phrase>basis function</phrase> for the spectrum sensing with 132MHz sensing resolution. A Low power integer-N QPLL with reduced reference spur by digital calibration on mismatch of the charge <phrase>pump</phrase> current supports the tuning frequencies with a linear tuned <phrase>wide range</phrase> two stage ring-VCO and a low power programmable true-single-phase-clock (TSPC) divider. The proposed <phrase>Triple-channel UWB-based cognitive radio</phrase> was fabricated in 1V 65nm CMOS GP process. The <phrase>test chip</phrase> size is 2.3×2 mm 2 , and the active area is 2.1mm 2. The data rate by using triangular shaped BPSK data is 1Gb/s at 1m communication. The lowest FoM of the energy/bit is 61pJ/bit, and the highest FoM is 102pJ/bit. It achieves BER from 9.2×10-7 to 1.1×10-4 according to <phrase>frequency allocation</phrase> of the triple-channels. The <phrase>triple-channel UWB-based cognitive radio</phrase> can provide <phrase>energy efficient</phrase> high-data rate <phrase>wireless communication</phrase> even with …
<phrase>Dallas</phrase> <phrase>Eeprom</phrase> Equipment Profile for Rapid Integration and System Modeling One definition of Responsive Space is the ability for mission-specific payloads and support systems to be rapidly integrated within a short period. However, as components are added to the spacecraft, the complex interactions between subsystems must be noted and, if possible, modeled. This process is extremely time consuming and, when done poorly (or not at all), is a major contributor to spacecraft failure. A new paradigm is needed for Rapid Integration and System Modeling. and Testing by functionally combining their respective satellites, Akoya and <phrase>Onyx</phrase>. Both vehicles were connected via a common power and data wiring <phrase>harness</phrase>, allowing one spacecraft to operate any device on either vehicle. Despite possessing minimal <phrase>prior knowledge</phrase> of the other school's subsystems, functional integration was achieved in less than thirty minutes. Each satellite uses a <phrase>distributed computing</phrase> architecture with a standardized interface and communication protocol. This architecture allows each subsystem to be developed separately and rapidly integrated into the spacecraft. The success of this experience led to an improved design for subsystem-level embedded operational intelligence. The <phrase>Dallas</phrase> <phrase>EEProm</phrase> Equipment Profile (DEEP) Architecture extends this standardized bus to include improved support for rapid integration and system modeling. DEEP is a protocol standard using the Maxim/<phrase>Dallas</phrase> 1-Wire bus allowing for <phrase>low level</phrase> control and monitoring of the spacecraft using <phrase>commercial-off-the-shelf</phrase> devices including memory and sensor devices. DEEP specifies a standard with which a representation of subsystem functionality is encoded within the subsystem itself, allowing for the creation of a satellite-wide model paralleling the physical integration of the spacecraft. This allows a stockpile of flight DEEP enabled subsystems, ready to be rapidly composed into a functional spacecraft. Each subsystem includes a subsystem model, with parameters such as thermal and power characteristics, allowing an anomaly management system to identify off-nominal conditions through model-<phrase>based reasoning</phrase>. Additional functionality includes, automated ground operations and ground integration and test software generation, standard command planning, <phrase>resource allocation</phrase>, and other areas of command and control. Nanosatellite competition operated by the <phrase>Air Force Research Laboratory</phrase>. This paper describes the current success of both <phrase>universities</phrase> with rapid integration, current development of the DEEP architecture, and future advances regarding responsive space.
A Readability Checker with <phrase>Supervised Learning</phrase> Using <phrase>Deep Syntactic and Semantic Indicators</phrase> Checking for readability or simplicity of texts is important for many institutional and individual users. Formulas for approximately measuring text readability have a long tradition. Usually, they exploit surface-oriented indicators like sentence length, word length, word frequency, etc. However, in many cases, this information is not adequate to realistically approximate the cognitive difficulties a person can have to understand a text. Therefore we use <phrase>deep syntactic and semantic indicators</phrase> in addition. The <phrase>syntactic information</phrase> is represented by a <phrase>dependency tree</phrase>, the <phrase>semantic information</phrase> by a <phrase>semantic network</phrase>. Both representations are <phrase>automatically generated</phrase> by a deep syntactico-<phrase>semantic analysis</phrase>. A global readability score is determined by applying a nearest neighbor algorithm on 3,000 ratings of 300 test persons. The evaluation showed, that the <phrase>deep syntactic and semantic indicators</phrase> lead to quite comparable results to most surface-based indicators. Finally, a graphical user interface has been developed which highlights difficult-to-read text passages, depending on the individual indicator values, and displays a global readability score.
Iddq <phrase>Test Challenges</phrase> in <phrase>Nanotechnologies</phrase>: a <phrase>Manufacturing Test</phrase> Strategy The implementation of <phrase>IDDQ test</phrase> is increasingly challenging with the shrinking of process geometry in <phrase>nanotechnologies</phrase>. This paper presents a case study of the <phrase>test challenges</phrase> that the industry is facing in <phrase>deep submicron</phrase> process. An IDDQ <phrase>manufacturing test</phrase> strategy is discussed to address the challenges.
<phrase>Crosstalk Fault</phrase> Testing by Using Oscillation Ring Testing Methodology for Soc Interconnection Lines The advance in IC processing technology rapidly reduces spacing between adjacent wires; which renders <phrase>crosstalk fault</phrase> an important source of anomaly in deep subcicrom VLSI. As a result, crosstalk <phrase>fault detection</phrase> should be an essential part in SOC testing. Although IEEE P1500 has been developed to test interconnects in SOC. this standard is not suitable for coupling fault testing. In this paper, we propose a new testing scheme, namely the oscillation ring testing. This method is very efficient for crosstalk <phrase>fault detection</phrase>, which is otherwise very difficult under traditional test scheme. We also propose a systematic way to find out oscillation rings and <phrase>test patterns</phrase>. We have conducted experiments on the proposed scheme with several test circuits consisting of ISCAS benchmarks, and the results show that <phrase>crosstalk faults</phrase> can be detected with a very small number of <phrase>test patterns</phrase>.
Force-detecting gripper and <phrase>force feedback</phrase> system for neurosurgery applications PURPOSE For the application of less invasive robotic neurosurgery to the resection of deep-seated tumors, a prototype system of a force-detecting gripper with a flexible micromanipulator and <phrase>force feedback</phrase> to the operating unit will be developed.   METHODS Gripping force applied on the gripper is detected by strain gauges attached to the gripper clip. The signal is transmitted to the amplifier by wires running through the inner tube of the manipulator. Proportional force is applied on the finger lever of the operating unit by the <phrase>surgeon</phrase> using a bilateral control program. A <phrase>pulling force</phrase> experienced by the gripper is also detected at the gripper clip. The signal for the <phrase>pulling force</phrase> is transmitted in a manner identical to that mentioned previously, and the proportional <phrase>torque</phrase> is applied on the touching roller of the finger lever of the operating unit. The <phrase>surgeon</phrase> can feel the gripping force as the resistance of the operating force of the finger and can feel the <phrase>pulling force</phrase> as the friction at the finger surface.   RESULTS A basic operation test showed that both the gripping force and <phrase>pulling force</phrase> were clearly detected in the gripping of soft material and that the operator could feel the gripping force and <phrase>pulling force</phrase> at the finger lever of the operating unit.   CONCLUSIONS A prototype of the <phrase>force feedback</phrase> in the microgripping manipulator system has been developed. The system will be useful for removing deep-seated brain tumors in future master-slave-type robotic neurosurgery.
<phrase>Cover-free Families</phrase> and Topology-transparent Communication Imagine that tens or hundreds of thousands of sensors are deployed in a person's <phrase>circulatory</phrase> system to monitor blood <phrase>chemistry</phrase>, travelling on currents in the <phrase>blood stream</phrase>. Our task is to enable these sensors to communicate with one another effectively , to provide multihop paths to fixed monitoring stations, and to provide clinical personnel with the critical data needed to respond. The sensors can move rapidly and unpredictably. They must operate at very low transmission power to avoid tissue damage. In this extreme case any knowledge of the <phrase>network topology</phrase> is at best severely limited. Indeed, topology discovery has limited value in the face of such rapid changes. Protocols are needed that do not require a sensor to know the identity of its <phrase>neighbours</phrase> — topology-transparent communication. With Violet Syrotiuk and others, we have recently shown that topology-transparent communication can be achieved using <phrase>cover-free families</phrase> of sets. These have a deep mathematical history in <phrase>group testing</phrase>. Combinatorial <phrase>group testing</phrase> has been the main tool to isolate defectives, by identifying pools of members with one or more defectives. Uses include disease screening, identifying clones in DNA libraries, and communications. In <phrase>satellite communications</phrase>, <phrase>group testing</phrase> is used to schedule polling of ground stations in groups. Instead our goal is to schedule transmissions so that every node pair has a collision-free transmission opportunity. Such topology-transparency has been, until now, a theoretical curiosity. We have made major inroads in changing this, by establishing that the substantial theory of <phrase>cover-free families</phrase> provides the necessary mathematical basis, and by overcoming many of the obstacles to implementation. The <phrase>basic research</phrase> to bring these into application in the large <phrase>sensor networks</phrase> to come requires generalization of the mathematical foundations from <phrase>cover-free families</phrase>, and their experimentation in practical scenarios. In this talk, an introduction to <phrase>cover-free families</phrase> and to topology-tranparency is given. Then we describe a three-state generalization of <phrase>cover-free families</phrase> for application in <phrase>sensor networks</phrase>. Since sensors are energy-limited, any transmission schedule must address energy savings. This requires the scheduling not only of times for transmission and reception, but also for sleep. The resulting combinato-rial problem yields a new, and largely unexplored, extension of <phrase>cover-free families</phrase> to permit three states rather than two. While focussing on the combinatorial problem , <phrase>experimental results</phrase> from simulations in <phrase>ad hoc</phrase> and <phrase>sensor networks</phrase> will also be presented.
Warm <phrase>Deep Drawing</phrase> of <phrase>Aluminium</phrase> Sheet <phrase>Aluminium</phrase> sheet drawing processes can be improved by manipulating local flow behaviour by means of elevated temperatures and <phrase>temperature gradients</phrase> in the tooling. Forming tests showed that a substantial improvement is possible not only for 5xxx but also for 6xxx series alloys. <phrase>Finite element method</phrase> simulations can be a powerful tool for the design of warm forming processes and tooling. Their accuracy will depend on the availability of materials models that are capable of describing the influence of temperature and <phrase>strain rate</phrase> on the flow stresses. Two models, an adapted Nadai <phrase>power law</phrase> and a <phrase>dislocation</phrase> based Bergström type model, are compared by means of simulations of a cup drawing process. Experimental drawing <phrase>test data</phrase> are used to validate the modelling approaches, whereas the model parameters follow from tensile tests. 1 Introduction The need for lighter car bodies on the one hand side and the complicated shapes of car parts on the other hand side result in a quest for improving the formability of <phrase>aluminium</phrase> sheet. <phrase>Aluminium</phrase> has a large potential for weight reduction, but press operations are more critical than with steel. The alloys used for automotive sheet components are mostly 5xxx and 6xxx alloys. The 5xxx alloys have the best formability, but cannot be used for outer panels because of stretcher strains. These are mostly made from 6xxx alloys, which are however less suitable for complicated inner parts because of a lesser formability. The formability can be improved by using elevated temperatures and <phrase>temperature gradients</phrase> in the tooling and blank, which make it possible to manipulate local flow [1-3]. An extra benefit of warm forming is that stretcher strains do not occur in 5xxx series alloys at elevated temperatures. In this paper the effect of warm drawing (in the temperature range up to 250 o C) on the process limits of a 1.2 mm gauge 5754-O and 6016-T4 <phrase>alloy</phrase> sheet and on the <phrase>mechanical properties</phrase> of the formed material are demonstrated. The introduction of warm drawing technology will be greatly helped if <phrase>finite element method</phrase> simulations are available for process and tooling design. Hence FEM simulations, including material models of warm flow behaviour, are developed and validated.
Functional and Structural Reasoning in Configuration Tasks A configuration problem within the technical domain is the problem of putting together different parts into a complex system, given a description of the possible parts and the functionality of the required system. While workin~ with our master thesis (Leitgeb, Pernler 95) at SICS ~ during 93 and 94, we found only a few configuration systems extracting the functional reasoning from the structural. We herein motivate the need for functional reasoning as a basis to find meaningful control structures separated from the application <phrase>domain knowledge</phrase>, and that this, together with a separated structural approach to describe the possible parts of the artefact, might be a neat solution. We have defined a model for representing configuration problems in two dimensions: Functional knowledge of the function of the artefact. Structural knowledge about structure and possible parts of the artefact. We have named this model the FAST model-the Functional And STructural model, and tested it on a real world application. When we create an object through synthesis 2, what do we strive for? In reality, we are not just looking for an object-no, we search for an object providing a certain number of functions in its environment. The object is characterised both by the functions it provides to its environment, and the internal functions it uses to fulfil the external functionality-without its internal functions , the object can not perform the external ones. In classical science, when talking about a physical object, we can characterise it both through its structure (its parts) and its functions (Pirsig 74). We claim that the most important characterisation is the functional in synthesis, we search for parts because of 2Such as configuration, design etc. their functions. We are not primarily concerned with aesthetical objects-we do not want amy structural part not motivated by a function. Each part realizes one or several necessary functions, or gives the overall system a required quality. Our thesis is that no structural description within the synthesis domain is justified by itself. Our willingness to use a structural description and a structural reasoning as the basis for synthesis, comes from our view of the object-we cam see its structure , touch its parts, divide it into pieces. You can hardly see all the external and internal functions of a <phrase>motorcycle</phrase>, for example. To divide its functions into sub-functions requires deep knowledge of the function of the <phrase>motorcycle</phrase>. Still, ff you want to repair …
Function Extraction (fx) Technology: Computing the Behavior of Software The Idea of Software Behavior Computation Modern society is dependent on software systems of ever-increasing scope and complexity. These systems continue to experience errors and vulnerabilities despite best efforts in their development. Programmers today have no practical means to determine the full functional behavior of software. This technology gap has been the source of problems and frustrations in software for decades. Software engineers must understand all of the behavior of software, intended or unintended, benign or malicious. While current <phrase>software development</phrase> and testing tools can help analyze specific properties and cases of behavior, what is needed is an " all cases of behavior " understanding of what software does. The CERT (Computer <phrase>Emergency Response</phrase> Team) organization of the <phrase>Software Engineering Institute</phrase> at <phrase>CMU</phrase> is developing the emerging technology of function extraction (FX) to automate the calculation of software behavior with mathematical precision to the maximum extent possible [1]. FX is a new type of engineering automation. The objective is to reduce dependence on slow, fallible, manual methods of software analysis by substituting fast, correct computation of behavior. Computing the precise behavior of software requires deriving its net functional effect; that is, how it transforms inputs into outputs in all circumstances of use, without heuristics or approximations. That information can be presented to analysts in non-procedural behavior displays that define all the possible effects a program can have, essentially, the " all cases of behavior " view. The ultimate objective is to move from an uncertain understanding of software derived in human time scale to a precise understanding computed in machine time scale. Theory-based function extraction operates on the deep functional semantics of software, and is not subject to the limitations of traditional syntactic methods [2], [3], [4], [5]. Controlled experimentation showed that users of an FX prototype were <phrase>orders of magnitude</phrase> faster than users of manual methods in determining software functionality, and were much more productive when computed behavior was available [6]. A Miniature Example of Behavior Computation In notional <phrase>illustration</phrase> of behavior computation, consider the following sequence structure of three instructions (" := " is the assignment operator) operating on integer variables x and y (machine precision aside), and the question of what it does: do x := x – y y := y + x x := y – x enddo The function extraction process computes a procedure-<phrase>free expression</phrase> of what this structure does from beginning to end …
Cheops: A Compact Explorer for Complex Hierarchies As the amount of electronic information explodes, hierarchies to handle this information become huge and complex. Visualizing and interacting with these hierarchies become daunting tasks. The problem is exacerbated if the visualization is to be done on mass-market <phrase>personal computers</phrase>, with limited processing power and visual resolution. Many of the current visualization techniques work effectively for hierarchies of 1000 nodes. But as the number of nodes increases toward 5000 nodes, these techniques tend to break down. Hierarchies above 5000 nodes usually require special modifications such as clustering, which can affect visual stability. This paper introduces Cheops. a novel approach to the representation, browsing and exploration of huge, complex information hierarchies such as the <phrase>Dewey</phrase> <phrase>Decimal</phrase> system, which can contain between I million and I billion nodes. The Cheops approach maintains context within a huge hierarchy, while simultaneously providing easy access to details. This paper will also present some <phrase>preliminary results</phrase> from usability tests performed on an 8 wide by 9 deep classification hierarchy, which if fully populated would contain over I9 million nodes.
<phrase>Wireless network</phrase> deployment configurations: Dwesa marginalized area as a case study Several technological initiatives have been, and will continue to be implemented across the world to tackle the major barriers in bridging the <phrase>digital divide</phrase>. These include the use of Internet and other ICTs. This is seen as a gigantic step in the alleviation of the different <phrase>paramount</phrase> social-economic problems, at the same time, in the transformation of the society and realization of a truly free and <phrase>democratic</phrase> world society. The use of technology in bridging this gap is a fundamental advancement because it also brings with it a mutual understanding and elimination of differential powers within communities in both developing and developed countries worldwide. This paper describes how different wireless access technologies can be combined and deployed to facilitate a continuous flow of information, in and out of the marginalized areas in <phrase>developing countries</phrase>. It further explains again, the major role of having Internet connectivity in these areas and how it's seen as an improvement in successfully deployment of ICT4D activities. Different technologies such as <phrase>WiFi</phrase>, <phrase>WiMAX</phrase> and <phrase>VSAT</phrase> will be explored and discussed for the best wireless deployment configurations for the Siyakhula Living Lab (SLL). SLL was chosen as our <phrase>test bed</phrase> and it is located in Dwesa Community. This is one of the deep marginalized rural communities in <phrase>South Africa</phrase> found in the <phrase>Eastern Cape</phrase> Region.
Measurement-Based Deep <phrase>Venous Thrombosis</phrase> Screening System An experimental system and interface that indicate the likelihood of deep <phrase>venous thrombosis</phrase> using objective measures was developed, based on ultrasound <phrase>image processing</phrase> using a modified Star-Kalman algorithm and a sensorized ultrasound probe. Force, location and <phrase>image data</phrase> is used to assess a vessel segment for compression. A <phrase>user interface</phrase> displays the results using a 3-D representation. A tissue phantom was developed for testing and validation. Initial results with this phantom and healthy volunteers are presented.
Classifying and Visualizing <phrase>Motion Capture</phrase> Sequences using Deep Neural Networks The <phrase>gesture recognition</phrase> using <phrase>motion capture</phrase> data and depth sensors has recently drawn more attention in vision recognition. Currently most systems only classify dataset with a couple of dozens different actions. Moreover, <phrase>feature extraction</phrase> from the data is often computational complex. In this paper, we propose a novel system to recognize the actions from <phrase>skeleton</phrase> data with simple, but effective, features using deep neural networks. Features are extracted for each frame based on the relative positions of joints (PO), temporal differences (<phrase>TD</phrase>), and normalized trajectories of motion (<phrase>NT</phrase>). Given these features a hybrid <phrase>multi-layer</phrase> <phrase>perceptron</phrase> is trained, which simultaneously classifies and reconstructs input data. We use deep autoencoder to visualize learnt features, and the experiments show that <phrase>deep neural networks</phrase> can capture more discriminative information than, for instance, <phrase>principal component analysis</phrase> can. We test our system on a public database with 65 classes and more than 2,000 motion sequences. We obtain an accuracy above 95% which is, to our knowledge, the state of the art result for such a large dataset.
<phrase>Circuit Design</phrase> for <phrase>Logic Automata</phrase> <phrase>Circuit Design</phrase> for <phrase>Logic Automata</phrase> The <phrase>Logic Automata</phrase> model is a universal <phrase>distributed computing</phrase> structure which pushes parallelism to the <phrase>bit-level</phrase> extreme. This new model drastically differs from conventional computer architectures in that it exposes, rather than hides, the physics underlying the computation by accomodating <phrase>data processing</phrase> and storage in a local and distributed manner. Based on <phrase>Logic Automata</phrase>, highly scalable computing struc-trues for <phrase>digital and analog</phrase> processing have been developed; and they are verified at the transistor level in this thesis. The Asynchronous <phrase>Logic Automata</phrase> (ALA) model is derived by adding the temporal locality, i.e., the asynchrony in data exchanges, in addition to the spacial locality of the <phrase>Logic Automata</phrase> model. As a demonstration of this incrementally extensible, clockless structure, we designed an ALA <phrase>cell library</phrase> in 90 nm <phrase>CMOS technology</phrase> and established a " pick-and-place " <phrase>design flow</phrase> for fast ALA circuit layout. The work flow gracefully aligns the description of computer programs and circuit realizations, providing a simpler and more scalable solution for <phrase>Application Specific Integrated Circuit</phrase> (ASIC) designs, which are currently limited by global contraints such as the clock and long interconnects. The potential of the ALA circuit <phrase>design flow</phrase> is tested with example applications for mathematical operations. The same <phrase>Logic Automata</phrase> model can also be augmented by relaxing the digital states into analog ones for interesting analog computations. The Analog <phrase>Logic Automata</phrase> (AnLA) model is a merge of the Analog Logic principle and the <phrase>Logic Automata</phrase> arhitecture, in which efficient processing is embedded onto a scalable construction. In order to study the unique property of this <phrase>mixed-signal</phrase> computing structure, we designed and fabricated an AnLA <phrase>test chip</phrase> in AMI 0.5μm <phrase>CMOS technology</phrase>. Chip tests of an AnLA Noise-Locked Loop (<phrase>NLL</phrase>) circuit as well as application tests of AnLA <phrase>image processing</phrase> and <phrase>Error-Correcting Code</phrase> (ECC) decoding, show large potential of the AnLA structure. Acknowledgments I would like to acknowledge the support of MIT's Center for Bits and <phrase>Atoms</phrase> and its sponsors. Thank you to my thesis supervisor, <phrase>Neil Gershenfeld</phrase>, for his intriguing guidance and encouragement over the past two years. His wide knowledge span and deep insight have been inspiring me to keep learning and thinking; and his openness to ideas has encouraged me to always seek better solutions to problems I encounter. Thank you for directing the Center for Bits and <phrase>Atoms</phrase> and the Physics and Media <phrase>Research Group</phrase>, which provide me great <phrase>intellectual freedom</phrase> and wide vision. He has …
Fault Clustering in deep-submicron CMOS Processes The fraction of ICs that pass all production tests but fail in the application is called the defect level. Defect levels depend on the average number of defects per IC, and also on the clustering of these defects. High clustering leads to a higher yield and a lower defect level.  This paper compiles the coefficients for defect clustering using research findings from 1970 until 2001. Because recent data for <phrase>deep submicron</phrase> processes are missing in the literature, the <phrase>clustering coefficient</phrase> has been calculated using scan fail distributions of ICs in a 180 nm process.  Clustering coefficients show a steady trend towards higher defect clustering. This is beneficial, but it is probably not sufficient to achieve today's ambitious target of 'zero defects'.
Generating Efficient <phrase>Test Sets</phrase> with a <phrase>Model Checker</phrase> It is well-known that counterexamples produced by <phrase>model checkers</phrase> can provide a basis for automated generation of <phrase>test cases</phrase>. However, when this approach is used to meet a coverage criterion, it generally results in very inefficient <phrase>test sets</phrase> having many tests and much redundancy. We describe an improved approach that uses <phrase>model checkers</phrase> to generate efficient <phrase>test sets</phrase>. Furthermore, the generation is itself efficient, and is able to reach deep regions of the statespace. We have prototyped the approach using the <phrase>model checkers</phrase> of our SAL system and have applied it to <phrase>model-based</phrase> designs developed in Stateflow. In one example, our method achieves complete state and transition coverage in a Stateflow model for the shift sched-uler of a 4-speed <phrase>automatic transmission</phrase> with a single <phrase>test case</phrase>.
SLOPE: a quick and accurate method for locating non-<phrase>SNP</phrase> structural variation from targeted <phrase>next-generation</phrase> sequence data MOTIVATION Targeted 'deep' sequencing of specific genes or regions is of great interest in clinical cancer diagnostics where some sequence variants, particularly <phrase>translocations</phrase> and indels, have known prognostic or diagnostic significance. In this setting, it is unnecessary to sequence an entire genome, and target capture methods can be applied to limit sequencing to important regions, thereby reducing costs and the time required to complete testing. Existing 'next-gen' sequencing analysis packages are optimized for efficiency in whole-genome studies and are unable to benefit from the particular structure of targeted sequence data.   RESULTS We developed SLOPE to detect structural variants from targeted short-DNA reads. We use both real and simulated data to demonstrate SLOPE's ability to rapidly detect insertion/deletion events of various sizes as well as <phrase>translocations</phrase> and <phrase>viral</phrase> integration sites with high sensitivity and low <phrase>false discovery rate</phrase>.   AVAILABILITY <phrase>Binary code</phrase> available at http://www-genepi.med.utah.edu/suppl/SLOPE/index.html
<phrase>Computer Chess</phrase> <phrase>Championship</phrase> fter twenty years of travel-ing from city to city across the <phrase>United States</phrase>, the ACM <phrase>North American</phrase> <phrase>Computer Chess</phrase> <phrase>Championship</phrase> came back to the place of its birth, the New <phrase>York</phrase> Hilton <phrase>Hotel</phrase>, where the competitions began in 1970. This latest five-round event ended in a two-way tie for first place between MEPHISTO and <phrase>DEEP THOUGHT/88</phrase>. Finishing in a two-way tie for third place were HITECH and M CHESS. A total of 10 teams participated, and the level of play was at the low grand-master level. A special three-round end-game <phrase>championship</phrase> was won by MEPHISTO, who also captured the prize for the best Small Computing System. A total of $8000 in prizes was divided up among the winners. <phrase>DEEP THOUGHT/88</phrase>, currently under development at IBM by researchers Feng-Hsiung Hsu, Murray Campbell, and Thomas Anantharaman along with two former associates at <phrase>Carnegie Mellon University</phrase> , Peter Jensen and Andreas Nowat-zyk, outplayed MEPHISTO in the third round but lost to HITECH in the next round. It entered the final round of play one-half point behind HITECH, who had won all of its games with the exception of a first-round draw with ZARKOV. <phrase>DEEP THOUGHT/88</phrase> defeated ZARKOV in the final round while HITECH lost on time to MEPHISTO in a dead-drawn game. MEPHISTO won all of its games with the exception of its third-round loss to <phrase>DEEP THOUGHT/88</phrase>. MEPHISTO played solid chess throughout the event, but was fortunate to win its game against HITECH in the final round. MEPHISTO, developed by Richard Lang of <phrase>Great Britain</phrase>, is marketed by the Ger-man company of Hegener & Glaser A.G. The rules of the <phrase>tournament</phrase> required each side to play all of its moves within a two-hour period ensuring that the games would last at most four hours. MEPHISTO played slightly faster than HITECH in the middle game and entered the endgame with approximately five more minutes on its clock. HITECH, who played even with MEPHISTO, was unable to regain the lost time and eventually lost a dead-drawn game on move 145. The <phrase>tournament</phrase> was marred by difficulties. This was the first time the <phrase>tournament</phrase> was played during the day, and Bob <phrase>Hyatt</phrase> was unable to make the necessary arrangements. <phrase>DEEP THOUGHT/88</phrase> was used by Hsu and company when they found they did not have sufficient time to test out their latest version. ZERKER, a new entry developed by James Testa at the University of California, …
Temperature-Aware SoC <phrase>Test Scheduling</phrase> Considering Inter-Chip <phrase>Process Variation</phrase> —Systems on Chip implemented with <phrase>deep submicron technologies</phrase> suffer from two undesirable effects, high <phrase>power density</phrase>, thus <phrase>high temperature</phrase>, and high <phrase>process variation</phrase>, which must be addressed in the <phrase>test process</phrase>. This paper presents two temperature-aware scheduling approaches to maximize the test throughput in the presence of inter-chip <phrase>process variation</phrase>. The first approach, an <phrase>off-line</phrase> technique, improves the test throughput by extending the traditional scheduling method. The second approach, a hybrid one, improves further the test throughput with a chip classification scheme at test time based on the reading of a temperature sensor. <phrase>Experimental results</phrase> have demonstrated the efficiency of the proposed methods.
Efficient electronic integrals and their generalized derivatives for <phrase>object oriented</phrase> implementations of <phrase>electronic structure</phrase> calculations For the new parallel implementation of <phrase>electronic structure</phrase> methods in <phrase>ACES</phrase> III (Lotrich et al., in preparation) the present state-of-the-art algorithms for the evaluation of electronic integrals and their generalized derivatives were implemented in new <phrase>object oriented</phrase> codes with attention paid to efficient execution on modern processors with a deep hierarchy of data storage including multiple caches and memory banks. Particular attention has been paid to define proper integral blocks as basic building objects. These objects are stand-alone units and are no longer tied to any specific software. They can hence be used by any <phrase>quantum chemistry</phrase> code without modification. The integral blocks can be called at any time and in any sequence during the execution of an <phrase>electronic structure</phrase> program. Evaluation efficiency of these integral objects has been carefully tested and it compares well with other fast integral programs in the community. Correctness of the objects has been demonstrated by several application runs on real systems using the <phrase>ACES</phrase> III program.
Use of Patterns for Detection of Likely Answer Strings: A Systematic Approach The paper describes the <phrase>Question Answering</phrase> approach applied first at TREC-10 QA track and developed systematically in TREC 2002 experiments. The approach is based on the assumption that answers can be identified by their correspondence to formulas describing the structure of strings carrying certain (generalized) semantics, supposed by the question type. These formulas, or patterns, are like <phrase>regular expressions</phrase> but include elements corresponding to predefined lists of terms. Complex patterns can be constructed from blocks corresponding to such semantic entities as persons' or organizations' names, posts, dates, locations, etc. Using various combinations of blocks and intermediate syntactic elements allows to build a great variety of patterns. Exact position of elements corresponding to the "exact answer" was localized within the structure of each pattern. Each pattern is characterized by a generalized semantics, thus the <phrase>pattern-matching</phrase> string must be checked for correlation with the question terms and/or their <phrase>synonyms</phrase>/substitutes. In 2002 TREC QA track tests we have further developed the approach described in [Soubbotin, 2001]. In general, our method lies in the domain of approaches examining the potential of <phrase>information extraction</phrase> for <phrase>question answering</phrase> tasks (MUCs), shows a certain shift from deep text analysis based on computational linguistic and NLP methods to surface techniques [<phrase>Eagles</phrase>, 1998]. Our approach can be considered as being in line with this tendency. More specifically, our approach is based on the use of formulas describing the structure of strings likely bearing certain <phrase>semantic information</phrase>. For example, string "<phrase>FBI</phrase> <phrase>Director</phrase> <phrase>Louis Freeh</phrase>" can be recognized, according to one of such formulas, as likely bearing the following information: a person represented by his/her first and last names occupies a (leading) post in an organization. The formula for this string is: a word composed of capital letters; an item from the list of posts in an organization; an item from the list of first names; a capitalized word. We can mark two first items in this formula as "exact answer", if we want to get answer to the question "Who is <phrase>Louis Freeh</phrase>?", and two last items, if the question is "Who is <phrase>FBI</phrase> head?" (question 1583 at TREC 2002). First used at TREC-10 QA track, formulas of such kind were called "patterns" [Soubbotin M.M and Soubbotin S.M, 2001]. The term "pattern" is widely used in the field of <phrase>Information Extraction</phrase>. Our concept of patterns as structural formulas for strings is obviously different from that in "traditional" IE field, but …
<phrase>Web Analytics</phrase> 2.0: Empowering Customer Centricity In this two-part article, we start by describing the most standard practices of <phrase>Web Analytics</phrase>; the first steps required to analyze a website and understand the behavior of its <phrase>surfers</phrase>. For this purpose we present a <phrase>Web Analytics</phrase> process created by the authors based on industry best practices. The paper details each step of the process, going from defining goals and KPIs to collect, analyze, and take action using website data. Instead of presenting a single <phrase>case study</phrase>, we chose to spread <phrase>real life</phrase> examples throughout the article, enabling readers to connect each section to its practical application more easily. Following the hands-on process, part II proposes a pioneering concept of the next generation <phrase>Web Analytics</phrase> or, as we call it, <phrase>Web Analytics</phrase> 2.0. This concept advocates a holistic approach to website analysis in which we consider several sources of knowledge: website data, multichannel analysis, testing, competitive analysis, and customers' voice. The papers are especially valuable to people managing, maintaining or optimizing websites because they provide the tools to analyze and improve online <phrase>customer experience</phrase> and website profitability. INTRODUCTION <phrase>Web Analytics</phrase> is the science and the art of improving websites to increase their profitability by improving the customer's website experience. It is a science because it uses statistics, <phrase>data mining</phrase> techniques, and a methodological process. It is an art because, like a brilliant <phrase>painter</phrase>, the analyst or marketer has to draw from a diverse <phrase>pallet</phrase> of colors (data sources) to find the perfect mix that will yield actionable insights. It is also an art because improving websites requires a deep level of <phrase>creativity</phrase>, balancing user-centric design, promotions, content, images, and more. Besides, the analyst is always walking the fine line among website designers, IT personnel, <phrase>marketers</phrase>, senior management and customers. By now, website managers are aware that visitor acquisition is a multi-faceted endeavor, which makes use of the following techniques: <phrase>email</phrase>, <phrase>mail</phrase>, <phrase>affiliate marketing</phrase> and of course search. With each option they have become better at finding the right visitor to bring to their websites. For example, every website now has a <phrase>Search Engine Optimization</phrase> (SEO) strategy that will help them rank highly on <phrase>search engine</phrase> organic results. Likewise they are also aware that Pay-Per-Click (<phrase>PPC</phrase>) campaigns can be effective at driving relevant visitors. Acquiring visitors is only the start of the process rather than, as many <phrase>marketers</phrase> believe, the end. Jim <phrase>Sterne</phrase> and Matt Cuttler provide an excellent explanation …
<phrase>Deep transfer</phrase> via second-order Markov logic Standard inductive learning requires that <phrase>training and test</phrase> instances come from the same distribution. <phrase>Transfer learning</phrase> seeks to remove this restriction. In shallow transfer, test instances are from the same domain, but have a different distribution. In <phrase>deep transfer</phrase>, test instances are from a different domain entirely (i.e., described by different predicates). Humans routinely perform <phrase>deep transfer</phrase>, but few learning systems, if any, are capable of it. In this paper we propose an approach based on a form of second-order Markov logic. Our algorithm discovers structural regularities in the source domain in the form of Markov logic formulas with predicate variables, and instantiates these formulas with predicates from the target domain. Using this approach, we have successfully transferred learned knowledge among <phrase>molecular biology</phrase>, <phrase>social network</phrase> and Web domains. The discovered patterns include broadly useful properties of predicates, like <phrase>symmetry</phrase> and transitivity, and relations among predicates, such as various forms of homophily.
Generating and Prioritizing Optimal Paths Using <phrase>Ant Colony Optimization</phrase> The assurance of software reliability partially depends on testing. Numbers of approaches for <phrase>software testing</phrase> are available with their proclaimed advantages and limitations, but accessibility of any one of them is a subject dependent. Time is a critical factor in deciding cost of any project. A deep insight has shown that executing <phrase>test cases</phrase> are time consuming and tedious activity. Thus stress has been given to develop algorithms which can suggest better pathways for testing. One such algorithm called Path Prioritization –<phrase>Ant Colony Optimization</phrase> (PP-<phrase>ACO</phrase>) has been suggested in this paper which is inspired by real Ant's foraging behavior to generate optimal paths sequence of a decision to decision (DD) path of a graph. The algorithm does full path coverage and suggests the best optimal sequences of path in path testing and prioritizes them according to path strength. 1 Introduction Testing is an important aspect of the <phrase>software development</phrase> life cycle. It focuses on the process of testing the <phrase>newly developed</phrase> / under development software system, prior to its use. The program is executed with desired input(s) and the output(s) is / are observed accordingly. Observed/Actual output(s) is compared with the expected output(s), if they are same then the program under test is said to be correct as per its specification(s), otherwise there is something wrong somewhere in the program. Testing is the process of executing a program with the intent of finding faults (Mayers, 1977). The growing importance of software Systems for small and big organization results in more complex software systems. Thus it is one of the logic that more stress has been given on quality of developed software (s). History has shown that software faults not only caused a loss of money but also precious human lives. Thus if we don't improve the quality while system is under development, these losses may only get bigger (Beizer, 1990). In this paper we have proposed a novel algorithm for automatic generation and prioritization of optimal
A method for studying <phrase>jaw muscle</phrase> activity during standardized jaw movements under experimental jaw <phrase>muscle pain</phrase>. This paper describes a method for studying superficial and deep <phrase>jaw muscle</phrase> activity during standardized jaw movements under experimental jaw <phrase>muscle pain</phrase>. In 22 healthy adults, pain was elicited in the right <phrase>masseter muscle</phrase> via tonic infusion of 4.5% hypertonic saline and which resulted in scores of 30-60 mm on a 100-mm visual analogue scale. Subjects performed tasks in five sessions in a repeated measures design, i.e., control 1, test 1 (during hypertonic or isotonic saline infusion), control 2 (without infusion), test 2 (during isotonic or hypertonic saline infusion), control 3 (without infusion). During each session, subjects performed maximal clenching and standardized jaw tasks, i.e., protrusion, lateral excursion, open/close, chewing. <phrase>Mandibular</phrase> movement was recorded with a 6-<phrase>degree-of-freedom</phrase> tracking system simultaneously with electromyographic (<phrase>EMG</phrase>) activity from the inferior head of the lateral pterygoid muscle with fine-wire electrodes (verified by computer tomography), and from posterior temporalis, the submandibular muscle group and bilateral <phrase>masseter</phrase> muscles with surface electrodes. <phrase>EMG</phrase> root mean square values were calculated at each 0.5 mm increment of <phrase>mandibular</phrase> <phrase>incisor</phrase> movement for all tasks under each experimental session. This establishes an experimental model for testing the effects of pain on <phrase>jaw muscle</phrase> activity where the jaw motor system is required to perform <phrase>goal-directed</phrase> tasks, and therefore should extend our understanding of the effects of pain on the jaw motor system.
The integrated OR Efficiency and effectiveness evaluation after two years use, a pilot study OBJECTIVES Technology evaluation of integrated/digital OR is needed since very little literature has been published on the subject. The integrated OR is a technological solution intended for minimally invasive surgery where the surgeons have complete control of the environment, devices and image distribution. Before such an investment, Health <phrase>Technology Assessment</phrase> can be used as a method to evaluate what vendors' state, i.e. the fact that the integrated OR is a very effective and efficient solution. Then a follow-up evaluation could be useful after the installation to test the users' satisfaction and give suggestions to the community about real- experienced integrated OR advantage.   METHODS A multiple answer questionnaire has been handed to 17 surgeons and 9 scrub nurses form <phrase>Varese</phrase> <phrase>Town</phrase> and University Hospital to evaluate the degree of satisfaction after 2 years of use of integrated ORs.   RESULTS Surgeons and scrub nurses agree that the integrated OR can be very effective in increasing quality, risk reduction and surgery time reduction through the use of digitalized video acquisition system, boom-mounted devices and multiple displays. Scrub nurses are a little bit more confident than surgeons that <phrase>medical device</phrase> control could reduce the confusion inside the OR and reduce the number of setting errors. A very positive judgment was given to the system's teaching capabilities, but both surgeons and scrub nurses agree that a great degree of education and a cultural change are needed to use the system in a correct and complete way.   CONCLUSIONS Results show that there is a deep appreciation of the system which proved to be efficient (reducing surgery time and enhancing surgical quality) and effective. This is a pilot study based on few collected data, but the questionnaire could be handed to many <phrase>hospitals</phrase> where integrated ORs are present, in order to achieve a significant degree of assessment and find common topics to be considered fundamental especially in the evaluating phase.
Contrast Limited Adaptive <phrase>Histogram</phrase> Equalization <phrase>image processing</phrase> to improve the detection of simulated spiculations in dense mammograms The purpose of this project was to determine whether Contrast Limited Adaptive <phrase>Histogram</phrase> Equalization (CLAHE) improves detection of simulated spiculations in dense mammograms. Lines simulating the appearance of spiculations, a common marker of <phrase>malignancy</phrase> when visualized with masses, were embedded in dense mammograms digitized at 50 micron pixels, 12 bits deep. Film images with no CLAHE applied were compared to film images with nine different combinations of clip levels and region sizes applied. A simulated spiculation was embedded in a background of dense breast tissue, with the orientation of the spiculation varied. The key variables involved in each trial included the orientation of the spiculation, contrast level of the spiculation and the CLAHE settings applied to the image. Combining the 10 CLAHE conditions, 4 contrast levels and 4 orientations gave 160 combinations. The trials were constructed by pairing 160 combinations of key variables with 40 backgrounds. Twenty student observers were asked to detect the orientation of the spiculation in the image. There was a <phrase>statistically significant</phrase> improvement in detection performance for spiculations with CLAHE over unenhanced images when the region size was set at 32 with a clip level of 2, and when the region size was set at 32 with a clip level of 4. The selected CLAHE settings should be tested in the clinic with digital mammograms to determine whether detection of spiculations associated with masses detected at <phrase>mammography</phrase> can be improved.
Monotone Systems under <phrase>Negative Feedback</phrase> 1 Oscillations in I/o Monotone Systems under <phrase>Negative Feedback</phrase> — Oscillatory behavior is a key property of many biological systems. The Small-Gain Theorem (SGT) for <phrase>input/output</phrase> monotone systems provides a sufficient condition for global asymptotic stability of an equilibrium and hence its violation is a necessary condition for the existence of periodic solutions. One advantage of the use of the monotone SGT technique is its robustness with respect to all perturbations that preserve monotonicity and stability properties of a very <phrase>low-dimensional</phrase> (in many interesting examples, just one-dimensional) model reduction. This robustness makes the technique useful in the analysis of molecular biological models in which there is large uncertainty regarding the values of <phrase>kinetic</phrase> and other parameters. However, verifying the conditions needed in order to apply the SGT is not always easy. This paper provides an approach to the verification of the needed properties, and illustrates the approach through an application to a classical model of <phrase>circadian</phrase> oscillations, as a nontrivial " <phrase>case study</phrase>, " and also provides a theorem in the converse direction of predicting oscillations when the SGT conditions fail. I. INTRODUCTION Motivated by applications to <phrase>cell signaling</phrase>, our previous paper [1] introduced the class of monotone <phrase>input/output</phrase> systems , and provided a technique for the analysis of negative <phrase>feedback loops</phrase> around such systems. The main theorem gave a simple graphical test which may be interpreted as a monotone small gain theorem (" SGT " from now on) for establishing the global asymptotic stability of a unique equilibrium, a stability that persists even under arbitrary transmission delays in the <phrase>feedback loop</phrase>. Since that paper, various papers have followed-up on these ideas, see for example The first purpose is to develop explicit conditions so as to make it easier to apply the SGT theorem, for a class of systems of biological significance, a subset of the class of tridiagonal systems with inputs and outputs. Tridiagonal systems (with no inputs and outputs) were introduced largely for the study of gene networks and population models, and many results are known for them, see for instance [32], [34]. Deep achievements of the theory include the generalization of the <phrase>Poincaré</phrase>-Bendixson Theorem, from planar systems to tridiagonal systems of arbitrary dimension, due to <phrase>Mallet</phrase>-Paret and Smith [29] as well as a later generalization to
Internet repositories for <phrase>collaborative learning</phrase>: supporting both students and teachers Most efforts to create <phrase>computer-supported</phrase> <phrase>collaborative learning</phrase> environments have been focused on students. However, without providing appropriate integration of collaborative activities into curricula, these efforts will have little widespread impact on educational practices. To improve education through technology, learning environments for students must be integrated with curriculum <phrase>development tools</phrase> for teachers to create an integrated collaboration-oriented classroom. This paper describes how software tools for Internet repositories can aid fundamental collaboration activities—locating, using, adapting, and sharing—at both the teacher level (with the Teacher's Curriculum Assistant) and the student level (with the Remote <phrase>Exploratorium</phrase>). It illustrates how tools for educators and tools for students can be orchestrated into integrated classroom support. 1. Collaborative Activities Require Support The goal of encouraging groups of learners to engage collaboratively in <phrase>problem-solving</phrase> activities has much merit. <phrase>Social interaction</phrase> fosters <phrase>deep learning</phrase> in which students develop intellectual structures that allow them to create their own knowledge [27]. It promotes <phrase>social skills</phrase> that help people participate in the social construction of their shared reality [3]. It increases student engagement and brings out the relevance of learning [16]. It allows the educational process to be more student-centered, less disciplinary, and more exciting [14, 15]. The use of technology to foster <phrase>collaborative learning</phrase> is often seen as a key to reforming <phrase>science education</phrase>—on the principle that the best way to learn science is to engage in the practice of science [10]. The practices of modern science involve the use of <phrase>technologic</phrase> tools for: • observing and measuring interesting phenomena in the world, • generating representations and visualizations of the data, and • creating simulations to understand observed processes and to test hypotheses. Importantly, the practice of modern science is highly collaborative. Scientists work together to incrementally design experiments and simulations, to convergently develop hypotheses and theories, and to test and evaluate their work [17, 22]. Many projects have successfully combined these elements to foster innovative forms of collaborative <phrase>science education</phrase> among students [8, 12, 24, 26]. However, research projects have often been unable to transfer their successful results to other sites or schools because they did not replicate the initial teacher learning that occurred implicitly in the teacher-researcher and teacher-teacher collaborations [21]. For educational change to succeed, teachers too must be supported in changing from an isolated teaching model to one of <phrase>collaborative learning</phrase> with other educators [4]. We believe that for <phrase>collaborative learning</phrase> to succeed in the classroom, collaborative …
Experiences with CiceRobot, a Museum Guide Cognitive Robot The paper describes CiceRobot, a robot based on a <phrase>cognitive architecture</phrase> for robot vision and action. The aim of the architecture is to integrate <phrase>visual perception</phrase> and actions with <phrase>knowledge representation</phrase>, in order to let the robot to generate a deep inner understanding of its environment. The principled integration of perception, action and of symbolic knowledge is based on the introduction of an intermediate representation based on Gärdenfors conceptual spaces. The architecture has been tested on a RWI B21 <phrase>autonomous robot</phrase> on tasks related with guided <phrase>tours</phrase> in the <phrase>Archaeological</phrase> Museum of <phrase>Agrigento</phrase>. <phrase>Experimental results</phrase> are presented.
Assessing Inquiry Learning Inquiry and <phrase>River City</phrase> In this paper, we provide an overview of the design of an <phrase>inquiry-based</phrase> curriculum project, and then offer a comparative analysis of the outcomes of two methods for assessing student understanding of the inquiry process. Our findings indicate that the complex nature of <phrase>scientific inquiry</phrase> is better captured using an alternative method of assessment in addition to a more traditional <phrase>multiple-choice</phrase> test. recently issued a position statement recommending the use of science inquiry as a method to help students understand the processes and content of science (<phrase>National Science Teachers Association</phrase>, 2004). However, currently, there is a competing push in science for coverage of material found on state and national <phrase>standardized tests</phrase>; in many situations, this competing push forces the emphasis in science classrooms to change from <phrase>inquiry-based</phrase> instruction to <phrase>test-preparation</phrase> (Falk & Drayton, 2004). Could this dilemma of teaching scientific process versus covering test content be resolved via the inclusion of more <phrase>inquiry-based</phrase> questions on these <phrase>standardized tests</phrase>? While this may provide teachers and schools with incentives to cover inquiry skills as well as factual content, this solution raises a different concern: Can learning from good <phrase>inquiry-based</phrase> projects be adequately assessed using a <phrase>standardized test</phrase> format? What kind of assessments will allow valid inferences about whether a student has learned how to engage in inquiry, particularly in the " <phrase>front end</phrase> " inquiry processes used to derive a strategy for making sense out of complexity: problem finding, hypothesis formation, experimental design? Using an <phrase>NSF</phrase>-funded MultiUser <phrase>Virtual Environment</phrase> (MUVE) as a pedagogical vehicle, our research team is exploring how a technology-intensive learning experience that immerses participants in a virtual " world " whose citizens face chronic illnesses can help <phrase>middle school</phrase> <phrase>students learn</phrase> both deep inquiry skills and science knowledge. In this paper, we provide an overview of the design of this <phrase>inquiry-based</phrase> curriculum project. We then offer a comparative analysis of the outcomes of two methods of assessing student understanding of the inquiry process in order to clarify the extent to which typical forms of test items can validly measure students' inquiry skills. Theoretical Underpinnings Inquiry What is " inquiry? " The range of possible responses to this question is large. Some refer to inquiry as a set of process skills that include questioning, hypothesizing and testing while others equate it to " hands-on " learning. The National <phrase>Science Education</phrase> Standards (NSES) define <phrase>scientific inquiry</phrase> as " the diverse ways …
Testing Symmetric Properties of Distributions Testing Symmetric Properties of Distributions We introduce the notion of a Canonical Tester for a class of properties on distributions , that is, a tester strong and general enough that " a distribution property in the class is testable if and only if the Canonical Tester tests it ". We construct a <phrase>Canon</phrase>-ical Tester for the class of properties of one or two distributions that are symmetric and satisfy a certain weak continuity condition. Analyzing the performance of the Canonical Tester on specific properties resolves several open problems, establishing lower bounds that match known upper bounds: we show that distinguishing between entropy < α or > β on distributions over [n] requires n α/β−o(1) samples, and distinguishing whether a pair of distributions has statistical distance < α or > β requires n 1−o(1) samples. Our techniques also resolve a conjecture about a property that our Canonical Tester does not apply to: distinguishing identical distributions from those with statistical distance > β requires Ω(n 2/3) samples. Acknowledgments I am indebted to each member of my thesis committee – Silvio Micali, Ronitt Rubin-feld and <phrase>Madhu</phrase> <phrase>Sudan</phrase> – for their longstanding guidance, support, and friendship. Silvio Micali, my advisor, has been a tireless source of inspiration, ideas, and raw energy since I first arrived at MIT. Our interactions are distinguished by his knack for recasting seemingly intractable problems into ones whose solutions are simple, natural, and deep. Where once I might have seen an insurmountable problem, I now realize that there are always many more ways around an obstacle than over it; nothing has broadened my view of the research <phrase>landscape</phrase> more. And despite his protestations that " perfection is the enemy of the good " , he has kept my eyes firmly directed towards that elusive goal, despite all cross-currents, obstacles, and distractions in the way. His dedication to speaking and writing with flair serves as a continuing example by which to improve my own work. Ronitt Rubinfeld is responsible for directing me to the beautiful problems with which this thesis is concerned. For this, and her inexhaustible enthusiasm about both the field of property testing and my efforts in it, I am deeply grateful. <phrase>Madhu</phrase> <phrase>Sudan</phrase> was my first advisor at MIT, and to him go my thanks for a smooth and effective introduction to the art of computer science research. To him I also owe the occasional temptation to regard all of computer science as …
<phrase>Large-scale</phrase> brain functional modularity is reflected in slow electroencephalographic rhythms across the human non-<phrase>rapid eye movement sleep</phrase> cycle <phrase>Large-scale</phrase> brain functional networks (measured with <phrase>functional magnetic resonance imaging</phrase>, fMRI) are organized into separated but interacting modules, an architecture supporting the integration of distinct dynamical processes. In this work we study how the aforementioned modular architecture changes with the progressive loss of vigilance occurring in the descent to <phrase>deep sleep</phrase> and we examine the relationship between the ensuing slow electroencephalographic rhythms and <phrase>large-scale</phrase> network modularity as measured with fMRI. Graph theoretical methods are used to analyze functional connectivity graphs obtained from fifty-five participants at wakefulness, light and <phrase>deep sleep</phrase>. Network modularity (a measure of functional <phrase>segregation</phrase>) was found to increase during deeper <phrase>sleep stages</phrase> but not in light sleep. By endowing functional networks with dynamical properties, we found a direct link between increased electroencephalographic (EEG) delta power (1-4 Hz) and a breakdown of inter-modular connectivity. Both EEG slowing and increased network modularity were found to quickly decrease during <phrase>awakenings</phrase> from <phrase>deep sleep</phrase> to wakefulness, in a highly <phrase>coordinated</phrase> fashion. Studying the modular structure itself by means of a <phrase>permutation</phrase> test, we revealed different module memberships when <phrase>deep sleep</phrase> was compared to wakefulness. Analysis of node roles in the modular structure revealed an increase in the number of locally well-connected nodes and a decrease in the number of globally well-connected hubs, which hinders interactions between separated functional modules. Our <phrase>results reveal</phrase> a well-defined sequence of changes in brain modular organization occurring during the descent to sleep and establish a close parallel between modularity alterations in <phrase>large-scale</phrase> functional networks (accessible through whole brain fMRI recordings) and the slowing of <phrase>scalp</phrase> oscillations (visible on EEG). The observed re-<phrase>arrangement</phrase> of connectivity might play an important role in the processes underlying loss of vigilance and sensory awareness during <phrase>deep sleep</phrase>.
Learning Detectors from Large Datasets for Object Retrieval in <phrase>Video Surveillance</phrase> —We address the problem of learning robust and efficient multi-view object detectors for surveillance video indexing and retrieval. Our philosophy is that effective solutions for this problem can be obtained by learning detectors from huge amounts of <phrase>training data</phrase>. Along this research direction, we propose a novel approach that consists of strategically partitioning the <phrase>training set</phrase> and learning a large array of complementary, compact, deep <phrase>cascade</phrase> detectors. At test time, given a video sequence captured by a fixed camera, a small number of detectors is automatically selected per image location. We demonstrate our approach on the problem of vehicle detection in challenging surveillance scenarios, using a large training dataset composed of around one million images. Our system runs at an impressive average rate of 125 frames per second on a conventional <phrase>laptop</phrase> computer.
An Effect of Dopamine Depletion on <phrase>Decision-making</phrase>: The Temporal Coupling of Deliberation and Execution When a decision between alternative actions has to be made, the <phrase>primate</phrase> brain is able to uncouple motor execution from mental deliberation, providing time for higher <phrase>cognitive processes</phrase> such as remembering and reasoning. The mental deliberation leading to the decision and the motor execution applying the decision are likely to involve different neuronal circuits linking the <phrase>basal ganglia</phrase> and the <phrase>frontal cortex</phrase>. Behavioral and physiological studies in <phrase>monkeys</phrase> indicate that dopamine depletion may result in a loss of functional <phrase>segregation</phrase> between these circuits, hence, in interference between the deliberation and execution processes. To test this hypothesis in humans, we analyzed the movements of parkinsonian patients in a go/no-go task, contrasting periods of uncertainty with periods of knowledge about the rule to be applied. Two groups of patients were compared to healthy subjects: one group was treated with dopaminergic medication and the other with <phrase>deep brain stimulation</phrase>; both groups were also tested without any treatment. In healthy subjects, the movement time was unaffected by uncertainty. In untreated patients, the movement time increased with uncertainty, reflecting interference between deliberation and execution processes. This interference was fully corrected with dopaminergic medication but was unchanged with <phrase>deep brain stimulation</phrase>. Moreover, decision-related hesitations were detectable in the movements of dopamine-depleted patients, revealing a temporal coupling of deliberation and execution. We suggest that such coupling may be related to the loss of dopamine-mediated functional <phrase>segregation</phrase> between <phrase>basal ganglia</phrase> circuits processing different stages of <phrase>goal-directed behavior</phrase>.
Automated Software Test Tool 1 OBJECTIVE The objective of the Graduate project described below is to introduce the development of a software tool which will be used to automate the <phrase>testing process</phrase> of a JPL-specific set of <phrase>software programs</phrase>. 2 INTRODUCTION The Multi-mission Ground System Office (MGSO), which is part of the <phrase>Jet Propulsion Laboratory</phrase> (JPL) organization, produces a multiple set of core <phrase>software programs</phrase> to assist in the generation of flight sequences that are uplinked to spacecraft through the <phrase>Deep Space Network</phrase> (DSN). MGSO develops these <phrase>software programs</phrase> b y collecting all the common requirements from different JPL/NASA projects. Upon delivery of the MGSO core software, each project modifies the program and tailors it to their specific needs and requirements by manipulating the necessary files. These programs are inter-linked together. For instance, the output of one software program is an input to another, in addition to passing initialization files such as the command database and flight rules. A Sequence Integration Engineer (SIE) may generate the initial input file by using one of these software tools. One can look at these programs as an " operating system " of the spacecraft, but with some differences. Consider the following: when a UNIX command directive such as " 1s " is entered at the <phrase>command line</phrase>, the result is a list of the current working directory. However, the steps involved in executing this command directive occur within the operating system and are transparent to the user. The " 1s " command, after some translations, is converted into binary, loaded into CPU memory, and then executed. A similar process occurs on the spacecraft which carries the computer system (spacecraft brain) onboard. Due to size and weight limitations, there is <a limited storage (<phrase>hard disk</phrase>) on board the spacecraft computer s ystern. Therefore, only a portion of the operating system is installed onboard, while the remaining portion remains in the ground system. In other words, the command translations and binary conversions remain in the ground system, and then command bits are uplinked to the spacecraft for command processing and execution. These programs are large in size and complexity. Many files could be manipulated during the process of adaptation and, therefore, each software program must be tested at the unit level. Also, other initialization files are created during the adaptation phase either manually, by software or combination of both. These files must also be tested for completeness and correctness. …
<phrase>Low-frequency</phrase> <phrase>perturbation theory</phrase> in <phrase>eddy-current</phrase> non-destructive evaluation A method is presented by which series solutions for the impedance change in an <phrase>eddy-current</phrase> test probe due to closed cracks in a non-magnetic, conducting half-space can be derived at <phrase>low frequency</phrase>. The series solution is applicable for flaws whose dimensions are much smaller than the electromagnetic skin-depth. The problem is formulated using an approach in which the flaw is represented by an equivalent distribution of current dipoles. The <phrase>electric field</phrase> scattered by the flaw is then written as an integral, over the flaw, of the product of the <phrase>dipole</phrase> density distribution and an appropriate <phrase>Green's function</phrase>. Terms in the series expansion for the <phrase>dipole</phrase> density are calculated by solving the <phrase>integral equation</phrase> at each order in the chosen small parameter, using <phrase>perturbation theory</phrase> and a dual <phrase>integral equation</phrase> method. The impedance change due to the crack is then calculated from the <phrase>dipole</phrase> distribution using the reciprocity theorem. Example solutions are given for semi-circular surface-breaking cracks and for long, uniformly deep surface-breaking cracks. Results are compared with other analytical solutions and the predictions of an independent numerical scheme, and very good agreement is observed.
A heuristic for thermal-safe SoC <phrase>test scheduling</phrase> 1 <phrase>High temperature</phrase> has become a technological barrier to the testing of <phrase>high performance</phrase> systems-on-chip, especially when <phrase>deep submicron technologies</phrase> are employed. In order to reduce test time while keeping the temperature of the cores under test within a safe range, thermal-aware <phrase>test scheduling</phrase> techniques are required. In this paper, we address the test time minimization problem as how to generate the shortest test schedule such that the temperature limits of individual cores and the limit on the test-bus bandwidth are satisfied. In order to avoid overheating during the test, we partition <phrase>test sets</phrase> into shorter test sub-sequences and add cooling periods in between, such that continuously applying a test sub-sequence will not drive the core temperature going beyond the limit. Further more, based on the test partitioning scheme, we interleave the test sub-sequences from different <phrase>test sets</phrase> in such a manner that a cooling period reserved for one core is utilized for the test transportation and application of another core. We have proposed a heuristic to minimize the test application time by exploring alternative test partitioning and interleaving schemes with variable length of test sub-sequences and cooling periods. <phrase>Experimental results</phrase> have shown the efficiency of the proposed heuristic.
Transfer of conflict and cooperation from experienced games to new games: a <phrase>connectionist</phrase> model of learning The question of whether, and if so how, learning can be transfered from previously experienced games to novel games has recently attracted the attention of the experimental <phrase>game theory</phrase> literature. Existing research presumes that learning operates over actions, beliefs or decision rules. This study instead uses a <phrase>connectionist</phrase> approach that learns a direct mapping from game payoffs to a <phrase>probability distribution</phrase> over own actions. Learning is operationalized as a backpropagation rule that adjusts the weights of <phrase>feedforward neural networks</phrase> in the direction of increasing the probability of an agent playing a <phrase>myopic</phrase> best response to the last game played. One advantage of this approach is that it expands the scope of the model to any possible n × n normal-form game allowing for a comprehensive model of transfer of learning. Agents are exposed to games drawn from one of seven classes of games with significantly different strategic characteristics and then forced to play games from previously unseen classes. I find significant transfer of learning, i.e., behavior that is path-dependent, or conditional on the previously seen games. Cooperation is more pronounced in new games when agents are previously exposed to games where the incentive to cooperate is stronger than the incentive to compete, i.e., when individual incentives are aligned. Prior exposure to <phrase>Prisoner's dilemma</phrase>, zero-sum and discoordination games led to a significant decrease in realized payoffs for all the game classes under investigation. A distinction is made between superficial and <phrase>deep transfer</phrase> of learning both-the former is driven by superficial payoff similarities between games, the latter by differences in the incentive structures or strategic implications of the games. I examine whether agents learn to play the <phrase>Nash equilibria</phrase> of games, how they select amongst multiple equilibria, and whether they transfer <phrase>Nash equilibrium</phrase> behavior to unseen games. Sufficient exposure to a strategically heterogeneous set of games is found to be a necessary condition for <phrase>deep learning</phrase> (and transfer) across game classes. Paradoxically, superficial transfer of learning is shown to lead to better outcomes than <phrase>deep transfer</phrase> for a wide range of game classes. The <phrase>simulation results</phrase> corroborate important experimental findings with human subjects, and make several novel predictions that can be tested experimentally.
Mass detection in digital breast tomosynthesis: <phrase>Deep convolutional neural network</phrase> with <phrase>transfer learning</phrase> from <phrase>mammography</phrase>. PURPOSE Develop a <phrase>computer-aided</phrase> detection (CAD) system for masses in digital breast tomosynthesis (DBT) volume using a <phrase>deep convolutional neural network</phrase> (DCNN) with <phrase>transfer learning</phrase> from mammograms.   METHODS A <phrase>data set</phrase> containing 2282 digitized film and digital mammograms and 324 DBT volumes were collected with IRB approval. The mass of interest on the images was marked by an experienced breast <phrase>radiologist</phrase> as reference standard. The <phrase>data set</phrase> was partitioned into a <phrase>training set</phrase> (2282 mammograms with 2461 masses and 230 DBT views with 228 masses) and an independent <phrase>test set</phrase> (94 DBT views with 89 masses). For DCNN training, the region of interest (ROI) containing the mass (true positive) was extracted from each image. <phrase>False positive</phrase> (FP) ROIs were identified at prescreening by their previously developed CAD systems. After data augmentation, a total of 45 072 mammographic ROIs and 37 450 DBT ROIs were obtained. Data normalization and reduction of non-uniformity in the ROIs across heterogeneous data was achieved using a background correction method applied to each ROI. A DCNN with four convolutional layers and three <phrase>fully connected</phrase> (FC) layers was first trained on the <phrase>mammography</phrase> data. Jittering and dropout techniques were used to reduce overfitting. After training with the mammographic ROIs, all weights in the first three convolutional layers were frozen, and only the last convolution layer and the FC layers were randomly initialized again and trained using the DBT training ROIs. The authors compared the performances of two CAD systems for mass detection in DBT: one used the DCNN-<phrase>based approach</phrase> and the other used their previously developed feature-based approach for FP reduction. The prescreening stage was identical in both systems, passing the same set of mass candidates to the FP reduction stage. For the feature-based CAD system, 3D clustering and <phrase>active contour</phrase> method was used for segmentation; morphological, gray level, and <phrase>texture features</phrase> were extracted and merged with a linear <phrase>discriminant</phrase> classifier to score the detected masses. For the DCNN-based CAD system, ROIs from five consecutive slices centered at each candidate were passed through the trained DCNN and a mass likelihood score was generated. The performances of the CAD systems were evaluated using free-response <phrase>ROC</phrase> curves and the performance difference was analyzed using a non-parametric method.   RESULTS Before <phrase>transfer learning</phrase>, the DCNN trained only on mammograms with an <phrase>AUC</phrase> of 0.99 classified DBT masses with an <phrase>AUC</phrase> of 0.81 in the DBT <phrase>training set</phrase>. After <phrase>transfer learning</phrase> with DBT, the <phrase>AUC</phrase> improved to 0.90. For breast-based CAD detection in the test set, the sensitivity for the feature-based and the DCNN-based CAD systems was 83% and 91%, respectively, at 1 FP/DBT volume. The difference between the performances for the two systems was <phrase>statistically significant</phrase> (<phrase>p-value</phrase> < 0.05).   CONCLUSIONS The image patterns learned from the mammograms were transferred to the mass detection on DBT slices through the DCNN. This study demonstrated that large <phrase>data sets</phrase> collected from <phrase>mammography</phrase> are useful for developing new CAD systems for DBT, alleviating the problem and effort of collecting entirely new large <phrase>data sets</phrase> for the new modality.
Testing in <phrase>Nanometer Technologies</phrase> The last 25 years have been a very exciting time for the people involved in testing. As an industry we had a very difficult time generating tests for boards which had only 1000 <phrase>logic gates</phrase> on them with packages which had only a few <phrase>logic gates</phrase> per module. Because of these difficulties a number of people started to look for different approaches to testing. It was clear to many that automatic <phrase>test generation</phrase> for sequential networks could not keep up with the rate of increasing network size. This resulted in many changes in the way designs were done. This increase in <phrase>gate count</phrase> resulted in the development of the area of <phrase>Design for Testability</phrase>. Test in these 25 years was driven by the increase in <phrase>gate count</phrase> coupled with the inability of automatic sequential <phrase>test generation</phrase> to keep pace. Today we are looking at an era before us which also has the <phrase>gate count</phrase> increasing at virtually the same rate. The <phrase>Design for Testability</phrase> techniques such as Full Scan, LSSD, etc. seems to be well in place. However, there is a significant difference brought on by the technology developments facing us. The onset of <phrase>deep sub-micron</phrase> (now currently alluded to as Nanometer Technology) is changing the way chips are being designed and manufactured. Because of the large capacity of these new chips, plus the expense of new designs, <phrase>embedded systems</phrase> are setting the pace for today and the future. New problems are arising that are driving <phrase>design automation</phrase> to integrate all the tools that are needed to successfully take a design from concept to reality in this new design environment. Test is one part of this process that is getting significant attention. An area once classified as a "back end" process in the <phrase>design flow</phrase> is moving closer to the "<phrase>front end</phrase>". Design methodologies are incorporating test-related structures in the beginning of the <phrase>design cycle</phrase>. In addition, standards to manage the test complexity of these large designs are being proposed. For example, IEEE P1500 is working towards defining a structure for embedded cores such that tests can be delivered to these cores. This alone is a strong challenge for the Test Community. It is clear that design and testing of <phrase>Embedded Systems</phrase> is the key challenge to the Test Community as we face these new technologies. This includes both the tools that are required to test these designs and the …
The New CCSDS <phrase>Image Compression</phrase> Recommendation 1 Process both frame and non-frame (push-broom) data 2 Offer adjustable coded data rate or image quality (up to lossless) 3 Accommodate from 4-bit to 16-bit input pixels 4 Provide real-time processing with space qualified electronics (≥20 Msamples/<phrase>sec</phrase>, ≤1 <phrase>watt</phrase>/Msamples/<phrase>sec</phrase>, based on year 2000 space electronics technology) 5 Require minimal ground operation 6 Limit the effects of a <phrase>packet loss</phrase> to a small region of the image. Abstract—The Consultative Committee for Space Data Systems (CCSDS) <phrase>data compression</phrase> working group has recently adopted a recommendation for image <phrase>data compression</phrase>, with a final release expected in 2005. The algorithm adopted in the recommendation consists of a two-dimensional <phrase>discrete wavelet transform</phrase> of the image, followed by progressive bit-plane coding of the transformed data. The algorithm can provide both lossless and <phrase>lossy compression</phrase>, and allows a user to directly control the compressed data volume or the fidelity with which the wavelet-transformed data can be reconstructed. The algorithm is suitable for both frame-based <phrase>image data</phrase> and <phrase>scan-based</phrase> sensor data, and has applications for near-Earth and deep-<phrase>space missions</phrase>. The standard will be accompanied by <phrase>free software</phrase> sources on a future <phrase>web site</phrase>. An <phrase>Application-Specific Integrated Circuit</phrase> (ASIC) implementation of the compressor is currently under development. This paper describes the compression algorithm along with the requirements that drove the selection of the algorithm. Performance results and comparisons with other compressors are given for a test set of space images.
<phrase>Machine Learning</phrase> Paradigms for <phrase>Speech Recognition</phrase>: An Overview —<phrase>Automatic Speech Recognition</phrase> (ASR) has historically been a driving force behind many <phrase>machine learning</phrase> (ML) techniques, including the ubiquitously used <phrase>hidden Markov model</phrase>, discriminative learning, structured sequence learning, Bayesian learning, and adaptive learning. Moreover, ML can and occasionally does use ASR as a large-scale, realistic application to rigorously test the effectiveness of a given technique, and to inspire new problems arising from the inherently sequential and dynamic nature of speech. On the other hand, even though ASR is available commercially for some applications, it is largely an unsolved problem—for almost all applications, the performance of ASR is not on par with human performance. New insight from modern ML methodology shows <phrase>great promise</phrase> to advance the state-of-the-art in ASR technology. This overview article provides readers with an overview of modern ML techniques as utilized in the current and as relevant to future ASR research and systems. The intent is to foster further cross-<phrase>pollination</phrase> between the ML and ASR communities than has occurred in the past. The article is organized according to the major ML paradigms that are either popular already or have potential for making significant contributions to ASR technology. The paradigms presented and elaborated in this overview include: generative and discriminative learning; supervised, unsupervised, <phrase>semi-supervised</phrase>, and <phrase>active learning</phrase>; adaptive and multi-task learning; and Bayesian learning. These learning paradigms are motivated and discussed in the context of ASR technology and applications. We finally present and analyze <phrase>recent developments</phrase> of <phrase>deep learning</phrase> and learning with sparse representations, focusing on their direct relevance to advancing ASR technology.
Adaptive <phrase>Robust Tracking</phrase> Control of <phrase>Deep Sea</phrase> Manipulator: Theory and Experiments Click here to enter text. Abstract The underwater hydraulic manipulator's challenging working condition, such as the unknown payload, varying speed, and hydraulic actuator's dynamics, makes common controller invalid. In this paper, based on the <phrase>nonlinear control</phrase> theory, the adaptive <phrase>robust control</phrase> method is proposed to enhance the joint tracking accuracy, the controller can also compensate the interactive dynamic effects between manipulator links. The deep-sea manipulators are just equipped with angular sensors, so an observer which can provide the smooth <phrase>angular velocity</phrase> estimation is designed. By using the Lyapunov approach, the proposed controller can be proved asymptotically stable for <phrase>trajectory tracking</phrase>. The comparative experiments are conducted on a <phrase>deep-sea</phrase> hydraulic manipulator, experiment results show the <phrase>control algorithm</phrase> could provide a fast, <phrase>high accuracy</phrase> tracking, and guarantee the tracking performance when subjected to payload change or different reference speed. Introduction In the deep sea exploration, hydraulic manipulators are commonly used, especially in work-class ROVs, hydraulic manipulators have much higher <phrase>power density</phrase>, <phrase>stiffness</phrase> and don't require thick protection shell. Most of the deep-sea manipulators are master-slave type, operator on the deck manipulates the master arm to guide the slave arm performing some task, such as connecting the oil pipeline, underwater equipment maintenance. So the accuracy of slave arm is a key indicator of system performance, a fast response and <phrase>high accuracy</phrase> hydraulic manipulator could greatly promote operator's work efficiency and reduce the operating mistakes. But the manipulator's working conditions are full of challenges, for example, when a manipulator connects the oil pipeline, the connectors are heavy and commonly in different weight, and desired joint speed is unpredictable, the common controller's accuracy couldn't meet requirements, so researches in this field is critical. Hydraulic system is nonlinear and difficult to control due to the fluid compressibility, control valve's dead band property. To guarantee the tracking performance, many control schemes were brought up. <phrase>Adaptive control</phrase> greatly enhances the system's robustness, which has been tested in many research fields, this algorithm can estimate the unknown parameter, and eventually, achieve zero <phrase>steady state</phrase> tracking. But it's designed to deal with a system with just parameter uncertainty. To overcome the shortcoming of <phrase>adaptive control</phrase>, the adaptive <phrase>robust control</phrase> (ARC) of hydraulic manipulator was brought up. This <phrase>control algorithm</phrase> preserves the high tracking accuracy, parameter adaptability of AC, and disturbance rejection property of <phrase>DRC</phrase>. When parameters change, the <phrase>parameter estimation</phrase> part of ARC controller will detect the change, and make …
Identifying Defects in <phrase>Deep-submicron</phrase> <phrase>Cmos Ics</phrase> Given the oft-<phrase>cited</phrase> difficulty of testing modern <phrase>integrated circuits</phrase>, the fact that <phrase>CMOS ICs</phrase> lend themselves to <phrase>IDDQ testing</phrase> is a piece of good fortune. But that valuable advantage is threatened by the <phrase>rush</phrase> of <phrase>semiconductor technology</phrase> to smaller <phrase>feature sizes</phrase> and faster, denser circuits, in line with the Semiconductor Industry Association's (<phrase>SIA</phrase>) Roadmap-its forecast for the CMOS IC industry. With safety margins for reliability, test, <phrase>failure analysis</phrase>, and <phrase>design verification</phrase> shrinking, it would be a shame to give up the IDDQ technique-and luckily, we may not have to. Steps can be taken to maintain its applicability as we <phrase>rush</phrase> deeper into the submicron regime. Before discussing them, however, a brief discussion of <phrase>IDDQ testing</phrase> seems to be in order. We will first examine why the <phrase>IDDQ test</phrase> serves several interests, then describe the challenge posed by 0.35-0.07-µm transistor geometries, and finally propose several solutions. CMOS IC <phrase>power supply</phrase> current can be <phrase>amperes</phrase> during logic state transitions, but only nanoamperes during the <phrase>steady state</phrase>, or quiescent, portion of the <phrase>clock cycle</phrase>. This low quiescent <phrase>power supply</phrase> current, known as IDDQ, is what gives CMOS its traditional <phrase>low-power</phrase> edge over its technology competitors. But it does more than that. Engineers in design, fabrication, and test have learned to use this low <phrase>quiescent current</phrase> as a sensitive test to identify defects, which often prove to be the reason for customer returns, whether as test escapes or reliability failures. In fact, test escape levels below 200 parts per million have recently been attainable only by adding <phrase>IDDQ testing</phrase>. The technique has also eliminated the need for burn-in for some mature product lines. More, IDDQ measurements speed <phrase>failure analysis</phrase> by providing current-voltage signatures and temporal characteristics. Detecting defects with current Current is a more effective parameter than voltage for <phrase>defect detection</phrase> in <phrase>CMOS ICs</phrase>, although both are necessary for complete testing. The simple logic circuit with a bridging defect <phrase>shown in Fig</phrase>. 1 illustrates how IDDQ increases in the presence of a flaw. Bridging defects and certain open-circuit defects typically elevate the nanoampere levels of a normal circuit by two to seven <phrase>orders of magnitude</phrase>, providing very sensitive <phrase>defect detection</phrase>. If ICs are correctly designed and fabricated for low background current, then IDDQ is a relatively simple measurement with many benefits. But as background IDDQ rises, for whatever the reason, the effectiveness of <phrase>IDDQ testing</phrase> diminishes. For optimum detection of manufacturing defects, the <phrase>defect-free</phrase> …
Assessing Computational Methods for <phrase>Transcription Factor</phrase> Target Gene Identification Based on <phrase>ChIP-seq</phrase> Data <phrase>Chromatin</phrase> <phrase>immunoprecipitation</phrase> coupled with <phrase>deep sequencing</phrase> (<phrase>ChIP-seq</phrase>) has great potential for elucidating transcriptional networks, by measuring <phrase>genome-wide</phrase> binding of transcription factors (TFs) at <phrase>high resolution</phrase>. Despite the precision of these experiments, identification of genes directly regulated by a <phrase>TF</phrase> (target genes) is not trivial. Numerous target gene scoring methods have been used in the past. However, their suitability for the task and their performance remain unclear, because a thorough comparative assessment of these methods is still lacking. Here we present a systematic evaluation of computational methods for defining <phrase>TF</phrase> targets based on <phrase>ChIP-seq</phrase> data. We validated predictions based on 68 <phrase>ChIP-seq</phrase> studies using a wide range of genomic expression data and functional information. We demonstrate that peak-to-gene assignment is the most crucial step for correct target gene prediction and propose a parameter-free method performing most consistently across the evaluation tests.
<phrase>Convolutional Neural Network</phrase> and <phrase>Convex Optimization</phrase> This report shows that the performance of <phrase>deep convolutional neural network</phrase> can be improved by incorporating <phrase>convex optimization</phrase> techniques. First, we find that the sub-models learned by dropout can be more effectively combined by solving a convex problem. Also, we generalize this idea to models that are not trained by dropout. Compared to traditional methods, we get an improvement of 0.22% and 0.76% test accuracy on CIFAR10 dataset. Second, we investigate the performance for different loss functions borrowed from the <phrase>convex optimization</phrase> community and find that selecting loss functions matters a lot. We also implement a novel loss based on the idea of One-Versus-One SVM, which has never been explored in the literature. Experiment shows that it can give performance comparable to the standard cross-entropy loss, without being fully tuned.
Utilizing Fault Cases for Supporting <phrase>Fault Diagnosis</phrase> Tasks When building large and complex systems, like satellites, all sorts of risks have to be managed if it were to be successful. Although there have been various techniques for <phrase>fault diagnosis</phrase>, applying them requires both deep <phrase>domain knowledge</phrase> and extensive effort of domain experts. In this paper, we present an approach to support <phrase>fault diagnosis</phrase> at relatively <phrase>low cost</phrase>. The approach utilizes fault cases experienced while testing of the system. We show the effectiveness of the approach by applying it to a case taken from a satellite development project.
Large Corpus-based Semantic <phrase>Feature Extraction</phrase> for Pronoun Coreference <phrase>Semantic information</phrase> is a very important factor in coreference resolution. The combination of large corpora and 'deep' analysis procedures has made it possible to acquire a range of <phrase>semantic information</phrase> and apply it to this task. In this paper , we generate two statistically-based semantic features from a large corpus and measure their influence on pronoun coreference. One is contextual compatibility , which decides if the antecedent can be used in the anaphor's context; the other is role pair, which decides if the actions asserted of the antecedent and the anaphor are likely to apply to the same entity. We apply a semantic labeling system and a baseline coreference system to a large corpus to generate semantic patterns and convert them into features in a MaxEnt model. These features produce an absolute gain of 1.5% to 1.7% in resolution accuracy (a 6% reduction in errors). To understand the limitations of these features, we also extract patterns from the test corpus, use these patterns to train a coreference model, and examine some of the cases where coreference still fails. We also compare the performance of patterns extracted from <phrase>semantic role</phrase> labeling and syntax.
Bottleneck features based on gammatone frequency cepstral coefficients Recent work demonstrates impressive success of the bottleneck (BN) feature in <phrase>speech recognition</phrase>, particularly with <phrase>deep networks</phrase> plus appropriate pre-training. A widely admitted advantage associated with the <phrase>BN feature</phrase> is that the network structure can learn multiple environmental conditions with abundant <phrase>training data</phrase>. For tasks with limited <phrase>training data</phrase>, however, this multi-condition training is unavailable, and so the networks tend to be over-fitted and sensitive to acoustic condition changes. A possible solution is to base the BN features on a channel-robust primary feature. In this paper, we propose to derive the <phrase>BN feature</phrase> based on Gammatone frequency cepstral coefficients (GFCCs). The GFCC feature has shown <phrase>nice</phrase> robustness against acoustic change, due to its capability of simulating the auditory system of humans. The idea is to integrate the advantage of the GFCC feature in acoustic robustness and the advantage of the <phrase>BN feature</phrase> in signal representation, so that the <phrase>BN feature</phrase> can be improved in the condition of mismatched training/test channels. This is particularly useful for <phrase>small-scale</phrase> tasks for which the training data are often limited. The experiments are conducted on the WSJCAM0 database, where the test utterances are mixed with noises at various SNR levels to simulate the channel change. The results confirm that the GFCC-based <phrase>BN feature</phrase> is much more robust than the BN features based on the MFCC and the PLP. Furthermore, the primary GFCC feature and the GFCC-based <phrase>BN feature</phrase> can be concatenated, leading to a more robust combined feature which provides considerable performance gains in all the tested noise conditions.
Generalized Conflict Learning for Hybrid Discrete/linear Optimization Generalized Conflict Learning for Hybrid Discrete/linear Optimization Conflict-directed <phrase>search algorithms</phrase> have formed the core of practical, model-<phrase>based reasoning</phrase> systems for the last three decades. In many of these applications there is a series of discrete constraint <phrase>optimization problems</phrase> and a conflict-directed <phrase>search algorithm</phrase>, which uses conflicts in the forward search step to focus search away from known infeasibilities and towards the optimal solution. In the <phrase>arena</phrase> of <phrase>model-based</phrase> <phrase>autonomy</phrase>, discrete systems, like <phrase>deep space</phrase> probes, have given way to more agile systems, such as <phrase>coordinated</phrase> vehicle control, which must robustly control their continuous dynamics. Controlling these systems requires optimizing over continuous, as well as discrete variables, using linear and non-linear as well as logical constraints. This paper explores the development of algorithms for solving hybrid discrete/linear <phrase>optimization problems</phrase> that use conflicts in the forward search direction, generalizing from the conflict-directed <phrase>search algorithms</phrase> of model-<phrase>based reasoning</phrase>. We introduce a novel algorithm called Generalized Conflict-directed <phrase>Branch and Bound</phrase> (<phrase>GCD</phrase>-BB). <phrase>GCD</phrase>-BB extends traditional <phrase>Branch and Bound</phrase> (B&B), by first constructing conflicts from nodes of the search tree that are found to be infeasible or sub-optimal, and then by using these conflicts to guide the forward search away from known infeasible and sub-optimal states. We evaluate <phrase>GCD</phrase>-BB empirically on a range of <phrase>test problems</phrase> of <phrase>coordinated</phrase> air vehicle control. <phrase>GCD</phrase>-BB demonstrates a substantial improvement in performance compared to a traditional B&B algorithm, applied to either disjunctive linear programs or an equivalent binary integer program encoding. Acknowledgments First of all, I would like to thank my advisor, <phrase>Brian Williams</phrase>, for his guidance and encouragement on my research and working so hard with me to make the thesis deadline. I would like to thank my caring roommates, Caroline Maier and Jit Kee Chin, especially Caroline, for feeding and taking care of me when I was overwhelmed by work, and cheering me up when I was down. They are not just my roommates; they are my family. I would like to thank my parents, for their unconditional love and support, and for their care and patience during the time when I was stuck in China for 9 months. I would like to thank MERS group, for making our lab a comfortable and stimulating part of my life. Especially, Thomas Léauté, for providing <phrase>test problems</phrase> for my algorithm, and giving immediate and helpful comments on my thesis, Lars <phrase>Blackmore</phrase>, for the insightful discussions we had, the comments he gave on the early draft of my …
<phrase>Deep-level</phrase> acoustic-to-articulatory mapping for DBN-<phrase>HMM based</phrase> phone recognition In this paper we experiment with methods based on <phrase>Deep Belief Networks</phrase> (DBNs) to recover measured articulatory data from speech acoustics. Our acoustic-to-articulatory mapping (AAM) processes go through multi-layered and hierarchical (i.e., deep) representations of the acoustic and the articulatory domains obtained through unsupervised learning of DBNs. The unsupervised learning of DBNs can serve two purposes: (i) pre-training of the <phrase>Multi-layer</phrase> Perceptrons that perform AAM; (ii) transformation of the articulatory domain that is recovered from acoustics through AAM. The recovered artic-ulatory features are combined with MFCCs to compute phone posteriors for phone recognition. Tested on the MOCHA-TIMIT corpus, the recovered articulatory features, when combined with MFCCs, lead to up to a remarkable 16.6% relative phone error reduction w.r.t. a phone recognizer that only uses MFCCs.
A corpus of clinical narratives annotated with <phrase>temporal information</phrase> Clinical reports often include descriptions of events in the patient's <phrase>medical history</phrase>, as well as explicit or implicit temporal information about these events. We are working towards applying deep <phrase>Natural Language Processing</phrase> tools towards understanding such narratives. This requires both the extraction and classification of the relevant events, and the placing of those events in time, or at least in relation to one another. Although several corpora of news data exist that have been annotated using the TimeML schema, similar corpora of clinical reports are not readily available.  In this paper we report on the design of a small corpus and the annotation schema we developed, based on data from the fourth i2b2/VA challenge. These data include, among others, annotations for medical problems, tests, and treatments in clinical reports from several healthcare institutions. We have selected a subset of clinical reports and added annotations similar to those used in the TempEval tasks for the annotation of events, time expressions and temporal relations for the news domain. The annotations have been made freely available to the <phrase>research community</phrase>.
CALMsystem: A Conversational Agent for Learner Modelling This paper describes a system which incorporates <phrase>natural language</phrase> technologies, database manipulation and educational theories in order to offer learners a Negotiated <phrase>Learner Model</phrase>, for integration into an <phrase>Intelligent Tutoring System</phrase>. The system presents the learner with their <phrase>learner model</phrase>, offering them the opportunity to compare their own beliefs regarding their capabilities with those inferred by the system. A conversational agent, or " chatbot " has been developed to allow the learner to negotiate over the representations held about them using <phrase>natural language</phrase>. The system aims to support the metacognitive goals of self-assessment and reflection, which are increasingly seen as key to learning and are being incorporated into <phrase>UK</phrase> educational policy. The paper describes the design of the system, and reports a user trial, in which the chatbot was found to support users in increasing the accuracy of their self-assessments, and in reducing the number of discrepancies between system and user beliefs in the <phrase>learner model</phrase>. Some lessons learned in the development have been highlighted and <phrase>future research</phrase> and experimentation directions are outlined. <phrase>Intelligent Tutoring Systems</phrase> (ITS) provide their users with an adaptive <phrase>learning environment</phrase>, with personalized tutoring and testing customised to meet the needs of the individual student. This adaptation is based on the contents of the <phrase>learner model</phrase>, a representation of the student's knowledge, gaps in understanding and misconceptions. Traditional ITSs have not made the contents of the <phrase>learner model</phrase> visible to the learner. However, it has been argued that an Open <phrase>Learner Model</phrase> (i.e. one that can be inspected by the student) can offer opportunities for learner reflection, <phrase>metacognition</phrase> and <phrase>deep learning</phrase>, which may enhance learning (e.g. [1], [2], [3], [4] and [5]), as well as improving the accuracy of the <phrase>learner model</phrase>. Educational theorists have emphasised the importance of learner reflection ([6], [7] and [8]). Some researchers have developed Open Learner Models (<phrase>OLM</phrase>) that
<phrase>Model-based</phrase> <phrase>Performance Assessment</phrase> <phrase>Model-based</phrase> <phrase>Performance Assessment</phrase> <phrase>Model-based</phrase> <phrase>Performance Assessment</phrase> <phrase>Performance-assessment</phrase> Learning Models <phrase>Problem Solving</phrase> Self-regulation Communication Content Understanding Collaboration Learning The findings and opinions expressed in this report do not reflect the position or policies of t h e There is concern in many quarters about the type and level of knowledge our children are acquiring in schools. Study) assessments, many students do poorly. Further, there is a growing concern in American business and industry (U.S. 1992) that young people entering the workforce are not adequately prepared for the world of work. These concerns are a major source of what is now almost a decade of effort to restructure what students are taught and the ways in which we can accurately assess their learning. For example, government policy makers have attempted to address these issues, beginning with the 1989 National achievement towards the Goals (see Goals 2000) on a national and state level, also advised that standards of learning be set so that studentsÕ progress towards the Goals could be determined. Goal 3, which states that Òall students will leave grades 4, 8, and 12 having demonstrated competency over challenging subject matter. . .Ó, was a particular focal point for the development of standards. The National Education Goals Panel also pointed to the need to define more specifically what constitutes Òchallenging subject matterÓ and ÒcompetencyÓ i n itÑfor instance, the kind of learning characterized by <phrase>higher order</phrase> thinking skills, deep content knowledge within and across subject areas, <phrase>problem-solving</phrase> abilityÑand the need to determine how that competency would be measured. Standards now have been developed for eight of the nine subject areas listed But goals for national educational achievement and resulting standards to describe those goals will not tell us how well our children are doing unless we also measure their progress in learning the content of the standards. To do this, new kinds of tests are being created, called performance assessments, in which students engage in tasks that may require significant amounts of time and i n which they are asked to Òcommunicate their understanding of content, of process <phrase>Performance assessment</phrase> also has been described by its proponents as a major strategy to assist teachers to improve the learning of their students. This piece will describe both the values ascribed to <phrase>performance assessment</phrase> and the major criticisms of assessment that have developed in the last few years of exploration. One approach, <phrase>model-based</phrase> <phrase>performance assessment</phrase>, will be described as a way to remedy and to avoid criticisms of <phrase>performance assessment</phrase>. Part …
The Inevitable Future of the Starless Core <phrase>Barnard 68</phrase> Dense, small <phrase>molecular cloud</phrase> cores have been identified as the direct progenitors of <phrase>stars</phrase>. One of the best studied examples is <phrase>Barnard 68</phrase> which is considered a prototype stable, spherical gas core, confined by a diffuse high-pressure environment. Observations of its radial density structure however indicate that <phrase>Barnard 68</phrase> should be gravitationally unstable and collapsing which appears to be inconsistent with its inferred long lifetime and stability. We argue that <phrase>Barnard 68</phrase> is currently experiencing a fatal collision with another small core which will lead to <phrase>gravitational</phrase> collapse. Despite the fact that this system is still in an early phase of interaction, our numerical simulations imply that the future <phrase>gravitational</phrase> collapse is already detectable in the outer surface density structure of the globule which mimicks the profile of a gravitationally unstable Bonnor-<phrase>Ebert</phrase> sphere. Within the next 2 × 10 5 years <phrase>Barnard 68</phrase> will condense into a <phrase>low-mass</phrase> solar-type star(s), formed in isolation, and surrounded by diffuse, hot <phrase>interstellar gas</phrase>. As witnessed in situ for <phrase>Barnard 68</phrase>, core mergers might in general play an important role in triggering <phrase>star formation</phrase> and shaping the molecular core mass distribution and by that also the <phrase>stellar</phrase> initial mass function. Subject headings: ISM: globules – ISM: clouds – ISM individual (<phrase>Barnard 68</phrase>) – <phrase>stars</phrase>: formation – <phrase>hydrodynamics</phrase> 1. INTRODUCTION <phrase>Barnard 68</phrase> is considered an excellent <phrase>test case</phrase> and the prototype of a dense <phrase>molecular cloud</phrase> core (Alves et al. 2001b). Because of its small distance (∼ 125 pc), this so called <phrase>Bok globule</phrase> /Bok & Reilly 1947) with a mass of M=2.1 M ⊙ , contained within a region of R = 12,500 <phrase>AU</phrase> has been observed with unprecedented accuracy (Alves et al. 2001b; <phrase>Lada</phrase> et al. 2003; Bergin et al. 2006; Redman et al. 2006; Maret et al. 2007). Deep near-infrared dust <phrase>extinction</phrase> measurements of starlight toward individual background <phrase>stars</phrase> observed through the cloud provided a detailed 2-dimensional map of its projected surface density distribution (Fig. 1) from which a <phrase>high-resolution</phrase> density profile was derived over the entire extent of the system. The striking agreement of the inferred density structure with the theoretical solution of an <phrase>isothermal</phrase>, pressure confined, hydrostatic gas sphere (so called Bonnor-<phrase>Ebert</phrase> sphere) was interpreted as a signature that the globule is old, thermally supported and stable, with the <phrase>pressure gradient</phrase> balancing the gravita-tional force. This conclusion has received additional support from molecular line observations (<phrase>Lada</phrase> et al. 2003) that …
Influencing the Perceived Emotions of Music with Intent 2. Introduction 1. Abstract Music is an immensely powerful affective medium that pervades our everyday life. With ever advancing technology, the reproduction and application of music for emotive and information transfer purposes has never been more prevalent. In this paper we introduce a rule-based engine for influencing the perceived emotions of music. Based on empirical <phrase>music psychology</phrase>, we attempt to formalise the relationship between musical elements and their perceived emotion. We examine the modification to structural aspects of music to allow for a graduated transition between perceived emotive states. This mechanism is intended to provide music reproduction systems with a finer grained control over this affective medium; where perceived musical emotion can be influenced with intent. This intent comes from both an external application and the audience. Using a series of <phrase>affective computing</phrase> technologies, an audience's response metrics and attitudes can be incorporated to model this intent. A generative <phrase>feedback loop</phrase> is set up between the external application, the influencing process and the audience's response to this, which together shape the modification of musical structure. The effectiveness of influencing perceived musical emotion was examined in earlier work, with a small test study providing generally encouraging results. No one is sure for what end music came about, be it a biological urge [1], an offshoot of our evolving language faculty [2] or simply another mechanism for the expression of self brought about by mankind's cultural explosion some 100,000 odd <phrase>years ago</phrase> [3]. Few however would argue that it is our intense and diverse emotional capacity that allows us to create music; the at times transcendental inexplicable. Given this deep emotional connection, and the ease with which sound be reproduced with modern technology, music is now a pervasive medium found throughout everyday life. It is used in almost every form of human communication and can be heard at the cinema, on <phrase>television</phrase>, on radio, in commercials, at the <phrase>ballet</phrase>, in shopping centres, on public and <phrase>private</phrase> transport, in waiting rooms and <phrase>restaurants</phrase>, to name but a few. One study found that within any waking 2 hour period, a person had on average a 44% chance of experiencing a
Efficient parsing of spoken inputs for <phrase>human-robot interaction</phrase> — The use of deep parsers in <phrase>spoken dialogue</phrase> systems is usually subject to strong performance requirements. This is particularly the case in <phrase>human-robot interaction</phrase>, where the computing resources are limited and must be shared by many components in parallel. A real-time dialogue system must be capable of responding quickly to any given utterance, even in the presence of noisy, ambiguous or distorted input. The parser must therefore ensure that the number of analyses remains bounded at every processing step. The paper presents a practical approach to addressing this issue in the context of deep parsers designed for <phrase>spoken dialogue</phrase>. The approach is based on a word lattice parser combined with a <phrase>statistical model</phrase> for parse selection. Each word lattice is parsed incrementally, word by word, and a discriminative model is applied at each incremental step to <phrase>prune</phrase> the set of resulting partial analyses. The model incorporates a wide range of linguistic and contextual features and can be trained with a simple <phrase>perceptron</phrase>. The approach is fully implemented as part of a <phrase>spoken dialogue</phrase> system for <phrase>human-robot interaction</phrase>. Evaluation results on a Wizard-of-Oz <phrase>test suite</phrase> demonstrate significant improvements in parsing time.
0 Th World <phrase>Congress</phrase> on Structural and Multidisciplinary Optimization Probability Collectives for Solving <phrase>Truss</phrase> Structure Problems 1. Abstract The approach of Probability Collectives (PC) in the <phrase>Collective Intelligence</phrase> (<phrase>COIN</phrase>) framework is one of the emerging <phrase>Artificial Intelligence</phrase> approaches dealing with the complex problems in a distributed way. It decomposes the entire system into subsystems and treats them as a group of learning, rational and self interested agents or a <phrase>Multi-Agent System</phrase> (MAS). These agents iteratively select their strategies to optimize their individual local goal which also makes the system to achieve the global optimum. The approach of PC has been tested and validated by solving a variety of practical problems in continuous domain. This paper demonstrates the ability of PC solving 2-D space <phrase>truss</phrase> structure and 3-D <phrase>truss</phrase> structure design problems with discrete as well as continuous variables. The approach is shown to be producing competent and sufficiently robust results. The associated strengths, weaknesses are also discussed. The solution to these problems indicates that the approach of PC can be further efficiently applied to solve a variety of practical/<phrase>real world</phrase> problems. 2. 3. Introduction In the framework of <phrase>Collective Intelligence</phrase> (<phrase>COIN</phrase>), the <phrase>Artificial Intelligence</phrase> (AI) tool referred to as Probability Collectives (PC) is becoming popular for modeling and controlling distributed <phrase>Multi-Agent System</phrase> (MAS) [1-15]. It was inspired from a sociophysics viewpoint, with deep connections to <phrase>Game Theory</phrase>, <phrase>Statistical Physics</phrase>, and Optimization [1, 2]. According to [1, 2, 9-11], the key characteristics of the PC methodology such as its ability to accommodate discrete and continuous variables as well as irregular and noisy functions, tolerance to subsystem/agent failure, ability to provide sensitivity information and ability to handle uncertainty in terms of probability, use of <phrase>homotopy</phrase> function to make the solution jump out of possible <phrase>local minima</phrase>, ability to avoid the <phrase>tragedy</phrase> of <phrase>commons</phrase>, high scalability, ability to achieve unique <phrase>Nash Equilibrium</phrase>, etc. makes it a very competitive choice over other contemporary algorithms. The approach of PC has been applied in variegated areas such as <phrase>airplane</phrase> <phrase>fleet</phrase> assignment problem [12] and various cases of the Multiple Traveling Salesmen Problems (MTSPs) [4, 7], continuous constrained problems such as benchmark <phrase>test problems</phrase> [7, 9, 13-15], two variations of the <phrase>Circle</phrase> Packing Problem (CPP) [5], <phrase>Sensor Network</phrase> Coverage Problem [10] as well as <phrase>fault-tolerant</phrase> system in association with the CPP [11]. Furthermore, the segmented beam problem [8], multimodal, nonlinear and non-separable <phrase>test problems</phrase> comparing the performance with <phrase>Genetic Algorithm</phrase> (GA) [24] as well as joint optimization of the routing and <phrase>resource allocation</phrase> …
Model neural prostheses with integrated <phrase>microfluidics</phrase>: a potential intervention strategy for controlling reactive cell and tissue responses Model silicon intracortical probes with <phrase>microfluidic</phrase> channels were fabricated and tested to examine the feasibility of using diffusion-mediated delivery to deliver therapeutic agents into the volume of tissue exhibiting reactive responses to implanted devices. Three-dimensional probe structures with <phrase>microfluidic</phrase> channels were fabricated using surface micromachining and <phrase>deep reactive ion etching</phrase> (DRIE) techniques. In vitro functional tests of devices were performed using <phrase>fluorescence</phrase> <phrase>microscopy</phrase> to record the transient release of Texas Red labeled <phrase>transferrin</phrase> (TR-<phrase>transferrin</phrase>) and <phrase>dextran</phrase> (TR-<phrase>dextran</phrase>) from the microchannels into 1% w/v <phrase>agarose</phrase> gel. In vivo performance was characterized by inserting devices loaded with TR-<phrase>transferrin</phrase> into the <phrase>premotor cortex</phrase> of adult male rats. Brain sections were imaged using <phrase>confocal</phrase> <phrase>microscopy</phrase>. Diffusion of TR-<phrase>transferrin</phrase> into the <phrase>extracellular space</phrase> and uptake by cells up to 400 microm from the implantation site was observed in brain slices taken 1 h postinsertion. The reactive tissue volume, as indicated by the presence of <phrase>phosphorylated</phrase> <phrase>mitogen</phrase>-activated protein <phrase>kinases</phrase> (MAPKs), was characterized using <phrase>immunohistochemistry</phrase> and <phrase>confocal</phrase> <phrase>microscopy</phrase>. The reactive tissue volume extended 600, 800, and 400 microm radially from the implantation site at 1 h, 24 h, and 6 weeks following insertion, respectively. These results indicate that diffusion-mediated delivery can be part of an effective intervention strategy for the treatment of reactive tissue responses around chronically implanted intracortical probes.
Evolution of <phrase>Deep Brain Stimulation</phrase>: Human <phrase>Electrometer</phrase> and Smart Devices Supporting the Next Generation of Therapy. <phrase>Deep Brain Stimulation</phrase> (DBS) provides therapeutic benefit for several neuropathologies including <phrase>Parkinson's disease</phrase> (PD), epilepsy, <phrase>chronic pain</phrase>, and depression. Despite well established clinical efficacy, the mechanism(s) of DBS remains <phrase>poorly understood</phrase>. In this review we begin by summarizing the current understanding of the DBS mechanism. Using this knowledge as a framework, we then explore a specific hypothesis regarding DBS of the subthalamic nucleus (STN) for the treatment of PD. This hypothesis states that therapeutic benefit is provided, at least in part, by activation of surviving nigrostriatal dopaminergic neurons, subsequent striatal dopamine release, and resumption of striatal target cell control by dopamine. While highly controversial, we present preliminary data that are consistent with specific predications testing this hypothesis. We additionally propose that developing new technologies, e.g., human <phrase>electrometer</phrase> and <phrase>closed-loop</phrase> smart devices, for monitoring dopaminergic <phrase>neurotransmission</phrase> during <phrase>STN DBS</phrase> will further advance this treatment approach.
Autonomous Onboard Science <phrase>Data Analysis</phrase> for Comet Missions Coming years will bring several comet rendezvous missions. The <phrase>Rosetta</phrase> spacecraft arrives at Comet 67P/Churyumov–Gerasimenko in 2014. Subsequent rendezvous might include a mission such as the proposed Comet Hopper with multiple surface landings, as well as <phrase>Comet Nucleus</phrase> Sample Return (CNSR) and <phrase>Coma</phrase> Rendezvous and Sample Return (CRSR). These encounters will begin to shed light on a population that, despite several previous flybys, remains mysterious and <phrase>poorly understood</phrase>. Scientists still have little direct knowledge of interactions between the nucleus and <phrase>coma</phrase>, their variation across different <phrase>comets</phrase> or their evolution over time. Activity may change on short timescales so it is challenging to characterize with scripted <phrase>data acquisition</phrase>. Here we investigate automatic onboard <phrase>image analysis</phrase> that could act faster than round-trip light time to capture unexpected outbursts and plume activity. We describe one edge-<phrase>based method</phrase> for detect comet nuclei and plumes, and test the approach on an existing catalog of comet images. Finally, we quantify benefits to specific measurement objectives by simulating a basic plume monitoring campaign. Previous comet encounters include international flybys of 1P/<phrase>Halley</phrase>, the flyby of 81P/Wild by Stardust, the Deep Impact and NeXT encounters with 9P/<phrase>Tempel 1</phrase>, and a <phrase>Deep Space 1</phrase> flyby of 19P/Borrelly. These few encounters have already revealed a very diverse population. <phrase>Comets</phrase> vary in size by <phrase>orders of magnitude</phrase>, with most having heterogeneous texture, <phrase>albedo</phrase> and composition. Each new visit reveals features not seen in previous cases. <phrase>Tempel 1</phrase> has morphological evidence of active geologic processes including scarps and outflows [1]. Its surface undergoes continuous modification, with visible change during the years between two flybys. The EPOXI flyby of comet Hartley 2 shows <phrase>skyscraper</phrase>-size <phrase>spires</phrase>, flat featureless plains that outgas H 2 O, regions of rough and mottled texture, bands of various shapes, and diverse surface <phrase>albedo</phrase>. <phrase>Comets</phrase>' active areas range from 10-90%, changing over time and distance to the <phrase>sun</phrase>. They manifest as both localized <phrase>jets</phrase> and diffuse regions (Figure 1). Still more exotic, recently discovered " active <phrase>asteroids</phrase> " suggest that primitive ice could survive for billions of years in the inner solar system. This challenges the fundamental distinction between <phrase>comets</phrase> and <phrase>asteroids</phrase> [2]. A fundamental underlying question is the temporal evolution and driving mechanisms of comet surface activity. It is likely that comet activity changes on timescales faster than ground control's traditional command cycle and uplink interval. Faster reaction time will be important to characterize the dynamic activity profile. Additionally, future …
<phrase>Static timing analysis</phrase> considering <phrase>power supply</phrase> variations <phrase>Power supply</phrase> integrity verification has become a key concern in <phrase>high performance</phrase> designs. In deep submicron technologies, <phrase>power supply noise</phrase> can significantly increase the <phrase>circuit delay</phrase> and lead to performance failures. Traditional <phrase>static timing analysis</phrase> which applies <phrase>worst-case</phrase> voltage margins to compute <phrase>circuit delay</phrase> leads to a very conservative analysis because the worst-case drop is localized to a small area of the die. In this paper, we propose a new approach for analyzing the impact of <phrase>power supply</phrase> variations on <phrase>circuit delay</phrase>. The <phrase>circuit delay</phrase> maximization problem is formulated as a constrained non-linear <phrase>optimization problem</phrase> which takes both IR and Ldi/dt drops into account The proposed approach does not require apriori knowledge of critical paths in the circuit and can be effectively incorporated in an existing <phrase>static timing analysis</phrase> framework. The proposed method has been implemented and tested on ISCAS85 <phrase>benchmark circuits</phrase> and compared with the traditional methods for computing <phrase>worst-case</phrase> <phrase>circuit delay</phrase> under supply variations.
<phrase>Circuit Design</phrase> for <phrase>Embedded Memory</phrase> in <phrase>Low-power</phrase> <phrase>Integrated Circuits</phrase> <phrase>Circuit Design</phrase> for <phrase>Embedded Memory</phrase> in <phrase>Low-power</phrase> <phrase>Integrated Circuits</phrase> This thesis explores the challenges for integrating embedded <phrase>static random access memory</phrase> (SRAM) and <phrase>non-volatile memory</phrase>—based on <phrase>ferroelectric</phrase> <phrase>capacitor</phrase> technology—into <phrase>low-power</phrase> <phrase>integrated circuits</phrase>. First considered is the impact of <phrase>process variation</phrase> in deep-submicron technologies on SRAM, which must exhibit higher density and performance at increased levels of integration with every new semiconductor generation. Techniques to speed up the <phrase>statistical analysis</phrase> of physical memory designs by a factor of 100 to 10,000 relative to the conventional <phrase>Monte Carlo Method</phrase> are developed. The proposed methods build upon the <phrase>Importance Sampling</phrase> simulation algorithm and efficiently explore the <phrase>sample space</phrase> of transistor parameter fluctuation. <phrase>Process variation</phrase> in SRAM at <phrase>low-voltage</phrase> is further investigated experimentally with a 512kb 8T SRAM <phrase>test chip</phrase> in 45nm SOI <phrase>CMOS technology</phrase>. For active operation, an AC coupled sense amplifier and regenerative global bitline scheme are designed to operate at the limit of on current and off current separation on a single-ended SRAM bitline. The SRAM operates from 1.2 V down to 0.57 V with access times from 400ps to 3.4ns. For <phrase>standby power</phrase>, a <phrase>data retention</phrase> voltage sensor predicts the mismatch-limited minimum <phrase>supply voltage</phrase> without corrupting the contents of the memory. The <phrase>leakage power</phrase> of SRAM forces the chip designer to seek <phrase>non-volatile memory</phrase> in applications such as portable electronics that retain significant quantities of data over long durations. In this scenario, the energy cost of accessing data must be minimized. This <phrase>thesis presents</phrase> a <phrase>ferroelectric</phrase> <phrase>random access memory</phrase> (<phrase>FRAM</phrase>) prototype that addresses the challenges of sensing diminishingly small charge under conditions favorable to low access energy with a time-to-digital sensing scheme. The 1 Mb 1T1C <phrase>FRAM</phrase> fabricated in 130 nm CMOS operates from 1.5 V to 1.0 V with corresponding access energy from 19.2 pJ to 9.8 pJ per bit. Finally, the computational state of sequential elements interspersed in CMOS logic, also restricts the ability to power gate. To enable simple and fast turn-on, <phrase>ferroelectric</phrase> capacitors are integrated into the design of a <phrase>standard cell</phrase> register, whose <phrase>non-volatile</phrase> operation is made compatible with the digital <phrase>design flow</phrase>. A <phrase>test-case</phrase> circuit containing <phrase>ferroelectric</phrase> registers exhibits <phrase>non-volatile</phrase> operation and consumes less than 1.3 pJ per bit of state information and less than 10 clock cycles to save or restore with no minimum <phrase>standby power</phrase> requirement in-between active periods. Acknowledgments I would like to acknowledge the following people and entities that have been involved in my graduate experience: • Colleagues from …
Test-Time, <phrase>Run-Time</phrase>, and Simulation-Time Temporal Assertions in <phrase>RSP</phrase> For <phrase>cost-effective</phrase> prototyping, system designers should have a clear understanding of the intended use of the prototype under development. This paper describes a classification of <phrase>formal specification</phrase> (temporal) assertions used during system prototyping. The classification introduces two new classes of assertions in addition to the well-known class of test-time assertions: (i) assertions used only during simulation, and (ii) deployable assertions integrated with <phrase>run-time</phrase> <phrase>control flow</phrase>. Separating the <phrase>formal specification</phrase> into three distinct classes allows system designers to develop more effective prototypes to evaluate the different system behaviors and constraints. A prototype of a <phrase>naval</phrase> <phrase>torpedo</phrase> system is used to illustrate the concept. 1 Introduction The analysis and design of complex <phrase>safety-critical</phrase> <phrase>embedded systems</phrase> pose many challenges. Feasible timing and safety requirements for these systems are difficult to formulate , understand, and meet without extensive prototyping. Traditional <phrase>timing analysis</phrase> techniques are not effective in evaluating <phrase>time-series</phrase> temporal behaviors (e.g. the maximum duration between consecutive missed deadlines must be greater than 5 seconds). This kind of requirements can only be evaluated through execution of the real-time systems or their prototypes. <phrase>Rapid prototyping</phrase> also helps system designers formulate and evaluate safety requirements of the system under development, by building two separate models (one for the system under development and the other for the environment (or equipment) under its control) and then exercising the two models in <phrase>tandem</phrase> to see if the simulation ends up in known hazardous states under normal <phrase>operating conditions</phrase> and under various failure conditions [AL]. <phrase>Run-time</phrase> Execution Monitoring of <phrase>formal specification</phrase> assertions (REM) is class of methods of tracking the temporal behavior of an underlying application. REM methods range from simple print-statement logging methods to <phrase>run-time</phrase> tracking of complex formal requirements (e.g., written in <phrase>temporal logic</phrase>) for verification purposes [D3]. Recently, NASA used REM for the verification of flight code for the Deep Impact project [DW]. Also recently, the U.S. <phrase>Ballistic Missile Defense</phrase> System has adopted REM as the primary verification method for the new <phrase>ballistic missile</phrase> battle manager because of its ability to scale, and its support for temporal assertions that include real-time and <phrase>time series</phrase> constraints [CDMSS]. In [DS], we showed that the use of <phrase>run-time</phrase> monitoring and verification of temporal assertions, in <phrase>tandem</phrase> with <phrase>rapid prototyping</phrase>, helps debug the requirements and identify errors earlier in the design process. For <phrase>cost-effective</phrase> prototyping, the system designers should have a clear understanding of the intended use of the prototype under …
<phrase>Quiescent Current</phrase> Testing of Cmos Data Converters <phrase>ii ACKNOWLEDGMENTS</phrase> I dedicate my work to my parents Mr. Nageswara Rao and Mrs. Vijaya <phrase>Lakshmi</phrase>, my brother and sister-in law Mr. Sapta Nag and Mrs. Parimala and my grandparents Mr. <phrase>Gandhi</phrase> and Mrs. Sambrajam for their <phrase>constant support</phrase> and encouragement throughout my life. I am very grateful to my advisor Dr. A. Srivastava for his guidance, patience and understanding throughout this work. His suggestions and discussion helped me to get deep insight into the field of <phrase>VLSI design</phrase> and testing.
<phrase>High-level</phrase> <phrase>Crosstalk Defect</phrase> Simulation for System-on-Chip Interconnects For system-on-chips (SoC) using <phrase>deep submicron</phrase> (DSM) technologies, interconnects are becoming critical determinants for performance and reliability. Buses and long interconnects are susceptible to <phrase>crosstalk defects</phrase> and may lead to functional and timing failure. Hence, testing for crosstalk errors on interconnects and buses in a SoC has become critical. To facilitate development of new crosstalk <phrase>test methodologies</phrase> and to efficiently evaluate <phrase>crosstalk defect coverage</phrase> for existing tests, there is a need for efficient <phrase>crosstalk defect coverage</phrase> analysis techniques. In this paper, we present an efficient <phrase>high-level</phrase> <phrase>crosstalk defect</phrase> simulation methodology. By using a novel <phrase>high-level</phrase> DSM error model for the interconnects, together with <phrase>HDL</phrase> models for the cores, our methodology enables fast <phrase>crosstalk defect</phrase> simulation to be conducted at <phrase>high level</phrase>. We validate the high-level interconnect DSM error model by comparing its outputs with HSPICE <phrase>simulation results</phrase>. The <phrase>fast and accurate</phrase> <phrase>high-level</phrase> <phrase>crosstalk defect</phrase> simulation methodology will enable evaluation and exploration of new crosstalk test techniques, as well as existing tests, leading to the development of <phrase>low-cost</phrase> crosstalk test.
<phrase>Deep Earth</phrase> Seismic Structure and <phrase>Earthquake Source</phrase> Processes from <phrase>Long Period</phrase> Waveform Modelling <phrase>Deep Earth</phrase> Seismic Structure and <phrase>Earthquake Source</phrase> Processes from <phrase>Long Period</phrase> Waveform Modelling <phrase>Deep Earth</phrase> Seismic Structure and <phrase>Earthquake Source</phrase> Processes from <phrase>Long Period</phrase> Waveform Modelling We model <phrase>long-period</phrase> seismic waveforms to investigate both the <phrase>deep Earth</phrase> velocity structure as well as <phrase>earthquake source</phrase> parameters. We utilize a <phrase>normal mode</phrase>-based perturbation approach to model and invert a global dataset of 3 component <phrase>long-period</phrase> seismic waveforms. The approach, which has been used for modelling <phrase>isotropic</phrase> velocity structure, is extended for modelling radial anisotropy, which describes an <phrase>anisotropic</phrase> medium with a vertical axis of <phrase>symmetry</phrase>. A model for shear velocity anisotropy near the core-mantle boundary is developed, and the stability and significance of the fit to the data is analyzed. The model has important implications for relations between flow and <phrase>observable</phrase> seismic anisotropy in this important thermal, chemical, and mechanical <phrase>boundary layer</phrase>. This modelling approach is extended to a multiple iteration inversion appropriate for a non-linear problem, and the <phrase>anisotropic</phrase> structure of the whole mantle is examined. Tests of the stability of the model and the influence of assumptions made in the modelling process are examined. Relations between mantle flow and seismic anisotropy are examined for a variety of depth ranges. We also perform inversions for <phrase>earthquake source</phrase> parameters using the same wave-form modelling approach, using the improved velocity model. Small, but systematic, relocations of events are observed, as well as small perturbations to the orientation of the mechanisms, and the updated sources result in significant improvement in fit to 2 the data. We also use a <phrase>finite-difference</phrase> approach to model regional, rather than tele-seismic, <phrase>long-period</phrase> waveforms to determine the significance of observed volumetric components of <phrase>earthquakes</phrase> in a <phrase>volcanic</phrase> region of eastern California.
Unlocking the Nature of the Phonological-Deep Dyslexia Continuum: The Keys to Reading Aloud Are in <phrase>Phonology</phrase> and Semantics It has been argued that normal reading and acquired dyslexias reflect the role of three underlying primary systems (<phrase>phonology</phrase>, semantics, and vision) rather than neural mechanisms dedicated to reading. This proposal is potentially consistent with the suggestion that phonological and deep dyslexia represent variants of a single reading disorder rather than two separate entities. The current study explored this possibility, the nature of any continuum between the disorders, and the possible underlying bases of it. A <phrase>case series</phrase> of patients were given an assessment battery to test for the characteristics of phonological and deep dyslexia. The status of their underlying phonological and semantic systems was also investigated. The majority of participants exhibited many of the symptoms associated with deep dyslexia whether or not they made semantic errors. Despite wide variation in word and nonword reading accuracy, there was considerable symptom overlap across the cohort and, thus, no sensible dividing line to separate the participants into distinct groups. The patient data indicated that the deep-phonological continuum might best be characterized according to the severity of the individual's reading impairment rather than in terms of a strict symptom succession. Assessments of phonological and semantic impairments suggested that the integrity of these primary systems underpinned the patients' reading performance. This proposal was supported by eliciting the symptoms of deep-phonological dyslexia in nonreading tasks.
<phrase>Content-delivery</phrase> Mechanisms Improving <phrase>Content Delivery</phrase> with Padis Improving <phrase>Content Delivery</phrase> with Padis <phrase>Cdn</phrase> Limitations Network Bottlenecks <phrase>Content-delivery</phrase> Costs <phrase>Content-delivery</phrase> networks (CDNs) originate a large fraction of <phrase>Internet traffic</phrase>; yet, due to how CDNs often perform traffic optimization, users aren't always assigned to the best servers for <phrase>end-user</phrase> performance. To improve user assignment of CDNs, the authors propose and deploy the Provider-aided Distance Information System (PaDIS), which lets ISPs augment the <phrase>CDN</phrase> server by utilizing their unique knowledge about network conditions and user locations. <phrase>Field tests</phrase> show that using PaDIS can result in significant improvements in download time. T he Internet is now a system in which users generate and share large amounts of content with other users via applications such as <phrase>online social networks</phrase>, video <phrase>portals</phrase>, one-click hosters (OCHs), <phrase>Web services</phrase>, <phrase>wikis</phrase>, <phrase>blogs</phrase>, and <phrase>peer-to-peer</phrase> (<phrase>P2P</phrase>) <phrase>file-sharing</phrase> applications. Multimedia content — including photos, music, and videos, as well as software down-loads and updates — constitutes most <phrase>Internet traffic</phrase>. <phrase>Recent studies</phrase> report that most users access this information via HT TP, which accounts for more than 50 percent of <phrase>Internet traffic</phrase> HTTP traf-fic's prevalence is due in large part to increased streaming content (such as that on Youtube) and the popularity of OCH-offered content 4 (from sites such as rapidshare.com). Such popular content is hosted by the Internet's " hyper <phrase>giants</phrase>, " 1 which include large content providers such as Google and <phrase>Yahoo</phrase> as well as <phrase>content-delivery</phrase> networks (CDNs) such as <phrase>Akamai</phrase> and <phrase>Limelight</phrase>. 5 For simplicity's <phrase>sake</phrase>, we refer to all these various players simply as CDNs. Although today's CDNs can optimize traffic flows, minimize costs, and bring content closer to <phrase>end users</phrase>, they come with some limitations. To overcome these limitations, we designed a Provider-aided Distance Information System (PaDIS), operated by an <phrase>ISP</phrase>, that helps ISPs improve user experiences by utilizing information about network bottlenecks and user locations. PaDIS fills a gap in the <phrase>content-delivery</phrase> <phrase>landscape</phrase>, in part because it <phrase>takes into account</phrase> <phrase>ISP</phrase> constraints and user performance. Although PaDIS is deployed at the moment as a platform for ISPs to improve <phrase>content delivery</phrase>, <phrase>long-term</phrase>, it's meant as a generic platform to help both ISPs and CDNs deliver content to users. So, we see PaDIS as an opportunity to think globally about <phrase>content delivery</phrase> by including the network and its users in the picture. To achieve high levels of performance and scalability, CDNs rely on distributed infra-structures. Some have even deployed servers <phrase>deep inside</phrase> ISPs in more than 5,000 locations throughout the Internet. 6 Others rely on …
Establishing <phrase>Correspondences between</phrase> Attribute Spaces and Complex Concept Spaces Using Meta-pgn Classifier In this paper, we present one approach for extending the learning set of a classification algorithm with additional metadata. It is used as a base for giving appropriate names to found regularities. The analysis of correspondence between connections established in the attribute space and existing links between concepts can be used as a test for creation of an adequate model of the observed world. Meta-PGN classifier is suggested as a possible tool for establishing these connections. Applying this approach in the field of <phrase>content-based image retrieval</phrase> of art paintings provides a tool for extracting specific feature combinations, which represent different sides of artists' styles, periods and movements. 1 Introduction The problem of resolving the gaps between computer analysis and human understanding has a deep philosophical background even in the problem of understanding between humans. <phrase>Lewis</phrase> Carroll in "Through the Looking-Glass" gives us an example of absurd use of semantics and <phrase>pragmatics</phrase> when <phrase>Humpty Dumpty</phrase> talks with Alice: "When I use a word it means just what I choose it to mean – neither more nor less". At the base of semantics lies the definition of concepts as names and corresponding content. Our life is full with learning concepts (their names and contents) in order to understand each other. There exist many well-suited theories in the area of concept formation based on the attribute models. Very close to this understanding is <phrase>formal concept analysis</phrase> [13], that uses the philosophical view of a concept as a unit consist
Self-healing reconfigurable logic using autonomous <phrase>group testing</phrase> Keywords: Autonomous systems <phrase>Group testing</phrase> Reconfigurable architectures Evolvable hardware Reliable systems Organic computing a b s t r a c t A <phrase>group-testing</phrase>-based fault resolution is incorporated into SRAM-based reconfigurable Field Programma-ble Gate Arrays (FPGAs) to provide an evolvable hardware system with self-healing and self-organizing properties. The proposed approach employs adaptive <phrase>group testing</phrase> techniques to autonomously maintain FPGA resource viability information as an organic means of transient and permanent fault resolution. Reconfigurability of the SRAM-based FPGA is leveraged to locate faulty logic resources which are successively excluded by <phrase>group testing</phrase> using alternate device configurations. This simplifies the system archi-tect's role to definition of functionality using a high-level <phrase>Hardware Description Language</phrase> (<phrase>HDL</phrase>) and system-level performance vs. availability operating point. System availability, throughput, and mean time to isolate faults are monitored and maintained using an observer–controller model. The proposed <phrase>group testing</phrase> method operates on the output response produced for real-time operational inputs, which eliminates the need for dedicated <phrase>test vectors</phrase>. The proposed system was demonstrated using a Data Encryption Standard (DES) core on 4-input and 6-input LUT-based <phrase>Xilinx</phrase> FPGA models. With a single simulated <phrase>stuck-at fault</phrase>, the system identifies a completely validated replacement configuration within a few test stages. Results also include approaches for optimizing group size, resource redundancy, and availability. The approach demonstrates a readily-implemented yet robust organic hardware application that features a high degree of autonomous self-control. Implementing a fault-tolerant system has become an essential part in many applications that require high degrees of availability and sustainability [1]. Mission critical systems, e.g. those used for <phrase>deep space exploration</phrase> and communication, in-orbit operations, and <phrase>unmanned</phrase> missions deployed in remote terrestrial and marine areas, are constantly exposed to a wide range of environment-related stress, and unknown and unexpected operational conditions not accounted for during design [2–5]. These factors could significantly increase the chance of device misbehaviors [6–8]. In the absence of spare capacity and human maintenance, the need for autonomous <phrase>fault tolerant</phrase> platforms with a self-adaptation property to achieve better availability and sustainability becomes increasingly important [1]. The autonomous, adaptive, and organic systems realized by reconfigurable devices such as FPGAs can be leveraged to address the mission sustainability needs of <phrase>space exploration</phrase> projects. NASA has identified autonomous systems as a key area for research [9], and research is being conducted on building self-managing applications and systems [10,11]. Auto-nomic Computing [12] defines the vision for self-managed Organic Systems. Such systems supplant human monitoring …
Adaptation and Evaluation of the Output-deviations Metric to Target <phrase>Small-delay Defects</phrase> in Industrial Circuits <phrase>Timing-related defects</phrase> are a major cause for test escapes and field returns for very-deep-sub-micron (VDSM) <phrase>integrated circuits</phrase> (ICs). Small-<phrase>delay variations</phrase> induced by crosstalk, <phrase>process variations</phrase>, <phrase>power-supply noise</phrase>, and <phrase>resistive opens</phrase> and shorts can cause <phrase>timing failures</phrase> in a design, thereby leading to quality and reliability concerns. We present the industrial application and case study of a <phrase>previously proposed</phrase> test-grading technique that uses the method of output deviations for screening <phrase>small-delay defects</phrase> (SDDs). The technique is shown to have significantly lower <phrase>computational complexity</phrase> and test <phrase>pattern count</phrase>, without loss of <phrase>test quality</phrase>, compared to a commercial <phrase>timing-aware</phrase> <phrase>automatic test pattern generation</phrase> (ATPG) tool.
Vertical distribution of zooplankton: density dependence and evidence for an ideal free distribution with costs BACKGROUND In lakes with a <phrase>deep-water</phrase> <phrase>algal</phrase> maximum, <phrase>herbivorous</phrase> zooplankton are faced with a <phrase>trade-off</phrase> between <phrase>high temperature</phrase> but low food availability in the surface layers and low temperature but sufficient food in deep layers. It has been suggested that zooplankton (<phrase>Daphnia</phrase>) faced with this <phrase>trade-off</phrase> distribute vertically according to an "Ideal Free Distribution (IFD) with Costs". An experiment has been designed to test the density (competition) dependence of the vertical distribution as this is a basic assumption of IFD theory.   RESULTS Experiments were performed in large, indoor mesocosms (<phrase>Plankton</phrase> Towers) with a <phrase>temperature gradient</phrase> of 10 degrees C and a <phrase>deep-water</phrase> <phrase>algal</phrase> maximum established below the <phrase>thermocline</phrase>. As expected, <phrase>Daphnia</phrase> aggregated at the interface between the two different habitats when their density was low. The distribution spread asymmetrically towards the <phrase>algal</phrase> maximum when the density increased until 80 % of the population dwelled in the cool, food-rich layers at high densities. Small individuals stayed higher in the <phrase>water column</phrase> than large ones, which conformed with the model for unequal competitors.   CONCLUSION The <phrase>Daphnia</phrase> distribution mimics the predictions of an IFD with costs model. This concept is useful for the analysis of zooplankton distributions under a large suite of environmental conditions shaping habitat suitability. <phrase>Fish</phrase> <phrase>predation</phrase> causing diel vertical migrations can be incorporated as additional costs. This is important as the vertical location of <phrase>grazing</phrase> zooplankton in a lake affects <phrase>phytoplankton</phrase> production and species composition, i.e. <phrase>ecosystem</phrase> function.
Implementation of <phrase>Delay and Power</phrase> Reduction in <phrase>Deep Sub-micron</phrase> Buses Using Coding Implementation of <phrase>Delay and Power</phrase> in Deep Sub-micron Buses Using Coding Acknowledgements <phrase>Delay and Power</phrase> have become the most important metrics in modern VLSI. Applications are becoming more demanding and the need for reducing both <phrase>delay and power</phrase> is emerging. Process scaling is constantly shifting larger portions of <phrase>delay and power</phrase> to buses and interconnect networks. This work focuses on the design of practical circuit implementations, that address these two problems. A coding scheme that eliminates delay-costly transitions is proposed, thus allowing faster clocking on the bus. An increase of 36% in the total throughput is achieved, while there is a <phrase>trade-off</phrase> of increased latency. Furthermore, a smart and efficient implementation of charge <phrase>recycling</phrase>, which reduces the dynamic <phrase>power dissipation</phrase> when driving long buses, is presented. The design circuit can be used for an arbitrary number of bus lines, and for the <phrase>test cases</phrase> that were examined energy savings up to 32% are reported.
Innovating for <phrase>emerging markets</phrase>: Confluence of user, design, business and technology research <phrase>user centered design</phrase>, new business opportunity development, contextual <phrase>invention</phrase> This paper describes a new research methodology, which brings together <phrase>ethnographic</phrase>, business, design and technical research in a focused way. The aim is to do this in a principled user-centered way, by using a deep understanding of user and cultural needs to drive design ideas, business modeling and technological investigations. This approach can be seen as an extension of Contextual Design in which social and cultural factors are considered in the deployment of an existing technology. We call this approach Contextual <phrase>Invention</phrase>, a process of using <phrase>ethnographic</phrase> data to generate new technology and business ideas in an interdisciplinary team. To develop and test this new approach, <phrase>HP Labs</phrase> India initiated a project towards understanding user needs and deriving concepts, at the same time providing <phrase>sustainable business</phrase> models for the end customer. Techniques used in the study include the collection of inspirational materials, the explicit discussion of new product concepts, the <phrase>feed-forward</phrase> of new concept ideas, and the circulation of user need and concept sheets. The project was successful in creating business opportunities and design concepts that were at the intersection of user needs, design, <phrase>business models</phrase> and technology feasibility.
What do exam results really measure? Students are evaluated using examinations, but how do we evaluate whether the examination is correctly measuring the students' knowledge or skill? This paper presents a methodology which we have used in an experiment: students' exam results were analysed to reveal which different cognitive skills were used in answering different questions. The analysis revealed that students were approaching several questions in ways that the instructor had not anticipated. Sometimes questions the instructor considered straightforward actually tested students' conceptual understanding; on other questions which were intended to require <phrase>problem-solving</phrase>, many students never identified the concepts involved, so that grades measured primarily the ability to avoid distraction. In other cases, we demonstrated that questions did assess deep understanding of the fundamental concepts, rather than <phrase>rote-learning</phrase> or simple <phrase>pattern-matching</phrase>.
On the evolutionary design of heterogeneous Bagging models Bagging is a popular ensemble algorithm based on the idea of data resampling. In this paper, aiming at increasing the incurred levels of ensemble diversity, we present an evolutionary approach for optimally designing Bagging models composed of heterogeneous components. To assess its potentials, experiments with well-known <phrase>learning algorithms</phrase> and classification datasets are discussed whereby the accuracy, generalization and diversity levels achieved with heterogeneous Bagging are matched against those delivered by standard Bagging with homogeneous components. Over the last decades, the strategy of combining <phrase>multiple classifiers</phrase> into ensembles has received increasing interest due to its potential in bringing about <phrase>significant improvements</phrase> in terms of training accuracy and learning generalization [1,2]. As the key for the success of any ensemble lies in how its components disagree on their predictions [3], several approaches for designing diverse components have been conceived, among which those using different subsets of <phrase>training data</phrase> jointly with a single learning method [4,5] and those adopting different learning methods associated with different predictors [6,7]. A well-known representative of the first group is Bagging, which is based on the idea of data resampling [4,5,8,9]. Diversity is promoted in Bagging by using bootstrapped replicas of the training dataset, each replica being generated by randomly drawing, with replacement, a subset of the training data. Typically, each new dataset will have the same number of instances of the original dataset; however, since some instances will appear repeatedly while others will not show up, the effective size will be lower and the datasets will overlap significantly. Each derived dataset is used to train a classifier, and then, for any test instance, the outputs of the <phrase>individual classifiers</phrase> are aggregated via the simple <phrase>majority vote</phrase> (MV) rule. Usually, unstable classifiers are adopted as base models, since this type of classifier can generate sufficiently different decision boundaries even for small perturbations in the training parameters [2,4]. In this paper, aiming at further increasing the diversity levels of the ensemble models produced by Bagging, we present an evolutionary approach for optimally designing Bagging models composed of heterogeneous components. Even though the idea of heterogeneous ensembles has been recently advocated [6,7], so far there is no deep investigation on the benefits of adopting different <phrase>learning algorithms</phrase> in the context of Bagging. In fact, this idea seems very reasonable since different classes of <phrase>learning algorithms</phrase> are usually associated with different search/represen-tation biases (and thus hypothesis spaces) [10], thereby foment-ing ensemble …
Diagram Interaction during <phrase>Intelligent Tutoring</phrase> in Geometry: Support for Knowledge Retention and <phrase>Deep Understanding</phrase> Prior research has shown that skilled problem solvers often use features of <phrase>visual representations</phrase> to cue relevant knowledge, but little is known about how to support learners in developing connections between visual and verbal knowledge components. In this research, we investigated two methods to support focus on key visual features during <phrase>problem solving</phrase> in an intelligent tutor: 1) student interaction with diagrams during <phrase>problem solving</phrase>, and 2) student explanations that connected diagram features to geometry rules at each <phrase>problem-solving</phrase> step. Research was conducted in 10 th grade classrooms using an experimental version of the Geometry <phrase>Cognitive Tutor</phrase>. Interaction with diagrams promoted <phrase>long-term</phrase> retention of <phrase>problem-solving</phrase> skills and supported deep understanding of geometry rules, as evidenced by items testing transfer and visual-verbal knowledge integration. Diagram-rule explanations did not significantly influence learning. <phrase>Findings suggest</phrase> that student focus on relevant <phrase>visual information</phrase> should be carefully integrated into <phrase>problem-solving</phrase> practice to support robust learning. <phrase>Visual Representations</phrase> in Skilled Performance Existing research has found that experts use <phrase>visual representations</phrase> in rich and interconnected ways during skilled <phrase>problem solving</phrase>. Stylianou (2002) studied the <phrase>problem-solving</phrase> processes of professional <phrase>mathematicians</phrase> and noted that <phrase>mathematicians</phrase> used diagrams extensively to inform their analysis of the problem, their selection of subgoals, and their eventual solutions. During <phrase>problem solving</phrase>, <phrase>mathematicians</phrase> created <phrase>visual representations</phrase> in a <phrase>step-by-step</phrase> manner, where <phrase>visual information</phrase> in the representation was analyzed at each step in order to inform reasoning and to cue relevant approaches. <phrase>Mathematicians</phrase> recognized important features and patterns in their diagrams and revised or annotated their diagrams to reflect the outcome of their analysis at each step. Stylianou's (2002) results <phrase>complement</phrase> <phrase>previous research</phrase> in expert <phrase>problem solving</phrase> that has demonstrated close connections between <phrase>visual representations</phrase> and existing knowledge. Koedinger and Anderson (1990) found that experts solving geometry problems made inferences that were strongly tied to geometry diagrams, and that features in the problem diagrams cued relevant <phrase>problem-solving</phrase> steps. Koedinger and Anderson found that the <phrase>problem solving</phrase> steps mentioned and skipped by experts could be successfully predicted by a model (the Diagram Configuration Model) that parsed diagrams into key geometric configurations and used these configurations to cue relevant schemas. The development of skilled performance in geometry appears to be correlated with attention to key diagram features, as well as successful association of those features with relevant geometry rules. Recent <phrase>eye-tracking</phrase> research suggests that learner focus on key <phrase>visual information</phrase> predicts successful performance even among non-experts. On insight …
Software emulation of programmable optical routers —Programmable optical networks are a key solution to <phrase>high performance</phrase> dynamic network services in Future Internet scenario. Modular router architecture which meets this concept is here defined and represented in a flexible software context, using Click! tools. Evaluation of logical performance and testing of <phrase>physical characteristics</phrase> of information forwarding within this framework is shown as feasible. The approach allows also to easily test functions and interactions of control and data planes to support enhanced future <phrase>Internet services</phrase>. Sample implementations of programmable router subsystems are given and discussed. I. INTRODUCTION Future Internet concepts strongly rely on network virtual-ization and fast, dynamic reconfigurations of network services. Emerging and foreseeable network-based applications might have heterogeneous requirements in terms of bandwidth, delay and latency, as well as dynamism and diversity [1]. New services, some of which not envisaged during network design, as well as additional resources can be dynamically added to or removed from the network. Smart network control integrates this scenario with fast, rich and flexible signaling procedures [2]. For these reasons, the success and growth of Future Internet applications require a deep rethink of node and network architectural concepts which have started increasing discussion in the networking <phrase>research community</phrase> [3]. Hence, it is vital to understand and redefine the role of networking, in order to implement <phrase>high-performance</phrase> network concepts able to support applications with wide range of requirements, by providing different transport services and offering a flexible, scalable and <phrase>cost-effective</phrase> solution to <phrase>service providers</phrase>. To respond to a flexible and <phrase>distributed environment</phrase>, flexible architectural and technological solutions should be considered to reflect the characteristics and requirements of dynamic network-enabled applications. The target to design <phrase>high performance</phrase> flexible architectural solutions, both for nodes and networks, should consider state-of-the-art optical and/or electronic technologies combined with modular architectural concepts in the awareness of characteristics and requirements of emerging network-enabled applications as well as <phrase>service providers</phrase>' needs. Network and node programmability are key concepts that need to be investigated in this framework [4]. Concerning high-capacity programmable optical routers, they are expected to consist of complementary modules, to support control and data plane functions and their interactions inside the node, possibly in a multi-provider context. This last aspect would be important
Multiscale brain modelling. A central difficulty of brain modelling is to span the range of spatio-temporal scales from <phrase>synapses</phrase> to the whole brain. This paper overviews results from a recent model of the generation of brain electrical activity that incorporates both basic microscopic <phrase>neurophysiology</phrase> and <phrase>large-scale</phrase> brain <phrase>anatomy</phrase> to predict brain electrical activity at scales from a few tenths of a millimetre to the whole brain. This model incorporates synaptic and dendritic dynamics, nonlinearity of the firing response, axonal propagation and corticocortical and corticothalamic pathways. Its relatively few parameters measure quantities such as synaptic strengths, corticothalamic delays, synaptic and dendritic time constants, and axonal ranges, and are all constrained by independent physiological measurements. It reproduces quantitative forms of electroencephalograms seen in various states of <phrase>arousal</phrase>, evoked response potentials, coherence functions, seizure dynamics and other phenomena. Fitting model predictions to <phrase>experimental data</phrase> enables underlying physiological parameters to be inferred, giving a new non-invasive window into <phrase>brain function</phrase> that complements slower, but finer-resolution, techniques such as fMRI. Because the parameters measure physiological quantities relating to multiple scales, and probe <phrase>deep structures</phrase> such as the thalamus, this will permit the testing of a range of hypotheses about vigilance, cognition, drug action and <phrase>brain function</phrase>. In addition, referencing to a standardized database of subjects adds strength and specificity to characterizations obtained.
Intelligent Collaborating Agents to Support <phrase>Teaching and Learning</phrase> This paper presents the Intelligent Multiagent Infrastructure for Distributed System in Education (I-MINDS), an innovative application using AI and mul-tiagent systems to help teachers teach better and <phrase>students learn</phrase> better. The I-MINDS system consists of a group of <phrase>intelligent agents</phrase> that work cooperatively in a <phrase>distributed computing</phrase> environment. A teacher agent monitors the student activities and helps the teacher manage and better adapt to the class. A student agent interacts with the teacher agent and other student agents to support <phrase>cooperative learning</phrase> activities behind the scene for a student. This paper describes two innovations in (a) automated ranking of questions and responses, and (b) agent-supported " buddy group " formation. The results of the <phrase>proof-of-concept</phrase> tests have demonstrated encouraging effectiveness of I-MINDS in terms of learning gain and <phrase>deep understanding</phrase> .
On the intrinsic <phrase>Rent parameter</phrase> and spectra-based partitioning methodologies The complexity of circuit designs has necessitated a top-down approach to layout synthesis. A large body of work shows that a good partitioning hierarchy, as measured by the associated <phrase>Rent parameter</phrase>, will correspond to an area-eecient layout. We deene the intrinsic <phrase>Rent parameter</phrase> of a <phrase>netlist</phrase> to be a <phrase>lower bound</phrase> on the <phrase>Rent parameter</phrase> of any partitioning hierarchy for the <phrase>netlist</phrase>. Experimental results show that spectra-based ratio cut partitioning methods yield partitioning hierarchies with the lowest observed <phrase>Rent parameter</phrase> over all benchmarks and over all algorithms tested. For examples where the intrinsic <phrase>Rent parameter</phrase> is known, spectral ratio cut partitioning yields a <phrase>Rent parameter</phrase> essentially identical to this theoretical optimum. We provide additional theoretical results supporting the close relationship between spectral partitioning and the intrinsic <phrase>Rent parameter</phrase>. These results have <phrase>deep implications</phrase> with respect to the choice of partitioning algorithms and new approaches to layout area estimation.
Race System Description Other Features Availability Future Plans Language RACE [3] is a successor of HAM-ALC [5, 2]. Based on sound and complete algorithms RACE currently implements TBox and <phrase>ABox reasoning</phrase> for the <phrase>description logic</phrase> ALCN H R + [4] that supports number restrictions, role hierarchies, and transitively closed roles. Note that this <phrase>DL</phrase> implies general concept inclusions as a language feature. RACE accepts TBoxes and ABoxes in the KRSS syntax with appropriate extensions for GCIs. RACE additionally provides a Web interface based on CL-HTTP, a <phrase>Common Lisp</phrase> <phrase>Hypermedia</phrase> Server. Implementation RACE employs standard (exploitation of told sub-sumers/subsumees by marking and caching operations for computing the subsumption hierarchy, lazy unfolding) and advanced <phrase>optimization techniques</phrase> (semantic branching, dependency-directed <phrase>backtracking</phrase>, taxo-nomic encoding, GCI absorption) in analogy to [6] as well as deep model merging and extensive model caching. It also exploits new <phrase>optimization techniques</phrase> for <phrase>ABox reasoning</phrase> [3]. The <phrase>programming language</phrase> is <phrase>ANSI</phrase> <phrase>Common Lisp</phrase>. Performance RACE is one of the fastest systems for testing concept consistency and outperforms any known <phrase>ABox reasoning</phrase> system by several <phrase>orders of magnitude</phrase>. For details on a <phrase>performance evaluation</phrase> regarding <phrase>ABox reasoning</phrase> see [3]. The tests presented in the Appendix were performed using RACE version 1.0 on a <phrase>Macintosh</phrase> G3 <phrase>Powerbook</phrase> (266 MHz) with <phrase>Macintosh</phrase> <phrase>Common Lisp</phrase> 4.2 in a 80 MB memory partition. If a size attribute is specified for the problem, the runtime is given for the last size that can be solved within the specified timeout limit. We plan to extend RACE to additionally supporting ABox <phrase>reasoning with</phrase> qualified number restrictions and inverse roles. Future versions of RACE will also include more advanced <phrase>optimization techniques</phrase> for the inference algorithms dealing with number restrictions. [3] V. Haarslev and R. Möller. An <phrase>empirical evaluation</phrase> of optimization strategies for <phrase>ABox reasoning</phrase> in expressive <phrase>description logics</phrase>.
Developing student help desk consultants: a skill-based modular approach Student Help Desk consultants are an essential resource in most college IT departments. Hiring and training these students presents special challenges: consultants need a set of skills that is both wide and deep, and supervisors may be unsure which skills to hire for and which to train for, how to provide training, and how to measure competency. At <phrase>Duquesne University</phrase> we have created a system for developing skilled consultants that is based on: how easy a skill is to teach; which mode of teaching best lends itself to a particular skill; and the need to accommodate different <phrase>learning styles</phrase> in the students.We first grouped the skills into five <phrase>core competency</phrase> areas: technical knowledge, <phrase>troubleshooting</phrase> and <phrase>problem-solving</phrase>, knowledge of our structure and procedure, customer service skills, and professional behavior. We then identified five modalities for acquiring these skills in our consultants, being:<ul><li>hire people with the skill;</li><li>provide classroom training;</li><li>train through <phrase>apprenticeship</phrase>;</li><li>rely on independent learning; or</li><li>provide ongoing training.</li></ul>.Different skill sets are emphasized within each modality, but most are addressed across several modalities so as to ensure mastery and accommodate different <phrase>learning styles</phrase>. For example, attitude and interpersonal skills are heavily weighted in the hiring criteria. Formal instruction in customer service skills is then provided in a classroom setting and application of these skills is emphasized in the <phrase>apprenticeship</phrase> phase of training. Finally, customer service skills may be further developed through independent learning and ongoing training.We use this approach to structure all phases of consultant development: candidate selection, training, evaluation, and ongoing support. This <phrase>poster</phrase> presentation provides an overview of the analysis and plan, together with a selection of the instruments (forms, tests, etc.) that we use in each phase.
A Diagnostic System for <phrase>Photolithography</phrase> Equipment This paper presents a general diagnostic system that can be applied to semiconductor equipment to assist the operator in finding the causes of decreased machine performance. Based on conventional <phrase>probability theory</phrase>, the diagnostic system incorporates both shallow and deep level information. From the observed evidence, and from the <phrase>conditional probabilities</phrase> of faults initially supplied by machine experts (and subsequently updated by the system), the fault probabilities and their bounds are calculated, given a specified confidence level. The rate of convergence of the fault probabilities has been derived in detail in the paper, and the procedure for combining the estimates of <phrase>conditional probabilities</phrase> given by the machine experts has also been described in detail. We have implemented a software version of the diagnostic system, and tested it on real <phrase>photolithography</phrase> equipment malfunctions and performance drifts. Initial <phrase>experimental results</phrase> are encouraging.
Deep Start: A Hybrid Strategy for Automated Performance Problem Searches To attack the problem of scalability of performance diagnosis tools with respect to application code size, we have developed the Deep Start <phrase>search strategy</phrase>—a new technique that uses stack sampling to augment an automated search for application performance problems. Our hybrid approach locates performance problems more quickly and finds performance problems hidden from a more straightforward <phrase>search strategy</phrase>. The Deep Start strategy uses stack samples collected as a by-product of normal search instrumentation. Using these samples, our strategy selects deep starters—functions that are likely to be application bottlenecks and thus are good locations to consider early in the search. With priorities and careful control of the search refinement, our strategy gives preference to experiments on the deep starter functions and their callees. This approach enables the Deep Start strategy to find application bottlenecks more efficiently and more effectively than a more straightforward <phrase>search strategy</phrase>. We implemented the Deep Start <phrase>search strategy</phrase> in the Performance Consultant, Paradyn's automated bottleneck detection component. In our tests, the Deep Start strategy found half of all known bottlenecks between 25% and 63% faster on average than the Performance Consultant's current <phrase>search strategy</phrase>. The Deep Start strategy found all bottlenecks in its search between 7% and 61% faster on average than the current strategy.
Lower extremity muscle activity during different types and speeds of underwater movement. To compare the activity of lower extremity muscles during land walking (LW), water walking (WW), and <phrase>deep-water</phrase> running (DWR), 9 healthy young subjects were tested at self-selected low, moderate, and high intensities for 8 <phrase>sec</phrase> with two repetitions. Surface <phrase>EMG</phrase> electrodes were placed on the tibialis anterior (TA), soleus (SOL), medial <phrase>gastrocnemius</phrase> (GAS), rectus femoris (RF), and <phrase>biceps</phrase> femoris (BF). During DWR, the SOL and GAS activities were lower than LW and WW. The BF activities were higher during DWR than LW and WW. It was considered that the lower activity of SOL and GAS depended on <phrase>water depth</phrase>, and higher activity of BF occurred by greater flexion of the <phrase>knee joint</phrase> or extension of the hip joint during exercise.
Improvements to CBCM (Charge-Based Capacitance Measurement) for <phrase>Deep Submicron</phrase> <phrase>CMOS Technology</phrase> Accurate measurement and analysis of interconnect capacitance is a critical component of nanometer technology verification. The Charged-Based Capacitance Measurement (CBCM) technique has been widely adopted as a robust technique to measure on-chip capacitance <phrase>test structures</phrase>. In this paper we present two design improvements for CBCM. The first is an area reduction by using bused circuit architecture to reduce probe pad area required for the test structure input and output signals. The second improvement involves techniques to reduce the impact of gate leakage and charge injection currents in 90 and 65nm process <phrase>technology nodes</phrase>. At the 90nm node we demonstrate accuracy improvement of an <phrase>order of magnitude</phrase> for small <phrase>test structures</phrase>.
Evaluating probabilities under <phrase>high-dimensional</phrase> <phrase>latent variable</phrase> models We present a simple new <phrase>Monte Carlo</phrase> algorithm for evaluating probabilities of observations in complex <phrase>latent variable</phrase> models, such as <phrase>Deep Belief Networks</phrase>. While the method is based on <phrase>Markov chains</phrase>, estimates based on short runs are formally unbiased. In expectation, the log probability of a test set will be underestimated , and this could form the basis of a probabilistic bound. The method is much cheaper than <phrase>gold-standard</phrase> annealing-<phrase>based methods</phrase> and only slightly more expensive than the cheapest <phrase>Monte Carlo methods</phrase>. We give examples of the new method substantially improving simple variational bounds at modest extra cost.
Test and Debug in Deep-submicron Technologies With the scaling of <phrase>feature sizes</phrase> into <phrase>Deep-Submicron</phrase> (DSM) values, the level of integration and performance achievable in VLSI chips increases. A lot of work has been directed to tackle design related issues arising out of scaling , like leakage mitigation etc. However efforts to enhance testability of such designs have not been sufficient. It is not viable to overlook testability issues arising out of these designs because the defect sizes do not scale proportional to the <phrase>feature sizes</phrase>. Previously effective <phrase>fault models</phrase> like stuck-at appear archaic and are unable to model faults accurately. This necessitates the need for more detailed models which can more explicitly model the behavior of faulty DSM chips. Also there is a significant increase in <phrase>delay faults</phrase> in logical paths of <phrase>integrated circuits</phrase>. <phrase>Delay faults</phrase> cause the delay of paths in a chip to be larger than expected resulting in the output of a chip to be deviant from the expected behavior, in spite of the chip being functionally correct. Efficient techniques are needed for detecting such defects in first silicon and eliminating them before the final versions of the chips are shipped. This requires efficient debug techniques for performance characterization of large complex <phrase>integrated circuits</phrase> in <phrase>deep-submicron</phrase> and nanome-ter technologies. In this paper we present an insight into <phrase>test challenges</phrase> arising out of <phrase>deep submicron technologies</phrase> and effective approaches to tackle the same.
Obfuscating Document <phrase>Stylometry</phrase> to Preserve Author <phrase>Anonymity</phrase> This paper explores techniques for reducing the effectiveness of standard authorship attribution techniques so that an author A can preserve <phrase>anonymity</phrase> for a particular document D. We discuss <phrase>feature selection</phrase> and adjustment and show how this information can be fed back to the author to create a new document D' for which the calculated attribution moves away from A. Since it can be labor intensive to adjust the document in this fashion , we attempt to quantify the amount of effort required to produce the ano-nymized document and introduce two levels of <phrase>anonymization</phrase>: shallow and deep. In our <phrase>test set</phrase>, we show that shallow <phrase>anonymization</phrase> can be achieved by making 14 changes per 1000 words to reduce the likelihood of identifying A as the author by an average of more than 83%. For deep <phrase>anonymization</phrase>, we adapt the unmasking work of Koppel and Schler to provide feedback that allows the author to choose the level of ano-nymization.
The Impact of <phrase>Electronic Commerce</phrase> on the <phrase>Retail</phrase> Brokerage Industry <phrase>Electronic commerce</phrase> has enjoyed great success in the <phrase>retail</phrase> brokerage industry. Attracted by commission savings, consumers' use of on-line brokerage firms has grown. However, brokerage customers may have difficulty comparing total trading costs, which consist of both the commission the broker charges and the cost of executing a trade. This paper reports on an experiment to examine whether order handling practices by traditional voice brokers and on-line brokerage firms lead to differences in the quality of trade execution. We test two hypotheses; the first is that execution quality differs among brokers and is positively related to commission rates, and the second is that total trading costs are converging as might be expected in a stable market. In the experiment, we conducted 196 trades, simultaneously purchasing or selling 100 share lots of stock using a voice-based broker, a "<phrase>brand</phrase>-name" online broker and a deep discount online broker in each trial. We found 36 percent of our orders received price improvement, a measure of execution quality. The differences among brokers in obtaining price improvements were (weakly) <phrase>statistically significant</phrase> for <phrase>NYSE</phrase>-listed shares only. The brokers do exhibit <phrase>statistically significant</phrase> differences in total trading costs; at a volume of 100 shares commission costs dominate execution quality. We discuss implications for larger lot sizes and speculate on the ability of full-service brokerage firms to maintain high commission charges. The paper concludes that <phrase>electronic commerce</phrase> is having a major impact on the brokerage industry, and has the potential to affect pricing in other industries with bundled products and services. 1 The authors wish to thank Professor Ingo Walter and the <phrase>Salomon Brothers</phrase> Center at the <phrase>Stern</phrase> School for their support and willingness to underwrite the research reported in this paper.
New Directions in <phrase>Eddy Current</phrase> Sensing Enhancing <phrase>Eddy Current Probes</phrase> <phrase>Nondestructive testing</phrase> needs an effective, inexpensive way of detecting deeply buried or small cracks at the edges of metallic parts and structures. One solution comes in the form of <phrase>solid-state magnetic</phrase> sensors based on giant <phrase>magneto</phrase>-resistance (GMR) and spin-dependent tunneling (SDT) effects integrated in <phrase>eddy current probes</phrase>. <phrase>Eddy current</phrase> testing is an effective way of detecting fatigue cracks and <phrase>corrosion</phrase> in conductive materials. The cost of using the technology is low, and the devices can monitor subsurface defects and defects under insulating coatings without touching the surface of the specimen. Although this technique is applicable for many tasks, the <phrase>aircraft</phrase> and <phrase>nuclear power</phrase> industries are the primary users of <phrase>eddy current probes</phrase> for in-service inspection. Because <phrase>safety-critical</phrase> systems depend on early detection of fatigue cracks to avoid major failures, there's an increasing need for <phrase>eddy current probes</phrase> that can reliably detect very small defects. Also there are increasing demands for probes that can detect deeply buried defects to avoid disassembling structures. <phrase>Eddy current</phrase> testing probes combine an excitation coil that induces eddy currents in a specimen and a detection element that identifies the perturbation of the currents caused by cracks or other defects. The detection elements can be coils, <phrase>superconducting</phrase> quantum interference detectors, or <phrase>solid-state magnetic</phrase> sensors (e.g., <phrase>Hall effect</phrase>, <phrase>magneto</phrase>-resistive, and spin-dependent-tunneling sensors). The use of low-field, <phrase>solid-state magnetic</phrase> sensors represents a significant advance over more traditional inductive probes in use today. Two key attributes will open opportunities for increased use of <phrase>eddy current probes</phrase>: constant sensitivity over a wide range of frequencies and development of smaller sensors. Probes that detect <phrase>eddy current</phrase> fields using inductive coils have less sensitivity at low frequencies. Unfortunately, this is where the device would have to operate to detect deep flaws. Small sensing coils, which are required to detect small defects, also have low sensitivity. In contrast, small, high-sensitivity thin film sensors can locally measure a <phrase>magnetic field</phrase> over an area comparable to the size of the sensor itself (tens of micrometers). A limitation of conventional <phrase>eddy current probes</phrase> is the difficulty of detecting small cracks originating at the edges of a specimen. This defect is the most common type encountered in practice. An example is the cracks that appear around the <phrase>fastener</phrase> or <phrase>rivet</phrase> holes in <phrase>aircraft</phrase> multilayered structures. Most inductive coil probes are sensitive to both the edge and the cracks initiating from or near the edge. The edge creates a …
A Gracefully Degrading and <phrase>Energy-Efficient</phrase> Modular Router Architecture for On-Chip Networks Packet-based on-<phrase>chip networks</phrase> are increasingly being adopted in complex System-on-Chip (SoC) designs supporting numerous homogeneous and heterogeneous functional blocks. These <phrase>Network-on-Chip</phrase> (NoC) architectures are required to not only provide <phrase>ultra-low</phrase> latency, but also occupy a small footprint and consume as little energy as possible. Further, reliability is rapidly becoming a major challenge in deep sub-micron technologies due to the increased prominence of <phrase>permanent faults</phrase> resulting from accelerated aging effects and manufacturing/testing challenges. Towards the goal of designing low-latency, energyefficient and reliable on-chip communication networks, we propose a novel <phrase>fine-grained</phrase> modular router architecture. The proposed architecture employs decoupled parallel arbiters and uses smaller crossbars for row and column connections to reduce output port contention probabilities as compared to existing designs. Furthermore, the router employs a new switch allocation technique known as "Mirroring Effect" to reduce <phrase>arbitration</phrase> depth and increase concurrency. In addition, the <phrase>modular design</phrase> permits graceful degradation of the network in the event of <phrase>permanent faults</phrase> and also helps to reduce the dynamic <phrase>power consumption</phrase>. Our simulation results indicate that in an 8 × 8 mesh network, the proposed architecture reduces packet latency by 4-40% and <phrase>power consumption</phrase> by 6-20% as compared to two existing router architectures. Evaluation using a combined performance, energy and fault-tolerance metric indicates that the proposed architecture provides 35-50% overall improvement compared to the two earlier routers.
Computationally Derived Points of Fragility of a Human <phrase>Cascade</phrase> Are Consistent with Current Therapeutic Strategies The role that mechanistic mathematical modeling and <phrase>systems biology</phrase> will play in <phrase>molecular medicine</phrase> and clinical development remains uncertain. In this study, mathematical modeling and <phrase>sensitivity analysis</phrase> were used to explore the working hypothesis that mechanistic models of human cascades, despite model uncertainty, can be computationally screened for points of fragility, and that these sensitive mechanisms could serve as therapeutic targets. We tested our working hypothesis by screening a model of the well-studied <phrase>coagulation</phrase> <phrase>cascade</phrase>, developed and validated from literature. The predicted sensitive mechanisms were then compared with the treatment literature. The model, composed of 92 proteins and 148 <phrase>protein-protein interactions</phrase>, was validated using 21 published datasets generated from two different quiescent in vitro <phrase>coagulation</phrase> models. Simulated <phrase>platelet</phrase> activation and <phrase>thrombin</phrase> generation profiles in the presence and absence of natural anticoagulants were consistent with measured values, with a mean correlation of 0.87 across all trials. Overall state sensitivity coefficients, which measure the robustness or fragility of a given mechanism, were calculated using a <phrase>Monte Carlo</phrase> strategy. In the absence of anticoagulants, fluid and surface phase factor X/activated factor X (fX/FXa) activity and <phrase>thrombin</phrase>-mediated <phrase>platelet</phrase> activation were found to be fragile, while fIX/FIXa and fVIII/FVIIIa activation and activity were robust. Both anti-fX/FXa and direct <phrase>thrombin</phrase> inhibitors are important classes of anticoagulants; for example, anti-fX/FXa inhibitors have <phrase>FDA</phrase> approval for the prevention of venous <phrase>thromboembolism</phrase> following surgical intervention and as an initial treatment for deep <phrase>venous thrombosis</phrase> and <phrase>pulmonary embolism</phrase>. Both in vitro and in vivo experimental evidence is reviewed supporting the prediction that fIX/FIXa activity is robust. When taken together, these results support our working hypothesis that computationally derived points of fragility of human relevant cascades could be used as a <phrase>rational basis</phrase> for target selection despite model uncertainty.
Combined Scheme for Fast Pn <phrase>Code Acquisition</phrase> I-introduction In direct-sequence wideband (DS-<phrase>WB</phrase>) systems, long spreading sequences are used to span multiple symbol intervals in order to remove <phrase>spectral lines</phrase> and to mitigate multiple access interference. The usage of long spreading sequences will result in a large <phrase>search space</phrase> during acquisition stage. As a result, the DS-<phrase>WB</phrase> systems will need a lot of time to achieve acquisition state. This paper proposes a combined acquisition scheme, known as Fast <phrase>Serial Search</phrase> Sequential Estimation (FSSSE), which could be applied to the <phrase>direct sequence spread spectrum</phrase> systems with long PN sequences. The proposed scheme is a combination of two acquisition schemes which are the <phrase>serial search</phrase> acquisition scheme and the rapid acquisition by sequential estimation (RASE) scheme. The proposed scheme gets the advantages of both schemes while overcome their drawbacks. The mean acquisition time of the proposed scheme is reduced 100 times compared to the conventional <phrase>serial search</phrase> scheme for PN sequence with a period of 2 15-1. search acquisition scheme-Fast <phrase>Serial search</phrase> sequential estimation acquisition scheme-Rapid acquisition by sequential estimation scheme. In <phrase>Direct-Sequence Spread Spectrum</phrase> (DSSS) systems, the goal of <phrase>code acquisition</phrase> is to achieve a coarse time alignment between the received PN code and the locally generated code to an accuracy of a fraction of one PN sequence chip [1, 2]. <phrase>Serial search</phrase> strategy [3] is the most popular approach to <phrase>code acquisition</phrase> which correlates the received and locally generated code sequences and then tests the synchronization based on either the crossing of threshold or the maximum correlation. A threshold value is determined depending on the <phrase>signal to noise ratio</phrase> at the matched filter output [2] and it may be adjusted to the partial correlation [1]. A direct approach [4] for obtaining statistics of the <phrase>code acquisition</phrase> time for <phrase>serial search</phrase> <phrase>spread-spectrum</phrase> receivers is presented. It combines algebraic characterization of the search with transform-domain methods. This approach gives a very deep insight into the nature of the acquisition process. This fact permits the author to propose two alternate search strategies that outperform the conventional ones when the code rewinding time is small in comparison to the dwell times. The <phrase>serial search</phrase> scheme has a drawback of its relatively long acquisition time for long periods PN codes. A double-dwell serial <phrase>code acquisition</phrase> system employing an adaptive threshold estimator is described and analyzed and a general expression for the mean acquisition time is derived [5].
Robust interpretation in dialogue by combining confidence scores with contextual features We present an approach to dialogue management and interpretation that evaluates and selects amongst candidate dialogue moves based on features at multiple levels. Multiple interpretation methods can be combined, multiple <phrase>speech recognition</phrase> and parsing hypotheses tested, and multiple candidate dialogue moves considered to choose the highest scoring hypothesis overall. We integrate hypotheses generated from shallow slot-filling methods and from relatively deep parsing, using pragmatic information. We show that this gives more robust performance than using either approach alone, allowing n-best list reordering to correct errors in <phrase>speech recognition</phrase> or parsing.
<phrase>Deep Learning</phrase> Based Semantic Video Indexing and Retrieval —We share the <phrase>implementation details</phrase> and testing results for video <phrase>retrieval system</phrase> based exclusively on <phrase>features extracted</phrase> by <phrase>convolutional neural networks</phrase>. We show that deep learned features might serve as universal signature for semantic content of video useful in many search and retrieval tasks. We further show that <phrase>graph-based</phrase> storage structure for video index allows to efficiently retrieving the content with complicated spatial and temporal search queries.
A fuzzified approach towards <phrase>global routing</phrase> in VLSI layout design —In DSM (<phrase>deep sub-micron</phrase>) regime, together with the integration density interconnects play a dominant role during layout design of <phrase>integrated circuits</phrase>. It eventually increases the importance of <phrase>global routing</phrase> problem making it more challenging day by day. To cope up with this ever increasing <phrase>design complexity</phrase>, the challenging time faced by researchers provides the important opportunity to explore new ideas to solve it within some reasonable time. Heuristic <phrase>based approaches</phrase> are generally used for <phrase>global routing</phrase>. Large problem space leads <phrase>global routing</phrase> problem to a <phrase>NP Complete</phrase> one which is less compatible with modern trends. The proposed <phrase>multi-objective</phrase> <phrase>global routing</phrase> technique is formulated using <phrase>fuzzy logic</phrase> to get rid of the limitations of deterministic approaches. After placement and prior to routing phase a set of guiding information is generated from our approach, which will help routing in subsequent steps. During <phrase>global routing</phrase> the decision is taken from a <phrase>fuzzy logic</phrase> expert system. A GUI is implemented based on the proposed algorithm which is tested for its feasibility study and experimental validation. Success of our proposed approach will open up an avenue for research in <phrase>global routing</phrase> phase.
A new <phrase>DVB-RCS</phrase> <phrase>satellite channel model</phrase> based on <phrase>Discrete Time</phrase> <phrase>Markov Chain</phrase> and Quality Degree — <phrase>DVB-RCS</phrase> is an open <phrase>satellite communications</phrase> standard allowing a bi-directional communication via satellite. These characteristics certainly promoted its enormous diffusion in the commercial area and the academic research interest. In this context, it is very important to test <phrase>DVB-RCS</phrase> systems using an efficient <phrase>satellite channel model</phrase>. In the literature many channel models have been carried out, but most of them work at the <phrase>bit level</phrase> or they investigate only some aspects of channel interaction. In this work we provide a high level <phrase>satellite channel model</phrase> based on <phrase>Discrete Time</phrase> <phrase>Markov Chain</phrase> (DTMC) modeling, useful in every simulation context. Our model, called Quality Degree-DTMC (QD-DTMC), is based on the concept of Quality Degree (QD) of a given observation windows: the idea is not to analyze a single packet, but fixing an observation window and evaluating the QD of the link, computing the Packet <phrase>Error Rate</phrase> (PER) associated to the specific window. The effectiveness of the proposed idea has been evaluated through a deep campaign of simulations.
Integrated Reliability Workshop Call for Papers The International Integrated Reliability Workshop focuses on ensuring semiconductor reliability through fabrication, design, testing, characterization, and simulation, as well as identification of the defects and physical mechanisms responsible for reliability problems. Through tutorials, discussion groups, special interest groups, and the informal format of the technical program, a unique environment is provided for understanding, developing, and sharing reliability technology for present and future semiconductor applications as well as ample opportunity for discussions and interactions with colleagues. Hot reliability topics for the workshop include: high-κ and nitrided SiO 2 gate dielectrics, NBTI, Cu interconnects and low-κ dielectrics, product reliability and burn-in strategy, impact of transistor degradation on circuit reliability, reliability <phrase>modeling and simulation</phrase>, SiGe and strained Si, III-V, SOI, <phrase>optoelectronics</phrase>, <phrase>single event</phrase> upsets, and reliability assessment of novel devices and future " nano "-technologies. We invite you to submit a presentation proposal that addresses any integrated semiconductor related reliability issue, including the following topics: • Designing-in reliability (products, circuits, systems, processes): methodologies and concepts, <phrase>modeling and simulation</phrase> tools, reliability-driven design rules and checkers, use of WLR and/ or reliability <phrase>test structures</phrase> for design rule verification, in-line detection and reliability analysis. • Customer product reliability requirements / manufacturer reliability tasks: limits to achieving future reliability targets, reliability evaluation methodologies and reporting systems, data bases, chip reliability, product reliability extrapolation to use conditions, wafer and package burn-in, packaging, strategies to eliminate burn-in, correlation between process, yield, and reliability, qualification strategies. • Root cause defects, physical mechanisms, and simulations nature of defects, physical and electrical characterization of defects, defect generation models, process induced defects and degradation, modeling/simulation of reliability related circuit constraints, accelerated testing and lifetime extrapolation. • Identification and characterization of reliability effects: <phrase>failure mechanisms</phrase> in new materials and device structures, reliability aspects of: high-k gate stack, Cu interconnects and low-k dielectrics, MOS and bipolar transistors including FinFETs and 3D gates, SiGe and strained Si, SOI, MEMS devices, <phrase>optoelectronics</phrase>, high voltage devices, unique reliability phenomena and <phrase>failure mechanisms</phrase> in <phrase>Non-Volatile</phrase> memories, new memory technologies, MRAM, <phrase>nanotechnology</phrase> reliability assessment, and limits to accelerated stressing. • <phrase>Deep sub-micron</phrase> transistor and circuit reliability: <phrase>single event effects</phrase> and <phrase>soft errors</phrase>, ESD, electromigration, mechanical stress related issues, hot carrier effects, NBTI, dielectric breakdown, reliability extrapolation, impact of new material systems, <phrase>modeling and simulation</phrase>, impact of scaling, • <phrase>Wafer level</phrase> reliability tests, test approaches, and reliability <phrase>test structures</phrase>: fast stress tests and analysis methodologies, reduction in development time, in-line monitors, …
Hybrid <phrase>Neural Network Architecture</phrase> for On-Line <phrase>Learning Approaches</phrase> to machine intelligence based on brain models have stressed the use of neural networks for generalization. Here we propose the use of a hybrid <phrase>neural network architecture</phrase> that uses two kind of <phrase>neural networks</phrase> simultaneously: (i) a surface learning agent that quickly adapt to new modes of operation; and, (ii) a <phrase>deep learning</phrase> agent that is very accurate within a specific regime of operation. The two networks of the <phrase>hybrid architecture</phrase> perform complementary functions that improve the overall performance. The performance of the <phrase>hybrid architecture</phrase> has been compared with that of back-propagation perceptrons and the CC and FC networks for chaotic <phrase>time-series</phrase> prediction, the CATS benchmark test, and smooth <phrase>function approximation</phrase>. It has been shown that the <phrase>hybrid architecture</phrase> provides a superior performance based on the RMS error criterion.
Taint-based directed whitebox fuzzing We present a new automated white box fuzzing technique and a tool, BuzzFuzz, that implements this technique. Unlike standard fuzzing techniques, which randomly change parts of the input file with little or no information about the underlying <phrase>syntactic structure</phrase> of the file, BuzzFuzz uses dynamic taint tracing to automatically locate regions of original seed input files that influence values used at key program attack points (points where the program may contain an error). BuzzFuzz then automatically generates new fuzzed test input files by fuzzing these identified regions of the original seed input files. Because these new test files typically preserve the underlying <phrase>syntactic structure</phrase> of the original seed input files, they tend to make it past the initial input parsing components to exercise code deep within the semantic core of the computation. We have used BuzzFuzz to automatically find errors in two <phrase>open-source</phrase> applications: Swfdec (an <phrase>Adobe</phrase> <phrase>Flash player</phrase>) and MuPDF (a PDF viewer). Our results indicate that our new directed fuzzing technique can effectively expose errors located deep within large programs. Because the directed fuzzing technique uses taint to automatically discover and exploit information about the input <phrase>file format</phrase>, it is especially appropriate for testing programs that have complex, highly structured input file formats.
Research Statement Combining Ensembles for <phrase>Semi-supervised Learning</phrase> [Webpage] [Link to CV] My research has centered around <phrase>machine learning</phrase>, which has exploded in the past two decades. The field has addressed <phrase>cornerstone</phrase> problems that recur in myriad application areas, and solved them with general, useful algorithms that are ultimately state-of-the-art across many of these areas given enough data, using statistics and other mathematics. I have been motivated during my PhD by this remarkable data-driven combination of generality and practicality, working to devise new algorithms that exploit <phrase>data structure</phrase> in practical learning scenarios. I have focused on two specific problem areas which share the above characteristics, answering the following questions over the course of my PhD: 1. <phrase>Semi-Supervised Learning</phrase> with Ensembles of Predictors: How can we devise an efficient, practical, and interpretable algorithm that uses large quantities of <phrase>unlabeled data</phrase> to best put together the predictions of a collection of classifiers of varying competence? 2. Sequential Algorithms and Stopping: How can we robustly adapt traditional statistical hypothesis tests to report accurate results when the <phrase>sample size</phrase> is unknown? How can we use as few samples as possible to detect an effect of unknown magnitude? These areas have interested me because they recur frequently in common practical problems, and yet are still new enough to researchers to present many basic fresh challenges. I now discuss my work in each of them in the first two sections. In the third section, I discuss my future interests, which involve applications of such algorithms to interdisciplinary research. In the fundamental learning problem of (binary) classification, each datum has one of two labels; the learner is required to predict those of some <phrase>unlabeled data</phrase>, given a set of <phrase>labeled data</phrase>. Many widely-used methods exist for this problem, like linear predictors, <phrase>decision trees</phrase>, Bayesian classifiers, and <phrase>deep neural nets</phrase>. A typical workflow might involve using the <phrase>labeled data</phrase> to train several different such algorithms and estimate their <phrase>error rates</phrase>, and finally choosing the best of the algorithms with which to predict on the <phrase>unlabeled data</phrase>. Justifying this empirical risk minimization procedure is part of the <phrase>bedrock</phrase> of classical learning theory ([Vapnik, 1982]). However, practical cutting-edge usage very frequently involves aspects not addressed in this account, which I have focused on in my work. a) First and foremost, reliably <phrase>labeled data</phrase> are expensive to obtain in many applications in medicine, NLP, and other areas – the phrase "<phrase>big data</phrase>" in classification often solely signifies abundant unlabeled …
Mathematical Problems Who of us would not be glad to lift the veil behind which the future lies hidden; to cast a glance at the next advances of our science and at the secrets of its development during future centuries? What particular goals will there be toward which the leading mathematical spirits of coming generations will strive? What new methods and new facts in the wide and rich field of mathematical thought will the new centuries disclose? History teaches the continuity of the development of science. We know that every age has its own problems, which the following age either solves or casts aside as profitless and replaces by new ones. If we would obtain an idea of the probable development of mathematical knowledge in the immediate future, we must let the unsettled questions pass before our minds and look over the problems which the science of today sets and whose solution we expect from the future. To such a review of problems the present day, lying at the meeting of the centuries, seems to me well adapted. For the close of a great epoch not only invites us to look back into the past but also directs our thoughts to the unknown future. The deep significance of certain problems for the advance of mathematical science in general and the important rôle which they play in the work of the individual investigator are not to be denied. As long as a branch of science offers an abundance of problems, so long is it alive; a lack of problems foreshadows <phrase>extinction</phrase> or the cessation of independent development. Just as every human undertaking pursues certain objects, so also mathematical research requires its problems. It is by the solution of problems that the investigator tests the temper of his steel; he finds new methods and new outlooks, and gains a wider and freer horizon. It is difficult and often impossible to <phrase>judge</phrase> the value of a problem correctly in advance; for the final award depends upon the grain which science obtains from the problem. Nevertheless we can ask whether there are general criteria which mark a good mathematical problem. An old <phrase>French</phrase> <phrase>mathematician</phrase> said: " A mathematical theory is not to be considered complete until you have made it so clear that you can explain it to the first man whom you meet on the street. " This clearness and ease of comprehension, here insisted …
Cross-Domain Dependency Parsing Using a <phrase>Deep Linguistic</phrase> Grammar Pure statistical parsing systems achieves high in-domain accuracy but performs poorly out-domain. In this paper, we propose two different approaches to produce syntactic dependency structures using a large-scale hand-crafted HPSG grammar. The dependency backbone of an HPSG analysis is used to provide general linguistic insights which, when combined with state-of-the-art statistical dependency parsing models, achieves performance improvements on out-domain tests. †
Geotechnical Parameters and Distribution Characteristics of the <phrase>Cobalt</phrase>-rich Manganese Crust for the <phrase>Miner</phrase> Design The importance of deep-ocean deposits for future <phrase>cobalt</phrase> resources and geological distribution characteristics have been well recognized. However, no sufficient geotechnical information of the <phrase>cobalt</phrase>-rich manganese deposit region is available to aid the design of seafloor mining systems. Based on the results of a recent large-diameter gravity coring, a model test of the coring, and analyses of geotechnical properties of <phrase>seamount</phrase> <phrase>sediments</phrase>, topographic and microtopographic distribution of the deposits, strength characteristics of the crusts and the <phrase>substrates</phrase>, and the geotechnical distribution characteristics of the deposits are presented. Although the present data represent a small coverage, the data will be useful as preliminary <phrase>miner</phrase> design parameters, and the initial design parameters are also discussed.
Extrinsic <phrase>embryonic</phrase> sensory stimulation alters multimodal behavior and cellular activation. <phrase>Embryonic</phrase> vision is generated and maintained by spontaneous neuronal <phrase>activation patterns</phrase>, yet extrinsic stimulation also sculpts sensory development. Because the sensory and motor systems are interconnected in embryogenesis, how extrinsic sensory activation guides multimodal differentiation is an important topic. Further, it is unknown whether extrinsic stimulation experienced near sensory sensitivity onset contributes to persistent brain changes, ultimately affecting postnatal behavior. To determine the effects of extrinsic stimulation on multimodal development, we delivered auditory stimulation to bobwhite <phrase>quail</phrase> groups during early, middle, or late embryogenesis, and then tested postnatal behavioral responsiveness to auditory or visual cues. Auditory preference tendencies were more consistently toward the <phrase>conspecific</phrase> stimulus for animals stimulated during late embryogenesis. Groups stimulated during middle or late embryogenesis showed altered postnatal species-typical visual responsiveness, demonstrating a persistent multimodal effect. We also examined whether auditory-related brain regions are receptive to extrinsic input during middle embryogenesis by measuring postnatal cellular activation. Stimulated <phrase>birds</phrase> showed a greater number of ZENK-immunopositive cells per unit volume of brain tissue in deep optic tectum, a <phrase>midbrain</phrase> region strongly implicated in multimodal function. We observed similar results in the medial and caudomedial nidopallia in the telencephalon. There were no ZENK differences between groups in <phrase>inferior colliculus</phrase> or in caudolateral nidopallium, <phrase>avian</phrase> analog to <phrase>prefrontal cortex</phrase>. To our knowledge, these are the first results linking extrinsic stimulation delivered so early in embryogenesis to changes in postnatal multimodal behavior and cellular activation. The potential role of competitive interactions between the sensory and motor systems is discussed.
Using Generation for Grammar Analysis and <phrase>Error Detection</phrase> We demonstrate that the bidirectionality of deep grammars, allowing them to generate as well as parse sentences, can be used to automatically and effectively identify errors in the grammars. The system is tested on two implemented HPSG grammars: Jacy for Japanese, and the <phrase>ERG</phrase> for English. Using this system, we were able to increase generation coverage in Jacy by 18% (45% to 63%) with only four weeks of grammar development.
Simultaneous Retrieval of <phrase>Sea Surface</phrase> <phrase>Wind Speed</phrase> and <phrase>Sea Surface Temperature</phrase> from a Multi-frequency Scanning <phrase>Microwave Radiometer</phrase> Derivation of geophysical parameters from satellite measured <phrase>brightness-temperature</phrase> (T B) is an important aspect of satellite <phrase>remote sensing</phrase>. Primarily, this involves development of complex inversion algorithms and empirical relations comprising T B and in situ data for parameter retrieval and algorithm validation. In the present work, an <phrase>Artificial Neural Network</phrase> model has been attempted to simultaneously obtain <phrase>sea surface</phrase> <phrase>wind speed</phrase> (WS) and <phrase>sea surface temperature</phrase> (SST) utilizing T B from 8 channels (including vertical and horizontal polarizations) of Multi-frequency Scanning <phrase>Microwave Radiometer</phrase> on board <phrase>Indian</phrase> <phrase>Remote Sensing</phrase> Satellite (<phrase>IRS</phrase>-P4) and <phrase>deep sea</phrase> ocean <phrase>buoys</phrase> in the <phrase>North Indian</phrase> Ocean region. The ANN obtained values are then compared with actual in situ observations as a test for the performance of the model. It is concluded that the ANN model is able to provide good estimates of WS and SST within acceptable error limits. The goal of the present work is to pre-establish the suitability of ANN approach for geophysical parameter retrieval from satellite measured T B in the <phrase>Indian</phrase> context particularly keeping in view the forth coming satellite launches like Megha Tropiques and Oceansat-3.
Novel Hierarchical Test Architecture for Soc <phrase>Test Methodology</phrase> Using Ieee Test Standards —SOC <phrase>test methodology</phrase> in ultra <phrase>deep sub-micron</phrase> (UDSM) technology with reasonable test time and cost has begun to satisfy <phrase>high quality</phrase> and reliability of the product. A novel hierarchical test architecture using <phrase>IEEE standard</phrase> 1149.1, 1149.7 and 1500 compliant facilities is proposed for the purpose of supporting flexible test environment to ensure SOC <phrase>test methodology</phrase>. Each embedded core in a system-on-a-chip (SOC) is controlled by test access ports (TAP) and TAP controller of <phrase>IEEE standard</phrase> 1149.1 as well as tested using <phrase>IEEE standard</phrase> 1500. An SOC device including TAPed cores is hierarchically organized by <phrase>IEEE standard</phrase> 1149.7 in wafer and chip level. As a result, it is possible to select/deselect all cores embedded in an SOC flexibly and reduce <phrase>test cost</phrase> dramatically using star scan topology.
The Cognitive Representation of <phrase>Computer-supported</phrase> Instructional Tools Two studies are reported whose aim was to assess whether undergraduates' cognitive representations of the psychological correlates of <phrase>computer-supported</phrase> instructional tools (CSIT) vary according to the features of tools themselves. A questionnaire investigating participants' conceptions about motivational and emotional aspects of learning through CSIT, behavior during the <phrase>learning process</phrase>, required capacities, mental operations, <phrase>metacognition</phrase>, preferred style of thinking, cognitive benefits and <phrase>learning outcomes</phrase> was employed in both studies. In Study 1 undergraduates were requested to <phrase>judge</phrase> to what extent such issues are involved in different kinds (virtual simulations, Web forums, and so on) of CSIT, whereas in Study 2 the same issues were considered by making reference to different dimensions (hypertext, multimedia, and so forth) of CSIT. <phrase>Results showed</phrase> that students have a well-defined and deep-rooted conception about what CSIT can introduce into a <phrase>learning process</phrase>, by attributing different psychological correlates to different kinds of tools. However, undergraduates failed to recognize that distinct dimensions of CSIT involve different psychological correlates. Implications of these findings for education and tool designing are discussed. From Goals to Effects: "Objective" and "Subjective" Perspectives <phrase>Computer-supported</phrase> instructional tools (CSIT) are often presented as instruments that teachers, trainers, and tutors might adopt in order to try to solve problems that they encounter in their job, particularly to lead trainees to overcome the difficulties that they experience in learning. The main question that educators ask when someone proposes to them to employ technological devices is: «Do this tool actually produce the expected outcomes?» (Giles, 2003). A way to conceptualize this question is the following. There is a need that requires to be satisfied (for example, rising the level of trainees' motivation) or a goal to be reached (for instance, improving students' understanding of a hard concept). Instructors look for, or devise by themselves, a tool which should help students in satisfying that need or in achieving that goal. Then they induce learners to use that tool. The outcomes produced by the tool are detected in order to test the alleged efficacy of the tool. Such an approach stresses the importance of the characteristics of the tool to be employed, assuming that the higher is its quality, the better are the learning results. In this perspectives the way of employing a tool derives directly from the affordances of the tool: What was "put" into the tool (provided that it was phenomenologically salient), that will be used properly by students, …
Learning from Text with Diagrams: Promoting <phrase>Mental Model</phrase> Development and Inference Generation Learning with Text and Diagrams Two experiments investigated <phrase>learning outcomes</phrase> and comprehension processes when students learned about the heart and <phrase>circulatory</phrase> system using (a) text only, (b) text with simplified diagrams designed to highlight important structural relations, or (c) text with more detailed diagrams reflecting a more accurate representation. Experiment 1 found that both types of diagrams supported <phrase>mental model</phrase> development, but simplified diagrams best supported factual learning. Experiment 2 replicated learning effects from Experiment 1 and tested the influence of diagrams on novices' comprehension processes. Protocol analyses indicated that both types of diagrams supported inference generation and reduced comprehension errors, but simplified diagrams most strongly supported information integration during learning. <phrase>Visual representations</phrase> appear to be most effective when they are designed to support the <phrase>cognitive processes</phrase> necessary for deep comprehension. As multimedia technology becomes increasingly popular in formal and informal educational settings, the importance of research investigating learning with visual and verbal materials takes on added value. Understanding the ways in which visual materials influence learning will be essential to developing multimedia tools with consistent and predictable benefits. Advancing technology has meant that multimedia often now includes complex forms of interactive and computationally intensive presentations; however, multimedia can be more simply defined as any presentation that includes verbal and <phrase>visual information</phrase> (Mayer, 2001). In practice, basic types of multimedia—such as pictures and text—still appear to be frequently used. Currently, many digital and print materials use pictures, diagrams, and text as their primary communication format. But how does the visual representation of information influence learning? Can changes in comprehension processes account for the impact of diagrams on learning? The purpose of this research was to investigate potential effects of different diagram representations on students' <phrase>learning outcomes</phrase> and comprehension processes when diagrams were added to a science text. Early research on pictures and text consistently demonstrated that students learned more after reading illustrated versus nonil-lustrated text (for a review, see Levie & Lentz, 1982). These studies predominantly administered memory measures for the source materials, including <phrase>multiple-choice</phrase> and fill-in-the-blank tests. But a long history of cognitive research has distinguished between rote memorization and deeper understanding (e. for a discussion). Deeper learning— evidenced by measures that assess application and transfer of information—was not widely tested in early research on illustrations. However, one early set of studies (see Dwyer, 1967, 1968, 1975) did test both memory and deep comprehension for an illustrated text. Dwyer (1967, 1968, 1975) found benefits …
<phrase>Gamma-ray</phrase> Irradiation Effects on Cmos Image Sensors in Deep Sub-micron Technology INTRODUCTION CMOS Active Pixel Sensors (APS) <phrase>excel</phrase> in domains that include <phrase>low power</phrase> operation and on-chip integration of analog and digital circuitry. Since these sensors are utilized for applications involving the detection of signals as low as a few <phrase>electrons</phrase>, radiation tolerance of such devices is of primary concern. All possible radiation effects are usually grouped into three basic types: transient effects (not dealt in this study), <phrase>ionization</phrase> damage and displacement damages [1], [2], [6]. <phrase>Ionization</phrase> damage has been considered to be the dominant mechanism when energetic <phrase>photons</phrase> (γ and X-rays) interact with <phrase>solid-state</phrase> matter. The major concerns due to this damage are charge build-up in the gate dielectric and radiation induced interface states. The introduction of discrete energy levels at the Si-Si0 2 interface leads to increased generation rates and thus higher surface <phrase>leakage currents</phrase>. Similarly, displacement of lattice <phrase>atoms</phrase> in the bulk leads to modified minority carrier lifetimes and increased bulk-generated <phrase>leakage currents</phrase> [2], [3], [4], [5]. EXPERIMENTAL " Pinned " CMOS photodiodes (Fig. 1) utilize a p + pinning layer that shields the <phrase>photodiode</phrase> from surface effects that contribute to the leakage mechanism. The doping of the layers are chosen such that the <phrase>photodiode</phrase> is depleted completely. One of the most dominant dark current mechanisms in these structures are the defective sidewalls and the edges of shallow trench isolations (STIs) separating the photodiodes [9], [10]. To test the effects of radiation, <phrase>test-structures</phrase> with and without p-well <phrase>protected</phrase> STIs (Shallow Trench Isolation) were fabricated in <phrase>Philips</phrase>' 0.18-μm <phrase>CMOS technology</phrase> (Table. 1). The gap between the STI and the <phrase>photodiode</phrase> is represented by the parameter NTA. The structures were tested by irradiating them with γ-rays (1.17 <phrase>MeV</phrase>, 1.33 <phrase>MeV</phrase>); dose rate of 75.9 Gray/min. Solving the <phrase>continuity equation</phrase> of a usual p + /n <phrase>photodiode</phrase> derives an analytic model for the internal spectral response of pinned photodiodes. An equivalent diode reverse voltage V d , is used to represent the depleted diode. The contribution from the p-type epitaxial region is included for the contribution from carriers collected through diffusion. This model is used to estimate the optical degradation of the sensors due to irradiation. Standard <phrase>CMOS process</phrase> parameters have been used for the simulation. The dispersive transport phenomenon in the SiO 2 can be modeled on the concept of small polaron hopping, called as CTRW (continuous-time <phrase>random walk</phrase>). The transport process varies with the fourth power of the oxide …
Randomness and Non-<phrase>determinism</phrase> <phrase>Exponentiation</phrase> makes the difference between the bit-size of this line and the number (≪ 2 300) of particles in the known Universe. The expulsion of exponential time algorithms from Computer Theory in the 60's broke its <phrase>umbilical cord</phrase> from <phrase>Mathematical Logic</phrase>. It created a deep gap between deterministic computation and – formerly its unremarkable tools – randomness and non-<phrase>determinism</phrase>. Little did we learn in the past decades about the power of either of these two basic " freedoms " of computation, but some vague pattern is emerging in relationships between them. The pattern of similar techniques <phrase>instrumental</phrase> for quite different results in this area seems even more interesting. Ideas like multilinear and low-degree multivariate <phrase>polynomials</phrase>, Fourier transformation over low-periodic groups seem very illuminating. The talk surveyed some recent results. One of them, given in a stronger form than <phrase>previously published</phrase>, is described below. |x| will denote the length of string x. Let P be the set of fast, i.e. <phrase>computable</phrase> in time T f (x) = |x| O(1) , algorithms f (x) on binary strings. [Blum Micali 82, Yao 82] proposed a fast deterministic way to generate " nearly perfect " randomness, using the idea of a hard core or hidden bit. They assume certain length preserving functions f ∈P to be one-way (OWF), i.e. infeasible to invert (a non-deterministically easy task). Suppose it is hard to compute from f (x) not only x but even its one bit b(x) ∈ {±1}, b ∈P. Moreover, assume that even guessing b(x) with any noticeable correlation is infeasible. If f is <phrase>bijective</phrase>, f (x) and b(x) are both random and appear to be independent to any feasible test, thus increasing the initial amount |x| of randomness by one bit. Then, a short random seed x can be transformed into an arbitrary long string α(1), α(2),. . .: α(i) = b(f (i) (x)). Such α passes any feasible randomness test. [Goldreich Levin 89] showed that every OWF f has such a hidden bit with security of f and b polynomially related. It also gives more details on the definitions below. Here this result is strengthened to yield the same security for f and b. Let P be the set of probabilistic algorithms A(x, ω) using <phrase>coin</phrase>-flips ω ∈ {0, 1} I N and running in average over ω time E ω T A(x,ω) = |x| O(1). An inverter I ∈ P for f …
Fabrication and Reliability Testing of Copper-filled Through-silicon Vias for Three-dimensional Chip Stacking Applications ALL RIGHTS RESERVED ii ABSTRACT Through-silicon vias (TSVs) have been extensively studied because of their ability to achieve chip stacking for enhanced system performance. The fabrication process is becoming somewhat mature. However, reliability issues need to be addressed in order for an eventual transition from laboratory to production. This dissertation discusses the TSV <phrase>fabrication process</phrase>, testing results for TSV reliability investigation of an integration of TSVs and <phrase>capacitor</phrase> devices. In our laboratory, vias with tapered sidewalls are formed through a modified Bosch process using <phrase>deep reactive ion etching</phrase> (DRIE). <phrase>Cryogenic</phrase> etching is also considered as a means to etch vias without sidewall scalloping that is observed for the Bosch process. Vias are lined with <phrase>silicon dioxide</phrase> using plasma enhanced <phrase>chemical vapor deposition</phrase> (PECVD) followed by a sputter deposited <phrase>titanium</phrase> barrier and a copper seed layer before filling by a reverse pulse copper <phrase>electroplating</phrase> process. Following attachment of the process wafer to a carrier wafer, the process wafer is thinned from the backside by a combination of mechanical methods and <phrase>reactive ion etching</phrase> (RIE). Fabricated vias are subjected to thermal <phrase>cycling</phrase> with temperatures ranging from-25 <phrase>°C</phrase> to 125 <phrase>°C</phrase>. TSVs are shown to be stable with small increases in measured resistance for 200 cycles. In addition, small changes in resistance are observed when vias are held at elevated temperatures for extended periods of time. Integration of decoupling capacitors with TSVs represents a good alternative to conventional 2-D layouts to achieve <phrase>miniaturization</phrase> and increased density. Therefore, decoupling capacitors can be brought in close proximity to the active elements, thereby, reducing iii their parasitic inductance and allowing higher clock rates. In this study, capacitors with anodized <phrase>tantalum</phrase> as the dielectric are integrated with TSVs without negatively impacting their operation. The performance of these capacitors was evaluated by measuring <phrase>resonant frequency</phrase>, parasitic inductance, and parasitic resistance. iv DEDICATION To my lovely wife, Dr. Yolande Konguep Tchouangue. To my late parents Marie Madeleine and <phrase>Emmanuel</phrase> Tegueu.
Multielectrode microprobes for <phrase>deep-brain stimulation</phrase> fabricated with a customizable 3-D <phrase>electroplating</phrase> process Although <phrase>deep-brain stimulation</phrase> (DBS) can be used to improve some of the severe symptoms of <phrase>Parkinson's disease</phrase> (e.g., Bradykinesia, rigidity, and tremors), the mechanisms by which the symptoms are eliminated are not well understood. Moreover, DBS does not prevent <phrase>neurodegeneration</phrase> that leads to <phrase>dementia</phrase> or <phrase>death</phrase>. In order to fully investigate DBS and to optimize its use, a comprehensive <phrase>long-term</phrase> stimulation study in an <phrase>animal model</phrase> is needed. However, since the brain region that must be stimulated, known as the subthalamic nucleus (STN), is extremely small (500 microm <phrase>x 500</phrase> microm <phrase>x 1</phrase> mm) and deep within the rat brain (10 mm), the stimulating probe must have geometric and <phrase>mechanical properties</phrase> that allow accurate positioning in the brain, while minimizing tissue damage. We have designed, fabricated, and tested a novel micromachined probe that is able to accurately stimulate the STN. The probe is designed to minimize damage to the surrounding tissue. The probe shank is coated with gold and the electrode interconnects are insulated with <phrase>silicon nitride</phrase> for <phrase>biocompatibility</phrase>. The probe has four <phrase>platinum</phrase> electrodes to provide a variety of spatially distributed stimuli, and is formed in a novel 3-D plating process that results in a microwire like geometry (i.e., smoothly tapering diameter) with a corresponding mechanically stable shank.
On <phrase>a Viterbi Decoder</phrase> Design for Low <phrase>Power Dissipation</phrase> on <phrase>a Viterbi Decoder</phrase> Design for Low <phrase>Power Dissipation</phrase> (Abstract) Convolutinal coding is a coding scheme often employed in <phrase>deep space</phrase> communications and recently in digital <phrase>wireless communications</phrase>. Viterbi decoders are used to decode convolutional codes. Viterbi decoders employed in digital <phrase>wireless communications</phrase> are complex and dissipate large power. With the proliferation of battery powered devices such as <phrase>cellular phones</phrase> and <phrase>laptop</phrase> computers, <phrase>power dissipation</phrase>, along with speed and area, is a major concern in <phrase>VLSI design</phrase>. In this thesis, we investigated a <phrase>low-power design</phrase> of Viterbi decoders for <phrase>wireless communications</phrase> applications. In <phrase>CMOS technology</phrase> the major source of <phrase>power dissipation</phrase> is attributed to dynamic <phrase>power dissipation</phrase>, which is due to the switching of signal values. The focus of our research in the <phrase>low-power design</phrase> of Viterbi decoders is reduction of dynamic <phrase>power dissipation</phrase> at logic level in the <phrase>standard cell</phrase> design environment. We considered two methods, clock-gating and toggle-filtering, in our design. <phrase>A Viterbi decoder</phrase> consists of five blocks. The clock-gating was applied to the survivor path storage block and the toggle-filtering to the trace-back block of <phrase>a Viterbi decoder</phrase>. We followed the <phrase>standard cell</phrase> design approach to implement the design. The behavior of <phrase>a Viterbi decoder</phrase> was described in VHDL, and then the VHDL description was modified to embed the <phrase>low-power design</phrase>. A <phrase>gate level</phrase> circuit was obtained from the behavioral description through <phrase>logic synthesis</phrase>, and a full scan design was incorporated into the gate level circuit to ease testing. The gate level circuit was placed and routed to generate a layout of the design. Our experimental result shows the proposed design reduces the <phrase>power dissipation</phrase> of <phrase>a Viterbi decoder</phrase> by about 42 percent compared with the on without considering the <phrase>low-power design</phrase>. iii Acknowledgements Special thanks are due to my committee <phrase>chairman</phrase> and advisor Dr Dong S. <phrase>Ha</phrase>. It was through his patience, understanding and invaluable guidance that this work was accomplished. I would also like to express my appreciation for my committee members Dr Nathaniel J. Davis IV and Dr James R. Armstrong for serving as my committee members and commenting on this work. I am extremely grateful for the VISC computing resources without which none of this work would have been possible. I am greatly indebted to Dr. Will <phrase>Ebel</phrase> (Visiting Faculty), for his expert advice and corrections on this work. I would also like to first thank Han Bin and then to all other friends for their advise, guidance and help. My parents …
<phrase>Threshold Voltage</phrase> and <phrase>Power-Supply</phrase> Tolerance of CMOS Logic Design Families The advent of <phrase>deep submicron technologies</phrase> brings new challenges to <phrase>digital circuit</phrase> design. A reduced <phrase>threshold voltage</phrase> (<phrase>VT</phrase>) and <phrase>power supply</phrase> (VDD) in addition to process variabilities have a direct impact on <phrase>circuit design</phrase>. In a semiconductor environment it is conventionally thought that parametric yield is high and stable and that the main yield losses are functional. Although functional yield remains the main focus of attention, modern and future circuits may not have the presumed high parametric yield. We present a study that compares the tolerance to process variability of various design families for metrics including timing and <phrase>power consumption</phrase> under V,Vdd scalability using a <phrase>NAND gate</phrase> as a test vehicle. Basically, the fundamental limitations to the scaling of the <phrase>supply voltage</phrase> due to the statistical variation of MOS V, are investigated and defined. The four logic families under study are: static CMOS, Differential Complementary Voltage Swing Logic (DCVSL), Domino and Pass Logic. 1. Introduction The problem of <phrase>threshold voltage</phrase> variability is considered to be one of the most serious concerns in future <phrase>Gigabit</phrase> scale VLSI and ULSI designs[1-8].However, until now there have been limited studies of performance fluctuations of <phrase>digital circuits</phrase> in terms of the statistical fluctuation of the MOSFET <phrase>threshold voltage</phrase> (V T)[9]. With the non-scalability of V T , due to increase in leakage <phrase>power dissipation</phrase>, <phrase>circuit design</phrase> in the deep sub-micron regime becomes a major challenge.
3D heterogeneous system integration: application driver for 3D <phrase>technology development</phrase> Three dimensional integration complements semiconductor scaling; it enables a higher integration density as well as heterogeneous technology integration. Using 3D chip stacking, it is possible to extend the number of functions per 3D chip well beyond the near-term capabilities of traditional scaling. The 3D strata may be realized using advanced <phrase>CMOS technology</phrase> nodes but may also exploit a wide variety of device technologies to optimize system performance.  3D of electronic circuits has become a field of great interest in the <phrase>microelectronics</phrase> community. A wide variety of approaches are being proposed for an equally wide variety of applications. These approaches can be <phrase>categorized</phrase> based on their intersection with the interconnect hierarchy on chip and in the package: from local-to-global interconnect, to bond-pad-level, package and board level. This paper will focus on 3D interconnects using through-Si-vias that allow interconnectivity at the on-chip global interconnect level. The physical dimensions of these TSVs are 5<phrase>&mu</phrase>;m diameter and are 50<phrase>&mu</phrase>;m deep and can be placed at a minimum pitch of 10<phrase>&mu</phrase>;m.  The study and development of 3D System integration requires a concurrent exploration of technology and design. This is particularly the case when considering heterogeneous systems, exploiting different technologies that cannot be integrated in a single IC process. 3D-technology with <phrase>high density</phrase> TSV's allow for the seamless integration of circuit blocks (<phrase>IP blocks</phrase>) of different technologies in the same manner as a traditional SOC architecture, resulting in a 3D-SOC implementation: concurrently designed IC's in heterogeneous technologies.  The 3D integration approach should be <phrase>cost-effective</phrase> and consider the potential <phrase>compound</phrase> yield risks by adopting the appropriate testing and known-good-die strategies. A path finding <phrase>design flow</phrase> for 3D has been implemented that enables the study the system-level <phrase>trade-offs</phrase> at an early phase of the system design. In this path finding <phrase>design flow</phrase>, the <phrase>physical characteristics</phrase> of a 3D stacked circuit implementation are accounted for. This includes detailed electrical models for the TSV and <phrase>&mu</phrase>;bump interconnections, compact mechanical models that account for the impact of mechanical stresses inducted buy the TSV on neighboring components, mechanical stresses imposed by the package and compact thermal models that allow for a fast estimation of chip temperatures across the die within a 3D-stack.
Markovian channel modeling for <phrase>DVB-RCS</phrase> satellite systems based on observation window concept — The utilization of the Return Channel Satellite (RCS) in DVB systems allows a bi-directional communication via satellite. With this particular characteristic, <phrase>DVB-RCS</phrase> systems promoted its enormous diffusion in the commercial area and the academic research interest and, in this context, it is very important to test <phrase>DVB-RCS</phrase> systems using an efficient <phrase>satellite channel model</phrase>. Many channel models have been proposed in the literature, but most of them work at the <phrase>bit level</phrase> or they investigate only some aspects of channel interaction. In this paper we present a high level <phrase>satellite channel model</phrase> based on <phrase>Markov Chains</phrase> (MCs) modeling, useful in every simulation context: the proposed Markovian <phrase>Satellite Channel Model</phrase> (MSCM) is based on the concept of windowed observations and the idea is not to analyze a single packet, but fixing an observation window to evaluate the degradation level of the link, computing the PER associated to the specific window. The model has been verified through a deep campaign of simulations, which demonstrate how the introduced error is minimized.
Testing <phrase>Network-on-Chip</phrase> Communication Fabrics —<phrase>Network-on-chip</phrase> (NoC) communication fabrics will be increasingly used in many large multicore system-on-chip designs in the near future. A relevant challenge that arises from this trend is that the test costs associated with NoC infrastruc-tures may account for a significant part of the total test budget. In this paper, we present a novel methodology for testing such NoC architectures. The proposed methodology offers a tradeoff between test time and on-chip self-test resources. The <phrase>fault models</phrase> used are specific to deep submicrometer technologies and account for <phrase>crosstalk effects</phrase> due to interwire coupling. The novelty of our approach lies in the progressive reuse of the NoC infrastructure to transport <phrase>test data</phrase> to the components under test in a recursive manner. It exploits the inherent parallelism of the data transport mechanism to reduce the test time and, implicitly, the test cost. We also describe a suitable <phrase>test-scheduling</phrase> approach. In this manner, the <phrase>test methodology</phrase> developed in this paper is able to reduce the test time significantly as compared to <phrase>previously proposed</phrase> solutions, offering speedup factors ranging from 2× to 34× for the <phrase>NoCs</phrase> considered for <phrase>experimental evaluation</phrase>.
Number <phrase>Recognition System</phrase> Using Electroencephalogram (eeg) Signals This paper focuses on number recognition from feature extracted using Electroencephalogram (EEG) readings. <phrase>EEG signals</phrase> were from 6 volunteer subjects. A <phrase>random number generator</phrase> Graphics <phrase>User Interface</phrase> was developed in VB7. It is used to display numbers from 0 to 9 which worked as Visually <phrase>Evoked Potential</phrase> (VEP) for the experiment. The database of 6 male right-handed subjects in the age group of (20-25) was created and used as training <phrase>data set</phrase>. By exposing the same set of subjects to the GUI again, new EEG recordings were collected. This new set of EEG readings was considered as testing <phrase>data set</phrase>. The testing data was searched and matched with trained <phrase>data set</phrase> for recognizing pattern of each number. The experiments were conducted by concentrating on Beta signal and Linear discriminate analysis (<phrase>LDA</phrase>) was implemented to classify the data. The <phrase>recognition rate</phrase> observed was different for different numbers. Overall <phrase>recognition rate</phrase> observed was 68.33%. It is also seen that there exist a unique pattern for each number. Introduction Functional <phrase>brain imaging</phrase> techniques that are designed to measure an aspect of <phrase>brain function</phrase> can be employed to obtain tangible information related to brain activity. EEG is one such technique which measures the <phrase>electric fields</phrase> that are produced by the activity in the brain [1, 2]. <phrase>EEG signals</phrase> arise due to <phrase>electrical potential</phrase> produced by the brain. Mostly EEG analysis has been used clinically for pathologies and epilepsies since Hans Berger's recordings of electrical potentials from the Human <phrase>scalp</phrase>. More recently new interaction techniques which directly connect a <phrase>human brain</phrase> and a machine are in use. EEG spectrum contain characteristic waveforms which fall in 4 frequency bands viz alpha(8-13 Hz), beta(13-30 Hz), theta (4-8 Hz), delta(< than 4 Hz). Alpha waves are found in normal awake people, not engaged in intense mental activity, which disappear when a person is asleep. Beta waves with higher frequency are seen during intense mental activity and stress. Delta waves occur during <phrase>deep sleep</phrase>, during infancy and in serious organic brain diseases. Theta waves appear during emotional stress in adults in sleep, particularly during disappoint
Challenges and Methods in Testing <phrase>the Remote Agent</phrase> Planner <phrase>The Remote Agent</phrase> Experiment (RAX) on the <phrase>Deep Space 1</phrase> (DS1) mission was the first time that an <phrase>artificially intelligent</phrase> agent controlled a NASA spacecraft. One of the key components of <phrase>the remote agent</phrase> is an on-board planner. Since there was no opportunity for human intervention between plan generation and execution , extensive testing was required to ensure that the planner would not endanger the spacecraft by producing an incorrect plan, or by not producing a plan at all. The <phrase>testing process</phrase> raised many challenging issues, several of which remain open. The planner and <phrase>domain model</phrase> are complex, with billions of possible inputs and outputs. How does one obtain adequate coverage with a reasonable number of <phrase>test cases</phrase>? How does one even measure coverage for a planner? How does one determine plan correctness? Other issues arise from developing a planner in the context of a larger operations-oriented project, such as limited workforce and changing domain models, interfaces and requirements. As planning systems are fielded in mission-critical applications , it becomes increasingly important to address these issues. This paper describes the major issues that we encountered while testing <phrase>the Remote Agent</phrase> planner, how we addressed them, and what issues remain open.
Discrepant Esd-cdm Test System and Failure Yield Prediction between Esd Association and Jedec Standards CDM test system and verification method of ESDA and JEDEC standards have been studied. There are several different items. They can be <phrase>categorized</phrase> into 5 major items, which are charging system, discharging system, verification module, waveform verification, and classification level. Regarding waveform verifications at each stress level, ESDA system provides higher peak current whereas lower rise time and lower full width at half maximum, compared to JEDEC system. It implies that ESDA standard provides higher inductance in a discharge system and higher discharge energy, which make it more severe system. The current continuously increases with the stress level. The linear relationship of <phrase>stress conditions</phrase> by these standards can be obviously observed. The electrical failure yield of each standard system is then predicted by a stress condition of the other system. INTRODUCTION Along with the development of technology, modern electronic systems become largely integrated, so they have become more and more sensitive to <phrase>electrostatic discharge</phrase> (ESD). This phenomenon has turned to be a serious problem for IC products fabricated by <phrase>deep-submicron</phrase> <phrase>CMOS technology</phrase> although design effort and awareness are significantly improved [1-3]. It impacts the functionality, reliability and lifetime of ICs [4]. Therefore, <phrase>EOS</phrase>/ESD test is required to qualify products based on customer expectations to minimize failures due to ESD from human and mechanical handling. ESD events have been divided into 4 models, which are <phrase>Human Body</phrase> Model (HBM), Machine Model (MM), Charge Device Model (CDM), and Socket Device Model (SDM). Regarding the increase of automated component handling systems, CDM becomes a significantly test for microelectronic components. It is performed to classify the susceptibility of a device to determine its ESD withstand voltage to such ESD events. This CDM test simulates that the device itself becomes charge and is rapidly discharged when it approaches a conductive object. The rapid transfer of an <phrase>electrical charge</phrase> causes most of the ESD damages in the electronic manufacturing. To predict and qualify the ESD immunity level, there are several organizations that make the ESD related primary standards. They are <phrase>Electrostatic Discharge</phrase> Association (ESDA), Automotive Electronics Council (AEC), <phrase>Electronic Industries Alliance</phrase>/Joint Electron Device <phrase>Engineering Council</phrase> (EIA/JEDEC) and US Military Standard (MIL-STD). The commonly used standards are released by ESDA and JEDEC. They have made their own definitive stipulations on <phrase>test methods</phrase> and instruments. With the different specification, some unavoidable questions, such as the similarity of the test system verification, occur. In this paper, we present the
Two-dimensional Capacitive Micromachined Ultrasonic Transducer (cmut) Arrays for a Miniature Integrated Volumetric Ultrasonic Imaging System We have designed, fabricated, and characterized two-dimensional 16x16-element capacitive micromachined ultrasonic transducer (CMUT) arrays. The CMUT array elements have a 250-µm pitch, and when tested in immersion, have a 5-MHz center frequency and 99% fractional bandwidth. The fabrication process is based on standard silicon micromachining techniques and therefore has the advantages of high yield, <phrase>low cost</phrase>, and ease of integration. The transducers have a Si 3 N 4 membrane and are fabricated on a 400-µm thick silicon substrate. A low <phrase>parasitic capacitance</phrase> through-wafer via connects each CMUT element to a <phrase>flip-chip</phrase> bond pad on the back side of the wafer. Each through-wafer via is 20 µ m in diameter and 400 µ m deep. The interconnects form metal-insulator-semiconductor (MIS) junctions with the surrounding high-resistivity silicon substrate to establish isolation and to reduce <phrase>parasitic capacitance</phrase>. Each through-wafer via has less than 0.06 pF of <phrase>parasitic capacitance</phrase>. We have investigated a <phrase>Au</phrase>-In <phrase>flip-chip</phrase> bonding process to connect the 2D CMUT array to a custom <phrase>integrated circuit</phrase> (IC) with transmit and receive electronics. To develop this process, we fabricated fanout structures on silicon, and <phrase>flip-chip</phrase> bonded these test dies to a flat surface coated with gold. The average series resistance per bump is about 3 Ohms, and 100% yield is obtained for a total of 30 bumps.
<phrase>Deep Web</phrase> Collection Selection The author hereby grants permission to the <phrase>Queensland University of Technology</phrase>, <phrase>Brisbane</phrase> to reproduce and distribute publicly paper and electronic copies of this thesis document in whole or in part. Abstract The deep web contains a massive number of collections that are mostly invisible to <phrase>search engines</phrase>. These collections often contain <phrase>high-quality</phrase>, structured information that cannot be crawled using traditional methods. An important problem is selecting which of these collections to search. Automatic collection selection methods try to solve this problem by suggesting the best subset of <phrase>deep web</phrase> collections to search based on a query. A few methods for <phrase>deep Web</phrase> collection selection have proposed in Collection Retrieval Inference Network system and Glossary of Servers Server system. The drawback in these methods is that they require communication between the search broker and the collections, and need metadata about each collection. This thesis compares three different sampling methods that do not require communication with the broker or metadata about each collection. It also transforms some traditional <phrase>information retrieval</phrase> <phrase>based techniques</phrase> to this area. In addition, the thesis tests these techniques using INEX collection for total 18 collections (including 12232 <phrase>XML documents</phrase>) and total 36 queries. The experiment shows that the performance of sample-based technique is satisfactory in average.
Learning Quadrotor Dynamics Using <phrase>Neural Network</phrase> for Flight Control — Traditional <phrase>learning approaches</phrase> proposed for controlling quadrotors or <phrase>helicopters</phrase> have focused on improving performance for specific trajectories by iteratively improving upon a nominal controller, for example learning from demonstrations , iterative learning, and <phrase>reinforcement learning</phrase>. In these schemes, however, it is not clear how the information gathered from the training trajectories can be used to synthesize controllers for more general trajectories. Recently, the efficacy of <phrase>deep learning</phrase> in inferring <phrase>helicopter</phrase> dynamics has been shown. Motivated by the generalization capability of <phrase>deep learning</phrase>, this paper investigates whether a <phrase>neural network based</phrase> dynamics model can be employed to synthesize control for trajectories different than those used for training. To test this, we learn a quadrotor dynamics model using only translational and only rotational training trajectories, each of which can be controlled independently, and then use it to simultaneously control the yaw and position of a quadrotor, which is non-trivial because of nonlinear couplings between the two motions. We validate our approach in experiments on a quadrotor <phrase>testbed</phrase>.
Polarization lidar for shallow <phrase>water depth</phrase> measurement. A <phrase>bathymetric</phrase>, polarization lidar system transmitting at 532 nm and using a single <phrase>photomultiplier tube</phrase> is employed for applications of shallow <phrase>water depth</phrase> measurement. The technique exploits polarization attributes of the probed water body to isolate surface and floor returns, enabling constant fraction detection schemes to determine depth. The minimum resolvable <phrase>water depth</phrase> is no longer dictated by the system's laser or detector pulse width and can achieve better than 1 <phrase>order of magnitude</phrase> improvement over current <phrase>water depth</phrase> determination techniques. In laboratory tests, an Nd:YAG <phrase>microchip</phrase> laser coupled with polarization <phrase>optics</phrase>, a <phrase>photomultiplier tube</phrase>, a constant fraction discriminator, and a time-to-digital converter are used to target various water depths with an ice floor to simulate a <phrase>glacial</phrase> meltpond. Measurement of 1 cm water depths with an uncertainty of ±3 mm are demonstrated using the technique. This novel approach enables new approaches to designing laser <phrase>bathymetry</phrase> systems for shallow depth determination from remote platforms while not compromising deep <phrase>water depth</phrase> measurement.
Automation in <phrase>Design for Test</phrase> for Asynchronous Null Conventional Logic (ncl) Circuits 1.0 Introduction The semiconductor era has been thriving since the past four decades but as we continually attain smaller chip sizes comprising of millions of transistors, the problems grow at an unmitigated speed too. Testing such huge circuits poses a huge problem unless tackled prudently. The best case scenario given the current circumstances would be to create a favorable test environment on-chip by implementing <phrase>Design for Test</phrase> (DFT) techniques. Asynchronous <phrase>digital design</phrase> methodologies are currently gaining popularity since they enjoy the benefits of reduced area, power and low <phrase>EMI</phrase>. In spite of several advantages, due to the absence of the global clock and the presence of more state holding gates, testing these circuits presents a challenge to the designer. A DFT implementation for one such asynchronous class known as Null Conventional Logic (NCL) circuits has been proposed in this paper. The methodology discussed, exploits the technique of test points' insertion in order to improve the controllability of difficult feedback paths. Enhanced observability of long paths in the circuits is achieved by introducing of balanced tree structures or scan latches. This approach has been automated and works with industry standard tool suites, such as <phrase>Mentor Graphics</phrase> and Synopsis. The implemented tool transforms <phrase>gate level</phrase> NCL descriptions to those understood by the ATPG library provided with the DFT CAD tools. A <phrase>test coverage</phrase> increase of around 75% has been achieved with a less than 5% increase in terms of cost and area. The digital world has been dominated by the growth of synchronous techniques for nearly four decades due only to the ease of design of these circuits. Also, CAD tools for <phrase>synchronous designs</phrase> have become more advanced and sophisticated allowing total automation of several stages of the design process. However, with clock speeds nearing the <phrase>gigahertz</phrase> range and <phrase>CMOS technology</phrase> reaching <phrase>deep submicron</phrase> ranges, serious doubts have been cast over the suitability of <phrase>synchronous designs</phrase> for <phrase>next generation</phrase> processors and systems. Problems associated with clock synchronization, <phrase>power consumption</phrase>, and noise in <phrase>synchronous designs</phrase> has forced designers to look for alternatives [1]. Designers are looking at <phrase>asynchronous circuits</phrase> as a potential solution to these problems as they are modular and do not require clock synchronization. Some of the possible benefits of asynchronous techniques include <phrase>low power</phrase>, less <phrase>EMI</phrase>, less noise, increased robustness and <phrase>design-reuse</phrase> [2]. A variety of approaches exist for the design and implementation of <phrase>asynchronous circuits</phrase>. Huffman's model and Muller's model form …
Improving robustness against <phrase>reverberation</phrase> for <phrase>automatic speech recognition</phrase> <phrase>Reverberation</phrase> is a phenomenon observed in almost all enclosed environments. Human listeners rarely experience problems in comprehending speech in reverberant environments, but <phrase>automatic speech recognition</phrase> (ASR) systems often suffer increased <phrase>error rates</phrase> under such conditions. In this work, we explore the role of robust acoustic features motivated by human <phrase>speech perception</phrase> studies, for building ASR systems robust to <phrase>reverberation</phrase> effects. Using the dataset distributed for the " <phrase>Automatic Speech Recognition</phrase> In Reverberant Environments " (ASpIRE-2015) challenge organized by IARPA, we explore Gaussian <phrase>mixture models</phrase> (GMMs), <phrase>deep neural nets</phrase> (DNNs) and convolutional <phrase>deep neural networks</phrase> (CDNN) as candidate <phrase>acoustic models</phrase> for recognizing <phrase>continuous speech</phrase> in reverberant environments. We demonstrate that DNN-based systems trained with robust features offer significant reduction in word <phrase>error rates</phrase> (<phrase>WERs</phrase>) compared to systems trained with baseline mel-filterbank features. We present a novel time-frequency convolution neural net (TFCNN) framework that performs convolution on the <phrase>feature space</phrase> across both the time and frequency scales, which we found to consistently outperform the CDNN systems for all feature sets across all testing conditions. Finally, we show that further WER reduction is achievable through system fusion of n-best lists from multiple systems.
Advances in <phrase>Supply Chain Management</phrase>: Potential to Improve Forecasting Accuracy Advances in <phrase>Supply Chain Management</phrase> <phrase>Decision Support Systems</phrase>: Potential to Improve Forecasting Accuracy 3. <phrase>Statistical Model</phrase>: Var-mgarch Working Paper Series Abstract Forecasting is a necessity almost in any operation. However, the tools of forecasting are still primitive in view of the great strides made by research and the increasing abundance of data made possible by automatic identification technologies, such as, <phrase>RFID</phrase>. The relationship of various parameters that may change and impact decisions are so abundant that any credible attempt to drive meaningful associations are in demand to deliver the value from acquired data. This paper proposes some modifications to adapt an advanced forecasting technique (GARCH) with the aim to develop it as a <phrase>decision support</phrase> tool applicable to a wide variety of operations including <phrase>supply chain management</phrase>. We have made an attempt to coalesce a few different ideas toward a " solutions " approach aimed to model volatility and in the process, perhaps, better manage risk. It is possible that industry, governments, corporations, <phrase>businesses</phrase>, security organizations, consulting firms and academics with <phrase>deep knowledge</phrase> in one or more fields, may spend the next few decades striving to synthesize one or more models of effective <phrase>modus operandi</phrase> to combine these ideas with other emerging concepts, tools, technologies and standards to collectively better understand, analyze and respond to uncertainty. However, the <phrase>inclination</phrase> to reject deep rooted ideas based on inconclusive results from pilot projects are a detrimental trend and begs to ask the question whether one can aspire to build an <phrase>elephant</phrase> using mouse as a model. Forecasting is an ancient activity and has become more sophisticated in recent years. For a long time steady steps in a <phrase>time series</phrase> <phrase>data set</phrase>, such as simple trends or cycles (such as seasonals) were observed and extended into the future. However, now a mixture of <phrase>time series</phrase>, <phrase>econometrics</phrase> and <phrase>economic theory</phrase> models can be employed to produce several forecasts which can then be interpreted jointly or combined in sensible fashions to give a superior value. The variable being forecast is a <phrase>random variable</phrase>. Originally attention was largely directed towards the mean of this variable; later to the variance, and now to the whole <phrase>marginal distribution</phrase>. Pre-testing of the data to find its essential features has become important and that has produced modern techniques such as <phrase>cointegration</phrase>. The horizon over which the forecast is attempted is also important, and longer-run forecasts are now being considered as well as forecasts of "breaks" in the series. The question of evaluation of forecasts has also been …
<phrase>Reverse Engineering</phrase> of Protocols from Network Traces —Communication protocols determine how network components interact with each other. Therefore, the ability to derive a specification of a protocol can be useful in various contexts, such as to support deeper black-box testing or effective defense mechanisms. Unfortunately, it is often hard to obtain the specification because systems implement closed (i.e., undocumented) protocols, or because a time consuming translation has to be performed, from the textual description of the protocol to a format readable by the tools. To address these issues, we propose a new methodology to automatically infer a specification of a protocol from network traces, which generates automata for the protocol language and <phrase>state machine</phrase>. Since our solution only resorts to interaction samples of the protocol, it is well-suited to uncover the message formats and protocol states of closed protocols and also to automate most of the process of specifying open protocols. The approach was implemented in a tool and experimentally evaluated with publicly available FTP traces. Our results show that the inferred specification is a good approximation of the reference specification, exhibiting a high level of precision and recall. I. INTRODUCTION Network protocols regulate the communication among entities by defining the syntax and semantics of the messages, and the order in which they need to be exchanged. The ability to obtain a protocol specification can, therefore, play an important role in several contexts. For example, it can help on the implementation of effective defense mechanisms, such as firewalls and <phrase>intrusion detection</phrase> systems, that use the specifications of the protocols to accurately identify malicious traffic by performing <phrase>deep packet inspection</phrase> [1]. Testing tools can take as input a protocol specification to generate <phrase>test cases</phrase> that cover the protocol space for conformance testing [2] or to verify if a server is vulnerable to remote attacks [3]. However, it is typically hard to produce protocol specifications. Closed (or undocumented) protocols require <phrase>reverse engineering</phrase> the seemingly arbitrary set of <phrase>bytes</phrase> that compose each message in order to determine their meaning and structure. Open protocols, on the other hand, are well documented and their (textual) specification is readily available (e.g., <phrase>IETF</phrase> protocols), but obtaining their specification is also difficult and time consuming because developers have to carefully analyze the textual description of the protocol and translate it into the format supported by the tools. Automatic protocol <phrase>reverse engineering</phrase> can address most of these difficulties by deducing an approximate specification of a protocol …
Targeting Leakage Constraints during ATPG In previous technology generations <phrase>IDDQ testing</phrase> used to be a powerful technique to detect physical faults that are not covered by standard <phrase>fault models</phrase> or functional tests. Due to shrinking <phrase>feature sizes</phrase> and consequently increasing <phrase>leakage currents</phrase> <phrase>IDDQ testing</phrase> becomes difficult in the deep-sub-micron area. One of the problems is the vector dependency of <phrase>leakage current</phrase>. Even in good devices the <phrase>leakage current</phrase> may vary significantly from one <phrase>test vector</phrase> to the next. In this work we present an ATPG framework that allows to generate <phrase>test vectors</phrase> within tight constraints on <phrase>leakage currents</phrase>. The target range for the <phrase>leakage current</phrase> is automatically determined. Experiments on the ITC99 benchmark suite yield testsets that achieve 100% <phrase>fault coverage</phrase> for the larger circuits, even when the range is narrowed down to 50% of the standard deviation of random vectors.
Deep Slab Instability, <phrase>Decision Making</phrase>, and Consequences: a Case Study On March 8 th , 2008 an avalanche occurred on Mt. Eyak, near Cordova, <phrase>Alaska</phrase>. It resulted in the <phrase>death</phrase> of an avalanche forecaster, and the broken <phrase>femur</phrase> of another person. Through a summary of weather observations and <phrase>snow</phrase> profile data taken in the same area for weeks prior, this <phrase>case study</phrase> explores many factors associated with this event, including deep slab instabilities, stability tests, strain softening, triggering mechanisms, <phrase>human factors</phrase>, <phrase>decision making</phrase>, and consequences.
Eeg and <phrase>Bold-contrast Fmri</phrase> in Brain Eeg and <phrase>Bold-contrast Fmri</phrase> in Brain Acknowledgements <phrase>Cerebrovascular</phrase> reactivity, suppression of neuronal activity, <phrase>global and local</phrase> <phrase>brain injury</phrase> <phrase>Cerebrovascular</phrase> reactivity, suppression of neuronal activity, <phrase>global and local</phrase> <phrase>brain injury</phrase> Academic Dissertation to be presented with the assent of Abstract The purpose of the present study was to gain more insight into the blood <phrase>oxygen</phrase> level-dependent (BOLD)-contrast functional MRI (fMRI) in the brain and its connection to EEG, both in <phrase>global and local</phrase> scales of their temporal and spatial relations. <phrase>BOLD signal changes</phrase> were studied during <phrase>hyperventilation</phrase> (HV) induced EEG reactivity of intermittent rhythmic delta activity (<phrase>IRDA</phrase>). The <phrase>BOLD signal</phrase> in <phrase>gray matter</phrase> decreased 30% more in subjects with <phrase>IRDA</phrase> (N = 4) than in controls (N = 4), during the first two minutes of HV. This difference disappeared during <phrase>IRDA</phrase> in EEG. <phrase>BOLD signal changes</phrase> may provide additional information about dynamic hemodynamic changes relative to HV induced EEG reactivity. <phrase>BOLD signal changes</phrase> were investigated during sudden deepening of thiopental anesthesia into EEG burst-suppression level in pigs (N = 5). Positive (6–8%) or negative (-3–-8%) group average <phrase>BOLD signal changes</phrase> correlated to the thiopental bolus injection were seen. Positive and negative responses covered 1.6% and 2.3% of the brain voxels, respectively. <phrase>BOLD signal changes</phrase> in brain are associated with sudden deepening of thiopental anesthesia into EEG burst-suppression level, but they are spatially inconsistent and scarce. <phrase>Somatosensory</phrase> BOLD response was studied in brain before and after globally induced <phrase>methotrexate</phrase> (MTX) exposition in pigs (N = 4). After the MTX exposure, reduced (from 2–4% to 0–1%) or negative (-2% to-3%) BOLD responses were detected. <phrase>Somatosensory</phrase> <phrase>BOLD-contrast</phrase> response shows a slight difference in brain before and after globally induced MTX exposition. An experimental epilepsy model for development of simultaneous EEG and <phrase>BOLD-contrast fMRI</phrase> in the localization of epilepsy was developed and tested. Dynamic <phrase>penicillin</phrase> induced local epilepsy was applied in deep <phrase>isoflurane</phrase> anesthesia in pigs (N = 6). Relatively high (10–20%) and localized <phrase>BOLD signal</phrase> increase was found. The dynamic <phrase>penicillin</phrase> induced focal epilepsy model in deep <phrase>isoflurane</phrase> anesthesia with simultaneous EEG and <phrase>BOLD-contrast fMRI</phrase> is feasible for the development of these methods for localization of <phrase>epileptic</phrase> focus or foci. In conclusion, with careful experimental design and analysis, <phrase>BOLD-contrast fMRI</phrase> with EEG provides a potential tool for monitoring and localising functional changes in the brain. When you meet a master swordsman, show him your <phrase>sword</phrase>. When you meet a man who is not a <phrase>poet</phrase>, do not show him your <phrase>poem</phrase>.-<phrase>Rinzai</phrase>
SRAM <phrase>delay fault</phrase> modeling and test algorithm development With the advent of deep-submicron VLSI technologies, the working speed of SRAM circuits has grown to <phrase>a level</phrase> that at-speed testing of SRAM has become an <phrase>important issue</phrase>. In this paper, we present <phrase>delay fault</phrase> models for SRAM, i.e., the faults that affect the access time of the SRAM circuit. We also develop the test algorithm that detects these faults. The proposed SRAM <phrase>delay-fault test</phrase> algorithm has a complexity of Read/Write operations, where is the number of words and is the word count in a row.
Biologically Inspired KFLANN Place Fields for <phrase>Robot Localization</phrase> – This paper presents a hippocampal inspired <phrase>robot localization</phrase> model that provides a means for a simple robotic platform with ultrasonic sensors to localize itself. There have been published <phrase>neurobiological</phrase> experiments where rats were found to have hippocampal cell activations that positively correlate with the location of the animal [2, 3, 5]. Such activations found in the hippocamal region are usually called Place fields (PF) or Place cells (PC). The Place Field model presented in this paper was designed using a unique K-Means Fast Learning <phrase>Artificial Neural Network</phrase> (KFLANN) [13, 14, 15] and establishes a series of localization minima points that act as references for navigation. While such evidence of place cells are seen in hippocampal (CA1) and deep layers of the <phrase>entorhinal cortex</phrase> (EC) [4], from a literature search, it is uncertain if any applications were ever designed using this biological evidence. The intent of this paper is to focus on <phrase>experimental results</phrase> relevant for a proof-of-concept of <phrase>robot localization</phrase>, rather than illustrating a robustly tested navigation system. As such, basic ultrasonic based experiments will suffice. With some <phrase>experimental results</phrase>, we show that the KFLANN is suitable for implementing atomic place field vectors (APFV), a <phrase>data structure</phrase> to encapsulate localization information. Autonomous <phrase>unmanned</phrase> navigational systems have been dependent on localization sensors as a primary means for navigation. In outdoor autonomous systems such as those seen in the <phrase>DARPA Grand Challenge</phrase>, most vehicles would make use of combinations of GPS, RADAR, LIDAR and INS [6]. There has also been an increasing amount of research in using only vision to navigate autonomous platforms through spaces such as laboratories and corridors [5]. While these have provided <phrase>a level</phrase> of success, the extent of cognitive robotic navigation is far from being solved as a single failure in localization sensors usually leads to a catastrophic system failure. In the midst of man's quest to create robust autonomous robotic systems that can navigate effortlessly through clutter and uncertain environments, laboratory mice used in experiments are revealing secrets of their navigational capabilities. Perhaps one of the differences in <phrase>mammalian</phrase> navigation as compared with robotic navigation is the dependence in cognitive capabilities as opposed to the heavy reliance on sensor technology. Technological advances need to be complemented with advances in aspects of cognitive processings. We present a method where localization of a robot within a structured space is done using ultrasound information and a <phrase>compass</phrase> direction. The …
<phrase>Parkinson's disease</phrase> tremor-related metabolic network: Characterization, progression, and treatment effects The circuit changes that mediate parkinsonian tremor, while likely differing from those underlying akinesia and rigidity, are not precisely known. In this study, to identify a specific metabolic brain network associated with this disease manifestation, we used FDG PET to scan nine tremor dominant <phrase>Parkinson's disease</phrase> (PD) patients at baseline and during ventral intermediate (Vim) thalamic nucleus <phrase>deep brain stimulation</phrase> (DBS). <phrase>Ordinal</phrase> trends canonical variates analysis (OrT/CVA) was performed on the within-subject scan data to detect a significant spatial covariance pattern with consistent changes in subject expression during stimulation-mediated tremor suppression. The metabolic pattern was characterized by covarying increases in the activity of the <phrase>cerebellum</phrase>/<phrase>dentate nucleus</phrase> and <phrase>primary motor cortex</phrase>, and, to a less degree, the <phrase>caudate</phrase>/<phrase>putamen</phrase>. Vim stimulation resulted in consistent reductions in pattern expression (p<0.005, <phrase>permutation</phrase> test). In the absence of stimulation, pattern expression values (subject scores) correlated significantly (r=0.85, p<0.02) with concurrent accelerometric measurements of tremor amplitude. To validate this spatial covariance pattern as an objective network <phrase>biomarker</phrase> of PD tremor, we prospectively quantified its expression on an individual subject basis in independent PD populations. The resulting subject scores for this PD tremor-related pattern (PDTP) were found to exhibit: (1) excellent test-retest <phrase>reproducibility</phrase> (p<0.0001); (2) <phrase>significant correlation</phrase> with independent clinical ratings of tremor (r=0.54, p<0.001) but not akinesia-rigidity; and (3) significant elevations (p<0.02) in tremor dominant relative to atremulous <phrase>PD patients</phrase>. Following validation, we assessed the <phrase>natural history</phrase> of <phrase>PDTP expression</phrase> in early stage patients scanned longitudinally with FDG PET over a 4-year interval. Significant increases in <phrase>PDTP expression</phrase> (p<0.01) were evident in this cohort over time; rate of progression, however, was slower than for the PD-related akinesia/rigidity pattern (PDRP). We also determined whether <phrase>PDTP expression</phrase> is modulated by interventions specifically directed at parkinsonian tremor. While Vim DBS was associated with changes in PDTP (p<0.001) but not PDRP expression, <phrase>subthalamic nucleus</phrase> (STN) DBS reduced the activity of both networks (p<0.05). <phrase>PDTP expression</phrase> was suppressed more by Vim than by STN stimulation (p<0.05). These <phrase>findings suggest</phrase> that parkinsonian tremor is mediated by a distinct metabolic network involving primarily cerebello-thalamo-cortical pathways. Indeed, effective treatment of this symptom is associated with significant reduction in <phrase>PDTP expression</phrase>. Quantification of treatment-mediated changes in both PDTP and PDRP scores can provide an objective means of evaluating the differential effects of novel antiparkinsonian interventions on the different motor features of the disorder.
Supporting user <phrase>extensibility</phrase> in a networked virtual . . .  C-VISions is a networked multiuser 3D <phrase>virtual environment</phrase> that allows users to interact with each other and learn experientially and collaboratively in simulation and articulation-oriented <phrase>virtual worlds</phrase>. The capabilities of <phrase>virtual worlds</phrase> can be more fully exploited if the freedom to create new worlds is given to the users. This paper describes the development of a graphical user interface (GUI) that allows users to do this. In the context of our research, the intended users are mainly <phrase>secondary school</phrase> students currently in the process of acquiring new <phrase>scientific knowledge</phrase>. In a <phrase>virtual world</phrase> scenario where the <phrase>virtual environment</phrase> mimics the <phrase>real world</phrase> environment in terms of physics phenomena, users are given a free hand in constructing science experiments in whatever ways they wish. Such a <phrase>learning environment</phrase> allows users to freely construct and test their personal scientific hypotheses. By relying on graphical tools that aid the visualization of <phrase>experiment results</phrase>, the <phrase>learning process</phrase> is further facilitated and the <phrase>learning environment</phrase> can help students overcome misconceptions and enhance deep understanding of scientific principles.
<phrase>Pap</phrase> smear <phrase>image classification</phrase> using <phrase>convolutional neural network</phrase> This article presents the result of a comprehensive study on <phrase>deep learning</phrase> based <phrase>Computer Aided</phrase> Diagnostic techniques for classification of <phrase>cervical</phrase> <phrase>dysplasia</phrase> using <phrase>Pap</phrase> smear images. All the experiments are performed on a real <phrase>indigenous</phrase> <phrase>image database</phrase> containing 1611 images, generated at two diagnostic centres. Focus is given on constructing an effective feature vector which can perform multiple level of representation of the features hidden in a <phrase>Pap</phrase> smear image. For this purpose <phrase>Deep Convolutional Neural Network</phrase> is used, followed by <phrase>feature selection</phrase> using an unsupervised technique with Maximal Information Compression Index as similarity measure. Finally performance of two classifiers namely Least Square <phrase>Support Vector Machine</phrase> (LSSVM) and Softmax Regression are monitored and classifier selection is performed based on five measures along with five <phrase>fold cross validation</phrase> technique. Output classes reflects the established <phrase>Bethesda</phrase> system of classification for identifying pre-<phrase>cancerous</phrase> and <phrase>cancerous</phrase> <phrase>lesion</phrase> of <phrase>cervix</phrase>. The proposed system is also compared with two existing conventional systems and also tested on a publicly available database. <phrase>Experimental results</phrase> and comparison shows that proposed system performs efficiently in <phrase>Pap</phrase> smear classification.
Modeling User Goals for Notification System Interfaces Responding to the need within the <phrase>human-computer interaction</phrase> field to address ubiquitous and <phrase>multitasking</phrase> systems more scientifically, this research will investigate the usefulness of a new research framework for a particular class of systems. Notification systems emerge as interfaces that are typically used in a divided-attention, <phrase>multitasking</phrase> situation, attempting to deliver current, valued information through a variety of platforms and modes in an efficient and effective manner. We recognize a lack of unifying framework for understanding, classifying, analyzing, developing , evaluating, and discussing notification systems–fundamentally inhibiting scientific growth and knowledge reuse. To this end, we have developed a framework (referred to here as the <phrase>IRC</phrase> framework) for notification systems research. The framework is based on a core taxonomy of critical parameters that describe notification system user goals in terms of expected sources of interaction utility. We conjecture that our framework will allow an improved usability engineering process and increased research cohesion to emerge for notification systems, lending efficiency to several aspects of a system <phrase>design cycle</phrase>. The proposed research will investigate this notion, focusing on three key aspects: 1. System description, allowing widely understood articulation of design objectives that are focused on critical user requirements. 2. Interface evaluation, enabling comparison of the design and user's models, while supporting generalizability of research and early identification of usability concerns. 3. Design comparison and reuse, saving time and effort in <phrase>requirements analysis</phrase> and early design stages by enabling <phrase>design reuse</phrase> and appreciation of design progress—both which are informed by reference task <phrase>benchmarking</phrase>. Each of these <phrase>design cycle</phrase> improvements can be provided, if supporting <phrase>analysis tools</phrase> and library systems are properly developed and tested. These development efforts are significant and require deep knowledge of the <phrase>IRC</phrase> framework, notification systems designs, usability engineering, and <phrase>design reuse</phrase> paradigms. Contributions resulting from the proposed research include a valuable and living implementation of a notification systems component claims library, a design assessment system for notification system conceptual models, and a demonstration of a research agenda that is integrated with key HCI processes. Proposed efforts include development and execution of assessment cases, procedures, and instruments to determine the feasibility of these assertions. Notification system development efforts emerging from classes, seminars, and independent study activities will contribute to this research as <phrase>test cases</phrase>. Successful execution and completion of this research will create many new research opportunities for other students and researchers within the HCI community.
Gain Control and Linearity Improvement for Low Noise Amplifiers in 5ghz Direct Conversion Receivers The members of the Committee appointed to examine the thesis of MALLESH RAJASHEKHARAIAH find it satisfactory and recommend that it be accepted. <phrase>______</phrase><phrase>______</phrase><phrase>_____</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase> Chair <phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>_____ _</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase>____ iii ACKNOWLEDGMENT This project was funded by the <phrase>NSF</phrase> Center for Design of Analog-Digital <phrase>Integrated Circuits</phrase> (www.eecs.wsu.edu/~cdadic) and I would like to express my heartfelt gratitude to CDADIC for the same. I would like to acknowledge the support and guidance received from my advisor Dr Heo, who made all this possible. Thanks are due also to my team member and friend, Parag Upadhyaya for all the learning and continued support which has thoroughly helped me in my program. I would also like to express my deep sense of gratitude to Prof. George LaRue for all the support he has provided in several ways, be it some problem in the testing lab, software tools or the regular dose of laughter the year round! Thanks to all my thesis committee members, Prof. Also, thanks to Jeff Dykstra of <phrase>Motorola</phrase> and Matt Miller of <phrase>Freescale Semiconductor</phrase> for their valuable suggestions and guidance during the course of the project. Special mention about Alaina McCully and Joanne Buteau for their administrative support. Thanks to Mark Fuller also, for support with the measurements.
Acoustic <phrase>Compaction Layer</phrase> Detection The ASAE standardized tool to detect the depth and strength of compaction layers in the field is the cone penetrometer. Since this method is point-to-point, researchers have experimented with on-the-fly alternatives that can be used as, or in combination with, a standard <phrase>tillage</phrase> tool. On-the-fly <phrase>compaction layer</phrase> sensing also enables adaptive <phrase>tillage</phrase>, where the soil is only tilled as deep as necessary, which can lead to significant energy savings and <phrase>erosion</phrase> reduction. Wedged tips, strain gauges mounted on a deflecting tine, air bubbles pushed into the soil, as well as <phrase>ground-penetrating radar</phrase> have been tested for this purpose. In this research, passive acoustics was used to detect the <phrase>compaction layer</phrase> by recording the sound of a cone being drawn through the soil. The premise was that a more compacted layer should cause higher sound levels, which might reveal the depth and strength of the <phrase>compaction layer</phrase>. Two experiments were conducted in the soil bins of the <phrase>USDA</phrase>-ARS National Soil Dynamics Laboratory in <phrase>Auburn</phrase>, <phrase>Alabama</phrase>. First, constant-depth tests (15 and 30 cm) at three compaction levels (0.72, 2.8, and 3.6 MPa) revealed the relationship of sound amplitude with depth and compaction. Second, to test the detection capability, the cone was gradually inserted in the soil, passing through an artificial <phrase>compaction layer</phrase>. A windowed, <phrase>short-time Fourier transform</phrase> (STFT) analysis showed that the <phrase>compaction layer</phrase> is detectable since the sound amplitude was positively related to depth and compaction levels, but only in the highest <phrase>frequency range</phrase> of the spectrum. This led to the conjecture that the soil-cone interface acts as a <phrase>low-pass</phrase> filtering mechanism, where the <phrase>cutoff frequency</phrase> becomes higher in the <phrase>compaction layer</phrase> due to a more intimate contact between sensor and soil. oil compaction, caused by either natural causes or human interference, is a major yield-limiting factor. This is because <phrase>soil compaction</phrase>: (1) reduces soil pore size, (2) changes pore size distribution, (3) increases soil strength, (4) reduces air and water permeability, (5) increases <phrase>heat capacity</phrase> and <phrase>bulk density</phrase>, and most importantly , (6) increases root penetration resistance (Al-Adawi and Reeder, 1996). Distinctively high-strength soil layers are commonly termed " hardpans " or " plow soles. " Hardpans impede plant roots from uptake of <phrase>nutrients</phrase> and soil water reserves in the deeper soil strata. They also decrease water infiltration, which can accelerate loss of <phrase>nutrients</phrase> due to <phrase>erosion</phrase> and runoff. Under wet conditions, roots above the hard-pan layer may suffocate due …
Space-time <phrase>Least-squares</phrase> <phrase>Finite-element Method</phrase> for <phrase>Shallow-water Equations</phrase> In this paper, a space-time <phrase>least-squares</phrase> <phrase>finite-element method</phrase> for the 2D nonlinear <phrase>shallow-water equations</phrase> (SWE) is developed. The method can easily handle <phrase>complex geometry</phrase>, bed slope (source term), and radiation <phrase>boundary condition</phrase> without any special treatment. Other advantages of the method include: <phrase>high order</phrase> approximations in space and time can easily be employed, no upwind scheme is needed, as well as the resulting system equations is symmetric and <phrase>positive-definite</phrase>, therefore, can be solved efficiently with the pre-conditioned <phrase>conjugate gradient method</phrase>. The model is applied to several benchmark tests, including <phrase>standing wave</phrase> in a flat closed basin, propagation of <phrase>sinusoidal</phrase> wave in a flat channel with open boundary, flow past an elliptic hump, and wave-cylinder interactions. Simulation of <phrase>standing wave</phrase> in a closed flat basin, with <phrase>water depth</phrase> ranging from <phrase>shallow water</phrase> to <phrase>deep water</phrase>, shows that prediction of SWE model is accurate for shallow waters but inaccurate for <phrase>deep waters</phrase> due to the hydrostatic pressure and non-dispersive assumption. Computation of propagation of <phrase>sinusoidal</phrase> wave in a flat channel shows open <phrase>boundary condition</phrase> is treated satisfactorily. Simulation of flow past an elliptical hump shows good conservation property, and complicate and detailed fine wave structures which were not observed using the low order approximations in previous study. Computed result of wave-cylinder interactions compare well with other numerical results.
Pre-workshop summary: workshop on iterative, adaptive, and agile processes Broadly, the goals of this workshop are to • promote a deep and shared understanding of various iterative, adaptive, and agile processes, including <phrase>Crystal</phrase>, DSDM, <phrase>Extreme Programming</phrase> (XP), Serum, and the Unified Process (UP) • to identify the factors for their successful adoption • to share relevant research results discover lines of research to further this discipline OVERVIEW We suspect very few methodologists created a process with the explicit intent, "Oh boy, this will be heavy and rigid!" And yet it's a common complaint. Why is that? It influences the motivation for this new generation of process ideas. Will the same forces that have led to "unhealthy" application of prior generations of <phrase>development processes</phrase> also inhibit the new ones? For example, cases are now emerging of organizations just doing occasional <phrase>unit-testing</phrase> or avoiding documentation, and calling it "XP." What are the skillful means and capabilities required to really help these new process ideas be adopted and succeed? What's working? What isn't? And what are the hard-data justifications? In this first year of the workshop, there are three major goals: • To provide a supportive and fun forum for interested stakeholders to share their passion, research, stories, and ideas. • Compare and contrast (via a summary of key details, a case study, and discussion) related processes, ideally including • Identify cases, best practices, <phrase>human factors</phrase>, and impediments to successful adoption of these process as a whole (such as XP), or specific practices (such as daily serum or <phrase>pair programming</phrase>). In this workshop, we invite practitioners and researchers interested in the application of iterative, adaptive, and agile processes. And we expect to learn from each other. We include those favoring detailed or <phrase>large-scale</phrase> process descriptions (such as the Rational UP) to those favoring minimalist descriptions (such as Serum), from process <phrase>evangelists</phrase> to process agnostics, from expert developers to business managers, from domains where speed is the top priority to domains where quality or verifiability are the top priority. Permission to make digital or <phrase>hard copies of</phrase> all or part of this work for personal or classroom use is <phrase>granted without fee provided</phrase> that copies are not made or distributed for <phrase>profit or commercial advantage and</phrase> that <phrase>copies bear</phrase> this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to fists, <phrase>requires prior specific permission</phrase> and/or a fee.
Evacon: a framework for integrating evolutionary and <phrase>concolic testing</phrase> for <phrase>object-oriented programs</phrase> <phrase>Achieving high</phrase> structural coverage such as <phrase>branch coverage</phrase> in <phrase>object oriented programs</phrase> is an important and yet challenging goal due to two main challenges. First, some branches involve complex program logics and generating tests to cover them requires deep knowledge of the program structure and semantics. Second, covering some branches requires special method sequences to lead the receiver object or non-primitive arguments to specific desirable states. Previous work has developed the <phrase>concolic testing</phrase> technique (a combination of concrete and symbolic testing techniques) and the <phrase>evolutionary testing</phrase> technique to address these two challenges, respectively. However, neither technique was designed to address both challenges at the same time. To address the respective weaknesses of these two previous techniques, we propose a novel framework called Evacon that integrates <phrase>evolutionary testing</phrase> (used to search for desirable method sequences) and <phrase>concolic testing</phrase> (used to generate desirable method arguments). We have implemented our framework and applied it on six classes taken from the Java <phrase>standard library</phrase> and basic <phrase>data structures</phrase>. The experimental results show that the tests generated using our framework can achieve higher <phrase>branch coverage</phrase> than <phrase>evolutionary testing</phrase> or <phrase>concolic testing</phrase> alone
Architectural Design III: Context catalog description 351-5 Design III: Context. Continuing study of architectural design. Projects of increased scope and complexity. Continue <phrase>design process</phrase> study (synthesis) and appropriate design presentation (communication). Working with impingement introduced by external agencies such as social, government, and community, as part of a larger context of planning. Study of the impact of site development, for on-site as well as external, contextual issues. prerequisites prerequisite: grade of D or better in ARC 252-Design II: Order prerequisite to: ARC 352-Design IV: Complexity philosophy Do not seek to follow in the footsteps of the Masters, seek what they sought.-<phrase>Zen</phrase> teaching, 8th century The education of an <phrase>architect</phrase> must begin with a framework for developing a personal consciousness, an awareness of the ever-changing and growing world around us. This consciousness is not developed by taking what is given and serving it up as a solution to the problem at hand. This consciousness is the result of deep <phrase>critical thinking</phrase>, the development of astute powers of observation, an attunement to poetic systemic thinking, and a desire to become a creative solver of the problems our homes, cities, environment, clients, children, etc. face on a daily basis. A great practitioner is conscious to this world and is ready and willing to embrace this aspect of their lives to the point that it spills unconsciously into daily work and professional life. They don't simply replicate our core disciplinary knowledge, but seek to advance it through their own insight, offering new ways of seeing and inhabiting the world. Throughout the curriculum, students must be asked to consider the translation of their consciousness as a human being into an expanded professional context-to vision the depth of their values and beliefs about architecture, test them, and bring them to life through critical work. Each student must be encouraged to doubt, question givens, and to generate keen alternatives to what architecture is today. As students transition into practice, many lack the insight to think deeply about their profession and ask significant questions: What is it to be conscious in the profession of architecture? What are the meaningful contributions I wish to make to the lives of other people, to the environment, and to our collective world? What enables a practice to transcend the norm, making lasting contributions, point to the future, and wake the light in all us? Great education offers its students the opportunity and challenge of becoming a …
Compared deep class-AB and class-B ageing on AlGaN/GaN HEMT in S-Band pulsed-RF operating life AlGaN/GaN HEMTs are on the way to lead the RF-power amplification field according to their outstanding performances. However, due to its relative youth, reliability studies in several types of <phrase>operating conditions</phrase> allow to understand mechanisms peculiar to this technology and responsible for the wearing out of devices. This paper reports the reliability study on two power amplifiers using NITRONEX AlGaN/GaN HEMT. Based on results of a previous study of 1280 hours in standard <phrase>operating conditions</phrase> wherein no evolution of <phrase>electrical parameters</phrase> have been observed, two ageing tests in deep class-AB (432 hours) and class-B (795 hours) are performed under pulsed-RF operating life at high drain bias voltages and saturated operation. This study shows a drift in RF performances which is linked with the evolution of <phrase>electrical parameters</phrase> (R DS ON , g m and V P). Similar kinetics and amplitude of degradations are observed revealing quasi similar contribution of thermal effects in both cases. Degradations are supposed to be related to trapped charges phenomena induced by high voltage <phrase>operating conditions</phrase>. Although, several results attest to this hypothesis, a part of the evolutions seems to be linked with structural changes.
Prediction of Diaphragm Wall Deflection in Deep Excavations Using Evolutionary <phrase>Support Vector Machine</phrase> Inference Model (esim) Problems in deep excavations are full of uncertain, vague, and incomplete information. In most instances, successfully solving such problems depends on experts' knowledge and experience. The primary object of this research was to propose an " Evolutionary <phrase>Support Vector Machine</phrase> Inference Model (ESIM) " to predict wall deformation in deep excavation in <phrase>Taipei</phrase> Basin. ESIM is developed based on a hybrid approach that fuses <phrase>support vector machines</phrase> (SVM) and fast messy <phrase>genetic algorithm</phrase> (fmGA). SVM is primarily concerned with learning and <phrase>curve fitting</phrase>; and fmGA with optimization. Fifty-seven wall deformation monitoring database were collected based on monitoring data and compiled from prior projects. Fifty-two of 57 were selected for training, leaving 5 valid cases available for testing. Results show that ESIM can successfully predict the deflection and apply to contractors utilizes the knowledge and experience from past projects to predict wall deformation of new projects. Therefore the construction and foundation construction contractors can update wall deflection monitoring data of different stages during deep excavation process, in order to predict the wall deformation of the next stage and examine whether the max deflection is within the controlled range. The results are used as guidelines on site safety and <phrase>risk management</phrase>.
<phrase>Empirical Methods</phrase> in AI The debate during the workshop can broadly be divided into three categories: (1) past successes of <phrase>empirical methods</phrase>, (2) the design of computational experiments, and (3) the widespread use of random problems. The following summary necessarily offers just a partial description of the topics discussed during the workshop. 1 Success Stories <phrase>Empirical methods</phrase> have been successful in recent years. Indeed, as Henry Kautz reminded the workshop participants , in the last year alone, the New <phrase>York</phrase> Times has reported two major empirical successes: (1) DEEP BLUE's defeat of Kasparov and (2) the computer generated proof of an <phrase>open problem</phrase> in Robbins algebra. Pan-durang Nayak (<phrase>NASA Ames</phrase>) described another highly publicized success, the diagnosis system for the Deep Space One spacecraft, which is based on a highly optimized satisfiability procedure. Although deciding satisfiability is intractable in general, this system generates plans in practice in essentially constant time for each step. It comes as quite a surprise to hear about real-time satisfiability testing. Henry Kautz listed several reasons for the success of <phrase>empirical methods</phrase>. First, empirical studies are often an integral part of AI because systems can be too complex or messy for theory. Second, theory is often too crude to provide useful insight. For example, a problem might be exponential in the worst case but tractable in practice. Third, some questions are purely empirical. As Pedro Meseguer (IIIA, <phrase>CSIC</phrase>, <phrase>Spain</phrase>) pointed out during one of the panels, two <phrase>search algorithms</phrase> s In the last few years, we have witnessed a major growth in the use of <phrase>empirical methods</phrase> in AI. In part, this growth has arisen from the availability of fast net-worked computers that allow certain problems of a practical size to be tackled for the first time. There is also a growing realization that <phrase>results obtained</phrase> empirically are no less valuable than theoretical results. Experiments can, for example , offer solutions to problems that have defeated a theoretical attack and provide insights that are not possible from a purely theoretical analysis. I identify some of the emerging trends in this area by describing a recent workshop that brought together researchers using <phrase>empirical methods</phrase> as far apart as robotics and <phrase>knowledge-based systems</phrase>. T wenty-five researchers gathered together during the Fifteenth International Joint Conference on <phrase>Artificial Intelligence</phrase> (IJCAI-97) in <phrase>Nagoya</phrase>, Japan, for the Second Workshop on Empirical <phrase>Artificial Intelligence</phrase>. The workshop continued the work of a similar workshop held alongside ECAI-96 in …
Improved Algorithm for Gradient Vector Flow Based <phrase>Active Contour</phrase> Model Using <phrase>Global and Local</phrase> Information <phrase>Active contour</phrase> models are used to extract object boundary from <phrase>digital image</phrase>, but there is poor convergence for the targets with deep concavities. We proposed an improved approach based on existing gradient vector flow methods. Main contributions of this paper are a new algorithm to determine the false part of <phrase>active contour</phrase> with <phrase>higher accuracy</phrase> from the global force of gradient vector flow and a new algorithm to update the external force field together with the local information of magnetostatic force. Our method has a semidynamic external force field, which is adjusted only when the false <phrase>active contour</phrase> exists. Thus, <phrase>active contours</phrase> have more chances to approximate the complex boundary, while the computational cost is limited effectively. The new algorithm is tested on irregular shapes and then on real images such as MRI and ultrasound medical data. <phrase>Experimental results</phrase> illustrate the efficiency of our method, and the <phrase>computational complexity</phrase> is also analyzed.
Simple <phrase>Design Tools</phrase> On 15 April 1999, people around the world sent thousands of e-mails to our group at <phrase>Stanford University</phrase> inquiring about the " secret " performance report of the <phrase>AMD</phrase> <phrase>K7</phrase> and Intel Coppermine chips. According to The Register <phrase>Web site</phrase> in the <phrase>United Kingdom</phrase> , there were rumors that some students at <phrase>Stanford University</phrase> had tested and compared the two chips using SPEC's <phrase>test suite</phrase>. At that time, the <phrase>AMD</phrase> <phrase>K7</phrase> (now known as <phrase>Athlon</phrase>) and the Intel Coppermine (now known as <phrase>Pentium III</phrase>) were not available to the general public, and it is easy to understand the excitement about this news. The real story was simpler. The students in our processor design class did not test the real chips. They estimated the cost and performance of the two chips as part of a case study. This study was based on information available from various public sources. 3 As the chip <phrase>implementation details</phrase> had not been released to the public, the instructors assumed the hardware designs were similar to the simulator default configurations. After comparing the performance and the costs, the students used the simulator tools to design improvements to each chip. Designing <phrase>deep-submicron</phrase> microprocessors is becoming an increasingly tedious and complicated process. 4 It takes many years and many engineers to design a commercial processor chip. In an introductory computer architecture course, students are usually required to simulate a simplified pipelined microprocessor such as DLX 5 using some <phrase>hardware description language</phrase> (typically Verilog or VHDL) or some graphical tool (such as HASE). 6 While it is important for a student to understand how a basic processor operates, students may not fully appreciate the complexity and the various <phrase>trade-offs involved in</phrase> designing a processor in the commercial environment. Students cannot afford to write tens of thousands of lines of code to model processor <phrase>microarchitecture</phrase>; it is unproductive to ask them to deal with the myriad details at this stage. Instead, we focus on <phrase>high-level</phrase> issues involving cost (area) and performance (execution time). The <phrase>main issues</phrase> are cache size, cycle time, <phrase>floating-point unit</phrase> (<phrase>FPU</phrase>) area, latencies, branch strategy, and issue width. By emphasizing a few primary <phrase>high-level</phrase> issues, students gain a better understanding of the <phrase>trade-offs involved in</phrase> overall computer architecture design. Table 1 lists the architectural <phrase>design tools</phrase> available in our class. Students use these tools in conjunction with the class <phrase>textbook</phrase>, 7 and they are also available for other …
Training Deep <phrase>Spiking</phrase> <phrase>Neural Networks</phrase> Using Backpropagation Deep <phrase>spiking</phrase> <phrase>neural networks</phrase> (SNNs) hold the potential for improving the latency and energy efficiency of <phrase>deep neural networks</phrase> through data-driven event-based computation. However, training such networks is difficult due to the non-differentiable nature of spike events. In this paper, we introduce a novel technique, which treats the membrane potentials of <phrase>spiking</phrase> neurons as differentiable signals, where discontinuities at spike times are considered as noise. This enables an error backpropagation mechanism for deep SNNs that follows the same principles as in conventional <phrase>deep networks</phrase>, but works directly on spike signals and membrane potentials. Compared with previous methods relying on indirect training and conversion, our technique has the potential to capture the statistics of spikes more precisely. We evaluate the proposed framework on artificially generated events from the original MNIST handwritten digit benchmark, and also on the N-MNIST benchmark recorded with an event-based dynamic vision sensor, in which the proposed method reduces the <phrase>error rate</phrase> by a factor of more than three compared to the best previous SNN, and also achieves a <phrase>higher accuracy</phrase> than a conventional <phrase>convolutional neural network</phrase> (CNN) trained and tested on the same data. We demonstrate in the context of the MNIST task that thanks to their <phrase>event-driven</phrase> operation, deep SNNs (both <phrase>fully connected</phrase> and convolutional) trained with our method achieve accuracy equivalent with conventional <phrase>neural networks</phrase>. In the N-MNIST example, equivalent accuracy is achieved with about five times fewer computational operations.
Flexible, <phrase>High Performance</phrase> <phrase>Convolutional Neural Networks</phrase> for <phrase>Image Classification</phrase> We present a fast, fully parameterizable GPU implementation of <phrase>Convolutional Neural Network</phrase> variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our <phrase>deep hierarchical</phrase> architec-tures achieve the best <phrase>published results</phrase> on benchmarks for <phrase>object classification</phrase> (NORB, CIFAR10) and handwritten digit recognition (MNIST), with <phrase>error rates</phrase> of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test <phrase>error rates</phrase> on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.
Applying <phrase>Convolutional Neural Networks</phrase> concepts to hybrid NN-HMM model for <phrase>speech recognition</phrase> <phrase>Convolutional Neural Networks</phrase> (CNN) have showed success in achieving translation invariance for many <phrase>image processing</phrase> tasks. The success is largely attributed to the use of local filtering and <phrase>max-pooling</phrase> in the CNN architecture. In this paper, we propose to apply CNN to <phrase>speech recognition</phrase> within the framework of hybrid NN-HMM model. We propose to use local filtering and <phrase>max-pooling</phrase> in <phrase>frequency domain</phrase> to normalize speaker variance to achieve higher multi-speaker <phrase>speech recognition</phrase> performance. In our method, a pair of local filtering layer and <phrase>max-pooling</phrase> layer is added at the lowest end of <phrase>neural network</phrase> (NN) to normalize spectral variations of speech signals. In our experiments, the proposed CNN architecture is evaluated in a speaker independent <phrase>speech recognition</phrase> task using the standard TIMIT <phrase>data sets</phrase>. Experimental results show that the proposed CNN method can achieve over 10% relative error reduction in the core TIMIT <phrase>test sets</phrase> when comparing with a regular NN using the same number of hidden layers and weights. Our results also show that the best result of the proposed CNN model is better than <phrase>previously published</phrase> results on the same TIMIT <phrase>test sets</phrase> that use a pre-trained deep NN model.
Multispectral <phrase>Image Compression</phrase> Based on DSC Combined with CCSDS-IDC <phrase>Remote sensing</phrase> multispectral <phrase>image compression</phrase> encoder requires <phrase>low complexity</phrase>, high robust, and <phrase>high performance</phrase> because it usually works on the satellite where the resources, such as power, memory, and processing capacity, are limited. For multispectral images, the compression algorithms based on 3D transform (like 3D <phrase>DWT</phrase>, 3D <phrase>DCT</phrase>) are too complex to be implemented in space mission. In this paper, we proposed a compression algorithm based on distributed source coding (DSC) combined with image <phrase>data compression</phrase> (IDC) approach recommended by CCSDS for multispectral images, which has <phrase>low complexity</phrase>, high robust, and <phrase>high performance</phrase>. First, each band is sparsely represented by <phrase>DWT</phrase> to obtain wavelet coefficients. Then, the wavelet coefficients are encoded by bit plane encoder (BPE). Finally, the BPE is merged to the DSC strategy of Slepian-<phrase>Wolf</phrase> (SW) based on <phrase>QC</phrase>-LDPC by deep coupling way to remove the residual redundancy between the adjacent bands. A series of multispectral images is used to test our algorithm. Experimental results show that the proposed DSC combined with the CCSDS-IDC (DSC-CCSDS)-based algorithm has better compression performance than the traditional compression approaches.
Motion deblurring as optimisation <phrase>Motion blur</phrase> is one of the most common causes of image degradation. It is of increasing interest due to the deep penetration of <phrase>digital cameras</phrase> into consumer applications. In this paper, we start with a hypothesis that there is sufficient information within a blurred image and approach the deblurring problem as an optimisation process where the deblurring is to be done by satisfying a set of conditions. These conditions are derived from first principles underlying the degradation process assuming noise-free environments. We propose a novel but effective method for removing <phrase>motion blur</phrase> from a single blurred image via an iterative algorithm. The strength of this method is that it enables deblurring without resorting to estimation of the blur kernel or blur depth. The proposed <phrase>iterative method</phrase> has been tested on several images with different degrees of blur. The obtained results have been compared with state of the art techniques including those that require more than one input image. The results are consistently of <phrase>high quality</phrase> and comparable or superior to the <phrase>existing methods</phrase> which demonstrates the effectiveness of the proposed technique.
Lockin-<phrase>thermography</phrase>: Principles, Nde-applications, and Trends A review is given about Lockin-<phrase>Thermography</phrase>, about its photoacoustic and photothermal roots, about the principle and modern applications for <phrase>nondestructive testing</phrase> using different kinds of options. As Lockin-<phrase>Thermography</phrase> is based on thermal waves, a short excursion to this kind of waves and some remarks on the way they were used seems to be appropriate. Some indications about promising futural developments are provided as well. When Fourier was involved in planning the <phrase>water supply</phrase> tubing for <phrase>Paris</phrase>, he was concerned with the problem how deep the <phrase>tubes</phrase> should be buried in the soil to prevent freezing in winter. So he dealt with the periodical temperature cycles on the surface and how deep they extend into the soil. He found out that the process is described by a <phrase>linear differential equation</phrase> whose solution is a highly attenuated wave where <phrase>thermal diffusivity</phrase> µ is the only parameter involved [1]. In order to solve the <phrase>linear differential equation</phrase> for non-<phrase>sinusoidal</phrase> <phrase>boundary condition</phrase> (daily and annual temperature cycle), he decomposed it into a sum of <phrase>sine</phrase> functions and superposed the solutions. Fourier became famous for developing the mathematical principle of solution which is broadly applicable. His solution is the " thermal wave " that describes the deviation T of temperature from its local average,) / (/ 0) , (   x t i x e e T t x T    where the " thermal diffusion length " µ is defined as 2 c     with <phrase>thermal conductivity</phrase> , density , <phrase>specific heat</phrase> c, and angular modulation frequency . This dependence on depth and time displays the strong <phrase>damping</phrase> and also the phase shift with increasing depth. Hence <phrase>phase velocity</phrase> is given by v = ωµ ~ √ω Obviously higher frequencies propagate faster which means that thermal waves are dispersive. The consequence is that a temperature pulse starting from the surface changes its shape while it propagates into the material. As only <phrase>sine</phrase> waves maintain their shape, in the following we deal only with them while Vavilov´s review at this conference reports on pulsed excitation. Fig. 1. Temporal and spatial behaviour of a one-dimensional thermal wave of 0.03 Hz propagating in Polyvinylchloride (<phrase>PVC</phrase>). From <phrase>thermal diffusivity</phrase> α = 0.0011 cm 2 /s) results a <phrase>phase velocity</phrase> of v <phrase>Ph</phrase> = 0.2036 mm/s at this frequency [2].
<phrase>High-performance</phrase> Neural Networks for Visual <phrase>Object Classification</phrase> <phrase>High-performance</phrase> Neural Networks for Visual <phrase>Object Classification</phrase> We present a fast, fully parameterizable GPU implementation of <phrase>Convolutional Neural Network</phrase> variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our <phrase>deep hierarchical</phrase> architectures achieve the best <phrase>published results</phrase> on benchmarks for <phrase>object classification</phrase> (NORB, CIFAR10) and handwritten digit recognition (MNIST), with <phrase>error rates</phrase> of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test <phrase>error rates</phrase> on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.
Factors Influencing Employees' Deep Usage of <phrase>Information Systems</phrase> The adoption process within organizational settings, which is different from traditional individual adoption, involves both the primary adoption by managers and the secondary adoption by employees. Understanding what determinants influence employees to deeply use <phrase>information systems</phrase> (IS) would help managers improve the process of facilitating system implementation. This study explores the determinants that can affect employees to deeply use <phrase>information systems</phrase>, in particular when there is an organizational mandate to adopt an IS. A new framework is developed that combines insights from the individual innovation post-acceptance model as well as from organizational adoption and assimilation frameworks. The research methodology used to test the hypotheses is discussed, and contributions of this study are presented.
A Divide-and-Conquer-Based Algorithm for Automatic Simulation <phrase>Vector Generation</phrase> TESTBENCHES play one of the most important roles in <phrase>simulation-based</phrase> <phrase>design verification</phrase>. Given a simulation scenario, a testbench provides specific vectors to simulate the design, then collects responses from the design to monitor whether the simulation has satisfied the scenario. 1 The <phrase>major bottleneck</phrase> in writing testbenches is generating valid <phrase>simulation vectors</phrase>. Traditionally, test-bench engineers generate these vectors manually—a time-consuming and troublesome task. Moreover, manually generated <phrase>simulation vectors</phrase> rarely cover all simulation scenarios. Automated generation of <phrase>simulation vectors</phrase> is therefore vital for effective simulation. Many current automatic-<phrase>vector-generation</phrase> methods focus on exploring a design's <phrase>state space</phrase>. Due to memory or runtime limitations, these methods cannot keep up with the rapid growth of <phrase>design complexity</phrase>. We propose a novel algorithm based on the divide-and-conquer paradigm that helps these methods decompose the design's complexity. The algorithm uses a partitioning method that recursively divides a design into smaller, more manageable components. Other approaches handle the divided components while maintaining the entire design's proper functioning. <phrase>Experimental results demonstrate</phrase> that <phrase>vector generation</phrase> methods, with the help of our algorithm, improve the coverage of simulation scenarios. Researchers have proposed many techniques for automatic simulation <phrase>vector generation</phrase>. These techniques generally fall into three categories: <phrase>random simulation</phrase>, symbolic solvers, and hybrid solvers. <phrase>Random simulation</phrase> generates sets of <phrase>simulation vectors</phrase> by randomly assigning the logic values to the design's primary inputs (<phrase>PIs</phrase>) one cycle at a time. Random simulation's strengths are that it allows easy acquisition of <phrase>simulation vectors</phrase>, and it offers a <phrase>deep state</phrase>-space search distance. Its weakness is that it uses only one trace to explore the state space. Because <phrase>random simulation</phrase> is easy to implement and can generate useful <phrase>simulation vectors</phrase>, most <phrase>vector generation</phrase> engines use this technique. Unlike <phrase>random simulation</phrase>, which uses only a single trace, symbolic solvers attempt to simultaneously enumerate all possible primary inputs to explore the entire <phrase>state space</phrase>. 2-6 They typically use binary decision diagrams (BDDs) or satisfiability (SAT) solvers as their core engine. A symbolic solver's main feature is its exhaustive search ability. Furthermore, a symbolic solver obtains <phrase>simulation vectors</phrase> that are much more compact than those obtained by <phrase>random simulation</phrase>. As the state space Editor's note: Divide-and-conquer is a natural way to cope with the complexity of automatic testbench generation. The key to developing an effective divide-and-conquer approach is to identify the partitioning boundaries where interactions among divided components are minimized. The authors propose a novel design decomposition scheme and …
Topic-focus articulation and anaphoric relations: corpus based probe 1. The objective of the paper is to analyze certain interrelationships between the <phrase>information structure</phrase>, i.e. the topic-focus articulation (TFA) of sentences, and anaphoric relations, on the material achieved during the annotation of TFA and of coreference in the <phrase>Prague</phrase> Dependency Treebank (PDT). 2.1 <phrase>Prague</phrase> Dependency Treebank (PDT) is conceived of as a collection of 3,168 samples of continuous running Czech texts (taken at random from the Czech National Corpus), annotated – besides a complex scheme of morphemic tags – on two layers of dependency-based sentence structure, the first of which – the analytic one – is considered to be an intermediate step towards the underlying level of annotation, the so-called tectogrammatical tree structures (TGTSs), in which nodes are also reconstructed for items deleted in the surface shape of the sentences. These structures are designed in a way that allows i.a. for an inclusion of information on both intra-and inter-sentential coreference relations. 2.2 In addition to the <phrase>deep syntactic</phrase> dependency relations in the <phrase>tree structure</phrase>, individual nodes are assigned one of the three values of contextual boundness: non-contrastive contextually bound " t " , contrastive contextually bound " c " and contextually non-bound " f ". This information at individual nodes of the dependency <phrase>tree structure</phrase> makes it possible to derive the <phrase>division</phrase> of the sentence into topic (in the prototypical case: what the sentence is about) and focus (what the sentence says about the topic); the basic algorithm for this procedure was formulated by Hajičová and Sgall (see Hajičová and Sgall 1985) and its implementation and testing on PDT is reported in Hajičová, Havelka and Veselá (2005). 2.3 In a separate path through the corpus annotated on this underlying level, basic coreference relations are being marked independently of the TFA values. In our project, two types of coreference are distinguished: grammatical – with <phrase>verbs</phrase> (and also some <phrase>nouns</phrase>) of control, with reflexive <phrase>pronouns</phrase>, with verbal complements and with relative <phrase>pronouns</phrase> – and textual, which may cross sentence boundaries. Both endophoric (anaphora) and exophoric (<phrase>deixis</phrase>) relations are taken into account as well as cataphora (see Kučová and Hajičová 2004). For the annotation of grammatical coreference (which has been given a systematic account in the description, see Kučová et al. 2003) a <phrase>semi-automatic</phrase> procedure has already been implemented, which is giving rather encouraging results. The manual annotation of textual coreference is carried out with the use of a user-friendly tool in …
Mathematical Formulation of Multilayer Networks A network representation is useful for describing the structure of a large variety of complex systems. However, most real and engineered systems have multiple subsystems and layers of connectivity, and the data produced by such systems are very rich. Achieving a deep understanding of such systems necessitates generalizing ''traditional'' <phrase>network theory</phrase>, and the newfound deluge of data now makes it possible to test increasingly general frameworks for the study of networks. In particular, although adjacency matrices are useful to describe traditional <phrase>single-layer</phrase> networks, such a representation is insufficient for the analysis and description of multiplex and time-dependent networks. One must therefore develop a more general mathematical framework to cope with the challenges posed by multilayer complex systems. In this paper, we introduce a tensorial framework to study multilayer networks, and we discuss the generalization of several important network descriptors and dynamical processes—including degree <phrase>centrality</phrase>, clustering coefficients, <phrase>eigenvector</phrase> <phrase>centrality</phrase>, modularity, <phrase>von Neumann entropy</phrase>, and diffusion—for this framework. We examine the impact of different choices in constructing these generalizations, and we illustrate how to obtain known results for the special cases of <phrase>single-layer</phrase> and multiplex networks. Our tensorial approach will be helpful for tackling pressing problems in multilayer complex systems, such as inferring who is influencing whom (and by which media) in multichannel <phrase>social networks</phrase> and developing routing techniques for multimodal transportation systems.
Deep Bottleneck Features for Spoken Language Identification A key problem in spoken language identification (LID) is to design effective representations which are specific to language information. For example, in recent years, representations based on both phonotactic and acoustic features have proven their effectiveness for LID. Although advances in <phrase>machine learning</phrase> have led to <phrase>significant improvements</phrase>, LID performance is still lacking, especially for short duration speech utterances. With the hypothesis that language information is weak and represented only latently in speech, and is largely dependent on the statistical properties of the speech content, existing representations may be insufficient. Furthermore they may be susceptible to the variations caused by different speakers, specific content of the speech segments, and background noise. To address this, we propose using Deep Bottleneck Features (DBF) for spoken LID, motivated by the success of <phrase>Deep Neural Networks</phrase> (DNN) in <phrase>speech recognition</phrase>. We show that DBFs can form a low-dimensional compact representation of the original inputs with a powerful descriptive and discriminative capability. To evaluate the effectiveness of this, we design two <phrase>acoustic models</phrase>, termed DBF-<phrase>TV</phrase> and parallel DBF-<phrase>TV</phrase> (PDBF-<phrase>TV</phrase>), using a DBF based i-vector representation for each speech utterance. Results on <phrase>NIST</phrase> language recognition evaluation 2009 (LRE09) show <phrase>significant improvements</phrase> over state-of-the-art systems. By fusing the output of phonotactic and acoustic approaches, we achieve an EER of 1.08%, 1.89% and 7.01% for 30 s, 10 s and 3 s test utterances respectively. Furthermore, various DBF configurations have been extensively evaluated, and an optimal system proposed.
<phrase>Comparative genomics</phrase> approach to detecting split-coding regions in a low-coverage genome: lessons from the <phrase>chimaera</phrase> Callorhinchus milii (<phrase>Holocephali</phrase>, <phrase>Chondrichthyes</phrase>) Recent development of <phrase>deep sequencing</phrase> technologies has facilitated de novo <phrase>genome sequencing</phrase> projects, now conducted even by individual laboratories. However, this will yield more and more genome sequences that are not well assembled, and will hinder thorough annotation when no <phrase>closely related</phrase> <phrase>reference genome</phrase> is available. One of the challenging issues is the identification of protein-coding sequences split into multiple unassembled genomic segments, which can confound orthology assignment and various laboratory experiments requiring the identification of individual genes. In this study, using the genome of a <phrase>cartilaginous</phrase> <phrase>fish</phrase>, Callorhinchus milii, as <phrase>test case</phrase>, we performed gene prediction using a model specifically trained for this genome. We implemented an algorithm, designated ESPRIT, to identify possible linkages between multiple protein-coding portions derived from a single genomic locus split into multiple unassembled genomic segments. We developed a validation framework based on an artificially fragmented <phrase>human genome</phrase>, improvements between early and recent mouse genome assemblies, comparison with experimentally validated sequences from <phrase>GenBank</phrase>, and <phrase>phylogenetic</phrase> analyses. Our strategy provided insights into practical solutions for efficient annotation of only partially sequenced (low-coverage) <phrase>genomes</phrase>. To our knowledge, our study is the first formulation of a method to link unassembled genomic segments based on proteomes of relatively distantly related species as references.
How to test for <phrase>dual-task</phrase>-specific effects in <phrase>brain imaging</phrase> studies - An evaluation of potential <phrase>analysis methods</phrase> The study of the concurrent performance of two tasks allows deep insights into the human cognitive system and, accordingly, an increasing number of <phrase>brain imaging</phrase> studies are conducted to identify the neuroanatomical correlates of such <phrase>dual-task</phrase> performance. In this overview we present currently used approaches to identify <phrase>dual-task</phrase>-specific activations in fMRI and PET studies. A comparison is made in order to identify the approaches which have the potential to validly detect <phrase>dual-task</phrase>-specific <phrase>activation patterns</phrase>, i.e. activation which cannot be explained by the individual performance of the component tasks alone. We demonstrate that while all approaches suffer from at least some drawbacks, the best (although potentially over-conservative) approach is to compare the <phrase>dual task</phrase> with the sum of the single tasks, the second-best is an interaction contrast, and the third-best a conjunction analysis. Comparisons of the <phrase>dual task</phrase> with the mean of single-task activity or with only one single task should be avoided except for a few specific situations. We generalize our conclusions to related research areas, such as <phrase>multisensory integration</phrase> or divided attention.
Machine <phrase>Lifelong Learning</phrase>: Challenges and Benefits for <phrase>Artificial General Intelligence</phrase> We propose that it is appropriate to more seriously consider the nature of systems that are capable of learning over a lifetime. There are three reasons for taking this position. First, there exists a body of related work for this research under names such as constructive induction, <phrase>continual</phrase> learning, sequential task learning and most recently learning with <phrase>deep architectures</phrase>. Second, the computational and data storage power of modern computers are capable of implementing and testing machine <phrase>lifelong learning</phrase> systems. Third, there are significant challenges and benefits to pursuing programs of research in the area to AGI and brain sciences. This <phrase>paper discusses</phrase> each of the above in the context of a general framework for machine <phrase>lifelong learning</phrase>.
JAWS: A <phrase>Javascript</phrase> API for the Efficient Testing and Integration of <phrase>Semantic Web Services</phrase> <phrase>Semantic Web Services</phrase> (<phrase>SWS</phrase>) hold a lot of potential to the future of the <phrase>Semantic Web</phrase>. In this area, a number of tools have been developed to facilitate their definition and deployment. Our goal is to support an efficient means of testing and integration within a <phrase>browser</phrase>-based solution. For this purpose we propose JAWS (<phrase>Javascript</phrase>, <phrase>AJAX</phrase>, <phrase>Web Service</phrase>) : A <phrase>Javascript</phrase> API to facilitate the testing and integration of <phrase>SWS</phrase>. This software decouples the process of <phrase>SWS</phrase> integration and development through facilitation of the <phrase>AJAX</phrase>/REST paradigm. By leveraging meta-programming and <phrase>deep integration</phrase> techniques we support <phrase>Web 2.0</phrase> inspired applications in the context of complete <phrase>browser</phrase>-based development.
Tunnel detection using near-surface seismic methods Geophysical detection of near-surface voids caused by mining, <phrase>tunnels</phrase>, karst features, etc., is a persistent problem that has not been solved either consistently or across multiple geologic settings. Multiple methods have been used with varying degrees of success. We present shallow seismic <phrase>data collected</phrase> at a test site with a 9.1-m deep tunnel in unconsolidated <phrase>sediments</phrase> similar to geologic settings found along the southwest US border. Test <phrase>results demonstrate</phrase> the capability of using <phrase>P-wave</phrase> <phrase>diffraction</phrase> and <phrase>surface wave</phrase> <phrase>backscatter</phrase> techniques to detect a purpose-built subterranean tunnel. Data were processed blindly by remote personnel, who were not aware of the target location, and results are in excellent agreement with the <phrase>ground truth</phrase>. These methods show promise for both efficient, production-scale <phrase>data acquisition</phrase> and real-time automated <phrase>data processing</phrase>.
Mechanical Coupling Error Suppression Technology for an Improved Decoupled Dual-Mass <phrase>Micro-Gyroscope</phrase> This paper presents technology for the suppression of the mechanical coupling errors for an improved decoupled dual-mass <phrase>micro-gyroscope</phrase> (DDMG). The improved <phrase>micro-gyroscope</phrase> structure decreases the moment arm of the drive decoupled <phrase>torque</phrase>, which benefits the suppression of the non-ideal decoupled error. Quadrature correction electrodes are added to eliminate the residual quadrature error. The structure principle and the quadrature error suppression means of the DDMG are described in detail. <phrase>ANSYS</phrase> software is used to simulate the <phrase>micro-gyroscope</phrase> structure to verify the mechanical coupling error suppression effect. Compared with the former structure, simulation <phrase>results demonstrate</phrase> that the rotational displacements of the sense frame in the improved structure are substantially suppressed in the drive mode. The improved DDMG structure chip is fabricated by the deep dry silicon on glass (DDSOG) process. The <phrase>feedback control</phrase> circuits with quadrature control loops are designed to suppress the residual mechanical coupling error. Finally, the system performance of the DDMG prototype is tested. Compared with the former DDMG, the quadrature error in the improved dual-mass <phrase>micro-gyroscope</phrase> is decreased 9.66-fold, and the offset error is decreased 6.36-fold. Compared with the open loop sense, the <phrase>feedback control</phrase> circuits with quadrature control loop decrease the bias drift by 20.59-fold and the <phrase>scale factor</phrase> non-linearity by 2.81-fold in the ±400°/s range.
Effect of EEG electrode number on <phrase>epileptic</phrase> <phrase>source localization</phrase> in pediatric patients. OBJECTIVE To investigate the relationship between EEG <phrase>source localization</phrase> and the number of <phrase>scalp</phrase> EEG recording channels.   METHODS 128 EEG channel recordings of 5 pediatric patients with medically intractable partial epilepsy were used to perform <phrase>source localization</phrase> of interictal spikes. The results were compared with surgical resection and intracranial recordings. Various electrode configurations were tested and a series of computer simulations based on a realistic head boundary element model were also performed in order to further validate the clinical findings.   RESULTS The improvement seen in <phrase>source localization</phrase> substantially decreases as the number of electrodes increases. This finding was evaluated using the surgical resection, intracranial recordings and computer simulation. It was also shown in the simulation that increasing the electrode numbers could remedy the localization error of deep sources. A plateauing effect was seen in deep and superficial sources with further increasing the electrode number.   CONCLUSION The <phrase>source localization</phrase> is improved when electrode numbers increase, but the absolute improvement in accuracy decreases with increasing electrode number.   SIGNIFICANCE Increasing the electrode number helps decrease localization error and thus can more ably assist the physician to better plan for surgical procedures.
3D <phrase>Object Recognition</phrase> with <phrase>Deep Belief</phrase> Nets We introduce a new type of top-level model for <phrase>Deep Belief</phrase> Nets and evaluate it on a 3D <phrase>object recognition</phrase> task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database (normalized-uniform version), which contains stereo-pair images of objects under different <phrase>lighting conditions</phrase> and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best published result for NORB (5.9%) using a <phrase>convolutional neural</phrase> net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for <phrase>semi-supervised learning</phrase>, and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra <phrase>unlabeled data</phrase> (and the same amount of <phrase>labeled data</phrase> as before), our model achieves 5.2% error.
Realizing High <phrase>Test Quality</phrase> Goals with Smart Test Resource Usage Growing ASIC design sizes and advanced <phrase>deep sub-micron technologies</phrase> require new <phrase>fault models</phrase> and more <phrase>test vectors</phrase> to meet high <phrase>test quality</phrase> goals. To realize these goals within given test resources and cost constraints, new DFT techniques must be used. This paper reports <phrase>test quality</phrase> metrics and the test cost of industrial designs for different <phrase>fault models</phrase> using three DFT techniques: ATPG for deterministic patterns, <phrase>Logic BIST</phrase> for <phrase>pseudo-random</phrase> patterns , and <phrase>EDT</phrase> for compressed deterministic patterns. It is shown how these techniques can be used to achieve the <phrase>high quality</phrase> goals within the test resources currently available for stuck-at tests.
Revealing skype traffic: when randomness plays with you Skype is a very popular <phrase>VoIP</phrase> software which has recently attracted the attention of the <phrase>research community</phrase> and network operators. Following a <phrase>closed source</phrase> and proprietary design, Skype protocols and algorithms are unknown. Moreover, strong encryption mechanisms are adopted by Skype, making it very difficult to even glimpse its presence from a traffic aggregate. In this paper, we propose a framework based on two complementary techniques to reveal Skypetraffic in real time. The first approach, based on Pearson'sChi-Square test and <phrase>agnostic</phrase> to <phrase>VoIP</phrase>-related trafficcharacteristics, is used to detect Skype's <phrase>fingerprint</phrase> from the packet framing structure, exploiting the randomness introduced at the <phrase>bit level</phrase> by the encryption process. Conversely, the second approach is based on a stochastic characterization of Skype traffic in terms of packet arrival rate and packet length, which are used as features of a decision process based on Naive Bayesian Classifiers.In order to assess the effectiveness of the above techniques, we develop an <phrase>off-line</phrase> cross-checking heuristic based on <phrase>deep-packet inspection</phrase> and flow correlation, which is interesting per se. This heuristic allows us to quantify the amount of false negatives and <phrase>false positives</phrase> gathered by means of the two proposed approaches: results obtained from measurements in different networks show that the technique is very effective in identifying Skype traffic. While both Bayesian classifier and <phrase>packet inspection</phrase> techniques are commonly used, the idea of leveraging on randomness to reveal traffic is novel. We adopt this to identify Skype traffic, but the same methodology can be applied to other <phrase>classification problems</phrase> as well.
A Probabilistic Model for <phrase>Path Delay Fault Testing</phrase> Testing path <phrase>delay faults</phrase> (PDFs) in <phrase>VLSI circuits</phrase> is becoming an <phrase>important issue</phrase> as we enter the deep submicron age. However, it is difficult in general since the number of faults is normally very large and most faults are either hard to sensitize or are untestable. In this paper, we propose a probabilistic PDF model. We investigate probability functions for the wire and <phrase>path delay</phrase> size to model the fault effect in the circuit under test. In our approach, the <phrase>delay fault</phrase> size is assumed to be randomly distributed. An analytical model is proposed to evaluate the PDF coverage. We show that the delay sizes of the untested paths are actually reduced if these paths are conjoined with other tested good paths. Therefore, using our approach, <phrase>path selection</phrase> and synthesis of PDF testable circuits can be done more accurately. Also, given a test set, more accurate <phrase>fault coverage</phrase> can be predicted by calculating the mean delay of the paths.
Invited Paper Special Section on Josephson Junctions — past 50 Years and Future — <phrase>Recent Developments</phrase> of High-t C Electronic Devices with Multilayer Structures and Ramp-edge Josephson Junctions * Seiji Adachi SUMMARY <phrase>Recent developments</phrase> of electronic devices containing Josephson junctions (JJ) with high-T c <phrase>superconductors</phrase> (HTS) are reported. In particular, the fabrication process and the properties of superconduct-ing quantum interference devices (SQUIDs) with a multilayer structure and ramp-edge-type JJs are described. The JJs were fabricated by re-<phrase>crystallization</phrase> of an artificially deposited Cu-poor precursory layer. The formation mechanism of the junction barrier is discussed. We have fabricated various types of gradiometers and magnetometers. They have been actually utilized for several application systems, such as a non-destructive evaluation (NDE) system for deep-lying defects in a metallic plate and a reel-to-reel testing system for striated HTS-coated conductors.
<phrase>Deep Neural Networks</phrase> for Improved, Impromptu <phrase>Trajectory Tracking</phrase> of Quadrotors — <phrase>Trajectory tracking</phrase> control for quadrotors is important for applications ranging from <phrase>surveying</phrase> and inspection , to film making. However, designing and tuning classical controllers, such as proportional-integral-derivative (PID) controllers, to achieve high tracking precision can be time-consuming and difficult, due to hidden dynamics and other non-idealities. The <phrase>Deep Neural Network</phrase> (DNN), with its superior capability of approximating abstract, nonlinear functions, proposes a novel approach for enhancing <phrase>trajectory tracking</phrase> control. This paper presents a DNN-based algorithm that improves the tracking performance of a classical feedback controller. Given a desired trajectory, the DNNs provide a tailored input to the controller based on their gained experience. The input aims to achieve a unity map between the desired and the output trajectory. The motivation for this work is an interactive " fly-as-you-draw " application, in which a user draws a trajectory on a <phrase>mobile device</phrase>, and a quadrotor instantly <phrase>flies</phrase> that trajectory with the DNN-enhanced control system. <phrase>Experimental results demonstrate</phrase> that the proposed approach improves the tracking precision for user-drawn trajectories after the DNNs are trained on selected periodic trajectories, suggesting the method's potential in <phrase>real-world applications</phrase>. Tracking errors are reduced by around 40-50% for both <phrase>training and testing</phrase> trajectories from users, highlighting the DNNs' capability of generalizing knowledge.
Vulnerability of <phrase>Machine Learning</phrase> Models to Adversarial Examples We propose a genetic algorithm for generating adversarial examples for <phrase>machine learning</phrase> models. Such approach is able to find adversarial examples without the access to model's parameters. Different models are tested, including both <phrase>deep and shallow</phrase> <phrase>neural networks</phrase> archi-tectures. We show that RBF networks and SVMs with Gaussian kernels tend to be rather robust and not prone to misclassification of adversarial examples.
<phrase>Electroluminescence</phrase> spectroscopy for reliability investigations of 1.55 mum bulk semiconductor <phrase>optical amplifier</phrase> This paper demonstrates the complementary relation between functional parameters and <phrase>electroluminescence</phrase> spectroscopy for reliability investigations of 1550 nm Semiconductor Optical Amplifiers of 700 µm length active region. Ageing tests have been set to 270 mA-100<phrase>°C</phrase>-1500 h and realized on two different wafers showing more impact on wafer 1 than on wafer 2. Our investigations are particularly focused on interpretation of <phrase>electroluminescence</phrase> spectra, from reference and aged <phrase>SOAs</phrase> of wafer 1, leading to an improvement of degradation mechanisms understanding. The shift rate to lower energies of the <phrase>recombination</phrase> energy peak at 1550 nm, as reported by <phrase>electroluminescence</phrase> spectra between reference and aged <phrase>SOAs</phrase> in relation with the decrease of <phrase>optical power</phrase> measured at 200 mA for the degraded <phrase>SOA</phrase> and completed by I(V) characterizations, suggest occurrence of non radiative deep centers near the buried ridge structure in relation with the cleaning process uniformity of interfaces before epitaxial overgrowth. These defects mainly trap majority injected carriers instead of minority carriers reducing the <phrase>luminescence</phrase> in the active zone. By monitoring the most sensitive failure indicator (pseudo-threshold current), lifetime distributions are also calculated to determine <phrase>failure rate</phrase>, between 150 and 200 FITs over 15 years for <phrase>operating conditions</phrase> (25<phrase>°C</phrase>-200 mA) using experimental degradation laws and statistic computations, demonstrating the overall robustness of this technology.
Considering diagnosis functionality during automatic system-<phrase>level design</phrase> of automotive networks Today, <phrase>design automation</phrase> approaches for automotive E/E-architectures focus solely on application functionality, neglecting <phrase>firmware</phrase>-related functionalities like diagnostic tests that are of utmost importance for quality features such as dependability or maintenance. However, the latter are typically considered dispensable since they do not provide direct service to the user. This paper proposes a novel approach for integrating optional diagnosis functionality into a holistic design <phrase>space exploration</phrase> of automotive E/E-architectures at system-level. Opposed to application functionality, hardware-diagnostics dig deep into the hardware-structures and, hence, require specific tailoring for the employed resources. A case study with <phrase>Software-Based Self</phrase>-Tests representing advanced diagnosis functionality gives evidence of the viability and efficiency of the proposed approach, highlighting the importance of a holistic consideration of application as well as <phrase>firmware</phrase>-related functionality.
<phrase>Client Side</phrase> Estimation of a Remote Service Execution — many use cases, concerning the monitoring and controlling of real physical instruments, demand deep interaction between users and services that virtualize the access to such instruments/devices. In addition, in order to realize high interoperable solutions, <phrase>SOA</phrase>-based Web/Grid Service technologies must be adopted. When the access to one of these services is performed via internet using a <phrase>Web Service</phrase> call, the remote <phrase>invocation</phrase> time becomes critical in order to understand if an instrument can be controlled properly, or the delays introduced by the wire and the serialization/deserialization process are unacceptable. This paper thus presents methodologies and algorithms, based on a 2 k <phrase>factorial</phrase> analysis and a Gaussian Majorization of previous service execution times, which enables the estimation of a generic remote method execution time. Furthermore it suggests three different <phrase>software architectures</phrase>, where the developed algorithms and methodology could be integrated in order to automatically profile the <phrase>end-to-end</phrase> service. It is worth noting that our proposals are validated using suitable benchmarks and <phrase>extensive tests</phrase> coming out from a real (not simulated) environment. In addition, the outcome of this paper have been used in the realization of a service for <phrase>remote control</phrase>, monitor, and manage of a pool of instruments/devices.
An Architectural Approach to the Management of Applications with Qos Requirements on Grid an Architectural Approach to the Management of Applications with Qos Requirements on Grid Title: an Architectural Approach to the Management of Applications with Qos Requirements on Grid <phrase>Grid Computing</phrase> is surely one of the most interesting and widely studied distributed paradigms of the last decade. During years of research, development and test, Grid has evolved in different directions, far away from its original conception. In particular, <phrase>Grid Computing</phrase> has been originally designed as an universal platform to share and access worldwide resources in order to support the management and execution of large scientific jobs. Despite the first academic target, the potentiality of Grid as an architecture for sharing any kind of resources had made the platform suitable for other kinds of tasks. For this, Grid evolved from scientific to commercial purposes, extending the number of potential applications. From a system perspective, this corresponds to an increase in the kinds of jobs to manage by the Grid and a raise of the QoS level that the Grid users demand. Under this perspective, it is important to find out how the new Grid applications can be efficiently managed over a platform that was not designed for their management in its original conception. In particular, the contribution of this thesis is dedicated to the study of jobs with time constraints on execution. Currently, these jobs are not efficiently managed by any Grid infrastructure. Because this topic is general enough, it has been studied from different perspectives, in order to make the research activity consistent and a good basis for further works. For this, the aim is to provide a proper classification of the problem and a contribution both at the theoretical and at the implementation levels. To this regard, this thesis is divided into three main parts. At first, a deep analysis of the state of the art regarding the management of QoS constraints is made. The aim of such analysis is double: in primis, it is important for showing the inadequacy of current Grid framework solutions for supporting time constraints on Grid; moreover, it allows to justify the need of time constraints management on Grid through an evaluation of the evolution of the Grid under a QoS perspective. The second contribution of the thesis is the definition of a proper Grid framework architecture , SoRTGrid, for supporting the management of time constraints on Grid. SoRTGrid framework has been defined and designed starting from the analysis of the state of the art carried out in the first part. In particular, the architectural choices have been made considering the lacks of …
Approximating the disambiguation of some German nominalizations by use of weak structural, lexical and corpus information Resumen: Entre el método clásico y simbólico de desambiguación de sentidos (WSD) que utiliza representaciones semánticas profundas de oraciones y textos, y el método estadístico que utiliza información relativa a la co-ocurrencia de palabras, existe una tendencia reciente a usar métodos híbridos. De manera similar a la llamada semántica <phrase>lightweight</phrase> (Marek, 2009), en este artículo se propone hacer <phrase>uso</phrase> de escasa información semántica. Describimos un modelo de aproximación sobre la base de Flat Underspecified Discourse Representation Structures Abstract: Between classical symbolic <phrase>word sense disambiguation</phrase> (wsd) using explicit <phrase>deep semantic</phrase> representations of sentences and texts and statistical wsd using word co-occurrence information, there is a recent tendency towards mediating methods. Similar to so-called <phrase>lightweight</phrase> semantics (Marek, 2009) we suggest to only make sparse use of <phrase>semantic information</phrase>. We describe an approximation model based upon flat underspecified discourse representation structures (FUDRSs, cf. Eberle, 2004) that weighs knowledge about context structure, lexical semantic restrictions and interpretation preferences. We give a catalogue of guidelines for human annotation of texts by corresponding indicators. Using this, the reliability of an analysis tool that implements the model can be tested with respect to annotation precision and disambiguation prediction and how both can be improved by <phrase>bootstrapping</phrase> the knowledge of the system using corpus information. For the balanced test corpus considered the <phrase>recognition rate</phrase> of the preferred reading is 80-90% (depending on the smoothing of parse errors).
Research on the Effects of Mechanical and <phrase>Physical Characteristics</phrase> on Peanut Shucking Shucking is one of the necessary parts for the deep process of peanut, and has great impact on the quality of peanut kernel.Taking the shucking force as the evaluation indicator, the effects of mechanical and <phrase>physical characteristics</phrase>, which include the loading orientation, loading rate, <phrase>moisture content</phrase>, partical size and the number of peanut kernel, on peanut shucking were investigated by means of single factor test. The results show that: 1) the loading orientation and <phrase>moisture content</phrase> affect the shucking force significantly, and the needed force is the minimum when the peanut is vertically placed and loaded. With the decrease of <phrase>moisture content</phrase>, the needed force decreases gradually.2) the loading rate, particle size and the number of peanut kernel have no great effect on the shucking force, but the deformation and maximum shucking force vary with the loading rate. Additionally, the needed force increases with the number of peanut kernel. The research results can provide some reference for the designment of peanut sheller and selection of the technique parameters.
Robust Multiscale <phrase>Stereo Matching</phrase> from Fundus Images with Radiometric Differences A robust multiscale <phrase>stereo matching</phrase> algorithm is proposed to find reliable <phrase>correspondences between</phrase> low contrast and weakly <phrase>textured</phrase> retinal image pairs with radiometric differences. Existing algorithms designed to deal with <phrase>piecewise</phrase> planar surfaces with distinct features and Lambertian reflectance do not apply in applications such as 3D reconstruction of medical images including stereo retinal images. In this paper, robust pixel feature vectors are formulated to extract discriminative features in the presence of noise in <phrase>scale space</phrase>, through which the response of <phrase>low-frequency</phrase> mechanisms alter and interact with the response of <phrase>high-frequency</phrase> mechanisms. The <phrase>deep structures</phrase> of the scene are represented with the evolution of disparity estimates in <phrase>scale space</phrase>, which distributes the matching ambiguity along the scale dimension to obtain globally coherent reconstructions. The performance is verified both qualitatively by face validity and quantitatively on our collection of stereo fundus image sets with <phrase>ground truth</phrase>, which have been made publicly available as an extension of standard test images for <phrase>performance evaluation</phrase>.
Diving decompression models and bubble metrics: Modern computer syntheses A quantitative summary of computer models in diving applications is presented, underscoring dual phase dynamics and quantifying metrics in tissue and blood. Algorithms covered include the multitissue, diffusion, split phase gradient, linear-exponential, asymmetric tissue, <phrase>thermodynamic</phrase>, varying permeability, reduced gradient bubble, tissue bubble diffusion, and linear-exponential phase models. Defining relationships are listed, and <phrase>diver</phrase> staging regimens are underscored. Implementations, diving sectors, and correlations are indicated for models with a history of widespread acceptance, utilization, and safe application across recreational, scientific, military, research, and technical communities. Presently, all models are incomplete, but many (included above) are useful, having resulted in diving tables, underwater meters, and dive planning software. Those herein employ varying degrees of calibration and data tuning. We discuss bubble metrics in tissue and blood as a backdrop against computer models. The past 15 years, or so, have witnessed changes and additions to diving protocols and table procedures, such as shorter nonstop time limits, slower ascent rates, shallow safety stops, ascending repetitive profiles, deep decompression stops, <phrase>helium</phrase> based breathing mixtures, permissible reverse profiles, multilevel techniques, both faster and slower controlling repetitive tissue halftimes, smaller critical tensions, longer flying-after-diving surface intervals, and others. Stimulated by Doppler and imaging technology, table and decompression meter development, theory, statistics, chamber and <phrase>animal testing</phrase>, or safer diving consensus, these modifications affect a <phrase>gamut</phrase> of activity, spanning bounce to decompression, single to multiday, and air to <phrase>mixed gas</phrase> diving. As it turns out, there is growing support for many protocols on operational, experimental, and theoretical grounds, with bubble models addressing many concerns on plausible bases, but with further testing or profile data analyses requisite.
Implementation of <phrase>Design For Test</phrase> for Asynchronous NCL Designs In the past two decades, the IC Design industry has set what one might refer to as milestones in the golden era of electronics and computers. Current statistics reveal that the number of gates on a chip in 2005 is 100K compared to 23K just five years back. With the chip density increasing at this rate, there is an inherent need to allow for some efficient testing mechanism on-chip to avail benefits in terms of quality as well as <phrase>economy</phrase>. Adding test capabilities to a chip being fabricated increases the initial infrastructure, but the savings that it brings about in terms of cost, time, and maintenance far exceeds the testing cost. In spite of all the innovations, testing asynchronous designs has remained dormant; the reason for this being its inherent complexity. Absence of the global <phrase>clock signal</phrase> and presence of more state-holding gates creates a more complex test environment for these designs. The motivation behind this paper stems from the requirement of an efficient testing methodology for a particular class of <phrase>asynchronous circuits</phrase> known as Null Conventional Logic (NCL) circuits. The methodology proposed in this paper is easy to use for testing fairly complex designs and is tailored to work with conventional DFT tools. The ease of design of synchronous circuits has been the sole reason for widespread development and use of synchronous design techniques. Also, CAD tools for <phrase>synchronous designs</phrase> have become more advanced and sophisticated allowing total automation of several stages of the design process. However, with clock speeds nearing the <phrase>gigahertz</phrase> range and the <phrase>CMOS technology</phrase> reaching <phrase>deep submicron</phrase> ranges, serious doubts have been cast over the suitability of <phrase>synchronous designs</phrase> for <phrase>next generation</phrase> processors and systems. Problems associated with clock synchronization, <phrase>power consumption</phrase>, and noise in <phrase>synchronous designs</phrase> has forced designers to look for alternatives [7]. 1.1 Need for Asynchronous Systems Designers are looking at <phrase>asynchronous circuits</phrase> as a potential solution to these problems as they are modular and do not require clock synchronization. Some of the possible benefits of asynchronous techniques are listed in Table 1. A variety of approaches exist for the design and implementation of <phrase>asynchronous circuits</phrase>. Huffman's model and Muller's model form the basis for many of these approaches. <phrase>Asynchronous circuits</phrase> fall into two main categories: delay-insensitive and bounded-delay models [13]. Table 1 – Advantages of Asynchronous Design No global clock <phrase>Low power</phrase> Average-case performance instead of <phrase>worst-case</phrase> Less <phrase>EMI</phrase> No <phrase>glitch</phrase> …
Book Review Handbook for Language Engineers The reader learns from this book's introduction (written by <phrase>Ali</phrase> Farghaly) that the objective of the book is to " equip <phrase>linguists</phrase> embarking on NLP assignments. " The introduction also explains why language engineers are needed and summarizes the contents of each chapter. " Domain Analysis and Representation " by Farghaly and Bruce Hedin offers a good discussion of the importance of domain analysis in <phrase>natural language processing</phrase> (NLP). It provides a helpful background on the notion of sublanguages and correctly notes that distinctions between the different domains can be blurred, as domains often overlap. The chapter would have been more convincing if there were further examples of NLP applications that benefit from narrowing down the domain of their operation. The chapter also discusses the analysis of domain into topics. Section 2.4.4 covers statistical approaches to classification, but no references or further reading pointers are given. " The Language of the Internet " by Naomi <phrase>Baron</phrase> gives an easy-to-read and useful chronological overview of the developments on the Internet. It provides information about a number of technologies that are used on the Internet, comments on the changing styles of <phrase>natural language</phrase> use, and briefly overviews Web <phrase>markup</phrase> and <phrase>programming languages</phrase> and the <phrase>Semantic Web</phrase>. At the same time, I found this chapter too general. It would have been useful if the chapter had covered specific new NLP applications that are relevant to the Internet, such as <phrase>question answering</phrase>, and had covered in greater detail low-quality <phrase>machine translation</phrase> for e-<phrase>mail</phrase> and chat or <phrase>text categorization</phrase> of <phrase>Web pages</phrase>. " Grammar Writing, Testing and Evaluation " by Miriam Butt and Tracy Holloway <phrase>King</phrase> provides a very good and accessible historical and linguistic account of grammars and parsing in NLP. It covers <phrase>deep and shallow</phrase> parsing and associated techniques as well as testing and evaluation of grammars and parsers. Morphological analyzers and part-of-speech taggers are briefly outlined too. A good practical point is the section on documentation of grammar writing. " Ontologies " by Natalya Noy is a concise and plainly written introduction to the topic of ontologies and their development. It clarifies key terms in the ontology development <phrase>jargon</phrase> and includes an overview of major ontologies, ontology libraries, and
An Approach for Designing On-Line Testable <phrase>State Machines</phrase> 1. Introduction Synthesis of <phrase>state machines</phrase> have attracted the attention of researchers for more than two decades. Several state assignment techniques that result in efficient implementation of the next state logic have been developed [1-3]. However, none of these addresses the testability of an implemented machine. A popular approach for enhancing the testability of a <phrase>state machine</phrase> is to modify its design by using the <phrase>scan path</phrase> technique e.g. LSSD [4]. A number of techniques for synthesizing testable <phrase>state machine</phrase> directly from their specifications have also been proposed [5-6]. The goal of these techniques is to make <phrase>state machines</phrase> fully testable for all single <phrase>stuck-at faults</phrase>, and to derive <phrase>test sequences</phrase> for them by using <phrase>test generation</phrase> techniques for combinational circuits. The major problem with these techniques as well as the LSSD is that they can only increase the <phrase>off-line</phrase> testability i.e. they simplify the detection of <phrase>permanent faults</phrase>. <phrase>Recent studies</phrase> show that <phrase>transient faults</phrase> will be the dominant faults in systems designed using <phrase>deep submicron technology</phrase> [7]. Traditional <phrase>off-line</phrase> testability approaches cannot guarantee the detection of <phrase>transient faults</phrase>; they have to be detected during <phrase>normal operation</phrase> of a circuit. This in turn requires that circuits be designed so that they have built-in mechanisms for on-line <phrase>fault detection</phrase>. Over the years some techniques have been proposed for designing <phrase>state machines</phrase> with on-line <phrase>fault detection</phrase> capability [8-10]. However, these techniques concentrate mainly on the post design modifications rather than designing machines that are on-line testable by design. This paper proposes a new approach for designing <phrase>state machines</phrase> that have built-in capability for enhancing on-line and <phrase>off-line</phrase> testability.
Simulating <phrase>remotely sensed</phrase> images of shoaling waves There is substantial current interest in the use of <phrase>remotely sensed</phrase> images to study littoral processes. Of particular interest is using the interaction of the ocean waves with the bottom to infer the <phrase>water depth</phrase> in coastal areas. Testing the accuracy of processing and <phrase>analysis methods</phrase> requires a known set of bottom depths at many points in the images to compare with depths extracted. Such knowledge is not available for most near-shore areas. Consequently, we have developed a simulator which produces a <phrase>time series</phrase> of images showing <phrase>sea surface</phrase> <phrase>elevation</phrase> for waves moving over a specified <phrase>piecewise</phrase> linear bottom of moderate slope. It provides a good approximation to the real <phrase>sea surface</phrase> for a wide range of conditions. Hence it can produce controlled datasets for many purposes in <phrase>shallow-water</phrase> research. The program uses linear wave theory to model a statistical realization of a <phrase>deep-water</phrase> wave field, with a realistic frequency spectrum and directional spreading, and applies the appropriate modifications for shoaling. Several examples show that the program produces reasonable results, indicating it should be a useful analytical tool in a range of problems.
<phrase>Deep learning</phrase> using partitioned data vectors —<phrase>Deep learning</phrase> is a popular field that encompasses a range of <phrase>multi-layer</phrase> <phrase>connectionist</phrase> techniques. While these techniques have achieved great success on a number of difficult <phrase>computer vision</phrase> problems, the representation biases that allow this success have not been thoroughly explored. In this paper, we examine the hypothesis that one strength of many <phrase>deep learning</phrase> algorithms is their ability to exploit <phrase>spatially local</phrase> statistical information. We present a formal description of how data vectors can be partitioned into sub-vectors that preserve <phrase>spatially local</phrase> information. As a <phrase>test case</phrase>, we then use statistical models to examine how much of such structure exists in the MNIST dataset. Finally, we present <phrase>experimental results</phrase> from training RBMs using partitioned data, and demonstrate the advantages they have over non-partitioned RBMs. Through these results, we show how the performance advantage is reliant on <phrase>spatially local</phrase> structure, by demonstrating the performance impact of randomly permuting the input data to destroy local structure. Overall, our results support the hypothesis that a representation bias reliant upon <phrase>spatially local</phrase> statistical information can improve performance, so long as this bias is a good match for the data. We also suggest statistical tools for determining a priori whether a dataset is a good match for this bias or not. I. INTRODUCTION In the past few years, the area of exploration known as <phrase>Deep Learning</phrase> has demonstrated the ability of multilayer con-nectionist networks to achieve good performance on a range of difficult <phrase>machine learning problems</phrase>, and has been gaining increasing prominence and attention in the <phrase>machine learning</phrase> and <phrase>neural network</phrase> communities. The theoretical basis for understanding why these techniques perform well on particular problems, however, has only begun to be explored. A deeper understanding of how and why <phrase>deep learning</phrase> algorithms work will not only help us to decide what problems are good candidates for their application, but may also suggest ways of improving existing techniques or harnessing their strengths in novel contexts. The hypothesis we seek to examine is the hypothesis that many <phrase>deep learning</phrase> algorithms make use of <phrase>spatially local</phrase> structure in their input data to help them achieve good performance. There has long been an intuition in the <phrase>deep learning</phrase> community that this hypothesis is likely to hold, based partly on everyday experience, and partly on the structure of the human <phrase>visual cortex</phrase>. To our knowledge, however, there has been no previous attempt to formalize and test this hypothesis specifically. …
Knowledge Matters: Importance of <phrase>Prior Information</phrase> for Optimization We explore the effect of introducing <phrase>prior information</phrase> into the intermediate level of deep supervised neural networks for a learning task on which all the black-box state-of-the-art <phrase>machine learning algorithms</phrase> tested have failed to learn. We motivate our work from the hypothesis that there is an optimization obstacle involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset for which each image input contains three sprites, and the binary target class is 1 if all three have the same shape. Black-box <phrase>machine learning algorithms</phrase> only got chance on this task. Standard deep supervised <phrase>neural networks</phrase> also failed. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allows to nail the task. Much better than chance but imperfect results are also obtained by exploring architecture and optimization variants, pointing towards a difficult optimization task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of effective <phrase>local minima</phrase> (possibly due to ill-conditioning and the training procedure not being able to escape what appears like a local minimum).
An Explicit Expression for the Newton Direction on the Complex Grassmann <phrase>Manifold</phrase> We have derived the basic statistical properties for the estimated rotary coefficient. These depend on the true value of the rotary coefficient , and the conjugate coherence 2 3 ; a nuisance parameter. Fortunately when the latter is estimated and debiased constructed <phrase>confidence intervals</phrase> maintain appropriate coverage probabilities, so such <phrase>confidence intervals</phrase> have practical utility as illustrated by the <phrase>Labrador Sea</phrase> current <phrase>data analysis</phrase>. ACKNOWLEDGMENT The authors would like to thank J. Lilly for making the <phrase>Labrador Sea</phrase> data available to them. Helpful comments and observations by the reviewers were much appreciated. A. Benignus, " Estimation of the coherence spectrum and its <phrase>confidence interval</phrase> using the <phrase>fast Fourier transform</phrase>, " IEEE Trans. Nonnull distribution of <phrase>likelihood ratio</phrase> criterion for reality of <phrase>covariance matrix</phrase>, " J. Multi-A rotary component method for analysing meteorological and <phrase>oceanographic</phrase> vector <phrase>time series</phrase>, " <phrase>Deep-Sea</phrase> Res. [9] Y. Hayashi, " Space-time spectral analysis of rotary vector series, " J. A variance equality test for two correlated complex Gaussian variables with application to spectral power comparison, " IEEE Trans. A technique for the cross spectrum analysis of pairs of <phrase>complex-valued</phrase> <phrase>time series</phrase>, with emphasis on properties of polarized components and rotational invariants, " <phrase>Deep-Sea</phrase> Res. The variance of mul-titaper spectrum estimates for real Gaussian processes, " IEEE Trans. The effective band-width of a multitaper spectral estimator, " <phrase>Biometrika</phrase>, vol. 82, pp. Abstract—Several important design problems in <phrase>signal processing</phrase> for communications can be cast as <phrase>optimization problems</phrase> in which the objective is a function of the <phrase>subspaces</phrase> spanned by tall complex matrix variables with orthonormal columns. Such problems can be viewed as <phrase>optimization problems</phrase> on the complex Grassmann <phrase>manifold</phrase>, and an effective means for performing this optimization is to use a <phrase>Grassmannian</phrase> version of <phrase>Newton's method</phrase>. To facilitate the implementation of that method, we provide an explicit expression for the <phrase>Grassmannian</phrase> Newton direction for an arbitrary twice <phrase>differentiable function</phrase>. We also use an example in which the pairwise <phrase>chordal</phrase> <phrase>Frobenius</phrase> norm between <phrase>subspaces</phrase> is to be optimized to outline a systematic procedure for obtaining the <phrase>Hessian matrix</phrase>.
<phrase>Variance reduction</phrase> using wafer patterns in I_ddQ data The subject of this paper is I ddQ testing for <phrase>deep sub-micron CMOS</phrase> technologies. The key concept introduced is the need to reduce the variance of good and faulty <phrase>I ddQ</phrase> distributions. Other <phrase>I ddQ</phrase> <phrase>based techniques</phrase> are reviewed within the context of <phrase>variance reduction</phrase>. Using the SEMATECH data and production data, <phrase>variance reduction</phrase> techniques are demonstrated. The main contribution of the paper is the systematic use of the die location and patterns in the I ddQ data to reduce variance. <phrase>Variance reduction</phrase> is completed before any <phrase>I ddQ</phrase> threshold limits are set. Challenge The use of <phrase>I ddQ</phrase> for <phrase>deep sub-micron technologies</phrase> is in question because of increased <phrase>leakage current</phrase> [1]. This poses an important challenge to the IC test industry because <phrase>I ddQ</phrase> tests have demonstrated good <phrase>fault coverage</phrase> for difficult to detect defects with small sets of <phrase>test vectors</phrase>. The scaling of V t and L eff produces faster parts but also higher leakage. Die, wafer and lot differences further complicate the interpretation of <phrase>I ddQ</phrase> .T he increases in the normal range of <phrase>I ddQ</phrase> currents for healthydevices makes it more difficult to distinguish them from faulty devices. There are different interpretations of the term <phrase>variance reduction</phrase>. When measuring <phrase>I ddQ</phrase> values, variance is reduced by repeated measurements of the same <phrase>test vector</phrase>. This is done to reduce the effects of noise in the experimental system. Asecond interpretation of <phrase>variance reduction</phrase> relates to the spread of the distribution formed from <phrase>I ddQ</phrase> measurements of tens to hundreds of <phrase>test vectors</phrase>. In this paper it is shown that estimates of an <phrase>I ddQ</phrase> value can be made solely from the measurements and that this residual has a variance smaller than the original <phrase>I ddQ</phrase> distribution. It is the second interpretation of <phrase>variance reduction</phrase> The key contribution of the paper is the systematic use the die location and patterns in the I ddQ data to reduce variance. Through <phrase>post-processing</phrase> of ATE <phrase>I ddQ</phrase> data new <phrase>pass/fail</phrase> criteria are defined. As a result, die with high intrinsic leakage are re-binned as passes and in-line inspection is used to identify real defects on the re-binned die. The paper begins with a reviewofrecent <phrase>I ddQ</phrase> research viewed from a perspective of <phrase>I ddQ</phrase> <phrase>variance reduction</phrase> of good and faulty die. Based on this review, <phrase>next generation</phrase> <phrase>I ddQ</phrase> <phrase>test methods</phrase> should concentrate on reducing variance before setting threshold limits. Using the …
Hierarchical Fault Response Modeling of Analog/rf Circuits i ABSTRACT In this thesis two methodologies have been proposed for evaluating the fault response of analog/RF circuits. These proposed approaches are used to evaluate the response of the faulty circuit in terms of specifications/measurements. Faulty response can be used to evaluate important test metrics like fail probability, fault coverage and yield coverage of given measurements under <phrase>process variations</phrase>. Once the models for faulty and <phrase>fault free</phrase> circuit are generated, one needs to perform <phrase>Monte Carlo</phrase> sampling (as opposed to <phrase>Monte Carlo</phrase> simulations) to compute these statistical parameters with <phrase>high accuracy</phrase>. The first method is based on adaptively determining the order of the model based on the error budget in terms of computing the statistical metrics and position of the threshold(s) to decide how precisely necessary models need to be extracted. In the second method, using hierarchy in <phrase>process variations</phrase> a hybrid of heuristics and localized linear models have been proposed. Experiments on LNA and Mixer using the adaptive model order selection procedure can reduce the number of necessary simulations by 7.54x and 7.03x respectively in the computation of fail probability for an error budget of 2%. Experiments on LNA using the hybrid approach can reduce the number of necessary simulations by 21.9x and 17x for four and six output parameters cases for improved accuracy in test statistics estimation. <phrase>ii ACKNOWLEDGMENTS</phrase> Although this thesis book lists only one author, in reality the ideas it molds together were contributed and refined by many extraordinarily insightful colleagues. My first thanks go to the almighty for directing me to Prof. Sule Ozev to conduct this masters research. On most days I leave her office after the meeting wondering if I have really got the benefit of my 18 years of <phrase>science education</phrase>. My interactions with her have proved time and again that learning is a continuous process and this gives me great deal of enthusiasm to delve deep into new and obscure topics. I thank her for all the guidance during this enduring period of research. I would also like to thank Christen for promptly agreeing to serve on the defense committee. I am indebted to my parents who have for all the motivation, emotional and <phrase>financial support</phrase> much needed during my educational career. I am thankful to Ender Yilmaz, Afsaneh Nassery, Osman Erol and other members of Prof. Ozev's <phrase>research group</phrase> for all the fruitful discussions not only restricted to research, but …
<phrase>Text-to-text</phrase> Similarity of Sentences <phrase>Text-to-text</phrase> Similarity of Sentences INTRODUCTION Computational approaches to <phrase>language understanding</phrase> can be classified into two major categories: true-understanding and <phrase>text-to-text</phrase> similarity. In true understanding, the goal is to map language statements onto a <phrase>deep semantic</phrase> representation that relate language constructs to world and <phrase>domain knowledge</phrase>. <phrase>Current state</phrase>-of-the-art approaches that fall into this true-understanding category offer adequate solutions only in very limited contexts (i.e. <phrase>toy</phrase>-domains) lacking scalability and thus having limited use in <phrase>real world applications</phrase> such as summarization or <phrase>intelligent tutoring systems</phrase>. ABSTRACT Assessing the semantic similarity between two texts is a central task in many applications, including sum-marization, <phrase>intelligent tutoring systems</phrase>, and <phrase>software testing</phrase>. Similarity of texts is typically explored at the level of word, sentence, paragraph, and document. The similarity can be defined quantitatively (e.g. in the form of a normalized value between 0 and 1) and qualitatively in the form of <phrase>semantic relations</phrase> such as elaboration, entailment, or paraphrase. In this chapter, we focus first on measuring quantitatively and then on detecting qualitatively sentence-level <phrase>text-to-text</phrase> <phrase>semantic relations</phrase>. A generic approach that relies on word-to-word <phrase>similarity measures</phrase> is presented as well as experiments and <phrase>results obtained</phrase> with various instantiations of the approach. In addition, we provide results of a study on the role of weighting in <phrase>Latent Semantic Analysis</phrase>, a statistical technique to assess similarity of texts. The results were obtained on two <phrase>data sets</phrase>: a standard <phrase>data set</phrase> on sentence-level paraphrase detection and a <phrase>data set</phrase> from an <phrase>intelligent tutoring system</phrase>. <phrase>Text-to-text</phrase> similarity approaches (T2T) to text <phrase>semantic analysis</phrase> avoid the hard task of true understanding by defining the meaning of a text based on its similarity to other texts, whose meaning is assumed to be known. Such methods are called <phrase>benchmarking</phrase> methods as they rely on a benchmark text, analyzed by experts, to indentify the meaning of new, unseen texts. We adopt in this chapter a T2T approach to semantic text analysis. In particular, we focus on the task of quantifying how similar two texts are, and based on this, we then decide whether they are similar enough to be considered a paraphrase or not. An example of two texts, a textbase (T) and student paraphrase (<phrase>SP</phrase>; reproduced as typed by the student in iSTART, an <phrase>intelligent tutoring system</phrase>; McNamara, Levin-stein, & Boonthum, 2004), is provided below (from the User Language Paraphrase Challenge; McCarthy & McNamara, 2008): • T: During vigorous exercise, the heat generated by working muscles can increase …
Toward Realizing a PRAM-on-a-Chip Vision Serial computing has become largely irrelevant for growth in computing performance at around 2003. Having already concluded that to maintain past performance growth rates, <phrase>general-purpose</phrase> computing must be overhauled to incorporate <phrase>parallel computing</phrase> at all levels of a computer system – including the programming model – all processor vendors put forward many-core roadmaps. They all expect exponential increase in the number of cores over at least a decade. This welcome development is also a cause for apprehension. The whole world of computing is now facing the challenge of coming up with a truly <phrase>general-purpose</phrase> parallel <phrase>computing platform</phrase>—the same challenge that eluded <phrase>high performance computing</phrase> (HPC) for so many years-and the clock is ticking. It is becoming common knowledge that if you want your program to run faster you will have to program for parallelism, but the vendors who set up the rules have not yet provided clear and effective means (e.g., programming models and languages) for doing that. How can <phrase>application software</phrase> vendors be expected to make a large investment in new software developments, when they know that in a few years they are likely to have a whole new set of options for getting much better performance?! Namely, we are already in a problematic transition stage that slows down performance growth, and may cause a <phrase>recession</phrase> if it lasts too long. Aware of the ills of HPC, some industry leaders are already predicting that the transition period can last a full decade. The PRAM-On-Chip project started at <phrase>UMD</phrase> in 1997 foreseeing this challenge and opportunity. Building on PRAM – a parallel algorithmic approach that has never been seriously challenged on ease of thinking, or wealth of its <phrase>knowledge-base</phrase> – a comprehensive and coherent platform for on-chip <phrase>general-purpose</phrase> <phrase>parallel computing</phrase> has been developed and prototyped. Optimizing single-task completion time, the platform accounts for application programming (VHDL/Verilog, <phrase>OpenGL</phrase>, MATLAB, etc), <phrase>parallel algorithms</phrase>, <phrase>parallel programming</phrase>, compiling, architecture and <phrase>deep-submicron</phrase> implementation, as well as <phrase>backward compatibility</phrase> on serial code. The approach goes after any type of application parallelism regardless of its amount, regularity, or <phrase>grain size</phrase>. Some prototyping highlights include: an eXplicit Multi-Threaded (XMT) architecture, a new 64-processor, 75MHz XMT (FPGA-based) computer, 90nm ASIC tapeout of the key interconnection network component, a basic compiler, class tested programming methodology where (even <phrase>high-school</phrase>) students are taught only <phrase>parallel algorithms</phrase> and pick the rest on their own, and up to 100X speedups on applications. The talk …
DEEP - Differential Evolution Entirely Parallel Method for Gene Regulatory Networks The Differential Evolution Entirely Parallel (DEEP) method is applied to the <phrase>biological data</phrase> fitting problem. We introduce a new migration scheme, in which the best member of the branch substitutes the oldest member of the next branch that provides a <phrase>high speed</phrase> of the algorithm convergence. We analyze the performance and efficiency of the developed algorithm on a test problem of finding the regulatory interactions within the network of gap genes that control the development of early <phrase>Drosophila</phrase> <phrase>embryo</phrase>. The parameters of a set of nonlinear <phrase>differential equations</phrase> are determined by minimizing the total error between the model behavior and experimental observations. The age of the individuum is defined by the number of iterations this individuum survived without changes. We used a ring topology for the network of computational nodes. The computer codes are available upon request.
Understanding the Inflexibility of <phrase>Process Integration</phrase> The objective of this study is to establish a base for understanding inflexibility, with different kinds of integration problems being examined and explained. Studies of <phrase>process integration</phrase> have focused mostly on the design and management of efficient operation with <phrase>information technology</phrase> but very little on the difficulty in making changes with tightly linked processes. In order to eliminate risks of integration failure, there is a need for deep understanding of the types and causes of inflexibility with <phrase>process integration</phrase>. Based on the literature and reported cases of <phrase>process integration</phrase> and enterprise flexibility/inflexibility, this study proposes that inflexibility can be classified as either operational, organizational, or systems inflexibility. The incompleteness of business design and the misalignment of users' and system designers' capability seem to be key sources of inflexibility. Though the propositions are to be further tested, the proposed framework with different categories of inflexibility and different sources for inflexibility provides a reference for planning effective management of <phrase>process integration</phrase>.
Eliminating Non-<phrase>Determinism</phrase> During Test of <phrase>High-Speed</phrase> Source Synchronous Differential Buses The at-speed <phrase>functional testing</phrase> of <phrase>deep sub-micron</phrase> devices equipped with <phrase>high-speed</phrase> I/O ports and the asynchronous nature of such I/O transactions poses significant challenges. In this paper, the problem of non-<phrase>determinism</phrase> in the output response of the device-under-test (DUT) is described. This can arise due to limited automated <phrase>test equipment</phrase> (ATE) edge placement accuracy (<phrase>EPA</phrase>) in the source synchronous clock of the stimulus stream to the <phrase>high-speed</phrase> I/O port from the tester. A simple yet effective solution that uses a trigger signal to initiate a deterministic transfer of test inputs into the core clock domain of the DUT from the <phrase>high-speed</phrase> I/O port is presented. The solution allows the application of at-speed functional patterns to the DUT, while incurring a very small <phrase>hardware overhead</phrase> and trivial increase in test application time. An analysis of the probability of non-<phrase>determinism</phrase> as a function of <phrase>clock speed</phrase> and <phrase>EPA</phrase> is presented. It shows that as the frequency of operation of <phrase>high-speed</phrase> I/Os continues to rise, non-<phrase>determinism</phrase> will become a significant problem that can result in an unacceptable <phrase>yield loss</phrase>.
A Novel Mems Silicon <phrase>Probe Card</phrase> We have developed a novel <phrase>cantilever</phrase>-type probe which is capable of less than 70 of pitch and 12g of force. This probe is suitable for <phrase>wafer-level</phrase> burn-in testing, function testing and <phrase>circuit-board</phrase> O/S testing including memory and RF devices. The probe was fabricated with epitaxial polysilicon on silicon substrate. The through via hole interconnection was formed in silicon wafer by <phrase>nickel</phrase> electroless plating and cupper <phrase>electroplating</phrase>. The electroless plating is easy method and can deposit film uniformly for deep trench and through hole. Especially, it can deposit film on any <phrase>substrates</phrase> without seed layer and allow it to be electroplated. The aspect ratio of through via hole was larger than 10:1 and the contact resistance was less than 1 <phrase>ohm</phrase>.
Incremental Evolution of Animats' Behaviors as a <phrase>Multi-objective</phrase> Optimization <phrase>Evolutionary algorithms</phrase> have been successfully used to create controllers for many animats. However, intuitive fitness functions like the survival time of the animat, often do not lead to interesting results because of the bootstrap problem, arguably one of the main challenges in <phrase>evolutionary robotics</phrase>: if all the individuals perform equally poorly, the evolutionary process cannot start. To overcome this problem, many authors defined ordered sub-tasks to bootstrap the process, leading to an incremental evolution scheme. Published methods require a deep knowledge of the underlying structure of the analyzed task, which is often not available to the experimenter. In this paper, we propose a new incre-mental scheme based on <phrase>multi-objective</phrase> evolution. This process is able to automatically switch between each sub-task resolution and does not require to order them. The proposed method has been successfully tested on the evolution of a neuro-controller for a complex-light seeking simulated robot, involving 8 sub-tasks.
Affordable and Effective Screening of <phrase>Delay Defects</phrase> in ASICs using the Inline Resistance <phrase>Fault Model</phrase> Transition <phrase>delay fault</phrase> (TDF) testing has become a necessary <phrase>test method</phrase> in very deep sub micron (VDSM) technologies due to the presence of resistive defects that cause subtle <phrase>timing failures</phrase>. The transition <phrase>delay fault model</phrase> is based on a slow-to-rise and slow-to-fall fault at each node in the circuit. Some resistive defects such as resistive vias actually induce both faults and the TDF <phrase>test set</phrase> can contain unnecessary test patterns for proper screening of this type of defect. The inline resistance fault (IRF) model more accurately represents this defect type and is studied in depth in this paper. ATPG Experimental results show that IRF patterns can be generated 1.4 to 1.8 <phrase>times faster</phrase> with 45% to 58% fewer patterns than traditional TDF patterns. IRF and TDF pattern <phrase>test results</phrase> are presented and show that the more expensive TDF remains a more comprehensive test than IRF as expected, but that the quality impact of using only the IRF <phrase>test set</phrase> is minimal, especially when combined with effective IDDQ outlier screening such as <phrase>statistical post processing</phrase>. Additionally, a methodology is presented for the determination of the number of <phrase>delay defects</phrase> that behave according to each model from the test data alone, which is necessary to accurately determine delay <phrase>defect coverage</phrase> from multiple <phrase>test coverage</phrase> metrics. 1. Introduction Testing devices in production is meant to screen out defective parts. Tests have to be generated that ensure a high probability of uncovering <phrase>physical defects</phrase> like shorts to ground or Vdd, bridges to neighboring signal lines, incorrect resistance of lines or vias, which negatively affect the speed of the device, and many more. Tests are based on <phrase>fault models</phrase> that abstract <phrase>physical defects</phrase>. <phrase>Fault models</phrase> provide a practical approach for tools, which generate or evaluate tests, such as the Automated <phrase>Test Pattern Generation</phrase> (ATPG). Good <phrase>fault models</phrase> enable the ATPG tool to be both, efficient and effective. The efficiency of the tool relies on <phrase>fault models</phrase> that are easy to handle within the tool. The effectiveness of the tool requires <phrase>fault models</phrase> that allow the tool to generate not only the fewest number of patterns, but patterns that are at the same time of <phrase>high quality</phrase>, thus detecting the highest number of <phrase>physical defects</phrase>. Traditionally, <phrase>test pattern generation</phrase> is based on the <phrase>stuck-at fault</phrase> (<phrase>SAF</phrase>) model [1,2]. However, geometry features of 0.18µm and below, as well as n ew materials and processes, cause new types …
Format-Transforming Encryption: More than Meets the DPI —<phrase>Nation-states</phrase> and other organizations are increasingly deploying <phrase>deep-packet inspection</phrase> (DPI) technologies to censor <phrase>Internet traffic</phrase> based on <phrase>application-layer</phrase> content. We introduce a new DPI circumvention approach, format-transforming encryption (FTE), that cryptographically transforms the format of arbitrary <phrase>plaintext</phrase> data (e.g. packet contents) into specified formats that are designed to bypass DPI tests. We show how to build a <phrase>general-purpose</phrase> FTE system, in which these formats are defined compactly by families of <phrase>regular expressions</phrase>. Moreover, we specify and implement a full FTE record-layer protocol. We exhibit formats that are guaranteed to avoid known filters, and give a framework for learning formats from non-censored HTTP traffic. These formats are put to use in our FTE record layer, to explore <phrase>trade-offs</phrase> between performance and steganographic capabilities. As one example, we visit the top 100 Alexa <phrase>webpages</phrase> through an FTE tunnel, incurring an average overhead of roughly 5%.
Learning Feature Hierarchies with Centered Deep Boltzmann Machines Deep Boltzmann machines are in principle powerful models for extracting the hierarchical structure of data. Unfortunately, attempts to train layers jointly (without greedy layer-wise pretraining) have been largely unsuccessful. We propose a modification of the <phrase>learning algorithm</phrase> that initially recenters the output of the activation functions to zero. This modification leads to a better conditioned <phrase>Hessian</phrase> and thus makes learning easier. We test the algorithm on <phrase>real data</phrase> and demonstrate that our suggestion, the centered deep Boltzmann machine, learns a hierarchy of increasingly abstract representations and a better generative model of data.
Suppression of <phrase>deep brain stimulation</phrase> artifacts from the electroencephalogram by <phrase>frequency-domain</phrase> Hampel filtering. OBJECTIVE Currently, <phrase>electroencephalography</phrase> (EEG) cannot be used to record cortical activity during clinically effective DBS due to the presence of large stimulation artifact with components that overlap the useful spectrum of the EEG. A filtering method is presented that removes these artifacts whilst preserving the spectral and temporal fidelity of the underlying EEG.   METHODS The filter is based on the Hampel identifier that treats artifacts as outliers in the <phrase>frequency domain</phrase> and replaces them with interpolated values. Performance of the filter was tested with a synthesized DBS signal and actual data recorded during bilateral monopolar DBS.   RESULTS Mean increases in <phrase>signal-to-noise ratio</phrase> of 7.8dB for single-frequency stimulation and 13.8dB for dual-frequency stimulation are reported. Correlation analysis between EEG with synthesized artifacts and artifact-free EEG reveals that distortion to the underlying EEG in the filtered signal is negligible (r(2)>0.99).   CONCLUSIONS <phrase>Frequency-domain</phrase> Hampel filtering has been shown to remove monopolar DBS artifacts under a number of common stimulation conditions used for the treatment of <phrase>Parkinson's disease</phrase>.   SIGNIFICANCE Application of <phrase>frequency-domain</phrase> Hampel filtering will allow the measurement of EEG in patients during clinically effective DBS and thus may increase our understanding of the mechanisms of action of this important therapeutic intervention.
Interconnect performance estimation models for design planning —This paper presents a set of interconnect performance estimation models for design planning with consideration of various effective interconnect layout <phrase>optimization techniques</phrase>, including optimal wire sizing, simultaneous driver and wire sizing, and simultaneous <phrase>buffer insertion</phrase>/sizing and wire sizing. These models are extremely efficient, yet provide high degree of accuracy. They have been tested on a wide range of parameters and shown to have over 90% accuracy on average compared to running best-available interconnect layout optimization algorithms directly. As a result, these fast yet accurate models can be used efficiently during high-<phrase>level design</phrase> <phrase>space exploration</phrase>, interconnect-driven design planning/synthesis, and timing-driven placement to ensure design convergence for deep submicrometer designs.
Parsing German Topological Fields with Probabilistic <phrase>Context-free</phrase> Grammars Abstract Parsing German Topological Fields with Probabilistic <phrase>Context-free</phrase> Grammars A research paper submitted in conformity with the requirements for the degree of M. Sc. 2009 <phrase>Syntactic analysis</phrase> is useful for many <phrase>natural language processing</phrase> applications requiring further <phrase>semantic analysis</phrase>. <phrase>Recent research</phrase> in statistical parsing has produced a number of <phrase>high-performance</phrase> parsers using probabilistic <phrase>context-free</phrase> (PCFG) models to parse English text, such these methods to parse sentences in freer-<phrase>word-order</phrase> languages. Such languages as <phrase>Russian</phrase>, <phrase>Warlpiri</phrase>, and German feature syntactic constructions that produce discontinuous constituents, directly violating one of the crucial assumptions of <phrase>context-free</phrase> models of syntax. While PCFG technologies may thus be inadequate for full <phrase>syntactic analysis</phrase> of all phrasal structure in these languages, clausal structure can still be fruitfully parsed with these methods. In particular, we examine applying PCFG parsing to parse the topological field structure of German. These topological fields provide a high-level description of the major sections of a clause in relation to the clausal main verb and the subordinating heads and appear in strict linear sequences amenable to PCFG parsing. They are useful for tasks such as deep <phrase>syntactic analysis</phrase>, <phrase>part-of-speech tagging</phrase> and coreference resolution. In this work, we apply an unlexicalized, <phrase>latent variable</phrase>-based parser (Petrov et al., 2006) to topological field parsing, and achieve state-of-the-art parsing results on two German <phrase>newspaper</phrase> corpora without any language-or model-dependent adaptation. We perform a qualitative error analysis of the parser output, and identify constructions like ellipses and parentheticals as the chief sources of remaining error. This is confirmed by a 3 further experiment in which parsing performance improves after restricting the training and <phrase>test set</phrase> to those sentences without these constructions. We also explore techniques for further improving parsing results. For example, discrimina-tive reranking of parses made by a generative parser could incorporate <phrase>linguistic information</phrase> such as those derived by our qualitative analysis. Self-training is another <phrase>semi-supervised</phrase> technique which utilizes additional unannotated data for training. 4 Acknowledgements Many people have contributed to the making of this document, whom I would like to thank here. First and foremost, I would like to thank my supervisor and mentor, Gerald <phrase>Penn</phrase>. I am continually amazed by the breadth and depth of his knowledge. Through this, he has given me a much better perspective of <phrase>computational linguistics</phrase>, yet I know that I have much more to learn from him in the future. I would also like to thank Graeme Hirst for his role as the second reader of this paper. His comments have …
Revealing non-analytic kinematic shifts in smooth <phrase>goal-directed</phrase> behaviour How do biological agents plan and organise a smooth accurate path to shift from one smooth mode of behaviour to another as part of graceful movement that is both plastic and controlled? This paper addresses the question in conducting a novel shape analysis of approach and adjustment phases in rapid voluntary target aiming and 2-D reaching hand actions. A number of mode changing experiments are reported that investigate these actions under a range of goals and conditions. After a typically roughly aimed approach, regular <phrase>projective</phrase> adjustment is observed that has height and velocity kinematic profiles that are scaled copies of one another. This empirical property is encapsulated as a novel self-similar shift function. The mathematics shows that the biological shifts consist of <phrase>continual</phrase> deviation from their full <phrase>Taylor series</phrase> everywhere throughout their interval, which is a deep form of plasticity not described before. The experimental results find the same approach and adjustment strategy to occur with behavioural trajectories over the full and varied range of tested goals and conditions. The trajectory shapes have a large degree of predictability through using the shift function to handle extensive variation in the trajectories' adjustment across individual <phrase>behaviours</phrase> and subjects. We provide connections between the behavioural features and results and various neural studies to show how the methodology may be exploited. The conclusion is that a roughly aimed approach followed by a specific highly plastic shift adjustment can provide a regular basis for <phrase>fast and accurate</phrase> <phrase>goal-directed</phrase> motion in a simple and generalisable way.
Toward Nature Inspired Computing 1 Formulation and Characteristics of Nic In this article, we take a look at an emerging computing paradigm called Nature Inspired Computing (NIC). We examine the impacts of NIC in two aspects. First, NIC enables us to explain the underlying mechanism of a real-world complex system by formulating computing models and testing hypotheses through controlled experimentation. The end product of such computing experiments is a deep understanding or a new discovery of the real working mechanism of the modeled system. Second, NIC enables us to embody autonomous (e.g., lifelike) behavior in solving computing problems. With detailed knowledge of the underlying mechanism, abstracted autonomous behavior can be used as a model for a <phrase>general-purpose</phrase> <phrase>problem-solving</phrase> strategy or method. Generally speaking, the objectives of NIC are twofold: (1) characterizing and understanding complex phenomena or systems behavior, and (2) designing and developing computing solutions to hard problems. Neither of these objectives can be achieved without formulating a model of the factors underlying a complex system. The modeling process can be started with a theoretical analysis from a macroscopic or microscopic view of the system. Alternatively, a blackbox or whitebox approach may be adopted. Blackbox approaches such as Markov models or <phrase>artificial neural networks</phrase> normally do not tell us much about the working mechanism. On the other hand, whitebox approaches such as agents with <phrase>bounded rationality</phrase> are more useful for explaining behavior [10]. NIC Formulation. The essence of NIC formulation lies in the conception of a computing system that is operated by population(s) of autonomous entities. The rest of the system is referred to as the environment. An autonomous entity consists of a detector (or a set of detectors), an effector (or again, a set of effectors), and a repository of local behavior rules (see Figure 1) [5][8].
<phrase>Silicon Debug</phrase> of Systems-on-Chips Modern semiconductor <phrase>process technologies</phrase>, advanced <phrase>design tools</phrase>, and the reinvented reuse paradigm enable the design of very complex ICs. Some call these ICs 'system-on-chip', referring to the fact that their functionality could until recently only be implemented by one or several <phrase>PCBs</phrase> filled with ICs. While it was always difficult to locate design errors, guaranteeing that a <phrase>deep sub-micron</phrase> 'system-on-chip' is design error free is a real challenge. Floating specifications, growing geographically-spread design teams, time-to-market pressure, and the increasing distance of IC designers to actual silicon all make it likely that 'buggy' hardware will become as common as 'buggy' software. Of course our industry does whatever is possible within given time and money budgets to prevent design errors before first silicon. Hereto techniques as simulation, emulation, and <phrase>formal verification</phrase> are used. However, all these techniques only deal with models of the IC, which do not take into account all effects that might occur on real silicon, and high computational costs often prevent exhaustive error coverage. In order to find design errors before the customer does, debug of actual silicon samples is inevitable. This <phrase>Hot Topic</phrase> session provides an overview of the state-of-the-art in physical and electrical <phrase>silicon debug</phrase>. The speakers address techniques currently in use, their applications and their limitations, and the research challenges for the future. (EJM) <phrase>Silicon Debug</phrase> can be needed at various stages in the lifecycle of a product (ASIC, system): during development, qualification , production ramp-up, or in maintenance-like activities in the field life of a product. The root cause of the problem-to-be-debugged can lie in the technology used, as well as in the design implementation (or the combination of these); the debugging process has to cover both aspects. Although it is clear that software tools and <phrase>Design-for-Test</phrase> are gaining in importance with the advent of systems-on-chip, physical de-bug methods have still an important role to play. One clear example is debugging of problems that are only visible by an increased <phrase>supply current</phrase>. The presentation gives an overview of the different categories of physical debug methods, and illustrates their role in the total debug process. Physical methods for internal electrical analysis (e.g., probing, E-beam test), both on active circuitry and on isolated building blocks. Physical failure localisation techniques, based on thermal effects, <phrase>photon</phrase> emission, or the interaction of the circuitry with electron, ion, or <phrase>photon</phrase> beams. Physical device modification (e.g., using <phrase>Focused Ion Beam</phrase>) to verify …
State estimation in <phrase>power distribution</phrase> networks with poorly synchronized measurements —We consider the problem of designing a state estimation architecture for the <phrase>power distribution</phrase> grid, capable of collecting raw measurements from low-end <phrase>phasor</phrase> measurement units, processing the data in order to minimize the effects of measurement noise, and presenting the state estimate to the control and monitoring applications that need it. The proposed approach is leader-less, scalable, and consists in the distributed solution of a least square problem which takes into explicit consideration the inexact time synchronization of inexpensive PMUs. Two algorithms are proposed: the first one is a specialization of the alternating directions method of multipliers (ADMM) for which a scalable implementation is proposed; the second one is an extremely <phrase>lightweight</phrase> Jacobi-like algorithm. Both algorithms can be implemented locally by local data aggregators, or by the individual PMUs, in a <phrase>peer-to-peer</phrase> fashion. The proposed approach is validated via simulations on the IEEE 123 test feeder, considering the <phrase>IEEE standard</phrase> C37.118-2005 for PMUs. We also analyze the performance of a distributed <phrase>control algorithm</phrase> that makes use of the grid state estimation and that we adopted as a prototype. We show that, via the proposed state estimation architecture, <phrase>feedback control</phrase> of the distribution grid becomes viable also with poorly synchronized sensors, and possibly also without GPS-based synchronization modules. I. INTRODUCTION The electric grid is currently undergoing a deep renovation process towards the so-called <phrase>smart grid</phrase>, featuring larger hosting capacity, widespread penetration of <phrase>renewable energy sources</phrase>, better quality of the service, and higher reliability. One of the major aspects of this modernization is the widespread deployment of <phrase>dispersed</phrase> measurement, monitoring , and actuation devices. In this paper, we consider one specific <phrase>thrust</phrase>, which is the deployment of phasorial measurement units (PMUs) in the medium and <phrase>low voltage</phrase> <phrase>power distribution</phrase> grid. The presence of PMUs is quite uncommon in today's <phrase>power distribution</phrase> networks. However, this scenario has become the subject of <phrase>recent research</phrase> efforts in the power systems community , including a 3-year <phrase>research project</phrase> involving University of California together with the Power Standards Lab and Lawrence <phrase>Berkeley</phrase> National Lab [1]. Previous investigation on the subject includes the experimental deployment and the extensive measurement campaign in [2], and the analysis proposed in [3], [4], [5] for the possible applications of such measurement devices, also called µPMUs, in the <phrase>power distribution</phrase> grid.
Sequence based <phrase>residue depth</phrase> prediction using evolutionary information and predicted <phrase>secondary structure</phrase> BACKGROUND <phrase>Residue depth</phrase> allows determining how deeply a given residue is buried, in contrast to the <phrase>solvent</phrase> accessibility that differentiates between buried and <phrase>solvent</phrase>-<phrase>exposed residues</phrase>. When compared with the <phrase>solvent</phrase> accessibility, the depth allows studying <phrase>deep-level</phrase> structures and functional sites, and formation of the <phrase>protein folding</phrase> nucleus. Accurate prediction of <phrase>residue depth</phrase> would provide valuable information for fold recognition, prediction of functional sites, and <phrase>protein design</phrase>.   RESULTS A new method, RDPred, for the real-value depth prediction from protein sequence is proposed. RDPred combines information extracted from the sequence, PSI-BLAST scoring matrices, and <phrase>secondary structure</phrase> predicted with PSIPRED. Three-fold/ten-<phrase>fold cross validation</phrase> based tests performed on three independent, low-identity datasets show that the distance based depth (computed using MSMS) predicted by RDPred is characterized by 0.67/0.67, 0.66/0.67, and 0.64/0.65 correlation with the actual depth, by the mean absolute errors equal 0.56/0.56, 0.61/0.60, and 0.58/0.57, and by the mean relative errors equal 17.0%/16.9%, 18.2%/18.1%, and 17.7%/17.6%, respectively. The mean absolute and the mean relative errors are shown to be statistically significantly better when compared with a method recently proposed by Yuan and Wang [Proteins 2008; 70:509-516]. The results show that three-<phrase>fold cross validation</phrase> underestimates the variability of the prediction quality when compared with the results based on the ten-<phrase>fold cross validation</phrase>. We also show that the <phrase>hydrophilic</phrase> and flexible residues are predicted more accurately than <phrase>hydrophobic</phrase> and rigid residues. Similarly, the charged residues that include Lys, <phrase>Glu</phrase>, Asp, and Arg are the most accurately predicted. Our analysis reveals that evolutionary information encoded using PSSM is characterized by stronger correlation with the depth for <phrase>hydrophilic</phrase> <phrase>amino acids</phrase> (AAs) and <phrase>aliphatic</phrase> AAs when compared with <phrase>hydrophobic</phrase> AAs and aromatic AAs. Finally, we show that the <phrase>secondary structure</phrase> of coils and strands is useful in depth prediction, in contrast to helices that have relatively uniform distribution over the protein depth. Application of the predicted <phrase>residue depth</phrase> to prediction of buried/<phrase>exposed residues</phrase> shows consistent improvements in detection rates of both buried and <phrase>exposed residues</phrase> when compared with the competing method. Finally, we contrasted the prediction performance among distance based (MSMS and DPX) and volume based (SADIC) depth definitions. We found that the distance based indices are harder to predict due to the more complex nature of the corresponding depth profiles.   CONCLUSION The proposed method, RDPred, provides statistically significantly better predictions of <phrase>residue depth</phrase> when compared with the competing method. The predicted depth can be used to provide improved prediction of both buried and <phrase>exposed residues</phrase>. The prediction of <phrase>exposed residues</phrase> has implications in characterization/prediction of interactions with <phrase>ligands</phrase> and other proteins, while the prediction of buried residues could be used in the context of folding predictions and simulations.
The use of on-line co-training to reduce the <phrase>training set</phrase> size in <phrase>pattern recognition</phrase> methods: Application to left ventricle segmentation in ultrasound The use of statistical <phrase>pattern recognition</phrase> models to segment the left ventricle of the heart in ultrasound images has gained substantial attention over the last few years. The main obstacle for the wider exploration of this methodology lies in the need for large annotated training sets, which are used for the estimation of the <phrase>statistical model</phrase> parameters. In this paper, we present a new on-line co-training methodology that reduces the need for large training sets for such <phrase>parameter estimation</phrase>. Our approach learns the initial parameters of two different models using a small manually annotated <phrase>training set</phrase>. Then, given each frame of a <phrase>test sequence</phrase>, the methodology not only produces the segmen-tation of the current frame, but it also uses the results of both classifiers to retrain each other incrementally. This on-line aspect of our approach has the advantages of producing segmentation results and retraining the classifiers on the fly as frames of a <phrase>test sequence</phrase> are presented, but it introduces a harder learning setting compared to the usual <phrase>off-line</phrase> co-training, where the algorithm has access to the whole set of un-annotated <phrase>training samples</phrase> from the beginning. Moreover, we introduce the use of the following new types of classifiers in the co-training framework: <phrase>deep belief network</phrase> and multiple model probabilistic data association. We show that our method leads to a fully automatic left ventricle segmentation system that achieves state-of-the-art accuracy on a public database with training sets containing at least twenty annotated images.
Fluxo: a system for <phrase>internet service</phrase> programming by non-expert developers Over the last 10-15 years, our industry has developed and deployed many <phrase>large-scale</phrase> <phrase>Internet services</phrase>, from <phrase>e-commerce</phrase> to <phrase>social networking</phrase> sites, all facing common challenges in latency, reliability, and scalability. Over time, a relatively small number of <phrase>architectural patterns</phrase> have emerged to address these challenges, such as tiering, caching, partitioning, and pre- or <phrase>post-processing</phrase> compute intensive tasks. Unfortunately, <i>following</i> these patterns requires developers to have a deep understanding of the <phrase>trade-offs involved in</phrase> these patterns as well as an <phrase>end-to-end</phrase> understanding of their own system and its expected workloads. The result is that non-expert developers have a hard time applying these patterns in their code, leading to low-performing, highly suboptimal applications.  In this paper, we propose FLUXO, a system that separates an Internet service's logical functionality from the architectural decisions made to support performance, scalability, and reliability. FLUXO achieves this separation through the use of a restricted <phrase>programming language</phrase> designed 1) to limit a developer's ability to write programs that are incompatible with widely used <phrase>Internet service</phrase> <phrase>architectural patterns</phrase>; and 2) to simplify the analysis needed to identify how <phrase>architectural patterns</phrase> should be applied to programs. Because <phrase>architectural patterns</phrase> are often highly dependent on application performance, workloads and data distributions, our platform captures such data as a runtime profile of the application and makes it available for use when determining how to apply <phrase>architectural patterns</phrase>. This separation makes service development accessible to non-experts by allowing them to focus on application features and leaving complicated architectural optimizations to experts writing application-<phrase>agnostic</phrase>, profile-guided optimization tools.  To evaluate FLUXO, we show how a variety of <phrase>architectural patterns</phrase> can be expressed as transformations applied to FLUXO programs. Even simple heuristics for automatically applying these optimizations can show reductions in latency ranging from 20-90% without requiring special effort from the application developer. We also demonstrate how a simple shared-nothing tiering and replication pattern is able to scale our <phrase>test suite</phrase>, a <phrase>web-based</phrase> IM, <phrase>email</phrase>, and addressbook application.
Total Dose and <phrase>Single Event Effects</phrase> (see) in a 0.25 M <phrase>Cmos Technology</phrase> Individual transistors, <phrase>resistors</phrase> and <phrase>shift registers</phrase> have been designed using radiation tolerant layout practices in a commercial quarter micron process. A modelling effort has led to a satisfactory formulation for the effective aspect ratio of the enclosed transistors used in these layout practices. All devices have been tested up to a total dose of 30Mrad(<phrase>SiO2</phrase>). The <phrase>threshold voltage</phrase> shift after irradiation and annealing was about +45mV for NMOS and-55mV for PMOS transistors, no <phrase>leakage current</phrase> appeared, and the mobility degradation was below 6%. The value of <phrase>resistors</phrase> increased by less than 10%. Noise measurements made on transistors with W=2mm and L varying between 0.36 and 0.64µm revealed a corner noise frequency of about 200kHz for the NMOS and 12kHz for the PMOS. Irradiation up to 30Mrad(<phrase>SiO2</phrase>) did not significantly affect the noise performance. The <phrase>shift registers</phrase> continuously operated at 1.25MHz during the irradiation, and no error was detected in the pattern propagation. No functional degradation was observed. An irradiation with a heavy <phrase>ion beam</phrase> was made on the <phrase>shift registers</phrase> to study their sensitivity to <phrase>Single Event Effects</phrase> (SEE). No <phrase>Single Event</phrase> Latch-up (SEL) was observed up to a LET of 89 MeVcm 2 mg-1. The register designed using dynamic logic, with a threshold LET lower than 3.2 MeVcm 2 mg-1 , proved to be considerably more sensitive to <phrase>Single Event Upset</phrase> (SEU) than its static logic counterpart, which had a threshold LET of about 15 MeVcm 2 mg-1. A novel SEU-tolerant design was demonstrated to be extremely effective as storage cell. 1. MOTIVATION In agreement with the first measurements on ultra-thin oxides [1], <phrase>recent studies</phrase> confirmed the total dose hardness of the thin oxide of a commercial 0.25µm <phrase>CMOS technology</phrase> [2]. The use of radiation tolerant layout practices, based on the systematic use of enclosed NMOS transistors and guardrings, was demonstrated to extend the tolerable total dose level well beyond the inherent technology limit [3]. All these results are extremely promising in view of the possible use of <phrase>deep submicron technologies</phrase> for the readout electronics of <phrase>LHC</phrase>. In this respect, some important issues not addressed in references [2] and [3] still needs to be studied. How to estimate the correct effective aspect ratio of enclosed geometry transistors, an important parameter entering in the design of a circuit, is not known. The noise of transistors in deep submicron technologies needs to be characterised. Moreover, no investigation of <phrase>Single Event Effects</phrase> …
<phrase>Power Supply</phrase> <phrase>Transient Signal Analysis</phrase> Under Real Process and Test Hardware Models A device testing <phrase>method called</phrase> <phrase>Transient Signal Analysis</phrase> (TSA) is subjected to elements of a real process and testing environment in this paper. Simulations experiments are designed to determine the effects of process skew (obtained from measured parameters of a real process) on the accuracy of TSA in estimating <phrase>path delays</phrase> from <phrase>power supply</phrase> I DDT and V DDT waveforms. The circuit model is designed to test TSA under <phrase>deep submicron</phrase> process models that incorporate advanced parameters such as transistor V t width dependencies. Modeling elements of a testing environment including the <phrase>probe card</phrase> are subsequently introduced as a means of evaluating the effects of tester measurement noise in an actual implementation. A testing method that uses <phrase>power supply</phrase> <phrase>transient signals</phrase> as a means of determining <phrase>path delay</phrase> characteristics of digital devices is attractive for several reasons. First, such a method may be useful in detecting resistive shorting and open defects; defects that traditionally have not been targeted by Stuck fault <phrase>based methods</phrase>. Second, reliable <phrase>delay fault</phrase> tests are difficult to generate (because of circuit hazards) and apply (because of structural tester timing accuracy). The global observability provided by <phrase>power supply</phrase> <phrase>transient signals</phrase> permits the measurement of delay without the need to sensitize paths to observation points such as Primary Outputs or scan-latches. Third, the supply tran-sients potentially provide a higher degree of resolution than logic-<phrase>based techniques</phrase> with respect to the parametric characteristics of the device. This information may be useful as a means of reducing <phrase>delay fault test</phrase> coverage constraints. Preliminary investigations have demonstrated that a <phrase>method called</phrase> <phrase>Transient Signal Analysis</phrase> (TSA) is capable of detecting <phrase>delay faults</phrase> [1] and estimating <phrase>path delays</phrase> in <phrase>defect-free</phrase> devices [2]. The focus of this work is to determine the robustness of <phrase>Transient Signal Analysis</phrase> (TSA) to advanced process and tester environment elements. To this end, simulations are conducted on a circuit model derived using the measured <phrase>circuit parameters</phrase> of a real process. For example, the simulation RC-transistor models are based on a sets of specifications sampled from the TSMC's 0.25µm process [3]. The BSIM transistor models used in this work incorporate important <phrase>deep submicron</phrase> parameters such as transistor V t width dependencies. The tester environment is also simulated using advanced <phrase>probe card</phrase> and tester <phrase>power supply</phrase> models obtained from the literature [4]. For example, the probing environment is modeled using an advanced membrane-style <phrase>probe card</phrase> model. We conduct the analysis on a …
Sense Making Alone Doesn't Do It: Fluency Matters Too! ITS Support for Robust Learning with <phrase>Multiple Representations</phrase> <phrase>Previous research</phrase> demonstrates that <phrase>multiple representations</phrase> of learning content can enhance students' learning, but also that <phrase>students learn</phrase> deeply from <phrase>multiple representations</phrase> only if the <phrase>learning environment</phrase> supports them in making connections between the representations. We hypothesized that connection-making support is most effective if it helps students make both in making sense of the content across representations and in becoming fluent in making connections. We tested this hypothesis in a classroom experiment with 599 4 th-and 5 th-grade students using an ITS for fractions. The experiment further contrasted two forms of support for sense making: auto-linked representations and the use of worked examples involving one representation to guide work with another. Results confirm our main hypothesis: A combination of worked examples and fluency support lead to more robust learning than versions of the ITS without connection-making support. Therefore, combining different types of connection-making support is crucial in promoting students' <phrase>deep learning</phrase> from <phrase>multiple representations</phrase>.
Promoting Reflection and its Effect on Learning in a Programming Tutor We studied the effect of post-practice reflection on learning, using programming tutors, and <phrase>multiple-choice</phrase> format for reflection. We conducted in-vivo controlled studies with introductory programming students from multiple schools over 3 semesters, and used mixed-factor <phrase>ANOVA</phrase> to analyze the collected data. We found that reflecting on the concept underlying each problem neither promotes greater learning, measured as pre-post increase in the average score per problem, nor promotes faster learning, measured as the problems solved per concept learned. We conjecture that the benefits of reflecting on the concept underlying each problem may be limited if a tutor already promotes deep understanding of the domain. Problets We have been developing software tutors, called problets (www.problets.org) to help <phrase>students learn</phrase> C/C++/Java/C# <phrase>programming language</phrase> concepts by <phrase>solving problems</phrase>. To date, we have developed, evaluated and deployed problets on expression evaluation (arithmetic, relational, logical, assignment), selection statements, loops (while and for) and C++ pointers. The problets present programs to the learner, ask the learner a question about the program, such as predicting its output or identifying bugs in it, grade the student's answer, and provide delayed feedback. <phrase>Figure 1 shows</phrase> a snapshot of a problet on selection statements, with the program in the left panel and the feedback in the right panel. Problets generate problems as instances of parameterized problem templates. Each template is associated with a concept in the domain, e.g., some selection statement concepts include executing a selection statement when the condition is true/false, executing nested if statements, executing if-else statements nested in cascaded/classification style, and executing a program with multiple dependent/independent selection statements. Similarly, some loop concepts include nested dependent and independent loops, multiple dependent and independent loops, loops that iterate zero or one time, and loops that update the loop variables multiple times. Problets administer the pre-test-practice-<phrase>post-test</phrase> protocol: pre-test to evaluate the learner's knowledge; an adaptive practice on only the concepts that the learner has not yet mastered [1], followed by <phrase>post-test</phrase> on only the concepts that the learner has practiced. During the pre-and post-tests, problets do not provide any feedback. During practice, problets provide delayed feedback, which includes a <phrase>narrative</phrase> of the <phrase>step-by-step</phrase> execution of the program [2]. Problets use the <phrase>concept map</phrase> of the domain, enhanced with learning objectives, as the <phrase>overlay</phrase> student model [3]. Problets use reified interfaces [4] which promote the use of mental models when <phrase>solving problems</phrase>, e.g., the learner enters the output of the program …
An assessment of package-organisation misalignment: institutional and <phrase>ontological</phrase> structures research interests focus on <phrase>business process management</phrase> issues surrounding initiatives such as <phrase>business process</phrase> redesign, enterprise systems, and web-enabled industry <phrase>process integration</phrase>. Christina Soh has 15 years of experience in industry and government related research in the areas of IT-enabled business strategy, enterprise system implementation, inter-organizational systems, IT investment and business value, and national IT policy. ABSTRACT Even with today's 'best practice' software, commercial packages continue to pose significant alignment challenges for many organizations. This paper proposes a <phrase>conceptual framework</phrase>, based on institutional theory and systems ontology, to assess the misalignments between package functionality and organisational requirements. We suggest that these misalignments can arise from incompatibility in the externally imposed or voluntarily adopted structures embedded in the organisation and package, as well as differences in the way the meaning of organisational reality is <phrase>ontologically</phrase> represented in the deep or surface structure of packages. The synthesis of the institutional-<phrase>ontological</phrase> dimensions leads us to identify four types of misalignments with varying degrees of severity—imposed-deep, imposed-surface, voluntary-deep, and voluntary-surface—and to predict their likely resolution. We test the predictions using over 400 misalignments from package implementations at three different sites. The findings support the predictions: the majority of imposed-deep misalignments were resolved via package customisation. Imposed-surface and voluntary-deep misalignments were more often resolved via organisational adaptation and voluntary-surface misalignments were almost always resolved via organisational adaptation. The extent of project success also appeared to be influenced by the number of misalignments and the proportion of imposed-deep misalignments. We conclude by suggesting strategies that implementing organisations and package vendors may pursue.
Organizing structured <phrase>web sources</phrase> by <phrase>query schemas</phrase>: a clustering approach In the <phrase>recent years</phrase>, the Web has been rapidly "deepened" with the prevalence of databases online. On this <phrase>deep Web</phrase>, many sources are &#60;i>structured&#60;/i> by providing structured query interfaces and results. Organizing such structured sources into a domain hierarchy is one of the critical steps toward the integration of heterogeneous <phrase>Web sources</phrase>. We observe that, for structured <phrase>Web sources</phrase>, <phrase>query schemas</phrase> &#60;i>ie&#60;/i>, attributes in query interfaces) are discriminative representatives of the sources and thus can be exploited for source characterization. In particular, by viewing <phrase>query schemas</phrase> as a type of categorical data, we abstract the problem of source organization into the clustering of categorical data. Our approach hypothesizes that "homogeneous sources" are characterized by the same hidden <phrase>generative models</phrase> for their schemas. To find clusters governed by such statistical distributions, we propose a new <phrase>objective function</phrase>, &#60;i><phrase>model-differentiation</phrase>&#60;/i>, which employs principled <phrase>hypothesis testing</phrase> to maximize statistical heterogeneity among clusters. Our evaluation over hundreds of real sources indicates that (1) the schema-based clustering accurately organizes sources by object domains &#60;i>eg&#60;/i>, Books, Movies), and (2) on clustering Web <phrase>query schemas</phrase>, the <phrase>model-differentiation</phrase> function outperforms existing ones, such as likelihood, entropy, and context linkages, with the hierarchical agglomerative clustering algorithm.
Reduction of Crosstalk <phrase>Pessimism</phrase> using Tendency Graph Approach — Accurate estimation of <phrase>worst-case</phrase> <phrase>crosstalk effects</phrase> is critical for a realistic estimation of the worst-case behavior of <phrase>deep sub-micron</phrase> circuits. Crosstalk analysis models usually assume that the worst-case crosstalk occurs with all the aggressors of a victim (net or path) simultaneously inducing crosstalk even though this may not be possible at all. This overestimated crosstalk is called false noise. Logic correlations have been explored to reduce false noise in [3], which also used <phrase>branch and bound</phrase> method to solve the problem. In this paper, we propose a novel approach, named Tendency Graph Approach (<phrase>TGA</phrase>), which preprocesses the logic constraints of the circuit to drastically speed up the fundamental <phrase>branch and bound</phrase> algorithm. The new approach has been implemented in C++ and tested on an industrial circuit in a current 90 nm technology, demonstrating that <phrase>TGA</phrase> considerably accelerates the solution to the false noise problem, and makes in many cases <phrase>branch and bound</phrase> feasible in the first place.
Abstract Shear Behavior of <phrase>Reinforced Concrete</phrase> Deep Beams Strengthened with Cfrp Laminates Shear Behavior of <phrase>Reinforced Concrete</phrase> Deep Beams Strengthened with Cfrp Laminates Shear Behavior of <phrase>Reinforced Concrete</phrase> Deep Beams Strengthened with Cfrp Laminates The <phrase>copyright law</phrase> of the <phrase>United States</phrase> (Title 17, <phrase>United States Code</phrase>) governs the making of photocopies or other reproductions of <phrase>copyrighted</phrase> material. Under certain conditions specified in the law, libraries and <phrase>archives</phrase> are authorized to furnish a photocopy or other reproduction. One of these specified conditions is that the photocopy or reproduction is not to be " used for any purpose other than <phrase>private</phrase> study, <phrase>scholarship</phrase>, or research. " If a, user makes a request for, or later uses, a photocopy or reproduction for purposes in excess of " <phrase>fair</phrase> use " that user may be liable for <phrase>copyright infringement</phrase>, This institution reserves the right to refuse to accept a copying order if, in its judgment, fulfillment of the order would involve violation of <phrase>copyright law</phrase>. Please Note: The author retains the copyright while the New <phrase>Jersey</phrase> Institute of Technology reserves the right to distribute this thesis or dissertation <phrase>Printing</phrase> note: If you do not wish to print this page, then select " Pages from: first page # to: last page # " on the print dialog screen The Van Houten library has removed some of the <phrase>personal information</phrase> and all signatures from the approval page and biographical sketches of theses and dissertations in order to protect the identity of <phrase>NJIT</phrase> graduates and faculty. Considerable research has been performed using CFRP plates bonded to normal <phrase>reinforced concrete</phrase> beams, the objective being to improve shear and flexural strength. However, very few tests have been performed on <phrase>reinforced concrete</phrase> deep beams with CFRP as shear reinforcement. The purpose of this experiment is to investigate the behavior of the deep beam in shear with various arrangements of <phrase>Sika</phrase> CarboDur laminates bonded to the sides. The beams were designed to be shear deficient with stirrups omitted, except at the supports and directly under the load. In total eight beams were tested, four with one-point loading and four with two-point loading. Each loading condition contained one control beam without CFRP, and three beams with the CFRP laminates attached at zero, forty-five, and ninety degrees with respect to the neutral axis. The ultimate loading capacity and behavior for each beam bonded with the CFRP laminate was observed and recorded. The mode of failure was shear in all the beams tested, which typically resulted from the delaminating between the concrete and the <phrase>epoxy</phrase>. The present test results show that the orientation of the CFRP about the …
Reliability Evaluation of NDT Techniques for Cu-Welds for <phrase>Risk Assessment</phrase> of <phrase>Nuclear Waste</phrase> Encapsulation In order to handle the long living <phrase>radioactive</phrase> waste (<phrase>spent nuclear fuel</phrase>) SKB is planning to build a deep repository that requires no monitoring by future generations. The <phrase>spent nuclear fuel</phrase> will be encapsulated in copper canisters consisting of a <phrase>graphite</phrase> <phrase>cast iron</phrase> insert shielded by an outer 30-50 mm thick copper cylinder for <phrase>corrosion</phrase> protection. The most critical part of the encapsulation process is the sealing of the canister, which is done by welding the copper lid to the cylindrical part of the copper shell using – radiographic and <phrase>ultrasonic testing</phrase>-must be satisfactorily determined and combined to derive assumptions regarding the frequency of undetected welding defects for the ensemble of canisters as input for the <phrase>risk assessment</phrase>. This is done using the POD method according to the " Reliability Handbook MIL 1823 " and its generalization to more complex defect situations in welds. friction stir welding and electron beam welding. The quality of the welding process and the reliability of the NDT system
Built-in Self-test and Calibration of <phrase>Mixed-signal</phrase> Devices Wide adoption of <phrase>deep sub-micron</phrase> and nanoscale technologies in the modern <phrase>semiconductor industry</phrase> is resulted in very large complex <phrase>mixed-signal</phrase> devices. It has then become more difficult to estimate and control device parameters, which are now increasingly vulnerable to fabrication <phrase>process variations</phrase>. Conventional <phrase>design-for-test</phrase> (DFT) methods have been already well studied for digital circuitry to ensure verification of its functionality and <phrase>fault coverage</phrase>. Built-in self-test (BIST) approaches have been developed for <phrase>design automation</phrase> of digital ICs. However, such DFT techniques cannot be applied to analog and <phrase>mixed-signal</phrase> circuits directly. Therefore, new techniques must be employed to detect faults in analog components and to provide certain level of calibration capability to dynamically adjust the parameters of an analog device for better yield of chips. The most important ana-log devices in a <phrase>mixed-signal</phrase> system-on-chip (SoC) are <phrase>analog-to-digital converter</phrase> (ADC) and <phrase>digital-to-analog converter</phrase> (DAC). Such converters transfer data between <phrase>digital and analog</phrase> circuits and convert analog signals to digital bits or vice versa. In this research, novel <phrase>digital signal processor</phrase> (DSP)-based post-<phrase>fabrication process</phrase>-independent BIST approaches and variation tolerant design technique for ADC and DAC are studied. We use a sigma-delta modulation technique for measurement and a polynomial fitting algorithm for device calibration. In the proposed technique, a <phrase>digital signal processor</phrase> is programmed and used as test <phrase>pattern generator</phrase> (<phrase>TPG</phrase>), output response analyzer (ORA) and test control unit. The polynomial fitting algorithm characterizes the nonlinearity errors and the polynomial is used to generate compensating signals to reduce nonlinearity errors to ±0.5LSB. This ii technique can be applied to other digitally-controllable <phrase>mixed-signal</phrase> devices and a general test-characterization-calibration approach modeled after this work can be developed to detect , measure, and compensate nonlinearity errors caused by device parameter deviations. iii Acknowledgments
<phrase>Chinese Room Problem</phrase> <phrase>Chinese Room Problem</phrase> <phrase>Chinese Room Problem</phrase> <phrase>Chinese Room Problem</phrase> The success of machines over the last few decades in performing tasks that were seeming impossible for humans to perform led to the discussion that can machines be made intelligent. The argument was based on the fact that there was no understanding and the computer merely followed human etc. is not a new one. The question evokes deep programmed rules without any consciousness. The other side countered that an argument like that was arguable, since the results were as if produced by an intelligent being and had meaning, the computer has produced proof of intelligence. In this paper, we would analyze the arguments of both the sides and present a clearer picture of the capabilities of machine. We'll begin by explaining the <phrase>Turing test</phrase>, a criteria to test the intelligence of a machine and then move to discuss <phrase>Chinese room problem</phrase> and its implications. I will be highlighting the objections raised against these problems and my own answers to these arguments.
<phrase>Kalman-filter</phrase>-based Semi-codeless Tracking of Weak Dual-frequency Gps Signals main research interests are GPS software receiver development and GPS applications, <phrase>Kalman filter</phrase> and estimation, orbit and attitude determination of satellites. Mechanical and <phrase>Aerospace Engineering</phrase> from <phrase>Princeton University</phrase>. His research interests are in the areas of estimation and filtering, spacecraft attitude and orbit determination, and GPS technology and applications. ABSTRACT Extended <phrase>Kalman-Filter</phrase> (EKF) <phrase>based methods</phrase> have been developed for semi-codeless tracking of the P(Y) code on weak dual-frequency GPS signals. Optimal Kalman filtering methods are essential to maintain lock with a semi-codeless approach during <phrase>ionospheric</phrase> scintillations, when deep power fades can occur. EKF-based semi-codeless techniques use maximum a posteriori estimation techniques to estimate the unknown W code that gets <phrase>modulo</phrase>-2 added to the P code in order to provide anti-spoofing protection. These tracking techniques take advantage of the known W-bit chip timing, whose chipping rate is approximately 500 KHz. They also use a posteriori cost functions that involve-log[cosh()] terms, which allow seamless transition between various W-bit decision techniques. The algorithm estimates the P(Y) carrier amplitudes, accounts for signal dynamics in the optimization process, and optimally couples its code and carrier tracking loops. The algorithm has been successfully tested with <phrase>real data</phrase> under normal <phrase>signal strength</phrase> conditions.
Feasibility of Iddq Tests for Shorts in <phrase>Deep Submicron</phrase> Ics Quiescent <phrase>supply current</phrase>(IDDQ) in <phrase>deep submicron</phrase> ICs is derived by <phrase>circuit simulation</phrase> and feasibility of IDDQ tests is examined for short defects in ICs fabricated with 0.18µm <phrase>CMOS process</phrase>. The results show that IDDQ of each gate depends on input logic values and that shorts can be detected by <phrase>IDDQ testing</phrase> if some <phrase>process variations</phrase> are small.
<phrase>Deep belief</phrase> net learning in a <phrase>long-range</phrase> vision system for autonomous off-road driving — We present a learning-based approach for <phrase>long-range</phrase> vision that is able to accurately classify complex terrain at distances up to the horizon, thus allowing <phrase>high-level</phrase> strategic planning. A <phrase>deep belief network</phrase> is trained with unsupervised data and a reconstruction criterion to extract features from an input image, and the features are used to train a realtime classifier to predict traversability. The online supervision is given by a stereo module that provides robust labels for nearby areas up to 12 meters distant. The approach was developed and tested on the LAGR <phrase>mobile robot</phrase>.
<phrase>Feature Learning</phrase> in <phrase>Deep Neural Networks</phrase> - Studies on <phrase>Speech Recognition</phrase> Tasks <phrase>Recent studies</phrase> have shown that <phrase>deep neural networks</phrase> (DNNs) perform significantly better than shallow networks and Gaussian <phrase>mixture models</phrase> (GMMs) on large vocabulary <phrase>speech recognition</phrase> tasks. In this paper, we argue that the improved accuracy achieved by the DNNs is the result of their ability to extract dis-criminative internal representations that are robust to the many sources of variability in speech signals. We show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth, which leads to better <phrase>speech recognition</phrase> performance with deeper networks. We also show that DNNs cannot extrapolate to test samples that are substantially different from the training examples. If the training data are sufficiently representative, however, internal features learned by the DNN are relatively stable with respect to speaker differences, bandwidth differences, and environment distortion. This enables DNN-based recognizers to perform as well or better than state-of-the-art systems based on GMMs or shallow networks without the need for explicit model adaptation or feature normalization.
Statistical correlations and risk analyses techniques for a diving dual phase bubble model and data bank using massively parallel supercomputers Linking model and data, we detail the <phrase>LANL</phrase> diving reduced gradient bubble model (RGBM), dynamical principles, and correlation with data in the <phrase>LANL</phrase> Data Bank. Table, profile, and meter risks are obtained from likelihood analysis and quoted for air, <phrase>nitrox</phrase>, helitrox no-decompression time limits, repetitive dive tables, and selected <phrase>mixed gas</phrase> and repetitive profiles. Application analyses include the EXPLORER decompression meter algorithm, NAUI tables, University of <phrase>Wisconsin</phrase> <phrase>Seafood</phrase> <phrase>Diver</phrase> tables, comparative NAUI, PADI, Oceanic NDLs and repetitive dives, comparative <phrase>nitrogen</phrase> and <phrase>helium</phrase> <phrase>mixed gas</phrase> risks, <phrase>USS</phrase> Perry deep <phrase>rebreather</phrase> (RB) exploration dive,world record open circuit (<phrase>OC</phrase>) dive, and Woodville Karst Plain Project (WKPP) extreme <phrase>cave</phrase> exploration profiles. The algorithm has seen extensive and utilitarian application in <phrase>mixed gas</phrase> diving, both in recreational and technical sectors, and forms the bases forreleased tables and decompression meters used by scientific, commercial, and research divers. The <phrase>LANL</phrase> Data Bank is described, and the methods used to deduce risk are detailed. Risk functions for dissolved gas and bubbles are summarized. Parameters that can be used to estimate profile risk are tallied. To fit data, a modified Levenberg-Marquardt routine is employed with L2 error norm. Appendices sketch the <phrase>numerical methods</phrase>, and list reports from <phrase>field testing</phrase> for (real) <phrase>mixed gas</phrase> diving. A <phrase>Monte Carlo</phrase>-like sampling scheme for fast <phrase>numerical analysis</phrase> of the data is also detailed, as a coupled <phrase>variance reduction</phrase> technique and additional check on the canonical approach to estimating diving risk. The method suggests alternatives to the canonical approach. This work represents a first time correlation effort linking a dynamical bubble model with deep stop data. <phrase>Supercomputing</phrase> resources are requisite to connect model and data in application.
<phrase>Knowledge Representation</phrase> and Sense Disambiguation for Interrogatives in E-<phrase>HowNet</phrase> In order to train machines to 'understand' <phrase>natural language</phrase>, we propose a meaning representation mechanism called E-<phrase>HowNet</phrase> to encode lexical senses. In this paper, we take interrogatives as examples to demonstrate the mechanisms of semantic representation and composition of <phrase>interrogative</phrase> constructions under the framework of E-<phrase>HowNet</phrase>. We classify the <phrase>interrogative</phrase> words into five classes according to their query types, and represent each type of interrogatives with <phrase>fine-grained</phrase> features and operators. The process of semantic composition and the difficulties of representation, such as <phrase>word sense disambiguation</phrase>, are addressed. Finally, machine understanding is tested by showing how machines derive the same <phrase>deep semantic</phrase> structure for synonymous sentences with different surface structures.
Research Note Nlp Revisited: Nonverbal Communications and Signals of Trustworthiness A core principle of neurolinguistic programming (NLP) is that rapport and trust develop through synchronization of modes of communication between the sender and receiver. Nonverbal signals are a particularly important mode of communications in the NLP perspective. This study extends the NLP framework by incorporating findings from <phrase>neuroscience</phrase> into research about nonverbal signals and sensory representational systems. Three independent but related studies are used to identify nonverbal cues associated with the representational systems, to test if descriptions of these nonverbal signals influence trustworthiness assessments, and, finally, to test if these nonverbal signals trigger buyer's positive assessments of salesperson trust-building characteristics as well as trustworthiness. Sales trainers, academics, and sales books consistently advise salespeople to build trust with their clients and customers without providing much practical guidance on how to initiate this relationship. The frequently stated goal of this rapport building is to establish a relationship with deep-rooted trust (<phrase>Rousseau</phrase> et al. 1998), which, as a wealth of research demonstrates, leads to positive outcomes for the relationship. However, the actual initiating behavioral correlates of rapport building are without theoretical underpinnings and lack empirical support (Tickle-Degnen and Rosenthal 1990). Advocates of neurolinguistic programming (NLP)—an approach to human communications that combines cogni-tive theory, <phrase>split-brain</phrase> processing, and sensory perception— have long suggested that their approach holds the key to understanding rapport building. They propose that understanding rapport and trust begins with the investigation of communications as a process while ignoring any content in the message (Dimmick 1995). This perspective has led, in <phrase>marketing</phrase> studies of NLP, to a methodological emphasis upon the signals of the exemplar salesperson as a sender of messages and cues rather than a focus on the buyer as the receiver and interpreter of the signal (Dowlen 1996). Unfortunately, investigations based upon NLP have a history of inconclusive and contradictory <phrase>empirical evidence</phrase> (Dowlen 1996; Thompson, Courtney, and Dickson 2002). The inconclusive evidence of previous NLP studies may have resulted from the emphasis upon the sender/salesperson rather than the receiver/ customer. Findings from <phrase>recent research</phrase> suggests that buyers today continue to assess the trustworthiness of relationship partners based on their impressions from the initial face-to-face encounter (<phrase>Chamberlin</phrase> 2000; McKnight, Cummings, and Chervany 1998). Multiple studies in <phrase>neuroscience</phrase> suggest that these impressions are formed by perceptions of nonverbal cues (Puce et al. 2003; <phrase>Winston</phrase> et al. 2002). Using <phrase>functional magnetic resonance imaging</phrase> (fMRI) techniques, these researchers map the structures of the brain that …
Characterizing <phrase>Data Structures</phrase> for Volatile Forensics —<phrase>Volatile memory</phrase> forensic tools can extract valuable evidence from latent <phrase>data structures</phrase> present in memory dumps. However, current techniques are generally limited by a lack of understanding of the underlying data without the use of expert knowledge. In this paper, we characterize the nature of such evidence by using deep analysis techniques to better understand the life-cycle and recoverability of latent program data in memory. We have developed Cafegrind, a tool that can systematically build an object map and track the use of <phrase>data structures</phrase> as a program is running. Statistics collected by our tool can show which <phrase>data structures</phrase> are the most numerous, which structures are the most frequently accessed and provide summary statistics to guide forensic analysts in the evidence gathering process. As programs grow increasingly complex and numerous, the ability to pinpoint specific evidence in memory dumps will be increasingly helpful. Cafegrind has been tested on a number of <phrase>real-world applications</phrase> and we have shown that it can successfully map up to 96% of heap accesses.
The adoption of <phrase>information systems</phrase> in SMEs: organizational issues and success factors This research approaches the issues of introducing ICTs (Information and Communication Technologies) into <phrase>Small and Medium Enterprises</phrase>, with the aim of finding some conditions that make the organizational context able to manage the change process needed to really get the potential benefits of these technologies. The final objective is to propose a theoretical base for a methodology able to improve analytical and diagnostic capabilities of organizational realities before and during planning and evaluation phases of their <phrase>Information Technology</phrase> investments. The study is planned to go through two phases. The first one is aimed to look for reliable indicators to forecast readiness and adequacy of a specific organizational context towards the positive adoption of I.T. systems through a deep analysis of five significant cases. The second phase will test such indicators through questionnaire research on a larger sample of <phrase>Italian</phrase> SMEs.
Through a Glass Darkly: <phrase>Information Technology</phrase> Design, Identity Verification, and Knowledge Contribution in <phrase>Online Communities</phrase> A variety of <phrase>information technology</phrase> (IT) artifacts, such as those supporting <phrase>reputation management</phrase> and digital <phrase>archives</phrase> of past interactions, are commonly deployed to support <phrase>online communities</phrase>. Despite their ubiq-uity, theoretical and <phrase>empirical research</phrase> investigating the impact of such IT-based features on <phrase>online community</phrase> communication and interaction is limited. Drawing on the <phrase>social psychology</phrase> literature, we describe an <phrase>identity-based</phrase> view to understand how the use of IT-based features in <phrase>online communities</phrase> is associated with online knowledge contribution. Specifically, the use of four categories of IT artifacts—those supporting virtual co-presence, persistent labeling, self-presentation, and deep profiling—is proposed to enhance perceived identity verification, which thereafter promotes satisfaction and knowledge contribution. To test the theoretical model, we surveyed more than 650 members of two <phrase>online communities</phrase>. In addition to the positive effects of community IT artifacts on perceived identity verification, we also find that perceived identity verification is strongly linked to member satisfaction and knowledge contribution. This paper offers a new perspective on the mechanisms through which IT features facilitate computer-mediated knowledge sharing, and it yields important implications for the design of the supporting IT infrastructure. 3 4 months for 2 revisions.
<phrase>Packet scheduling</phrase> for <phrase>deep packet inspection</phrase> on <phrase>multi-core</phrase> architectures <phrase>Multi-core</phrase> architectures are commonly used for network applications because the workload is highly parallelizable. <phrase>Packet scheduling</phrase> is a critical performance component of these applications and significantly impacts how well they scale. <phrase>Deep packet inspection</phrase> (DPI) applications are more complex than most network applications. This makes <phrase>packet scheduling</phrase> more difficult, but it can have a larger impact on performance. Also, packet latency and ordering requirements differ depending on whether the DPI application is deployed inline. Therefore, different <phrase>packet scheduling</phrase> tradeoffs can be made based on the deployment.  In this paper, we evaluate three <phrase>packet scheduling</phrase> algorithms with the Protocol Analysis Module (PAM) as our DPI application using network traces acquired from production networks where intrusion prevention systems (IPS) are deployed. One of the <phrase>packet scheduling</phrase> algorithms we evaluate is commonly used in production applications; thus, it is useful for comparison. The other two are of our own design. Our results show that <phrase>packet scheduling</phrase> based on cache affinity is more important than trying to balance packets. More specifically, for the three network traces we tested, our cache affinity packet scheduler outperformed the other two schedulers increasing throughput by as much as 38%.
Fiber. Optic Reference <phrase>Frequency Distribution</phrase> to Remote Beam <phrase>Waveguide</phrase> Antennas* III the NASA/JPL <phrase>Deep Space Network</phrase> (DSN), radio science experiments (probing outer <phrase>planet</phrase> atmospheres, rings, <phrase>gravitational</phrase> waves, etc.) and very long-base interfcrometry (VL?H) require ultra–<phrase>sfa</phrase> ble, low <phrase>phase noise</phrase> rejerence frequency signals at the user locations. Ijpical locations Jor radio science/VLlll exciters and down-converters are the cone areas o~ the 34 m high efllciency antennas or the 70 m anfennas, located several hundred meters from the reference frequency standards. Over the past three years, <phrase>fiber optic</phrase> distribution links have replaced <phrase>coaxial cable</phrase> distribution for reference frequencies to these antenna sites. <phrase>Optical fibers</phrase> are the preferred medium for distribution because of their low <phrase>attenuation</phrase>, immunity to EM1/lWl, and temperature stability. A new network of Beam <phrase>Waveguide</phrase> (BWG) antennas p~esently under construction in the ])SN requires hydrogen <phrase>maser</phrase> stability at tens oj kilometers disfance jrom the jrequency standards central location. 'l'he topic oj this paper is the design and imphm~entation oj an optical jiber distribution link which provides ultra-stable rejerence frequencies to users at a remote IIWG antenna. 7'he temperature profile jrom the earth's surjace to a depth oj six jeet over a time period oj six mon(hs was used to optimize the pIacement oj the jiber optic cables. In-situ evaluation oj the <phrase>fiber optic</phrase> link performance indicates AUaR deviation on the order ojparts in JO-15 at ]000 and 10,000 seconds averaging time; thus, the link sfability degradation due to environmental conditions still preserves hydrogen <phrase>maser</phrase> stability at the user locations. l-his <phrase>paper reports</phrase> on the implementation oj optical ~[bers and elecfro-optic devices jor distributing very stable, low <phrase>phase noise</phrase> rejerence signals to remote BWG antenna locations. AUan deviation and <phrase>phase noise</phrase> <phrase>test results</phrase> jor a J 6 km <phrase>fiber optic</phrase> distribution link are presented in the paper.
Quantifiers vs. Quantification Theory The syntax and semantics of quantifiers is of crucial significance in current linguistic theorizing for more than one reason. The last statement of his grammatical theories by the late <phrase>Richard Montague</phrase> (1 973) is modestly entitled " The Proper Treatment of Quantification in Ordinary English ". In the authoritative statement of his " Generative Semantics " , <phrase>George Lakoff</phrase> (1971, especially pp. 238-267) uses as his first and foremost testing-ground the grammar of certain English quantifiers. In particular, they serve to illustrate, and show need of, his use of global constraints governing the derivation of English sentences. Evidence from the behavior of quantifiers (including numerical expressions 1) has likewise played a major role in recent discussions of such vital problems as the alleged meaningdpreservation of transformations , 2 co-reference, 3 the role of surface structure in semantical interpretation, and so on. In all these problems, the behavior of <phrase>natural-language</phrase> quantifiers is one of the <phrase>main issues</phrase>. Quantifiers have nevertheless entered the Methodenstreit of contemporary <phrase>linguistics</phrase> in another major way, too. (These two groups of problems are of course interrelated.) This is the idea that the structures studied iln the so-called quantification theory of <phrase>symbolic logic</phrase>-otherwise know as <phrase>first-order logic</phrase>, (lower) <phrase>predicate calculus</phrase>, or elementary logic-can serve and suffice 6 as semantical representations of English sentences. Views of this general type have been proposed by McCawley (1971)* and Lakoff (1972)' (among others). A related theory of " <phrase>Deep Structure</phrase> as <phrase>Logical Form</phrase> " has been put forward and defended by G. Harman (1972). Theories of this general type may be compared with the traditiomal idea that quantification 'theory can be viewed as an abstraction from the behavior
Towards Anti-<phrase>model-based Testing</phrase> <phrase>Software testing</phrase> refers to the dynamic verification of a system's behavior based on the observation of a selected set of controlled executions, or <phrase>test cases</phrase> [2]. While in <phrase>traditional approaches</phrase> <phrase>test cases</phrase> were selected aimed at covering the program <phrase>source code</phrase> [5], nowadays we can apply a variety of testing techniques all along the development process , by basing test selection on different <phrase>pre-code</phrase> artifacts, such as requirements, specifications and design models [2]. <phrase>Model-based testing</phrase> consists in deriving a suite of <phrase>test cases</phrase> from a model representing software behavior. Such a model can be generated from a <phrase>formal specification</phrase> or designed by software engineers through diagrammatic tools. In principle, the derivation of the <phrase>test cases</phrase> can be done automatically , and indeed several approaches have been recently proposed that do this starting from models in different languages, e.g., [9, 1, 6]. By executing the <phrase>model-based</phrase> <phrase>test cases</phrase>, the conformance of the implemented system to its specification can be validated. <phrase>Model-based testing</phrase> is certainly useful and effective; however, there can be several reasons why such an approach cannot be applied or is too expensive for deployment in a specific context. One generic barrier to a wide adoption of <phrase>model-based testing</phrase> is its inherent complexity, which requires a deep expertise in <phrase>formal methods</phrase>, even where tool support is available – as testified by <phrase>case studies</phrase> in the AGEDIS project [1]. Another obstacle is the difficulty in forcing the implementation to take a defined path as identified in the model derived <phrase>test sequences</phrase>. The latter are generally expressed at an abstract level, while the ex-ecutable <phrase>test cases</phrase> must be more concrete and more informative (e.g., [3]). Finally, one further counter-motivation to the practice of <phrase>model-based testing</phrase> can be the use of legacy systems or <phrase>COTS</phrase>, for which behavior models are not available. Considering in particular component-based <phrase>software development</phrase> , a system is generally obtained by assembling already existing components, for which we cannot a-priori assume that a specification or the <phrase>source code</phrase> are available. In such cases, <phrase>model-based testing</phrase> is not applicable, or would be too costly. We assume in fact that the system <phrase>assembler</phrase> has a high-level specification of the global architecture, but can only pose in practice very basic requirements on the behavior of the acquired components. This is the rationale for an " anti-<phrase>model-based testing</phrase> approach " as the one we outline in this paper. While <phrase>model-based testing</phrase> starts from …
<phrase>Heat Transfer</phrase> Analysis for a Winged Reentry Flight <phrase>Test Bed</phrase> In this paper we deal with the aero-heating analysis of a reentry flight demonstrator helpful to the <phrase>research activities</phrase> for the design and development of a possible winged Reusable <phrase>Launch Vehicle</phrase>. In fact, to reduce risks in the development of <phrase>next generation</phrase> reusable launch vehicles, as first step it is suitable to gain deep design knowledge by means of extensive numerical computations, in particular for the aero-thermal environment the vehicle has to withstand during reentry. The demonstrator under study is a reentry space glider, to be used both as Crew <phrase>Rescue</phrase> Vehicle and Crew Transfer Vehicle for the <phrase>International Space Station</phrase>. It is designed to have large atmospheric manoeuvring capability, to test the whole path from the orbit down to subsonic speeds and then to the landing on a conventional <phrase>runway</phrase>. Several <phrase>analysis tools</phrase> are integrated in the framework of the vehicle aerothermal design. Between the others, we used computational analyses to simulate aerothermodynamic flowfield around the spacecraft and <phrase>heat flux</phrase> distributions over the vehicle surfaces for the assessment of the vehicle Thermal Protection System design. <phrase>Heat flux</phrase> distributions, provided for equilibrium conditions of radiation at wall and thermal shield <phrase>emissivity</phrase> equal to 0.85, highlight that the vehicle thermal shield has to withstand with about 1500 [kW/m 2 ] and 400 [kW/m 2 ] at nose and wing <phrase>leading edge</phrase>, respectively. Therefore, the fast developing new generation of thermal protection materials, such as <phrase>Ultra High</phrase> Temperature Ceramics, are available candidate to built the thermal shield in the most solicited vehicle parts. On the other hand, away from spacecraft leading edges, due to the low angle of attack profile followed by the vehicle during descent, the <phrase>heat flux</phrase> is close to values attainable with conventional <phrase>heat shield</phrase>. Also, the paper shows that the flying <phrase>test bed</phrase> is able to validate <phrase>hypersonic</phrase> aerothermodynamic design database and passenger experiments, including thermal shield and hot structures, giving confidence that a full-scale development can successfully proceed. n) q & Q r r R ℜ Re S t T v Unit normal <phrase>Heat flux</phrase>, W/m 2
Parallel <phrase>Gradient Descent</phrase> for Multilayer <phrase>Feedforward Neural Networks</phrase> We present a parallel approach to classification using <phrase>neural networks</phrase> as the hypothesis class. <phrase>Neural networks</phrase> can have millions of parameters and learning the optimum value of all parameters from huge datasets in a serial implementation can be a very time consuming task. In this work, we have implemented parallel <phrase>gradient descent</phrase> to train multilayer <phrase>feedforward neural networks</phrase>. Specifically, we analyze two kinds of par-allelization techniques: (a) <phrase>parallel processing</phrase> of multiple training examples across several threads and (b) parallelizing matrix operations for a single training example. We have implemented a serial minibatch <phrase>gradient descent</phrase> algorithm, its parallel multithreaded version (using Pthread library in C++), a BLAS parallelized version and a <phrase>CUDA</phrase> implementation on a GPU. All implementations have been compared and analyzed for the speedup obtained across various network architectures and increasing <phrase>problem sizes</phrase>. We have performed our tests on the benchmark dataset: MNIST, and finally also compared our implementations with the corresponding implementations in the state-of-the-art <phrase>deep learning</phrase> library: <phrase>Theano</phrase>.
MACE: Model-inference-Assisted Concolic Exploration for Protocol and Vulnerability <phrase>Discovery Program</phrase> state-<phrase>space exploration</phrase> is central to software security , testing, and verification. In this paper, we propose a novel technique for <phrase>state-space</phrase> exploration of software that maintains an ongoing interaction with its environment. Our technique uses a combination of symbolic and concrete execution to build an abstract model of the analyzed application, in the form of a finite-state <phrase>automaton</phrase>, and uses the model to guide further state-<phrase>space exploration</phrase>. Through exploration, MACE further refines the abstract model. Using the abstract model as a scaffold, our technique wields more control over the search process. In particular: (1) shifting search to different parts of the <phrase>search-space</phrase> becomes easier, resulting in higher <phrase>code coverage</phrase>, and (2) the search is less likely to get stuck in small local state-<phrase>subspaces</phrase> (e.g., loops) irrelevant to the application's interaction with the environment. Preliminary experimental results show significant increases in the <phrase>code coverage</phrase> and exploration depth. Further, our approach found a number of new deep vulnerabilities.
Threshold Measurements and when the locking loop is closed. The <phrase>closed-loop</phrase> frequency fluctuations exhibit a root-mean-square (r.m.s.) frequency deviation of ,310 kHz over a 1-min integration time. The residual noise exhibits a <phrase>white noise</phrase> spectrum with an Allan variance of 3 £ 10 211 /t 1/2 for an averaging time of 1 s # t # 25.6 s, and is limited by the electronic noise of our detection system (PD3 in Fig. 4a). A straightforward improvement would be to use a saturable <phrase>absorption line</phrase> (which can have a linewidth as narrow as 1 MHz; ref. 13) as a reference. The improved <phrase>signal-to-noise ratio</phrase> in HC-<phrase>PCF</phrase> also makes <phrase>overtone</phrase> absorptions in the visible and near-infrared (for example, P11 of 12 C 2 H 2 at 790.703 nm) accessible to laser frequency <phrase>metrology</phrase>. To test for gas leakage, the pressure-dependent linewidth of the P9 <phrase>acetylene</phrase> absorption was monitored daily over two months. The results gave an average value of ,468 MHz with a standard deviation of 30 MHz, that is, the linewidth is consistent with the Doppler limited value and was constant within experimental error, indicating no measurable leakage of gas. In conclusion, all-fibre, ultra-compact, <phrase>high performance</phrase>, easy-to-use and unconditionally stable gas-laser devices have been reported. The commercial availability of a wide range of all-fibre components (for example, <phrase>lasers</phrase>, phase modulators, power attenua-tors, isolators, Bragg gratings and beam-splitters) makes complex systems easy to design and construct. It is now possible to imagine miniature laser–gas devices, occupying a tiny volume and containing minute amounts of gas, with everyday applications in fields such as the colour conversion of laser light (perhaps using a built-in diode <phrase>pump</phrase> laser), and the measurement and stabilization of laser frequency. The unique features of HC-PCFs make these gas–laser devices extremely efficient, and their impact in many laser-related fields is likely to be deep and lasting. A Methods Splicing procedure Fusion-splicing was carried out using a commercial filament-based splicer (Vytran FFS-2000-PM). In this splicer, the splicing region is continuously purged by <phrase>argon</phrase> gas to stop the filament burning. This prevents contamination of the splice by solid deposits and water <phrase>condensation</phrase>, and also prevents <phrase>combustion</phrase> of flammable gases such as hydrogen and <phrase>acetylene</phrase>. For high fill pressures, the splice could be consolidated using heat-curable glue. The energy threshold measurement for the generation of the first Stokes line S 00 (1) was carried out in the following manner. The input power was varied by …
Enhancing Learning through <phrase>Self-Explanation</phrase> <phrase>Self-explanation</phrase> is an effective teaching/learning strategy that has been used in several <phrase>intelligent tutoring systems</phrase> in the domains of Mathematics and Physics to facilitate <phrase>deep learning</phrase>. Since all these domains are well structured, the instructional material to self-explain can be clearly defined. We are interested in investigating whether <phrase>self-explanation</phrase> can be used in an open-ended domain. For this purpose, we enhanced <phrase>KERMIT</phrase>, an <phrase>intelligent tutoring system</phrase> that teaches conceptual <phrase>database design</phrase>. The resulting system, <phrase>KERMIT</phrase>-SE, supports <phrase>self-explanation</phrase> by engaging students in tutorial dialogues when their solutions are erroneous. We plan to conduct an evaluation in July 2002, to test the hypothesis that students will learn better with <phrase>KERMIT</phrase>-SE than without <phrase>self-explanation</phrase>.
Subthreshold Behaviour Modeling of Fgmos Transistors Using the Acm and the Bsim3v3 Models The modeling behaviour of FGMOS devices in the subthreshold region is investigated. Experimental and simulated <phrase>ID</phrase>-VCG, and gmlID vs. log(1D) data for the standard CMOS 0.6 pm process are compared. The BSIM3V3 and the Advanced Compact Model, ACM are adopted to model the behaviour of the FGMOS transistors in the subthreshold region. Two different DC simulation macromodels are used for the DC simulation of the floating gate devices. The performance of the two MOSFET models and of the two DC simulation macromodels in modeling the FGMOS transistor in the subthreshold region is discussed. I. INTRODUCTION Floating gate MOS transistors have been widely used as storage devices in structures such as EEPROMS and flash memories. They have also been used as nonvolatile information storage devices for analog applications[ 1-31. The model proposed in [4] considers FGMOS transistors as devices which consist of multiple control gates capacitively coupled into a floating gate, and thus the floating gate voltage is a weighted summation of the input voltages where <phrase>capacitive coupling</phrase> coefficients act as the weights. In <phrase>low voltage</phrase> analog circuits the MOSFET very often operates in the weak and moderate inversion regions. Subthreshold models for the FGMOS transistor are presented in [5,6]. Modeling of the deep submicron MOS transistor for <phrase>low-voltage</phrase>, <phrase>low-power</phrase> analog circuits is a difficult task. It depends on the adopted <phrase>MOSFET model</phrase>. A set of tests can be done to evaluate the performance of the <phrase>MOSFET model</phrase> [7]. Moreover the FGMOS transistor has a floating node, so a macromodel has to be developed for DC simulation purposes to avoid DC convergence problems. In this work the subthreshold behaviour modeling of the FGMOS transistor is investigated using two compact MOSFET models, the <phrase>Berkeley</phrase> BSIM3V3 [SI, and the ACM model [9].
<phrase>Arbitrage</phrase> Tests of Israel's <phrase>Currency Options</phrase> Markets <phrase>Arbitrage</phrase> Tests of Israel's <phrase>Currency Options</phrase> Markets This paper focuses on the <phrase>Israeli</phrase> <phrase>currency options</phrase> market, which includes <phrase>currency options</phrase> traded on the <phrase>Tel Aviv</phrase> <phrase>Stock Exchange</phrase> and (non-tradable) Bank of <phrase>Israel</phrase> <phrase>currency options</phrase>. The three aims of this study are: (a) To examine the <phrase>null hypothesis</phrase> that the <phrase>Israeli</phrase> <phrase>currency options</phrase> market is efficient, an issue that has not yet been thoroughly investigated. Ex-post tests of <phrase>arbitrage</phrase> and dominance conditions do not permit rejection of the <phrase>null hypothesis</phrase>, except for very-near-maturity, deep-in-the-money (ITM) options. (b) To test the validity of the Black and <phrase>Scholes</phrase> (B-S) model as a native option-pricing model for the case of an <phrase>exchange-rate</phrase> target zone. We find that although we cannot reject the weakly <phrase>efficient market hypothesis</phrase> (except for very-near-maturity deep-ITM options), we can reject the strongly efficient market and/or the B-S model validity hypotheses. The <phrase>banking</phrase> sector could have utilized <phrase>arbitrage</phrase> opportunities, notably for out-of-the-money, at-the-money, and far-from-maturity options, especially when employing inter-temporal weighted-average implied <phrase>standard deviation</phrase>. (c) To address the issue of the liquidity premium evaluation; this (rather surprisingly) is found to be negative for some options. This study extends <phrase>previous studies</phrase> of options by examining the efficiency of currency option markets and the validity of the Black and <phrase>Scholes</phrase> model under a target zone <phrase>exchange rate</phrase> regime. It also compares the performance of currency option trading on the exchange and over-the-counter for the same period.
Progress towards Physics-based Space <phrase>Weather Forecasting</phrase> with Exascale Computing Particle in cell simulations applied to <phrase>space weather</phrase> modelling represent an excellent application for code-sign efforts. PIC codes are simple and flexible with many variants addressing different physical conditions (e.g. explicit, implicit, hybrid, gyrokinetic, fluid) and different architectures (e.g. vector, parallel, GPU). It is relatively easy to consider radical changes and test them in a short time. For this reason, the project DEEP funded by the <phrase>European Commission</phrase> (www.deep-project.eu) and the Intel Exascience Lab (www.exascience.com) have used PIC as one of their target application for a codesign approach aiming at developing PIC methods for future exascale comupters. The present work focuses on the efforts within DEEP. The <phrase>starting point</phrase> is the iPic3D implicit PIC approach. Here we report on the analysis of code performance, on the use of <phrase>GPUs</phrase> and the new MICs (Intel Phi processors). We describe how the method can be rethought for hybrid architectures composed of MICs and CPUs (as in the new Deep <phrase>Supercomputer</phrase> in Juelich, as well as in others). The focus is on a codesign approach where <phrase>computer science</phrase> issues motivate modifications of the algorithms used while physics constraints what should be eventually achieved.
Multiple dynamic models for tracking the left ventricle of the heart from ultrasound data using particle filters and <phrase>deep learning</phrase> architectures The problem of automatic tracking and segmentation of the left ventricle (LV) of the heart from ultrasound images can be formulated with an algorithm that computes the expected segmentation value in the current time step given all previous and current observations using a filtering distribution. This filtering distribution depends on the observation and transition models, and since it is hard to compute the expected value using the whole parameter space of segmen-tations, one has to <phrase>resort</phrase> to <phrase>Monte Carlo</phrase> sampling techniques to compute the expected segmentation parameters. Generally, it is straightforward to compute probability values using the filtering distribution, but it is hard to sample from it, which indicates the need to use a proposal distribution to provide an easier sampling method. In order to be useful, this proposal distribution must be carefully designed to represent a reasonable approximation for the filtering distribution. In this paper, we introduce a new LV tracking and <phrase>segmentation algorithm</phrase> based on the method described above, where our contributions are focused on a new transition and observation models, and a new proposal distribution. Our tracking and <phrase>segmentation algorithm</phrase> achieves better overall results on a previously tested dataset used as a benchmark by the <phrase>current state</phrase>-of-the-art tracking algorithms of the left ventricle of the heart from ultrasound images.
An <phrase>Active Contour</phrase>-Based Atlas Registration Model Applied to Automatic <phrase>Subthalamic Nucleus</phrase> Targeting on MRI: Method and Validation This paper presents a new non parametric atlas registration framework, derived from the <phrase>optical flow</phrase> model and the <phrase>active contour</phrase> theory, applied to automatic <phrase>subthalamic nucleus</phrase> (STN) targeting in <phrase>deep brain stimulation</phrase> (DBS) surgery. In a previous work, we demonstrated that the STN position can be predicted based on the position of surrounding visible structures, namely the lateral and third ventricles. A STN targeting process can thus be obtained by registering these structures of interest between a <phrase>brain atlas</phrase> and the patient image. Here we aim to improve the results of the state of the art targeting methods and at the same time to reduce the computational time. Our simultaneous segmentation and registration model shows mean STN localization errors statistically similar to the most performing <phrase>registration algorithms</phrase> tested so far and to the targeting expert's variability. Moreover, the computational time of our registration method is much lower, which is a worthwhile improvement from a clinical <phrase>point of view</phrase>.
Deploying a <phrase>Wireless Sensor Network</phrase> in <phrase>Iceland</phrase> the Glacsweb Project A <phrase>wireless sensor network</phrase> deployment on a glacier in <phrase>Iceland</phrase> is described. The system uses <phrase>power management</phrase> as well as power harvesting to provide <phrase>long-term</phrase> environment sensing. Advances in <phrase>base station</phrase> and sensor node design as well as initial results are described. The Glacsweb project [1] aimed to study glacier dynamics through the use of <phrase>wireless sensor networks</phrase>. It replaced wired instruments which had previously been used with radio-linked subglacial probes which contained many sensors. The base of a glacier has a large controlling effect on a glacier's response to <phrase>climate change</phrase> and there is a growing need to study it in order to build better models of their behaviour. Several generations of systems were deployed in Briksdalsbreen, an outlet of the Jostedal icecap in <phrase>Norway</phrase>. As a multidisciplinary <phrase>research project</phrase> it involved people from many domains: electronics, <phrase>computer science</phrase>, <phrase>glaciology</phrase>, <phrase>electrical engineering</phrase>, <phrase>mechanical engineering</phrase> and GIS. Initial deployments had to solve the mechanical design of the probe cases and the unknown <phrase>radio communication</phrase> issues. The solutions involved craft as much as science and engineering but the key success has been to create data which had not existed before [2,3,4,5] while advancing our knowledge of <phrase>sensor network</phrase> deployments. Hot water drills are used in order to produce holes which reach the glacier bed. Most probes are placed 10-30cm under the ice while some are placed within the ice. Due to the relatively slowly changing environment the probe sense rate is normally set to once every four hours, although an adaptive sampling algorithm has been developed in the lab [6] which would optimise this sampling rate. Skalafellsjökull is a part of the large <phrase>Vatnajökull</phrase> icecap in <phrase>Iceland</phrase> and our site was chosen at (64°15'27.09"N, 15°50'37.68"W) around 800m <phrase>altitude</phrase> near an access road. Although there was no local internet connection there was a <phrase>mobile phone</phrase> signal which we used for the main internet link. The glacier is deep enough to test beyond 100m depth in the future (we used 60-80m). Due to the nature of the team and time available a few key topics were chosen for the developments for the <phrase>Iceland</phrase> deployment. One main area was to improve the basestation design through the use of a Gumstix processor. This would provide a better development environment and easier package management. The probes would maintain the PIC18 microcontroller but would gain an improved <phrase>power supply</phrase> and simplified code. In terms of sensors the …
Classification of layered tissue <phrase>phantoms</phrase> for detection of changes in <phrase>epithelial</phrase> tissue below the surface using a stochastic decomposition model for scattered signal This paper answers the question of whether it is possible to detect changes inside <phrase>epithelium</phrase> layered structures using a Stochastic Decomposition Method (SDM) [1, 2] that models the scattered light reflected from the layered structure over an area (2-D scan) <phrase>illuminated</phrase> by an optical sensor (fiber) emitting light at either one wavelength or with white light. Our technique correlates the differential changes in the reflected tissue texture with the morphological and physical changes that occur in the tissue occurring below the surface of the structure. This work has great potential in detecting changes in mucosal structures and may lead to enhanced <phrase>endoscopy</phrase> when the disease is developing to the below the surface and hence becoming hidden during <phrase>colonoscopy</phrase> or <phrase>endoscopic</phrase> examination. Tests are performed on layered tissue <phrase>phantoms</phrase> and the <phrase>results obtained</phrase> show great effectiveness of the model and method in picking up changes in the morphology of the layered tissue <phrase>phantoms</phrase> occurring below the surface (greater than 0.6mm deep). 1. INTRODUCTION Various optical techniques have been developed for early diagnosis of <phrase>epithelial</phrase> cancer. Detailed reviews of these available optical techniques are presented in the literature with analysis of their <phrase>advantages and disadvantages</phrase> [3]. The motivation behind using the optical <phrase>based techniques</phrase> is that for the cases of <phrase>malignancy</phrase> detected at an earlier stage in <phrase>colorectal cancer</phrase>, a 5-year survival in excess of 97% can occur [4]. In recent years considerable progress has been made to evaluate subsurface structures in biological tissues in vivo. <phrase>Confocal</phrase> laser endomicroscopy, which is the closest step towards virtual <phrase>histology</phrase> has lead to the evaluation of the whole mucosal layer with an infiltration depth up to 250 μm [5]. Likewise, <phrase>confocal</phrase> <phrase>fluorescence</phrase> endomicroscopy may have the capacity to reach to 15-100 μm depth of penetration [6]. While those techniques use florescent <phrase>dyes</phrase> to interrogate subsurface information, other researchers are focusing on extracting information from tissue layers at different depths using reflected light [7, 8] without the use of <phrase>dyes</phrase>. Thus, the ultimate goal is to detect changes in the sub-<phrase>epithelial</phrase> tissue since early cancer development might occur at this level, hence attention needs to be focused on detection of morphological changes below the surface [9]. In this paper we aim
XSIM: An Efficient Crosstalk Simulator for Analysis and Modelling of <phrase>Signal Integrity</phrase> Faults The paper presents an efficient crosstalk simulator tool " XSIM " and it's methodology developed by the <phrase>signal integrity</phrase> <phrase>fault modeling</phrase> and test <phrase>research group</phrase> of ITEM, University of <phrase>Bremen</phrase>, for analysis and modeling of <phrase>signal integrity</phrase> faults in <phrase>deep sub-micron</phrase> (DSM) chip. The tool can be used for analyzing the <phrase>crosstalk coupling</phrase> behavior in both defective and <phrase>defect-free</phrase> parallel interconnects. Using the XSIM tool one can also determine the critical values of various interconnect's parasitics and the critical values of crosstalk <phrase>coupling capacitance</phrase> and <phrase>resistive bridging</phrase> (i.e. mutual conductance, if any) beyond which device will most likely suffer from the <phrase>signal integrity</phrase> losses, whereas for lower coupling values device will continue to behave as crosstalk <phrase>fault tolerant</phrase>. The special features of this simulation tool are that it is based on an <phrase>indigenous</phrase> methodology implemented in C++ and Microsoft Foundation Classes (MFC) and can be run on any standard PC with <phrase>Windows 2000</phrase> or <phrase>Windows XP</phrase> operating system. The advantage of such implementation is that it provides a user-friendly <phrase>Graphical-User-Interface</phrase> (GUI) which not only makes the tool very easy-to-use but also flexible and at least 11 times faster than commercial circuit simulator like PSPICE and provides very accurate <phrase>simulation results</phrase> which are very close to the one obtained from latter.
Arabic QA4MRE <phrase>at CLEF</phrase> 2012: Arabic <phrase>Question Answering</phrase> for Machine Reading Evaluation This paper presents the work carried out at ANLP <phrase>Research Group</phrase> for the <phrase>CLEF</phrase>-QA4MRE 2012 competition. This year, the <phrase>Arabic language</phrase> was introduced for the first time on QA4MRE lab <phrase>at CLEF</phrase> whose intention was to ask questions which require a deep knowledge of individual short texts and in which systems were required to choose one answer from multiple answer choices, by analyzing the corresponding test document in conjunction with background collections. In our participation, we have proposed an approach which can answer questions with multiple answer choices from short Arabic texts. This approach is constituted essentially of shallow <phrase>information retrieval</phrase> methods. The evaluation results of the running submitted has given the following scores: accuracy calculated overall all questions is 0.19 (i.e., 31 correct questions answered correctly among 160), while overall c@1 measure is also 0.19. The overall <phrase>results obtained</phrase> are not enough satisfactory comparing to the top works realized last year in QA4MRE lab. But as a first step at the roadmap of the evolution of the QA to Machine Reading (MR) systems in <phrase>Arabic language</phrase> and with the lack of researches investigated in the MR and <phrase>deep knowledge</phrase> reasoning in <phrase>Arabic language</phrase>, it is an encouraging step. Our proposed approach with its shallow criterion has succeeded to obtain the goal fixed at the beginning which is: select answers to questions from short texts without required enough external knowledge and complex inference.
Gait assessment in <phrase>Parkinson's disease</phrase>: toward an ambulatory system for <phrase>long-term</phrase> monitoring An ambulatory <phrase>gait analysis</phrase> method using body-attached <phrase>gyroscopes</phrase> to estimate spatio-temporal parameters of gait has been proposed and validated against a reference system for normal and pathologic gait. Later, ten <phrase>Parkinson's disease</phrase> (PD) patients with <phrase>subthalamic nucleus</phrase> <phrase>deep brain stimulation</phrase> (<phrase>STN-DBS</phrase>) implantation participated in gait measurements using our device. They walked one to three times on <phrase>a 20</phrase>-m walkway. Patients did the test twice: once <phrase>STN-DBS</phrase> was ON and once 180 min after turning it OFF. A group of ten age-matched normal subjects were also measured as controls. For each gait cycle, spatio-temporal parameters such as stride length (SL), stride velocity (SV), stance (ST), double support (DS), and gait cycle time (GC) were calculated. We found that <phrase>PD patients</phrase> had significantly different gait parameters comparing to controls. They had 52% less SV, 60% less SL, and 40% longer GC. Also they had significantly longer ST and DS (11% and 59% more, respectively) than controls. <phrase>STN-DBS</phrase> significantly improved gait parameters. During the <phrase>stim</phrase> ON period, <phrase>PD patients</phrase> had 31% faster SV, 26% longer SL, 6% shorter ST, and 26% shorter DS. GC, however, was not significantly different. Some of the gait parameters had high correlation with Unified <phrase>Parkinson's Disease</phrase> Rating Scale (UPDRS) subscores including SL with a <phrase>significant correlation</phrase> (r = -0.90) with UPDRS gait subscore. We concluded that our method provides a simple yet effective way of ambulatory <phrase>gait analysis</phrase> in <phrase>PD patients</phrase> with results confirming those obtained from much more complex and expensive methods used in gait labs.
Xmach-1: a <phrase>Multi-user</phrase> Benchmark for Xml <phrase>Data Management</phrase> The specification of XMach-1 (XML <phrase>Data Management</phrase> benchmark, Version 1) was developed at the University of <phrase>Leipzig</phrase> in 2000 and published at the beginning of 2001 [BöRa01]. It was the first <phrase>XML database</phrase> benchmark. The benchmark defines a database of <phrase>XML documents</phrase> and a set of operations covering important characteristics of XML processing and querying. <phrase>Key features</phrase> of XMach-1 are scalability, multiuser simulation and the evaluation of the entire <phrase>data management</phrase> system. It has been sucessfully implemented for a variety of native <phrase>XML database</phrase> systems and XML-enabled relational and object-relational <phrase>DBMS</phrase>. The benchmark is based on a <phrase>web application</phrase> in order to model a typical use case of a <phrase>XML data</phrase> management system. The system architecture consists of four parts: the <phrase>XML database</phrase>, application servers, loaders to populate the database and <phrase>browser</phrase> clients. The application servers run a web (HTTP) server and other <phrase>middleware</phrase> components to support processing of the <phrase>XML documents</phrase> and to interact with the backend database. The <phrase>XML database</phrase> contains both <phrase>document-centric</phrase> and data-centric <phrase>XML documents</phrase>. The largest part is <phrase>document-centric</phrase> consisting of <phrase>semi-structured</phrase> documents with larger text portions such as books or essays. These documents are synthetically produced by a parameterizable generator. In order to achieve close-to-reality results when storing and querying text contents, text is generated from the 10,000 most frequent English words, using a distribution corresponding to <phrase>natural language</phrase> text. The documents varies in size (2-100 kB) as well as in structure (flat and deep element hierarchy). The second part of the database is a data-centric directory containing the metadata of the other documents such as document URL, name, insert-and update time. All data in this document is stored in attributes (no mixed content) and the order of element siblings is free. Compared to stuctured data in <phrase>relational databases</phrase> it shows some <phrase>semi-structured</phrase> properties such as variable path length using recursive elements or optional attributes. The database can be scaled by increasing the number of documents, e.g. from 1000 text-oriented documents to 10 million documents. The metadata document scales proportionally with the number of text documents. A distinctive feature of XMach-1 is that documents may be schema-less or schema-based and that we increase the number schemas with the number of documents. This allows us to test a database system's ability to cope with an increasing number of different element types and to test query execution across multiple schemas. Additionally the benchmark supports evaluating schema-less …
GeneTUC, GENIA and Google: <phrase>Natural Language Understanding</phrase> in <phrase>Molecular Biology</phrase> Literature With the increasing amount of biomedical literature, there is a need for automatic extraction of information to support biomedical researchers. GeneTUC has been developed to be able to read biological texts and answer questions about them afterwards. The <phrase>knowledge base</phrase> of the system is constructed by parsing <phrase>MEDLINE</phrase> abstracts or other online text strings retrieved by the Google API. When the system encounters words that are not in the dictionary, the Google API can be used to automatically determine the semantic class of the word and add it to the dictionary. The performance of the GeneTUC parser was tested and compared to the manually tagged GENIA corpus with EvalB, giving bracketing precision and recall scores of 70,6% and 53,9% respectively. GeneTUC was able to parse 60,2% of the sentences, and the POS-tagging accuracy was 86.0%. This is not as high as the best taggers and parsers available, but GeneTUC is also capable of doing deep reasoning, like anaphora resolution and <phrase>question answering</phrase>, which is not a part of the state-of-the-art parsers.
Salem's Secrets: a Case Study on <phrase>Hypothesis Testing</phrase> and <phrase>Data Analysis</phrase>* Part I—salem's Secrets Th ere was a chill in the courtroom that day—a chill colder than could be explained by the unbearable winter. It was a cold that started at the back of the neck and lodged deep in the spine. Something evil was afoot. Th e question was: to whom did that evil belong? " She killed Goodwife Betty's baby. She killed it with those evil eyes. I saw her staring, as in a <phrase>trance</phrase>, at Betty's <phrase>house</phrase> at sunset one evening last week. Th en her <phrase>cow</phrase> and her baby died. She also makes poisons in her <phrase>house</phrase>. When people won't take her <phrase>poison</phrase>, she sends her <phrase>spirit</phrase> to force them by choking them until they <phrase>swallow</phrase> it. I see her <phrase>spirit</phrase> here now. It is over near Abby. Oh Abby, Abby! Be careful Abby, she has pins and they are red hot! Stop her, she is pricking me! Help me, I am burning… Help me… " Th e courtroom hummed with whispers as the spectators watched two young girls, Elisabeth, the speaker, and Abby, her best friend, tear and swat at their <phrase>arms</phrase> and legs as if swarmed by invisible <phrase>bees</phrase>. Th eir contortions escalated into convulsive fi ts, which were so <phrase>grotesque</phrase> and violent that witnesses agreed they could not be manufactured. Soon, as if on cue, other girls from Elisabeth and Abby's <phrase>circle</phrase> of friends joined in. Th <phrase>e girls</phrase> collapsed in exhaustion. Dr. William Griggs, the <phrase>village</phrase> physician, examined the girls and, fi nding only bruised skin, made a diagnosis; " … the evil hand is upon them. Th ey are <phrase>bewitched</phrase>. " Hathorne, the <phrase>magistrate</phrase>, directed his attention to Sarah Good, the latest woman to be accused of <phrase>witchcraft</phrase> in <phrase>Salem</phrase> in , and in a powerful voice demanded, " Goodwife, why do you <phrase>torture</phrase> these girls so? " " Sir, I do not hurt them. " " Who do you employ then to do it? " " Scientifi c research can reduce <phrase>superstition</phrase> by encouraging people to think and survey things in terms of cause and eff <phrase>ect</phrase>. Certain it is that a conviction, akin to <phrase>religious</phrase> feeling, of the rationality or intelligibility of the world lies behind all scientifi c work of a <phrase>higher order</phrase>. " —<phrase>Albert Einstein</phrase>
Conae <phrase>Microwave Radiometer</phrase> (mwr) Counts to <phrase>Brightness Temperature</phrase> Algorithm This dissertation concerns the development of the <phrase>MicroWave Radiometer</phrase> (MWR) <phrase>brightness temperature</phrase> (<phrase>Tb</phrase>) algorithm and the associated algorithm validation using on-orbit MWR <phrase>Tb</phrase> measurements. Mission, a joint international science mission, between NASA and the <phrase>Argentine</phrase> Space Agency (Comision Nacional de Actividades Espaciales, CONAE). The MWR is a CONAE developed passive microwave instrument operating at 23.8 GHz (K-band) H-pol and 36.5 GHz (<phrase>Ka-band</phrase>) H-& V-pol designed to <phrase>complement</phrase> the Aquarius L-band radiometer/<phrase>scatterometer</phrase>, which is the prime sensor for measuring <phrase>sea surface</phrase> salinity (SSS). MWR measures the Earth's <phrase>brightness temperature</phrase> and retrieves simultaneous, spatially collocated, environmental measurements (surface <phrase>wind speed</phrase>, rain rate, <phrase>water vapor</phrase>, and <phrase>sea ice</phrase> concentration) to assist in the measurement of SSS. This dissertation research addressed several areas including development of: 1) a <phrase>signal processing</phrase> procedure for determining and correcting radiometer system non-linearity; 2) an empirical method to retrieve switch matrix loss coefficients during thermal-<phrase>vacuum</phrase> (T/V) radiometric calibration test; and 3) an antenna pattern correction (APC) algorithm using Inter-satellite radiometric cross-calibration of MWR with the WindSat satellite radiometer. The validation of the MWR counts-to-<phrase>Tb</phrase> algorithm was performed using two years of on-orbit data, which included special <phrase>deep space</phrase> calibration measurements and routine clear sky ocean/land measurements. iv ACKNOWLEDGMENTS
<phrase>Low-power</phrase> systems on chips (SOCs) <phrase>Low-power</phrase> Issues for SoCs by <phrase>Christian</phrase> Piguet, CSEM £. For innovative portable products, Systems on Chips (SoCs) containing several processors, memories and specialised modules are obviously required. Performances but also <phrase>low-power</phrase> are <phrase>main issues</phrase> in the design of such SoCs. Are these <phrase>low-power</phrase> SoCs only constructed with <phrase>low-power</phrase> processors, memories and logic blocks? If the latter are unavoidable , many other issues are quite important for <phrase>low-power</phrase> SoCs, such as the way to synchronise the communications between processors as well as test procedures, on-line testing, <phrase>software design</phrase> and <phrase>development tools</phrase>. This paper is a general framework for the design of <phrase>low-power</phrase> SoCs, starting from the system level to the architecture level, assuming that the SoC is mainly based on the re-use of <phrase>low-power</phrase> processors, memories and logic peripherals. £. SoCs with many processors, co-processors, memories and peripherals cannot be synchronised with a single master clock, due to larger and larger wire delays in deep sub-micron technologies. Several clocking schemes have been proposed, such as GALS (Globally Asynchronous Locally Synchronous) but also full asynchronous architectures. This paper will present the <phrase>advantages and disadvantages</phrase> of these SoC clocking strategies as well as the impacts on <phrase>low-power</phrase>. ¥. For embedded SoCs containing several processors, one has to write several pieces of software for each processor starting typically from a high-level specification using the C/C++ language. In order to tackle this problem, we propose to first transform the original specification by means of a systematic script of platform-independent <phrase>source code</phrase> transformations. That is illustrated by applying global loop transformation techniques to identify asynchronous <phrase>partitions</phrase> exhibiting little communication and high locality of access characteristics. In a second stage, we explore multiple–instruction multiple–data (<phrase>MIMD</phrase>) mapping onto a given (partly) predefined platform using advanced space– time analysis techniques to maintain low data transfer rates while <phrase>achieving high</phrase> system throughput. At the SoC level, accurate cost feedback including <phrase>high-level</phrase> power estimation is required. From this essential information, energy <phrase>trade-offs</phrase> between application sub-modules can for example be used to refine the solution further. In the case of mapping onto programmable cores with a shared <phrase>memory hierarchy</phrase> , a final refinement consists in reorganising the data layout for efficient cache utilisation.ward <phrase>embedded Systems</phrase> on Chip is quite clear. Systems on Chip (SoCs) consist to integrate several components on the same chip in order to improve performances and reduce the cost. Few years ago, a system was a multichip device containing …
Usability Studies of WWW Sites: Heuristic Evaluation vs. Laboratory Testing This paper describes the strengths and weaknesses of two usability assessment methods frequently applied to <phrase>web sites</phrase>. It uses <phrase>case histories</phrase> of WWW usability studies conducted by the authors to illustrate issues of special interest to designers of <phrase>web sites</phrase>. The discussion not only compares the two methods, but also discusses how an effective usability process can combine them, applying the methods at different times during site development. The two methods discussed in this paper for assessing the usability of <phrase>web sites</phrase> both require the usability specialist to have three vital pieces of background information: the purpose of the <phrase>web site</phrase>; profiles of its intended users; and typical scenarios for users accessing the site. These elements are equally important in evaluating the usability of any product or service, but here is how they apply especially to <phrase>Web site</phrase> evaluation. • When discussing the purpose of a <phrase>web site</phrase>, it's helpful to consider three categories. <phrase>Web sites</phrase> that supply descriptions of companies (or other organizations) and their products, services, informational offerings, or events can be described as informational sites. <phrase>Web sites</phrase> that provide explicit links to extensive databases are called search sites. <phrase>Web sites</phrase> that behave like products, where users perform other tasks in addition to reading or retrieving information, are referred to as transactional sites. Multipurpose sites blur these boundaries. • Unfortunately, the definition of user in our increasingly Web-centric environment is becoming more vague, because " anyone can access the site. " However, we must keep in mind which site visitors are the most likely—or the most welcome—and focus usability efforts on those subgroups. • Finally, the scenario for accessing a <phrase>web site</phrase> might be a straightforward URL to a <phrase>home page</phrase>, or a more <phrase>roundabout</phrase> path through a link in <phrase>search-engine</phrase> results to a page deep in the bowels of a site. Evaluators should keep in mind that any web page might be the user's door to that <phrase>web site</phrase>. Although users may perform more complex tasks in transactional sites, the free-form nature of navigation in any type of <phrase>web site</phrase> makes ensuring (and measuring) success more complex in the Web environment.
Player Modeling, <phrase>Search Algorithms</phrase> and Strategies in Multi-player Games For a <phrase>long period</phrase> of time, two person zero-sum games have been in the focus of researchers of various communities. The efforts were mainly driven by the fascination of special competitions like <phrase>Deep Blue</phrase> vs. Kasparov, and of the beauty of parlor games like Checkers, <phrase>Backgammon</phrase>, Othello and Go. Multi-player games, however, have been inspected by far less, and although literature of <phrase>game theory</phrase> fills books about equilibrium-strategies in such games, practical experiences are rare. Korf, Sturtevant and a few others started highly interesting <phrase>research activities</phrase>. We have started investigating a four-person chess variant, in order to understand the peculiarities of multi player games without chance components. In this contribution, we present player models and <phrase>search algorithms</phrase> that we tested in the four-player chess world. As a result, we can say that the more successful player models can benefit from more efficient algorithms and speed, because searching deeper leads to better results. Moreover, we present a meta-strategy, which beats a paranoid alphabeta player, the best known player in multi player games.
The Characterizing <phrase>Substrate Coupling</phrase> in <phrase>Deep-submicron Designs</phrase> <phrase>Substrate Coupling</phrase> Ieee Design & Test of Computers INDUSTRY TREND of integrating <phrase>higher levels</phrase> of circuit functionality in chips designed for compact consumer electronic products and the widespread growth of <phrase>wireless communications</phrase> have triggered the proliferation of mixed analog-digital systems. <phrase>Single-chip</phrase> designs combining <phrase>digital and analog</phrase> blocks built over a common substrate feature reduced levels of power dissi-pation, smaller package counts, and smaller package interconnect parasitics. Designing such systems, however, is becoming increasingly difficult owing to coupling problems resulting from the combined requirements for <phrase>high-speed</phrase> digital and <phrase>high-precision</phrase> analog components. Noise coupling caused by the common chip substrate's nonideal isolation contributes significantly to the coupling problem in <phrase>mixed-signal</phrase> designs. 1,2 Fast-switching logic components inject current into the substrate, causing voltage fluctuation. Because substrate bias strongly affects the transistor <phrase>threshold voltage</phrase>, voltage fluctuations can affect the operation of sensitive analog circuitry through the body effect. Figure 1a illustrates this coupling mechanism, in which a switching digital node injects current into the substrate (currents J 1 and J 2 are drawn to ground, but J 2 affects the analog transistor bulk potential), causing the local substrate potential V b to vary at an analog node. Figure 1b illustrates this interaction from the circuit viewpoint. Other known mechanisms for current injection into the substrate include hot-carrier injection and parasitic bipolar transistors. 2 The effects of <phrase>substrate coupling</phrase> largely depend on the layout specifics. Therefore, accurate analysis of these effects is possible only after extraction of the circuit features and the parasitics. As technology and <phrase>circuit design</phrase> advance, substrate noise is beginning to <phrase>plague</phrase> even fully <phrase>digital circuits</phrase>. In these circuits, the cumulative effect of thousands or millions of <phrase>logic gates</phrase> changing state across the chip causes current pulses that are injected and absorbed into the substrate. Those currents are then transmitted to power and ground buses through direct feed-through and load charge and discharge. Such couplings are highly destructive because puls-ing currents, partially injected into the substrate through impact <phrase>ionization</phrase> and <phrase>capacitive coupling</phrase> , can be <phrase>broadcast</phrase> over great distances and picked up by sensitive circuits through <phrase>capacitive coupling</phrase> and the body effect. The resulting <phrase>threshold voltage</phrase> modulation dynam-4 The <phrase>accurate modeling</phrase> of noise-coupling effects caused by crosstalk through the substrate is an <phrase>increasingly important</phrase> concern for design and verification of analog, digital, and mixed systems. With the technique described here, designers can efficiently extract accurate <phrase>substrate-coupling</phrase> parameters from <phrase>deep-submicron designs</phrase>. ically changes gate delays locally, affecting performance unpredictably. Switching noise …
<phrase>Cyborg</phrase> Systems as Platforms for <phrase>Computer-Vision</phrase> Algorithm-Development for <phrase>Astrobiology</phrase> Employing the allegorical imagery from the film " The Matrix " , we motivate and discuss our '<phrase>Cyborg</phrase> Astro-<phrase>biologist</phrase>' research program. In this research program, we are using a wearable computer and video <phrase>camcorder</phrase> in order to test and train a <phrase>computer-vision</phrase> system to be a field-<phrase>geologist</phrase> and field-astrobiologist. 1 Matrix <phrase>Preamble</phrase> We choose to use the allegorical symbolism from the film, The Matrix (Wachowski & Wachowski, 1999). (The speaker emphasized the bold-faced words, and spoke very slowly and with a deep, precise voice, like the Matrix character, Morpheus, did.) You also have a choice. You may choose between the blue pill and the red pill (see Figure 1). Fig. 1. You have a choice between the blue pill on the right and the red pill on the left. (This picture and all the following Matrix pictures are from the original Matrix film (Wachowski & Wachowski, 1999).) If you should choose the blue pill, the next robots that you send to Mars to search for life, will have no or little A.I. (<phrase>Artificial Intelligence</phrase>) in them. All the science that these robots will do will be done by astrobiologists and <phrase>geologists</phrase> such as yourselves, here on the Earth, after the data has been telemetried by the robots from Mars back to Earth. If you should choose the blue pill, all of the intelligence will remain here on the Earth (see Figure 2). However, if you should choose the red pill, these robots, bound for Mars, will have on-board, scientific, astrobiolog-ical <phrase>Artificial Intelligence</phrase>. This A.I. will allow you and the robots, together, to accomplish much more science, especially given the several minutes of delay for delivery of commands from the Earth or for delivery of data from Mars (see Figure 2). Fig. 2. When we explore Mars, you have a choice between keeping all the <phrase>Geologist</phrase> intelligence in human form here on the <phrase>blue planet</phrase>, the Earth (on the right), and sending significant <phrase>Geologist</phrase> intelligence in robotic form to the <phrase>red planet</phrase>, Mars (on the left). If you should choose the red pill, some of our intelligence will be bound for Mars... In the Mars Exploration Workshop in <phrase>Madrid</phrase>, we demonstrated to you some of the early capabilities of our '<phrase>Cyborg</phrase>'
Automatic <phrase>test generation</phrase> for linear digital systems with bi-level search using matrix transform methods Linear state variable digital systems, commonly implemented in bit-serial architecture using silicon compilers , are difficult to test for manufacturing defects due to deep sequentiality, low controllability and observability, and high latency. A novel hierarchical testing approach, based on matrix manipulation and constrained <phrase>low-level</phrase> <phrase>test generation</phrase> , is reported here. FEAST (Functional Extractor and Sequential Test generator) operates at the high level, where the circuit is described as an interconnection of arithmetic modules. CREST (Constrained Sequential Test generator) operates at the <phrase>low level</phrase> description of the individual modules, and generates <phrase>test sets</phrase> satisfying constraints imposed by the high-level modules and their interconnection structure. The new approach was found to perform better when compared to automatic <phrase>test generation</phrase> at the gate level using existing algorithms for several large circuits.
Model Formulation: Multimethod Evaluation of Information and Communication Technologies in Health in the Context of Wicked Problems and Sociotechnical Theory OBJECTIVE Few research designs look at the <phrase>deep structure</phrase> of complex social systems. We report the design and implementation of a multimethod evaluation model to assess the impact of computerized order entry systems on both the technical and social systems within a <phrase>health care</phrase> organization.   DESIGN We designed a multimethod evaluation model informed by sociotechnical theory and an appreciation of the nature of wicked problems. We mobilized this model to assess the impact of an electronic medication management system via a three-year program of research at a major academic hospital.   MEASUREMENTS Model components include measurements relating to three dimensions of system impact: safety and quality, <phrase>organizational culture</phrase>, and work and communication patterns.   RESULTS Application of the evaluation model required the <phrase>development and testing</phrase> of purpose-built measurement tools such as software to collect multidimensional work measurement data. The model applied established <phrase>research methods</phrase> including medication error audits and <phrase>social network analysis</phrase>. Design features of these tools and techniques are described, along with the practical challenges of their implementation. The <phrase>distinctiveness</phrase> of doing research within a unique paradigm of complex systems, explicating the wickedness and the dimensionality of sociotechnical theory, is articulated.   CONCLUSION Designing an effective evaluation model requires a deep understanding of the nature and complexity of the problems that <phrase>information technology</phrase> interventions in <phrase>health care</phrase> are trying to address. Adopting a sociotechnical perspective for model generation improves our ability to develop evaluation models that are adaptive and sensitive to the characteristics of wicked problems and provides a strong theoretical basis from which to analyze and interpret findings.
Bonding to Er-<phrase>YAG-laser</phrase>-treated dentin. Er-<phrase>YAG laser</phrase> irradiation has been claimed to improve the adhesive properties of dentin. We tested the hypothesis that dentin adhesion is affected by Er-<phrase>YAG laser</phrase> conditioning. Superficial or deep dentin from human molars was: (a) acid-etched with 35% H3PO4; (b) irradiated with an Er-<phrase>YAG laser</phrase> (KaVo) at 2 Hz and 180 mJ, with <phrase>water-cooling</phrase>; and (c) laser- and acid-etched. <phrase>Single Bond</phrase> (3M ESPE) and <phrase>Z100</phrase> composite (3M ESPE) were bonded to the prepared surfaces. After storage, specimens were tested in shear to failure. Bonded interfaces were demineralized in <phrase>EDTA</phrase> and processed for <phrase>transmission electron microscopy</phrase>. Two-way <phrase>ANOVA</phrase> revealed that conditioning treatment and interaction between treatment and dentin depth significantly influenced shear <phrase>bond strength</phrase> results. Acid-etching alone yielded shear <phrase>bond strength</phrase> values that were <phrase>significantly higher</phrase> than those achieved with <phrase>laser ablation</phrase> alone, or in combination with acid-etching. The Er-<phrase>YAG laser</phrase> created a laser-modified layer that adversely affects adhesion to dentin, so it does not constitute an alternative bonding strategy to conventional acid etching.
3d <phrase>Motion Planning</phrase> for Steerable Needles Using Path Sets INTRODUCTION Bevel-tipped flexible needles can be steered in <phrase>soft tissue</phrase> to clinical targets along curved paths in 3D while avoiding critical structures. Duty-cycled rotation [1] during insertion allows for control of the curvature of the needle. These capabilities of 3D steerable needles make it potentially suitable for applications such as <phrase>deep brain stimulation</phrase> (DBS) and <phrase>drug delivery</phrase> to brain tumors [2]. Manually guiding a steerable needle is unintuitive due to the <phrase>high-dimensional</phrase> nature of the problem. <phrase>Planning algorithms</phrase> can help provide a feasible motion plan for steering the needle between the start and target locations while avoiding critical structures in the tissue. Imaging feedback is incorporated to control the needle along the pre-planned path. In this paper, we present an algorithm for <phrase>path planning</phrase> of steerable needles based on A* graph search on path sets in 3D. Path sets have been utilized extensively, in the past, for <phrase>motion planning</phrase> of <phrase>mobile robots</phrase> with nonholonomic constraints [3]. The formulation using path sets allows the planner to ouput paths that respect the differential or nonholonomic constraints associated with needle steering. Compared to more commonly used sampling-<phrase>based approaches</phrase> such as RRT [4,5,6], an A* search-<phrase>based approach</phrase> can guarantee optimality of the path with respect to a <phrase>pre-defined</phrase> cost function. Since safety is of <phrase>paramount</phrase> importance in our application, we consider two factors in our path cost that influence safety: a) path length and b) curvature. Critical structures are avoided by treating them as obstacles. MATERIALS AND METHODS <phrase>Figure 1 shows</phrase> the simulation environment used for evaluating our <phrase>path planning</phrase> algorithm. To test our <phrase>path planning</phrase> algorithm we use the following DBS application scenario in simulation. The needle enters near Kocher's point on the cortical surface (2.5 cm off the midline at the level of the <phrase>coronal</phrase> <phrase>suture</phrase>) and must reach the subthalamic nucleus. Critical anatomical obstacles that must be avoided include the corticospinal tracts, <phrase>basal ganglia</phrase>, and thalamus. These are approximated by combinations of simple geometrical shapes. Algorithm: The algorithm utilizes the needle steering motion model
Automatic Indexing for <phrase>Content Analysis</phrase> of Whale Recordings and XML Representation This paper focuses on the robust indexing of <phrase>sperm whale</phrase> <phrase>hydrophone</phrase> recordings based on a set of features extracted from a real-time passive underwater acoustic tracking algorithm for multiple whales using four hydrophones. Acoustic localization permits the study of whale behavior in <phrase>deep water</phrase> without interfering with the environment. Given the position coordinates, we are able to generate different features such as the speed, energy of the clicks, Inter-Click-Interval (<phrase>ICI</phrase>), and so on. These features allow to construct different markers which allow us to index and structure the audio files. Thus, the behavior study is facilitated by choosing and accessing the corresponding index in the audio file. The complete indexing algorithm is processed on <phrase>real data</phrase> from the NUWC (<phrase>Naval Undersea Warfare Center</phrase> of the US <phrase>Navy</phrase>) and the AUTEC (Atlantic Undersea Test & Evaluation Center-<phrase>Bahamas</phrase>). Our model is validated by similar results from the US <phrase>Navy</phrase> (NUWC) and SOEST (School of Ocean and <phrase>Earth Science</phrase> and Technology) <phrase>Hawaii</phrase> university labs in a single whale case. Finally, as an <phrase>illustration</phrase>, we index a single whale sound file using the extracted whale's features provided by the tracking, and we present an example of an XML script structuring it.
Title of Dissertation: Compatibility Testing for Component- Based Systems Compatibility Testing for Component-based Systems Many component-based systems are deployed in diverse environments, each with different components and with different component versions. To ensure the system builds correctly for all deployable combinations (or, configurations), developers often perform compatibility testing by building their systems on various configurations. However, due to the large number of possible configurations, testing all configurations is often infeasible, and in practice, only a handful of popular configurations are tested; as a result, errors can escape to the field. This problem is compounded when components evolve over time and when test resources are limited. To address these problems, in this dissertation I introduce a process, algorithms and a tool called Rachet. First, I describe a formal modeling scheme for capturing the system <phrase>configuration space</phrase>, and a sampling criterion that determines the portion of the space to test. I describe an algorithm to sample configurations satisfying the sampling criterion and methods to test the sampled configurations. Second, I present an approach that incrementally tests compatibility between components, so as to accommodate component evolution. I describe methods to compute test obligations, and algorithms to produce configurations that test the obligations, attempting to reuse test artifacts. Third, I present an approach that prioritizes and tests configurations based on developers' preferences. Configurations are tested, by default starting from the most preferred one as requested by a developer, but cost-related factors are also considered to reduce overall testing time. The testing approaches presented are applied to two <phrase>large-scale</phrase> systems in the <phrase>high-performance computing</phrase> domain, and experimental results show that the approaches can (1) identify compatibility between components effectively and efficiently , (2) make the process of compatibility testing more practical under constant component evolution, and also (3) help developers achieve preferred compatibility results early in the overall <phrase>testing process</phrase> when time and resources are limited. 2010 DEDICATION To my wife – Heejong for her endless love and support. ii ACKNOWLEDGEMENTS It is my honor to know so many good people who helped me to complete this dissertation. First and foremost, I deeply thank my advisor, Prof. Alan Sussman. He guided me to set up my research direction and helped me with his deep insight to realize my rough ideas into concrete artifacts. He has always been generous to me and also encouraged me to be confident. I would also like to thank Prof. <phrase>Adam</phrase> <phrase>Porter</phrase> and Prof. Atif Memon. Throughout discussions with them, I could get many ideas …
<phrase>Seachange</phrase>: design of online quiz questions to foster <phrase>deep learning</phrase> The design of different types of quiz question will influence the extent to which formative and summative feedback is presented to students. Typically, quiz questions are considered limited in their capacity to assess <phrase>higher order</phrase> cognitive skills. This paper extends the notion of online quiz design by presenting examples in a <phrase>WebCT</phrase> <phrase>learning environment</phrase> in order to demonstrate a formative approach to assessment, closely integrated with learning processes. A matrix of questions is presented using Bloom's taxonomy showing the type of question, pedagogical underpinnings and cognitive skills required. The implications of this type of question design is that automated quiz type questions do not necessarily imply a narrow focus on recall, but may assess a range of learning processes. Limits of traditional assessment Educators can be in no doubt of the demands of society for lifelong capable learners who are able to perform cognitive, metacognitive and metacognitive tasks and demonstrate competencies such as <phrase>problem solving</phrase>, <phrase>critical thinking</phrase>, questioning, searching for information, making judgments and evaluating information (Reeves, 2000). Assessment processes are now in the <phrase>limelight</phrase>, with increasing emphasis placed not on testing discrete skills or on measuring what people know, but on fostering learning and transfer of knowledge. The traditional approach to assessment is largely a form of objective testing which tends to value students' capacity to memorize facts and then recall them during a test situation. Magone et al (1994) calls this the one right answer mentality. A second form of assessment is the measurement of competencies, or what we call 'sequestered <phrase>problem solving</phrase>' (Schwartz et al, 2000). In these contexts students are asked to solve problems in isolation and without the resources that are typically available in the <phrase>real world</phrase> such as texts, Web-resources and peers. Often these tests of aptitude are single <phrase>shot</phrase>, and summative rather than formative. In contrast, assessment that supports learning and <phrase>knowledge transfer</phrase> provides the basis for future learning, and continuing motivation to learn. This approach is sometimes called the alternative assessment movement, as it is concerned with assessing performance (Cumming & Maxwell, 1999). Both testing and measuring competence as forms of assessment have been critiqued as being controlling, limiting and contrary to student-centered <phrase>teaching and learning</phrase>. Other indicators of the need to rethink online and <phrase>off-line</phrase> assessment have come from Bull & McKenna (2000) who argue that "the development and integration of <phrase>computer-aided</phrase> assessment has been done in an <phrase>ad hoc</phrase> manner". …
<phrase>Data Analysis</phrase> Knowledge among Preservice Elementary Education Teachers  This study investigated whether elementary education <phrase>majors</phrase> in the teacher education program at <phrase>Montana State University</phrase> (<phrase>MSU</phrase>) acquire and retain knowledge of statistical <phrase>data analysis</phrase> concepts and skills consistent with expectations specified in the NCTM "Principles and Standards for School Mathematics" (2000). The following statistical topics were covered: Finding, describing and interpreting mean, median and mode; interpreting the spread of a set of data; understanding the meaning of the shape and features of a graph; comparing centers, spreads, and graphical representations of related <phrase>data sets</phrase>; and using scatter plots and lines of best fit. PURPOSE The purpose of this study was to answer the question "To what degree do university students acquire and retain the statistical <phrase>data analysis</phrase> content required for elementary and <phrase>middle school</phrase> as defined by the National Council of Teachers of Mathematics?" BACKGROUND Today's candidate teachers who will become instructors in grades 5-8 often have the same mathematics background as those who will become teachers in grade K-4, yet they are expected to teach more complex content. The additional challenges inherent in the more ambitious curricular material often require them to be more like mathematics specialists than their original training may have prepared them to be. Furthermore, these teachers often have had little exposure to some of the mathematical ideas that ambitious curricula will require them to teach. Teaching mathematics and statistics in ways that make it understandable by students requires deep, flexible knowledge on the part of the teacher. " Students need to know about <phrase>data analysis</phrase> and related aspects of probability in order to reason statistically – skills necessary to becoming informed citizens and intelligent consumers " (National Council of Teachers of Mathematics, 2000 p. 48). To be effective, teachers must know their subject matter so thoroughly that they can present it in a challenging, clear, and compelling way (NCATE, 1998). The National Council of Teachers of Mathematics (NCTM) offers detailed recommendations for teaching statistics in grades K-12 in both Curriculum and Evaluation Standards for School Mathematics (1989) and Principles and Standards for School Mathematics (2000). <phrase>DATA COLLECTION</phrase> Since the <phrase>MSU</phrase> Elementary Teacher Education Program is stable in content and format from year to year, student (n=232) characteristics were sampled simultaneously at four different stages of the Teacher Education Program. Students were given a 38-item paper/pencil test that examined their knowledge of elementary statistics applicable to elementary education. The NCTM Standards for Statistics for PreK-8 …
Field Experiments in Mobility and Navigation with a <phrase>Lunar Rover</phrase> Prototype <phrase>Scarab</phrase> is a prototype rover for lunar missions to survey resources, particularly <phrase>water ice</phrase>, in polar <phrase>craters</phrase>. It is designed as a prospector that would use a deep coring drill and apply soil analysis instruments. Its <phrase>chassis</phrase> can transform to stabilize its drill in contact with the ground and can also adjust posture to ascend and descent steep slopes. <phrase>Scarab</phrase> has undergone <phrase>field testing</phrase> at lunar analogue sites in <phrase>Washington</phrase> and <phrase>Hawaii</phrase> in an effort to quantify and validate its mobility and navigation capabilities. We report on results of experiments in slope ascent and descent and in autonomous kilometer-distance navigation in darkness.
DOA - The <phrase>Deductive</phrase> <phrase>Object-Oriented</phrase> Approach to the Development of Adaptive <phrase>Natural Language Interfaces</phrase> (Abstract) We present the <phrase>Deductive</phrase> <phrase>Object-oriented</phrase> Approach (DOA), a new framework for <phrase>natural language</phrase> <phrase>interface design</phrase>. It uses a deduc-tive <phrase>object-oriented</phrase> database (DOOD) for developing the interface as component of the database system. This provides the basis for a consistent and ecient mapping of the user input to the target representation. Furthermore, we propose adaptive techniques to deal eciently with the dicult task of dynamic <phrase>knowledge engineering</phrase>. As rst feasibility test we have developed an adaptive interface for Japanese, which is applied to the question support facility of a collaborative education system. Despite the long tradition of the research eld of <phrase>natural language interfaces</phrase> , we have to face the situation that <phrase>natural language interfaces</phrase> are still far away from widespread practicable use [1]. One of the main responsible factors for this is missing integration, which results in insu-cient performance and wrong interpretations. Therefore, we <phrase>dene</phrase> a new framework for <phrase>natural language</phrase> <phrase>interface design</phrase> in which the interface constitutes a component of the DOOD ROCK & ROLL [2]. Especially its <phrase>inheritance</phrase> mechanisms make it possible to structure the <phrase>linguistic knowledge</phrase> hierarchically to guarantee a compact representation. Another main diculty for <phrase>natural language interfaces</phrase> is the high amount of necessary manual <phrase>knowledge engineering</phrase>. For each portation to a new application this elaborate process has to be repeated. Furthermore, the interfaces are often part of dynamic environments with constant changes concerning topics and user population. Therefore, we apply linguistic resources and methods from <phrase>machine learning</phrase> to the automatic acquisition of <phrase>linguistic knowledge</phrase>. The applied interface architecture consists of the two main parts lexical and semantic component. The lexical component possesses three central modules: <phrase>morpho</phrase>-<phrase>lexical analysis</phrase>, unknown value list (UVL) analysis, and spelling error correction. <phrase>Morpho</phrase>-<phrase>lexical analysis</phrase> performs the tok-enization of Japanese input, i.e. the segmentation into individual input words. By accessing a domain-independent lexicon the module transforms the input into a deep form list (DFL), which indicates for each token its surface form, category, and a set of associated deep forms. During UVL analysis we deal with domain-specic terms in the input and we also support spelling error correction of those terms. Besides this, we apply the following three adaptive techniques: (1) the use of the
Multiscale 3-D <phrase>Shape Representation</phrase> and Segmentation Using Spherical <phrase>Wavelets</phrase> This paper presents a novel multiscale <phrase>shape representation</phrase> and <phrase>segmentation algorithm</phrase> based on the spherical <phrase>wavelet transform</phrase>. This work is motivated by the need to compactly and accurately encode variations at multiple scales in the <phrase>shape representation</phrase> in order to drive the segmentation and shape analysis of <phrase>deep brain structures</phrase>, such as the <phrase>caudate nucleus</phrase> or the <phrase>hippocampus</phrase>. Our proposed <phrase>shape representation</phrase> can be optimized to compactly encode shape variations in a population at the needed scale and spatial locations, enabling the construction of more descriptive, nonglobal, nonuniform shape probability priors to be included in the segmentation and shape analysis framework. In particular, this representation addresses the shortcomings of techniques that learn a global shape prior at a single scale of analysis and cannot represent fine, local variations in a population of shapes in the presence of a limited dataset. Specifically, our technique defines a multiscale parametric model of surfaces belonging to the same population using a compact set of spherical <phrase>wavelets</phrase> targeted to that population. We further refine the <phrase>shape representation</phrase> by separating into groups wavelet coefficients that describe independent global and/or local biological variations in the population, using spectral graph partitioning. We then learn a <phrase>prior probability</phrase> distribution induced over each group to explicitly encode these variations at different scales and spatial locations. Based on this representation, we derive a parametric active surface evolution using the multiscale prior coefficients as parameters for our optimization procedure to naturally include the prior for segmentation. Additionally, the optimization method can be applied in a coarse-to-fine manner. We apply our algorithm to two different <phrase>brain structures</phrase>, the <phrase>caudate nucleus</phrase> and the <phrase>hippocampus</phrase>, of interest in the study of <phrase>schizophrenia</phrase>. We show: 1) a reconstruction task of a test set to validate the expressiveness of our multiscale prior and 2) a segmentation task. In the reconstruction task, our results show that for a given <phrase>training set</phrase> size, our algorithm significantly improves the approximation of shapes in a testing set over the Point Distribution Model, which tends to oversmooth data. In the segmentation task, our validation shows our algorithm is computationally efficient and outperforms the <phrase>Active Shape Model</phrase> algorithm, by capturing finer shape details.
Volatility and Option Pricing Acknowledgements It is a pleasure to thank those who made this thesis possible. It is difficult to overstate my gratitude to my <phrase>Ph</phrase>. <phrase>Avellaneda</phrase>. His great enthusiasm, deep inspiration, and insightful understanding of <phrase>financial mathematics</phrase> influenced me tremendously ever since the beginning of my PhD study at Courant. I deeply appreciate his patience, efforts, and <phrase>creativity</phrase> in guiding me. This thesis would not have been possible without him. I would like to thank Prof. Jonathan <phrase>Goodman</phrase> for his passionate teaching and his time answering my questions. I am also grateful to Prof. Robert Kohn for encouraging and personal guidance. I wish to express my sincere thanks to Prof. Jim Gatheral for reading my thesis and his constructive comments. I thank Prof. Petter Kolm for being in my thesis committee. My warm thanks are due to Mike Lipkin for his extensive discussions around my work. I wish to extend my thanks to all those who have helped me with my thesis at Courant. The <phrase>financial support</phrase> of the MacCracken Fellowship Program is gratefully acknowledged. I am indebted to my many student colleagues for providing a stimulating and fun environment in which to learn and grow. I am lucky to meet these intellectual and caring people, who have helped me through the difficult times. I owe my special loving thanks to Stella Chen for supporting me spiritually throughout my study. Abstract This thesis studies leveraged exchange-traded funds (<phrase>ETF</phrase>) and options written on them. In the first part, we give an exact formula linking the price evolution of a leveraged <phrase>ETF</phrase> (LETF) with the price of its underlying <phrase>ETF</phrase>. We test the formula empirically on historical data for 56 leveraged funds (44 double-leveraged, 12 triple-leveraged) using daily closing prices. The results indicate excellent agreement between the formula and the empirical data. The formula shows that the evolution of the price of an LETF is sensitive to the realized volatility of the underlying product. The relationship between an LETF and its underlying asset is " path-dependent. " The second part of the study focuses on the relations between options on LETFs and options on the underlying <phrase>ETFs</phrase>. The main result shows that an option on an LETF can be replicated by a basket of options on the underlying <phrase>ETF</phrase> after a suitable choice of strikes and notionals. In particular, we obtain a new, relative-value, model for pricing LETF options. The derivation makes strong use …
Noise tolerant <phrase>low voltage</phrase> <phrase>XOR-XNOR</phrase> for fast arithmetic With scaling down to <phrase>deep submicron</phrase> and <phrase>nanometer technologies</phrase>, noise immunity is becoming a metric of the same importance as power, speed, and area. Smaller <phrase>feature sizes</phrase>, <phrase>low voltage</phrase>, and <phrase>high frequency</phrase> are the characteristics for <phrase>deep submicron</phrase> circuits. This paper proposes a <phrase>low voltage</phrase> noise tolerant <phrase>XOR-XNOR</phrase> gate with 8 transistors. The proposed gate has been implanted in an already existing (5-2) compressor cell to test its driving capability. The proposed gate is characterized and compared with those published ones for reliability and energy efficiency. The average noise threshold energy (ANTE) and the energy normalized ANTE metrics are used for quantifying the noise immunity and energy efficiency respectively. Results using 0.18m <phrase>CMOS technology</phrase> and HSPICE for simulation show that the proposed <phrase>XOR-XNOR</phrase> circuit is more noise-immune and displays better power-delay product characteristics than the compared circuit. Also, the circuit proves to be faster in operation and works at all ranges of <phrase>supply voltage</phrase> starting from 0.6V to 3.3V.
Fast <phrase>path selection</phrase> for testing of <phrase>small delay defects</phrase> considering path correlations —<phrase>Statistical timing</phrase> models have been proposed to describe <phrase>delay variations</phrase> in very deep sub-micro <phrase>process technologies</phrase>, which have increasingly significant influence on <phrase>circuit performance</phrase>. Under a <phrase>statistical timing</phrase> model, testing of a path can detect potential delay failures caused by different <phrase>small delay defects</phrase>. Due to path correlations, the potential delay failures captured by two different paths overlap between each other more or less. It is difficult to find a given number of paths that can capture most potential delay failures. In this paper, the <phrase>path selection</phrase> problem is converted to a minimal space intersection problem, and a greedy <phrase>path selection</phrase> heuristics is proposed, the key point of which is to calculate the probability that the paths in a specified path set all meet the delay constraint. <phrase>Statistical timing</phrase> analysis technologies and heuristics are used in the calculation. Experimental results show that the proposed approach is time-efficient and achieves a higher probability of capturing delay failures in comparison with conventional <phrase>path selection</phrase> approaches.
Finding parametric representations of the cortical sulci using an <phrase>active contour</phrase> model The cortical sulci are <phrase>brain structures</phrase> resembling thin convoluted ribbons embedded in three dimensions. The importance of the sulci lies primarily in their relation to the cytoarchitectonic and functional organization of the underlying cortex and in their utilization as features in <phrase>non-rigid registration</phrase> methods. This paper presents a methodology for extracting parametric representations of the cerebral sulcus from <phrase>magnetic resonance images</phrase>. The proposed methodology is based on <phrase>deformable models</phrase> utilizing characteristics of the cortical shape. Specifically, a parametric representation of a sulcus is determined by the motion of an <phrase>active contour</phrase> along the medial surface of the corresponding cortical fold. The <phrase>active contour</phrase> is initialized along the outer boundary of the brain and deforms toward the deep root of a sulcus under the influence of an external force field, restricting it to lie along the medial surface of the particular cortical fold. A parametric representation of the medial surface of the sulcus is obtained as the <phrase>active contour</phrase> traverses the sulcus. Based on the first fundamental form of this representation, the location and degree of an interruption of a sulcus can be readily quantified; based on its second fundamental form, shape properties of the sulcus can be determined. This methodology is tested on <phrase>magnetic resonance images</phrase> and it is applied to three <phrase>medical imaging</phrase> problems: quantitative morphological analysis of the <phrase>central sulcus</phrase>; mapping of functional activation along the <phrase>primary motor cortex</phrase> and <phrase>non-rigid registration</phrase> of <phrase>brain images</phrase>.
Linguistic feature analysis for <phrase>protein interaction</phrase> extraction BACKGROUND The rapid growth of the amount of publicly available reports on biomedical <phrase>experimental results</phrase> has recently caused a boost of <phrase>text mining</phrase> approaches for <phrase>protein interaction</phrase> extraction. Most approaches rely implicitly or explicitly on linguistic, i.e., lexical and syntactic, data extracted from text. However, only few attempts have been made to evaluate the contribution of the different feature types. In this work, we contribute to this evaluation by studying the relative importance of deep <phrase>syntactic features</phrase>, i.e., grammatical relations, shallow <phrase>syntactic features</phrase> (part-of-speech information) and lexical features. For this purpose, we use a recently proposed approach that uses <phrase>support vector machines</phrase> with structured kernels.   RESULTS Our <phrase>results reveal</phrase> that the contribution of the different feature types varies for the different <phrase>data sets</phrase> on which the experiments were conducted. The smaller the training corpus compared to the test data, the more important the role of grammatical relations becomes. Moreover, deep <phrase>syntactic information</phrase> based classifiers prove to be more robust on heterogeneous texts where no or only limited common vocabulary is shared.   CONCLUSION Our <phrase>findings suggest</phrase> that grammatical relations play an important role in the interaction extraction task. Moreover, the net advantage of adding lexical and shallow <phrase>syntactic features</phrase> is small related to the number of added features. This implies that efficient classifiers can be built by using only a small fraction of the features that are typically being used in recent approaches.
Performance-driven register insertion in placement As the <phrase>CMOS technology</phrase> is scaled into the dimension of nanometer, the clock frequencies and die sizes of ICs are shown to be increasing steadily [5]. Today, global wires that require multiple clock cycles to propagate electrical signal are prevalent in many <phrase>deep sub-micron</phrase> designs. Efforts have been made to pipe <phrase>ine</phrase> the long wires by introducing registers along these global paths, trying to reduce the impact of wire delay dominance [2, 8].The technique of retiming to relocate registers in a circuit without affecting the circuit functionality can be applied in this problem. Though the problem of retiming with gate and wire delay has been studied recent y [17, 1], the placement of registers after retiming is a new challenge. In this paper, we study the problem of realizing a retiming solution on a global <phrase>netlist</phrase> by inserting registers in the placement to achieve the target clock period.In contrast to many <phrase>previous works</phrase> [16, 11] that performed simple calculations to determine the positions of the registers, our proposed algorithm can preserve the given clock period and utilize as few registers as possible in the realization. What is more, the algorithm is shown to be optimal for nets with 4 or fewer pins and this type of nets constitutes over 90% of the nets in a sequential circuit on average.Using the ISCAS89 benchmark suite, we tested our algorithm with a 0.35 &#956;m CMOS standard <phrase>cell library</phrase>, and Silicon Ensemble was used to layout the design with row utilization of 50%. Experimenta <phrase>results showed</phrase> that our algorithm can find the best sharing of registers for a net in most of the cases, i.e., using the minimum number of registers while preserving the target clock period, within a minute running on an <phrase>Intel Pentium</phrase> IV 1.5GHz PC with 512MB <phrase>RAM</phrase>.
Applying the Model-view-controller Paradigm to <phrase>Adaptive Test</phrase> Putting It All Together <phrase>Yield Learning</phrase> Processes and Methods <phrase>Mvc</phrase> Implementation Experimental Demonstration <phrase>Yield Learning</phrase> Processes and Methods h DESPITE THE GROWING complexity of semiconductor devices, <phrase>adaptive test</phrase> deployments for such devices remain elusive. The high cost and difficulties of developing a custom <phrase>adaptive test</phrase> solution are likely causes for this <phrase>sparsity</phrase>. Indeed, there are many requirements that an adaptive test solution should meet. A good <phrase>adaptive test</phrase> system should dynamically adjust to <phrase>process variation</phrase> statistics that may not be stationary. It should modulate the <phrase>test program</phrase> at high granularities (die level or even sub-die level), have low adaptation latency to achieve real-time adaptivity, and comfortably handle learning from <phrase>terabyte</phrase>-scale historical <phrase>test data</phrase>. Moreover, it should achieve these goals without violating stringent industrial requirements on permissible test error. Finally, many of the specific details of these requirements are likely to differ across product lines, e.g., consumer products certainly have very different comfort levels for permissible test error than automotive products. However, the advantages of a successful <phrase>adaptive test</phrase> deployment are clear: test time reduction, <phrase>test quality</phrase> improvement, improved <phrase>data analysis</phrase> ability, and <phrase>acceleration</phrase> of <phrase>yield learning</phrase>. Therefore, substantial industrial interest exists for making <phrase>adaptive test</phrase> a reality in the near term. This interest is driving groups from a broad swath of industry to collaborate on <phrase>adaptive test</phrase> development. The Test & <phrase>Test Equipment</phrase> team within the <phrase>International Technology Roadmap for Semiconductors</phrase> (ITRS) has created a <phrase>subgroup</phrase> specifically targeted at the <phrase>adaptive test</phrase> problem [1]. The requirements that an adaptive test system must meet can be broken down to three discrete sets of challenges. First, there is the problem of data handling: device, wafer, and lot measurements should be linked and traceable throughout the fabrication process, data should be standardized and easy to exchange between <phrase>adaptive test</phrase> programs and throughout the organization, and large <phrase>data sets</phrase> should be readily accessible. Second, innovation within the <phrase>adaptive test</phrase> community is rapidly increasing the number of <phrase>adaptive test</phrase> techniques that have been demonstrated in literature. Thus, an adaptive test system should simplify integration of new <phrase>adaptive test</phrase> techniques as they become available. Third, test engineers should be provided with a convenient means of obtaining deep visibility into the inner workings of an adaptive test system. For Editor's note: Adaptive Testing has been a focus area for IC testing in the last few years. The ''Model-View-Controller'' (<phrase>MVC</phrase>) architecture has the potential to improve engineering productivity for analysis and application of Adaptive Testing. The vision of <phrase>MVC</phrase> is that it will be used to …
Moderate Effect of <phrase>Job Commitment</phrase> on the Relationship between Employees' <phrase>Emotional Labor</phrase> and Burnout <phrase>Emotional Labor</phrase> has important effect on jobs which contact people frequently, such as teachers, salespersons and so on. It is necessary to explore the circumstances of <phrase>emotional labor</phrase> about these special jobs. This study tested the impact of <phrase>emotional labor</phrase> on job burnout with the methods of interview and questionnaire survey, and explored the moderate effect of <phrase>job commitment</phrase> on the relationship between employees' <phrase>emotional labor</phrase> and burnout for teachers and sales employees. <phrase>Results showed</phrase> that 1)Surface acting can positively predict exhaustion and <phrase>depersonalization</phrase>, while deep acting negatively predict all three components of burnout; 2)Employees with higher <phrase>job commitment</phrase> tend to use more deep acting in work than those with lower <phrase>job commitment</phrase>; 3)<phrase>Job commitment</phrase> can moderate the relationship between surface acting and <phrase>depersonalization</phrase>. The relationship will be strong while <phrase>job commitment</phrase> is high, and will be weak while <phrase>job commitment</phrase> is low. At last, compared with teachers, sales employees tend to use more surface acting in work.
<phrase>Semi-automatic</phrase> <phrase>Syntactic and Semantic</phrase> Corpus Annotation with a <phrase>Deep Parser</phrase> We describe a <phrase>semi-automatic</phrase> method for linguistically rich corpus annotation using a broad-coverage <phrase>deep parser</phrase> to generate <phrase>syntactic structure</phrase>, semantic representation and discourse information for task-oriented dialogs. The parser-generated analyses are checked by trained annotators. Incomplete coverage and incorrect analyses are addressed through lexicon and grammar development, after which the dialogs undergo another cycle of parsing and checking. Currently we have 85% correct annotations in our <phrase>emergency</phrase> <phrase>rescue</phrase> task domain and 70% in our medication scheduling domain. This iterative process of corpus annotation allows us to create <phrase>domain-specific</phrase> <phrase>gold-standard</phrase> corpora for <phrase>test suites</phrase> and corpus-based experiments as part of general system development.
<phrase>Ab Initio</phrase> <phrase>Galaxy Formation</phrase> The formation and evolution of <phrase>galaxies</phrase> can be followed in the context of <phrase>cosmological</phrase> <phrase>structure formation</phrase> using the technique of semi-analytic modelling. We give a brief outline of the features incorporated into the semi-analytic model of Cole etal (1999). We present two examples of model predictions that can be tested using photometric red-shift techniques. The first prediction, of the <phrase>star formation</phrase> history of the universe, has already been shown to be in broad argeement with the observational estimates. The second prediction, of the evolution of <phrase>galaxy</phrase> clustering with <phrase>redshift</phrase>, will be addressed with some of the forthcoming deep, multi-filter imaging surveys discussed at this meeting.
<phrase>Fault tolerant</phrase> bus architecture for <phrase>deep submicron</phrase> based processors In the <phrase>Deep Submicron era</phrase> testing of processors is gaining prominence due to the effect of many analog faults. These faults have a direct impact on the <phrase>signal integrity</phrase> in interconnects. In <phrase>deep submicron</phrase> based processors electromigration poses a major challenge for interconnect reliability. This is of major concern in processor bus architectures. Though a lot of research has been focused on <phrase>fault tolerance</phrase> at the module level, no effective research has been carried out to evolve <phrase>fault tolerant</phrase> interconnect structures. Faults due to electromigration give rise to weak ones and zeros, thus affecting the <phrase>signal integrity</phrase>. This paper presents a fault tolerant scheme for electromigration which also leads to unification of detection and correction of <phrase>crosstalk faults</phrase> and <phrase>stuck-at faults</phrase>. In the scheme presented, the signal is rerouted through adjacent spare interconnects. The spare interconnect also acts as a shield against crosstalk under <phrase>fault free</phrase> system operation. Moreover, the entire <phrase>testing process</phrase> is carried out concurrently with the processor instruction execution. The <phrase>hardware overhead</phrase> is also insignificant compared to the overall device count in a <phrase>deep submicron</phrase> based processor. To the best of our knowledge this is the first time a fault tolerant scheme for bus architectures is being presented.
A Readability Checker with <phrase>Supervised Learning</phrase> Using Deep Indicators Checking for readability or simplicity of texts is important for many institutional and individual users. Formulas for approximately measuring text readability have a long tradition. Usually, they exploit surface-oriented indicators like sentence length, word length, word frequency, etc. However, in many cases, this information is not adequate to realistically approximate the cognitive difficulties a person can have to understand a text. Therefore we use <phrase>deep syntactic and semantic indicators</phrase> in addition. The <phrase>syntactic information</phrase> is represented by a <phrase>dependency tree</phrase>, the <phrase>semantic information</phrase> by a <phrase>semantic network</phrase>. Both representations are <phrase>automatically generated</phrase> by a deep syntactico-<phrase>semantic analysis</phrase>. A global readability score is determined by applying a nearest neighbor algorithm on 3,000 ratings of 300 test persons. The evaluation showed that the <phrase>deep syntactic and semantic indicators</phrase> lead to <phrase>promising results</phrase> comparable to the best surface-based indicators. The combination of <phrase>deep and shallow</phrase> indicators leads to an improvement over shallow indicators alone. Finally, a graphical user interface was developed which highlights difficult passages, depending on the individual indicator values, and displays a global readability score. Povzetek: Strojno učenje z odvisnostnimi drevesi je uporabljeno za ugotavljanje berljivosti besedil.
A Deep Validation Process for Open <phrase>Document Repositories</phrase> Institutional <phrase>document repositories</phrase> show a systematic growth as well as a sustainable deployment. Therefore, they represent the current backbone of a distributed repository infrastructure. Many developments for <phrase>electronic publishing</phrase> through digital repositories are heading in the direction of innovative value-added services such as <phrase>citation analysis</phrase> or annotation systems. A rich service-layer based on machine-to-machine communication between value-added services and <phrase>document repositories</phrase> requires a reliable operation and <phrase>data management</phrase> within each repository. Aggregating search services such as OAISTER and BASE provide good results. But in order to provide good quality they also have to overcome heterogeneity by normalizing many of the data they receive and build specific profiles for sometimes even one individual repository. Since much of the normalization is done at the side of the service provider, it often remains unclear — maybe sometimes also to the manager of a local repository — exactly which data and services are exposed to the public. Here, an exploratory validation method for testing specification compliance in repositories is presented. 1. Introduction Many developments for <phrase>electronic publishing</phrase> through digital repositories are heading in the direction of innovative value-added services (e.g. <phrase>citation analysis</phrase> or annotation systems) and abstract data representation of all types of resources (e.g. primary data or educational material). Still, conventional <phrase>document repositories</phrase> show the most systematic growth [1] as well as <phrase>high-quality</phrase>, sustainable deployment. Therewith, they represent the current backbone of a distributed repository infrastructure. A rich service-layer based on machine-to-machine communication between value-added services and <phrase>document repositories</phrase> requires a reliable operation and <phrase>data management</phrase> within each repository. Aggregating search services such as OAISTER [2] and BASE [3] provide good results. But in order to provide good quality they also have to normalize many of the data they receive and build specific profiles and processes for even individual repositories. Since much of the normalization is done at the side of the service provider, it often remains unclear — maybe sometimes also to the manager of a repository — exactly which data and services are exposed to the public by a local data provider. Existing validation techniques [4, 5] are important but their current scope is only at the level of testing basic compliance with OAI-PMH and simple-DC. As a consequence, quantitative assessments of <phrase>data quality</phrase>, i.e. of what is specifically exposed by a repository and by the whole repository <phrase>landscape</phrase> are widely missing. Mature infrastructures should, however, provide reliable data resources, robust …
<phrase>Pennsylvania</phrase> Earthquake <phrase>Pennsylvania</phrase> Earthquake Digest <phrase>Earthquakes</phrase> in midplate regions have been considered enigmatic since they cannot be easily associated with major <phrase>plate boundary</phrase> deformations. In <phrase>North America</phrase>, small magnitude midplate <phrase>seismicity</phrase> is common in the a moderate-size earthquake occurred in northwestern <phrase>Pennsylvania</phrase>, near the border of <phrase>Ohio</phrase> and <phrase>Pennsylvania</phrase>. Rapid analyses of seismic waveforms generated by the m bLg 5.2 <phrase>Penn</phrase>-<phrase>sylvania-Ohio</phrase> border region earthquake suggested an unusual, <phrase>non double-couple</phrase> component to the faulting mechanism. The existence of a substantial <phrase>non double-couple</phrase> component to the faulting mechanism has important implications for the cause of the earthquake (hydrologically induced shallow faulting or " typical " eastern <phrase>North America</phrase> basement faulting?). Preliminary checks of the near real-time solutions suggested the <phrase>non double-couple</phrase> component may have been an artifact caused by the available data. One of the goals of this study was to investigate the size of the <phrase>non double-couple</phrase> faulting component in the Pymatuning earthquake the other was to improve mechanism and depth estimates for the event. To investigate the detailed nature of this event I used the observed seismograms (from the <phrase>United States</phrase> National Seismic Network, USNSN, and from the <phrase>Canada</phrase> National Seismic Network, CNSN) to constrain faulting parameters including the source depth, fault strike, dip, and slip, and to explore the reason(s) why early estimates contained large <phrase>non double-couple</phrase> 1 2 source components. I performed moment <phrase>tensor</phrase> inversions with L2 and L1 norms for the closest stations (epicentral distance less than 1000 km), and the results agree with the previous near-real time studies. To test the significance of the <phrase>non double-couple</phrase> component I sought a solution constrained to be a pure <phrase>double couple</phrase> by checking the match to the observations for all values of strike, dip, and rake (grid search) for depths between 2.5 and 25 km. The final results show that the Pymatuning earthquake can be explained with a pure <phrase>double couple</phrase> faulting mechanism, corresponding to a near-vertical, mostly strike-slip fault with planes striking 110 o and 13 o , with dips of 70 o and 71 o , and rakes of 20 o and 159 o. The estimated moment for this inversion is 5.610 22 <phrase>dyne</phrase>-cm, which corresponds to a <phrase>moment magnitude</phrase> of 4.5. All three inversions (L2, L1 moment <phrase>tensor</phrase> and L1 Grid Search) match the observed seismograms well for a source depth less than 7.5 km. The best " formal " fit is for a 2.5 km deep source, but the …
An Efficient Parallelized <phrase>L7-Filter</phrase> Design for Multicore Servers <phrase>L7-filter</phrase> is a significant <phrase>deep packet inspection</phrase> (DPI) extension to <phrase>Netfilter</phrase> in Linux's QoS framework. It classifies <phrase>network traffic</phrase> based on information hidden in the packet payload. Although the computationally intensive payload classification can be accelerated with multiple processors, the default OS scheduler is oblivious to both the software characteristics and the underlying multicore architecture. In this paper, we present a parallelized <phrase>L7-filter</phrase> algorithm and an efficient scheduler technique for multicore servers. Our multithreaded <phrase>L7-filter</phrase> algorithm can process the incoming packets on multiple servers boosting the throughput tremendously. Our <phrase>scheduling algorithm</phrase> is based on Highest Random Weight (<phrase>HRW</phrase>), which maintains the connection locality for the incoming traffic, but only guarantees load balance at the connection level. We present an Adapted Highest Random Weight (AHRW) algorithm that enhances <phrase>HRW</phrase> by applying packet-level <phrase>load balancing</phrase> with an additional feedback vector corresponding to the queue length at each processor. We further introduce a Hierarchical AHRW (AHRW-tree) algorithm that considers characteristics of the multicore architecture such as cache and hardware topology by developing a hash tree architecture. The algorithm reduces the scheduling overhead to <i>O</i>(log <i>N</i>) instead of <i>O</i>(<i>N</i>) and produces a better balance between locality and <phrase>load balancing</phrase>. Results show that the AHRW-tree scheduler can improve the <phrase>L7-filter</phrase> throughput by about 50% on a <phrase>Sun</phrase>-Niagara- 2-based server compared to a connection locality-based scheduler. Although extensively tested for <phrase>L7-filter</phrase> traces, our technique is applicable to many other packet processing applications, where connection locality and <phrase>load balancing</phrase> are important while executing on multiple processors. With these speedups and inherent software flexibility, our design and implementation provide a <phrase>cost-effective</phrase> alternative to the traffic monitoring and filtering ASICs.
Radiotelemetry comes of age--perhaps just in time. IN ACTUALITY, THE DISCIPLINE of <phrase>functional genomics</phrase> deals with the science and art of phenotyping. Identifying the genetic bases for complex integrated systems, as well as complex diseases, relies on the validity and accuracy of the " trait measurement. " Any trait, real or erroneous, may be mapped, whether in humans or animal models. Such errors in quantitative trait locus mapping, or even gene assignment, may be compounded because few replicate studies are conducted. In an effort to avoid such errors, behavioral scientists use three tests of " validity " for a trait (phenotype): " construct, " " face, " and " predictive " (11, 12). However, even these tests may be insufficient if the actual measurement methodology is compromised. There may be many reasons that contribute to inadequate , inappropriate, or erroneous phenotyping, among them the potential interaction between the experimenter (or environment) and the test subject. In human studies the cognitive capabilities of the test subject present the challenge. This problem is further exacerbated with animal models, mainly due to the inability to bridge the cognitive gap between animals and humans. One cannot simply ask a rat or mouse how they feel that day or whether the technician's <phrase>aftershave</phrase> <phrase>lotion</phrase> is disturbing to them! Technology has encouraged major advances over the past decade in monitoring physiological and behavioral traits of human and model organisms. Among the factors contributing to these advances are innovations in the fields of computer and <phrase>information technology</phrase>, as well as <phrase>miniaturization</phrase> of instru-mentation needed for <phrase>telemetry</phrase> or non-hard-wired monitoring. These advances address a major confound, the " biological <phrase>uncertainty principle</phrase>, " which derives from the influence of measurement method and/or experimenter on trait expression or subject performance. The development of small implantable <phrase>telemetry</phrase> probes has sparked a <phrase>revolution</phrase> in the laboratory not unlike that ignited in <phrase>molecular biology</phrase> and medicine through the discovery of thermostable polymerases and the development of the <phrase>polymerase chain reaction</phrase>. Commercialization of small implantable <phrase>telemetry</phrase> probes has made such instrumen-tation available to nearly every investigator. Recent advances in radiotelemetry probes provide new opportunities for real-time monitoring of traits such as deep body temperature; motor, neuronal, and cardiovascular activities; and even sleep states. Furthermore, radiotelemetry introduces additional opportunities in experimental design by permitting <phrase>time-series</phrase> measurements over days, weeks, and months. <phrase>Transference</phrase> of the methodology to even smaller <phrase>rodents</phrase>, such as Mus, combined with the assumption of the Mus species as an …
<phrase>Neural Network</phrase> Regularization via Robust Weight Factorization Regularization is essential when training large <phrase>neural networks</phrase>. As <phrase>deep neural networks</phrase> can be mathematically interpreted as universal function approximators, they are effective at memorizing sampling noise in the training data. This results in poor generalization to unseen data. Therefore, it is no surprise that a new reg-ularization technique, Dropout, was partially responsible for the now-ubiquitous winning entry to ImageNet 2012 by the University of <phrase>Toronto</phrase>. Currently, Dropout (and related methods such as DropConnect) are the most effective means of regu-larizing large <phrase>neural networks</phrase>. These amount to efficiently visiting a large number of related models at training time, while aggregating them to a single predictor at test time. The proposed FaMe model aims to apply a similar strategy, yet learns a factorization of each weight matrix such that the factors are robust to noise.
Spare Line Borrowing Technique for Distributed <phrase>Memory Cores</phrase> in Soc In this paper, a new architecture of distributed embedded <phrase>memory cores</phrase> for SoC is proposed and an effective memory repair method by using the proposed Spare Line Borrowing (software-driven reconfiguration) technique is investigated. It is known that faulty cells in memory core show spatial locality, also known as fault clustering. This physical phenomenon tends to occur more often as <phrase>deep submicron technology</phrase> advances due to defects that span multiple circuit elements and sophisticated <phrase>circuit design</phrase>. The combination of new architecture & repair method proposed in this paper ensures <phrase>fault tolerance</phrase> enhancement in SoC, especially in case of fault clustering. This <phrase>fault tolerance</phrase> enhancement is obtained through optimal redundancy utilization: Spare redundancy in a fault-resistant memory core is used to fix the fault in a fault-prone memory core. The effect of Spare Line Borrowing technique on the reliability of distributed <phrase>memory cores</phrase> is analyzed through modeling and extensive parametric simulation. I. INTRODUCTION System on a Chip (SoC) is driving the VLSI (<phrase>Very Large Scale Integration</phrase>) industry today. Chip designers can implement everything from processors, memories, and other logic cores to bus interfaces onto a single chip with the <phrase>deep submicron technology</phrase>. Due to the fact that once System on a Chip (SoC) is fabricated it is impossible to remove, recondition, and replace a faulty core deeply embedded into the chip, test and repair process is one of the challenging issues for the success of <phrase>SoC design</phrase>. Therefore, more reliable and dependable <phrase>SoC design</phrase> with built-in self test/diagnosis/ repair is desirable and more suggested than
Cutting Recursive Autoencoder Trees <phrase>Deep Learning</phrase> models enjoy considerable success in <phrase>Natural Language Processing</phrase>. While <phrase>deep architectures</phrase> produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. In this paper, we rely on empirical tests to see whether a particular structure makes sense. We present an analysis of the <phrase>Semi-Supervised</phrase> Recursive Autoencoder, a well-known model that produces structural representations of text. We show that for certain tasks, the structure of the autoencoder can be significantly reduced without loss of <phrase>classification accuracy</phrase> and we evaluate the produced structures using human judgment.
Automated generation of massive image knowledge collections using Microsoft Live Labs Pivot to promote <phrase>neuroimaging</phrase> and <phrase>translational research</phrase> BACKGROUND Massive datasets comprising <phrase>high-resolution</phrase> images, generated in neuro-imaging studies and in clinical imaging research, are increasingly challenging our ability to analyze, share, and filter such images in clinical and basic <phrase>translational research</phrase>. Pivot collection exploratory analysis provides each user the ability to fully interact with the massive amounts of visual data to fully facilitate sufficient sorting, flexibility and speed to fluidly access, explore or analyze the massive <phrase>image data</phrase> sets of <phrase>high-resolution</phrase> images and their associated meta information, such as neuro-imaging databases from the Allen <phrase>Brain Atlas</phrase>. It is used in clustering, filtering, data sharing and classifying of the visual data into various deep zoom levels and meta information categories to detect the underlying hidden pattern within the <phrase>data set</phrase> that has been used.   METHOD We deployed prototype Pivot collections using the Linux <phrase>CentOS</phrase> running on the <phrase>Apache</phrase> <phrase>web server</phrase>. We also tested the prototype Pivot collections on other <phrase>operating systems</phrase> like Windows (the most common variants) and UNIX, etc. It is demonstrated that the approach yields very good results when compared with other approaches used by some researchers for generation, creation, and clustering of massive image collections such as the <phrase>coronal</phrase> and horizontal sections of the mouse brain from the Allen <phrase>Brain Atlas</phrase>.   RESULTS Pivot <phrase>visual analytics</phrase> was used to analyze a prototype of dataset Dab2 co-expressed genes from the Allen <phrase>Brain Atlas</phrase>. The metadata along with <phrase>high-resolution</phrase> images were automatically extracted using the Allen <phrase>Brain Atlas</phrase> API. It is then used to identify the hidden information based on the various categories and conditions applied by using options generated from automated collection. A metadata category like <phrase>chromosome</phrase>, as well as data for individual cases like sex, age, and plan attributes of a particular gene, is used to filter, sort and to determine if there exist other genes with a similar characteristics to Dab2. And online access to the mouse brain pivot collection can be viewed using the link http://edtech-dev.uthsc.edu/CTSI/teeDev1/unittest/PaPa/collection.html (user name: tviangte and <phrase>password</phrase>: demome)   CONCLUSIONS Our proposed algorithm has automated the creation of large image Pivot collections; this will enable investigators of <phrase>clinical research</phrase> projects to easily and quickly analyse the image collections through a perspective that is useful for making critical decisions about the image patterns discovered.
Testability of <phrase>Cryptographic Hardware</phrase> and Detection of Hardware <phrase>Trojans</phrase> —<phrase>Cryptographic algorithms</phrase> are routinely used to perform computationally intense operations over increasingly larger volumes of data, and in order to meet the <phrase>high throughput</phrase> requirements of the applications, are often implemented by <phrase>VLSI designs</phrase>. The high complexity of such implementations raises concern about their reliability. In order to improve upon the testability of <phrase>sequential circuits</phrase>, both at fabrication time and also in the field, <phrase>Design For Testability</phrase> (DFT) techniques are commonly employed. However conventional DFT methodologies for <phrase>digital circuits</phrase> have been found to compromise the security of the <phrase>cryptographic hardware</phrase>. In this tutorial we first discuss the challenges and potential attacks on <phrase>cipher</phrase> hardware through standard DFT techniques, and then potential solutions against them. Also, as the electronic design industry has grown globally, economic reasons dictate the widespread participation of external agents in modern design and manufacture of <phrase>integrated circuits</phrase> (ICs), which decreases the control that the IC design houses used to traditionally have over their own designs. This issue raises the question of ensuring Trust in an <phrase>integrated circuit</phrase>, and whether the IC can be certified to be free of malicious, hard-to-detect circuitry, commonly referred to as Hardware <phrase>Trojans</phrase>. In this tutorial, we would explore the unique challenges and testing solutions to detect/prevent such malicious modifications. I. INTRODUCTION Reliability of devices has become a serious concern, with the increase of complexity of ICs and the advent of <phrase>deep sub-micron</phrase> <phrase>process technology</phrase>. The growth of applications of <phrase>cryptographic algorithms</phrase> and their requirement for real time-processing has necessitated the design of <phrase>crypto</phrase>-hardware. But along with the design of such devices, testability is a key issue. What makes testability of these circuits more challenging compared to other digital designs, is the fact that popular <phrase>Design for Testability</phrase> (DFT) methodologies, such as " <phrase>scan-chain</phrase> insertion " , can be used as a double-edged <phrase>sword</phrase>. <phrase>Scan-chain</phrase> insertion, which is a desirable test technique owing to its high fault coverage and small <phrase>hardware overhead</phrase>, open " side-channels " for <phrase>cryptanalysis</phrase> [1], [2]. <phrase>Scan-chains</phrase> are used to access intermediate values stored in the <phrase>flip-flops</phrase>, thereby ascertaining the secret information, often known as " key ". Conventional <phrase>scan-chains</phrase> fail to solve the conflicting requirements of effective testing and security [2]. So, one of the solutions that have been suggested is to blow off the <phrase>scan-chains</phrase> from the <phrase>crypto</phrase> ICs, before they are released into the market. But such an approach is unsatisfactory and directly conflicts the paradigm …
Research on <phrase>Fault Diagnosis</phrase> for Cnc Machine of Flexible <phrase>Manufacturing System</phrase> In order to find and treat fault or abnormal occurrences in numerically controlled <phrase>machine tool</phrase> of flexible <phrase>manufacturing system</phrase> in real-time, a study for computer numerically controlled (CNC) on-line fault diagnostics system has been carried out. The system can look up one or more sticking points of numerically controlled <phrase>machine tool</phrase> fault simply, locate CNC faults rapidly and accurately, but solve the problem between <phrase>fault diagnosis</phrase> and knowledge application. To provide CNC apparatus terminal with reliable trouble <phrase>shooting</phrase> suggestion, give a deep analysis between servo control subassembly and background service, make a description of fault database and test procedure design. The research has solved the problem of system maintenance, expansion and upgrade.
Vers un système générique de réécriture de graphes pour l’enrichissement de structures syntaxiques <phrase>RÉSUMÉ</phrase> Ce travail présente une nouvelle approche pour injecter des dépendances profondes (sujet des verbes à contrôle, partage du sujet en cas d'ellipses,. . .) dans un corpus arboré présentant un schéma d'annotation surfacique et projectif. Nous nous appuyons sur un système de réécriture de graphes utilisant des techniques de programmation par contraintes pour produire des règles génériques qui s'appliquent aux phrases du corpus. Par ailleurs, nous testons la généricité des règles en utilisant des sorties de trois analyseurs syntaxiques différents, afin d'évaluer la dégradation exacte de l'application des règles sur des analyses syntaxiques prédites. ABSTRACT Towards a generic graph rewriting system to enrich syntactic structures This work aims to present a new approach for injecting deep dependencies (subject of control <phrase>verbs</phrase>, subject sharing in case of <phrase>ellipsis</phrase>,. . .) into a surfacic and <phrase>projective</phrase> treebank. We use a graph rewriting system with <phrase>constraint programming</phrase> techniques for producing generic rules which can be easily applied to a treebank. Moreover, we are testing the genericity of our rules by using output of three different parsers to evaluate how the rules behave on predicted <phrase>parse trees</phrase>. MOTS-CLÉS : réécriture de graphes, évaluation de shéma d'annotations, parsing, analyse en syntaxe profonde.
<phrase>Qualitative Case</phrase> Study Guidelines Although widely used, the <phrase>qualitative case</phrase> study method is not well understood. Due to conflicting <phrase>epistemological</phrase> presuppositions and the complexity inherent in <phrase>qualitative case</phrase>-based studies, scientific rigor can be difficult to demonstrate, and any resulting findings can be difficult to justify. For that reason, this <phrase>paper discusses</phrase> methodological problems associated with <phrase>qualitative case</phrase>-based research and offers guidelines for overcoming them. Due to its nearly universal acceptance, Yin's six-stage <phrase>case study</phrase> process is adopted and elaborated on. Moreover, additional principles from the wider methodological literature are integrated and explained. Finally, some modifications to the dependencies between the six <phrase>case study</phrase> stages are suggested. It is expected that following the guidelines presented in this paper may facilitate the collection of the most relevant data in the most efficient and effective manner, simplify the subsequent analysis, as well as enhance the validity of the resulting findings. The paper should be of interest to students (honour, masters, doctoral), academics, and practitioners involved with conducting and reviewing <phrase>qualitative case</phrase>-based studies. Where quantitative research is mainly concerned with the testing of hypotheses and statistical generalisations (Jackson, 2008), <phrase>qualitative research</phrase> does not usually employ statistical procedures or other means of quantification, focusing instead on understanding the nature of the research problem rather than on the quantity of observed characteristics (<phrase>Strauss</phrase> & Corbin, 1994). Given that qualitative researchers generally assume that social reality is a human creation, they interpret and contextualise meanings from people's beliefs and practices (Denzin & <phrase>Lincoln</phrase>, 2011). <phrase>Case study</phrase> research involves " intensive study of a single unit for the purpose of understanding a larger class of (similar) units … observed at a single point in time or over some delimited period of time " (Gerring, 2004, p. 342). As such, <phrase>case studies</phrase> provide an opportunity for the researcher to gain a deep holistic view of the research problem, and may facilitate describing, understanding and explaining a research problem or situation Besides being widely used in <phrase>academia</phrase>, the method is also popular with practitioners as a tool for evaluation and organisational learning. However, although widely used, the <phrase>qualitative case</phrase> study method is not well understood Given the considerable time and resource requirements associated with conducting such studies (GAO, 1990), any misunderstandings regarding the purpose and implementation of the method as well as the validity of the resulting findings can have significant negative consequences. In the context of academic studies, significant misunderstandings identified during the …
Preventing <phrase>Single Event</phrase> Latchup with Deep P-well on P-substrate —We propose a method that prevents <phrase>single event</phrase> latchup (SEL) using deep P-well on P-substrate. To confirm the effectiveness of the proposed method, SEL and <phrase>single event upset</phrase> (SEU) are evaluated for three well configurations; double-well, ordinary triple-well and the proposed deep P-well on P-substrate. Neutron irradiation test shows that the proposed method achieves SEL prevention without SEU increase.
Traffic analysis for on-chip networks design of multimedia applications The objective of this paper is to introduce <phrase>self-similarity</phrase> as a fundamental property exhibited by the bursty traffic between on-chip modules in typical <phrase>MPEG-2</phrase> video applications. <phrase>Statistical tests</phrase> performed on relevant traces extracted from common video clips establish unequivocally the existence of <phrase>self-similarity</phrase> in video traffic. Using a generic communication architecture, we also discuss the implications of our findings on on-chip buffer space allocation and present quantitative evaluations for typical video streams. We believe that our findings open up new directions of research with <phrase>deep implications</phrase> on some fundamental issues in on-chip network design for multimedia applications.
Signal Classification for Acoustic Neutrino Detection This article focuses on signal classification for <phrase>deep-sea</phrase> acoustic neutrino detection. In the deep sea, the background of <phrase>transient signals</phrase> is very diverse. Approaches like matched filtering are not sufficient to distinguish between neutrino-like signals and other <phrase>transient signals</phrase> with similar signature, which are forming the acoustic background for neutrino detection in the deep-sea environment. A classification system based on <phrase>machine learning algorithms</phrase> is analysed with the goal to find a robust and effective way to perform this task. For a well-trained model, a testing error on the level of one percent is achieved for strong classifiers like <phrase>Random Forest</phrase> and Boosting Trees using the extracted features of the signal as input and utilising dense clusters of sensors instead of single sensors.
Online Test Macro Scheduling and Assignment in MPSoC Design —Due to unreliability of the cores in <phrase>embedded systems</phrase> in deep sub-micron technologies, a method for testing cores in the field is needed. In this paper an online method for testing cores of embedded designs is presented. The proposed task scheduling method runs the test routine ASAP periodically considering the real time constraints. A software test routine based on a <phrase>proposed method</phrase> will be generated and a task scheduling process including the test task (for each core) and other existing applications of the embedded system will be presented. A software based <phrase>checksum</phrase> is issued for online test result analysis that shortens the memory usage of the <phrase>test process</phrase>. Experimental results show that this method improves the test application time (TAT) and <phrase>fault coverage</phrase> (in proportion to TAT) as compared with the <phrase>existing methods</phrase>.
<phrase>Mixed-Signal</phrase> <phrase>Hardware Description</phrase> Languages in the Era of System-on-Silicon: Challenges and Opportunities (Abstract of Embedded Tutorial) SPICE-based simulation is recognized as a vital tool in shortening time-to-market, reducing product cost, and improving system reliability. It continues to have a profound impact on today's electronic industry. Behavioral <phrase>modeling and simulation</phrase> with Analog and <phrase>Mixed-Signal</phrase> <phrase>Hardware Description</phrase> Languages (AHDLs) are becoming critical to address the challenge of designing today's increasingly complicated electronic systems. These systems may have millions of transistors, are radically diverse (e.g., micro-electro-mechanical) and governed by tightly coupled physical (e.g. thermal-electronic and <phrase>deep-submicron</phrase>) effects. The trend towards behavioral <phrase>modeling and simulation</phrase> has been exemplified by the success of proprietary modeling languages and simulators and by the emergence of several language standards (VHDL-AMS and Verilog-AMS) and their application from full system verification to <phrase>design reuse</phrase>, virtual prototyping, testing, and even synthesis. In this tutorial, we will overview some of <phrase>recent developments</phrase> in AHDLs and their applications, with particular emphasis on challenges and opportunities.
Challenges of biological realism and validation in <phrase>simulation-based</phrase> medical education OVERVIEW Simulation, both physical and computer-based, has a rich history in support of medical education. Essentially all these efforts have been aimed at instilling concrete measurable skills, akin to vocational training. They present learners with choices, facilitating a degree of learning by doing. The sets of learner choices are usually limited, with choices clearly classified into "right" and "wrong". But much of medicine is not much like a <phrase>multiple-choice</phrase> test. The realm of choices is broad and not always easily converted to a short list. The "correct" answer is not always known by the experienced physician beforehand, sometimes not even after the die is cast and the future unfolds. Computer simulation of human disease and its treatment can in principle be tremendously useful in the education of both basic and clinical scientists. This paper describes some challenges in the construction of <phrase>simulation-based</phrase> "<phrase>liberal arts</phrase>" biomedical education.   OBJECTIVES The <phrase>educator</phrase> attempting to develop a <phrase>learning environment</phrase> based on simulation of biology faces some special challenges. The challenges addressed in this paper are: face validity and deep validity; finding the right degree of realism; authoring biomedical models efficiently; managing randomness. To illustrate the issues, we trace the history of the <phrase>Oncology</phrase> Thinking <phrase>Cap</phrase> throughout several versions and expansions of educational objectives, and describe the detection and remediation of shortcomings related to these issues.   DESIGN Dealing effectively with issues of validity and realism can be accomplished if the acquisition of information driving and justifying the model development choices is documented, preferably automatically, during the process. Efficiency in authoring is greatly enhanced by judicious modularity to encourage re-use, and by the use of templated statements rather than raw code or exotic graphical components to represent the instructions driving the model. Randomness can be used to familiarize learners with the true relative proportions of types of cases, or to enrich the encountered cases with rarer but more instructive cases. When a learner repeats an encounter with a scenario while changing a single option, proper management of randomness is essential to avoid artifacts of <phrase>random number generators</phrase>. Otherwise an outcome change caused by a shift in random number streams may masquerade as an outcome change due to the changed option.   CONCLUSION Effective use of computer simulation of human disease and its treatment for biomedical education faces daunting obstacles, but these problems can be solved.
A <phrase>Process Technology</phrase> for Realizing Integrated <phrase>Inertial Sensors</phrase> Using <phrase>Deep Reactive Ion Etching</phrase> (drie) and Aligned <phrase>Wafer Bonding</phrase> a <phrase>Process Technology</phrase> for Realizing Integrated <phrase>Inertial Sensors</phrase> Using <phrase>Deep Reactive Ion Etching</phrase> (drie) and Aligned <phrase>Wafer Bonding</phrase> The demand for silicon micromachined <phrase>inertial sensors</phrase> is expected to grow tremendously in the next few years. Potential benefits such as improved performance, enhanced reliability and lower cost can be gained by integrating these sensors with on-chip electronics. Using <phrase>deep reactive ion etching</phrase> (DRIE) and aligned <phrase>wafer bonding</phrase>, a <phrase>process technology</phrase> for realizing integrated <phrase>inertial sensors</phrase> is developed. DRIE allows the formation of high-<phrase>aspect-ratio</phrase> structures especially crucial for lateral <phrase>inertial sensors</phrase>. Compatibility with standard IC processes is achieved by the sealed-cavity approach as enabled by <phrase>wafer bonding</phrase>. This process also realizes a new interconnection scheme which permits signal crossovers. During process development, DRIE gap-widening and footing effects are observed. These effects are characterized and ways to minimize them found. The <phrase>process technology</phrase> is successfully demonstrated by the fabrication of functional <phrase>accelerometers</phrase> and <phrase>gyroscopes</phrase>. The characteristics of the <phrase>accelerometers</phrase> are measured by shaker tests and Computer <phrase>Microvision</phrase>. Some deviation from the design values is observed, however, its cause is not completely understood.
Integration and Evaluation of a <phrase>Video Surveillance</phrase> System Integration and Evaluation of a <phrase>Video Surveillance</phrase> System <phrase>Visual surveillance</phrase> systems are getting a lot of attention over the last few years, due to a growing need for <phrase>surveillance applications</phrase>. In this thesis, we present a <phrase>visual surveillance</phrase> system that integrates modules for <phrase>motion detection</phrase>, tracking, and trajectory characterization to achieve robust monitoring of <phrase>moving objects</phrase> in scenes under surveillance. The system operates on video sequences acquired by stationary color and <phrase>infra-red</phrase> surveillance cameras. <phrase>Motion detection</phrase> is implemented using an algorithm that combines thresh-olding of temporal variance and background modeling. The tracking algorithm combines motion and appearance information into an appearance model and uses a <phrase>particle filter</phrase> framework for <phrase>object tracking</phrase>. The trajectory analysis module builds a model for a given normal activity using a factorization approach, and uses this model for the detection of any abnormal motion pattern. The system was tested on a large ground-truthed <phrase>data set</phrase> containing hundreds of color and <phrase>FLIR</phrase> <phrase>image sequences</phrase>. Results of <phrase>performance evaluation</phrase> using these sequences are reported in this thesis. DEDICATION This thesis is dedicated to my beloved parents, sisters, and brother. <phrase>ii ACKNOWLEDGMENTS</phrase> I would like to thank many people who helped me to bring this work together. First, to my advisor, Prof <phrase>Rama</phrase> Chellappa, whose guidance and support over the last two years have been of great help. To Dr Qinfen Zheng, for his valuable support and discussions during the course of this work. I would like to also thank the rest of my committee, Prof <phrase>Steve Marcus</phrase> and Prof Min Wu, for the valuable assistance they gave me either in classes or in my thesis work. I would like to thank my group mates, Aswin Sankaranarayanan and Seong-wook Joo for helping me to start this work with there software and for their valuable discussions. I would also like to express me deep gratitude to Ahmed Sadek and Wael Abd-Almageed for their valuable comments and help in the final stages of this work.
<phrase>Data Mining</phrase> Tools Used in <phrase>Deep Brain Stimulation</phrase> - Analysis Results <phrase>Parkinson's disease</phrase> is associated with <phrase>motor symptoms</phrase>, including tremor. The DBS (<phrase>Deep Brain Stimulation</phrase>) involves electrode implantation into sub-cortical structures for <phrase>long-term</phrase> stimulation at frequencies greater than 100Hz. We performed linear and nonlinear analysis of the tremor signals to determine a set of parameters and rules for recognizing the behavior of the investigated patient and to characterize the typical responses for several forms of DBS. We found patterns for homogeneous group for data reduction. We used <phrase>Data Mining</phrase> and <phrase>Knowledge discovery</phrase> techniques to reduce the number of data. To support such predictions, we develop a model of the tremor, to perform tests determining the DBS reducing the tremor or inducing tolerance and <phrase>lesion</phrase> if the stimulation is chronic. <phrase>Parkinson's disease</phrase> (PD) is a serious <phrase>neurological disorder</phrase> with a large spectrum of symptoms (rest tremor, bradykinesia, muscular rigidity and postural instability). The neurons do not produce dopamine anymore or produce very low level of this chemical mediator, necessary on movement coordination. <phrase>Parkinson's disease</phrase> seems to occur in about 100-250 cases on 100 000 individuals. In <phrase>Europe</phrase> were reported about 1.2 million <phrase>Parkinson</phrase> patients [1]. The missing of a good clinical test, combined with the patient's reticence to attend a physician, make the diagnostic to be established very often too late. The accuracy of the diagnosis of PD varies from 73 to 92 percent depending on the clinical criteria used. Parkinsonian tremor is a rhythmic, involuntary muscular contraction characterized by oscillation of a part of the body. Initial symptoms include resting tremor beginning distally in one arm at a 4 – 6 Hz frequency. <phrase>Deep Brain Stimulation</phrase> (DBS) is an electric therapy approved by <phrase>FDA</phrase> (<phrase>Food and Drug Administration</phrase>) for the treatment of <phrase>Parkinson's disease</phrase> (PD) in 2002 and used now to treat the <phrase>motor symptoms</phrase> like <phrase>essential tremor</phrase>. It consists of a regular <phrase>high frequency</phrase> stimulation of specific subcortical sites involved in the movement-related neural patterns [1]. The exact <phrase>neurobiological</phrase> mechanism by which DBS exerts modulator effects on brain tissue are not yet full understood. It is unknown which part of the neuronal
A micromachined silicon multielectrode for multiunit recording. A 16-channel multielectrode was used to record propagating <phrase>action potentials</phrase> from multiple units in the ventral nerve cord of the <phrase>cricket</phrase> Gryllus bimaculatus. The multielectrode was fabricated using photolithographic and bulk silicon etching techniques. The fabrication differs from standard methods in its use of <phrase>deep reactive ion etching</phrase> (DRIE) to form the bulk electrode structure. This technique enables the fabrication of relatively thick (>100 microm), rigid structures whose top surface can have any form of thin film electronics. The multielectrode tested in this paper consists of 16 narrow silicon bridges, 150 microm wide and 350 microm tall, spaced evenly over a centimeter, with passive rectangular gold recording sites on the top surface. The nerve cord was placed perpendicularly across the bridges. In this geometry, the nerve spans a 350 microm deep, 450 microm wide trench between each recording site, permitting adequate isolation of recording sites from each other and a <phrase>platinum</phrase> <phrase>ground plane</phrase>. Spike templates for eight neurons were formed using principle component analysis and clustering of the concatenated multichannel waveforms. Clean templates were generated from a 40 s recording of stimulus evoked activity. Conduction velocities ranged from 2.59+/-0.05 to 4.99+/-0.12 m/s. Two limitations of <phrase>extracellular</phrase> electrode arrays are the resolution of overlapping spikes and relation of discriminated units to known <phrase>anatomy</phrase>. The <phrase>high density</phrase>, precise positioning, and controlled impedance of recording sites achievable in microfabricated devices such as this one will aid in overcoming these limitations. The rigid devices fabricated using this process offer stable positioning of recording sites over relatively large distances (several millimeters) and are suitable for clamping or squeezing of nerve cords.
Learning <phrase>long-range</phrase> vision for autonomous off-road driving Most vision-<phrase>based approaches</phrase> to <phrase>mobile robotics</phrase> suffer from the limitations imposed by stereo <phrase>obstacle detection</phrase>, which is short-range and prone to failure. We present a self-<phrase>supervised learning</phrase> process for <phrase>long-range</phrase> vision that is able to accurately classify complex terrain at distances up to the horizon, thus allowing superior strategic planning. The success of the <phrase>learning process</phrase> is due to the self-supervised <phrase>training data</phrase> that is generated on every frame: robust, visually consistent labels from a stereo module, normalized wide-context input windows, and a discrimina-tive and concise feature representation. A <phrase>deep hierarchical</phrase> network is trained to extract informative and meaningful features from an input image, and the features are used to train a realtime classifier to predict traversability. The trained classifier sees obstacles and paths from 5 to over 100 meters, far beyond the maximum stereo range of 12 meters, and adapts very quickly to new environments. The process was developed and tested on the LAGR <phrase>mobile robot</phrase>. Results from a <phrase>ground truth</phrase> dataset are given as well as field <phrase>test results</phrase>.
Thermo-mechanical Forming of Al–mg–si Alloys: Modeling and Experiments In an ongoing quest to realize lighter vehicles with improved <phrase>fuel efficiency</phrase>, deformation characteristics of the material AA 6016 is investigated. In the first part of this study, material behavior of Al–Mg–Si sheet <phrase>alloy</phrase> is investigated under different process (temperature and <phrase>strain rate</phrase>) and loading (uniaxial and biaxial) conditions experimentally. Later, warm cylindrical cup <phrase>deep drawing</phrase> experiments were performed to study the effect of various parameters on warm forming processes, such as the effect of punch velocity, holding time, temper and temperature on force-displacement response. The plastic anisotropy of the material which can be directly reflected by the earing behavior of the drawn cups has also been studied. <phrase>Finite element</phrase> simulations can be a powerful tool for the design of warm forming processes and tooling. Their accuracy will depend on the availability of material models that are capable of describing the influence of temperature and <phrase>strain rate</phrase> on the flow stresses. The physically based <phrase>Nes</phrase> model is used to describe the influence of temperature and <phrase>strain rate</phrase> and the Vegter yield criterion is used to describe the plastic anisotropy of the sheet. Experimental drawing <phrase>test data</phrase> are used to validate the modeling approaches.
<phrase>Face Recognition</phrase> Based on <phrase>Deep Neural Network</phrase> In modern life, we see more techniques of biometric features recognition have been used to our surrounding life, especially the applications in <phrase>telephones</phrase> and <phrase>laptops</phrase>. These <phrase>biometric recognition</phrase> techniques contain <phrase>face recognition</phrase>, <phrase>fingerprint recognition</phrase> and <phrase>iris recognition</phrase>. Our work focuses on the <phrase>face recognition</phrase> problem and uses a <phrase>deep learning</phrase> method, <phrase>convolutional neural network</phrase>, to solve it. And we use the Sobel operator to improve our result accuracy. LFW dataset is used for <phrase>training and testing</phrase> which gets a considerable result. And we also test our system on other face dataset, which also has a <phrase>high accuracy</phrase> on the recognition.
BIST for <phrase>Deep Submicron</phrase> ASIC Memories with <phrase>High Performance</phrase> Application Today's ASIC designs consist of more memory in terms of both area and number of instances. The shrinking of geometries has an even greater effect upon memories due to their tight layouts. These two trends are putting much greater demands upon <phrase>memory BIST</phrase> requirements. At-speed testing and custom test algorithms are becoming essential for insuring overall product quality. At-speed testing on memories that now operate in the 10 to 800 MHz range can be a challenge. Another demand upon <phrase>memory BIST</phrase> is determining the location of defects so that the cause can be diagnosed, or repaired with redundant cells. A tool and methodology that meets these difficult requirements is discussed.
A Comparative Evaluation of <phrase>Deep and Shallow</phrase> Approaches to the Automatic Detection of Common Grammatical Errors This paper compares a deep and a shallow processing approach to the problem of classifying a sentence as grammatically well-formed or ill-formed. The <phrase>deep processing</phrase> approach uses the XLE LFG parser and En-glish grammar: two versions are presented, one which uses the XLE directly to perform the classification, and another one which uses a <phrase>decision tree</phrase> trained on features consisting of the XLE's output statistics. The shallow processing approach predicts gram-maticality based on <phrase>n-gram</phrase> frequency statistics: we present two versions, one which uses frequency thresholds and one which uses a <phrase>decision tree</phrase> trained on the frequencies of the rarest n-grams in the input sentence. We find that the use of a <phrase>decision tree</phrase> improves on the basic approach only for the <phrase>deep parser</phrase>-<phrase>based approach</phrase>. We also show that combining both the shallow and deep <phrase>decision tree</phrase> features is effective. Our evaluation is carried out using a large <phrase>test set</phrase> of grammatical and ungrammatical sentences. The ungrammatical <phrase>test set</phrase> is generated automatically by inserting grammatical errors into well-formed <phrase>BNC</phrase> sentences.
<phrase>Pattern Selection</phrase> for Testing of <phrase>Deep Sub-Micron</phrase> <phrase>Timing Defects</phrase> Due to <phrase>process variations</phrase> in deep sub-micron (DSM) technologies, the effects of <phrase>timing defects</phrase> are difficult to capture. This paper presents a novel coverage metric for estimating the <phrase>test quality</phrase> with respect to <phrase>timing defects</phrase> under <phrase>process variations</phrase>. Based on the proposed metric and a dynamic timing analyzer, we develop a <phrase>pattern-selection</phrase> algorithm for selecting the minimal number of patterns that can achieve the maximal <phrase>test quality</phrase>. To shorten the runtime in dynamic <phrase>timing analysis</phrase>, we propose an algorithm to speed up the <phrase>Monte-Carlo</phrase>-based simulation. Our experimental results show that, selecting a small percentage of patterns from a multiple-detection <phrase>transition fault</phrase> pattern set is suf.cient to maintain the <phrase>test quality</phrase> given by the entire pattern set. We present <phrase>run-time</phrase> and accuracy comparisons to demonstrate the efficiency and effectiveness of our <phrase>pattern selection</phrase> framework.
A comparison of classical scheduling approaches in <phrase>power-constrained</phrase> <phrase>block-test</phrase> scheduling Classical scheduling approaches are applied here to overcome the problem of unequal-length block-<phrase>test scheduling</phrase> under <phrase>power dissipation</phrase> constraints. List scheduling-like approaches are proposed j r s t as greedy algorithms to tackle the fore mentioned problem. Then, distribution-<phrase>graph based</phrase> approaches are described in order to achieve balanced test concurrency and test <phrase>power dissipation</phrase>. An extended tree growing technique is also used in combination with these classical approaches in order to improve the test concurrency having assignedpower dissipation limits. A comparison between the results of the test scheduling experiments highlights the <phrase>advantages and disadvantages</phrase> of applying different classical scheduling algorithms to the <phrase>power-constrained</phrase> <phrase>test scheduling</phrase> problem. partitioned testing with run to completion defined in [2]. An efficient scheme for overlaying the block-tests, called extended tree growing technique is employed to model the fore mentioned problem. Thus, traditional <phrase>high-level</phrase> synthesis approaches (e.g. left-edge algorithm, list scheduling and distribution-<phrase>graph based</phrase> scheduling) can be employed together with this model to search for <phrase>power-constrained</phrase> <phrase>block-test</phrase> schedule profiles in a polynomial time. The algorithm fully exploits test parallelism under <phrase>power dissipation</phrase> constraints. This is achieved by overlaying the <phrase>block-test</phrase> intervals of compatible subcircuits to test as many of them as possible concurrently so that the maximum accumulated <phrase>power dissipation</phrase> does not go over the given limit. A constant additive model is employed for the <phrase>power dissipation</phrase> estimation throughout the algorithms. 1 INTRODUCTION <phrase>Power dissipation</phrase> has become a critical factor in the <phrase>normal operation</phrase> of nowadays' <phrase>deep-submicron</phrase> digital systems or under testing conditions. <phrase>VLSI circuits</phrase> running in <phrase>test mode</phrase> may consume more power than when running in <phrase>normal mode</phrase>. It is reported in [I] that one of the major considerations in <phrase>test scheduling</phrase> is the fact that heat dissipated during <phrase>test application</phrase> is typically <phrase>significantly higher</phrase> than the heat dissipated during the circuits' <phrase>normal operation</phrase> (sometimes 100-200% higher). <phrase>Test scheduling</phrase> is strongly related to test concurrency. Test concurrency is a design property which strongly impacts testability and power dis-sipation. To satisfy high <phrase>fault coverage</phrase> goals with reduced test application time under certain <phrase>power dissipation</phrase> constraints , the testing of all components on the system should be performed in parallel to the greatest extent possible. <phrase>Power-constrained</phrase> <phrase>test scheduling</phrase> will soon become a very <phrase>important issue</phrase> for the SOC designs as well. Therefore , this paper focuses on the high-level <phrase>power-constrained</phrase> <phrase>block-test</phrase> <phrase>scheduling problem</phrase> which lacks of practical solutions. The test scheduling discipline assumed here is the Paper 33.3 …
Numerical simulation of two-phase flow in deformable porous media: Application to <phrase>carbon dioxide</phrase> storage in the subsurface In this paper, <phrase>conceptual modeling</phrase> as well as numerical simulation of two-phase flow in deep, deformable geological formations induced by CO 2 injection are presented. The conceptual approach is based on balance equations for mass, <phrase>momentum</phrase> and energy completed by appropriate constitutive relations for the fluid phases as well as the solid matrix. Within the context of the primary effects under consideration, the fluid motion will be expressed by the extended <phrase>Darcy's law</phrase> for two phase flow. Additionally, constraint conditions for the partial saturations and the pressure fractions of <phrase>carbon dioxide</phrase> and <phrase>brine</phrase> are defined. To characterize the stress state in the solid matrix, the effective stress principle is applied. Furthermore, the interaction of fluid and solid phases is illustrated by constitutive models for <phrase>capillary</phrase> pressure, <phrase>porosity</phrase> and permeability as functions of saturation. Based on this <phrase>conceptual model</phrase>, a coupled system of nonlinear <phrase>differential equations</phrase> for two-phase flow in a deformable porous matrix (H 2 M model) is formulated. As the displacement vector acts as primary variable for the solid matrix, multiphase flow is simulated using both pressure/pressure or pressure/saturation formulations. An <phrase>object-oriented</phrase> <phrase>finite element method</phrase> is used to solve the multi-field problem numerically. The capabilities of the model and the numerical tools to treat complex processes during CO 2 <phrase>sequestration</phrase> are demonstrated on three benchmark examples: (1) a 1-D case to investigate the influence of variable fluid properties, (2) 2-D vertical axi-symmetric <phrase>cross-section</phrase> to study the interaction between hydraulic and deformation processes, and (3) 3-D to test the stability and computational costs of the H 2 M model for real applications.
A <phrase>comparative study</phrase> of probability collectives based <phrase>multi-agent systems</phrase> and <phrase>genetic algorithms</phrase> We compare <phrase>Genetic Algorithms</phrase> (GA's) with Probability Collectives (PC), a new framework for distributed optimization and control. In contrast to GA's, PC-<phrase>based methods</phrase> do not update populations of solutions. Instead they update an explicitly parameterized <phrase>probability distribution</phrase> <i>p</i> over the space of solutions. That updating of <i>p</i> arises as the optimization of a functional of <i>p</i>. The functional is chosen so that any <i>p</i> that optimizes it should be <i>p</i> peaked about good solutions. The PC approach has deep connections with both <phrase>game theory</phrase> and <phrase>statistical physics</phrase>. We review the PC approach using its motivation as the information theoretic formulation of <phrase>bounded rationality</phrase> for <phrase>multi-agent systems</phrase> (MAS). It is then compared with GA's on a diverse set of problems. To handle <phrase>high dimensional</phrase> surfaces, in the PC method investigated here <i>p</i> is restricted to a product distribution. Each distribution in that product is controlled by a separate agent. The test functions were selected for their difficulty using either traditional <phrase>gradient descent</phrase> or <phrase>genetic algorithms</phrase>. On those functions the PC-<phrase>based approach</phrase> significantly outperforms traditional GA's in both rate of descent, trapping in false minima, and <phrase>long term</phrase> optimization.
On the Relation of Speech to Language Opinion Opinion Opinion Opinion Opinion perception of the sounds that convey phonetic structure – one finds two very different views of its relation to language. The more conventional holds that speech is merely a vehicle, bearing no organic relationship to the linguistic baggage it carries. On that view, speech is produced and perceived by processes that are not specialized for language but rather serve horizontally the broadest possible variety of behaviors, linguistic and non-linguistic alike. The outcomes of those primary processes are then presumably sent on to language proper, a separate domain where they find the mental machinery capable of the heavy lifting required by <phrase>phonology</phrase>, morphology and syntax. Preferring a name that reflects the nature of a theory rather than its currency, we will call the conventional view 'horizon-tal' (as in Ref. 1). The other, less conventional view is that the biological roots of language run deep, penetrating even to the level of speech and to the primary motor and <phrase>perceptual</phrase> processes that are engaged there. Seen from that perspective, speech is a constituent of a vertically organized system, specialized from top to bottom for linguistic communication. Such a view may be appropriately called 'vertical'. To evaluate these strongly contrasting views, we propose here to determine which of the two provides the more coherent and plausible account when challenged by several simple and seemingly obvious biological considerations. As we will try to show, those considerations provide decisive tests of any theory of speech, yet they do not normally figure in the calculations of the theorists, nor have they been permitted to ruffle the implicit assumptions that guide (or misguide) almost all applied language research, including that which is aimed at determining how to convert fluency in speech to fluency in the use of its alphabetic transcription. Indeed, bringing those considerations to notice is one purpose of this paper. But first, we will give a brief description of the theories. The horizontal viewpoint The first assumption of the horizontal view is that the elements of speech are sounds. That is not merely to say the obvious, which is that speech exploits an acoustic medium, but rather to identify sounds as the primitives that are exchanged when linguistic communication occurs. The invariant acoustic patterns that might justify such an assumption have, indeed, been claimed for a variety of phonetic segments, including, for example , stops 2 , nasals 3 , and the voicing contrast of fricatives …
DeepBot: a focused crawler for accessing <phrase>hidden web</phrase> content The crawler engines of today cannot reach most of the information contained in the Web. A great amount of valuable information is "hidden" behind the query forms of online databases, and/or is dynamically generated by technologies such as <phrase>Javascript</phrase>. This portion of the web is usually known as the Deep Web or the <phrase>Hidden Web</phrase>. We have built DeepBot, a prototype of <phrase>hidden-web</phrase> focused crawler able to access such content. DeepBot receives a set of domain definitions as an input, each one describing a specific data-collecting task and automatically identifies and learns to execute queries on the forms relevant to them. In this paper we describe the techniques employed for building DeepBot and report the experimental results obtained when testing it with several <phrase>real world</phrase> <phrase>data collection</phrase> tasks.
Understanding the mind from an evolutionary perspective: an overview of <phrase>evolutionary psychology</phrase>. UNLABELLED The theory of evolution by <phrase>natural selection</phrase> provides the only scientific explanation for the existence of complex adaptations. The design features of the brain, like any <phrase>organ</phrase>, are the result of selection pressures operating over deep time. <phrase>Evolutionary psychology</phrase> posits that the <phrase>human brain</phrase> comprises a multitude of evolved psychological mechanisms, adaptations to specific and recurrent problems of survival and reproduction faced over human evolutionary history. Although some mistakenly view <phrase>evolutionary psychology</phrase> as promoting <phrase>genetic determinism</phrase>, <phrase>evolutionary psychologists</phrase> appreciate and emphasize the interactions between genes and environments. This approach to psychology has led to a richer understanding of a variety of psychological phenomena, and has provided a powerful foundation for generating novel hypotheses. Critics argue that <phrase>evolutionary psychologists</phrase> <phrase>resort</phrase> to <phrase>storytelling</phrase>, but as with any branch of science, empirical testing is a vital component of the field, with hypotheses standing or falling with the weight of the evidence. <phrase>Evolutionary psychology</phrase> is uniquely suited to provide a unifying theoretical framework for the disparate subdisciplines of psychology. An evolutionary perspective has provided insights into several subdisciplines of psychology, while simultaneously demonstrating the arbitrary nature of dividing psychological science into such subdisciplines. <phrase>Evolutionary psychologists</phrase> have amassed a substantial empirical and theoretical literature, but as a relatively new approach to psychology, many questions remain, with several promising directions for <phrase>future research</phrase>. For further resources related to this article, please visit the WIREs website.   CONFLICT OF INTEREST The authors have declared no conflicts of interest for this article.
Statistical Diagnosis for Intermittent <phrase>Scan Chain</phrase> Hold-Time Fault Intermittent <phrase>scan chain</phrase> hold-time fault is discussed in this paper and a method to diagnose the faulty site in a <phrase>scan chain</phrase> is proposed as well. Unlike the previous <phrase>scan chain</phrase> diagnosis methods that targeted <phrase>permanent faults</phrase> only, the proposed method targets both <phrase>permanent faults</phrase> and intermittent faults. Three ideas are presented in this paper. First an enhanced upper bound on the location of candidate faulty <phrase>scan cells</phrase> is obtained. Second a new method to determine a <phrase>lower bound</phrase> is proposed. Finally a statistical diagnosis algorithm is proposed to calculate the probabilities of the bounded set of candidate faulty <phrase>scan cells</phrase>. The proposed algorithm is shown to be efficient and effective for large industrial designs with multiple faulty <phrase>scan chains</phrase>. 1 Introduction The development of improved semiconductor <phrase>process technologies</phrase> and EDA tools continuously allow the realization of a more complex system on a single die with higher clock frequency and larger system density. However, several new problems, as described below, appear in designing, manufacturing and testing a System-On-a-Chip (SOC) with multi-million transistors using <phrase>deep-submicron</phrase> (DSM) technologies. (1) <phrase>Signal integrity</phrase> (SI) and design integrity (DI) issues become major challenges with the evolution to DSM. SI issues include crosstalk, IR drop, power and ground bounce, etc. DI issues include electron migration, hot <phrase>electrons</phrase>, wire self-heating, etc [1]. To increase density, interconnects on the chip come closer and are made narrower and thicker [2]. Hence the crosstalk is exacerbated by the increased inter-line capacitive and <phrase>inductive coupling</phrase>. Switching signals at a faster rate might cause ground bounce and lowering <phrase>supply voltages</phrase>, which lead to decreased <phrase>noise margins</phrase>. All the above-mentioned factors would make the signals on the chip more sensitive to unpredicted disturbances from either internal signal changes or external noises. These problems sometimes lead to incorrect functional behaviors, which are called intermittent faults. Unlike the <phrase>permanent faults</phrase> that are deterministically modeled by some known <phrase>fault models</phrase> (e.g. stuck-at or bridging), the intermittent faults are stochastically observed and difficult to be modeled. (2) At DSM levels of technologies, <phrase>statistical timing</phrase> analysis is the only way to account for wiring delay and <phrase>process variations</phrase> [3]. Therefore, instead of thinking that a signal transition happens at a fixed time, we now have to think the transition could happen within a small range of time with an estimated <phrase>probability distribution</phrase> density. For example, when we use traditional <phrase>static timing analysis</phrase>, we might know that Signal A …
DHSNet: <phrase>Deep Hierarchical</phrase> Saliency Network for Salient <phrase>Object Detection</phrase> Traditional 1 salient <phrase>object detection</phrase> models often use hand-crafted features to formulate contrast and various <phrase>prior knowledge</phrase>, and then combine them artificially. In this work, we propose a novel <phrase>end-to-end</phrase> <phrase>deep hierarchical</phrase> saliency network (DHSNet) based on <phrase>convolutional neural networks</phrase> for detecting salient objects. DHSNet first makes a coarse global prediction by automatically learning various global structured saliency cues, including global contrast, objectness, compactness, and their optimal combination. Then a novel hierarchical recurrent <phrase>convolutional neural network</phrase> (HRCNN) is adopted to further hierarchically and progressively refine the details of saliency maps <phrase>step by step</phrase> via integrating local context information. The whole architecture works in a global to local and coarse to fine manner. DHSNet is directly trained using whole images and corresponding <phrase>ground truth</phrase> saliency masks. When testing, saliency maps can be generated by directly and efficiently feedforwarding testing images through the network, without relying on any other techniques. Evaluations on four benchmark datasets and comparisons with other 11 state-of-the-art algorithms demonstrate that DHSNet not only shows its significant superiority in terms of performance, but also achieves a real-time speed of 23 <phrase>FPS</phrase> on modern <phrase>GPUs</phrase>.
A Fully Integrated Microneedle-based Transdermal <phrase>Drug Delivery</phrase> System The left picture on the front cover shows an integrated microneedle-based <phrase>drug delivery</phrase> system fabricated and used for experiments by the author. An array of microneedles is located on the other side of the device. The right picture on the cover shows a magnified view of these side-opened hollow microneedles, likewise fabricated by the author. The needles are designed to penetrate skin tissue using a low insertion force. The needles are fabricated by <phrase>deep reactive ion etching</phrase> of silicon and the length of the needles is 400 μm. ABSTRACT iii Abstract Patch-based transdermal <phrase>drug delivery</phrase> offers a convenient way to administer drugs without the drawbacks of standard hypodermic injections relating to issues such as patient acceptability and injection safety. However, conventional transdermal <phrase>drug delivery</phrase> is limited to therapeutics where the drug can diffuse across the skin barrier. By using miniaturized needles, a pathway into the <phrase>human body</phrase> can be established which allow transport of <phrase>macromolecular</phrase> drugs such as insulins or <phrase>vaccines</phrase>. These microneedles only penetrate the outermost skin layers, superficial enough not to reach the nerve receptors of the lower skin. Thus, microneedle insertions are perceived as painless. The <phrase>thesis presents</phrase> research in the field of microneedle-based <phrase>drug delivery</phrase> with the specific aim of investigating a microneedle-based <phrase>transdermal patch</phrase> concept. To enable controllable drug infusion and still maintain an unobtrusive and easy-to-use, patch-like design, the system includes a small active dispenser mechanism. The dispenser is based on a novel thermal <phrase>actuator</phrase> consisting of highly expandable micro-spheres. When actuated, the microspheres expand into a liquid <phrase>reservoir</phrase> and, subsequently , dispense stored liquid through outlet holes. The microneedles are fabricated in monocrystalline silicon by <phrase>Deep Reactive Ion Etching</phrase>. The needles are organized in arrays situated on a chip. To allow active delivery, the microneedles are hollow with the needle bore-opening located on the side of the needle. This way, the needle can have a sharp and well-defined needle tip. A sharp needle is a further requirement to achieve microneedle insertion into skin by hand. The <phrase>thesis presents</phrase> fabrication and evaluation of both the microneedle structure and the <phrase>transdermal patch</phrase> as such. Issues such as penetration reliability, liquid delivery into the skin and microneedle packaging are discussed. The microneedle patch was also tested and studied in vivo for <phrase>insulin</phrase> delivery. Results show that intradermal administration with microneedles give rise to similar <phrase>insulin</phrase> concentration as standard subcutaneous delivery with the same dose rate. iv …
Breadcrumb navigation: there's more to <phrase>hansel</phrase> and <phrase>gretel</phrase> than meets the eye In a popular <phrase>Grimm</phrase> fairytale, <phrase>Hansel</phrase> and <phrase>Gretel</phrase> are taken deep into the forest in the hope that they will not find their way out. However, clever <phrase>Hansel</phrase> has left a <phrase>trail</phrase> of breadcrumbs to show their return path. While the story inspires the term " breadcrumb navigation, " it also provides a metaphor of <phrase>Web usage</phrase> that is much stronger than many designers realize 1. An alarming number of sites (both Web and <phrase>intranet</phrase>) are still designed and usability-tested with the <phrase>home page</phrase> as the <phrase>starting point</phrase>. Yet there are over three billion searches conducted each month in the U.S. alone, averaging 32 per <phrase>search engine</phrase> user. While some of these searches may link to a homepage, the vast majority lead directly to a page deep within a site. This is the Internet equivalent of <phrase>Hansel</phrase> and Gretel's scenario and probably the most important reason for having breadcrumb navigation. Figure 1 is an example from a site selling electronic and <phrase>mechanical engineering</phrase> supplies. There are a number of reasons why breadcrumb navigation is better than the alternatives: • Providing only a link to the <phrase>home page</phrase> is inadequate for all but the simplest sites. Users are already in an area they find interesting. With a <phrase>home page</phrase> link you are making them leave the forest so they can find their way back to where they started. • Site navigation is often difficult to follow backwards—a little like trying to follow <phrase>trail</phrase> signs in the wrong direction. This is : / 79 i n t e r a c t i o n s / s e p t e m b e r + o c t o b e r 2 0 0 4 hci and the web hci and the web i n t e r a c t i o n s / s e p t e m b e r + o c t o b e r 2 0 0 4 hci and the web especially true when the location of each page being viewed is not reflected in the navigation itself (with a highlight or other marker of some type). • Site navigation usually does not (and probably should not) reflect the whole depth of a hierarchy on each page of a site. In the example above, the product page is at the sixth level in the hierarchy (or more if there …
An Efficient <phrase>Network on Chip</phrase> (noc) for a Parallel, <phrase>Low-power</phrase>, Low-area Homogeneous Many-core Dsp Platform an Efficient <phrase>Network on Chip</phrase> Targeted to a Parallel, <phrase>Low-power</phrase>, Low-area Homogenous Many-core Dsp Platform This <phrase>thesis presents</phrase> an NoC architecture that is optimized for a course-grained, deterministic many core DSP platform supporting up to 256 cores. The proposed network supports both local and long-distance communication in the event that large applications or multiple smaller applications are mapped onto the platform by means of a hierarchical cluster topology. The NoC is designed to optimize the area-and power-to-performance ratio through implementing the following key characteristics: low hop-count long distance communication, optimized flit buffer size, efficient <phrase>virtual channel</phrase> implementation, and a highly restricted <phrase>virtual channel</phrase> flow control. The NoC architecture is implemented in 65 nm <phrase>CMOS technology</phrase> with a nominal <phrase>supply voltage</phrase> of 1V. Place and Route results show that the proposed architecture saves up to 33% in area and up to 87.6% in energy-per-flit in comparison to some currently-implemented <phrase>NoCs</phrase>. Through several traffic <phrase>pattern tests</phrase> on a network of 16 cores, the NoC attains a throughput of up to 21.7Gbps. A 256-point FFT mapped onto 16 cores executes in 4.3µs and dissipates 0.649W. This is an improvement of 46% and 81% in latency and <phrase>power dissipation</phrase> over a 256-point <phrase>Xilinx</phrase> FFT IP Core implemented on a Virtex 6 FPGA. Dedication It goes without saying that this thesis is dedicated to the wide and talented academic community in hopes to propagate its research endeavors and its cause. However, without two people in particular, I'm not sure I would have made it this far. They are my two grandfathers, Thomas " Pappy " Chandler and Charles " <phrase>Pop</phrase>-<phrase>pop</phrase> " <phrase>Vogt</phrase>. They always took a deep interest in my endeavors, encouraging me to take them further. They pushed me. They provided moral and motivational support. They were there for me. They were always proud of me (I'm sure I'll figure that out when I have children or grand-children of my own). Without dragging this on too much longer, I dedicate this thesis to Thomas Chandler and Charles <phrase>Vogt</phrase>. I miss you both. The journey doesn't stop here. Thank you. iii Acknowledgments I owe thanks and gratitude to all who have made it possible for my completion of this thesis. First, I want to thank my advisor, Dr. Tinoosh Mohsenin, for her direction throughout the last year and a half. Her experience in the field and vast library of publications have most definitely helped me complete this work. Thank you for the opportunity to work on this challenging and, more …
Instance-Based Classification by Emerging Patterns Emerging patterns (EPs), namely <phrase>itemsets</phrase> whose supports change significantly from one class to another, capture discriminating features that sharply contrast instances between the classes. Recently, <phrase>EP</phrase>-based classifiers have been proposed, which first mine as many EPs as possible (called eager-learning) from the training data and then aggregate the discriminating power of the mined EPs for classifying new instances. We propose here a new, instance-based classifier using EPs, called DeEPs, to achieve much better accuracy and efficiency than the <phrase>previously proposed</phrase> <phrase>EP</phrase>-based classifiers. <phrase>High accuracy</phrase> is achieved because the instance-<phrase>based approach</phrase> enables DeEPs to pinpoint all EPs relevant to a test instance, some of which are missed by the eager-<phrase>learning approaches</phrase>. High efficiency is obtained using a series of data reduction and concise data-representation techniques. Experiments show that DeEPs' decision time is linearly scalable over the number of training instances and nearly linearly over the number of attributes. Experiments on 40 datasets also show that DeEPs is superior to other classifiers on accuracy.
Development of hybrid integrated endoscope-holder system for <phrase>endoscopic</phrase> microneurosurgery. OBJECTIVE <phrase>Endoscopic</phrase> techniques in the field of neurosurgery are under development. To perform sophisticated bimanual procedures in the delicate surgical fields of neurosurgery, rigid endoscope fixation devices with accurate locking and a safe <phrase>releasing</phrase> system are required. Here, we report the development of a new hybrid integrated endoscope-holder system.   INSTRUMENTATION The basic concepts of the holding device were as follows: 1) it should combine both video system and holding device; 2) it should have easy maneuverability and accurate fixation and be equipped with a safe <phrase>releasing</phrase> mechanism; and 3) it should be able to be used universally either in a primary or an assisting <phrase>endoscopic</phrase> procedure. A negatively actuated air-locking system and a <phrase>bayonet</phrase>-shaped endoscope were <phrase>newly developed</phrase>. <phrase>Clinical trials</phrase> of 25 patients were performed, and each prototype was tested and modified until the <phrase>functional requirements</phrase> were fulfilled. The final version was tested for accuracy and security in fixation and <phrase>releasing</phrase> mechanism.   RESULTS Eight problems were encountered in the <phrase>clinical trials</phrase> and improved. Accuracy in fixation of the final version was superior to the most advanced endoscope-holder on the market. No dangerous events were observed during repetitive insertion, fixation, and release in the simulated deep surgical field in the cadaveric skull. No apparent complications were noted in the clinical application.   CONCLUSION We have developed a highly reliable, accurately fixable, and easily maneuverable hybrid endoscope-holder system. To achieve a safer, more accurate, and less invasive surgery in the current socioeconomic demands, commercial manufacturers and surgeons in multiple centers need to combine their efforts to create useful techniques.
Editorial: <phrase>Alan Turing</phrase> and <phrase>Artificial Intelligence</phrase> His [Turing's] point was that we should not be species-chauvinistic, or <phrase>anthropocentric</phrase>, about the insides of an intelligent being, for there might be inhuman ways of being intelligent. was one of the most eminent scientists of the 20th century (Figure 1). His research was a central <phrase>catalyst</phrase> of the computer <phrase>revolution</phrase>. The concept of a <phrase>Turing machine</phrase>, which he developed in the 1930s, is still one of the most widely used models of computation in theoretical <phrase>computer science</phrase>, but this monumental contribution was only the first of many. His biographer <phrase>Andrew Hodges</phrase> – author of Hodges (1992, 1997), and main-tainer of the " <phrase>Alan Turing</phrase> <phrase>Home Page</phrase> " – divides Turing's publications into five areas: <phrase>mathematical logic</phrase>, mechanical intelligence, <phrase>pure mathematics</phrase>, <phrase>morphogen</phrase>-esis, and <phrase>cryptanalysis</phrase>. Moreover, in Sir Roger Penrose's words, Turing was also " a deep and influential <phrase>philosopher</phrase> in addition to his having made contributions to mathematics, technology and code-breaking that profoundly contribute to our present-day well being " (Hodges, 1998). In a landmark article published in October 1950 in the philosophy journal Mind (Figure 2) Turing made a famous assertion. He predicted that by the year 2000 it would be feasible to write a program that would, after five minutes of questioning , have at least a 30% chance of fooling an average conversational partner into believing it was a human being (Turing, 1950). As Charniak and McDermott (1985: 10) remark " Actually, the [Mind] paper makes it sound as if Turing had in mind the computer pretending to be a woman in the man/woman game, but the point is not completely clear, and most have assumed that he intended the test to be a person/computer one, and not woman/computer. " See Saygin et al. (1999) for an attempt at clarification.
<phrase>Quantum Gravity</phrase> with a Positive <phrase>Cosmological Constant</phrase> A <phrase>quantum theory</phrase> of gravity is described in the case of a positive <phrase>cosmological constant</phrase> in 3 + 1 dimensions. Both old and new results are described, which support the case that <phrase>loop quantum gravity</phrase> provides a satisfactory <phrase>quantum theory</phrase> of gravity. These include the existence of a <phrase>ground state</phrase>, discoverd by Kodama, which both is an exact solution to the constraints of <phrase>quantum gravity</phrase> and has a semiclassical limit which is deSitter space-time. The long wavelength excitations of this state are studied and are shown to reproduce both gravitons and, when matter is included, <phrase>quantum field theory</phrase> on deSitter <phrase>spacetime</phrase>. Furthermore, one may derive directly from the Wheeler-deWitt equation corrections to the energy-<phrase>momentum</phrase> relations for matter fields of the form E 2 = p 2 +m 2 +αl P l <phrase>E 3</phrase> +.. . where α is a <phrase>computable</phrase> <phrase>dimensionless</phrase> constant. This may lead in the next few years to <phrase>experimental tests</phrase> of the theory. To study the excitations of the Kodama state exactly requires the use of the spin network representation, which is quantum deformed due to the <phrase>cosmological constant</phrase>. The theory may be developed within a single horizon, and the boundary states described exactly in terms of a boundary Chern-Simons theory. The Bekenstein bound is recovered and the N bound of Banks is given a background independent explanation. The paper is written as an introduction to <phrase>loop quantum gravity</phrase>, requiring no <phrase>prior knowledge</phrase> of the subject. The deep relationship between <phrase>quantum gravity</phrase> and topological <phrase>field theory</phrase> is stressed throughout.
Critical Insights into <phrase>Nhs</phrase> <phrase>Information Systems</phrase> Deployment AbstrAct This chapter discusses a systems methodology called strategic assumption surfacing and testing (SAST) that was used to understand the design and deployment of <phrase>information systems</phrase> in the healthcare context. It is based on the experiences of conducting SAST with a group of healthcare professionals, working in the <phrase>National Health Service</phrase> (<phrase>NHS</phrase>) in <phrase>England</phrase>. This application of SAST in the <phrase>NHS</phrase> setting highlighted deep <phrase>politico</phrase>-cultural concerns in the organizational setting, and it helped towards the conception of a <phrase>normative</phrase> inclusive approach for health infor-matics design and deployment. This approach introduces the understanding that the development of <phrase>information systems</phrase> in healthcare is a complex agenda, the success of which demands the active involvement of all stakeholders through all the key stages of the process. Critical perspectives on SAST have also been considered and the assumptions fostered towards arriving at the conclusions, have been highlighted.
An Efficient <phrase>Speaker Diarization</phrase> Using Privacy Preserving <phrase>Audio Features</phrase> Based of Speech/non Speech Detection — <phrase>Privacy-sensitive audio features</phrase> for speaker diarization in multiparty conversations: i.e., a set of <phrase>audio features</phrase> having low <phrase>linguistic information</phrase> for speaker diarization in a single and multiple distant microphone scenarios is a challenging research field in now-a-days. Existing system used a supervised framework using deep neural architecture for deriving <phrase>privacy-sensitive audio features</phrase>. In proposed system Patterns of speech/nonspeech detection (SND) is utilized for <phrase>privacy-sensitive audio</phrase> feature to capture <phrase>real-world</phrase> audio. SND and diarization can then be used to analyze <phrase>social interactions</phrase>. In this research privacy preserving <phrase>audio features</phrase> has been investigated instead for recording and storage that can respect privacy by minimizing the amount of <phrase>linguistic information</phrase>, whereas achieving modern performance in conversational <phrase>speech processing</phrase> tasks. Certainly, the main contribution of the proposed system is the achievement of state-of-the-art performances in speech/nonspeech detection and <phrase>speaker diarization</phrase> tasks using such features, which we refer to, as <phrase>privacy-sensitive</phrase>. In addition a comprehensive analysis of these features has been provided for the two tasks in a variety of conditions, such as indoor (predominantly) and outdoor audio. To objectively evaluate the notion of privacy, the proposed system use <phrase>automatic speech recognition</phrase> tests, with <phrase>higher accuracy</phrase> in either being interpreted as yielding lower privacy. I. INTRODUCTION Speech is acoustic signal which contains information of idea that is formed in speaker's mind. Speech is bimodal in nature [1] [2]. <phrase>Speech processing</phrase> can be performed at different three levels. Signal level processing considers the <phrase>anatomy</phrase> of human auditory system and process signal in form of small chunks called frames [3]. In phoneme level processing, speech <phrase>phonemes</phrase> are acquired and processed. The major work of <phrase>speech recognition</phrase> has been explored to analyse the <phrase>social interactions</phrase> using multimodal sensors with an emphasis on audio. Analysis of conversations can then carried by modeling the speech/speaker activities produced by a <phrase>speaker diarization</phrase> system. The objective of <phrase>speaker diarization</phrase> is to segment an audio recording into speaker-homogeneous regions whereas the output of a diarization system may appear to be restrictive, there are a growing number of applications
Putting Trust in Software Code Chris Wysopal is <phrase>director</phrase> of development at <phrase>Symantec Corporation</phrase>, where he leads research on how to build and test software for security vulnerabilities. the co-father of UNIX, wrote a paper about the quandary of not being able to trust code that you didn't create yourself. The paper, " Reflections on Trusting Trust, " 1 details a novel approach to attacking a system. Thompson inserts a back door into the UNIX login program when it is compiled and shows how the compiler can do this in a way that can't be detected by auditing the compiler <phrase>source code</phrase>. He writes: " You can't trust code that you did not totally create yourself. No amount of source-level verification or scrutiny will protect you from using <phrase>untrusted</phrase> code. In demonstrating the possibility of this kind of attack, I picked on the C compiler. I could have picked on any program-handling program such as an <phrase>assembler</phrase>, a loader, or even hardware <phrase>microcode</phrase>. " Twenty years after Thompson' s seminal paper was published, developments in the field of automated binary analysis of executable code are tackling the problem of trusting code you didn't write. Binary analysis can take on a range of techniques, from building call trees and looking for external function calls to full decompilation and modeling of a program' s <phrase>control flow</phrase> and <phrase>data flow</phrase>. The latter, which I call deep binary analysis, works by reading the executable <phrase>machine code</phrase> and building a language-neutral representation of the program' s behavior. This model can be traversed by automated scans to find security vulnerabilities caused by coding errors and to find many simple back doors. A <phrase>source code</phrase> emitter can then take the model and generate a human-readable <phrase>source code</phrase> representation of the pro-gram' s behavior. This enables manual code auditing for design-level <phrase>security issues</phrase> and subtle back doors that will typically escape automated scans. The steps of the decompilation process are as follows: 1. <phrase>Front end</phrase> decodes binary to <phrase>intermediate language</phrase>. 2. <phrase>Data flow</phrase> transformer reconstructs variable lifetimes and type information. 3. <phrase>Control flow</phrase> transformer reconstructs loops, condi-tionals, and exceptions. 4. Back end performs language-specific transformation and exports <phrase>high-level</phrase> code. To be useful the model must have a query engine that can answer questions for security scanning scripts:
Easily Testable and <phrase>Fault-tolerant</phrase> Fft <phrase>Butterfly</phrase> Networks —With the advent of deep submicron <phrase>very large scale integration</phrase> technology, the integration of a large <phrase>fast-Fourier-transform</phrase> (FFT) network into a single chip is becoming possible. However, a practical FFT chip is normally very big, so effective testing and fault-tolerance techniques usually are required. In this paper, we first propose a C-testable FFT network design. Only 20 <phrase>test patterns</phrase> are required to cover all combinational single-cell faults and interconnect stuck-at and break faults for the FFT network, regardless of its size. A spare-row based <phrase>fault-tolerant</phrase> FFT network design is subsequently proposed. Compared with <phrase>previous works</phrase>, our approach shows higher reliability and lower <phrase>hardware overhead</phrase>, and only three <phrase>bit-level</phrase> cell types are needed for repairing a faulty row in the multiply–subtract–add module. Also, special cell design is not required to implement the reconfiguration scheme. The <phrase>hardware overhead</phrase> for the testable design is low—about 4% for 16-bit numbers, regardless of the FFT network size.
The CSIRO <phrase>enterprise search</phrase> <phrase>test collection</phrase> This article describes a new TREC Enterprise Track search <phrase>test collection</phrase> -- CERC. The collection is designed to represent some <phrase>real-world</phrase> search activity within the enterprise, using as a specific example the <phrase>Commonwealth Scientific and Industrial Research Organisation</phrase> (CSIRO). It has a deep crawl of CSIRO's public-facing information, that is very similar to the crawl of a real-world search service provided by CSIRO. The search tasks are based on the activities of CSIRO Science Communicators, who are CSIRO employees that deal with public-facing information. Topics and judgments are tied to the Science Communicators in various ways, for example by involving them in the topic development process. The overall approach is to enhance the validity of the <phrase>test collection</phrase> as a model of <phrase>enterprise search</phrase>, by tying it to <phrase>real-world</phrase> examples.
Design, <phrase>Development and Testing</phrase> of <phrase>Underwater Vehicles</phrase>: ITB Experience The last decade has witnessed increasing worldwide interest in the research of underwater robotics with particular focus on the area of autonomous <phrase>underwater vehicles</phrase> (AUVs). The underwater robotics technology has enabled human to access the depth of the ocean to conduct environmental surveys, resources mapping as well as scientific and military missions. This capability is especially valuable for countries with major water or oceanic resources. As an archipelagic nation with more than 13,000 islands, <phrase>Indonesia</phrase> has one of the most abundant living and non-organic oceanic resources. The needs for the mapping, exploration, and environmental preservation of the vast marine resources are therefore imperative. The challenge of the <phrase>deep water</phrase> exploration has been the complex issues associated with hazardous and unstructured undersea and sea-bed environments. The <phrase>paper reports</phrase> the design, <phrase>development and testing</phrase> efforts of <phrase>underwater vehicle</phrase> that have been conducted at Institut Teknologi <phrase>Bandung</phrase>. Key technology areas have been identified and <phrase>step-by-step</phrase> development is presented in conjunction with the need to meet the challenge of <phrase>underwater vehicle</phrase> operation. A number of <phrase>future research</phrase> directions are also highlighted.
Preference Constructors for Deeply Personalized Database Queries Preference Constructors for Deeply Personalized Database Queries Deep <phrase>personalization</phrase> of database queries requires a semantically rich, easy to handle and flexible preference model. Building on preferences as strict partial orders we provide a variety of intuitive and customizable base preference constructors for numerical and categorical data. For complex constructors we introduce the notion of 'substitutable values' (SV-semantics). Preferences with SV-semantics solve major open problems with <phrase>Pareto</phrase> and prioritized preferences. Known laws from preference <phrase>relational algebra</phrase> remain valid under SV-semantics. These powerful modeling capabilities even contribute to improve efficient preference query evaluation. Moreover, for the first time we point out a semantic guided way to cope with the infamous flooding effect of query engines. Performing a series of test queries on sample data from an <phrase>e-procurement</phrase> application, we provide evidence that the flooding problem comes under control for deeply personalized database queries.
On the intersubject <phrase>generalization ability</phrase> in extracting kinematic information from afferent <phrase>nervous</phrase> signals In the recent past, many efforts have been carried out in order to evaluate the feasibility of implementing <phrase>closed-loop</phrase> controlled neuroprostheses based on the processing of sensory electroneurographic (ENG) signals. The success of these techniques mostly relies on the development of processing algorithms capable of extracting the necessary kinematic information from these signals. <phrase>Soft-computing</phrase> algorithms can be very useful when dealing with the complexity of the <phrase>neuromuscular</phrase> system because of their <phrase>generalization ability</phrase> and model-free structure. In this paper, these techniques were used to extract angular position information from the ENG signals recorded from muscle afferents in <phrase>animal model</phrase> using cuff electrodes. Specifically, a genetic algorithm-based dynamic nonsingleton <phrase>fuzzy logic</phrase> system (named GA-DNSFLS) was developed and tested on different types of angular trajectories (characterized by small or large angular excursions). In particular, two different <phrase>Takagi-Sugeno</phrase>-Kang (TSK)-like structures were used in the consequent part of the neuro-fuzzy model in order to verify which one could improve the generalization abilities (intrasubject and intersubject). The <phrase>results showed</phrase> that the GA-DNSFLS was able to reconstruct the trajectories giving interesting results in terms of correlation between the actual and the predicted trajectories for small excursion movements during intrasubject and intersubject tests. Particularly, one of the TSK models showed better results in terms of intersubject generalization. The simulations conducted with the large excursion movements led in some cases to interesting results but further experiments are necessary in order to analyze this point more in deep.
Project Quality of Off-shore <phrase>Virtual Teams</phrase> Engaged in Software <phrase>Requirements Analysis</phrase>: an Exploratory <phrase>Comparative Study</phrase> Project Quality of Off-shore <phrase>Virtual Teams</phrase> Engaged in Software <phrase>Requirements Analysis</phrase> AbstrAct The offshore <phrase>software development</phrase> companies in countries such as India use a global delivery model in which initial requirement analysis phase of software projects get executed at client locations to leverage frequent and deep interaction between user and developer teams. Subsequent phases such as design, coding and testing are completed at offshore locations. Emerging trends indicate an increasing interest in off-shoring even <phrase>requirements analysis</phrase> phase using <phrase>computer mediated communication</phrase>. We conducted an exploratory research study involving students from Management Development Institute (MDI), India and <phrase>Marquette University</phrase> (MU), <phrase>U.S.A</phrase>. to determine quality of such off-shored <phrase>requirements analysis</phrase> projects. Our <phrase>findings suggest</phrase> that project quality of teams engaged in pure offshore mode is comparable to that of teams engaged in collocated mode. However, the effect of controls such as user project monitoring on the quality of off-shored projects needs to be studied further.
Post-placement temperature reduction techniques With technology scaled to <phrase>deep submicron era</phrase>, temperature and <phrase>temperature gradient</phrase> have emerged as important design criteria. We propose two post-placement techniques to reduce peak temperature by intelligently allocating <phrase>whitespace</phrase> in the hotspots. Both methods are fully compliant with commercial technologies, and can be easily integrated with state-of-the-art thermal-aware <phrase>design flow</phrase>. Experiments in a set of tests on circuits implemented in <phrase>STM</phrase> 65nm technologies show that our methods achieve better peak temperature reduction than directly increasing circuit's area.
Detecting spammers and content promoters in online video <phrase>social networks</phrase> A number of online video <phrase>social networks</phrase>, out of which YouTube is the most popular, provides features that allow users to post a video as a response to a discussion topic. These features open opportunities for users to introduce polluted content, or simply pollution, into the system. For instance, <i>spammers</i> may post an unrelated video as response to a popular one aiming at increasing the likelihood of the <i>response</i> being viewed by a larger number of users. Moreover, opportunistic users--<i>promoters</i>--may try to gain visibility to a specific video by posting a large number of (potentially unrelated) responses to boost the rank of the <i>responded video</i>, making it appear in the top lists maintained by the system. Content pollution may jeopardize the trust of users on the system, thus compromising its success in promoting <phrase>social interactions</phrase>. In spite of that, the available literature is very limited in providing a deep understanding of this problem.  In this paper, we go a step further by addressing the issue of detecting video <phrase>spammers and promoters</phrase>. Towards that end, we manually build a <phrase>test collection</phrase> of real YouTube users, classifying them as spammers, promoters, and legitimates. Using our <phrase>test collection</phrase>, we provide a characterization of social and content attributes that may help distinguish each user class. We also investigate the feasibility of using a state-of-the-art supervised classification algorithm to detect <phrase>spammers and promoters</phrase>, and assess its effectiveness in our <phrase>test collection</phrase>. We found that our approach is able to correctly identify the majority of the promoters, misclassifying only a small percentage of legitimate users. In contrast, although we are able to detect a significant fraction of spammers, they showed to be much harder to distinguish from legitimate users.
Teaching <phrase>motion planning</phrase> concepts to <phrase>undergraduate students</phrase> —<phrase>Motion planning</phrase> is a central problem in robotics. Although it is an engaging topic for <phrase>undergraduate students</phrase>, it is difficult to teach, and as a result, the material is often only covered at an abstract level. <phrase>Deep learning</phrase> could be achieved by having students implement and test different algorithms. However, there is usually no time within a single class to have students completely implement several <phrase>motion planning</phrase> algorithms as they require the development of many lower-level <phrase>data structures</phrase>. We present an ongoing project to develop a teaching module for robotic <phrase>motion planning</phrase> centered around an integrated software environment. The module can be taught early in the undergraduate curriculum, after students have taken an introductory programming class.
Tele-Operated Echography and Remote Guidance for Performing Tele-Echography on Geographically Isolated Patients OBJECTIVE To evaluate the performance of three tele-echography systems for routine use in isolated medical centers.   METHODS Three systems were used for deep (<phrase>abdomen</phrase>, <phrase>pelvis</phrase>, <phrase>fetal</phrase>) and superficial (muscle, <phrase>thyroid</phrase>, <phrase>carotid</phrase> artery) examinations: (a) a <phrase>robotic arm</phrase> (RA) holding an echographic probe; (b) an echograph with a motorized probe (MP); and (c) remote guidance (RG) where the patient site operator performed the examination assisted by an expert via <phrase>videoconference</phrase>. All systems were tested in the same medical center located 60 km away from the university hospital.   RESULTS A total of 340 remote echography examinations were performed (41% RA and MP, 59% RG). MP and RA allowed full control of the probe orientation by the expert, and provided diagnoses in 97% of cases. The use of RG was sufficient for superficial vessel examinations and provided diagnoses in 98% of cases but was not suited for deep or superficial organs. Assessment of superficial organs was best accomplished using the MP.   DISCUSSION Both teleoperated systems provided control of the probe orientation by the expert necessary for obtaining appropriate views of deep organs but the MP was much more ergonomic and easier to use than the RA. RG was appropriate for superficial vessels while the MP was better for superficial volumic organs.
<phrase>Coaxial</phrase> Needle Insertion Assistant With Enhanced <phrase>Force Feedback</phrase> Many medical procedures involving needle insertion into soft tissues, such as anesthesia, <phrase>biopsy</phrase>, <phrase>brachytherapy</phrase>, and placement of electrodes, are performed without image guidance. In such procedures, <phrase>haptic</phrase> detection of changing tissue properties at different depths during needle insertion is important for needle localization and detection of subsurface structures. However, changes in tissue <phrase>mechanical properties</phrase> <phrase>deep inside</phrase> the tissue are difficult for human operators to sense, because the relatively large friction force between the needle shaft and the surrounding tissue masks the smaller tip forces. A novel robotic <phrase>coaxial</phrase> needle insertion assistant, which enhances operator force perception, is presented. This one-<phrase>degree-of-freedom</phrase> cable-driven robot provides to the operator a scaled version of the force applied by the needle tip to the tissue, using a novel design and sensors that separate the needle tip force from the shaft friction force. The ability of human operators to use the robot to detect membranes embedded in artificial <phrase>soft tissue</phrase> was tested under the conditions of 1) tip force and shaft <phrase>force feedback</phrase>, and 2) tip force only feedback. The ratio of successful to unsuccessful membrane detections was <phrase>significantly higher</phrase> (up to 50%) when only the needle tip force was provided to the user.
The Limits of <phrase>Speculative</phrase> Trace Reuse on Deeply Pipelined Processors Trace reuse improves the performance of processors by skipping the execution of sequences of redundant instructions. However, many reusable traces do not have all of their inputs ready by the time the reuse test is done. For these cases, we developed a new technique called Reuse through <phrase>Speculation</phrase> on Traces (RST), where trace inputs may be predicted. This paper studies the limits of RST for modern processors with deep pipelines, as well as the effects of constraining resources on performance. We show that our approach reuses more traces than the non-<phrase>speculative</phrase> trace reuse technique , with speedups of 43% over a non-<phrase>speculative</phrase> trace reuse and 57% when <phrase>memory accesses</phrase> are reused.
Acoustic imaging and visualization of plumes discharging from black smoker vents on the deep seafloor Visualization and quantification methods are being developed to analyze our acoustic images of thermal plumes containing metallic <phrase>mineral</phrase> particles that discharge from <phrase>hot springs</phrase> on the deep seafloor. The acoustic images record intensity of backscattering from the particulate matter suspended in the plumes. The visualization methods extract, classify, visualize, measure and track reconstructions of the plumes, depicted by isointensity surfaces as 3D volume objects and 2D slices. The parameters measured, including plume volume , cross sectional area, centerline location (trajec-<phrase>tory</phrase>), surface area and isosurfaces at percentages of maximum <phrase>backscatter</phrase> intensity, are being used to derive elements of plume behavior including expansion with height, dilution, and mechanisms of entrainment of surrounding <phrase>seawater</phrase>. Our aim is to compare the observational data with predictions of plume theory to test and advance models of the behavior of hydrothermal plumes through the use of <phrase>multiple representations</phrase>. ments; and laboratory tank simulations. Of these various approaches, acoustic methods are particularly well suited to synoptically image 3D hydrothermal plumes since they are large (linear dimensions 10'-103m) and contain suspended particulate matter. Acoustic imaging of black smoker plumes is based on Rayleigh backscattering from the metallic <phrase>mineral</phrase> particles suspended in the entire volume of the plume. The particles are small (microns) relative to the wavelength of the acoustic frequency employed (wlcm at 330 kHz; [3]). Our initial calculations indicated that this type suspended particulate matter could be detected at ranges of up to hundreds of meters using acoustic frequencies of hundreds of kHz [3]. We modified an existing <phrase>sonar</phrase> system to calculate specifications (frequency 330 kHz; pulse duration 100 microseconds; transmit power level of 220 dB).
Using Formal Tools to Study Complex Circuits Behaviour We use a formal tool to extract Finite <phrase>State Machines</phrase> (FSM) based representations (lists of states and transitions) of <phrase>sequential circuits</phrase> described by <phrase>flip-flops</phrase> and gates. These complete and optimized representations helps the designer to understand the accurate behaviour of the circuit. This <phrase>deep understanding</phrase> is a prerequisite for any verification or <phrase>test process</phrase>. An example is fully presented to illustrate our method. This simple pipelined processor comes from our experience in computer architecture and <phrase>digital design</phrase> education. ([2])
Designing <phrase>Binding Pockets</phrase> on Protein Surfaces using the A* Algorithm The in silico design of <phrase>ligands</phrase> binding to the protein surface instead of deep <phrase>binding pockets</phrase> is still a great challenge. Representative examples are small <phrase>molecules</phrase> that target <phrase>protein-protein interactions</phrase> [1]. The unbound experimental protein structures often lack appropriate <phrase>binding pockets</phrase> and thus standard virtual screening techniques will fail. We previously presented a pocket detection protocol that provides a <phrase>starting point</phrase> for in silico <phrase>drug design</phrase> for such cases [2]. Unfortunately, the underlying <phrase>molecular dynamics</phrase> simulations make this protocol quite time-consuming. However, if the potential <phrase>binding site</phrase> of a <phrase>ligand</phrase> is known, conformational sampling focused on this region appears more promising than scanning the whole protein surface for transient pockets. Here, we present two new algorithms for designing tailored lig-and <phrase>binding pockets</phrase> on the protein surface that account for backbone and side chain flexibility. At first, a prede-fined region of the protein surface is scanned for potential pocket positions using a program named PocketScanner. This program minimizes the protein energetically in the presence of generic pocket spheres representing the <phrase>binding pockets</phrase> whose positions remain fixed. The side chains of the relaxed protein conformations are then further refined by a second program named PocketBuilder that searches for the best combination of side chain rotamers using the A* algorithm. The approach was tested on the proteins <phrase>BCL</phrase>-XL, <phrase>IL-2</phrase>, and <phrase>MDM2</phrase> which are involved in <phrase>protein-protein interactions</phrase> and hence challenging drug targets. Although the native <phrase>ligand</phrase> binding pocket was not or not fully open in the unbound <phrase>crystal structure</phrase>, PocketScanner and PocketBuilder successfully generated conformations with pockets into which the known inhib-itors could be docked in orientations similar to those seen in the inhibitor bound <phrase>crystal</phrase> structures.
A New Hyperheuristic Algorithm for Cross-domain Search Problems <phrase>Diplom</phrase>-ingenieur <phrase>Computational Intelligence</phrase> a New Hyperheuristic Algorithm for Cross-domain Search Problems <phrase>Diplom</phrase>-ingenieur <phrase>Computational Intelligence</phrase> Erklärung Zur Verfassung Der Arbeit (Ort, Datum) (Unterschrift Verfasser) i Acknowledgements I would like to express my deep gratitude to my advisor, Priv.-Doz. Dr. Nysret Musliu, who guided me with great patience and encouragement throughout the process of this work. Without him, I would not have developed my keen interest in the area of AI <phrase>problem solving</phrase> and without his competent guidance, I could never have finished this work. I am also very grateful for the care and support from my loving parents who made it possible for me to enjoy an academic education. Finally, I would like to thank my partner, for she bore with me in difficult times, gave me strength through her love and never lost <phrase>faith</phrase> in me. iii Abstract Hyperheuristics are an emergent area of search methodologies which try to address computationally hard problems at a new <phrase>level of abstraction</phrase>. Instead of having a single algorithm that is optimised to perform well on a certain class of <phrase>problem instances</phrase>, hyperheuristics try to leverage the power of a whole set of problem specific heuristic algorithms. By combining and parametrising these heuristics or heuristic components in different ways, the overall algorithm should be able to perform better over a wider range of <phrase>problem instances</phrase>. Ideally, a hyperheuristic does not need to know anything about the problem class it operates on and only very little about the available heuris-tics. Because of this, hyperheuristics can often be applied without modifications to whole new problem domains and therefore represent a further step towards a very general <phrase>problem solving</phrase> algorithm. This thesis describes a new hyperheuristic algorithm that was specifically designed to be completely problem domain independent and which works at a clearly defined <phrase>level of abstraction</phrase>. The algorithm is structured in different search phases that try to balance intensification and diversification of the search process. The available <phrase>low-level</phrase> heuristics are evaluated repeatedly in terms of certain properties and ranked according to a quality based metric that was found to work well across all tested <phrase>low-level</phrase> heuristics. Furthermore, the algorithm incorporates ideas from a number of different areas of <phrase>metaheuristic</phrase> research, such as iterated <phrase>local search</phrase>, <phrase>simulated annealing</phrase>, <phrase>tabu search</phrase> and <phrase>genetic algorithms</phrase>. The main goal of this work was to develop a hyperheuristic that achieves good overall performance on a wide variety of unrelated search domains. In order to assess the generality of the algorithm, it was tested on six different, well-studied problem domains from …
Boosting attribute and phone estimation accuracies with <phrase>deep neural networks</phrase> for detection-based <phrase>speech recognition</phrase> Generation of <phrase>high-precision</phrase> sub-phonetic attribute (also known as phonological features) and phone lattices is a key frontend component for detection-based bottom-up <phrase>speech recognition</phrase>. In this paper we employ <phrase>deep neural networks</phrase> (DNNs) to improve detection accuracy over conventional shallow MLPs (<phrase>multi-layer</phrase> perceptrons) with one <phrase>hidden layer</phrase>. A range of DNN architectures with five to seven hidden layers and up to 2048 <phrase>hidden units</phrase> per layer have been explored. Training on the SI84 and testing on the Nov92 <phrase>WSJ</phrase> data, the proposed DNNs achieve <phrase>significant improvements</phrase> over the shallow MLPs, producing greater than 90% frame-level attribute estimation accuracies for all 21 attributes tested for the full system. On the phone detection task, we also obtain excellent frame-level accuracy of 86.6%. With this level of <phrase>high-precision</phrase> detection of basic speech units we have opened the door to a new family of flexible speech <phrase>recognition system</phrase> design for both top-down and bottom-up, lattice-based search strategies and knowledge integration.
<phrase>Low Power</phrase> <phrase>Floating Point</phrase> Computation Sharing Multiplier for <phrase>Signal Processing</phrase> Applications Design of <phrase>low power</phrase>, higher performance <phrase>digital signal processing</phrase> elements are the major requirements in ultra <phrase>deep sub-micron technology</phrase>. I. INTRODUCTION The <phrase>finite impulse response</phrase> (FIR) filters are used in <phrase>signal processing</phrase> applications ranging from video and <phrase>image processing</phrase> to <phrase>wireless communications</phrase>. In some applications, such as <phrase>video processing</phrase>, the <phrase>FIR filter</phrase> circuit must be able to operate at high-frequencies, while in other applications, such as cellular <phrase>telephony</phrase>, the <phrase>FIR filter</phrase> circuit must be a low-power circuit, capable of operating at moderate frequencies. These demands led the designers to focus on the algorithmic as well as numerical strength reduction techniques for the <phrase>low-complexity</phrase> design of <phrase>FIR filter</phrase>. Strength reduction at the algorithmic level can be used to reduce the number of computations (additions and multiplications). Numerical strength reduction improves the performance of a computation. In this paper, numerical strength reduction is the area of interest. Many previous efforts like Common sub-expressions elimination [4], [5] and differential coefficients method [6], [7] explore <phrase>low-complexity</phrase> design of FIR filters by minimizing the number of additions in filtering operations. Canonical signed digit (CSD) [8] is used to reduce the number of the required additions and subtractions for filtering operation by reducing the total number of nonzero bits in coefficients. In [6] the differences between absolute values of filter coefficients were employed to reduce the complexity of computation. All these techniques are limited to the optimization of hardware for a particular fixed coefficient set. A computation sharing multiplier (CSHM) architecture, which identifies common computations and shares them between different multiplications was suggested in [1], [2] overcomes this drawback and applicable for applications with programmable filter coefficients. CSHM achieves <phrase>high-performance</phrase> programmable filtering operation by reusing the optimal precomputations and low-<phrase>power consumption</phrase>, since, redundant computations are removed. In addition to <phrase>signal processing</phrase> applications, the multipliers can also be used to test <phrase>data compression</phrase>/decompression VLSI test applications [16]. The reconfigurable multiplier design based on reordering of partial product [14] and row-bypassing technique [15] are proposed to reduce the switching power. In the literature [2], [10] and [11] CSHM is used only in <phrase>fixed-point</phrase> <phrase>FIR filter</phrase> implementation. The main contribution of this paper is proposition of a <phrase>floating-point</phrase> multiplier based on the CSHM technique, and effective implementation of <phrase>floating-point</phrase> <phrase>FIR filter</phrase>. Henceforth, in this paper this multiplier will be referred as FCSHM. The <phrase>floating-point</phrase> input values are taken in single-precision IEEE-754 standard format. In the rounding stage of multiplier …
Frame Selection Key to Improve <phrase>Video Compression</phrase> The huge usage of digital multimedia via communications media, <phrase>wireless communications</phrase>, Internet, <phrase>Intranet</phrase> and cellular mobile leads to incurable growth of <phrase>data flow</phrase> through these Media. The researchers go deep in developing efficient techniques in these fields such as <phrase>data compression</phrase> <phrase>image compression</phrase> and <phrase>video compression</phrase>. Recently, <phrase>video compression</phrase> techniques and their applications in many areas (educational, <phrase>agriculture</phrase>, medical …) cause this field to be one of the most intersect fields. <phrase>Wavelet transform</phrase> is an efficient method that can be used to perform an efficient compression technique. The proposed approach deal with the developing of efficient <phrase>video compression</phrase> approach based on frame selection key that concentrated on the calculation of frame near distance (or difference between frames). The selection of the meaningful frame depends on many factors such as the compression performance, frame details and frame size and near distance between frames. In this paper, many videos are used and tested to insure the efficiency of the proposed technique, in addition a good performance results has been obtained.
Bakar Kiasan: Flexible Contract Checking for Critical Systems Using <phrase>Symbolic Execution</phrase> List of Figures List of Tables Bakar Kiasan: Flexible Contract Checking for Critical Systems Using <phrase>Symbolic Execution</phrase> Spark, a subset of Ada for engineering safety and security-critical systems , is one of the best commercially available frameworks for <phrase>formal-methods</phrase>-supported development of critical software. Spark is designed for verification and includes a software contract language for specifying functional properties of procedures. Even though Spark and its <phrase>static analysis</phrase> components are beneficial and easy to use, its contract language is almost never used due to the burdens the associated tool support imposes on developers. <phrase>Symbolic execution</phrase> (SymExe) techniques have made significant strides in automating <phrase>reasoning about</phrase> <phrase>deep semantic</phrase> properties of <phrase>source code</phrase>. However, most work on SymExe has focused on bug-finding and <phrase>test case</phrase> generation as opposed to tasks that are more verification-oriented such as contract checking. In this paper, we present: (a) SymExe techniques for checking software contracts in embedded critical systems, and (b) Bakar Kiasan, a tool that implements these techniques in an <phrase>integrated development environment</phrase> for Spark. We describe a methodology for using Bakar Kiasan that provides significant increases in automation, usability, and functionality over existing Spark tools, and we present results from experiments on its application to industrial examples.
<phrase>Tactile Sensing</phrase> and Control of a Planar Manipulator <phrase>Tactile Sensing</phrase> and Control of a Planar Manipulator C Abstract <phrase>Tactile Sensing</phrase> and Control of a Planar Manipulator This dissertation explores the shape sensing capabilities of cylindrical <phrase>tactile sensing</phrase> ngers. Starting with an elastostatic model for the deformation of <phrase>rubber</phrase> ngers, sensor spacing and depth requirements are determined to allow reconstruction of subsurface strain elds with insigniicant <phrase>aliasing</phrase>. Given this bandlimited version of the strain eld, theoretical limits are found to classiication and scaling of the perceived indentation. These theoretical results lead to the design of a <phrase>silicone</phrase> <phrase>rubber</phrase> tactile sensor which is characterized and calibrated to the model. The reliability of curvature estimates from the sensor is then determined. Finally, use of the sensor during manipulation is demonstrated. A <phrase>spatial frequency</phrase> <phrase>domain model</phrase> for the deformation of an elastic cylinder with a rigid core in plane strain is derived. Based on the <phrase>transfer function</phrase> from surface pressure to subsurface strain, several conclusions can be made about bandlimited <phrase>tactile sensing</phrase>. First, we show that shear strain measurements are not useful for shape estimation. Secondly we show that a ratio of core radius to outer radius greater than 0.85 is required for indenter classiication given sensor noise of 1.7% <phrase>peak strain</phrase>. Thirdly we show that, for deep sensors, indenter wedge angle may be inferred from an indenter radius estimate. These theoretical results are tested through experiments with a capacitive <phrase>silicone</phrase> <phrase>rubber</phrase> tactile sensor. The sensor has a noise level of 0.5% <phrase>peak strain</phrase>, linearity of 1% <phrase>peak strain</phrase>, and a sensitivity to nearby conductors of 3% <phrase>peak strain</phrase>. Identiication of the map from surface pressure fourier coeecients to sensor output is accomplished with a residual error of 1.4% <phrase>peak strain</phrase>. Nine diierent indenter radii ranging from a radius of 0.5 mm to 12.7 mm are estimated with a standard deviation of 0.6 mm for 200 N/m loads over 40 degrees of the sensor. Contact location is estimated with an accuracy of 0:19 o 2 (0.043 mm). Given the <phrase>high accuracy</phrase> of the position estimation, position feedback is integrated into a grasp controller to allow optimal regrasping and manipulation of disks and rectangles. Tactile curvature estimates are displayed to the operator at a 10Hz rate during the manipulation. iii Contents
K/<phrase>ka-band</phrase> Channel Characterization for Mobile Satellite Systems Mobile satellite systems allow truly ubiquitous <phrase>wireless communications</phrase> to users anywhere and anytime, NASA's Advanced Communications Technology Satellite (ACTS) provides an ideal space-based platform for the measurement of K/<phrase>Ka-band</phrase> propagation characteristics in a land mobile satellite application. <phrase>Field tests</phrase> for K-band propagation were conducted in three basic environments: clear line-of-sight (LOS) highways, lightly shadowed suburban, and heavily shadowed suburban. <phrase>Preliminary results</phrase> of these <phrase>field tests</phrase> indicate very little multipath for rural environments and for clear LOS links while deep fades were experienced in shadowed areas, especially those where tree canopies covered the road.
<phrase>Accurate Modeling</phrase> Method for <phrase>Deep Sub-micron</phrase> Cu Interconnect This paper newly proposes an <phrase>accurate modeling</phrase> method of the copper interconnect <phrase>cross-section</phrase> in which the width and thickness dependence on layout patterns and density are fully incorporated and universally expressed. In addition, we have developed specific test patterns for the model parameters extraction, and an efficient extraction flow. We have extracted the model parameters for 0.15µm CMOS using this method and confirmed that 10% τpd error n ormally observed with conventional LPE (Layout Parameters Extraction) was completely dissolved. This is the first time that the practical and <phrase>accurate modeling</phrase> methodology for layout pattern sensitive Cu Interconnect is ever reported.
Robotics <phrase>Olympiads</phrase>: A New Means to Facilitate <phrase>Conceptualization</phrase> of Knowledge Acquired in Robot Projects This paper proposes theoretical robotics competitions, offered in conjunction with robot contests, as the framework to foster <phrase>deep learning</phrase> of concepts which underlie the practical projects and to facilitate the development of engineering aptitude. We present our experiences with integrating theoretical tests in the <phrase>Trinity College</phrase> Fire-Fighting Home Robot Contest and National Botball <phrase>Tournament</phrase>.
<phrase>Scaffolding</phrase> for <phrase>computer supported</phrase> writing to learn activities in vocational training Dual-T project investigates how ICT can support learning activities involving sharing and reflection about professional experience in order to harmonize school learning with practical experience. In this study we tested the effects of low and high <phrase>scaffolding</phrase> on <phrase>collaborative writing</phrase> activities on professional procedures. We expected longer, more correct texts to emerge from strongly scaffolded activities than from weakly scaffolded activities. <phrase>Recent research</phrase> on initial vocational training education has shown the existence of a gap between field knowledge and knowledge taught in <phrase>vocational schools</phrase> (Filliettaz, 2008). One of the <phrase>main issues</phrase> concerns knowledge and skill transfer between school and workplace (Eraut, 2004). In our project, we are interested in identifying original technological support and pedagogical designs for professional skills learning and transfer in vocational educational training (VET). In this context, we adopt a " writing-to-learn " approach (Hayes and <phrase>Flower</phrase>, 1980; Hayes, 1996). It assumes that writing promotes the acquisition of knowledge, since <phrase>domain knowledge</phrase> should be retrieved, reorganized and incorporated into a linear and understandable form. Extending this cognitive view, <phrase>Galbraith</phrase> (1999) claims that knowledge transformation leads to knowledge <phrase>constitution</phrase>, which makes writing a promising instructional tool. Professional procedure learning and transfer is a critical issue in VET. Anderson's <phrase>ACT-R</phrase> (1993) model claims that procedure acquisition is based on learning from declarative traces of initial <phrase>problem solving</phrase>. Writing could then be a powerful tool for constructing and refining the declarative representation of procedures. Moreover, confrontation between learners' conceptions and experiences should promote reflexive thinking and <phrase>epistemic</phrase> monitoring, embodied in the written productions In addition, <phrase>collaborative writing</phrase> activities should support not only individual <phrase>knowledge acquisition</phrase> but also the collaborative dimension of domain <phrase>knowledge building</phrase>. Tynjälä, Mason and Lonka (2001) show that studies of the effects of <phrase>collaborative writing</phrase> on learning are still rare Most of the research is done on the improvement of the writing process and writing skills. We consider that a peer collaborative approach to writing-to-learn in a VET context should be valuable in terms of <phrase>knowledge building</phrase>, procedure understanding and acquisition. Thus, in this research we are interested in investigating the impact of <phrase>collaborative writing</phrase> activities on the construction of a mutual declarative representation of the procedures. This is the basis for deep understating of procedures thus for acquisition and transfer. <phrase>Computer supported</phrase> <phrase>collaborative writing</phrase> to learn activities can be supported by many types of tools. Considering our context and the population we are working with, we …
RTL Emulation: The Next Leap in System Verification The worldwide electronics market is booming, fueled by the customers' insatiable appetite for <phrase>low-cost</phrase> computers , connectivity and appliances packed with high-technology features. While the sheer number of new design starts may not be noteworthy, the development of new chips and systems that are becoming more complex at a phenomenal rate. The effect of increase in <phrase>design complexity</phrase> is dual fold. First, designers are quickly migrating to <phrase>higher levels</phrase> of abstraction. Production use of text-based methodology has enabled designers to capture designs of hundreds of thousands of gates using graphic ESDA tools. Second, is the change in <phrase>manufacturing process</phrase>. Today, designers are using 0.35 micron or even 0.25 <phrase>micron technology</phrase>. Submicron manufacturing capabilities enable millions of gates on a single chip. Sematech predicts that 0.25 <phrase>micron technology</phrase> will be ubiquitous by 1998 and the average chip complexity will be 20 million transistors. With most ASIC fabs running 0.5 micron processes reliably, and several delivering or scheduling a move to 0.25 micron soon, manufacturing capabilities are quickly outgrowing the capacity of current <phrase>design tools</phrase>. The obstacles in developing new generation of electronics revolve around three key questions: 1. Can we design such complex chips and systems? 2. Can our factories fabricate these designs? 3. Can we verify the accuracy of these complex designs? The advances in high <phrase>level design</phrase> brought about by languages such as Verilog and VHDL coupled with <phrase>logic synthesis</phrase> technologies has largely addressed the first question. To answer the second question, semiconductor fabrications can now put millions of <phrase>logic gates</phrase> on a single chip using <phrase>deep sub-micron technology</phrase>. This rapid increase in the complexity of chips and systems has outstripped traditional verification techniques. This is further complicated when a chip or an ASIC is plugged into the final system. This is because of The famous 90/50 rule. Accordingly 90% of ASICs will work first time when tested stand-alone. However, only 50 % will work right when brought into the final system. A typical system level sign-off requires billions of clock cycles. When using traditional verification techniques, based upon software simulation, it will be several months, maybe even years before you can obtain a system sign-off in applications. Such as Real Time Video, ATM, <phrase>PCI</phrase> Protocols and DSP applications. In a typical <phrase>MPEG</phrase> <phrase>video compression</phrase> design it is not uncommon to visually verify 5 seconds of real time decoded video (about 150 frames). Using RTL simulation tools running at …
Multiphase Technique to Speed-up Delay Measurement via Sub-sampling —A multi-phase technique for speeding up the measurement of delays via sub-sampling is presented. Measurement of delays using the sub-sampling approach leads to a very simple system implementation, and also provides the opportunity of trading off between bandwidth and accuracy. Such a scheme becomes extremely attractive for <phrase>deep sub-micron</phrase> processes due to its highly-digital nature and the ability to offer compact, <phrase>low power</phrase>, <phrase>mixed-signal</phrase> implementation alternatives. However, a drawback is the amount of averaging (measurement time) that is needed to get accurate results. A multiphase input clock scheme is proposed to address this issue, especially for the measurement of small delays, thereby speeding up the overall measurement. <phrase>Simulation results</phrase> from MATLAB <phrase>Simulink</phrase> confirm the speedup achieved upto a factor of eight with an eight-phase clock input for sufficiently small fixed test delays and also an improvement in SNR upto 11dB for slowly varying test delays.
Joint Application of CCSDS File Delivery Protocol and Erasure Coding Schemes over Space Communications — The rising demand for multimedia services even in hazardous environments, such as <phrase>space missions</phrase> and military <phrase>theatres</phrase>, and the consequent need of proper <phrase>internetworking</phrase> technologies have revealed the inapplicability of <phrase>TCP/IP</phrase> architectures and highlighted the importance of the communication features provided by the protocol architectures proposed by the Consultative Committee for Space Data Systems (CCSDS). This paper proposes a CCSDS File Delivery Protocol (CFDP) extension, based on the implementation of erasure coding schemes, in order to assure high reliability to the <phrase>data communication</phrase> even in presence of very critical conditions, such as hard shadowing, deep fading periods and intermittent links. Different encoding techniques are considered and various channel conditions, in terms of Bit Error Ratio and bandwidth values, are tested. I. INTRODUCTION The integration of satellite and <phrase>wireless network</phrase> segments in the traditional internet has determined an increasing demand for communication technologies able to transport and deliver multimedia services to the final users. On the other hand, it has focused on the limits imposed by <phrase>TCP/IP</phrase> protocol architectures, when applied in hazardous environments characterized by long <phrase>propagation delays</phrase>, intermittent, and asymmetric links, frequent transmission errors. Taking the satellite technology as reference for its clear benefits concerning the wide area coverage and its inherent capability of <phrase>broadcast</phrase>/<phrase>multicast</phrase> communications, a protocol architecture alternative to <phrase>TCP/IP</phrase> has been designed by the Consultative Committee for Space Data Systems (CCSDS).
<phrase>Deep Belief Networks</phrase> for Phone Recognition <phrase>Hidden Markov Models</phrase> (HMMs) have been the state-of-the-art techniques for <phrase>acoustic modeling</phrase> despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. There are many proposals in the <phrase>research community</phrase> for deeper models that are capable of modeling the many types of variability present in the speech generation process. <phrase>Deep Belief Networks</phrase> (DBNs) have recently proved to be very effective for a variety of <phrase>machine learning problems</phrase> and this paper applies DBNs to <phrase>acoustic modeling</phrase>. On the standard TIMIT corpus, DBNs consistently outperform other techniques and the best DBN achieves a phone <phrase>error rate</phrase> (PER) of 23.0% on the TIMIT core <phrase>test set</phrase>.
When Timing Matters: Enabling Time Accurate & Scalable Simulation of <phrase>Sensor Network</phrase> Applications The raising complexity of <phrase>data processing</phrase> algorithms in <phrase>sensor networks</phrase> combined with the severely limited computing power of sensor nodes make a deep understanding of the timely behavior of implementations a necessity. However , only cycle accurate emulation and test-beds provide a detailed and accurate insight into the timely behavior of <phrase>sensor networks</phrase>. In this paper we introduce automated instrumentation of simulation models with detailed timing information derived from the sensor node and application binary to provide detailed timing information. The presented approach bridges the gap between scalable but abstracting simulation and cycle accurate emulation for <phrase>sensor network</phrase> evaluation. By mapping device specific code with simulation models , we can derive the timing of each <phrase>source code</phrase> line on a sensor node. As a result of such a mapping it is possible to determine the time and duration a certain code line gets executed. Hence, eliminating the need to use expensive instruction level <phrase>emulators</phrase> with limited speed, restricted scal-ability and portability. Furthermore, the proposed design is not bound to a specific hardware platform, a major advantage compared to existing <phrase>emulators</phrase>. Our <phrase>evaluation shows</phrase>, that the proposed technique achieves a timing accuracy of 99% compared to emulation while adding only a limited overhead. Concluding, it combines essential properties like accuracy, speed and scalability on a single simulation platform.
Test Oracle Automation for V&v of an Autonomous Spacecraft's Planner Challenges and Opportunities Overall Approach Context The NASA " <phrase>Deep Space 1</phrase> " (DS-1) spacecraft was launched in 1998 to evaluate promising new technologies and instruments. The " <phrase>Remote Agent</phrase> " , an <phrase>artificial intelligence</phrase> based <phrase>autonomy</phrase> architecture, was one of these technologies, and in 1999 this software ran on the spacecraft's flight processor and controlled the spacecraft for several days. We built automation to assist the <phrase>software testing</phrase> efforts associated with <phrase>the Remote Agent</phrase> experiment. In particular, our focus was upon introducing test <phrase>oracles</phrase> into the testing of the planning and scheduling system component. This summary is intended to provide an overview of the work. <phrase>The Remote Agent</phrase> experiment used an on-board planner to generate sequences of <phrase>high-level</phrase> commands that would control the spacecraft. From a Verification and Validation (V&V) perspective, the crucial observation is that <phrase>the Remote Agent</phrase> was to be a self-sufficient autonomous system operating a spacecraft over an extended period, without human intervention or oversight. Hence, V&V of its components, the planner included, was crucial. Thorough testing was the primary means by which V&V of the planner was performed. However, the nature of the testing effort differed significantly from that of testing more traditional spacecraft control mechanisms. In particular: • The planner's output (plans) were detailed and voluminous, ranging from 1,000 to 5,000 lines long. Plans were intended to be read by software, and were not designed for easy perusal by humans. • The planner could be called upon to operate (generate a plan) in a wide range of circumstances. This variety stems from the many possible initial conditions (state of the spacecraft) and the many plausible goals (objectives the plan is to achieve). Thorough V&V required testing the planner on thousands of <phrase>test cases</phrase>, yielding a separate plan for each. • Each plan must satisfy all of the flight rules that characterize correct operation of the spacecraft. (Flight rules may refer to the state of the spacecraft and the activities it performs, and describe temporal conditions required among those states and activities.) The information pertinent to deciding whether or not a plan passes a flight rule was <phrase>dispersed</phrase> throughout the plan. This exacerbated the problems of human perusal. As a consequence, manual inspection of more than a small fragment of plans generated in the course of testing was recognized to be impractical. The nature of the task – V&V of a software component of an autonomous system – meant that …
Advances in optimizing <phrase>recurrent networks</phrase> After a more than decade-<phrase>long period</phrase> of relatively little research activity in the area of <phrase>recurrent neural networks</phrase>, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of <phrase>recurrent networks</phrase>. These advances have been motivated by and related to the optimization issues surrounding <phrase>deep learning</phrase>. Although <phrase>recurrent networks</phrase> are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of <phrase>long-term</phrase> dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced <phrase>momentum</phrase> techniques, using more powerful output probability models, and encouraging sparser gradients to help <phrase>symmetry breaking</phrase> and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both <phrase>training and test</phrase> error.
MIRPLib - A library of maritime inventory routing <phrase>problem instances</phrase>: Survey, core model, and benchmark results This paper presents a detailed description of a particular class of deterministic single product maritime inventory routing problems (MIRPs), which we call <phrase>deep-sea</phrase> MIRPs with inventory tracking at every port. This class involves vessel travel times between ports that are significantly longer than the time spent in port and require inventory levels at all ports to be monitored throughout the planning horizon. After providing a comprehensive literature survey of this class, we introduce a core model for it cast as a mixed-integer <phrase>linear program</phrase>. This formulation is quite general and incorporates assumptions and families of constraints that are most prevalent in practice. We also discuss other modeling features commonly found in the literature and how they can be incorporated into the core model. We then offer a unified discussion of some of the most common advanced techniques used for improving the bounds of these problems. Finally, we present a library, called MIRPLib, of publicly available test <phrase>problem instances</phrase> for MIRPs with inventory tracking at every port. Despite a growing interest in combined routing and inventory management problems in a maritime setting, no <phrase>data sets</phrase> are publicly available, which represents a significant " barrier to entry " for those interested in related research. Our main goal for MIRPLib is to help maritime inventory routing gain maturity as an important and interesting class of <phrase>planning problems</phrase>. As a means to this end, we (1) make available benchmark instances for this particular class of MIRPs; (2) provide the mixed-integer <phrase>linear programming</phrase> community with a set of <phrase>optimization problem</phrase> instances from the maritime transportation domain in LP and MPS format; and (3) provide a template for other researchers when specifying characteristics of MIRPs arising in other settings. Best known computational results are reported for each instance.
Comparing Nonlinear Vector Network Analyzer Methodologies S cattering parameters (S-parameters) were first mentioned in articles and textbooks in the 1950s and 1960s by Mat-thews, Collins and Kurokawa and popularized by the release of Hewlett Packard's first network analyzers in the 1960s. Since then, S-parameters have been used to describe the complex characteristics of a network by quantifying the RF power flowing between its ports. S-parameters are essentially the ratio of the reflected and transmitted signal at a given port to the incident signal at a given port under perfect match conditions (see Figure 1). When referring to a transistor , these parameters can be used to calculate return loss, gain and isolation. 1 It is important to understand that S-parameters are only truly valid for linear networks, where the characteristics of the network are independent of the level of the signal being driven into the network. When considering a semiconductor application such as a transistor, this refers to its small-signal operating condition , in which the input drive power does not affect the S-parameters of the transistor. Under small-signal <phrase>operating conditions</phrase>, a transistor will have linear gain at the fundamental drive frequency and not excite additional powers at the <phrase>harmonic</phrase> frequencies. Linear operation of a transistor and the associated input and output waveforms are shown in Figure 2. The question arises how to thoroughly describe and model a transistor operating under nonlinear large-signal conditions. These conditions occur commonly for high power and high efficiency power amplifiers operating in advanced classes of operation. Under these <phrase>operating conditions</phrase>, powers are induced at <phrase>harmonic</phrase> frequencies when excited by a <phrase>sine wave</phrase>, and in-band and out-of-band modulation products are created under modulation excita-tion. Considering a simple <phrase>sine wave</phrase> excitation, a transistor's characteristics are dependent on the level of the input drive signal, shown in both frequency and time domains in Figure 3. When the transistor is in deep compression and its output is composed of multiple <phrase>harmonics</phrase> , the device behavior cannot be described correctly by S-parameters, which are <phrase>frequency domain</phrase> quantities. It is much more natural to analyze the behavior of the device under test (DUT) in terms of time domain RF voltage and current waveforms. Clear evidence of this is provided by the theoretical description of the different modes of operation of power amplifiers , which is completely done in time domain. In this case, a nonlinear vector network ana-lyzer (NVNA) can be used to measure the incident …
Entity-Focused Sentence Simplification for Relation Extraction Relations between entities in text have been widely researched in the <phrase>natural language processing</phrase> and <phrase>information-extraction</phrase> communities. The region connecting a pair of entities (in a parsed sentence) is often used to construct kernels or feature vectors that can recognize and extract interesting relations. Such regions are useful, but they can also incorporate unnecessary distracting information. In this paper, we propose a rule-based method to remove the information that is unnecessary for relation extraction. Protein–<phrase>protein interaction</phrase> (PPI) is used as an example <phrase>relation extraction</phrase> problem. A dozen simple rules are defined on output from a <phrase>deep parser</phrase>. Each rule specifically examines the entities in one target interaction pair. These simple rules were tested using several PPI corpora. The <phrase>PPI extraction</phrase> performance was improved on all the PPI corpora.
A <phrase>Cross Validation</phrase> Study of <phrase>Deep Brain Stimulation</phrase> Targeting: From Experts to Atlas-Based, Segmentation-Based and Automatic <phrase>Registration Algorithms</phrase> Validation of <phrase>image registration</phrase> algorithms is a difficult task and open-ended problem, usually application-dependent. In this paper, we focus on <phrase>deep brain stimulation</phrase> (DBS) targeting for the treatment of movement disorders like <phrase>Parkinson's disease</phrase> and <phrase>essential tremor</phrase>. DBS involves implantation of an electrode <phrase>deep inside</phrase> the brain to electrically stimulate specific areas shutting down the disease's symptoms. The subthalamic nucleus (STN) has turned out to be the optimal target for this kind of surgery. Unfortunately, the STN is in general not clearly distinguishable in common medical <phrase>imaging modalities</phrase>. Usual techniques to infer its location are the use of anatomical atlases and visible surrounding landmarks. Surgeons have to adjust the electrode intraoperatively using <phrase>electrophysiological</phrase> recordings and macrostimulation tests. We constructed a <phrase>ground truth</phrase> derived from specific patients whose STNs are clearly visible on <phrase>magnetic resonance</phrase> (MR) T2-weighted images. A patient is chosen as atlas both for the right and left sides. Then, by registering each patient with the atlas using different methods, several estimations of <phrase>the STN location</phrase> are obtained. Two studies are driven using our proposed validation scheme. First, a comparison between different atlas-based and <phrase>nonrigid</phrase> <phrase>registration algorithms</phrase> with a evaluation of their performance and usability to locate the STN automatically. Second, a study of which visible surrounding structures influence <phrase>the STN location</phrase>. The two studies are cross validated between them and against expert's variability. Using this scheme, we evaluated the expert's ability against the estimation error provided by the tested algorithms and we demonstrated that automatic STN targeting is possible and as accurate as the expert-driven techniques currently used. We also show which structures have to be taken into account to accurately estimate <phrase>the STN location</phrase>.
Image Watermarking Using Psychovisual Threshold over the Edge Currently the digital multimedia data can easily be copied. <phrase>Digital image</phrase> watermarking is an alternative approach to <phrase>authentication</phrase> and <phrase>copyright protection</phrase> of <phrase>digital image</phrase> content. An alternative embedding watermark based on <phrase>human eye</phrase> properties can be used to effectively hide the watermark image. This paper introduces the embedding watermark scheme along the edge based on the concept of psychovisual threshold. This paper will investigate the sensitivity of minor changes in <phrase>DCT</phrase> coefficients against <phrase>JPEG</phrase> quantization tables. Based on the concept of psychovisual threshold, there are still deep holes in <phrase>JPEG</phrase> quantization values to embed a watermark. This paper locates and utilizes them to embed a watermark. The proposed scheme has been tested against various non-malicious attacks. The experiment results show the watermark is robust against <phrase>JPEG</phrase> <phrase>image compression</phrase>, noise attacks and <phrase>low pass</phrase> filtering. 1 Introduction Currently, an efficient access internet makes it easy to duplicate <phrase>digital image</phrase> contents. In addition, current <phrase>mobile devices</phrase> view and transfer compressed images heavily [1]-[4]. Image watermarking is one of the popular techniques to manage and protect the copyright <phrase>digital image</phrase> content. Most of the image watermarking techniques exploits the characteristic of <phrase>Human Visual System</phrase> (HVS) in effectively embedding a robust watermark [5]-[7]. HVS is less sensitive to noise in highly <phrase>textured</phrase> area [8] and significantly changing region of an image. <phrase>Human visual</phrase> properties can be utilized in embedding process by insert more bits of watermark image for each block which has complex textures or edges on an image. The watermark with significant coefficients is more robust if it resides near round edges and texture areas of the image [9]. This paper proposes an embedding watermark scheme along the edge of the host image. This scheme enables the watermark to be more robust against non-malicious attacks. The organization of this paper is given as follows. The next section will give a brief description on the concept of pshycovisual threshold for image watermarking. Section 3 presents an experimental design of the image watermarking. The
Concurrent Testing of <phrase>Digital Circuits</phrase> for Non-classical <phrase>Fault Models</phrase>: <phrase>Resistive Bridging</phrase> <phrase>Fault Model</phrase> and N-detect Test This work is concerned with the development of generic, non-intrusive and flexible algorithms for the design of <phrase>digital circuits</phrase> with on line testing (OLT) capability. Most of the works presented in the literature on OLT have used single <phrase>stuck at fault</phrase> models. However, in <phrase>deep submicron era</phrase> single s-a <phrase>fault models</phrase> may not capture more than a fraction of the real defects. To cater to the problem it is now advocated that additional <phrase>fault models</phrase> such as <phrase>Resistive Bridging</phrase> faults, <phrase>Transition faults</phrase>, <phrase>Delay faults</phrase> etc. are also used. The proposed technique is one of the first works that enables on-line detection of <phrase>resistive bridging</phrase> faults and provides a high value of n for the n-Detect tests. The technique can handle generic <phrase>digital circuits</phrase> with cell count as high as 15,000 and having the order of 2 500 states. Results for design of on-line detectors for various ISCAS89 <phrase>benchmark circuits</phrase> are provided. The results illustrate that with marginal increase in <phrase>area overhead</phrase>, if compared to ones with single s-a <phrase>fault coverage</phrase>, the proposed scheme also provides coverage for <phrase>resistive bridging</phrase> faults and high value of n for n-Detect coverage. The results have also been verified in silicon using FPGAs
Current Approaches to XML Management XML management systems vary widely in their expressive power and query-processing efficiency, and users should choose the XMLMS that best meets their needs. T he <phrase>Extensible Markup Language</phrase> has become the standard for information <phrase>interchange</phrase> on the Web. Developed primarily as a document <phrase>markup language</phrase> more powerful than <phrase>HTML</phrase> yet less complex than <phrase>SGML</phrase>, XML does not require content to adhere to structural rules. XML gives a single, human-readable syntax for representing data, including data in relational format. Hence XML appeals to both the document and the database communities. Early developers of XML content storage tools, who came from the database community, regarded XML as yet another data format for adapting relational and sometimes object-relational <phrase>data-processing</phrase> tools. While this use of XML is acceptable , it does not <phrase>harness</phrase> XML's full power. XML is inherently <phrase>semistructured</phrase>. However , documents subscribing to the data-centric view of XML are highly structured and can be represented equivalently in tables or in XML with document type definitions (DTDs) or <phrase>XML schema</phrase> specifications (see the sidebar, " Related <phrase>W3C</phrase> Documents , " for this and other XML specifications). As in traditional <phrase>relational databases</phrase>, sibling element order is unimportant in such documents. We refer to documents with implicitly ordered XML content as <phrase>document-centric</phrase>. The file's element order (as siblings in a tree-like representation) conveys its implicit order, whereas a document attribute or tag expresses an explicit order. Although it is easy to express explicit order in <phrase>relational databases</phrase>, capturing the implicit order while converting a <phrase>document -centric</phrase> XML document into a <phrase>rela</phrase>-tional database is a problem. Besides the implicit order, <phrase>document-centric</phrase> <phrase>XML documents</phrase> allow little or no structure, deep nesting, and hyperlinked components. Tables can represent implicit order, nesting, and hyperlinks but only with costly time and space transformations. This article studies the data-and <phrase>document -centric</phrase> uses of XML management systems (XMLMS). We want to provide <phrase>XML data</phrase> users with a guideline for choosing the <phrase>data management</phrase> system that best meets their needs. Because the systems we test are first-generation
Learning Factored Representations in a Deep Mixture of Experts Mixtures of Experts combine the outputs of several " expert " networks, each of which specializes in a different part of the input space. This is achieved by training a " gating " network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent (" where ") experts at the first layer, and class-specific (" what ") experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.
Is <phrase>IDDQ testing</phrase> not applicable for <phrase>deep submicron VLSI</phrase> in year 2011? In this work, IDDQ current for the <phrase>deep submicron VLSI</phrase> in year 2011 is estimated with a statistical approach according to the <phrase>International Technology Roadmap for Semiconductors</phrase> 1999 Edition considering <phrase>process variations</phrase> and different input vectors. The estimated results show that the standard deviation of the IDDQ current is proportional to the <phrase>square root</phrase> of the circuit size and the IDDQ currents of the <phrase>defect-free</phrase> and the defective devices, which are of the size up to 1x10' gates, are still differentiable under the condition of <phrase>random process</phrase> deviations and input vectors. Two new <phrase>IDDQ testing</phrase> schemes, which detect the defective current based on the two separate lDDQ distributions, are proposed. From the study, it is concluded that <phrase>IDDQ testing</phrase> is still applicable for the <phrase>deep submicron VLSI</phrase> for the next ten years.
A novel scan segmentation design method for avoiding shift timing failure in scan testing High <phrase>power consumption</phrase> in scan testing can cause undue <phrase>yield loss</phrase> which has increasingly become a serious problem for <phrase>deep-submicron VLSI</phrase> circuits. Growing evidence attributes this problem to shift <phrase>timing failures</phrase>, which are primarily caused by excessive <phrase>switching activity</phrase> in the proximities of clock paths that tends to introduce severe <phrase>clock skew</phrase> due to IR-drop-induced delay increase. This paper is the first of its kind to address this critical issue with a novel layout-aware scheme based on scan segmentation design, called LCTI-<phrase>SS</phrase> (Low-Clock-Tree-Impact Scan Segmentation). An optimal combination of scan segments is identified for simultaneous clocking so that the <phrase>switching activity</phrase> in the proximities of clock trees is reduced while maintaining the average <phrase>power reduction</phrase> effect on conventional scan segmentation. Experimental results on benchmark and industrial circuits have demonstrated the advantage of the LCTI-<phrase>SS</phrase> scheme.
RL$^2$: Fast <phrase>Reinforcement Learning</phrase> via Slow <phrase>Reinforcement Learning</phrase> Deep <phrase>reinforcement learning</phrase> (deep RL) has been successful in learning sophisticated behaviors automatically; however, the <phrase>learning process</phrase> requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their <phrase>prior knowledge</phrase> about the world. This paper seeks to bridge this gap. Rather than designing a " fast " <phrase>reinforcement learning</phrase> algorithm, we propose to represent it as a recurrent <phrase>neural network</phrase> (RNN) and learn it from data. In our <phrase>proposed method</phrase>, RL 2 , the algorithm is encoded in the weights of the RNN, which are learned slowly through a <phrase>general-purpose</phrase> (" slow ") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations , actions, <phrase>rewards</phrase>, and termination <phrase>flags</phrase>; and it retains its state across episodes in a given <phrase>Markov Decision Process</phrase> (<phrase>MDP</phrase>). The activations of the RNN store the state of the " fast " RL algorithm on the current (previously unseen) <phrase>MDP</phrase>. We evaluate RL 2 experimentally on both <phrase>small-scale</phrase> and <phrase>large-scale</phrase> problems. On the <phrase>small-scale</phrase> side, we train it to solve randomly generated multi-armed <phrase>bandit</phrase> problems and finite <phrase>MDPs</phrase>. After RL 2 is trained, its performance on new <phrase>MDPs</phrase> is close to human-designed algorithms with optimality guarantees. On the <phrase>large-scale</phrase> side, we test RL 2 on a vision-based navigation task and show that it scales up to <phrase>high-dimensional</phrase> problems.
Enforcing Applicability of <phrase>Real-Time Scheduling Theory</phrase> <phrase>Feasibility Tests</phrase> with the Use of <phrase>Design-Patterns</phrase> This article deals with performance verifications of architecture models of real-time <phrase>embedded systems</phrase>. We focus on models verified with the <phrase>real-time scheduling theory</phrase>. To perform verifications with the <phrase>real-time scheduling theory</phrase>, the architecture designers must check that their models are compliant with the assumptions of this theory. Unfortunately , this task is difficult since it requires that designers have a deep understanding of the <phrase>real-time scheduling theory</phrase>. In this article, we investigate how to help designers to check that an architecture model is compliant with this theory. We focus on <phrase>feasibility tests</phrase>. <phrase>Feasibility tests</phrase> are analytical methods proposed by the <phrase>real-time scheduling theory</phrase>. We show how to explicitly model the relationships between an architectural model and <phrase>feasibility tests</phrase>. From these models, we apply a <phrase>model-based</phrase> engineering process to generate a decision tool what is able to detect from an architecture model which are the <phrase>feasibility tests</phrase> that the designer can apply.
Cultural Ant Algorithm for Continuous <phrase>Optimization Problems</phrase> In order to overcome prematurity of <phrase>ant colony</phrase> algorithm, the conception of <phrase>belief space</phrase> originated in cultural algorithm is introduced, and a new cultural ant algorithm is proposed for continuous <phrase>optimization problems</phrase>. Firstly, the coding scheme for <phrase>ant colony</phrase> algorithm to solve continuous <phrase>optimization problems</phrase> is discussed. Then <phrase>belief space</phrase> is brought in, and designed as the form of two parts: individual <phrase>belief space</phrase> and population <phrase>belief space</phrase>. The former is used to conduct individuals' deep search for better solutions, and the other to help worse individuals drop their current bad solution space for broad search. The update rules of both population space and <phrase>belief space</phrase> are given subsequently. Eight common standard functions are used to test the new algorithm, which is compared with four other algorithms at the same time. The results show effectiveness and superiority of the new algorithm. Finally the effect of the parameter used in the algorithm is discussed, and so does the both two <phrase>belief space</phrase>.
<phrase>Tongue line</phrase> extraction <phrase>Tongue line</phrase> refers to the surface of the tongue covered with fissures or lines in deep or shallow shape and is one type of important features in clinical practice of <phrase>Traditional Chinese</phrase> Tongue Diagnosis (TCTD). However , it is hard to extract tongue lines completely due to the large variation of the widths of tongue lines and the strong noise caused by the rough surface of tongue and uneven illumination. In this paper, an improved wide line detector (WLD) is presented for <phrase>tongue line</phrase> extraction. Based on the characteristics of tongue lines, the original WLD is improved to avoid the undesired separation of a wide line and the influence of uneven <phrase>lighting conditions</phrase>. The proposed method has been tested on a total of 286 <phrase>tongue line</phrase> images and our <phrase>experimental results demonstrate</phrase> that the improved WLD significantly outperforms the original WLD for <phrase>tongue line</phrase> extraction by improving the TPR 16.5%, FPR 44.6% and PM 33.4%, respectively.
Extremal combinatorics deals with the problem combinatorics deals with the problem of determining or estimating the maximum or minimum possible value of an invariant of a combinatorial object that satisfies certain requirements. My research is on problems of extremal combinatorics motivated by applications in computer science, engineering, biology, and <phrase>nanotechnology</phrase>. The following is a sample of topics that I am currently interested in. • <phrase>Coding Theory</phrase>: How to construct good codes that would protect against errors introduced by communication channels when information is transmitted across them? Of particular interest a r e u n c o n v e n t i o n a l c h a n n e l s s u c h a s insertion/deletion channels, q-<phrase>ary</phrase> channels (q > 2), quantum transmission channels, and DNA storage channels. • <phrase>Electronic Design Automation</phrase>: Power efficiency and <phrase>signal integrity</phrase> are two important design criteria in <phrase>deep submicron VLSI</phrase>. How do we design information transmission schemes that are power-efficient, and protects against interference at the deep submicron level? • <phrase>Group Testing</phrase>: Nonadaptive <phrase>group testing</phrase> algorithms are useful when we want to trade resources for time, in identifying objects of a certain trait in large populations. What is the best tradeoff we can obtain? • <phrase>Nanotechnology</phrase>: Nanometer-scale components are prone to manufacturing errors. What and how much redundancy must be introduced in the design of these components so that in the face of manufacturing defects, we can reconfigure via the redundancy introduced to render the component still useful? Extremal combinatorial objects are often difficult to determine. Examples of such objects are optimal codes, combinatorial designs, Ramsey graphs, and various geometric packings and coverings. I am also interested in the design of practical algorithms for searching for extremal combinatorial objects, and whenever possible to enumerate them. In general, I am interested in multidisciplinary problems, especially those lying in the intersection of <phrase>discrete mathematics</phrase>, <phrase>computer science</phrase>, and engineering. Limit on the addressability of <phrase>fault-tolerant</phrase> <phrase>nanowire</phrase> decoders, " IEEE Transactions on Computers. divisible codes and their application in the construction of optimal constant-composition codes of weight three " , IEEE The sizes of optimal q-<phrase>ary</phrase> codes of weight three and distance four: a complete solution " , IEEE Transactions on Information On extremal k-graphs without repeated copies of 2-intersecting edges " , <phrase>SIAM</phrase> Journal on Discrete Optimal memoryless encoding for <phrase>low power</phrase> off-chip data buses " , in ICCAD Asymptotically optimal erasure-resilient codes for large disk arrays …
Testing the untestable: <phrase>model testing</phrase> of complex software-intensive systems Increasingly, we are faced with systems that are <i>untestable</i>, meaning that traditional <phrase>testing methods</phrase> are expensive, time-consuming or infeasible to apply due to factors such as the systems' continuous interactions with the environment and the deep intertwining of software with hardware.  In this paper we outline our vision to enable testing of untestable systems. Our key idea is to frame testing on <i>models</i> rather than operational systems. We refer to such testing as <i><phrase>model testing</phrase></i>. Our goal is to raise the <phrase>level of abstraction</phrase> of testing from operational systems to models of their behaviors and properties. The models that underlie <phrase>model testing</phrase> are executable representations of the relevant aspects of a system and its environment, alongside the risks of system failures. Such models necessarily have uncertainties due to complex, dynamic environment behaviors and the unknowns about the system. This makes it crucial for <phrase>model testing</phrase> to be uncertainty-aware. We propose to synergistically combine <phrase>metaheuristic</phrase> search, increasingly used in traditional <phrase>software testing</phrase>, with system and risk models to drive the search for faults that entail the most risk.  We expect <phrase>model testing</phrase> to bring early and <phrase>cost-effective</phrase> automation to the testing of many critical systems that defy existing automation techniques, thus significantly improving the dependability of such systems.
Hybrid Simulated-Emulated Platform for Heterogeneous Access Networks Performance Investigations —The availability of different access technologies enables the creation of <phrase>heterogeneous networks</phrase> supporting users mobility and assuring several different services. Meanwhile these networks require complex control techniques to assure Quality of Service (QoS) to users. Before implementing such networks, a deep <phrase>performance analysis</phrase>, through the use of network simulators or real models, is necessary. In particular the first ones (e.g. Network Simulator 3-ns-3 among the others) are quite simple and easy to manage and configure, while the second ones assure the handling of real traffic flows. The main contribution of this paper is the description of an hybrid simulated and emulated network evaluation platform, developed by the authors. The platform purpose is to execute a performance analysis of different <phrase>wireless networks</phrase> such as <phrase>Long Term</phrase> Evolution (<phrase>LTE</phrase>) and <phrase>Wi-Fi</phrase>, connected to a <phrase>core network</phrase> implementing the Differentiated Service (DiffServ) protocol. The paper contains also the results of preliminary validation tests.
<phrase>Semantic Role</phrase> Labeling Using Lexical Statistical Information Our system for <phrase>semantic role</phrase> labeling is multi-stage in nature, being based on tree pruning techniques, <phrase>statistical methods</phrase> for lexicalised feature encoding, and a C4.5 <phrase>decision tree</phrase> classifier. We use both shallow and deep <phrase>syntactic information</phrase> from <phrase>automatically generated</phrase> chunks and <phrase>parse trees</phrase>, and develop a model for learning the semantic arguments of predicates as a <phrase>multi-class</phrase> <phrase>decision problem</phrase>. We evaluate the performance on a set of relatively 'cheap' features and report an F 1 score of 68.13% on the overall <phrase>test set</phrase>.
Where Am I? Excerpt from Brainstorms: Philosophical Essays on Mind and Psychology Now that I've won my suit under the Freedom of Information Act, I am at <phrase>liberty</phrase> to reveal for the first time a curious episode in my life that may be of interest not only to those engaged in research in the philosophy of mind, <phrase>artificial intelligence</phrase>, and <phrase>neuroscience</phrase> but also to the general public. Several <phrase>years ago</phrase> I was approached by <phrase>Pentagon</phrase> officials who asked me to volunteer for a highly dangerous and secret mission. In collaboration with NASA and <phrase>Howard Hughes</phrase>, the Department of Defense was spending billions to develop a Supersonic Tunneling Underground Device, or STUD. It was supposed to tunnel through the earth's core at great speed and deliver a specially designed atomic <phrase>warhead</phrase> "right up the Red's <phrase>missile</phrase> silos," as one of the <phrase>Pentagon</phrase> <phrase>brass</phrase> put it. The problem was that in an early test they had succeeded in lodging a <phrase>warhead</phrase> about a mile deep under <phrase>Tulsa</phrase>, <phrase>Oklahoma</phrase>, and they wanted me to retrieve it for them. "Why me?" I asked. Well, the mission involved some pioneering applications of current brain research, and they had heard of my interest in brains and of course my <phrase>Faustian</phrase> curiosity and great courage and so forth.... Well, how could I refuse? The difficulty that brought the <phrase>Pentagon</phrase> to my door was that the device I'd been asked to recover was fiercely <phrase>radioactive</phrase>, in a new way. According to monitoring instruments, something about the nature of the device and its complex interactions with pockets of material deep in the earth had produced radiation that could cause severe abnormalities in certain tissues of the brain. No way had been found to shield the brain from these deadly rays, which were apparently harmless to other tissues and organs of the body. So it had been decided that the person sent to recover the device should leave his brain behind. It would be kept in a sale place as there it could execute its normal control functions by elaborate radio links. Would I submit to a <phrase>surgical procedure</phrase> that would completely remove my brain, which would then be placed in a <phrase>life-support</phrase> system at the <phrase>Manned Spacecraft Center</phrase> in <phrase>Houston</phrase>? Each input and output pathway, as it was severed, would be restored by a pair of microminiaturized radio transceivers, one attached precisely to the brain, the other to the nerve stumps in the empty <phrase>cranium</phrase>. No information would be lost, all …
Lacunar strokes: does shape matter? U p to one in every four <phrase>ischemic</phrase> strokes can be <phrase>lacunar infarcts</phrase>. Traditionally, <phrase>deep brain</phrase> infarctions with a maximum diameter of 15-20 mm have been attributed to occlusion of a single penetrating artery by lipohyalinosis or microatheromatosis, but it has been suggested that other etiologies such as <phrase>embolism</phrase> may be responsible for up to one third of the cases. Lacunes can be detected by <phrase>neuroimaging</phrase> in asympto-matic individuals and are associated with greater risk of cognitive decline 1,2. <phrase>Lacunar infarcts</phrase> can have a diameter greater than 15 mm on axial sections and greater than 20 mm on <phrase>coronal</phrase> or <phrase>sagittal</phrase> MRI sections 3,4. It has been argued that shapes of <phrase>lacunar infarcts</phrase> may be linked to <phrase>pathogenesis</phrase>: lesions with an irregular shape may result from occlu-<phrase>sion</phrase> of largest perforating arteries, or from confluence of lesions, or even from secondary tissue degeneration. Lacunes with ovoid or <phrase>spheroid</phrase>, regular shapes may reflect involvement of smallest arteries 5. In this issue of Arquivos de Neuro-Psiquiatria, Feng et al. 6 evaluated clinical and imaging features as well as <phrase>prognosis</phrase> of 204 Chinese patients admitted with acute <phrase>lacunar infarcts</phrase>, defined as hypointense focal lesions in <phrase>T1-weighted</phrase> images and hyperintense focal le-sions in T2-weighted, FLAIR, and DWI images with a diameter ranging from 3 mm to 20 mm. <phrase>Radiologists</phrase> (unaware of clinical characteristics) classified infarcts as regular or irregular by visual inspection of MRI images. The authors hypothesized that <phrase>pathogenesis</phrase>, clinical symptoms , and <phrase>prognosis</phrase> would differ between patients with acute regular or irregular infarcts. Sizes of the infarcts and extent of leukoaraiosis were also evaluated. <phrase>Blood pressure</phrase>, <phrase>blood glucose</phrase>, <phrase>hemoglobin</phrase> A1c and <phrase>lipids</phrase>, as well as <phrase>blood pressure</phrase> varia bility (BPV) were checked within the first 24 hours after stroke. The authors did not mention criteria used to define risk factors for <phrase>vascular disease</phrase> such as arterial hyperten-<phrase>sion</phrase>, <phrase>diabetes mellitus</phrase>, <phrase>dyslipidemia</phrase> (history? medical records? measurements/tests performed only before stroke, or either before or after stroke?). Logistic regressions were used to test for associations between risk factors for <phrase>vascular disease</phrase>, variation in systolic blood, shape and size of lacunar lesions, extent of leukoaraiosis, neurological deterioration (increase in <phrase>NIH</phrase> stroke scale scores at two weeks after stroke), and <phrase>prognosis</phrase> (modified Rankin scale at 3 months). The only variable independently associated with shapes of <phrase>lacunar infarcts</phrase> was BPV within the first 24 hours. The authors concluded that " BPV is an independent <phrase>risk factor</phrase> for irregularly shaped …
Deterministically testing sparse polynomial identities of unbounded degree We present two deterministic algorithms for the arithmetic circuit identity testing problem. The running time of our algorithms is polynomially bounded in s and m, where s is the size of the circuit and m is an upper bound on the number monomials with non-zero coefficients in its standard representation. The running time of our algorithms also has a <phrase>logarithmic</phrase> dependence on the degree of the polynomial but, since a circuit of size s can only compute <phrase>polynomials</phrase> of degree at most 2 s , the running time does not depend on its degree. Before this work, all such deterministic algorithms had a polynomial dependence on the degree and therefore an exponential dependence on s. Our first algorithm works over the integers and it requires only black-box access to the given circuit. Though this algorithm is quite simple, the analysis of why it works relies on Linnik's Theorem, a deep result from <phrase>number theory</phrase> about the size of the smallest prime in an <phrase>arithmetic progression</phrase>. Our second algorithm, unlike the first, uses elementary arguments and works over any integral domains, but it uses the circuit in a less restricted manner. In both cases the running time has a <phrase>logarithmic</phrase> dependence on the largest coefficient of the polynomial.
An evaluation of Penny: a system for fine grain implicit parallelism The Penny system is an implementation of AKL, a concurrent constraint language with deep guards, on <phrase>shared-memory</phrase> multiprocessors. It automatically extracts paral-lelism in arbitrary AKL programs. No user annotations are required nor there is any compiler support to extract par-allelism. We give an overview of the system and present <phrase>empirical evaluation</phrase> results from a set of benchmarks with varying characteristics. The <phrase>evaluation shows</phrase> that it is possible to build a system that automatically exploits ne-grain parallelism for a wide range of programs. 1 Introduction The Penny system is an implementation of AKL, a concurrent constraint language with deep guards. The system has been implemented on a <phrase>high-performance</phrase> <phrase>shared-memory</phrase> multiprocessor and is able to outperform C implementations of algorithms with complex dependencies without any user annotations. In this paper we describe a <phrase>performance evaluation</phrase> of the system. Extensive measurements were done using both smaller benchmarks as well as <phrase>real-life</phrase> programs. The evaluation uses detailed instruction-level simulation, including cache-performance, to explain the behavior of the system. Section 2 of the <phrase>evaluation shows</phrase> the performance of the system for diierent classes of benchmarks. The tests include simple recursive, stream parallel and <phrase>non-deterministic</phrase> benchmarks. The next two section show the limitations of the system when the granularity of work decreases. In Section 3
Institute of Physics <phrase>Publishing</phrase> Journal of Micromechanics and Microengineering Fabrication of Keyhole-free Ultra-deep High-<phrase>aspect-ratio</phrase> Isolation Trench and Its Applications An ultra-deep (40–120 µm) keyhole-free electrical isolation trench with an aspect ratio of more than 20:1 has been fabricated. The process combines DRIE (<phrase>deep reactive ion</phrase> etch), LPCVD insulating materials refilling and TMAH or KOH backside etching technologies. Employing multi-step DRIE with optimized etching conditions and a sacrificial polysilicon layer, the keyholes in trenches are prevented; as a result the mechanical strength and reliability of isolation trenches are improved. Electrical tests show that such an isolation trench can electrically isolate the MEMS structures effectively from each other and from on-chip detection circuits. The average resistance in the range of 0–100 V is more than 10 12 , and the <phrase>breakdown voltage</phrase> is above 205 V. This technology has been successfully employed in the fabrication of the monolithic integrated bulk micromachining MEMS gyroscope.
Power Crisis in <phrase>SoC Design</phrase>: Strategies for Constructing <phrase>Low-Power</phrase>, <phrase>High-Performance</phrase> SoC Designs This special panel session brings together several leading technologists to discuss the challenges and solutions in constructing SoC designs that achieve their performance goals within a very tight power budget. These challenges are addressed from the often conflicting perspectives of semiconductor design teams and commercial solutions providers of EDA construction tools, EDA <phrase>analysis tools</phrase> and semiconductor IP (SIP). With the addition of <phrase>high-performance</phrase> features to battery-operated devices and packages <phrase>hitting</phrase> thermal limits in desktop and server devices, automated SoC <phrase>low-power</phrase> <phrase>design methodology</phrase> is at a crisis. In order for designers to develop algorithms and partition their systems for optimized power, they must depend on a set of power-efficient and accurately power-characterized SIP components (standard cells, memories, I/O, and <phrase>mixed signal</phrase>). Designers then construct their SoCs using a long chain of EDA <phrase>design tools</phrase> from design planning, simulation, and synthesis, to placement and routing. They must analyze these designs using a chain of extraction and verification tools for timing, power, test, design rule compliance and <phrase>signal integrity</phrase>. Traditionally, synthesis tools have focussed on achieving timing closure, routers with area minimization and floor <phrase>planners</phrase> arbitrating between those two often conflicting goals. Dynamic and static power minimization have been relegated to <phrase>analysis tools</phrase> that point out problems designed-in by the construction tools. Today's <phrase>deep submicron CMOS</phrase> transistors are incredibly dense, amazingly fast, but ravenously hungry in dynamic and static power— often making the task of <phrase>power optimization</phrase> the major design challenge. The results are often missed power budgets, larger batteries and heavy heat sinks. <phrase>SoC design</phrase> is a <phrase>team sport</phrase> requiring the close coordination of many IC design disciplines with the commercial solutions providers of EDA tools and SoC IP. Specifically: • What are the most significant power issues? • Who should drive the SIP power characterization standards shared by the construction and <phrase>analysis tools</phrase>? • Where do designers need the most <phrase>low power</phrase> <phrase>design methodology</phrase> <phrase>leadership</phrase> to be successful and from where is it going to come?
Situated interaction on spatial topics Acknowledgements Several people and institutions were involved in process of creating this work, and I would like to express my gratefulness towards them here. Professor Wahlster provided me with the chance to work on my PhD at his chair, and I would like to thank him for finding the time to advise me in spite of the tremendous amount of his other duties. Professor Freaks agreed to become my second advisor, for which I am very grateful. The Klaus Tschira Foundation (KTS) supported me – either directly or indirectly – throughout nearly the entire time I was working on my PhD. I would like to express my gratitude for providing me with two project grants (MapTalk and SISTO), not only towards the KTS but also towards Klaus Tschira personally. The work has also been partially supported by the <phrase>Deutsche Forschungsgemeinschaft</phrase> (DFG) within the Collaborative <phrase>Research Center</phrase> 378. A thesis such as this one is never realized in the void. I would therefore like to thank my colleagues at the European <phrase>Media Lab</phrase> as well as those at other institutes, who worked hard with me to realize Deep Map, for the good cooperation. The same applies to my coworkers at the German <phrase>Research Center</phrase> for <phrase>Artificial Intelligence</phrase> and at the chair of professor Wahlster. I also want to thank the coauthors of the publications that I was involved in for inspiring conversations and for jointly working out many ideas. I am especially thankful for the constructive criticism from the readers of the many draft versions of this thesis: Short abstract In this thesis, we present a model and an implementation to handle situational interactions on spatial topics as well as several adaptation strategies to cope with common problems in <phrase>real-world applications</phrase>. The model is designed to incorporate situational factors in spatial reasoning processes at the basic level and to facilitate its use in a wide range of applications. The implementation realizing the model corresponds very closely to the structure of the model, and was put to test in a scenario of a mobile <phrase>tourist</phrase> guide. The adaptation strategies address the lack of information , resource restrictions as well as the problem of varying availability and quality of positional information. Abstract The interaction on spatial topics is highly important no only in the context of mobile and situated systems but also in other fields such as <phrase>natural language</phrase> access to maps or …
The effects of bilateral <phrase>subthalamic nucleus</phrase> <phrase>deep brain stimulation</phrase> (<phrase>STN DBS</phrase>) on cognition in <phrase>Parkinson disease</phrase>. The effects of <phrase>subthalamic nucleus</phrase> (STN) stimulation on cognition and mood have not been well established. The authors estimated cognitive and mood effects of bilateral <phrase>subthalamic nucleus</phrase> <phrase>deep brain stimulation</phrase> (<phrase>STN DBS</phrase>) in patients with Parkinson's disease (PD) at 6 months and 1 year postoperatively. Forty-six patients were recruited from the <phrase>Movement Disorder</phrase> Center at <phrase>Seoul</phrase> <phrase>National University Hospital</phrase>. Neuropsychologic tests were performed three times, before, 6 months after, and 1 year after surgery. Mean patient age was 58 and mean education duration 8 years. Eighteen of the 46 patients were men. The instruments used for assessing cognitive functions were; the <phrase>Mini</phrase>-<phrase>Mental Status Examination</phrase> (MMSE), the <phrase>Trail</phrase> Making Test (TMT), the <phrase>Korean</phrase> <phrase>Boston</phrase> Naming Test (K-BNT), the Rey-Kim Memory Battery, the Grooved pegboard test, the Stroop test, a fluency test, the <phrase>Wisconsin</phrase> Card Sorting test (WCST), and the <phrase>Beck depression inventory</phrase> (<phrase>BDI</phrase>). Of these tests, the verbal memory test, the Stroop test, and the fluency test showed <phrase>statistically significant</phrase> changes. The verbal memory test using the Rey-Kim memory battery showed a decline in delayed recall and recognition at 6 months and 1 year postoperatively, whereas nonverbal memory showed no meaningful change. In terms of <phrase>frontal lobe</phrase> function tests, Stroop test and fluency test findings were found to be aggravated at 6 months and this continued at 1 year postoperatively. <phrase>Previous studies</phrase> have consistently reported a reduction in verbal fluency and improvements in self-reported symptoms of depression after <phrase>STN DBS</phrase>. However, in the present study, <phrase>Beck depression inventory</phrase> (B.D.I.) was not significantly changed. Other tests, namely, MMSE, TMT, K-BNT, Grooved pegboard test, and the WCST also failed to show significant changes. Of the baseline characteristics, age at onset, number of years in full-time education, and <phrase>L-dopa</phrase> equivalent dosage were found to be correlated with a postoperative decline in <phrase>neuropsychological test</phrase> results. The correlation of motor improvement and cognitive deterioration was not significant, which suggests that the stimulation effect is rather confined to the motor-related part in the STN. In conclusion, bilateral <phrase>STN DBS</phrase> in <phrase>Parkinson's disease</phrase> did not lead to a significant global deterioration in <phrase>cognitive function</phrase>. However, our <phrase>findings suggest</phrase> that it has minor detrimental <phrase>long-term</phrase> impacts on memory and <phrase>frontal lobe</phrase> function.
Supporting children's learning with body-based metaphors in a <phrase>mixed reality</phrase> environment We describe an approach to designing immersive learning experiences for children using <i>body-based metaphors</i>. <phrase>Previous research</phrase> shows benefits for learning through physical interactions in virtual spaces (e.g., [1, 16])---here we look specifically at using <phrase>mixed reality</phrase> to embed children as elements within the systems they are attempting to learn. Using gross body-movements the children are able to test predictions and have their intuitions challenged, laying the foundation for deeper conceptual understanding. We present data from a study we conducted comparing the <phrase>mixed reality</phrase> experience with a desktop version of the same simulation. <phrase>Results suggest</phrase> that children's interactions with designs supporting body-based metaphors can lead them to better grasp the "<phrase>deep structure</phrase>" of the learning domain.
Extracting Relations with Integrated Information Using <phrase>Kernel Methods</phrase> Entity relation detection is a form of <phrase>information extraction</phrase> that finds predefined relations between pairs of entities in text. This paper describes a relation detection approach that combines clues from different levels of syntactic processing using <phrase>kernel methods</phrase>. Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis. Each source of information is represented by kernel functions. Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels. We present an evaluation of these methods on the 2004 <phrase>ACE</phrase> relation detection task, using <phrase>Support Vector Machines</phrase>, and show that each level of syntactic processing contributes useful information for this task. When evaluated on the official <phrase>test data</phrase>, our approach produced very competitive <phrase>ACE</phrase> value scores. We also compare the SVM with KNN on different kernels.
<phrase>Deep sub-micron</phrase> <phrase>IDDQ testing</phrase>: issues and solutions The effectiveness of I/sub <phrase>DDQ/ testing</phrase> in deep sub-micron is threatened by the increased transistor sub-threshold <phrase>leakage current</phrase>. In this article, we survey possible solutions and propose a <phrase>deep sub-micron</phrase> I/sub DDQ/ <phrase>test mode</phrase>. The methodology provides means for unambiguous measurements of I/sub DDQ/ components and defect diagnosis. The effectiveness of the <phrase>test mode</phrase> is demonstrated with a <phrase>real life</phrase> example.
Adaptive acquisition and tracking for <phrase>deep space</phrase> array feed antennas The use of <phrase>radial basis function</phrase> (RBF) networks and <phrase>least squares</phrase> algorithms for acquisition and fine tracking of NASA's 70-m-<phrase>deep space network</phrase> antennas is described and evaluated. We demonstrate that such a network, trained using the computationally efficient orthogonal <phrase>least squares</phrase> algorithm and working in conjunction with an array feed compensation system, can point a 70-m-<phrase>deep space</phrase> antenna with root mean square (rms) errors of 0.1-0.5 millidegrees (mdeg) under a wide range of <phrase>signal-to-noise</phrase> ratios and antenna elevations. This pointing accuracy is significantly better than the 0.8 mdeg benchmark for communications at <phrase>Ka-band</phrase> frequencies (32 GHz). Continuous adaptation strategies for the RBF network were also implemented to compensate for antenna aging, thermal gradients, and other factors leading to time-varying changes in the antenna structure, resulting in dramatic improvements in system performance. The systems described here are currently in testing phases at NASA's Goldstone <phrase>Deep Space Network</phrase> (DSN) and were evaluated using <phrase>Ka-band</phrase> <phrase>telemetry</phrase> from the <phrase>Cassini</phrase> spacecraft.
MATHESIS: An Intelligent <phrase>Web-Based</phrase> Algebra Tutoring School This article describes an intelligent, integrated, <phrase>web-based</phrase> school for tutoring expansion and factoring of algebraic expressions. It provides full support for the management of the usual teaching tasks in a traditional school: Student and teacher registration, creation and management of classes and test papers, individualized assignment of exercises, intelligent <phrase>step by step</phrase> guidance in solving exercises, student interaction recording, skill mastery statistics and student assessment. The intelligence of the system lies in its Algebra Tutor, a model-tracing tutor developed within the MATHESIS project, that teaches a breadth of 16 top-level math skills (algebraic operations): <phrase>monomial</phrase> <phrase>multiplication</phrase>, <phrase>division</phrase> and power, <phrase>monomial</phrase>-polynomial and polynomial-polynomial <phrase>multiplication</phrase>, parentheses elimination, collect like terms, identities (square of sum and difference, product of sum by difference, <phrase>cube</phrase> of sum and difference), factoring (common factor, term grouping, identities, <phrase>quadratic form</phrase>). These skills are further decomposed in simpler ones giving a deep domain expertise model of 104 primitive skills. The tutor has two novel features: a) it exhibits intelligent task recognition by identifying all skills present in any expression through intelligent parsing, and b) for each identified skill, the tutor traces all the sub-skills, a feature we call deep model tracing. Furthermore, based on these features, the tutor achieves broad knowledge monitoring by recording student performance for all skills present in any expression. Forty teachers who evaluated the system in a 3-hours workshop appreciated the <phrase>fine-grained</phrase> <phrase>step-by-step</phrase> guidance of the student, the equally <phrase>fine grained</phrase> student model created by the tutor and its ability to tutor any exercise that contains the aforementioned math skills. The system was also used in a real junior <phrase>high school</phrase> classroom with 20 students for three months. Evaluation of the students' performance in the domain of factoring gave positive learning results.
Dynamic <phrase>Data Mining</phrase>: Exploring Large Rule Spaces by Sampling A great challenge for data <phrase>mining techniques</phrase> is the huge space of potential rules which can be generated. If there are tens of thousands of items, then potential rules involving three items number in the trillions. Traditional <phrase>data mining</phrase> techniques rely on downward-closed measures such as support to <phrase>prune</phrase> the space of rules. However, in many applications, such pruning techniques either do not suuciently reduce the space of rules, or they are overly restrictive. We propose a new solution to this problem, called Dynamic <phrase>Data Mining</phrase> DDM. DDM foregoes the completeness ooered by traditional techniques based on downward-closed measures in favor of the ability to drill deep into the space of rules and provide the user with a better view of the structure present in a <phrase>data set</phrase>. Instead of a single determinstic run, DDM runs continuously, exploring more and more of the rule space. Instead of using a downward-closed measure such as support to guide its exploration, DDM uses a user-deened measure called weight, which is not restricted to be downward closed. The exploration is guided by a heuristic called the Heavy Edge Property. The system incorporates user feedback b y allowing weight to be redeened dynamically. We test the system on a particularly diicult <phrase>data set</phrase> the word usage in a large subset of the <phrase>World Wide Web</phrase>. We nd that Dynamic <phrase>Data Mining</phrase> is an eeective tool for mining such diicult <phrase>data sets</phrase>.
A Comprehensive Benchmark of <phrase>Kernel Methods</phrase> to Extract Protein–<phrase>Protein Interactions</phrase> from Literature The most important way of conveying new findings in biomedical research is scientific publication. Extraction of <phrase>protein-protein interactions</phrase> (PPIs) reported in scientific publications is one of the core topics of <phrase>text mining</phrase> in the <phrase>life sciences</phrase>. Recently, a new class of such methods has been proposed - convolution kernels that identify PPIs using deep parses of sentences. However, comparing <phrase>published results</phrase> of different <phrase>PPI extraction</phrase> methods is impossible due to the use of different evaluation corpora, different evaluation metrics, different tuning procedures, etc. In this paper, we study whether the reported performance metrics are robust across different corpora and learning settings and whether the use of deep parsing actually leads to an increase in extraction quality. Our ultimate goal is to identify the one method that performs best in <phrase>real-life</phrase> scenarios, where <phrase>information extraction</phrase> is performed on unseen text and not on specifically prepared evaluation data. We performed a comprehensive <phrase>benchmarking</phrase> of nine different methods for <phrase>PPI extraction</phrase> that use convolution kernels on rich <phrase>linguistic information</phrase>. Methods were evaluated on five different public corpora using <phrase>cross-validation</phrase>, cross-learning, and cross-corpus evaluation. Our study confirms that kernels using dependency trees generally outperform kernels based on syntax trees. However, our study also shows that only the best <phrase>kernel methods</phrase> can compete with a simple <phrase>rule-based</phrase> approach when the evaluation prevents <phrase>information leakage</phrase> between <phrase>training and test</phrase> corpora. Our results further reveal that the F-score of many approaches drops significantly if no corpus-specific parameter optimization is applied and that methods reaching a good <phrase>AUC</phrase> score often perform much worse in terms of F-score. We conclude that for most kernels no sensible estimation of <phrase>PPI extraction</phrase> performance on new text is possible, given the current heterogeneity in evaluation data. Nevertheless, our study shows that three kernels are clearly superior to the other methods.
<phrase>Radar Imaging</phrase> and Sounding of Polar Ice Sheets We developed a Synthetic Apertur Radar (SAR) for imaging the <phrase>ice-bed interface</phrase>, and a wideband radar for measuring ice thickness and fine-resolution mapping of internal layers. We designed the <phrase>synthetic aperture radar</phrase> (SAR) to operate in bistatic or monostatic modes for generating two-dimensional re-flectivity maps of the bed, which can be used to determine basal conditions. The SAR operates at 80, 150 and 350 MHz. We also developed a compact, wide-band, dual-mode radar for measuring ice thickness and mapping internal layers in both shallow and deep ice. For ice thickness measurements and mapping layers at depth, it operates over the <phrase>frequency range</phrase> from 50 to 200 MHz, and for fine-resolution mapping of near surface layers it operates over 500 to 2000 MHz [1, 2]. During the 2004 field <phrase>season</phrase>, at <phrase>SUMMIT</phrase> camp on the <phrase>Greenland</phrase> <phrase>ice sheet</phrase>, we collected radar data over 3-km lines at 80, 150, and 350 MHz with HH polarization. We aquired data along parallel paths offset by 2-10 m to test the feasibility of an interferomet-ric SAR to generate basal <phrase>topography</phrase>. The <phrase>preliminary results</phrase> demontrate that the <phrase>ice-bed interface</phrase> can be imaged with the SAR operating in monostatic mode at incidence angles between 5 and 15 degrees. <phrase>Figure 1 shows</phrase> sample images collected along two offset passes. We believe that these images are the first and only successful demonstration of imaging the <phrase>ice-bed interface</phrase> through 3-km thick ice. Figure 1: SAR images of <phrase>ice-bed interface</phrase> at <phrase>SUMMIT</phrase> camp. Based on the results, we are developing a system that operates over the <phrase>frequency range</phrase> from 100 to 300 MHz to image the <phrase>ice-bed interface</phrase> with 1-10 m resolution. We will be using this sytem to collect data over <phrase>a 20</phrase>-km swath between the GISP and GRIP cores during July 05. The wide <phrase>frequency range</phrase> and fine resolution will be useful for unambiguous determination of basal conditions. Such a system will be useful for identifying frozen or liquid water on Mars, and sub-surface characterization of other <phrase>planets</phrase>. We also collected data over <phrase>a 10</phrase> km x 10 km grid with the dual-mode radar. The <phrase>results demonstrate</phrase> that we can sound 3-km thick ice and map deep internal layers with about 2 m resolution, and can map near-surface internal-layer echoes to depths of about 150 m with about 15 cm resolution, as shown in Figure 2 [3]. Figure 2: Radar echogram of near-surface internal layers at SUMIMIT. In this …
Diversity in Global <phrase>Virtual Teams</phrase>: A Partnership Development Perspective This dissertation is an attempt to develop and test a comprehensive model for <phrase>global virtual</phrase> team effectiveness based on development of partnership among diverse team members and the moderating role of <phrase>collaborative technology</phrase> and task interdependence. The model will be tested using filed survey methodology. The model is based on traditional I-P-O framework for understanding team effectiveness. Team diversity in terms of, surface level, <phrase>deep level</phrase> and functional, and diversity perceptions are treated as the central tenant of team inputs. Collaborative partnership quality is at the process level, moderated by task interdependence and use of <phrase>collaborative technology</phrase> as characterized by parallelism, transparency, and sociality. At the outcome level this dissertation is more interested in <phrase>global virtual</phrase> team effectiveness as measured by team performance and individual member satisfaction and the effect of partnership development towards relational conflict.
An <phrase>Empirical Evaluation</phrase> of Four Algorithms for <phrase>Multi-Class</phrase> Classification: Mart, ABC-Mart, Robust LogitBoost, and <phrase>ABC-LogitBoost</phrase> This empirical study is mainly devoted to comparing four <phrase>tree-based</phrase> boosting algorithms: mart, abc-mart, robust logitboost, and <phrase>abc-logitboost</phrase>, for <phrase>multi-class</phrase> classification on a variety of publicly available datasets. Some of those datasets have been thoroughly tested in prior studies using a broad range of classification algorithms including SVM, <phrase>neural nets</phrase>, and <phrase>deep learning</phrase>. In terms of the empirical classification errors, our experiment <phrase>results demonstrate</phrase>: 1. Abc-mart considerably improves mart. 2. <phrase>Abc-logitboost</phrase> considerably improves (robust) logitboost. 3. (Robust) logitboost considerably improves mart on most datasets. 4. <phrase>Abc-logitboost</phrase> considerably improves abc-mart on most datasets. 5. These four boosting algorithms (especially <phrase>abc-logitboost</phrase>) outperform SVM on many datasets. 6. Compared to the best <phrase>deep learning</phrase> methods, these four boosting algorithms (especially <phrase>abc-logitboost</phrase>) are competitive.
In Search of the Optimum <phrase>Test Set</phrase> - <phrase>Adaptive Test</phrase> Methods for Maximum <phrase>Defect Coverage</phrase> and Lowest <phrase>Test Cost</phrase> Maintaining product quality at reasonable <phrase>test cost</phrase> in very deep sub-micron process has become a major challenge especially due to multiple manufacturing locations with varying defect and parametric distributions. Increasing vector counts and <phrase>binary search</phrase> routines are now necessary for subtle defect screening. In addition, parametric tests and at-spec testing is still often necessary to ensure customer quality. Systematic defects are becoming more common and threaten to dominate the yield <phrase>Pareto</phrase>. <phrase>Adaptive test</phrase> methods are introduced in this paper that demonstrate the capability of increasing or decreasing the <phrase>test coverage</phrase> based on the predicted or measured defect and parametric behavior of the silicon being tested. Results promise an increase in product quality at the same time a reduction in test costs. 1. Introduction <phrase>Abraham</phrase> <phrase>Lincoln</phrase> said " You may deceive all the people part of the time, and part of the people all the time, but not all the people all the time. " In the quest for identifying the most optimum <phrase>test set</phrase> for defect and performance based testing an appropriate adaptation of that famous quote could be: " You can test for all of the defects part of the time, part of the defects all of the time but you cannot test for all the defects all of the time! " This sums up the test cost vs. <phrase>test quality</phrase> <phrase>trade-off</phrase> problem. The cost of applying all the " necessary " <phrase>test vectors</phrase>, temperatures, voltages, Fmax and MinVDD (with binary searches), IDDQ, Functional and At-Speed patterns, <phrase>Path Delay</phrase> tests, I/O tests and diagnostic patterns is excessive and certainly not in line with our profitability targets. However, the behavior of defects in Very Deep Sub-Micron (VDSM) processes is systematic and often design dependant in nature [1]. This is increasing the need to adapt new <phrase>test methods</phrase> to supplement the traditional stuck-at tests. Also, consider the fact that if an IC is <phrase>defect-free</phrase> and meets all of the performance requirements, it does not need to be tested at all! The same issue applies for burn-in where often 100% of the ICs are burned in even though a very small percentage of the ICs have latent defects. The burn-in of <phrase>defect-free</phrase> die actually reduces the wear out lifetime of the IC, increasing danger of damage due to handling, <phrase>thermal runaway</phrase> or overstress. The obvious problem is that we do not know, in advance, which ICs are <phrase>defect-free</phrase> and which ones meet …
Simple <phrase>Design Tools</phrase> First-year Graduate Design <phrase>Students Learn</phrase> to Quickly Evaluate Complicated Cost-performance Processor <phrase>Trade-offs</phrase> Using Simple the world sent thousands of e-mails to our group at <phrase>Stanford University</phrase> inquiring about the " secret " performance report of the <phrase>AMD</phrase> <phrase>K7</phrase> and Intel Coppermine chips. According to The Register <phrase>Web site</phrase> in the <phrase>United Kingdom</phrase> , there were rumors that some students at <phrase>Stanford University</phrase> had tested and compared the two chips using SPEC's <phrase>test suite</phrase>. At that time, the <phrase>AMD</phrase> <phrase>K7</phrase> (now known as <phrase>Athlon</phrase>) and the Intel Coppermine (now known as <phrase>Pentium III</phrase>) were not available to the general public, and it is easy to understand the excitement about this news. The real story was simpler. The students in our processor design class did not test the real chips. They estimated the cost and performance of the two chips as part of a case study. This study was based on information available from various public sources. 3 As the chip <phrase>implementation details</phrase> had not been released to the public, the instructors assumed the hardware designs were similar to the simulator default configurations. After comparing the performance and the costs, the students used the simulator tools to design improvements to each chip. Designing <phrase>deep-submicron</phrase> microprocessors is becoming an increasingly tedious and complicated process. 4 It takes many years and many engineers to design a commercial processor chip. In an introductory computer architecture course, students are usually required to simulate a simplified pipelined microprocessor such as DLX 5 using some <phrase>hardware description language</phrase> (typically Verilog or VHDL) or some graphical tool (such as HASE). 6 While it is important for a student to understand how a basic processor operates, students may not fully appreciate the complexity and the various <phrase>trade-offs involved in</phrase> designing a processor in the commercial environment. Students cannot afford to write tens of thousands of lines of code to model processor <phrase>microarchitecture</phrase>; it is unproductive to ask them to deal with the myriad details at this stage. Instead, we focus on <phrase>high-level</phrase> issues involving cost (area) and performance (execution time). The <phrase>main issues</phrase> are cache size, cycle time, <phrase>floating-point unit</phrase> (<phrase>FPU</phrase>) area, latencies, branch strategy, and issue width. By emphasizing a few primary <phrase>high-level</phrase> issues, students gain a better understanding of the <phrase>trade-offs involved in</phrase> overall computer architecture design. Table 1 lists the architectural <phrase>design tools</phrase> available in our class. Students use these tools in conjunction with the class <phrase>textbook</phrase>, 7 and they are also available for other researchers and
A common neonatal <phrase>image phenotype</phrase> predicts adverse neurodevelopmental outcome in children born preterm <phrase>Diffuse white matter injury</phrase> is common in <phrase>preterm infants</phrase> and is a candidate substrate for later cognitive impairment. This injury pattern is associated with morphological changes in deep grey nuclei, the localization of which is uncertain. We test the hypotheses that <phrase>diffuse white matter injury</phrase> is associated with discrete focal tissue loss, and that this <phrase>image phenotype</phrase> is associated with impairment at 2years. We acquired <phrase>magnetic resonance images</phrase> from 80 <phrase>preterm infants at</phrase> <phrase>term equivalent</phrase> (mean <phrase>gestational age</phrase> 29(+6)weeks) and 20 control infants (mean GA 39(+2)weeks). <phrase>Diffuse white matter injury</phrase> was defined by abnormal apparent diffusion coefficient values in one or more <phrase>white matter</phrase> region (frontal, central or posterior <phrase>white matter</phrase> at the level of the centrum semiovale), and morphological difference between groups was calculated from 3D images using deformation based morphometry. Neurodevelopmental assessments were obtained from <phrase>preterm infants at</phrase> a mean chronological age of 27.5months, and from controls at a mean age of 31.1months. We identified a common <phrase>image phenotype</phrase> in 66 of 80 <phrase>preterm infants at</phrase> <phrase>term equivalent</phrase> comprising: <phrase>diffuse white matter injury</phrase>; and tissue volume reduction in the dorsomedial nucleus of the thalamus, the <phrase>globus</phrase> pallidus, periventricular <phrase>white matter</phrase>, the <phrase>corona</phrase> radiata and within the central region of the centrum semiovale (t=4.42 p<0.001 <phrase>false discovery rate</phrase> corrected). The abnormal <phrase>image phenotype</phrase> is associated with reduced median developmental quotient (DQ) at 2years (DQ=92) compared with control infants (DQ=112), p<0.001. These findings indicate that specific neural systems are susceptible to maldevelopment after <phrase>preterm birth</phrase>, and suggest that neonatal <phrase>image phenotype</phrase> may serve as a useful <phrase>biomarker</phrase> for studying mechanisms of injury and the effect of putative therapeutic interventions.
Using Svg and <phrase>Xslt</phrase> for Graphic Representation Biography Andres Baravalle is a full-time Ph.D. student at <phrase>Turin</phrase> University, Department of <phrase>Informatics</phrase>. His actual research topics are <phrase>artificial intelligence</phrase>, <phrase>human-computer interaction</phrase>, usability. He took his degree summa cum laude in Communications at <phrase>Turin</phrase> University. His degree thesis discusses about different technologies for information storage (databases, XML etc.) and about languages and techniques for multimodal web interaction with desktop and mobile users. Economical aspects concerning implementation are also covered. Among his key qualifications he lists a deep knowledge of the main languages and technologies related to <phrase>web development</phrase>. He worked at several projects as consultant. Among them he worked to develop multimodal interaction systems based on XML and server side technologies for the <phrase>web sites</phrase> costameno.it and loescher.it. Publications "Remote web <phrase>usability testing</phrase>: a proxy approach", to be published at HCI2003, Creete. "Remote web <phrase>usability testing</phrase>", Measuring Behaviour 2002, <phrase>Amsterdam</phrase>. "A usable web for long-stay hospitalised children", HCI2002, <phrase>London</phrase>. Biography Vitaveska Lanfranchi is a full-time Ph.D. student at <phrase>Turin</phrase> University, Department of <phrase>Informatics</phrase>. Her actual research topics are <phrase>artificial intelligence</phrase>, <phrase>human-computer interaction</phrase>, usability. She took his degree summa cum laude in Communications at <phrase>Turin</phrase> University. Her degree thesis discusses about different languages and 08/05/2004 techniques for multimodal web interaction with desktop and mobile users. Among her key qualifications she lists a deep knowledge of the main languages and technologies related to <phrase>semantic web</phrase>. She worked at several projects as consultant. Among them she worked to develop multimodal interaction systems based on XML and server side technologies for the <phrase>web sites</phrase> costameno.it and loescher.it. Selected publications: "Remote web <phrase>usability testing</phrase>: a proxy approach", to be published at HCI2003, Creete; "Remote web <phrase>usability testing</phrase>", Measuring Behaviour 2002, <phrase>Amsterdam</phrase>: "A usable web for long-stay hospitalised children", HCI2002, <phrase>London</phrase>.
Stratified prototype selection based on a <phrase>steady-state</phrase> memetic algorithm: a study of scalability Prototype selection (PS) is a suitable data reduction process for refining the <phrase>training set</phrase> of a <phrase>data mining</phrase> algorithm. Performing PS processes over existing datasets can sometimes be an inefficient task, especially as the size of the problem increases. However, in recent years some techniques have been developed to avoid the drawbacks that appeared due to the lack of scalability of the classical PS approaches. One of these techniques is known as <phrase>stratification</phrase>. In this study, we test the combination of <phrase>stratification</phrase> with a <phrase>previously published</phrase> <phrase>steady-state</phrase> memetic algorithm for PS in various problems, ranging from 50,000 to more than 1 million instances. We perform a comparison with some well-known PS methods, and make a deep study of the effects of strati-fication in the behavior of the selected method, focused on its time complexity, accuracy and convergence capabilities. Furthermore, the <phrase>trade-off</phrase> between accuracy and efficiency of the proposed combination is analyzed, concluding that it is a very suitable option to perform PS tasks when the size of the problem exceeds the capabilities of the classical PS methods.
Multimodality Imaging of the Peripheral Venous System The purpose of this article is to review the spectrum of <phrase>image-based</phrase> diagnostic tools used in the investigation of suspected <phrase>deep vein thrombosis</phrase> (DVT). Summary of the experience gained by the author as well as relevant publications, regarding vein <phrase>imaging modalities</phrase> taken from a computerized database, was reviewed. The <phrase>imaging modalities</phrase> reviewed include phlebography, color Doppler duplex <phrase>ultrasonography</phrase> (CDDUS), computerized tomography <phrase>angiography</phrase> (<phrase>CTA</phrase>) and venography (<phrase>CTV</phrase>), <phrase>magnetic resonance</phrase> venography (MRV), and <phrase>radionuclide</phrase> venography (RNV). CDDUS is recommended as the modality of choice for the diagnosis of DVT. A strategy combining clinical score and <phrase>D-dimer</phrase> test refines the selection of patients. Phlebography is reserved for discrepant noninvasive studies.
rSW-seq: Algorithm for detection of <phrase>copy number</phrase> alterations in <phrase>deep sequencing</phrase> data BACKGROUND Recent advances in sequencing technologies have enabled generation of <phrase>large-scale</phrase> <phrase>genome sequencing</phrase> data. These data can be used to characterize a variety of genomic features, including the DNA <phrase>copy number</phrase> profile of a cancer genome. A robust and reliable method for screening <phrase>chromosomal</phrase> alterations would allow a detailed characterization of the cancer genome with unprecedented accuracy.   RESULTS We develop a method for identification of <phrase>copy number</phrase> alterations in a tumor genome compared to its matched control, based on application of Smith-Waterman algorithm to single-end sequencing data. In a performance test with simulated data, our algorithm shows >90% sensitivity and >90% precision in detecting a single <phrase>copy number</phrase> change that contains approximately 500 reads for the normal sample. With 100-bp reads, this corresponds to a ~50 kb region for 1X genome coverage of the <phrase>human genome</phrase>. We further refine the algorithm to develop rSW-seq, (recursive Smith-Waterman-seq) to identify alterations in a complex configuration, which are commonly observed in the human cancer genome. To validate our approach, we compare our algorithm with an existing algorithm using simulated and publicly available datasets. We also compare the sequencing-based profiles to <phrase>microarray</phrase>-based results.   CONCLUSION We propose rSW-seq as an efficient method for detecting <phrase>copy number</phrase> changes in the tumor genome.
A Linux-based Real-time Operating System This work describes the design, implementation, and possible applications of Real-Time Linux | a hard real-time version of the Linux operating system. In this system, a standard <phrase>time-sharing</phrase> OS and a real-time executive run on the same computer. Interrupt controller emulation is used to guarantee a low maximum interrupt latency independently of the base system. The use of a one-<phrase>shot</phrase> timer makes it possible to achieve a low task release <phrase>jitter</phrase> without compromising throughput. Lock-free <phrase>FIFO</phrase> buuers are employed for communication between real-time tasks and Linux processes. User-deened schedulers are allowed as are <phrase>run-time</phrase> changes in the scheduling policy. The system is in active use for real-time <phrase>data acquisition</phrase>, control, and communications. Acknowledgements I would like to express my deep appreciation to my advisor, Victor Yodaiken, for his support , encouragement, and friendship. His original ideas, <phrase>comments and suggestions</phrase> were invaluable. His help was essential in bringing this thesis to completion. I also wish to thank Victor for his patience in answering my numerous questions. I thank other members of my committee: Dr. Lassez and Dr. Mazumdar, for their time and eeort in reviewing my thesis. I found Dr. Lassez's comments on my presentation style to be immensely helpful. I am grateful to Dr. Mazumdar for encouraging me to perform more substantial testing of the system. Thanks to Yuri G. <phrase>Karpov</phrase>, my academic advisor in <phrase>Russia</phrase>, for giving me an opportunity to study in the US and helping me deal with many important matters. Last, but not least, I would like to thank Olga Tomina for her patience and understanding , and my parents, <phrase>Alexander</phrase> and Irina, for everything good they have done for me.
Mixed PLB and Interconnect BIST for FPGAs Without <phrase>Fault-Free</phrase> Assumptions We tackle the problem of <phrase>fault-free</phrase> assumptions in current PLB and interconnect built-in-self-test (BIST) techniques for FPGAs. These assumptions were made in order to develop strong BIST methods for one class of components (PLBs or interconnects) while assuming that the other class is <phrase>fault-free</phrase>. This results in a cyclical conundrum that renders current PLB and interconnect <phrase>BIST techniques</phrase> impractical, since current <phrase>deep-submicron</phrase> FPGAs as well as those of emerging single-digit <phrase>nanometer technologies</phrase> are expected to have a profusion of hard (permanent) PLB as well as interconnect faults. We address this issue here and develop a novel method M-BIST that uses a combination of (i) iterative <phrase>bootstrapping</phrase> that without any knowledge of the state of any PLB or interconnect determines a minimum contingent of <phrase>fault-free</phrase> test circuit components with high probability , and (ii) mixed testing of PLBs and interconnects in an interleaved manner that identifies <phrase>fault-free</phrase> components that are used in subsequent testing phases until the entire FPGA is tested. This approach is overlaid on <phrase>current state</phrase>-of-the-art PLB and interconnect <phrase>BIST techniques</phrase>. <phrase>Simulation results</phrase> obtained for faults present in both PLBs and interconnects show significant improvements in both fault coverage and <phrase>false positives</phrase> yielded by M-BIST compared to the PLB-only and interconnect-only <phrase>BIST techniques</phrase> used within the M-BIST wrapper that make <phrase>fault-free</phrase> assumptions about the other component type.
<phrase>Glaucoma</phrase> Surgery: Taking the Sub-Conjunctival Route We are currently in the midst of a surge in interest in <phrase>glaucoma</phrase> surgery. Novel pathways for reducing <phrase>intraocular pressure</phrase> (IOP) have been tried with various levels of success over the last few years. While the trabecular bypass and suprachoroidal approaches have captured much of the attention, filtering aqueous into the sub-conjunctival space remains the <phrase>gold standard</phrase> for lowering IOP. This review attempts to focus on current research in surgical methods to enhance <phrase>filtration</phrase> by potentially improving on tried and tested methods like the trabeculectomy, deep sclerectomy, and tube surgeries.
Overview of the NASA/JPL Lasercom program The NASA-funded opticai communications program being conducted at JPL is described, The spacecraft transceiver terminal developments, a test infrastructure for assessing the performance of future space terminals, an atmospheric visibility monitoring program, some recent systems-level demonstrations, and a communications technology roadmap for planetary missions over the next 25 years are presented. 1. INTRODUCTION Communications demands for spacecraft are ever-increasing and the technology required to satisfy those link demands often dominates the archkecture of the spacecraft structures. This has been true for some time on military and NASA missions, and more recently has become a driver for many proposed commercial satellite networks, Accordingly, NASA has been developing <phrase>optical communications</phrase> technology so that titure missions can satisfy those demands with much less impact on the space platforms, or the launch vehicles required to lift them off the Earth's surface. Studies, <phrase>technology development</phrase>, <phrase>systems design</phrase> and deployment planning for this technology have been underway at NASA's <phrase>Jet Propulsion Laboratory</phrase> for the past 18 years [1,2]. In this paper the <phrase>optical communications</phrase> space terminai technology being developed to address these applications will be described. Next, the development of a test stand to evaluate this such spacecraft terminals is described. Following this,, a program to gather detailed statistics on the cloud-cover outages for space-to-ground links will be described, including the data distributions that have been produced from those data. Finally, a roadmap for how thk technology can augment or enable deep-<phrase>space missions</phrase> of the future will be described. 2, OPTICAL SPACECRAFT TERMINAL DEVELOPMENT The centerpiece of the NASA spacecraft <phrase>technology development</phrase> is the Opticai Communications Demonstrator (OCD) program [3]. This program is developing an engineering model of a flight terminal capable of returning kbps to Mbps from the <phrase>planets</phrase>, // or Mbps-bps from high-<phrase>Earth-orbit</phrase> to the ground. The system uses a " minimum-complexity " architecture that uses only one detector array and one fine steering mirror to accomplish beacon signal acquisition, tracking, transmit beam pointing, and transmit/receive co-alignment (with point-ahead to accommodate cross velocity). Tracking of the beacon signal is accomplished by using a windowed sub-frame readout from the detector array. Initial development of the concept involved setting up a tracking system <phrase>breadboard</phrase> in the laboratory. This was followed by a contract with 20/20 Systems Inc. to package the <phrase>CCD/Camera</phrase> readout electronics, and the Tracking Processor Assembly (TPA) , which determines, filters and conditions the tracking error signals for actuation of the steering mirror (and …
<phrase>Voxel</phrase>-based analysis derived from fractional anisotropy images of <phrase>white matter</phrase> volume changes with aging Although age-related effects on brain volume have been extensively investigated post mortem and in vivo using <phrase>magnetic resonance imaging</phrase> (MRI), regional and temporal patterns of <phrase>white matter</phrase> (WM) volume changes with aging are not defined yet. The aim of this study was to assess the topographical distribution of age-related WM volume changes using a <phrase>recently developed</phrase> <phrase>voxel</phrase>-<phrase>based method</phrase> to obtain estimates of WM <phrase>fiber bundle</phrase> volumes using diffusion <phrase>tensor</phrase> (DT) MRI. Brain conventional and DT MRI were obtained from 84 healthy subjects (mean age=44 years, range=13-70). Linear and non-linear relationships between age and WM <phrase>fiber bundle</phrase> volume changes were tested. A negative linear correlation was found between age and WM volume decline in the <phrase>corona</phrase> radiata, anterior cingulum, body and crus of the fornix and left superior cerebellar <phrase>peduncle</phrase>. A positive linear correlation was found between age and volume increase of the right deep temporal association fibers. The non-<phrase>linear regression</phrase> analysis also showed age-related changes of the genu of the <phrase>corpus callosum</phrase> and fitted better the volume changes of the right deep temporal association fibers. WM volume decline with age is unevenly distributed across brain regions. Our approach holds promise to gain <phrase>additional information</phrase> on the pathological changes associated to <phrase>neurological disorders</phrase> of the elderly.
Performance Analysis of <phrase>Adsl</phrase> This study will present a deep research regarding the characteristics of <phrase>ADSL</phrase> in term of its <phrase>advantages and disadvantages</phrase>. The <phrase>performance analysis</phrase> concentrates on the impact of the services. Tests are being carried out to prove that the performance is affected by loop length and also noise issue as well as their impairment issues especially crosstalk. Crosstalk modeling is performed to discuss the performance of <phrase>ADSL</phrase> with the supports of a proposed statistical methodology. At the end of the study, a comparison of <phrase>ADSL</phrase> with other copper using fixed line broadband communication system, <phrase>Cable Modem</phrase> is discussed.
Injecting Various Faults for the Dependability Validation of Commercial <phrase>Microcontrollers</phrase> Faults will be a great challenge in modern <phrase>VLSI circuits</phrase>. Different faults are injected into the VHDL model of a commercial microcontroller and their effects have been compared. Bridging fault and <phrase>stuck-at faults</phrase> are injected. The methodology used is VHDL based <phrase>fault injection</phrase> technique, which allows an exhaustive analysis of the influence of different <phrase>fault models</phrase> and system parameters. <phrase>Fault models</phrase> are used at logic and RTL levels. From the simulation result, the occurrences of failures and have been found out. Fault coverage and <phrase>test coverage</phrase> are found out using DFT compiler and Tetramax tool. I.INTRODUCTION Faults will have a great impact in deep submicron technologies. <phrase>Fault models</phrase> used are <phrase>stuck-at fault</phrase>, bridging fault, <phrase>delay fault</phrase>. A <phrase>stuck-at fault</phrase> is a particular <phrase>fault model</phrase> used by fault simulators and <phrase>Automatic test pattern generation</phrase> (ATPG) tools to mimic a manufacturing defect within an <phrase>integrated circuit</phrase>. Individual signals and pins are assumed to be stuck at Logical '1', '0' and 'X. The fault can be at an input or output of a gate. The <phrase>stuck at fault</phrase> model assumes that only one input on one gate will be faulty at a time, assuming that if more are faulty, a test that can detect any single fault, should easily find multiple faults. To use this <phrase>fault model</phrase>, each input pin on each gate in turn, is assumed to be grounded, and a <phrase>test vector</phrase> is developed to indicate the circuit is faulty. Two signals are connected together when they should not be. Depending on the logic circuitry employed, this may result in a wired-OR or wired-AND logic function. In order to study the impact of various faults, a methodology based on <phrase>fault injection</phrase> is used. <phrase>Fault injection</phrase> technique allows a controlled introduction of faults in the system, not being necessary to wait for a long time to log the apparition of real faults. VHDL based <phrase>fault injection</phrase> can be a very suitable option due to its flexibility as well as the high observability and controllability of all the model components. II. <phrase>FAULT INJECTION</phrase> EXPERIMENTS. <phrase>Fault injection</phrase> experiments were carried out on the VHDL model of 8051 microcontroller. <phrase>Stuck-at fault</phrase>, ,<phrase>bridging faults</phrase> are injected into the <phrase>RAM</phrase> and PSW register of 8051 microcontroller. A <phrase>stuck-at fault</phrase> is a particular <phrase>fault model</phrase> used by fault simulators and <phrase>Automatic test pattern generation</phrase> (ATPG) tools to mimic a manufacturing defect within an <phrase>integrated circuit</phrase>. Individual signals and …
Atpg for Crosstalk Using Hybrid Structural Sat As technology evolves into the deep sub-micron era, <phrase>signal integrity</phrase> problems are growing into a major challenge. An important source of <phrase>signal integrity</phrase> problems is the <phrase>crosstalk noise</phrase> generated by coupling capacitances between wires. <phrase>Test vectors</phrase> that activate and propagate <phrase>crosstalk noise</phrase> effects are becoming an essential part of <phrase>design verification</phrase> and <phrase>manufacturing test</phrase>. However, deriving such vectors is a complex task. In this paper, we propose HyAC, a fast yet accurate hybrid ATPG method targeting multiple-aggressor induced crosstalk errors. Given a victim and a set of aggressors, the proposed ATPG method searches for <phrase>test vectors</phrase> to activate and propagate a crosstalk error for the victim. Due to logic constraints, it may not be possible to trigger all aggressors simultaneously. Therefore, firstly we use an implication graph (IG) that consists of logic variables and structural information to check for logic conflicts. If the current set of aggressors is not feasible, our algorithm automatically searches for the next-best subset of aggressors (resulting in the largest noise). After a set of feasible aggressors is identified, we use a modified PODEM [21] algorithm to search for <phrase>test vectors</phrase>. This hybrid structural SAT-based ATPG method inherits advantages from both <phrase>Boolean</phrase>– satisfiabilitity <phrase>based methods</phrase> and structural-<phrase>based methods</phrase> to achieve flexibility and efficiency. We demonstrate the accuracy, <phrase>high quality</phrase>, and <phrase>run time</phrase> efficiency of HyAC through experiments conducted on several <phrase>benchmark circuits</phrase> as well as a circuit from a commercial processor.
Syntactic discriminative <phrase>language model</phrase> rerankers for <phrase>statistical machine translation</phrase> This article describes a method that successfully exploits <phrase>syntactic features</phrase> for n-best translation candidate reranking using perceptrons. We motivate the utility of syntax by demonstrating the superior performance of parsers over <phrase>n-gram</phrase> language models in differentiating between <phrase>Statistical Machine Translation</phrase> output and human translations. Our approach uses discriminative language modelling to rerank the n-best translations generated by a <phrase>statistical machine translation</phrase> system. The performance is evaluated for Arabic-to-English translation using NIST's MT-Eval benchmarks. While deep features extracted from <phrase>parse trees</phrase> do not consistently help, we show how features extracted from a shallow Part-of-Speech annotation layer outper-form a competitive baseline and a state-of-the-art comparative reranking approach, leading to significant BLEU improvements on three different <phrase>test sets</phrase>.
Stacked <phrase>Extreme Learning</phrase> Machines <phrase>Extreme learning</phrase> machine (ELM) has recently attracted many researchers' interest due to its very fast learning speed, good <phrase>generalization ability</phrase>, and ease of implementation. It provides a unified solution that can be used directly to solve regression, binary, and <phrase>multiclass</phrase> <phrase>classification problems</phrase>. In this paper, we propose a stacked ELMs (S-ELMs) that is specially designed for solving large and complex data problems. The S-ELMs divides a single large ELM network into multiple stacked small ELMs which are serially connected. The S-ELMs can approximate a very large ELM network with small memory requirement. To further improve the testing accuracy on <phrase>big data</phrase> problems, the ELM autoencoder can be implemented during each iteration of the S-ELMs algorithm. The simulation results show that the S-ELMs even with random hidden nodes can achieve similar testing accuracy to <phrase>support vector machine</phrase> (SVM) while having low memory requirements. With the help of ELM autoencoder, the S-ELMs can achieve much better testing accuracy than SVM and slightly better accuracy than <phrase>deep belief network</phrase> (DBN) with much faster training speed.
Studying the Characteristics of a "Good" GUI <phrase>Test Suite</phrase> The widespread deployment of <phrase>graphical-user interfaces</phrase> (GUIs) has increased the overall complexity of testing. A <phrase>GUI test</phrase> designer needs to perform the daunting task of adequately testing the GUI, which typically has very large input interaction spaces, while considering tradeoffs between GUI <phrase>test suite</phrase> characteristics such as the number of <phrase>test cases</phrase> (each modeled as a sequence of events), their lengths, and the event composition of each <phrase>test case</phrase>. There are no published empirical studies on GUI testing that a <phrase>GUI test</phrase> designer may reference to make decisions about these characteristics. Consequently, in practice, very few GUI testers know how to design their <phrase>test suites</phrase>. This paper takes the first step towards assisting in <phrase>GUI test</phrase> design by presenting an empirical study that evaluates the effect of these characteristics on testing cost and <phrase>fault detection</phrase> effectiveness. The results show that two factors significantly effect the <phrase>fault-detection</phrase> effectiveness of a test suite: (1) the diversity of states in which an event executes and (2) the event coverage of the suite. Test designers need to improve the diversity of states in which each event executes by developing a large number of short <phrase>test cases</phrase> to detect the majority of " shallow " faults, which are artifacts of modern GUI design. Additional resources should be used to develop a small number of long <phrase>test cases</phrase> to detect a small number of " deep " faults.
The UOT system: improve string-to-tree translation using <phrase>head-driven phrase structure grammar</phrase> and <phrase>predicate-argument</phrase> structures We present the UOT <phrase>Machine Translation</phrase> System that was used in the IWSLT-09 evaluation campaign. This year, we participated in the <phrase>BTEC</phrase> track for Chinese-to-English translation. Our system is based on a string-to-tree framework. To integrate deep <phrase>syntactic information</phrase>, we propose the use of <phrase>parse trees</phrase> and semantic dependencies on English sentences described respectively by <phrase>Head-driven Phrase Structure Grammar</phrase> and <phrase>Predicate-Argument</phrase> Structures. We report the results of our system on both the development and <phrase>test sets</phrase>.
Compensation of <phrase>Deep Drawing</phrase> Tools for Springback and Tool-deformation Manual tool reworking is one of the most time-consuming stages in the preparation of a <phrase>deep drawing</phrase> process. Finite Elements (FE) analyses are now widely applied to test the feasibility of the forming process, and with the increasing accuracy of the results, even the springback of a blank can be predicted. In this paper, the results of an FE analysis are used to carry out tool compensation for both springback and tool/press deformations. Especially when high-strength steels are used, or when large body panels are produced, tool compensation in the <phrase>digital domain</phrase> helps to reduce work and save time in the press workshop. A successful compensation depends on accurate and efficient FE-prediction, as well as a flexible and process-oriented compensation algorithm. This paper is divided in two sections. The first section deals with efficient modeling of tool/press deformations, but does not discuss compensation. The second section is focused on springback, but here the focus is on the compensation algorithm instead of the springback phenomenon itself.
Systematicity as a Selection Constraint in Analogical Mapping Analogy is often viewed as a partial similarity match between domains. But not all partial similarities qualify as analogy: There must be some selection of which commonalities count. Three experiments tested a particular selection constraint i n analogical mapping, namely, systemoticity. That is, we tested whether a given predicate is more likely to figure i n the interpretation of and prediction from an analogy if the predicate participates i n a common system of relations. In Experiment 1, subjects iudged two matches to be i ncl uded i n an analogy: an isolated match, and a match embedded i n a larger matching system. Subjects preferred the embedded match. In Experiments 2 and 3, subjects made analogical predictions about a target domain. Subjects predicted information that followed from a causal system that matched the base domain, rather than information that was equally plausible, but that created an isolated match with the base. Results support Gentner's (1983, 1989) structure-mapping theory i n that analogical mapping concerns systems and not i ndi vi dual predicates, and that attention to shared systematic structure constrains the selection of information to i ncl ude i n on analogy. In an analogy, a familiar domain is used to understand a novel domain in order to highlight important similarities between the domains, or to predict new features of the novel domain. For example, we use our knowledge about water flow to elucidate properties of electric circuitry. Such an analogy can lead to useful inferences, and reveal deep structural features about a domain. In this research we ask how an analogical mapping is constructed. In particular , we ask whether systematic relational structure acts as a psychological selection constraint in interpreting an analogy. An analogy can be seen as a partial similarity match between situations. But not all partial similarity matches qualify as analogy: There must be some selection of which commonalities count. The selection problem exists
Built-In Self-Test for <phrase>Signal Integrity</phrase> Unacceptable loss of <phrase>signal integrity</phrase> may harm the functionality of SoCs permanently or intermittently. We propose a systematic approach to model and test <phrase>signal integrity</phrase> in <phrase>deep-submicron</phrase> <phrase>high-speed</phrase> interconnects. Various <phrase>signal integrity</phrase> problems occurring on such interconnects (e.g. crosstalk, overshoot, noise, skew, etc.) are considered in a unified model. We also present a <phrase>test methodology</phrase> that uses a noise detection circuitry to detect low integrity signals and an inexpensive test architecture to measure and read the statistics for final observation and analysis.
Understanding <phrase>deep learning</phrase> requires rethinking generalization Despite their massive size, successful deep <phrase>artificial neural networks</phrase> can exhibit a remarkably small difference between <phrase>training and test</phrase> performance. <phrase>Conventional wisdom</phrase> attributes small generalization error either to properties of the model family , or to the regularization techniques used during training. Through extensive systematic experiments, we show how these <phrase>traditional approaches</phrase> fail to explain why large <phrase>neural networks</phrase> generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for <phrase>image classification</phrase> trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two <phrase>neural networks</phrase> already have perfect <phrase>finite sample</phrase> expressivity as soon as the number of parameters exceeds the number of <phrase>data points</phrase> as it usually does in practice. We interpret our experimental findings by comparison with traditional models.
Verifying Distributed Protocols using <phrase>MSC</phrase>-Assertions, <phrase>Run-time</phrase> Monitoring, and Automatic <phrase>Test Generation</phrase> 1 The research reported in this article was funded in part by a grant from the U.S. <phrase>Missile Defense Agency</phrase>. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright annotations thereon. Abstract This paper addresses the need for <phrase>formal specification</phrase> and runtime verification of system-level requirements of distributed reactive systems. It describes a formalism for specifying global system behaviors in terms of Message Sequence <phrase>Chart</phrase> assertions and a technique for the evaluation of the likelihood of success of a distributed protocol under non-trivial communication conditions via <phrase>discrete event simulation</phrase> and runtime execution monitoring. We constructed a proof-of-concept prototype for the leader-<phrase>election</phrase> algorithm within a 4-node ring network. The prototype consists of the following components: (i) an OMNeT++ model of the network using non-trivial communication conditions, (ii) C++ code for the network agents, (iii) a system-level assertion stipulating the formal requirement for a correct, time-bound, leader <phrase>election</phrase>, (iv) simulation of the formal assertion, (v) automatic scenario generation, and (vi) <phrase>run-time</phrase> monitoring of the formal assertion and stochastic-based estimation of the likelihood of success of this assertion under the specified communication conditions. 1 Introduction The design and implementation of reliable applications on top of asynchronous <phrase>distributed systems</phrase> that are prone to processor and network crashes is a difficult and complex task. A distributed system is made up of several components, executing concurrently and interacting with each other under the control of specialized procedures called protocols. Individual components usually do not have real-time knowledge of the global state of the system , and it may not even have the notion of a global clock. Moreover, whenever the application departs from its correct " state " due to processor crashes, the live processors must execute some algorithms (i.e. protocols) to restore the application back to the correct state. Runtime Execution Monitoring (REM) is a class of methods for tracking the temporal behavior of an underlying application. REM methods range from simple print statement logging methods to <phrase>run-time</phrase> tracking of complete formal requirements for verification purposes. NASA used REM to verify the flight code for its Deep Impact project [5]. A recent paper by the authors describes <phrase>run-time</phrase> verification of the <phrase>CARA</phrase> infusion <phrase>pump</phrase> using UML-statechart …
Advanced Deepwater Monitoring System This study investigates new methods to improve deepwater monitoring and addresses installation of advanced sensors on " already deployed " risers, flowlines, trees, and other deepwater devices. A major shortcoming of post installed monitoring systems in subsea is poor coupling between the sensor and structure. This study provided methods to overcome this problem. Both <phrase>field testing</phrase> in subsea environments and laboratory testing were performed. Test articles included actual flowline pipe and steel <phrase>catenary</phrase> risers up to twenty-four inches in diameter. A monitoring device resulting from this study can be installed in-situ on underwater structures and could enhance productivity and improve safety of offshore operations. This paper details the <phrase>test results</phrase> to determine coupling methods for attaching <phrase>fiber optic</phrase> sensor systems to deepwater structures that have already been deployed. Subsea attachment methods were evaluated in a forty foot deep pool by divers. Afterword, structural testing was conducted on the systems at the NASA <phrase>Johnson Space Center</phrase>. Additionally a 7,000 foot deep sensor station was attached to a flowline with the aid of a remote operated vehicle. Various sensor to pipe coupling methods were tested to measure tensile load, <phrase>shear strength</phrase> and coupling capability. Several adhesive bonding methods in a subsea environment were investigated and subsea testing yielded exceptionally good results. Tensile and shear properties of subsea application were approximately 80 percent of those values obtained in dry conditions. Additionally, a <phrase>carbide</phrase> <phrase>alloy</phrase> coating was found to increase the <phrase>shear strength</phrase> of metal to metal clamping interface by up to 46 percent. This study provides valuable data for assessing the feasibility of developing the next generation <phrase>fiber optic</phrase> sensor system that could be retrofitted onto existing subsea pipeline structures.
Stochastic analysis of interconnect performance in the presence of <phrase>process variations</phrase> Deformations in interconnect due to <phrase>process variations</phrase> can lead to significant performance degradation in deep sub-micron circuits. Timing analyzers attempt to capture the effects of variation on delay with simplified models. The timing verification of RC or RLC networks requires the substitution of such simplified models with spatial <phrase>stochastic processes</phrase> that capture the random nature of <phrase>process variations</phrase>. The present work proposes a new and viable method to compute the stochastic response of interconnects. The technique models the stochastic response in an infinite dimensional <phrase>Hilbert space</phrase> in terms of orthogonal polynomial expansions. A finite representation is obtained by using the Galerkin approach of minimizing the <phrase>Hilbert space</phrase> norm of the residual error. The key advance of the proposed method is that it provides a functional representation of the response of the system in terms of the <phrase>random variables</phrase> that represent the <phrase>process variations</phrase>. The proposed algorithm has been implemented in a procedure called <phrase>OPERA</phrase>. Results from <phrase>OPERA</phrase> simulations on commercial design <phrase>test cases</phrase> match well with those from the classical <phrase>Monte Carlo</phrase> SPICE simulations and from perturbation methods. Additionally <phrase>OPERA</phrase> shows good computational efficiency: speedup factor of 60 has been observed over <phrase>Monte Carlo</phrase> SPICE simulations. Introduction The performance of <phrase>integrated circuits</phrase> (ICs) is increasingly less predictable as device dimensions shrink below the sub-100 nanometer scale. The modeling accuracy problem stems from poor control of the physical device and interconnect characteristics during the <phrase>manufacturing process</phrase>. Uncertainties due to variations in the <phrase>manufacturing process</phrase> are reflected in variations in the <phrase>circuit parameters</phrase>. Examples of manufacturing variations are the variations in materials, variations in geometry (t <phrase>ox</phrase> L eff , W) and doping profiles of MOSFETs, material and <phrase>geometric variations</phrase> of the interconnects etc. The many sources of variations in the IC <phrase>fabrication process</phrase> lead to a hierarchy of random and systematic effects on <phrase>circuit performance</phrase> [16]. A common way of <phrase>accounting</phrase> for <phrase>process variations</phrase> is to use a <phrase>linear model</phrase> to represent a circuit parameter. For instance, a parameter p would be expressed as p = µ p + 1,p + 2,p , where µ p is a (nominal) mean value, 1,p and 2,p are <phrase>random variables</phrase> with mean zero and variances σ 1,p and σ 2,p. These represent the inter-die and <phrase>intra-die</phrase> variations, respectively. Designers interested in <phrase>performance analysis</phrase> and optimization typically use only a single value of σ p. The performance of devices and interconnects depends on several parameters …
<phrase>Space Communication</phrase> Channel Emulation Using <phrase>Digital and Analog</phrase> <phrase>Signal Processing</phrase> Title: <phrase>Space Communication</phrase> Channel Emulation Using <phrase>Digital and Analog</phrase> <phrase>Signal Processing</phrase> <phrase>Space Communication</phrase> Channel Emulation Using <phrase>Digital and Analog</phrase> <phrase>Signal Processing</phrase> New communication protocols intended for large distances, including low orbit and <phrase>deep space</phrase>, can be inherently difficult to evaluate since trial implementations are often impractical. In order to accurately measure the performance of a new protocol, it is important to evaluate it in an environment that most closely matches that in which it will be used. This thesis demonstrates the ability to emulate a space communications channel through <phrase>digitizing</phrase> a transmission centered at an <phrase>intermediate frequency</phrase> of 70 MHz with a bandwidth of 24 MHz, digitally introducing the characteristics of a transmission through space, and reconstructing the digital data to its analog counterpart. Delay, <phrase>Doppler shift</phrase>, <phrase>Gaussian noise</phrase>, and fading are among the most prevalent characteristics of such a channel, and thus were the focus of this thesis. Special care was given to the design of each <phrase>digital and analog</phrase> component to maintain the integrity of the original signal by minimizing all undesired noise introduced. The final design can accurately produce a given dynamic transmission signature or continually output a static set of channel characteristic parameters to test new communication protocols.
<phrase>Natural Language</phrase> for <phrase>Human Robot Interaction</phrase> 2. Embodied <phrase>Construction Grammar</phrase> 3. System Architecture <phrase>Natural Language Understanding</phrase> (NLU) was one of the main original goals of <phrase>artificial intelligence</phrase> and <phrase>cognitive science</phrase>. This has proven to be extremely challenging and was nearly abandoned for decades. We describe an implemented system that supports full NLU for tasks of moderate complexity. The <phrase>natural language</phrase> interface is based on Embodied <phrase>Construction Grammar</phrase> and simulation semantics. The system described here supports human dialog with an agent controlling a simulated robot, but is flexible with respect to both input language and output task. Interfaces – <phrase>Natural language</phrase>. <phrase>Natural language interfaces</phrase> have long been a topic of HRI research. Winograd's 1971 <phrase>SHRDLU</phrase> was a landmark program that allowed a user to command a simulated arm and to ask about the state of the block world (Winograd, 1971).There is currently intense interest in both the promise and potential dangers of much more capable robots. 1) Much more computation 2) NLP technology 3) <phrase>Construction Grammar</phrase>: form-meaning pairs 4) 10) General NLU <phrase>front end</phrase>: Modest effort to link to a new Action side As shown in Table 1, we believe that there have been sufficient scientific and technical advances to now make NLU of moderate scale an achievable goal. The first two points are obvious and general. All of the others except for point 8 are discussed in this paper. The CPRM mechanisms were not needed in the current system, but are essential for more complex actions and simulation (Barrett 2010). This work is based on the Embodied <phrase>Construction Grammar</phrase> (ECG), and builds on decades of work on the Neural Theory of Language (<phrase>NTL</phrase>) project. The meaning side of an ECG construction is a schema based on embodied <phrase>cognitive linguistics</phrase>. (Feldman, <phrase>Dodge</phrase>, and Bryant 2009). ECG is designed to support the following functions: 1) A formalism for capturing the shared grammar and beliefs of a language community. 2) A precise notation for technical linguistic work 3) An implemented specification for grammar testing 4) A <phrase>front end</phrase> for applications involving deep semantics 5) A high level description for neural and behavioral experiments. 6) A basis for theories and models of language learning. In this work, we focus on point 4; we are using ECG for the <phrase>natural language</phrase> interface to a robot simulator. We suggest that NLU can now be the foundation for HRI with the current generation of robots of limited complexity. Any foreseeable robot will have limited capabilities and will not be able …
MetaNet: <phrase>Deep semantic</phrase> automatic metaphor analysis This paper describes a system that makes use of a repository of formalized frames and metaphors to automatically detect, categorize, and analyze expressions of metaphor in corpora. The output of this system can be used as a basis for making further refinements to the system , as well as supporting <phrase>deep semantic</phrase> analysis of metaphor expressions in corpora. This in turn provides a way to ground and test empirical <phrase>conceptual metaphor</phrase> theory, as well as serving as a means to gain insights into the ways conceptual metaphors are expressed in language.
A Tunneling Model for Gate Oxide Failure in Deep Sub-Micron Technology Parametric failures in CMOS IC <phrase>nanoelectronics</phrase>, leads to strong detection problem. In order to develop new defect oriented <phrase>test methods</phrase>, it is of prime importance to study the behavior of the transistor affected by those kind of failures. In this paper, we present a new electricaltransistor model, which allows to study the impact of gate oxide thickness drop. It is shown that electrical behavior of the proposed model matches in a satisfactory way the defective transistor behavior.
Double-Tree Scan: A Novel <phrase>Low-Power</phrase> <phrase>Scan-Path</phrase> Architecture states at circuit nodes may erroneously change. Further, BIST schemes with random <phrase>test patterns</phrase> may need an excessive amount of energy because of longer test length. Abstract I n a <phrase>scan-based</phrase> system with a large number of <phrase>flip-flops</phrase>, a major component of power is consumed during scan-shift and clocking operation in <phrase>test mode</phrase>. In this paper, a novel <phrase>scan-path</phrase> architecture called double-tree scan (DTS) is proposed that drastically reduces the scan-shifi and clock activity during testing. The inherent combinatorial properties of double-<phrase>tree structure</phrase> are employed to design the scan architecture, clock gating logic, and a simple shift controller. The design is independent of the structure of the circuit-under-test (CUT) or its <phrase>test set</phrase>. It provides a <phrase>significant reduction</phrase> both in instantaneous and average power needed for clocking and scan-shifting. The architecture fits well to built-in self-test (BIST) scheme under <phrase>random testing</phrase>, as well as to deterministic test environment. 1. Introduction With the emergence of <phrase>mobile devices</phrase>, design of <phrase>low-power</phrase> VLSI systems has become a major concern in circuit synthesis. A significant component of the power consumed in <phrase>CMOS circuits</phrase> is caused by the <phrase>switching activity</phrase> (SA) at various circuit nodes during operation. The dynamic power consumed at a circuit node is proportional to the total number of 0 + 1 and 1-+ 0 transitions that the logic signal undergoes at that node multiplied by its capacitance and the frequency of operation. Powertenergy minimization during testing has become important in the context of <phrase>deep sub-micron technology</phrase> because of higher device densities and clock rates. In a <phrase>scan-based</phrase> system, a significant amount of power is consumed during the scan operations, as the activity in the <phrase>scan-path</phrase>, clock tree, and in the CUT becomes very high [ 11. The average-<phrase>power optimization</phrase> extends the battery life in mobile applications. Maximum sustained power over a specified limit, may cause excessive heating of the device, whereas, the instantaneous power may cause excessive (inductive) <phrase>voltage drop</phrase> in the power and ground lines because of current swing. Thus, the logic 2. Background Existing powertenergy minimization techniques include <phrase>test scheduling</phrase> [ 2 ] , toggle suppression and blocking useless patterns [13], designing <phrase>low-power</phrase> <phrase>TPG</phrase> for BIST applications [8, 121, use of Golomb coding for scan testing [7], and power-<phrase>aware ATPG</phrase> [14, 151. For deterministic testing, <phrase>power reduction</phrase> can be achieved by reordering <phrase>scan chains</phrase> and <phrase>test vectors</phrase> [4]. Compaction of <phrase>test vectors</phrase> for <phrase>low power</phrase> in a <phrase>scan-based</phrase> system …
<phrase>Deep brain stimulation</phrase> induces BOLD activation in motor and non-motor networks: An fMRI comparison study of STN and EN/GPi DBS in large animals The combination of <phrase>deep brain stimulation</phrase> (DBS) and functional MRI (fMRI) is a powerful means of tracing brain circuitry and testing the modulatory effects of electrical stimulation on a neuronal network in vivo. The goal of this study was to trace DBS-induced global neuronal network activation in a large <phrase>animal model</phrase> by monitoring the blood oxygenation level-dependent (BOLD) response on fMRI. We conducted DBS in normal anesthetized pigs, targeting the subthalamic nucleus (STN) (n=7) and the entopeduncular nucleus (EN), the non-<phrase>primate</phrase> analog of the <phrase>primate</phrase> <phrase>globus</phrase> pallidus interna (n=4). Using a normalized functional activation map for group analysis and the application of general linear modeling across subjects, we found that both STN and EN/GPi DBS significantly increased BOLD activation in the ipsilateral <phrase>sensorimotor</phrase> network (<phrase>FDR</phrase><0.001). In addition, we found differential, target-specific, non-motor network effects. In each group the activated brain areas showed a distinctive correlation pattern forming a group of network connections. <phrase>Results suggest</phrase> that the scope of DBS extends beyond an <phrase>ablation</phrase>-like effect and that it may have modulatory effects not only on circuits that facilitate motor function but also on those involved in higher cognitive and emotional processing. Taken together, our results show that the swine model for DBS fMRI, which conforms to human implanted DBS electrode configurations and human <phrase>neuroanatomy</phrase>, may be a useful platform for translational studies investigating the global neuromodulatory effects of DBS.
Active <phrase>Stereovision</phrase> Using Invariant Visual Servoing — The objective of this paper is to propose an innovative visual servoing method in order to improve the 3D reconstruction of objects for quantitative measurements. The method uses a <phrase>stereo vision</phrase> system that allows to obtain various shots of an object, at regular intervals according to a predefined trajectory. In our case, the stereo rig is equipped with two different cameras, the first one is fixed while the other one is mounted on pan and tilt. So, the intrinsic parameters are not the same for the two cameras. To validate our approach, first experiments have been conducted by simulation and under laboratory conditions. I. INTRODUCTION The aim of our research is to develop, implement and test an original robotic method dedicated to compute a metric 3D reconstruction in order to describe and quantify biodiversity in <phrase>deep-sea</phrase> fragmented habitats [17]. The objective consists in applying techniques of <phrase>computer vision</phrase> to create tools adapted to the exploitation of underwater images. Today, some <phrase>deep-sea</phrase> vehicles are equipped with measuring devices and a manipulator arm. Vision systems are often used to complete information provided by acoustic sensors [12]. Taking advantage of possibilities allowed by <phrase>underwater vehicles</phrase>, our goal is to develop a methodology based on a vision system, to obtain quantitative measurements through a 3D reconstruction of underwater scenes. The images used for the reconstruction are acquired when the vehicle is deployed on the <phrase>sea floor</phrase> at a fixed and stable attitude. The images are subject to several constraints linked to the underwater environment. First of all, the observed scenes are unknown, and the objects to be reconstructed in these scenes are made up of random textures and shapes. We only know that the objects are rigid and have a vertical overall shape. Moreover, the <phrase>refraction</phrase>, the presence of particles, the absorption and the problems of lighting in an underwater environment considerably alter the image quality. Noisy images and an unknown model of the object have a knock-on effect on the 3D reconstruction accuracy. The idea is thus to define a trajectory adapted to the type of object in order to improve the reconstruction accuracy. The originality of the method lies in the use of an active stereo rig (i.e. with variable geometry) mounted on a 6 DOF manipulator arm effector, which equips the <phrase>underwater vehicle</phrase>. The stereo rig is equipped with two different underwater cameras, the first one is fixed while the …
Paper Special Section on Low-leakage, <phrase>Low-voltage</phrase>, <phrase>Low-power</phrase> and <phrase>High-speed</phrase> Technologies for System Lsis in <phrase>Deep-submicron Era</phrase> Generalized Stochastic <phrase>Collocation</phrase> Method for Variation-aware Capacitance Extraction of Interconnects Considering Arbitrary Random Probability SUMMARY For variation-aware capacitance extraction, stochastic col-location method (SCM) based on Homogeneous Chaos expansion has the exponential convergence rate for Gaussian <phrase>geometric variations</phrase>, and is considered as the optimal solution using a quadratic model to model the parasitic capacitances. However, when <phrase>geometric variations</phrase> are measured from the real <phrase>test chip</phrase>, they are not necessarily Gaussian, which will significantly compromise the exponential convergence property of SCM. In order to pursue the exponential convergence, in this paper, a generalized stochas-tic <phrase>collocation</phrase> method (gSCM) based on generalized Polynomial Chaos (gPC) expansion and generalized Sparse Grid quadrature is proposed for variation-aware capacitance extraction that further considers the arbitrary random probability of real <phrase>geometric variations</phrase>. Additionally, a <phrase>recycling</phrase> technique based on <phrase>Minimum Spanning Tree</phrase> (MST) structure is proposed to reduce the computation cost at each <phrase>collocation</phrase> point, for not only " <phrase>recycling</phrase> " the initial value, but also " <phrase>recycling</phrase> " the preconditioning matrix. The exponential convergence of the proposed gSCM is clearly shown in the numerical results for the <phrase>geometric variations</phrase> with arbitrary random probability.
Neighbor <phrase>Current Ratio</phrase> (<phrase>NCR</phrase>): A New Metric for IDDQ <phrase>Data Analysis</phrase> I DDQ test loses its effectiveness for <phrase>deep sub-micron</phrase> chips since it cannot distinguish between faulty and <phrase>fault-free</phrase> currents. The concept of <phrase>current ratios</phrase>, in which the ratio of maximum to minimum <phrase>I DDQ</phrase> is used to screen faulty chips, has been <phrase>previously proposed</phrase>. At the <phrase>wafer level</phrase> neighboring chips have similar <phrase>fault-free</phrase> properties and are correlated. In this paper, use of <phrase>spatial correlation</phrase> in combination with <phrase>current ratios</phrase> is investigated. By differentiating chips based on their nonconformance to local <phrase>I DDQ</phrase> variation, outliers are identified. The analysis of SEMATECH data is presented.
Robust TTS duration modelling using DNNS Accurate modelling and prediction of speech-sound durations is an important component in generating more natural synthetic speech. <phrase>Deep neural networks</phrase> (DNNs) offer a powerful modelling paradigm, and large, found corpora of natural and expressive speech are easy to acquire for training them. Unfortunately, found datasets are seldom subject to the <phrase>quality-control</phrase> that traditional synthesis methods expect. Common issues likely to affect duration modelling include transcription errors, reductions, filled pauses, and forced-alignment inaccuracies. To combat this, we propose to improve modelling and prediction of speech durations using methods from <phrase>robust statistics</phrase>, which are able to disregard ill-fitting points in the training material. We describe a robust fitting criterion based on the density power divergence (the-divergence) and a robust generation heuristic using mixture density networks (MDNs). <phrase>Perceptual</phrase> tests indicate that subjects prefer synthetic speech generated using robust models of duration over the baselines.
The ties that bind: <phrase>Social network</phrase> principles in <phrase>online communities</phrase> In a <phrase>Web 2.0</phrase> environment, the <phrase>online community</phrase> is fundamental to the <phrase>business model</phrase>, and participants in the <phrase>online community</phrase> are often motivated and rewarded by abstract concepts of <phrase>social capital</phrase>. How networks of relationships in <phrase>online communities</phrase> are structured has important implications for how <phrase>social capital</phrase> may be generated, which is critical to both attract and govern the necessary user base to sustain the site. We examine a popular website, <phrase>Slashdot</phrase>, which uses a system by which users can declare relationships with other users, and also has an embedded <phrase>reputation</phrase> system to rank users called '<phrase>Karma</phrase>'. We test the relationship between user's <phrase>Karma</phrase> level and the <phrase>social network</phrase> structure, measured by structural holes, to evaluate the brokerage and closure theories of <phrase>social capital</phrase> development. We find that <phrase>Slashdot</phrase> users develop <phrase>deep networks</phrase> at lower levels of participation indicating value from closure and that participation intensity helps increase the returns. We conclude with some comments on <phrase>mechanism design</phrase> which would exploit these findings to optimize the <phrase>social networks</phrase> and potentially increase the opportunities for <phrase>monetization</phrase>. Since the opening of the Internet to commercial organizations, there have been many experiments with <phrase>business models</phrase> to capitalize on the features and capabilities of the <phrase>World Wide Web</phrase>. Some have become mainstream , like online storefronts and auctions, and some seem to be fading, like subscription based <phrase>newspapers</phrase>. Several of the newest sets of online business opportunities involve " <phrase>Web 2.0</phrase> " concepts, especially those centered on <phrase>virtual communities</phrase>. We use the term <phrase>Web 2.0</phrase> to refer to websites which provide content that gets richer as more people use the website, harnessing " the power of user contribution, <phrase>collective intelligence</phrase>, and network effects " [24,25]. The most prominent of these sites include YouTube, <phrase>MySpace</phrase>, Wikipedia, <phrase>Facebook</phrase> and <phrase>Orkut</phrase>, which together encompass a sizable share of the world's most popular sites (#3, 6, 7, 8 and 11 respectively by ranking of global traffic (Alexa.com July 2008). Clearly, this form of online organization is creating a large impact in the business community. However, the business principles behind websites based around <phrase>social networks</phrase> are not well-understood. This paper seeks to illuminate a part of this area of study. Online organizations face unique challenges managing their organizational interests given their customer base is physically distant, psychologically unknown and literally faceless. This becomes more critical as <phrase>businesses</phrase> move from a model of simple transactional online functionality to one in …
Impedance Profile of a Commercial <phrase>Power Grid</phrase> and Test System An impedance profile of a commercial <phrase>power grid</phrase> and a tester <phrase>power distribution</phrase> system is developed in this paper. The profile is used to identify the measurable <phrase>frequency range</phrase> of the power supply <phrase>transient signals</phrase> generated by a chip. Several resistance-capacitance (RC) models of the <phrase>power grid</phrase> are analyzed to determine the impact of each capacitance type. The impedance profile of a C4-based production testing environment is then developed. The impedance profile of the combined <phrase>probe card</phrase> and the <phrase>power grid</phrase> RC models illustrates the range of frequencies that are measurable at the supply ports of the chip-under-test (CUT). The results suggest that it is possible to measure the important frequency components of a chip's <phrase>power supply</phrase> transients in a production test environment for use in fault detection and localization procedures. Conventional <phrase>testing methods</phrase> are challenged by changing circuit sensitivities and emerging defect mechanisms resulting from the use of new fabrication materials in very deep submicron processes [1]. For example, the change from a <phrase>subtractive</phrase> <phrase>aluminum</phrase> process to damascene Cu may lead to more particle-related blocked-etch <phrase>resistive opens</phrase>. Technology scaling also increases the probability of resis-tive vias caused by incomplete etch. The additional delays introduced by these types of resistive defects in combination with increased circuit sensitivity due to shorter clock cycles, reduced timing slack, crosstalk and <phrase>PWR</phrase>/GND bounce increase the likelihood of random defects causing delay fails. Similarly, <phrase>hardware-based</phrase> fault localization is challenged by increases in chip complexity as well as additional interconnection levels and the limitations on the spatial resolution of imaging technology. The increase in difficulty and cost of performing hardware physical <phrase>failure analysis</phrase> is likely to move it into a sampling/verification role. These trends continue to increase the importance of developing alternative software-based fault localization procedures. We believe that <phrase>power supply</phrase> <phrase>testing methods</phrase> are well aligned with these needs and others as described in the <phrase>International Technology Roadmap for Semiconductors</phrase>. In our previous work, a testing method is presented for <phrase>fault detection</phrase> that uses correlation analysis of multiple simultaneously measured <phrase>power supply</phrase> <phrase>transient signals</phrase> [2]. The transients at each of the supply ports of a chip-under-test (CUT) are cross-correlated to reduce the adverse effects of <phrase>process variations</phrase> on <phrase>fault detection</phrase> resolution. The multiple supply port measurements are analyzed for the regional signal anomalies introduced by defects. The <phrase>regression analysis</phrase> technique that we propose in [3] is able to detect anomalies in the ratios of the waveform …
Differential expression analysis of Digital <phrase>Gene Expression data</phrase>: RNA-tag filtering, comparison of t-type tests and their <phrase>genome-wide</phrase> co-expression based adjustments <phrase>Deep sequencing</phrase> techniques have shown a promising impact on biomedical studies. Based on a recently published two-sample Digital <phrase>Gene Expression</phrase> (DGE) <phrase>data set</phrase>, we compared three widely used t-type tests for the differential expression analysis. Both the 'soft' and 'hard' filtering strategies were considered. For the 'hard' filtering strategy, we also considered a <phrase>genome-wide</phrase> co-expression based adjustment for each t-type test. Our <phrase>results suggest</phrase> that excluding RNA-tags at an appropriate level of data variability can improve the control of <phrase>false positives</phrase>. Furthermore, the <phrase>genome-wide</phrase> co-expression based adjustments consistently provide comparably low levels of <phrase>false positive</phrase> control for different exclusion criteria.
Neonatal Seizure Detection using Blind Adaptive Fusion Seizure is the result of excessive electrical discharges of neurons, which usually develops synchronously and happens suddenly in the central <phrase>nervous</phrase> system. Clinically, it is difficult for physician to identify neonatal seizures visually, while EEG seizures can be recognized by the trained experts. Usually, in NICUs, EEG monitoring systems are used instead of the expensive on-site supervision. However, it is time-consuming to review an overnight recording, which motivates the researchers to develop automated seizure detection algorithms. Although, there are few detection algorithms existed in the literature, it is difficult to evaluate these <phrase>mathematical model</phrase> based algorithms since their performances vary significantly on different <phrase>data sets</phrase>. By extending our previous results on mul-tichannel information fusion, we propose a distributed detection system consisting of the existing detectors and a fusion center to detect the seizure activities in the newborn EEG. The advantage of our technique is that it does not require any <phrase>prior knowledge</phrase> of the hypotheses or the detector performances, which are often unknown in real applications. Therefore, this proposed technique has the potential to improve the performances of the existing neonatal seizure detectors. In this thesis, we first review two newborn EEG models, one of which is used to generate neonatal <phrase>EEG signals</phrase>. The synthetic data is used later for testing purpose. III We also review three existing algorithms and implement them to work as the local detectors. Then, we introduce the fusion algorithms applied in the fusion center for two different scenarios: large <phrase>sample size</phrase> and small <phrase>sample size</phrase>. We finally provide some numerical results to show the applicability, effectiveness, and the adaptability of the blind algorithms in the seizure detection problem. We also provide the testing <phrase>results obtained</phrase> using the synthetic to show the improvement of the detection system. IV Acknow ledgements I wish to express my deep and sincere gratitude to my superviosr, Dr. Aleksandar Jeremic, for his encouragement and guidance from the initial to the final stage of my graduated study and for his support and detailed comments on the completion of this thesis. I wish to express my thanks to my parents, for their continuous support on my study from the day I went aboard from home. Without their <phrase>financial support</phrase>, it is impossible to complete my undergraduate study at <phrase>McMaster University</phrase>. Without their support on taking care of my daughter, it is impossible to complete the research and write this thesis. I also would …
Objectively Structured <phrase>Performance Evaluation</phrase> – a Learning Tool The <phrase>teaching and learning</phrase> of medical students has always been a complicated process. Even the best of teachers at time may struggle in communicating knowledge and assessing its uptake. Simulated clinical and practical tools have recently gained popularity across the globe. They provide information regarding all the three aspects of assessment namely knowledge, skills and attitude. <phrase>OSCE</phrase> was first introduced by Harden in 1975. It encourages <phrase>deep learning</phrase> by testing higher cognitive functions. University of Health Sciences <phrase>Lahore</phrase> (UHS) modified the <phrase>OSCE</phrase> and introduced Objectively Structured <phrase>Performance Evaluation</phrase> (OSPE) in 2008. In <phrase>Pakistan</phrase>, it is a relatively new assessment method. The aim of OSPE is to make practical examinations <phrase>fair</phrase>, objective and standardized in line with Best Evidence Medical Education (BEME) and the local needs. Assessment techniques appear to have an impact on students' study strategies' and influence their performance, that is, " Assessment Drives Learning. " Therefore in order to cope with assessments, students adapt different <phrase>learning styles</phrase>, viz., Deep Approach (DA), Surface Apathetic Approach (SAA) and Strategic Approach (SA). OSPE assesses students' knowledge, different skills and attitude at the same time, therefore, leads the students to read the subject widely and to practice clinical skills extensively. It helps students not to just remember theory but also helps them to critically reflect on their learning course and its outcomes, therefore covering not only the cognitive but also effective domains. The aim of this paper is to review the impact of OSPE on students learning i.e. OSPE as a learning tool. Literature has been reviewed extensively using <phrase>Pubmed</phrase> <phrase>Medline</phrase>, Paknet, Mediscape and Goo-gle <phrase>Socratic</phrase>. Review of literature has shown OSPE is a valuable learning tool. INTRODUCTION The <phrase>teaching and learning</phrase> of medical students has always been a complicated process, At times even the best of teachers may struggle in communicating knowledge and assessing its uptake. Assessment of gained knowledge is probably more difficult than delivering it. Assessment of clinical skills is far more important and complex as it directly link with patients care. The aim of this paper is to review the impact of OSPE on students learning i.e. OSPE as a learning tool. Literature has been reviewed extensively using <phrase>Pubmed</phrase> <phrase>Medline</phrase>, Paknet, Mediscape and Google <phrase>Socratic</phrase>.
A low-overhead self-healing embedded system for ensuring high yield and <phrase>long-term</phrase> sustainability of 60GHz 4Gb/s radio-on-a-chip The available <phrase>ISM band</phrase> from 57-65GHz has become attractive for <phrase>high-speed</phrase> wireless applications including mass data transfer, streaming <phrase>high-definition video</phrase> and even biomedical applications. While silicon based data transceivers at <phrase>mm-wave</phrase> frequencies have become increasingly mature in recent years [1,2,3], the primary focus of the circuit community remains on the design of <phrase>mm-wave</phrase> front-ends to achieve higher data rates through <phrase>higher-order</phrase> modulation and <phrase>beamforming</phrase> techniques. However, the sustainability of such <phrase>mm-wave</phrase> systems when integrated in a SoC has not been addressed in the context of die performance yield and device aging. This problem is especially challenging for the implementation of <phrase>mm-wave</phrase> SoC's in deep sub-micron technology due to its process & <phrase>operating temperature</phrase> variations and limited ft / fmax with respect to the operation frequency. To address the issue of sustainability in integrated <phrase>mm-wave</phrase> transceivers, this paper presents a low overhead self-healing system that can be embedded in an <phrase>mm-wave</phrase> transceiver to continually monitor and optimize its performance throughout the lifetime of the radio. The proposed system monitors key trans-ceiver parameters including <phrase>transmitter</phrase> image rejection, <phrase>transmitter</phrase> P1dB, OIM3, and the receiver <phrase>noise figure</phrase> and OIM3. The <phrase>mm-wave</phrase> <phrase>front-end</phrase> is designed to be extremely tunable allowing for adjustments to be made automatically as the device ages. Figure 18.5.1 shows the architecture of the self-healing <phrase>mm-wave</phrase> transceiver that implements a dual-controller with cautious tracking for the <phrase>robust control</phrase> of multiple circuit knobs within the mm-transceivers to meet a target performance specification <phrase>shown in Fig</phrase>. 18.5.6. Using the numerically controlled oscillators (<phrase>NCOs</phrase>), the probe generator in the self-healing controller (SHC) produces test tones with programmable frequencies and amplitudes to probe the <phrase>mm-wave</phrase> transceiver. With known test tones, transceiver impairments, such as image, noise, and <phrase>intermodulation</phrase> distortion, can be measured by sensors embedded throughout the transceiver. These sensors measure envelop variations, power level, and temperature. The measured parameters are digitized by the 10b, 5MSps instrument ADC and processed by the parameter estimator (PE), which contains a 128-point FFT processor used for spectral analysis and a statistical processor used to produce reliability measures employed by the dual controller for cautious tracking. To meet aggressive performance metrics such as better than-40dBc of image level and OIM3 in the presence of background circuit noise, the controller through cautious tracking can dynamically adjust the rate of control of various tuning knobs in the transceiver according to the reliability measures from the PE. The self-healing <phrase>transmitter</phrase> is shown in …
Current Testing Procedure for <phrase>Deep Submicron</phrase> Devices This paper presents a test technique that employs two different <phrase>supply voltages</phrase> for the same Iddq pattern. The results of the two measurements are subtracted in order to eliminate the inherent subthreshold leakage. Summary of the experiment carried out on " System on a Chip " (SOC) device build in 0.35µ technology is also shown. These experiments proved that the method is effective in detecting failures not detectable with the single limit Iddq.
Designed -in-diagnostics: a New Optical Method 2. a New Optical Sampling Scheme An in-circuit <phrase>diagnostic test</phrase> structure triggered by a light pulse captures logic states on-chip with <phrase>picosecond</phrase> timing accuracy, and the results read out via a <phrase>scan chain</phrase> thus providing precise logic transition time information from <phrase>deep inside</phrase> the chip, greatly aiding <phrase>failure analysis</phrase>. The method could also make time measurement of switching events inside an IC when it is mounted in a <phrase>printed circuit board</phrase> environment, enabling correlation of these events to board level logic timing, i.e. system validation.. 1. Introduction Today only two optical techniques for the internal <phrase>timing analysis</phrase> of today's most advanced IC designs are viable. Laser voltage probe instruments provide waveforms from any transistor on the device under test (DUT) but require the locking of the DUT <phrase>test pattern</phrase> to a mode locked laser providing the probing pulses [1]. Dynamic emission detection instruments are passive, detecting the weak <phrase>photon</phrase> emission produced by device switching action. Dynamic emission is the simpler method to use but the timing data obtained by it shows only the falling edge positions of N-channel devices clearly. The rising edges of N-channel devices and any P-channel device switching are poorly detected [2]. With both instruments the <phrase>signal-to-noise ratio</phrase> is poor, so that signal averaging is required to give accurate waveforms. <phrase>Signal-to-noise</phrase> ratios will fall further as device geometry shrinks and as core voltages decrease, so that acquisition times will become extremely long. Averaging times of tens of minutes to hours will be required for devices running <phrase>test patterns</phrase> of 100's of microseconds. Another serious problem for these instruments is that optical separation of nearby transistors is limited even with the best lenses to around 0.25 µm. Transistor spacing of 0.25 µm is expected to be reached at the 45 nm node so that waveforms from separate transistors will be confusingly merged together. Device testing is moving more towards structural on-chip testing. The proposed optically-driven logic-sampling scheme would provide rapid on-chip waveform acquisition capabilities for design debug and will be viable for present and future IC technologies. A test chip has been produced to demonstrate the capabilities of the method. Tests of the basic structure have begun and the results are presented here for the first time. The proposed new optical method, while requiring on-chip circuits, would allow critical logic timing waveforms to be obtained quickly from internal nodes. The on-chip circuits provide means for generating a logic sample pulse, for capturing the logic …
Research on Scene Infrared Image Simulation Vega has been widely used in the <phrase>Virtual Reality</phrase> field. Its infrared (IR) module can implement <phrase>IR simulation</phrase>, but Vega IR imaging simulation's general approach does not apply to the complex scene. This article deeps into the scene's <phrase>IR simulation</phrase> method based on Vega. We design and realize a real time scene IR image simulation system in this article. We quantitatively define the scene as a simple and complex scene according to the scene range and whether it includes <phrase>Digital Elevation Model</phrase> (DEM) / Digital Surface Model (DSM) data. For the simple scene, we directly process IR image simulation according to the Vega general <phrase>IR simulation</phrase> process. While for the complex scene, we propose an IR image <phrase>simulation method</phrase> based on <phrase>image classification</phrase> and automatic texture material mapping technique. At the aspect of <phrase>image classification</phrase>, we develop a coarse to fine <phrase>K-means clustering</phrase> method based on the consistency of image color for color <phrase>image classification</phrase> and an additional <phrase>Support Vector Machine</phrase> (SVM) classification method based on <phrase>texture features</phrase> for gray level <phrase>image classification</phrase>. The method was tested on different scene's <phrase>IR simulation</phrase>. Experimental results show that the proposed approach can achieve better applicability and greater efficiency than the popular Vega <phrase>IR simulation</phrase> method.
A Simulation Game for Teaching Secure Data Communications Protocols With the widespread commercial use of the Internet, secure data communications over the Internet has become an important aspect of business operations. Thus, it is an important study for <phrase>information technology</phrase> and management students. The Security Protocol Game is an interactive group activity for exploring secure <phrase>data communication</phrase> protocols. Using pen and paper, envelopes and game tokens, students simulate <phrase>security protocols</phrase> and possible attacks against them. The game provides simple and intuitive representations for cryptographic methods, including both <phrase>public key</phrase> and secret key techniques. Using these representations, students can simulate Internet application protocols such as Pretty Good Privacy (used to secure <phrase>email</phrase>) and <phrase>Transport Layer Security</phrase> (used for secure web transactions). They can explore well-known protocols for <phrase>authentication</phrase>, <phrase>key exchange</phrase> and <phrase>blind signatures</phrase>. Students can also develop and test their own protocols using <phrase>public key</phrase> certificates, encrypted key transmission, tunnelling and other well-known techniques. Through this learning activity, students gain a deep understanding of how <phrase>security protocols</phrase> operate and are designed. The game has been used in <phrase>tertiary</phrase> units of study for managers and <phrase>information technology</phrase> students.
Software <phrase>test program</phrase>: a software residency experience The Software <phrase>Test Program</phrase> (STP) is a cooperation between <phrase>Motorola</phrase> and the Center for <phrase>Informatics</phrase> of the <phrase>Federal University of Pernambuco</phrase>. It has been conceived with inspiration on the Medical Residency, adjusted to the <phrase>software development</phrase> practice. A Software Residency includes the formal teaching of the relevant concepts and deep practice, with specialization on some specific subject; here the focus is on <phrase>software testing</phrase>. The STP has been of great benefit to all <phrase>parties</phrase> involved.
Testing the reliability of genetic methods of species identification via simulation. Although genetic methods of species identification, especially <phrase>DNA barcoding</phrase>, are strongly debated, tests of these methods have been restricted to a few empirical cases for pragmatic reasons. Here we use simulation to test the performance of methods based on sequence comparison (BLAST and <phrase>genetic distance</phrase>) and tree topology over a wide range of evolutionary scenarios. Sequences were simulated on a range of gene trees spanning almost three <phrase>orders of magnitude</phrase> in tree depth and in coalescent depth; that is, deep or shallow trees with deep or shallow coalescences. When the query's <phrase>conspecific</phrase> sequences were included in the reference alignment, the rate of positive identification was related to the degree to which different species were genetically differentiated. The BLAST, distance, and liberal <phrase>tree-based</phrase> methods returned higher rates of correct identification than did the strict <phrase>tree-based</phrase> requirement that the query was within, but not sister to, a single-species <phrase>clade</phrase>. Under this more conservative approach, ambiguous outcomes occurred in inverse proportion to the number of reference sequences per species. When the query's <phrase>conspecific</phrase> sequences were not in the reference alignment, only the strict <phrase>tree-based</phrase> approach was relatively immune to making <phrase>false-positive</phrase> identifications. Thresholds affected the rates at which <phrase>false-positive</phrase> identifications were made when the query's species was unrepresented in the reference alignment but did not otherwise influence outcomes. A conservative approach using the strict <phrase>tree-based</phrase> method should be used initially in <phrase>large-scale</phrase> identification systems, with effort made to maximize sequence sampling within species. Once the <phrase>genetic variation</phrase> within a taxonomic group is well characterized and the taxonomy resolved, then the choice of method used should be dictated by considerations of computational efficiency. The requirement for extensive genetic sampling may render these techniques inappropriate in some circumstances.
Emotion technology, wearables, and surprises Could we help people have healthier lives and better experiences if computers could measure and help communicate our emotion? <phrase>Years ago</phrase>, my students at MIT and I began to design, build, and test both wearable and other sensors for recognizing emotion. We designed studies, gathered data, and developed <phrase>signal processing</phrase> and <phrase>machine learning</phrase> techniques to see what could be reliably extracted. In this talk I will highlight several of the most surprising findings during this <phrase>adventure</phrase>. These include new insights about the "true smile of happiness," discovering that regular cameras (and your <phrase>smartphone</phrase>, even in your <phrase>handbag</phrase>) can compute some of your biosignals, finding electrical signals on the wrist that give insight into <phrase>deep brain</phrase> activity, and learning surprising implications of wearable sensing for autism, <phrase>anxiety</phrase>, depression, sleep-<phrase>memory consolidation</phrase>, epilepsy, and more.
Active Traffic Monitoring for <phrase>Heterogeneous Environments</phrase> Traffic management of IP networks comprises increasing challenges due to the occurrence of sudden and deep traffic variations that can be mainly attributed to the combined effects of several factors, like the great diversity of supported applications and services, different user's behaviors and different mechanisms of traffic generation and control. In this context, active traffic monitoring is particularly important as it enables characterizing essential aspects in network operation, like for example, quality of service as measured in terms of packet delays and losses. The main goal of this work is to carry out active measurements in a real operational network consisting in a heterogeneous environment that includes both wired and wireless <phrase>LANs</phrase>. In order to perform this task, a measurement methodology, and its corresponding measurement platform, will be proposed. The measurement methodology is based on the One-Way Active Measurement Protocol (OWAMP), a recent proposal from the <phrase>Internet2</phrase> and <phrase>IETF</phrase> IPPM groups for active measurements of delays and losses in a single direction. The measurement platform was implemented, tested and conveniently validated. This paper begins by a brief presentation of the measurements that we intend to perform, then it describes the OWAMP protocol and the developed measurement system, including its implementation, test and validation through its application to different network scenarios.
<phrase>Pattern Mining</phrase> with <phrase>Natural Language Processing</phrase>: An Exploratory Approach <phrase>Pattern mining</phrase> derives from the need of discovering hidden knowledge in very large amounts of data, regardless of the form in which it is presented. When it comes to <phrase>Natural Language Processing</phrase> (NLP), it arose along the humans' necessity of being understood by computers. In this paper we present an exploratory approach that aims at bringing together the best of both worlds. Our goal is to discover patterns in linguistically processed texts, through the usage of NLP state-of-the-art tools and traditional <phrase>pattern mining</phrase> algorithms. Articles from a <phrase>Portuguese</phrase> <phrase>newspaper</phrase> are the input of a series of tests described in this paper. First, they are processed by an NLP chain, which performs a <phrase>deep linguistic</phrase> analysis of text; afterwards, <phrase>pattern mining</phrase> algorithms Apriori and GenPrefixSpan are used. <phrase>Results showed</phrase> the applicability of <phrase>pattern mining</phrase> techniques in textual <phrase>structured data</phrase>, and also provided several evidences about the structure of the language.
A lateral contribution learning algorithm for multi MLP architecture gradient is large and , for close to is close to. On the other hand and if lc is sufficiently sharp this has no far range influence, this " cooperation principle " is only local. And the snowball effect spreads this influence from one NN to its neighbors. Of course, we can use existing optimization of the backpropagation <phrase>learning algorithm</phrase> [13] to increase the quickness and find the edge of a deep well of l in one. When this edge is found we can use an algorithm, such as BFGS [9][3], to go rapidly to this <phrase>local minima</phrase> and then go back to the previous optimized <phrase>LCL</phrase> algorithm to search an other edge of a deep well of l. We can also improve it by an optimization of lc(.) kernel, by defining one kernel per connection. The heuristic of this idea is that each feature extracted by one weight in could be the same for an other. So, each component of this new kernel (matrix indexed by ij) can be rewriten as: where is the lateral contribution rate for the connection ij with regard to and. This rate can be on-line adapted with regard to the comparison of and the direction for each pair at each step, or <phrase>off-line</phrase> adapted with regard to the <phrase>symmetries</phrase> of the function F(.). This <phrase>off-line</phrase> adaptation can be included in the term of distortion , allowing the VQ algorithm to move two classes closer than if the term of distortion should only take the input space topology into account. The main improvements of <phrase>LCL</phrase> algorithm are not only the quickness of convergence of learning processing but also a new insight of the synaptic weight, showing that the behavior of a neural network in context is directly linked to the behavior of every NNs in close to. This link is due to the strong constraint that must be continuous. It is now natural to try to merge these NNs into one with synaptic weights estimated through the behavior of context. Accordingly, we have developed an architecture for weight estimation named OWE (Orthogonal Weight Estimator) and tested it on control problems[5]. The ongoing studies correspond to testing the <phrase>LCL</phrase> algorithm for more and more complex input space, and defining generalized models to embed LCLA and the on-line <phrase>Vector Quantization</phrase> [6]. W t θ () θ θ 0 W t θ 0 () θ 0 NN θ …
Efficient Interconnect Test Patterns for Crosstalk and Static Faults In this paper, we present efficient test patterns for the crosstalk–induced faults on System-on-a-Chip and board level interconnects considering actual effective aggressors to minimize the pattern size. All static faults also can be detected. The proposed method achieved the <phrase>significant reduction</phrase> of the number of <phrase>test patterns</phrase> than prior works, while preserving 100% <phrase>fault coverage</phrase>. We are in the process of extending the proposed technique to built-in-self test logics. As <phrase>deep submicron</phrase> techniques are increasingly developed, system-on-a-chips (SoCs) include more reusable cores such as processors, memories, and peripheral interfaces. Today's boards become tomorrow's IC's, whereby today's IC's become tomorrow's cores. It becomes highly important to capture critical <phrase>timing defects</phrase> as well as conventional static faults on the interconnect lines among SoCs on a board and cores on an SoC. The IEEE 1149.1 boundary scan and IEEE 1500 are standards for testing boards and SoCs, and the interconnect testing is performed by serially scanning <phrase>test patterns</phrase> through the boundary wrapper cells following the standards [1], [2]. When boundary scan design techniques are adopted, interconnect <phrase>test generation</phrase> and the application of <phrase>test patterns</phrase> are greatly simplified. Various <phrase>test generation</phrase> algorithms have been developed to address the static interconnect faults [3]-[6]. <phrase>Crosstalk faults</phrase> generated through <phrase>high-speed</phrase> signal transmissions become significant challenges for SoC interconnect testing [7]-[13]. A <phrase>fast and accurate</phrase> technique to estimate the crosstalk <phrase>fault coverage</phrase> of any general <phrase>test set</phrase> was developed [7]. Although linear feedback <phrase>shift registers</phrase> (LFSR) were extensively adopted to generate a few random patterns, the fault coverage was not satisfied. Several deterministic and pseudorandom <phrase>test pattern</phrase> generators for embedding necessary patterns of <phrase>crosstalk faults</phrase> have been proposed [9], [10], and 4n+1 patterns were presented against 6n patterns considering ineffective aggressors [13]. All of the test patterns for interconnect faults introduced in the literatures target a single victim line. However, if an effective distance among interconnects can be extracted from the physical layout information, more than one victim can be targeted by a single <phrase>test pattern</phrase>. In this paper, we introduce highly compact interconnect test patterns for crosstalk and static faults, which is independent of the number of total interconnects. This paper is organized as follows. Section Ⅱ describes definitions and <phrase>fault models</phrase>, related works are introduced in Section Ⅲ, and the proposed method is investigated in Section Ⅳ, which is followed by concluding remarks. Ⅱ. Definitions and <phrase>fault models</phrase> <phrase>Crosstalk faults</phrase> can be classified into positive <phrase>glitch</phrase>, …
Testing a Cmos <phrase>Operational Amplifier</phrase> Circuit Using a Combination of Oscillation and <phrase>I Ddq</phrase> <phrase>Test Methods</phrase> and my brother Kiran, for their constant prayers and encouragement throughout my life. I am very grateful to my advisor Dr. A. Srivastava for his guidance, patience and understanding throughout this work. His suggestions, discussions and constant encouragement have helped me to get a deep insight in the field of <phrase>VLSI design</phrase>. Departments, for supporting me financially during my stay at LSU. I am very thankful to my friends Anand, Satish and Syam for their extensive help throughout my graduate studies at LSU. I take this opportunity to thank my friends Maruthi, Sudheer, Anoop and <phrase>Siva</phrase> for their help and encouragement at times I needed them. I would also like to thank all my friends here who made my stay at LSU an enjoyable and a memorable one. Last of all I thank <phrase>GOD</phrase> for keeping me in good health and spirits throughout my stay at LSU.
Ieee <phrase>Intelligent Systems</phrase> <phrase>Knowledge Representation and Reasoning</phrase> Ai's Greatest Trends and Controversies Except perhaps for the AI naysayers, AI practitioners are creators—of software artifacts, their underlying algorithms, and their underlying theories. We begin our feature with Herbert Simon, one of our three contributors (together with John McCarthy and Oliver Selfridge) who are our links to the landmark <phrase>Dartmouth</phrase> conference in 1956, where modern AI is often said to have begun. Simon paints a broad picture of AI as a discipline constantly pursuing computational creations that challenge the uniqueness of biologically grounded intelligence. AI has been thought controversial because it challenged the uniqueness of human thought, as <phrase>Darwin</phrase> challenged the uniqueness of human origins. The boundaries of AI continue to expand rapidly, settling the controversy for those who know the evidence. AI first demonstrated that important intellectual tasks could be accomplished by selective heuristic search, often in a thoroughly human way. GPS is one product of that line of research. Then AI explored the role of large bodies of knowledge in expert thinking. Dendral was an early important success, as was the extensive research on human chess expertise, modeled with such programs as Chrest (not <phrase>Deep Blue</phrase>, which is only partly <phrase>humanoid</phrase>). A third successful line has been the research on learning— for, example, Siklossy's ZBIE program, which learned <phrase>natural language</phrase> by comparing sentences with pictures. Finally, there has been the great recent advance in robotics, based on progress in simulating sensory and motor functions. The basic strategy of AI has always been to seek out progressively more complex human tasks and show how computers can do them, in <phrase>humanoid</phrase> ways or by brute force. With a half-century of steady progress, we have assembled a <phrase>solid body</phrase> of tested theory on the processes of human thinking and the ways to simulate and supplement them. In what media do AI practitioners create? The answer to this question is a depiction of AI itself, so it is not too surprising that most of our contributions address this question. <phrase>Wolfgang</phrase> Bibel, a proponent of the formalist agenda in AI, argues for the need for sophisticated logic formalisms and inferential methods for AI. Alan Bundy adds to these arguments, discussing further the advances achieved by those taking the <phrase>formal-logic</phrase> approach to AI, especially in light of the critiques raised by those on the other side of the AI fence. Among the controversies in AI, none is as persisting as the one about logic's role in AI. It …
A Bayesian Approach to Relevance in Game Playing 1. The point of <phrase>game tree</phrase> search is to insulate oneself from errors in the evaluation function. The standard approach is to grow a full width tree as deep as time allows, and then value the tree as if the leaf evaluations were exact. This has been eeective in many games because of the computational eeciency of the <phrase>alpha-beta</phrase> algorithm. Our approach is to form a Bayesian model of our uncertainty. We adopt an evaluation function that returns a <phrase>probability distribution</phrase> estimating the probability of various errors in valuing each position. These estimates are obtained by training from data. We thus use <phrase>additional information</phrase> at each leaf not available to the standard approach. We utilize this information in three ways: to evaluate which move is best after we are done expanding, to allocate additional thinking time to moves where additional time is most relevant to game outcome, and, perhaps most importantly, to expand the tree along the most relevant lines. Our measure of the relevance of expanding a given leaf provably approximates a measure of the impact of expanding the leaf on expected payoo, including the impact of the outcome of the leaf expansion on later expansion decisions. Our algorithms run (under reasonable assumptions) in time linear in the size of the nal tree and hence except for a small constant factor, are as time eecient as <phrase>alpha-beta</phrase>. In a given amount of time our algorithm can thus in principle grow a tree several times as deep as <phrase>alpha-beta</phrase> along the relevant lines. We have tested our approach on a variety of games, including Othello, Kalah, <phrase>Warri</phrase>, and others. Our probability independence approximations are seen to be signiicantly violated, but nonetheless our tree valuation scheme was found to play signiicantly better than <phrase>minimax</phrase> or the Probability <phrase>Product rule</phrase> when both competitors search the same tree. Our full <phrase>search algorithm</phrase> was found to outplay a highly ranked, directly comparable <phrase>alpha-beta</phrase> Othello program even when the <phrase>alpha-beta</phrase> program was given sizeable time odds, and also performed well against the three top Othello programs on the Internet Othello Server.
IDDX-based <phrase>test methods</phrase>: A survey <phrase>Supply current</phrase> measurement-based test is a valuable defect-based test method for semiconductor chips. Both static <phrase>leakage current</phrase> (I<sub>DDQ</sub>) and transient current (I<sub>DDT</sub>) based tests have the capability of detecting unique defects that improve the <phrase>fault detection</phrase> capacity of a test suite. Collectively these <phrase>test methods</phrase> are known as I<sub>DDX</sub> tests. However, due to advances in the <phrase>semiconductor manufacturing</phrase> process, the future of these <phrase>test methods</phrase> is uncertain. This paper presents a survey of the research reported in the literature to extend the use of I<sub>DDX</sub> tests to <phrase>deep sub-micron</phrase> (DSM) technologies.
A New Method of Implementing Hierarchical OPC For emerging deep-subwavelength lithography technologies (90 nm and following) the data volume and the complexity of Optical Proximity Correction (OPC) have increased dramatically. This has added to the total cost of IC manufacturing and become an increasingly critical issue in optical lithography. In this paper, we present a new method of implementing hierarchical OPC to explore its merits in runtime saving. The interactions and propagating corrections between neighboring cells during OPC have been discussed and appropriate solutions have been proposed. Segment-Moving Map (SMM) and dynamic correction are brought forward for the first time to identify the interacting regions in hierarchical OPC and automatically adjust the corrections in these regions. Furthermore, total Edge Placement Error (<phrase>EPE</phrase>) is calculated in controlled experiments to test the accuracy of this method. Results have shown that approximately 5X speedup has been achieved with similar accuracy when compared with the conventional OPC method.
Lithology identification of aquifers from geophysical well logs and <phrase>fuzzy logic</phrase> analysis: Shui-Lin Area, <phrase>Taiwan</phrase> The purpose of this study is to construct a <phrase>fuzzy lithology</phrase> system from well logs to identify formation lithology of a groundwater <phrase>aquifer</phrase> system in order to better apply conventional well logging interpretation in hydro-geologic studies because well log responses of aquifers are sometimes different from those of conventional oil and gas <phrase>reservoirs</phrase>. The input variables for this system are the <phrase>gamma-ray</phrase> log reading, the separation between the spherically focused resistivity and the deep very-enhanced resistivity curves, and the <phrase>borehole</phrase> compensated sonic log reading. The output variable is groundwater formation lithology. All linguistic variables are based on five linguistic terms with a <phrase>trapezoidal</phrase> membership function. In this study, 50 <phrase>data sets</phrase> are clustered into 40 training sets and 10 testing sets for constructing the <phrase>fuzzy lithology</phrase> system and validating the ability of system prediction, respectively. The <phrase>rule-based</phrase> database containing 12 <phrase>fuzzy lithology</phrase> rules is developed from the training data sets, and the rule strength is weighted. A Madani inference system and the bisector of area defuzzification method are used for <phrase>fuzzy inference</phrase> and defuzzification. The success of training performance and the prediction ability were both 90%, with the calculated correlation of <phrase>training and testing</phrase> equal to 0.925 and 0.928, respectively. Well logs and core data from a <phrase>clastic</phrase> <phrase>aquifer</phrase> (depths 100–198 m) in the Shui-Lin area of west-central <phrase>Taiwan</phrase> are used for testing the system's construction. Comparison of results from core analysis, well logging and the <phrase>fuzzy lithology</phrase> system indicates that even though the well logging method can easily define a permeable <phrase>sand</phrase> formation, distinguishing between silts and sands and determining <phrase>grain size</phrase> variation in sands is more subjective. These shortcomings can be improved by a <phrase>fuzzy lithology</phrase> system that is able to yield more objective decisions than some conventional methods of log interpretation.
Component Repair Using Laser Direct Metal Deposition <phrase>Recent studies</phrase> have indicated that laser direct metal deposition can be used for repairing deep or internal cracks and defects in metallic components. In order to implement the method, it is necessary to machine a groove or slot to the depth of the defect and refill it. This work investigates advantages and potential problems with the technique and compares the results from using two different slot geometries: one rectangular and one triangular in <phrase>cross-section</phrase>. H13 hot-work <phrase>tool steel</phrase> components are used and H13 powder is deposited using a 1.5 kW diode laser and lateral <phrase>nozzle</phrase>. Different combinations of deposition parameters are tested and each sample is analysed in terms of mass deposition rate, deposition <phrase>microstructure</phrase>, evidence of <phrase>porosity</phrase>, size of the heat-affected zone, and microhardness. Results are evaluated using <phrase>statistical techniques</phrase> and the important parameters that control each variable are identified. The work provides evidence that the method can produce <phrase>high-quality</phrase> repairs, but <phrase>poros</phrase>-ity at the boundaries between the original part and the added material is a problem.
Yield Enhancement Methodology for CMOS Standard Cells In order to maximize the yield of random logic in today's advanced <phrase>Deep Sub-Micron CMOS</phrase> technologies we have developed a complete yield enhancement methodology for Cmos standard cells. This methodology based on a test vehicle approach covers design, industrial test, <phrase>data collection</phrase> and volume analysis, design debug, failure location and analysis. It has proven to be successful on three consecutive <phrase>technology nodes</phrase> down to 65nm. This paper will explain the methodology and demonstrate the results and benefits of this work through illustrated examples.
<phrase>Memory Bist</phrase> With the advent of deep-submicron VLSI technology, core-based system-on-chip (SOC) design is attracting an increasing attention. On an SOC, popular reusable cores include memories (such as ROM, SRAM, <phrase>DRAM</phrase> and <phrase>flash memory</phrase>), processors (such as CPU, DSP and microcontroller), <phrase>input/output</phrase> circuits, etc. <phrase>Memory cores</phrase> are obviously among the most universal ones-almost all system chips contain some type of <phrase>embedded memory</phrase> [5]. However, to provide a low cost-cost test solution for the on-chip memory cores is not a trivial task. This paper presents a study on <phrase>memory BIST</phrase>, algorithms of different <phrase>test patterns</phrase>, survey of <phrase>memory BIST</phrase> implementations, and discussion of some novel design issues. This paper shall serve as a <phrase>knowledge base</phrase> for future design in <phrase>memory BIST</phrase>.
Methodology on Extracting Compact Layout Rules for Latchup Prevention in <phrase>Deep-submicron</phrase> Bulk <phrase>Cmos Technology</phrase> —An experimental methodology to find area-efficient compact layout rules to prevent latchup in bulk complimentary metal–oxide–semiconductor (CMOS) <phrase>integrated circuits</phrase> (ICs) is proposed. The layout rules are extracted from the <phrase>test patterns</phrase> with different layout spacings or distances. A new latchup prevention design by adding the additional internal double guard rings between <phrase>input/output</phrase> cells and internal circuits is first reported in the literature, and its effectiveness has been successfully proven in three different bulk <phrase>CMOS processes</phrase>. Through detailed experimental verification including temperature effect, the proposed methodology to extract compact layout rules has been established to save <phrase>silicon area</phrase> of <phrase>CMOS ICs</phrase> but still to have high enough latchup immunity. This proposed methodology has been successfully verified in a 0.5-m nonsilicided, a 0.35-m silicided, and a 0.25-m silicided shallow-trench-isolation bulk <phrase>CMOS processes</phrase>.
Implant Survival, Adverse Events, and <phrase>Bone Remodeling</phrase> of Osseointegrated <phrase>Percutaneous</phrase> Implants for <phrase>Transhumeral Amputees</phrase> BACKGROUND Osseointegrated <phrase>percutaneous</phrase> implants provide direct <phrase>anchorage</phrase> of the limb <phrase>prosthesis</phrase> to the residual limb. These implants have been used for the rehabilitation of <phrase>transhumeral amputees</phrase> in <phrase>Sweden</phrase> since 1995 using a two-stage surgical approach with a 6-month interval between the stages, but results on implant survival, adverse events, and radiologic signs of <phrase>osseointegration</phrase> and adaptive <phrase>bone remodeling</phrase> in <phrase>transhumeral amputees</phrase> treated with this method are still lacking.   QUESTIONS/PURPOSES This study reports on 2- and 5-year implant survival, adverse events, and radiologic signs of <phrase>osseointegration</phrase> and <phrase>bone remodeling</phrase> in <phrase>transhumeral amputees</phrase> treated with osseointegrated prostheses.   METHODS Between 1995 and 2010, we performed 18 primary osseointegrated <phrase>percutaneous</phrase> implants and two implant revisions in 18 <phrase>transhumeral amputees</phrase>; of those, 16 patients were available for followup at a minimum of 2 years (median, 8 years; range, 2-19 years). These include all <phrase>transhumeral amputees</phrase> who have received osseointegrated prostheses and represented approximately 20% of the all <phrase>transhumeral amputees</phrase> we evaluated for potential <phrase>osseointegration</phrase> during that time; general indications for this approach included transhumeral <phrase>amputation</phrase> resulting from trauma or tumor, inability to wear or severe problems wearing a conventional socket <phrase>prosthesis</phrase>, eg, very short residual limb, and compliant patients. Medical charts and plain radiographs were retrospectively evaluated.   RESULTS The 2- and 5-year implant survival rates were 83% and 80%, respectively. Two primary and one revised implant failed and were removed because of early loosening. A fourth implant was partially removed because of ipsilateral shoulder <phrase>osteoarthritis</phrase> and subsequent <phrase>arthrodesis</phrase>. The most common <phrase>adverse event</phrase> was superficial <phrase>infection</phrase> of the skin penetration site (15 infections in five patients) followed by skin reactions of the skin penetration site (eight), incomplete fracture at the first surgery (eight), defective bony <phrase>canal</phrase> at the second surgery (three), avascular skin flap <phrase>necrosis</phrase> (three), and one deep implant <phrase>infection</phrase>. The most common radiologic finding was proximal trabecular buttressing (10 of 20 implants) followed by endosteal <phrase>bone resorption</phrase> and cancellization (seven of 20), cortical thinning (five of 20), and distal <phrase>bone resorption</phrase> (three of 20).   CONCLUSIONS The implant system presented a survivorship of 83% at 5 years and a 38% 5-year incidence of infectious complications related to the skin penetration site that were easily managed with nonoperative treatment, which make it a potentially attractive alternative to conventional socket arm prostheses. Osseointegrated arm prostheses have so far only been used in transhumeral amputations resulting from either trauma or tumor. Their use has not been tested and is therefore not recommended in transhumeral amputations resulting from <phrase>vascular disease</phrase>. This method could theoretically be superior to socket prostheses, especially in <phrase>transhumeral amputees</phrase> with very short residual <phrase>humerus</phrase> in which the suspension of a conventional <phrase>prosthesis</phrase> is difficult. Comparative studies are needed to support its potential superiority. Moreover, the radiological findings in this study need to be followed over time because some of them are of uncertain <phrase>long-term</phrase> clinical relevance.
Optimizing and Contrasting Recurrent <phrase>Neural Network</phrase> Architectures <phrase>Recurrent Neural Networks</phrase> (RNNs) have long been recognized for their potential to model complex <phrase>time series</phrase>. However, it remains to be determined what <phrase>optimization techniques</phrase> and recurrent architectures can be used to best realize this potential. The experiments presented take a deep look into <phrase>Hessian</phrase> free optimization, a powerful second order optimization method that has shown <phrase>promising results</phrase>, but still does not enjoy widespread use. This algorithm was used to train to a number of RNN architectures including standard RNNs, long <phrase>short-term memory</phrase>, multiplicative RNNs, and stacked RNNs on the task of character prediction. The insights from these experiments led to the creation of a new multiplicative LSTM <phrase>hybrid architecture</phrase> that outperformed both LSTM and multiplicative RNNs. When tested on a larger scale, multiplicative LSTM achieved character level modelling results competitive with the state of the art for RNNs using very different methodology.
<phrase>Stress Migration</phrase> and Electromigration Improvement for Copper <phrase>Dual Damascene</phrase> Interconnection <phrase>Stress migration</phrase> ͑SM͒ and electromigration ͑EM͒ were widely used to study the performance of interconnection process of metal/via formation in copper <phrase>dual damascene</phrase> of wafers. Necking and voids at the via bottom were important in causing failures in tests of <phrase>stress migration</phrase> and electromigration. In this report, the contamination of the bottom of via, which results in poor step coverage, the adhesion of seed layers, and poor copper grain formation are identified to be the underlying causes of the necking and void formation after the first EM and SM tests are performed. The contamination of the via formation processes included via etching, trench etching, and barrier/seed layer depositions. A well-shaped via profile can be optimized using three methods, the first involves Cu/<phrase>SiN</phrase> interface stress, the second involves Cu grain growth, and the third involves post via etching clean study. Eliminating the contamination of the via bottom and optimizing step coverage and adhesion of the barrier seed layers improve the EM and SM performance from time-to-fail ϭ 13 to 59 s, in the copper-related processes for fabricating 300 mm wafers using technology that is beyond 0.13 ␮m technology. Copper ͑Cu͒ has been adopted in deep submicrometer ultralarge scale integration ͑ULSI͒ metallization and interconnection because it has low resistivity and better reliability than other <phrase>metals</phrase>. 1-4 A <phrase>dual damascene</phrase> process has been used for fabricating copper inter-connections. Via/trenching etch, <phrase>tantalum</phrase> <phrase>nitride</phrase> ͑TaN͒ deposition, copper seed layer deposition, Cu <phrase>electrochemical</phrase> plating ͑ECP͒, and copper chemical mechanical polishing ͑CMP͒ are the major processes in the fabrication of <phrase>dual damascene</phrase>. <phrase>Stress migration</phrase> ͑SM͒ and electromigration ͑EM͒ tests are usually used to qualify the performance of the interconnection process associated with metal/via formation in copper <phrase>dual damascene</phrase> of wafers. Ogawa et al. 5 explained electromigration in terms of mass transport. Fischer et al. 6 established a strong correlation of electromigration failure with local defects from processes such as liner deposition, preclean, or trench etches. Tokei et al. 7 claimed that electromigration was influenced by <phrase>argon</phrase> preclean. <phrase>Suzuki</phrase> et al. 8 showed that the <phrase>failure rate</phrase> of <phrase>stress migration</phrase> depends on both the line width and via diameter. <phrase>Ishikawa</phrase> et al. 9 revealed that <phrase>stress migration</phrase> of Cu interconnects depends strongly on the adhesive strength of the barrier metal/Cu interface and the step coverage. Alers et al. 10 explained copper contamination of via sidewalls and interlevel dielectric ͑ILD͒ damage during sputter preclean affects <phrase>stress migration</phrase>. Ogawa et …
Using Symbolic Learning to Improve <phrase>Knowledge-Based</phrase> <phrase>Neural Networks</phrase> The previously-reported Kbann system integrates existing knowledge into <phrase>neural networks</phrase> by deening the <phrase>network topology</phrase> and setting initial link weights. Standard neural <phrase>learning techniques</phrase> can then be used to train such networks, thereby reening the information upon which the network is based. However, standard neural <phrase>learning techniques</phrase> are reputed to have dii-culty training networks with multiple layers of <phrase>hidden units</phrase>; Kbann commonly creates such networks. In addition , standard neural <phrase>learning techniques</phrase> ignore some of the information contained in the networks created by Kbann. This paper describes a symbolic inductive learning algorithm for training such networks that uses this previously-ignored information and which helps to address the problems of training \deep" networks. <phrase>Empirical evidence</phrase> shows that this method improves not only learning speed, but also the ability of networks to generalize correctly to testing examples.
<phrase>Magnetization</phrase> transfer ratio in neuro-<phrase>Behçet disease</phrase>. The aim of this study was to determine the contribution of <phrase>magnetization</phrase> transfer ratios (MTRs) in detecting disease in normal-appearing brain regions of patients with neuro-Behçet (NB) disease. Thirty-two patients with NB disease were assessed. Fifteen healthy volunteers were examined as the control group. <phrase>Magnetic resonance</phrase> (MR) imaging of the head was performed without and with <phrase>magnetization</phrase> transfer (MT) contrast. Signal intensity measurements were obtained from ten anatomical regions (centrum semiovale, <phrase>corona</phrase> radiata, <phrase>internal capsule</phrase>, <phrase>forceps</phrase> major, <phrase>forceps</phrase> minor, thalamus, <phrase>substantia nigra</phrase> pars compacta, <phrase>substantia nigra</phrase> pars grisea, inferior <phrase>pons</phrase> and middle cerebellar <phrase>peduncle</phrase>) in both groups. Also measured in the NB group were parenchymal lesions in the <phrase>brain stem</phrase>, <phrase>basal ganglia</phrase> and cerebral deep <phrase>white matter</phrase>. MTR was calculated for each measurement. <phrase>Statistical analysis</phrase> was performed with Mann-<phrase>Whitney</phrase> U and independent t-tests with computer-based <phrase>SPSS</phrase> 11.0 for Windows software. A <phrase>P value</phrase> below 0.05 was considered <phrase>statistically significant</phrase>. The mean MTR of the parenchymal lesions in the NB group was lower than the mean MTR of the normal-appearing <phrase>parenchyma</phrase> in both the NB patients and the normal group. For the normal-appearing <phrase>parenchyma</phrase> the mean MTR in the NB group was higher than that for the controls for all regions except the <phrase>corona</phrase> radiata; however, the difference was <phrase>statistically significant</phrase> only for the thalamus. The MRI-visible parenchymal involvement of <phrase>Behçet's disease</phrase> causes a decrease in MTR. For the normal-appearing brain, although lacking <phrase>statistical significance</phrase> for the most regions studied, the tendency for higher MTR in NB patients compared with controls may offer an insight into the <phrase>pathophysiology</phrase> of <phrase>Behçet's disease</phrase>.
Electromigration-Aware <phrase>Physical Design</phrase> of <phrase>Integrated Circuits</phrase> <phrase>High-speed</phrase> interconnect technology : on-chip and off-chip p. 7 Testing nanometer digital integration circuits : myths, reality and the road ahead p. 8 SoC <phrase>design methodology</phrase> : a practical approach p. 10 <phrase>Test methodologies</phrase> in the <phrase>deep submicron era</phrase>-analog, <phrase>mixed-signal</phrase>, and RF p. 12 Recent advances in verification, <phrase>equivalence</phrase> checking and SAT-solvers p. 14 Compact MOSFET models for <phrase>low power</phrase> analog CMOS design p. 15 Physics and technology : towards <phrase>low-power</phrase> DSM design p. 16 Architectural, system level and protocol level techniques for <phrase>power optimization</phrase> for networked <phrase>embedded systems</phrase> p. 18 The high walls have crumpled p. 21 65nm omnibudsman p. 25 ESL-the next <phrase>leadership</phrase> opportunity for India? p. 26 <phrase>VLSI design</phrase> challenges for gigascale integration p. 27 <phrase>Moore's law</phrase> is unconstitutional p. 31 Configurable processor the building block for SOC (system-on-a-chip) p. 35 Modeling usable and reusable transactors in system verilog p. 36 Optimizing SoC manufacturability p. 37 <phrase>Tuple</phrase> detection for path <phrase>delay faults</phrase> : a method for improving <phrase>test set</phrase> quality p. 41 A delay test to differentiate resistive interconnect faults from weak transistor defects p. 47 Efficient space/time compression to reduce <phrase>test data volume</phrase> and testing time for <phrase>IP cores</phrase> p. 53 On efficient X-handling using a selective compaction scheme to achieve high test response compaction ratios p. 59 Heterogeneous and multi-level compression techniques for test volume reduction in systems-on-chip p. 65 <phrase>Cellular automata</phrase> based <phrase>test structures</phrase> with logic folding p. 71 Electromigration-aware <phrase>physical design</phrase> of <phrase>integrated circuits</phrase> p. 77 <phrase>Variance reduction</phrase> in <phrase>Monte Carlo</phrase> capacitance extraction p. 85 A fast buffered routing tree construction algorithm under accurate delay model p. 91 Improved layout-driven area-constrained timing optimization by Net buffering p. 97 Battery model for <phrase>embedded systems</phrase> p. 105 Rapid embedded hardware/software system generation p. 111 A unified architecture for adaptive compression of data and code on <phrase>embedded systems</phrase> p. 117 A heuristic for peak <phrase>power constrained</phrase> design of <phrase>network-on-chip</phrase> (NoC) based multimode systems p. 124 A low-power current-mode clock distribution scheme for multi-GHz NoC-based SoCs p. 130 Implementing LDPC decoding on <phrase>network-on-chip</phrase> p. 134 A <phrase>RISC</phrase> hardware platform for <phrase>low power</phrase> Java p. 138 A low power reprogrammable <phrase>parallel processing</phrase> VLSI architecture for computation of <phrase>B-spline</phrase> based medical <phrase>image processing</phrase> system for fast characterization of tiny objects suspended in cellular fluid p. 147
Prediction of Financial <phrase>Time Series</phrase> with <phrase>Hidden Markov Models</phrase> Prediction of Financial <phrase>Time Series</phrase> with <phrase>Hidden Markov Models</phrase> In this thesis, we develop an extension of the <phrase>Hidden Markov Model</phrase> (HMM) that addresses two of the most important challenges of financial <phrase>time series</phrase> modeling: non-stationary and non-linearity. Specifically, we extend the HMM to include a novel exponentially weighted Expectation-Maximization (EM) algorithm to handle these two challenges. We show that this extension allows the HMM algorithm to model not only sequence data but also dynamic financial <phrase>time series</phrase>. We show the update rules for the HMM parameters can be written in a form of exponential moving averages of the model variables so that we can take the advantage of existing <phrase>technical analysis</phrase> techniques. We further propose a double weighted <phrase>EM algorithm</phrase> that is able to adjust training sensitivity automatically. Convergence results for the proposed algorithms are proved using techniques from the EM Theorem. Experimental results show that our models consistently beat the <phrase>S&P 500 Index</phrase> over five 400-day testing periods from 1994 to 2002, including both bull and bear markets. Our models also consistently outperform the top 5 <phrase>S&P 500</phrase> <phrase>mutual funds</phrase> in terms of the <phrase>Sharpe Ratio</phrase>. iii To my mom iv Acknowledgments I would like to express my deep gratitude to my senior supervisor, Dr. Anoop Sarkar for his great patience, detailed instructions and insightful comments at every stage of this thesis. What I have learned from him goes beyond the knowledge he taught me; his attitude toward research, carefulness and commitment has and will have a profound impact on my future academic activities. I am also indebted to my co-senior supervisor, Dr. Andrey <phrase>Pavlov</phrase>, for trusting me before I realize my results. I still remember the day when I first presented to him the " super five day pattern " program. He is the source of my knowledge in <phrase>finance</phrase> that I used in this thesis. Without the countless time he spent with me, this thesis surely would not have been impossible. I cannot thank enough my supervisor, Dr. Oliver Schulte. Part of this thesis derives from a course project I did for his <phrase>machine learning</phrase> course in which he introduced me the world of machine learning and interdisciplinary research approaches. Many of my <phrase>fellow</phrase> graduate students offered great help during my study and research at the <phrase>computing science</phrase> school. I thank <phrase>Wei</phrase> Luo for helping me with many issues regarding the L A T E Xand Unix; Cheng Lu and Chris Demwell for sharing their …
MoveMine: Mining <phrase>moving object data</phrase> for discovery of animal movement patterns With the maturity and wide availability of GPS, wireless, <phrase>telecommunication</phrase>, and Web technologies, massive amounts of object movement data have been collected from various <phrase>moving object</phrase> targets, such as animals, <phrase>mobile devices</phrase>, vehicles, and climate <phrase>radars</phrase>. Analyzing such data has <phrase>deep implications</phrase> in many applications, such as, <phrase>ecological</phrase> study, traffic control, mobile communication management, and climatological forecast. In this article, we focus our study on animal movement <phrase>data analysis</phrase> and examine advanced <phrase>data mining</phrase> methods for discovery of various animal movement patterns. In particular, we introduce a <phrase>moving object</phrase> <phrase>data mining</phrase> system, MoveMine, which integrates multiple <phrase>data mining</phrase> functions, including sophisticated <phrase>pattern mining</phrase> and trajectory analysis. In this system, two interesting <phrase>moving object</phrase> <phrase>pattern mining</phrase> functions are <phrase>newly developed</phrase>: (1) <i><phrase>periodic behavior</phrase> mining</i> and (2) <i>swarm <phrase>pattern mining</phrase></i>. For mining periodic behaviors, a reference location-<phrase>based method</phrase> is developed, which first detects the reference locations, discovers the periods in complex movements, and then finds periodic patterns by <phrase>hierarchical clustering</phrase>. For mining swarm patterns, an efficient method is developed to uncover flexible <phrase>moving object</phrase> clusters by relaxing the popularly-enforced collective movement constraints.  In the MoveMine system, a set of commonly used <phrase>moving object</phrase> mining functions are built and a user-friendly interface is provided to facilitate interactive exploration of <phrase>moving object</phrase> <phrase>data mining</phrase> and flexible tuning of the mining constraints and parameters. MoveMine has been tested on multiple kinds of real datasets, especially for MoveBank applications and other <phrase>moving object data</phrase> analysis. The system will benefit scientists and other users to carry out versatile analysis tasks to analyze object movement regularities and anomalies. Moreover, it will benefit researchers to realize the importance and limitations of current techniques and promote future studies on <phrase>moving object</phrase> <phrase>data mining</phrase>. As expected, a mastery of animal movement patterns and trends will improve our understanding of the interactions between and the changes of the animal world and the <phrase>ecosystem</phrase> and therefore help ensure the sustainability of our <phrase>ecosystem</phrase>.
Bayesian Generic Priors for <phrase>Causal Learning</phrase> Structure and Strength in Causal Models Causal Graphs Lend Themselves to the Development of Rational Models Based on Bayesian We present a Bayesian model of <phrase>causal learning</phrase> that incorporates generic priors on distributions of weights representing potential powers to either produce or prevent an effect. These generic priors favor necessary and sufficient causes. The NS power model couples these priors with a causal <phrase>generating function</phrase> derived from the power PC theory (Cheng, 1997). We test this and other alternative Bayesian models using the strategy of computational cognitive <phrase>psychophysics</phrase>, fitting multiple <phrase>data sets</phrase> in which several parameters are varied parametrically across multiple types of judgments. The NS power model accounts for a wide range of data concerning judgments of both causal strength (the power of a cause to produce or prevent an effect) and causal structure (whether or not a causal link exists). For both types of causal judgments, a generic prior favoring a cause that is jointly necessary and sufficient explains interactions involving causal direction (generative versus preventive causes). For structure judgments, an additional prior that a new candidate cause will be deterministic (i.e., sufficient or else ineffective) explains why people's causal structure judgments are based primarily on causal power and the base rate of the effect, rather than <phrase>sample size</phrase>. Alternative Bayesian formulations that lack either causal power assumptions or generic priors for necessity and sufficiency proved inadequate. Broader implications of the <phrase>Bayesian framework</phrase> for human learning are discussed. Intelligent behavior in a complex and potentially hostile environment depends on acquiring and exploiting knowledge of " what causes what. " Although causal induction can certainly be constrained in a top-down fashion by prior causal knowledge, new causal knowledge ultimately depends on an <phrase>inference engine</phrase> that takes non-causal knowledge (most notably, information about temporal order and covariation) as input and yields causal knowledge as its output (Cheng, 1993). It is likely that the cognitive mechanisms for <phrase>causal learning</phrase> have deep evolutionary roots, a conjecture supported by many parallels between phenomena in animal conditioning and human <phrase>causal learning</phrase> (see <phrase>Shanks</phrase>, 2004). It is therefore a plausible hypothesis that the system for general <phrase>causal learning</phrase> is well-adapted to the characteristics of causes that operate in the <phrase>natural environment</phrase>, and hence will be characterized by <phrase>bounded rationality</phrase> (Simon, 1955). A useful formalism for representing causal hypotheses is provided by directed causal graphs, simple examples of which are shown in Figure 1. Within a causal graph, each directed <phrase>arrow</phrase> connects a node representing a cause to one of its effects, where it is …
<phrase>Video Compression</phrase> Algorithm Based on Frame Difference Approaches The huge usage of digital multimedia via communications, <phrase>wireless communications</phrase>, Internet, <phrase>Intranet</phrase> and cellular mobile leads to incurable growth of <phrase>data flow</phrase> through these Media. The researchers go deep in developing efficient techniques in these fields such as compression of data, image and video. Recently, <phrase>video compression</phrase> techniques and their applications in many areas (educational, <phrase>agriculture</phrase>, medical …) cause this field to be one of the most interested fields. <phrase>Wavelet transform</phrase> is an efficient method that can be used to perform an efficient compression technique. This work deals with the developing of an efficient <phrase>video compression</phrase> approach based on frames difference approaches that concentrated on the calculation of frame near distance (difference between frames). The selection of the meaningful frame depends on many factors such as compression performance, frame details, frame size and near distance between frames. Three different approaches are applied for removing the lowest frame difference. In this paper, many videos are tested to insure the efficiency of this technique, in addition a good performance results has been obtained.
<phrase>Electronic commerce</phrase> and the <phrase>strategic management</phrase> of <phrase>deep-sea</phrase> container shipping companies: an exploratory survey analysis Despite the increased adoption of <phrase>electronic commerce</phrase> (EC) among container shipping companies, the role of EC practice on their <phrase>strategic management</phrase> is <phrase>poorly understood</phrase>. This study takes an interdisciplinary approach to examine the uses, motivations, barriers and strategic relevance of EC in the container shipping industry in 1992 and 2002. To test five hypotheses, a 41-question survey was sent to 297 shipping companies, yielding an 11.1% response rate. The research found that the role of EC became more strategic and relevant for the identification of business goals. This change, however, could not be explained solely by the desire of companies to exploit EC externally with clients, but also by their ability to master EC internally. The role of EC was more strategic in companies where service is a key element of the company's competitive strategy and EC is regarded as a competitive necessity.
<phrase>RT-level</phrase> fast fault simulator In this paper a new fast fault simulation technique is presented for calculation of fault propagation through HLPs (<phrase>High Level</phrase> Primitives). ROTDDs (Reduced Ordered <phrase>Ternary</phrase> Decision Diagrams) are used to describe HLP modules. The technique is implemented in the HTDD <phrase>RT-level</phrase> fault simulator. The simulator is evaluated with some ITC99 benchmarks. A hypothesis is proved that a test set coverage of physical failures can be anticipated with <phrase>high accuracy</phrase> when RTL <phrase>fault model</phrase> <phrase>takes into account</phrase> optimization strategies that are used in CAE system applied. 1. Introduction <phrase>Recent developments</phrase> in the area of <phrase>deep-submicron technology</phrase> have challenged <phrase>integrated circuit</phrase> (IC) <phrase>test methods</phrase> as never before [1]. The increasing complexity of systems being designed makes test development more time-consuming. Moreover, nanometer technology has introduced new problems such as new types of defects or higher data rates. To reduce manufacturing cost and time-to-market, efficient fault detection and location should be used. One of the most essential tasks in <phrase>fault diagnosis</phrase> is fault simulation [2]. Current <phrase>Computer-Aided Engineering</phrase> (CAE) tools must address the needs for new generation of ICs e.g., systems-on-a-chip (SOC). Recent works in this area have increased emphasis on new design techniques such as <phrase>high-level</phrase> synthesis, behavioral synthesis, <phrase>design reuse</phrase> and IP-based design. For this reason, new ATPG tools that reflect new design flows should be developed, especially tools working at higher <phrase>level of abstraction</phrase> than <phrase>gate-level</phrase>. Several approaches for <phrase>High-Level</phrase> <phrase>Automatic Test Pattern Generation</phrase> (HLATPG) have already been proposed. In <phrase>Artist</phrase> [3,4] a quality of generated <phrase>test sequence</phrase> is measured in terms of the number of blocks of statements in source description of a system activated during its true-value <phrase>RT-level</phrase> simulation. <phrase>Artist</phrase> accepts synthesizable functional <phrase>register transfer level</phrase> (RTL)
A Scalable Model of the Substrate Network in Deep n-Well RF MOSFETs with Multiple Fingers A novel scalable model of substrate components for deep n-well (DNW) RF MOSFETs with different number of fingers is presented for the first time. The test structure developed in [1] is employed to directly access the characteristics of the substrate to extract the different substrate components. A methodology is developed to directly extract the parameters for the substrate network from the measured data. By using the measured two-port data of a set of nMOSFETs with different number of fingers, with the DNW in grounded and float configuration, respectively, the parameters of the scalable substrate model are obtained. The method and the substrate model are further verified and validated by matching the measured and simulated output admit-tances. Excellent agreement up to 40 GHz for configurations in common-source has been achieved.
Clipper package (Version 1.14.0) 1 Along signal paths: an empirical <phrase>gene set</phrase> approach exploiting pathway topology 1.1 clipper approach Different experimental conditions are usually compared in terms of their <phrase>gene expression</phrase> mean differences. In the <phrase>univariate</phrase> case, if a <phrase>gene set</phrase> changes significantly its multivariate mean expression in one condition with respect to the other, it is said to be differentially expressed. However, the difference in mean expression levels does not necessarily result in a change of the interaction strength among genes. In this case, we will have pathways with significant altered mean expression levels but unaltered biological interactions. On the contrary, if transcripts abundances ratios are altered, we expect a significant alteration not only of their mean, but also of the strength of their connections, resulting in pathways with completely corrupted functionality. Therefore, to look for pathways strongly involved in a biological process, we should look at pathways with both mean and variance significantly altered. clipper is based on a two-step approach: 1) it selects pathways with covariance matrices and/or means significantly different between experimental conditions and 2) on such pathways, it identifies the sub-paths mostly associated to the phenotype. This is a very peculiar feature in <phrase>pathway analysis</phrase>. To our knowledge this is the first approach able to systematically inspect a pathway deep in its different portions. Currently there are some <phrase>pathway analysis</phrase> methods implemented in Bioconductor (probably the most famous is GSEA), but very few of them try to exploit pathway topology. Example of the latter category are SPIA and DEGraph. In Martini et al. 2012 is provided a detailed comparison of the performance of non-topological analysis (GSEA) and topological analysis (SPIA and clipper) using both real and simulated data. In the next few words, we are going to highlight the defferences of these apperoaches. GSEA uses pathway as a list of genes without taking into account the structure of the pathway while SPIA <phrase>takes into account</phrase> pathway topological information, gene fold-changes and pathway enrichment scores. Then SPIA takes as input only the list of differentially expressed genes. So, from a practical <phrase>point of view</phrase> clipper and SPIA test different null hypotheses. More importantly, clipper is able to highlight the portions (sub-path) of the pathway that are mostly involved in the phenotype under study using graph decomposition theory. For more ditails please refer to Martini et al. 2012.
<phrase>Scaffolding</phrase> <phrase>Problem Solving</phrase> with Annotated, Worked-Out Examples to Promote <phrase>Deep Learning</phrase> This study seeks to compare the relative utility for learning college-level physics of <phrase>intelligent tutoring systems</phrase> that have procedural based hints and worked-out examples. In order to test which produced better gains, a modified version of <phrase>Andes</phrase> was used in which participants either received hints or annotated, worked-out examples in response to their help requests. We found that providing annotated, worked-out examples instead of hint sequences was more efficient in the number of problems it took to obtain basic mastery.
Better <phrase>Mixing</phrase> via Deep Representations It has been hypothesized, and supported with experimental evidence, that deeper representations , when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce <phrase>Markov chains</phrase> that mix faster between modes. Consequently, <phrase>mixing</phrase> between modes would be more efficient at <phrase>higher levels</phrase> of representation. To better understand this, we propose a secondary conjecture: the <phrase>higher-level</phrase> samples fill more uniformly the space they occupy and the <phrase>high-density</phrase> <phrase>manifolds</phrase> tend to unfold when represented at <phrase>higher levels</phrase>. The <phrase>paper discusses</phrase> these hypotheses and tests them experimentally through visualization and measurements of <phrase>mixing</phrase> between modes and <phrase>interpolating</phrase> between samples.
A Sparse and Locally Shift Invariant Feature Extractor Applied to Document Images We describe an <phrase>unsupervised learning</phrase> algorithm for extracting sparse and locally shift-<phrase>invariant features</phrase>. We also devise a principled procedure for learning hierarchies of <phrase>invariant features</phrase>. Each feature detector is composed of a set of trainable convolutional filters followed by a <phrase>max-pooling</phrase> layer over non-overlapping windows, and a point-wise sig-moid non-linearity. A second stage of more <phrase>invariant features</phrase> is fed with patches provided by the first stage feature extractor, and is trained in the same way. The method is used to pre-train the first four layers of a deep convolutional network which achieves state-of-the-art performance on the MNIST dataset of handwritten digits. The final testing <phrase>error rate</phrase> is equal to 0.42%. Preliminary experiments on compression of bitonal document images show very promising results in terms of <phrase>compression ratio</phrase> and reconstruction error .
<phrase>Deep Learning</phrase> of Appearance Models for Online <phrase>Object Tracking</phrase> —This paper introduces a novel <phrase>deep learning</phrase> based approach for vision based single target tracking. We address this problem by proposing a <phrase>network architecture</phrase> which takes the input video frames and directly computes the tracking score for any candidate target location by estimating the <phrase>probability distributions</phrase> of the positive and negative examples. This is achieved by combining a <phrase>deep convolutional neural network</phrase> with a Bayesian loss layer in a unified framework. In order to deal with the limited number of positive training examples, the network is <phrase>pre-trained</phrase> offline for a generic image feature representation and then is fine-tuned in multiple steps. An online fine-tuning step is carried out at every frame to learn the appearance of the target. We adopt a two-stage iterative algorithm to adaptively update the network parameters and maintain a <phrase>probability density</phrase> for target/non-target regions. The tracker has been tested on the standard tracking benchmark and the results indicate that the proposed solution achieves state-of-the-art tracking results.
Enabling <phrase>Large-Scale</phrase> <phrase>Pervasive Logic</phrase> Verification through Multi-Algorithmic Formal Reasoning — <phrase>Pervasive Logic</phrase> is a broad term applied to the variety of logic present in hardware designs, yet not a part of their primary functionality. Examples of <phrase>pervasive logic</phrase> include initialization and self-test logic. Because <phrase>pervasive logic</phrase> is intertwined with the func-tionality of chips, the verification of such logic tends to require very deep <phrase>sequential analysis</phrase> of very large slices of the design. For this reason, <phrase>pervasive logic</phrase> verification has hitherto been a task for which formal algorithms were not considered applicable. In this paper, we discuss several <phrase>pervasive logic</phrase> verification tasks for which we have found the proper combination of algorithms to enable <phrase>formal analysis</phrase>. We describe the nature of these verification tasks, and the testbenches used in the verification process. We furthermore discuss the types of algorithms needed to solve these verification tasks, and the type of tuning we performed on these algorithms to enable this analysis.
Hybrid-SBST Methodology for Efficient Testing of Processor Cores &AGGRESSIVE SEMICONDUCTOR FABRICATION processes have resulted in state-of-the-art processor designs and SoCs built around processor cores that contain more than a billion transistors and operate at <phrase>gigahertz</phrase> frequencies. <phrase>Deep-submicron</phrase> geometries and complex speed-enhancing mechanisms produce excellent performance, but serious testability challenges arise. New types of defects require the deployment of at-speed tests to achieve high <phrase>test quality</phrase>. Traditional processor at-speed manufacturing tests based on functional ATE cannot be considered an economically viable scheme. Several <phrase>software-based self</phrase>-test (SBST) methodolo-gies have been proposed as an effective alternative or supplement for the <phrase>manufacturing test</phrase> of microprocessors and embedded processors in SoCs (see the ''Related work'' sidebar). SBST is a nonintrusive approach that embeds a ''software tester'' in the form of a self-<phrase>test program</phrase> in a processor's on-chip memory. The ATE loads the program into the processor's on-chip memory. During <phrase>test application</phrase>, the processor executes this self-test program at its normal operational frequency, thereby achieving at-speed testing. Test responses collected from the processor are stored in the on-chip data memory. Finally, the ATE unloads the test responses from the on-chip memory. Modern microprocessors integrate large caches on the same die, which enables self-<phrase>test program</phrase> execution from the on-chip cache, provided there is a cache loader mechanism to load the <phrase>test program</phrase> and unload the test response. Thus, by changing the external ATE's role from actual <phrase>test application</phrase> to that of an interface with the on-chip memory before and after the test, SBST achieves the goal of at-speed testing using low-speed, <phrase>low-cost</phrase> external ATE. SBST is a scalable, portable, and reusable methodology for <phrase>high-quality</phrase> testing that incurs virtually zero performance, power, and circuit <phrase>area overhead</phrase>. In addition, because the vehicles for applying SBST programs are existing processor instructions, at-speed testing is feasible without the risk of thermal damage due to excessive signal activity in special test modes. Furthermore, by using the processor's <phrase>instruction set architecture</phrase> (ISA) and complying with all the restrictions enforced by both the ISA and the designers' decisions, SBST avoids overtesting for faults that don't appear during normal circuit operation and saves valuable yield. Despite the significant advantages, however, most present forms of SBST represent a <phrase>semi-automated</phrase> approach that requires test engineer expertise for self-<phrase>test program</phrase> development. Modern commercial processors are characterized by a high level of complexity , and their architectural features introduce <phrase>test challenges</phrase> that no single SBST methodology can effectively address. Additionally, SBST methodologies have their individual advantages and …
<phrase>Digital Design</phrase> with KP-Lab KP-Lab is an EU Integrated Project envisioning a learning system that facilitates innovative practices of sharing, creating and working with knowledge in education and workplaces. The project exploits a novel pedagogical view, the knowledge-creation metaphor of learning. According to such " trialogical " approach, cognition arises through <phrase>collaborative work</phrase> in systematically developing shared " knowledge artefacts " , such as concepts, plans ,material products, or social practices. The paper presents the plan of a pilot course to test the KP-Lab methodologies and tools in the field of <phrase>Digital Design</phrase>. 1 Engineering education today In spite of the recent hype on new educational technologies and their potential for a novel approach on learning and acquiring knowledge, university education has remained the same that it has been since the first <phrase>universities</phrase> have been established, many centuries ago. The transmission model is still dominant and now, as in the Middle Age, students of <phrase>higher education</phrase> take many years in acquisition-oriented and teacher-centered studies. Technical education at a university level has a different history, since the first engineering schools in <phrase>Europe</phrase> were established only during the nineteen century. For example, the origins of the University of <phrase>Genoa</phrase> go back to the thirteen century, while its first technical school, the " <phrase>Royal Naval School</phrase> " has been established only in 1870. Before the diffusion of formal engineering studies, the practitioners got their training outside a classroom, usually by joining a community of people already engaged in the exercise of the profession, acquiring and sharing knowledge, information, processes and practices. A natural and <phrase>deep learning</phrase> resulted from the deep interaction among people themselves and among people and the objects of their practice. The accelerated rate of technological innovation brought about by the <phrase>industrial revolution</phrase> showed the limitations of this ageless mode of learning and prompted the birth of formal technical education. An engineering school is, in fact, more effective in transmitting knowledge at a fast pace and in providing the stronger theoretical background necessary for the new technologies. An unwanted <phrase>casualty</phrase> of this process has been, unfortunately, the <phrase>active learning</phrase> mode typical of the former mode. Engineering curricula are able to adjust their contents to support the new technologies, but are still based largely on frontal lectures, assembled into courses that provide fragmented pieces of knowledge and skills. There is hardly any explicit continuation from one course to another. In conclusion, today's engineering curricula embody in …
Logical Validation, Answer Merging and Witness Selection - A Study in Multi-Stream <phrase>Question Answering</phrase> The paper presents an approach to multi-stream <phrase>question answering</phrase> (QA) using <phrase>deep semantic</phrase> parsing and logical validation for filtering answer candidates. A robust entailment check is accomplished by embedding the prover in a relaxation loop. Fallback strategies ensure a graceful degradation of performance in the case of parsing problems. The logical validity score is complemented by <phrase>false-positive</phrase> tests and heuristic quality indicators which also affect the selection of the most trusted answers. Separate criteria are used for choosing a suitable 'witness', i.e. a text passage which substantiates the answer. We present two experiments in which the method is applied for merging the results of various state-of-the-art QA systems. The evaluation demonstrates that the approach is applicable to heterogeneous QA streams – in particular it improves results for a combination of precision-oriented and recall-oriented answer streams. The method automatically adapts to these characteristics of a QA system by learning parameters from a training sample.
An Information Theoretic Analysis of Decision in <phrase>Computer Chess</phrase> The basis of the method proposed in this article is the idea that information is one of the most important factors in strategic decisions, including decisions in <phrase>computer chess</phrase> and other strategy games. The model proposed in this article and the algorithm described are based on the idea of a information theoretic basis of decision in strategy games. The model generalizes and provides a mathematical justification for one of the most popular <phrase>search algorithms</phrase> used in leading <phrase>computer chess</phrase> programs, the fractional ply scheme. However, despite its success in leading <phrase>computer chess</phrase> applications, until now few has been published about this method. The article creates a fundamental basis for this method in the <phrase>axioms</phrase> of <phrase>information theory</phrase>, then derives the principles used in programming the search and describes mathematically the form of the coefficients. One of the most important parameters of the fractional ply search is derived from fundamental principles. Until now this coefficient has been usually handcrafted or determined from intuitive elements or <phrase>data mining</phrase>. There is a deep, information theoretical justification for such a parameter. In one way the method proposed is a generalization of previous methods. More important, it shows why the fractional depth ply scheme is so powerful. It is because the algorithm navigates along the lines where the highest information gain is possible. A working and original implementation has been written and tested for this algorithm and is provided in the appendix. The article is essentially self-contained and gives proper background knowledge and references. The assumptions are intuitive and in the direction expected and described intuitively by great champions of chess.
<phrase>High-fidelity</phrase> <phrase>Image-based</phrase> Modeling Figure 1: Shaded and texture-mapped images of the reconstructions of a Polynesian <phrase>statue</phrase>, a <phrase>Homo Heidelbergensis</phrase> skull, resting on a circular <phrase>cork</phrase> stand, and an <phrase>action figure</phrase> depicting a <phrase>Roman</phrase> <phrase>soldier</phrase>. Abstract This article presents a novel method for acquiring <phrase>high-fidelity</phrase> solid models of complex 3D shapes from multiple calibrated photographs. The proposed approach enforces both the photometric and geometric constraints associated with available <phrase>image data</phrase> using a simple iterative deformation process. Concretely, a wide-baseline <phrase>stereo matching</phrase> technique based on Mikolajczyk's and Schmid's affine regions is first used to reconstruct a dense set of patches on the surface of the object of interest. Next, the boundary of the object's visual <phrase>hull</phrase> is deformed to pass through the centers of the reconstructed patches and recover the surface's main structural features and concavities. Fine surface details are finally reconstructed using a local refinement process that enforces <phrase>smoothness</phrase>, photometric, and geometric constraints at every vertex of the surface. The proposed approach has been implemented, and tested on 10 real datasets including objects with fine details, high-curvature areas, and deep concavities, and an object with little texture. Qualitative and quantitative comparisons with models obtained by state-of-the-art <phrase>image-based</phrase> modeling algorithms and <phrase>laser range</phrase> scanners are also presented.
<phrase>Temperature dependence</phrase> of neutron-induced soft errors in SRAMs Available online xxxx a b s t r a c t We irradiated commercial SRAMs with wide-spectrum <phrase>neutrons</phrase> at different temperatures. We observed that, depending on the vendor, the <phrase>soft error rate</phrase> either increases or slightly decreases with temperature, even in devices belonging to the same technology node. SPICE simulations were used to investigate the <phrase>temperature dependence</phrase> of the cell feedback time and restoring current. The shape and magnitude of the particle-induced transient current is discussed as a function of temperature. The variability in the response is attributed to the balance of contrasting factors, such as cell speed reduction and increased diffusion with increasing temperature. <phrase>Soft errors</phrase> are a serious concern not only in space but also at <phrase>sea level</phrase>, due to <phrase>neutrons</phrase> originating from the interactions of <phrase>cosmic rays</phrase> with the <phrase>atmosphere</phrase>, and <phrase>alpha particles</phrase>, coming from <phrase>radioactive</phrase> contaminants in the chip package and <phrase>solder</phrase> materials [1]. A large amount of work has been carried out in the field of <phrase>soft errors</phrase> (also known as <phrase>Single Event</phrase> Upsets, SEU). Experimental, simulation, and modeling efforts have led to a deep understanding of these phenomena, especially in SRAM cells, which, nowadays, are the most radiation-sensitive type of <phrase>solid-state</phrase> storage [1]. Funneling phenomena have been discovered and modeled [2]. The role of the struck junction load on the induced transient has been elucidated [3]. Neutron nuclear reactions leading to charged byproducts have been analyzed in the context of terrestrial <phrase>soft error rate</phrase> (SER) [4]. Yet, despite the huge amount of work, some areas have received relatively little attention from the <phrase>research community</phrase>. It is well known that electronic chips must operate at temperatures <phrase>significantly higher</phrase> than <phrase>room temperature</phrase>, especially for <phrase>high-performance</phrase>, space, or automotive applications. Contrary to <phrase>Single Event</phrase> Latchup (SEL) tests that are performed at the highest <phrase>operating temperature</phrase> [5], SEU measurements are usually carried out at <phrase>room temperature</phrase> [6], and little is known about the SEU <phrase>temperature dependence</phrase>. Only few works in the literature analyzed temperature effects on SER [7–9]. Some conclude that temperature plays a marginal role in determining the critical charge in SRAMs and, in general, the SER [7]. Sexton et al. observed an increased SEU sensitivity at <phrase>high temperature</phrase> in hardened memories and attributed this behavior to variations in the conductivity of the feedback and load <phrase>resistors</phrase> [8]. Truyen et al. performed TCAD simulations of heavy-ion strikes in 180-nm 6-T cells as a function of temperature [9]. …
Retrieval Term Prediction Using <phrase>Deep Belief Networks</phrase> This paper presents a method to predict retrieval terms from relevant/surrounding words or descriptive texts in Japanese by using <phrase>deep belief networks</phrase> (DBN), one of two typical types of <phrase>deep learning</phrase>. To determine the effectiveness of using DBN for this task, we tested it along with baseline methods using example-<phrase>based approaches</phrase> and conventional <phrase>machine learning</phrase> methods, i.e., <phrase>multi-layer</phrase> <phrase>perceptron</phrase> (MLP) and <phrase>support vector machines</phrase> (SVM), for comparison. The data for training and testing were obtained from the Web in manual and automatic manners. Automatically created pseudo data was also used. A grid search was adopted for obtaining the optimal hyper-parameters of these <phrase>machine learning</phrase> methods by performing <phrase>cross-validation</phrase> on <phrase>training data</phrase>. <phrase>Experimental results</phrase> showed that (1) using DBN has far higher prediction precisions than using baseline methods and higher prediction precisions than using either MLP or SVM; (2) adding automatically gathered data and pseudo data to the manually gathered data as <phrase>training data</phrase> is an effective measure for further improving the prediction precisions; and (3) DBN is able to deal with noisier <phrase>training data</phrase> than MLP, i.e., the prediction precision of DBN can be improved by adding noisy <phrase>training data</phrase>, but that of MLP cannot be.
<phrase>Genome-wide</phrase> generalized additive models 13 <phrase>Chromatin</phrase> <phrase>immunoprecipitation</phrase> followed by <phrase>deep sequencing</phrase> (<phrase>ChIP-Seq</phrase>) is a widely used 14 approach to study protein-DNA interactions. To analyze <phrase>ChIP-Seq</phrase> data, practitioners are 15 required to combine tools based on different statistical assumptions and dedicated to spe-16 cific applications such as calling protein occupancy <phrase>peaks</phrase> or testing for differential occu-17 pancies. Here, we present GenoGAM (<phrase>Genome-wide</phrase> Generalized Additive Model), which 18 brings the well-established and flexible generalized additive models framework to genomic 19 applications using a data parallelism strategy. We model <phrase>ChIP-Seq</phrase> read count frequencies 20 as products of smooth functions along <phrase>chromosomes</phrase>. Smoothing parameters are estimated 21 from the data eliminating <phrase>ad-hoc</phrase> binning and windowing needed by current approaches.
Fault Simulation of <phrase>Digital Circuits</phrase> at <phrase>Register Transfer Level</phrase> As the complexity of <phrase>Very Large Scale Integration</phrase> (VLSI) is growing, testing becomes tedious and tougher. As of now <phrase>fault models</phrase> are used to test <phrase>digital circuits</phrase> at the gate level or below that level. By using <phrase>fault models</phrase> at the lower levels, testing becomes cumbersome and will lead to delays in the <phrase>design cycle</phrase>. In addition, developments in <phrase>deep submicron technology</phrase> provide an opening to new defects. We must develop efficient fault detection and location methods in order to reduce manufacturing costs and time to market. Thus there is a need to look for a new approach of testing the circuits at <phrase>higher levels</phrase> to speed up the <phrase>design cycle</phrase>. This paper proposes on <phrase>Register Transfer Level</phrase> (RTL) modeling for <phrase>digital circuits</phrase> and computing the fault coverage. The result obtained through this work establishes that the fault coverage with the RTL <phrase>fault model</phrase> is comparable to the gate level <phrase>fault coverage</phrase>.
Screening VDSM Outliers using Nominal and Subthreshold <phrase>Supply Voltage</phrase> IDDQ Very Deep Sub-Micron (VDSM) defects are resolved as <phrase>Statistical Post-Processing</phrase>™ (<phrase>SPP</phrase>) outliers of a new IDDQ screen. The screen applies an IDDQ pattern once to the Device Under Test (DUT) and takes two <phrase>quiescent current</phrase> measurements. The <phrase>quiescent current</phrase> measurements are taken at nominal and at subthreshold <phrase>supply voltages</phrase>. The screen is demonstrated with 0.18µm and 0.13µm volume data. The screen's effectiveness is compared to stuck-at and other IDDQ screens. 1. Introduction This paper describes a new, two-measurement <phrase>IDDQ test</phrase> scheme called STIDDQ. The screen applies an IDDQ pattern once to the Device Under Test (DUT) but takes two <phrase>quiescent current</phrase> measurements. The first IDDQ measurement is taken at or above the nominal <phrase>supply voltage</phrase> and the second at a <phrase>supply voltage</phrase> less than 2Vt. Similar to <phrase>Statistical Post-Processing</phrase>™ methods, the ATE collects the raw-data and <phrase>pass/fail</phrase> is determined off-tester. The paper shows that the two-measurement IDDQ yields an outlier screen that is suitable for very-<phrase>deep sub-micron technologies</phrase>. The paper presents results for a 0.18µm ASIC and a 0.13µm large die, low yield ASIC.
YAFIMA: Yet Another Frequent Itemset Mining Algorithm Efficient discovery of <phrase>frequent patterns</phrase> from large databases is an active research area in <phrase>data mining</phrase> with broad applications in industry and <phrase>deep implications</phrase> in many areas of <phrase>data mining</phrase>. Although many efficient frequent-<phrase>pattern mining</phrase> techniques have been developed in the last decade, most of them assume relatively small databases, leaving extremely large but realistic datasets out of reach. A practical and appealing direction is to mine for closed or maximal <phrase>itemsets</phrase>. These are subsets of all <phrase>frequent patterns</phrase> but good representatives since they eliminate what is known as redundant patterns. The practicality of discovering closed or maximal <phrase>itemsets</phrase> comes from the relatively inexpensive process to mine them in comparison to finding all patterns. In this paper we introduce a new approach for traversing the <phrase>search space</phrase> to discover all <phrase>frequent patterns</phrase>, the closed or the maximal patterns efficiently in extremely large datasets. We present <phrase>experimental results</phrase> for finding all three types of patterns with very large database sizes never reported before. Our implementation tested on real and synthetic data shows that our approach outperforms similar state-of-the-art algorithms by at least one <phrase>order of magnitude</phrase> in terms of both execution time and memory usage, in particular when dealing with very large databases.
Automated Testing of Planning Models – Automated planning systems (APS) are maturing to the point that they have been used in experimental mode on both the NASA <phrase>Deep Space 1</phrase> spacecraft and the NASA Earth Orbiter 1 satellite. One challenge is to improve the <phrase>test coverage</phrase> of APS to ensure that no unsafe plans can be generated. Unsafe plans can cause wasted resources or damage to hardware. <phrase>Model checkers</phrase> can be used to increase <phrase>test coverage</phrase> for large complex <phrase>distributed systems</phrase> and to prove the absence of certain types of errors. In this work we have built a generalized tool to convert the input models of an APS to Promela, the <phrase>modeling language</phrase> of the Spin <phrase>model checker</phrase>. We demonstrate on a mission sized APS input model, that we with Spin can explore a large part of the space of possible plans and verify with high probability the absence of unsafe plans.
Northern <phrase>Gulf of Mexico</phrase> Stock 98º 96º 94º 92º 90º 88º 86º 84º 82º 80º 98º 96º 94º 92º 90º 88º 86º 84º 82º 80º 23º 25º 27º 29º 31º 33º 23º 25º 27º 29º 31º 33º <phrase>TX</phrase> LA MS AL <phrase>FL</phrase> GA <phrase>TX</phrase> LA MS AL <phrase>FL</phrase> GA Figure 1. Distribution of <phrase>Atlantic spotted</phrase> <phrase>dolphin</phrase> sightings from SEFSC spring and fall vessel surveys during 1996-2001. All the on-effort sightings are shown, though not all were used to estimate abundance. Solid lines indicate the 100 m and 1000 m isobaths and the dotted line shows the offshore extent of the U. STOCK DEFINITION AND GEOGRAPHIC RANGE The <phrase>Atlantic spotted</phrase> <phrase>dolphin</phrase> is <phrase>endemic</phrase> to the <phrase>Atlantic Ocean</phrase> in <phrase>temperate</phrase> to tropical waters (Perrin et al. 1987, 1994). In the <phrase>Gulf of Mexico</phrase>, <phrase>Atlantic spotted</phrase> <phrase>dolphins</phrase> occur primarily from <phrase>continental shelf</phrase> waters 10-200 m deep to slope waters <500 m deep (<phrase>Fulling</phrase> et al. 2003; Mullin and <phrase>Fulling</phrase>, in review). This species has also been reported around oceanic islands and far offshore in other areas (Perrin et al. 1994). <phrase>Atlantic spotted</phrase> <phrase>dolphins</phrase> were seen in all seasons during GulfCet aerial surveys of the northern <phrase>Gulf of Mexico</phrase> from 1992 to 1998 (Hansen et al. 1996; Mullin and Hoggard 2003). It has been suggested that this species may move inshore seasonally during spring, but data supporting this hypothesis are limited (Caldwell and Caldwell 1966; Fritts et al. 1983). In a recent study, Bero (2001) presented strong genetic support for differentiation between <phrase>Gulf of Mexico</phrase> and <phrase>western</phrase> <phrase>North Atlantic</phrase> management stocks using both <phrase>mitochondrial</phrase> and nuclear markers. However, this study did not test for further population subdivision with the <phrase>Gulf of Mexico</phrase>. Perrin et al. (1994) suggested that <phrase>island</phrase> and offshore form of the <phrase>Atlantic spotted</phrase> <phrase>dolphin</phrase> may be a different stock from those occurring on the <phrase>continental shelf</phrase>. However, the <phrase>Atlantic spotted</phrase> <phrase>dolphin</phrase> has not been sighted in the <phrase>deep waters</phrase> of the northern <phrase>Gulf of Mexico</phrase> (Mullin and <phrase>Fulling</phrase>, in review). POPULATION SIZE Estimates of abundance were derived through the application of distance sampling analysis (Buckland et al. 2001) and the computer program DISTANCE (Thomas et al. 1998) to sighting data. From 1991 through 1994, line-<phrase>transect</phrase> vessel surveys were conducted during spring in the northern <phrase>Gulf of Mexico</phrase> from the 200 m isobath to the seaward extent of the U.S. <phrase>Exclusive Economic Zone</phrase> (<phrase>EEZ</phrase>) (Hansen et al. 1995). Survey effort-weighted estimated average abundance of <phrase>Atlantic spotted</phrase> <phrase>dolphins</phrase> …
Waveform Analysis of UWB GPR Antennas <phrase>Ground Penetrating Radar</phrase> (GPR) systems fall into the category of <phrase>ultra-wideband</phrase> (UWB) devices. Most GPR equipment covers a <phrase>frequency range</phrase> between an <phrase>octave</phrase> and a decade by using short-time pulses. Each signal recorded by a GPR gathers a temporal log of attenuated and distorted versions of these pulses (due to the effect of the propagation medium) plus possible electromagnetic interferences and noise. In order to make a good interpretation of this data and extract the most possible information during processing, a deep knowledge of the wavelet emitted by the antennas is essential. Moreover, some advanced processing techniques require specific knowledge of this signal to obtain satisfactory results. In this work, we carried out a series of tests in order to determine the source wavelet emitted by a ground-coupled antenna with a 500 MHz central frequency.
Precision Dsn Radiometer Systems: Impact on Microwave Calibrations Precision Dsn Radiometer Systems: Impact on Microwave Calibrations The <phrase>NASA Deep Space Network</phrase> (DSN) has a long history of providing large parabolic dish antennas with precision surfac%s, low-loss feeds and <phrase>ultra-low</phrase> noise amplifiers for <phrase>deep space</phrase> <phrase>telecommunications</phrase>. To realize the benefits of high sensitivity, it is important that receiving systems are accurately calibrated and monitored to maintain peak performance. A method is described to measure system performance and to calibrate the receiving system using procedures, software and commercial instruments that are easy to implement and efficient to use, The utillity of the measurement procedures and the precision of the receiver calibration technque were demonstrated by performing tests at <phrase>Ka-band</phrase> (32 and 33.68 GHz) frequencies at Goldstone on a 34-m beam-<phrase>waveguide</phrase> antenna Observations of multiple calibration radio sources are used to measure the dependence of <phrase>antenna gain</phrase> and system <phrase>noise temperature</phrase> on source elovat " km and derive the peak value. Receiving system non-linearities are frequently overlooked as an error source in the calibration of microwave radiometers. 1 he experimental resutts described in this paper illustrate some of the ways that receiving system non-linearity can negatively impact system performance. A simple radiometer calibration technque and analysis provide quantitative information that enables the system engineer to adjust and linearize the receiving system. When that is not practical, <phrase>tha</phrase> experimenter or the operator can apply correction coefficients to the measured values of system <phrase>noise temperature</phrase> and thereby compensate for the receiving system non-linearity. The hgh performance antennas and the sensitive receiving systems of the DSN are valuablo resources for scientific research in addition to the primary telecommun'k,ation tasks that support spaoe missions. The <phrase>antenna gain</phrase> and system <phrase>noise temperature</phrase> measurements and the radiometer calibration method described in this paper are also useful to perform precision research experiments.
Enhanced launch-off-capture <phrase>transition fault</phrase> testing With today's design size in millions of gates and working frequency in <phrase>gigahertz</phrase> range, <phrase>timing-related defects</phrase> are high proportion of the total chip defects and at-speed test is crucial. The <phrase>transition fault</phrase> model is widely used for detecting delay-induced defects. There are two <phrase>transition fault</phrase> <phrase>pattern generation</phrase> methods; i.e. launch-off-shift (LOS) and launch-off-capture (LOC). In LOS, the transition is launched during the last shift cycle from the <phrase>scan path</phrase> (non-functional). The <phrase>scan enable</phrase> (SEN) is high during the last shift and must go low to enable response capture during the capture cycle. The time period for SEN to make this transition corresponds to the functional frequency. This is not applicable for very <phrase>low cost</phrase> ATE, which have a limitation of one at-speed signal port. In LOC method, the at-speed constraint on the SEN signal is relaxed and the transition is launched from the functional path. The controlla-bility of launching a transition at the target gate is less as it depends on the functional response of the circuit under test to the initialization vector. A novel <phrase>scan-based</phrase> at-speed test is proposed in which a transition can be launched either from the <phrase>scan path</phrase> or the functional path. The technique improves the controllability of <phrase>transition fault</phrase> testing and it does not require the <phrase>scan enable</phrase> to change at-speed. The <phrase>scan enable</phrase> control information is encapsulated in the test data and transferred during the scan operation to generate the local <phrase>scan enable</phrase> signals during the launch and capture cycle. A new scan cell, referred to as local <phrase>scan enable</phrase> generator (LSEG), is inserted in the <phrase>scan chains</phrase> to generate the local <phrase>scan enable</phrase> signals. The proposed technique is robust, practice-oriented and suitable for designs targeted for very <phrase>low cost</phrase> ATEs. I. INTRODUCTION The semiconductor industry is adopting new fabrication processes to meet the area, power and performance requirements. As a result, modern ICs are growing more complex in terms of <phrase>gate count</phrase> and <phrase>operating frequency</phrase> [1]. The deep-submicron (DSM) effects are becoming more prominent with shrinking technology, thereby increasing the probability of <phrase>timing-related defects</phrase> [2] [3]. For DSM designs, the <phrase>stuck-at fault</phrase> test alone cannot ensure <phrase>high quality</phrase> level of chips. In the past, functional patterns were used for at-speed test. However, <phrase>functional testing</phrase> is not a viable solution because of the difficulty and time to generate these tests for complex designs with very high gate density. Moreover, the number of required patterns …
Leveraging <phrase>Infrastructure IP</phrase> for SoC Yield In addition to the functional <phrase>IP cores</phrase>, today's SOC necessitates embedding a special family of <phrase>IP blocks</phrase>, called <phrase>Infrastructure IP</phrase> blocks. These are meant to ensure the manufacturability of the SOC and to achieve adequate levels of <phrase>yield and reliability</phrase>. The <phrase>Infrastructure IP</phrase> leverages the manufacturing knowledge and feeds back the information into the design phase. This keynote address analyzes the key trends and challenges resulting in manufacturing susceptibility and field reliability that necessitate the use of such <phrase>Infrastructure IP</phrase>. Then, it concentrates on certain examples of such embedded IPs for detection, analysis and correction. 1. Introduction This Every new <phrase>semiconductor technology</phrase> node provides further <phrase>miniaturization</phrase> and higher performance, thus increasing the functions that electronic products could offer. Although adding such new functions do benefit the <phrase>end-user</phrase>, but they also necessitate finer and denser semiconductor fabrication processes, which make chips more susceptible to defects. Today's very deep-submicron semiconductor technologies of 130 nanometers and below are reaching defect susceptibility levels that result in lowering the <phrase>manufacturing yield</phrase> and reliability, and hence lengthening the production ramp-up period, and therefore the time to volume (TTV). The very deep submicron impact on yield, reliability and TTV is very critical for the semiconductor industry. It puts the conventional IC realization flow at an impasse. In fact, every single phase in the IC realization flow impacts <phrase>yield and reliability</phrase>. This includes the design phase, prototyping or production ramp up, volume fabrication, test, assembly, packaging, and even the <phrase>post-production</phrase> life cycle of the chip. In order to optimize yield and reach acceptable TTV levels, the semiconductor industry needs to adopt advanced yield optimization solutions. These solutions need to be implemented at different phases of the chip realization flow. The conventional <phrase>semiconductor manufacturing</phrase> infrastructure, i.e. the external equipment and processes, alone are insufficient to handle such advanced yield optimization solutions; supplemental on-chip infrastructure is needed. To optimize <phrase>yield and reliability</phrase>, the industry has recently introduced a range of embedded <phrase>intellectual-property</phrase> (IP) blocks. Hence, In addition to the functional <phrase>IP cores</phrase>, today's SOC necessitates embedding a special family of <phrase>IP blocks</phrase>, called <phrase>Infrastructure IP</phrase> blocks. These are meant to ensure the manufacturability of the SOC and to achieve adequate levels of <phrase>yield and reliability</phrase>. The Infrastructure leverages the manufacturing knowledge and feeds back the information into the design phase. This embedded tutorial analyzes the key trends and challenges resulting in manufacturing susceptibility and field reliability that necessitate the use …
A comparison of usability methods for testing interactive health technologies: Methodological aspects and <phrase>empirical evidence</phrase> OBJECTIVE <phrase>Usability evaluation</phrase> is now widely recognized as critical to the success of interactive <phrase>health care</phrase> applications. However, the broad range of usability inspection and <phrase>testing methods</phrase> available may make it difficult to decide on a usability assessment plan. To guide novices in the <phrase>human-computer interaction</phrase> field, we provide an overview of the methodological and <phrase>empirical research</phrase> available on the three usability inspection and <phrase>testing methods</phrase> most often used.   METHODS We describe two 'expert-based' and one 'user-based' usability method: (1) the heuristic evaluation, (2) the cognitive walkthrough, and (3) the think aloud.   RESULTS All three <phrase>usability evaluation</phrase> methods are applied in laboratory settings. Heuristic evaluation is a relatively efficient <phrase>usability evaluation</phrase> method with a high benefit-cost ratio, but requires high skills and usability experience of the evaluators to produce reliable results. The cognitive walkthrough is a more structured approach than the heuristic evaluation with a stronger focus on the learnability of a computer application. Major drawbacks of the cognitive walkthrough are the required level of detail of task and user background descriptions for an adequate application of the latest version of the technique. The think aloud is a very direct method to gain deep insight in the problems <phrase>end users</phrase> encounter in interaction with a system but data analyses is extensive and requires a high level of expertise both in the cognitive <phrase>ergonomics</phrase> and in computer system application domain.   DISCUSSION AND CONCLUSIONS Each of the three <phrase>usability evaluation</phrase> methods has shown its usefulness, has its own <phrase>advantages and disadvantages</phrase>; no single method has revealed any significant results indicating that it is singularly effective in all circumstances. A combination of different techniques that compliment one another should preferably be used as their collective application will be more powerful than applied in isolation. Innovative mobile and automated solutions to support <phrase>end-user</phrase> testing have emerged making combined approaches of laboratory, field and remote usability evaluations of new <phrase>health care</phrase> applications more feasible.
<phrase>Recent developments</phrase> in large vocabulary <phrase>continuous speech recognition</phrase> — This paper overviews a series of recent approaches to <phrase>front-end</phrase> processing, <phrase>acoustic modeling</phrase>, language modeling, and back-end search and system combination which have made contributions for large vocabulary <phrase>continuous speech recognition</phrase> (LVCSR) systems. These approaches include the feature transformations, speaker-adaptive features, and discriminative features in <phrase>front-end</phrase> processing, the <phrase>feature-space</phrase> and model-space <phrase>discriminative training</phrase>, <phrase>deep neural networks</phrase>, and speaker adaptation in <phrase>acoustic modeling</phrase>, the backoff smoothing, large-span modeling, and model regularization in language modeling, and the system combination, cross-adaptation, and boosting in search and system combination. Some future directions for LVCSR research are also addressed. I. INTRODUCTION Over the past decade, several advances have been made to the design of modern LVCSR systems to the point where their application has broadened from early speaker-dependent dictation systems to speaker-independent automatic <phrase>broadcast</phrase> news transcription and indexing, lectures and meetings transcription , conversational <phrase>telephone</phrase> speech transcription, open-domain voice search, medical and legal <phrase>speech recognition</phrase> and call center applications to name a few. The commercial success of these systems is an impressive testimony to how far research in LVCSR has come and the aim of this paper is to describe some of the technological underpinnings of modern systems. It must be said however that, despite the commercial success and widespread adoption, the problem of large vocabulary <phrase>speech recognition</phrase> is far from being solved: background noise, channel distortions, foreign accents, casual and disfluent speech or unexpected topic change can cause automated systems to make egregious recognition errors. This is because current LVCSR systems are not robust to mismatched <phrase>training and test</phrase> conditions and cannot handle context as well as human listeners despite being trained on thousands of hours of speech and billions of words of text. Technological improvements have been made in four components of an LVCSR system: <phrase>front-end</phrase> processing, <phrase>acoustic modeling</phrase>, language modeling, hypothesis search and system combination. A comprehensive survey of early LVCSR systems was presented in [35]. The state of the art in LVCSR has shifted considerably since then through the advent of powerful speaker adaptation, <phrase>discriminative training</phrase> and language modeling techniques. This paper reports some advanced developments which are a substantial step toward making a number of high-utility applications possible [28].
Decoding the Value of <phrase>Computer Science</phrase> In The <phrase>Social Network</phrase>, a computer-programming <phrase>prodigy</phrase> goes to <phrase>Harvard</phrase> and creates a technology company in his sophomore dorm. Six year later, the company is worth billions and touches one out of every 14 people on earth. <phrase>Facebook</phrase> is a familiar American success story, with its founder, <phrase>Mark Zuckerberg</phrase>, following a path blazed by Bill Gates and others like him. But it may also become increasingly rare. Far fewer students are studying <phrase>computer science</phrase> in college than once did. This is a problem in more ways than one. The signs are everywhere. This year, for the first time in decades, the <phrase>College Board</phrase> failed to offer <phrase>high-school</phrase> students the <phrase>Advanced Placement</phrase> AB <phrase>Computer Science</phrase> exam. The number of <phrase>high schools</phrase> teaching <phrase>computer science</phrase> is shrinking, and last year only about 5,000 students sat for the AB test. Two decades ago, I was one of them. I have never held an <phrase>information-technology</phrase> job. Yet the more time passes, the more I understand how important that education was. Something is lost when students no longer study the working of things. My childhood interest in programming was a product of nature and nurture. My father worked as a computer scientist, first in a university and then as a researcher for <phrase>General Electric</phrase>. As a kid, I tagged along to his lab on weekends, watching him connect single-board DEC computers into ring networks and two-dimensional arrays, feeling the <phrase>ozone</phrase> hum of closet-sized machines. By my <phrase>adolescence</phrase>, in the mid-1980s, we had moved to a well-off <phrase>suburb</phrase> whose <phrase>high school</phrase> could afford its own <phrase>mainframe</phrase>. That plus social awkwardness meant many a night plugged into a 300-<phrase>baud</phrase> <phrase>modem</phrase>, battling other 15-year-olds in rudimentary <phrase>deep-space</phrase> combat and accumulating treasure in <phrase>Ascii</phrase>-rendered dungeons without end. Before long I wanted to understand where those games came from and how, exactly, they worked. So I took to programming, first in Basic and then Pascal. Coding taught me the shape of logic, the value of brevity, and the attention to detail that debugging requires. I learned that a beautiful program with a single misplaced <phrase>semicolon</phrase> is like a <phrase>sports car</phrase> with one <phrase>piston</phrase> out of line. Both are dead machines, functionally indistinguishable from junk. I learned that you are good enough to build things that do what they are supposed to do, or you are not. I left for college intending to major in computer science. That lasted until about …
Exploiting <phrase>Scene Context</phrase> for Image Captioning This paper presents a framework for image captioning by exploiting the scene context. To date, most of the captioning models have been relying on the combination of <phrase>Convolutional Neural Networks</phrase> (CNN) and the Long-<phrase>Short Term Memory</phrase> (LSTM) model, trained in an <phrase>end-to-end</phrase> fashion. Recently, there has been extensive research towards improving the <phrase>language model</phrase> and the CNN architecture, utilizing attention mechanisms, and improving the <phrase>learning techniques</phrase> in such systems. A less studied area is the contribution of the scene context in the captioning. In this work, we study the role of the scene context, consisting of the scene type and objects. To this end, we augment the CNN features with <phrase>scene context</phrase> features, including scene detectors, objects and their localization, and their combinations. We use the scene context features as an initialization feature at the zeroth time step in a LSTM model with deep residual connections. In subsequent time steps, the model, however, uses the original CNN features. The proposed <phrase>language model</phrase>, contrary to more conventional ones, thus has access to visual features through the whole process of sentence generation. We demonstrate that the scene context features affect the language formation and improve the captioning results in the proposed framework. We also report results from the Microsoft COCO benchmark, where our model achieves the state-of-the-art performance on the test set.
<phrase>Capture-power</phrase>-aware test <phrase>data compression</phrase> using selective encoding Keywords: <phrase>Test compression</phrase> <phrase>Low-power</phrase> testing <phrase>Scan-based</phrase> testing a b s t r a c t Ever-increasing <phrase>test data volume</phrase> and excessive test power are two of the main concerns of VLSI testing. The ''don't-care'' bits (also known as X-bits) in given test <phrase>cube</phrase> can be exploited for test <phrase>data compression</phrase> and/or test <phrase>power reduction</phrase>, and these techniques may contradict to each other because the very same X-bits are likely to be used for different optimization objectives. This paper proposes a <phrase>capture-power</phrase>-aware <phrase>test compression</phrase> scheme that is able to keep <phrase>capture-power</phrase> under a safe limit with low test <phrase>compression ratio</phrase> loss. Experimental results on <phrase>benchmark circuits</phrase> validate the effectiveness of the proposed solution. The <phrase>test data volume</phrase> for today's very <phrase>large scale</phrase> integrated (VLSI) circuits has been exploding with the ever-increasing integration capability of <phrase>semiconductor technology</phrase> [1]. In addition , besides the <phrase>test vectors</phrase> targeting traditional <phrase>stuck-at faults</phrase>, <phrase>test patterns</phrase> targeting <phrase>delay faults</phrase> and many other subtle faults are becoming essential to improve <phrase>test quality</phrase> for deep submi-<phrase>cron</phrase> designs. Large <phrase>test data volume</phrase> not only raises memory depth requirements for the <phrase>automatic test equipment</phrase> (ATE), but also prolongs ICs' testing time, thus significantly increasing <phrase>test cost</phrase>. To address this issue, various <phrase>test compression</phrase> techniques [2–24] have been proposed in the literature [25], and most of them exploited the ''don't-care'' bits (also known as X-bits) in given test cubes for effective <phrase>test compression</phrase>. 1 Generally speaking , the more X-bits in test cubes, the higher the test <phrase>compression ratio</phrase> can be achieved. At the same time, <phrase>power dissipation</phrase> during <phrase>scan-based</phrase> testing of <phrase>VLSI circuits</phrase> can be <phrase>significantly higher</phrase> than that during <phrase>normal operation</phrase> [26]. Elevated average test power, dominated by scan shift-power may cause structural damage to the circuit under test (CUT); while excessive peak test power in the capture phase is likely to cause good circuit to fail test, thus leading to unnecessary <phrase>yield loss</phrase> [27]. There is a rich literature on reducing test power in shift mode, in which <phrase>design-for-testability</phrase> (DfT) <phrase>based methods</phrase> such as <phrase>scan chain</phrase> partitioning technique [28–33] are very effective (when compared to X-filling techniques such as [34]). Compared to shift-power, <phrase>yield loss</phrase> caused by excessive <phrase>capture-power</phrase> has become a more serious concern with technology scaling. There are, however, no such effective DfT-<phrase>based techniques</phrase> for <phrase>capture-power</phrase> reduction, and we mainly <phrase>resort</phrase> to X-filling techniques (e.g., [35–39]) to reduce the excessive <phrase>capture-power</phrase> in <phrase>scan-based</phrase> testing. There is usually …
<phrase>Risk assessment</phrase> of software-system specifications —Summary & Conclusions—This paper presents a methodology and an example of <phrase>risk assessment</phrase> of functional-requirement specifications for complex real-time software systems. A heuristic <phrase>risk-assessment</phrase> technique based on CPN (colored <phrase>Petri-net</phrase>) models is presented. This technique is used to classify software functional-requirement specification components according to their relative importance in terms of such factors as severity and complexity. A dynamic complexity measure, based on concurrence in the <phrase>functional requirements</phrase>, is introduced. This technique is applied on the Earth Operation Commanding Center (EOC COMMANDING), a large component of the NASA Earth Observing System (<phrase>EOS</phrase>) project. Two specification models of the system are considered. Results of applying this technique to both models are presented. The <phrase>risk assessment</phrase> methodology in this paper suggests the following conclusions: • <phrase>Risk assessment</phrase> at the functional-requirement specification phase can be used to classify <phrase>functional requirements</phrase> in terms of their complexity & severity. The methodology identifies high-risk functional specification components that require appreciable development & verification resources during design, implementation, and testing. • Dynamic Complexity metrics and the concurrence metric (introduced in this paper) can important in assessing the risk factors based on the complexity of functional specifications. • The Concurrence complexity metric (introduced in this paper) is an important aspect of dynamic complexity. • CPN models can be used to build an executable specification of the system, which helps the analyst not only to acquire deep understanding of the system but also to study the dynamic behavior of the system by simulating the model. <phrase>Future research</phrase> in early <phrase>risk assessment</phrase> and complexity analysis could focus on: 1) <phrase>Software Architectures</phrase> based on Object Technology: The technique in this paper, with some modifications on complexity analysis and severity analysis, applies to the <phrase>design methods</phrase> and <phrase>software architectures</phrase> based on object technology. Further research is required to establish the complexity metrics for object-based systems. 2) SRE (Software <phrase>Reliability Engineering</phrase>): One of the main tasks in SRE is designing the operational profiles. Operational profiles are built according to the <phrase>user profile</phrase> and the understanding of the system analyst/designer. These profiles can be used for estimating the system reliability at the early phases of development. Results obtained from this analysis can be incorporated into SRE for conducting reliability analysis at the analysis/design phases, based on dynamic simulation. More research is needed to establish a method for incorporating the <phrase>risk assessment</phrase> method within the SRE process. <phrase>ACRONYMS</phrase> 1 AOI accept operator input commands BSRC …
Batch Mode Sparse <phrase>Active Learning</phrase> —Sparse representation, due to its clear and powerful insight deep into the structure of data, has seen a recent surge of interest in the classification community. Based on this, a family of reliable classification methods have been proposed. On the other hand, obtaining sufficiently labeled <phrase>training data</phrase> has long been a challenging problem, thus considerable research has been done regarding active selection of instances to be labeled. In our work, we will present a novel unified framework, i.e. BMSAL(Batch Mode Sparse <phrase>Active Learning</phrase>). Based on the existing sparse family of classifiers, we define rigorously the corresponding BMSAL family and explore their shared properties, most importantly (approximate) submodu-larity. We focus on the feasibility and reliability of the BMSAL family: The first one inspires us to optimize the algorithms and conduct experiments comparing with state-of-the-art methods; for reliability, we give error-bounded algorithms, as well as detailed logical deductions and empirical tests for applying sparse in non-linear <phrase>data sets</phrase>.
Review of "Exploiting Software: How to Break Code by Greg Hoglund and Gary McGraw" A book with a dark-gray hat on its cover and the subtitle " How to Break Code " makes a strong statement. It does not disappoint. It covers many form of exploiting software that you never dared to explore. The book approaches its problem from many security disciplines. It takes on the <phrase>reverse-engineering</phrase> angle to break <phrase>copyright protection</phrase> systems or to find software defects. It takes the pentest (<phrase>penetration test</phrase>) view when it explores how to attack server-side software, with local and remote attack options. It describes the botnet (robot network) master's options when it targets client software problems. It shows how to hide <phrase>malware</phrase> (<phrase>malicious software</phrase>) via the <phrase>rootkit</phrase> approach, diving deep—even into <phrase>flash memory</phrase>—and evading forensic analysis. The authors also present more conceptual views, such as the root cause of software security problems, 49 <phrase>attack patterns</phrase>, how to craft malicious input, and buffer overflows in all variations. Each topic includes a tutorial, sample systems or code, and known exploits using these techniques. If the topic is unfamiliar, the tutorial may be insufficient, but links to further information are provided. The sample code is clear enough to allow smarter scripters to elaborate on it. There are not many details on the known exploits, but a simple <phrase>Web search</phrase> on any of the key terms will provide all that are necessary. The book is about 450 pages, and contains eight chapters. The three longest chapters are on <phrase>reverse engineering</phrase> , <phrase>buffer overflow</phrase>, and rootkits. The others are on software, <phrase>attack patterns</phrase>, exploiting server software, and exploiting client software Who should read this book? The authors start by defending why anyone would write such a book. They show that anything they describe has been exploited already. They spell out how it was done, loud and clear. This takes away ignorance. So, if your job is to build secure software systems, to implement license or <phrase>copyright protection</phrase> systems, to pentest systems, or to do forensic analysis, you will benefit from reading the book. Ed Felten, <phrase>Princeton University</phrase> professor of computer science, is quoted on the cover: " It's hard to protect yourself if you don't know what you're up against. " But having that knowledge, after reading this book, may not improve your <phrase>peace</phrase> of mind. —A. Mariën Passin, a principal <phrase>systems engineer</phrase> specializing in <phrase>data modeling</phrase>, <phrase>Web databases</phrase>, and XML projects. Thus, it is not unexpected to find that he covers …
Extrapolation of Pile Capacity from Non-failed Load Tests Static pile load test to failure is the ultimate procedure available to examine the capacity and integrity of deep foundations. Being expensive and time consuming, the procedure is often substituted for the application of a load to a certain factor (most often two) times the contemplated design load. In fact, only a proof test is carried out while the <phrase>ultimate capacity</phrase> and actual factor of safety remain unknown. This procedure results in uneconomic foundation solution, unknown capacity when modifications are required and inability of the engineer to gain insight of the controlling mechanism for improved design. The described state of practice calls for the ability to reliably estimate the ultimate pile capacity for non-failed load tests. A practical analytical method is proposed, capable of extrapolating the measured load-<phrase>settlement</phrase> relations beyond the maximum tested load. The proposed procedure along with two other possible methods are evaluated. The procedures are examined through a data base of 63 piles load tested to failure. Loading is assumed to be known for only 25%, 33%, 50%, 75% and 100% of the actual <phrase>ultimate capacity</phrase>. The " known " data is then extrapolated using the different methods and the obtained <phrase>ultimate capacity</phrase> is compared to the actual measurements. For consistency, only one failure criterion (Davidson) is applied. The obtained results are analyzed statistically to evaluate the accuracy and reliability of the proposed methods of analysis. It is shown that the accuracy of the proposed method is 0.99 ± 0.21 (1S.D.), 0.96 ± 0.27, 0.87 ± 0.3. and 0.78 ± 0.33 when assuming 75%, 50%, 33% and 25% of the <phrase>ultimate capacity</phrase> to be known, respectively. The obtained results for the 63 data base cases suggest that even when the predicted <phrase>ultimate capacity</phrase> is four times the maximum actual tested load, the associated risk is zero for exceeding the design load, when using the extrapolated value with a factor of safety of 2.0. Case history analyses of six load-tested piles at two sites are presented. The analyzed cases indicate possible substantial savings when the <phrase>ultimate capacity</phrase> well exceeds the maximum applied load. Moreover, the method already demonstrates its enormous importance from both aspects, engineering and financial.
Closer to Deep Underwater Science with <phrase>Odyssey</phrase> Iv Class Hovering <phrase>Autonomous Underwater Vehicle</phrase> (hauv) Closer to Deep Underwater Science with <phrase>Odyssey</phrase> W Class Hovering <phrase>Autonomous Underwater Vehicle</phrase> (hauv) The <phrase>Autonomous Underwater Vehicle</phrase> Laboratory (AUVLAB) at The <phrase>Massachusetts Institute of Technology</phrase> (MIT) is currently building and testing a new, <phrase>general purpose</phrase> and inexpensive 6000 meter capable Hovering <phrase>Autonomous Underwater Vehicle</phrase> (HAUV), the '<phrase>ODYSSEY</phrase> IV class'. The vehicle is intended for rapid deployments, potentially with minimal navigation, thus supporting episodic dives for exploratory missions. For that, the vehicle is capable of fast dive times, short survey on bottom and simple navigation.
FDM: a <phrase>graph-based</phrase> statistical method to detect <phrase>differential transcription</phrase> using <phrase>RNA-seq</phrase> data MOTIVATION In <phrase>eukaryotic cells</phrase>, <phrase>alternative splicing</phrase> expands the diversity of RNA transcripts and plays an important role in tissue-specific differentiation, and can be misregulated in disease. To understand these processes, there is a great need for methods to detect <phrase>differential transcription</phrase> between samples. Our focus is on samples observed using short-read RNA sequencing (<phrase>RNA-seq</phrase>).   METHODS We characterize <phrase>differential transcription</phrase> between two samples as the difference in the relative abundance of the transcript isoforms present in the samples. The magnitude of <phrase>differential transcription</phrase> of a gene between two samples can be measured by the <phrase>square root</phrase> of the Jensen Shannon Divergence (JSD*) between the gene's transcript abundance vectors in each sample. We define a weighted splice-graph representation of <phrase>RNA-seq</phrase> data, summarizing in compact form the alignment of <phrase>RNA-seq</phrase> reads to a <phrase>reference genome</phrase>. The flow difference metric (FDM) identifies regions of differential RNA transcript expression between pairs of splice graphs, without need for an underlying gene model or catalog of transcripts. We present a novel non-parametric statistical test between splice graphs to assess the significance of <phrase>differential transcription</phrase>, and extend it to group-wise comparison incorporating sample replicates.   RESULTS Using simulated <phrase>RNA-seq</phrase> data consisting of four technical replicates of two samples with varying transcription between genes, we show that (i) the FDM is highly correlated with JSD* (r=0.82) when average <phrase>RNA-seq</phrase> coverage of the transcripts is sufficiently deep; and (ii) the FDM is able to identify 90% of genes with <phrase>differential transcription</phrase> when JSD* >0.28 and coverage >7. This represents higher sensitivity than Cufflinks (without annotations) and rDiff (MMD), which respectively identified 69 and 49% of the genes in this region as differential transcribed. Using annotations identifying the transcripts, Cufflinks was able to identify 86% of the genes in this region as differentially transcribed. Using <phrase>experimental data</phrase> consisting of four replicates each for two cancer cell lines (MCF7 and SUM102), FDM identified 1425 genes as significantly different in transcription. Subsequent study of the samples using quantitative real time <phrase>polymerase chain reaction</phrase> (qRT-<phrase>PCR</phrase>) of several <phrase>differential transcription</phrase> sites identified by FDM, confirmed significant differences at these sites.   AVAILABILITY http://csbio-linux001.cs.unc.edu/nextgen/software/FDM  CONTACT:  darshan@email.unc.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at <phrase>Bioinformatics</phrase> online.
<phrase>High Frequency</phrase> Power Transformer Measurement and Modeling Transformer Design Example <phrase>Figure 1 shows</phrase> a simple 1:1 transformer. The transformer uses an ungapped EPC-25 core from <phrase>TDK</phrase>, made from PC-44 material. This transformer was designed for use in a 60 W forward converter with 36-60 V input and 12 V output. Figure 2 shows the winding layout, with just a <phrase>single layer</phrase> of 18 turns for the primary winding, a layer of thin insulation tape, and a <phrase>single layer</phrase> of 18 turns for the secondary winding. This is a very straightforward, easy-to-manufacture design of a two-winding transformer. However, as you will see below, the resulting circuit element created is anything but simple. Transformer Model Figure 3 shows a commonly-used model for a two-winding transformer. On the primary side, the winding resistance is represented by R p , the leakage inductance by L lk , magnetizing inductance by L m , core loss by R c , and self-capacitance by C p. The secondary winding resistance is R s , the secondary self-capacitance is C s , and the primary to secondary capaci-tance is C m. The elements of this transformer model are used for several purposes-characterizing components, identifying problem design areas, and <phrase>circuit simulation</phrase>. However, this apparently simple model is complicated by the fact that all of the <phrase>resistors</phrase> and <phrase>inductors</phrase> of the model are nonlinear functions of either frequency, or excitation level, or both. The capacitors can also exhibit minor nonlinearities, but are further complicated by the fact that the lumped elements <phrase>shown in Fig</phrase>. 3 are a very crude approximation to the true multiple interwinding capacitance effects that really exist in the component. Transformer <phrase>Frequency Response</phrase> Measurements It is very useful to make <phrase>frequency response</phrase> measurements on a <phrase>high-frequency</phrase> power transformer, using a wide range of frequencies. For a two-winding transformer such as the example above, the most common measurements are impedance measurements from the primary side, with the secondary both open-circuited, and short-circuited. Fig. 4 shows a typical test setup for impedance measurements, using the AP200 <phrase>frequency response</phrase> analyzer. More detail of this test can be found in [1]. While many manufacturers only Despite efforts from some magnetics vendors to provide off-the-shelf components to <phrase>power supply</phrase> designers, almost all <phrase>high-performance</phrase> magnetics are custom. There are many deep and complex issues involved in the design of magnetics. I will try to cast some light on just a few of these issues. This article points out some …
Revisiting UCS: Description, <phrase>Fitness Sharing</phrase>, and Comparison with <phrase>XCS</phrase> This paper provides a deep insight into the learning mechanisms of UCS, a <phrase>learning classifier system</phrase> (<phrase>LCS</phrase>) derived from <phrase>XCS</phrase> that works under a <phrase>supervised learning</phrase> scheme. A complete description of the system is given with the aim of being useful as an implementation guide. Besides, we review the fitness computation, based on the individual accuracy of each rule, and introduce a <phrase>fitness sharing</phrase> scheme to UCS. We analyze the dynamics of UCS both with <phrase>fitness sharing</phrase> and without <phrase>fitness sharing</phrase> over five binary-input problems widely used in the LCSs framework. Also <phrase>XCS</phrase> is included in the comparison to analyze the differences in behavior between both systems. Results show the benefits of <phrase>fitness sharing</phrase> in all the tested problems, specially those with class imbalances. Comparison with <phrase>XCS</phrase> highlights the dynamics differences between both systems.
Center for Reliable Computing Elf35 Experiment -chip and Experiment Design Elf35 Experiment -chip and Experiment Design A test chip has been designed and manufactured to evaluate the effectiveness of different test techniques for <phrase>deep submicron technologies</phrase>. The <phrase>test chip</phrase> uses <phrase>LSI</phrase> Logic G10p 0.35µm cell-based technology. It has approximately 265k <phrase>LSI</phrase> Logic equivalent gates. There are six types of circuits-under-test(CUTs). Two CUTs are arithmetic processors, which perform the same function but were implemented in different ways. These two CUTs are full-scan <phrase>sequential circuits</phrase>. The other four are combinational circuits. Three of these four combinational CUTs are datapath circuits. The other one is a <phrase>translator</phrase> that maps a <phrase>pseudo-random</phrase> sequence into a binary sequence. The tests include <phrase>stuck-at fault</phrase>, <phrase>delay fault</phrase>, <phrase>transition fault</phrase>, <phrase>design verification</phrase>, <phrase>pseudo-random</phrase>, weighted-random, and <phrase>I DDQ</phrase> tests. The I DDQ <phrase>test sets</phrase> were generated based on various <phrase>fault models</phrase>, such as pseudo-stuck-at and <phrase>bridging faults</phrase>. A built-in-self-test (BIST) circuitry was implemented for one datapath CUT. Emulated BIST <phrase>test sets</phrase> for some CUTs will also be used. This report describes the chip design and the test applied. Future reports will describe the experimental results and <phrase>data analysis</phrase>. ABSTRACT A test chip has been designed and manufactured to evaluate the effectiveness of different test techniques for <phrase>deep submicron technologies</phrase>. The <phrase>test chip</phrase> uses <phrase>LSI</phrase> Logic G10p 0.35µm cell-based technology. It has approximately 265k <phrase>LSI</phrase> Logic equivalent gates. There are six types of circuits-under-test (CUTs). Two CUTs are arithmetic processors, which perform the same function but were implemented in different ways. These two CUTs are full-scan <phrase>sequential circuits</phrase>. The other four are combinational circuits. Three of these four combinational CUTs are datapath circuits. The other one is a <phrase>translator</phrase> that maps a <phrase>pseudo-random</phrase> sequence into a binary sequence. The tests include <phrase>stuck-at fault</phrase>, <phrase>delay fault</phrase>, <phrase>transition fault</phrase>, <phrase>design verification</phrase>, <phrase>pseudo-random</phrase>, weighted-random, and <phrase>I DDQ</phrase> tests. A built-in-self-test (BIST) circuitry was implemented for one datapath CUT. Emulated BIST patterns for some CUTs will also be used. This report describes the chip design and the test applied. Future reports will describe the experimental results and <phrase>data analysis</phrase>.
Building Deep Dependency Structures using a Wide-Coverage <phrase>CCG</phrase> Parser This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (<phrase>CCG</phrase>) to derive dependency structures. The parser differs from most existing wide-coverage tree-bank parsers in capturing the <phrase>long-range</phrase> dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local <phrase>predicate-argument</phrase> dependencies. A set of dependency structures used for <phrase>training and testing</phrase> the parser is obtained from a treebank of <phrase>CCG</phrase> normal-form derivations , which have been derived (semi-) automatically from the <phrase>Penn</phrase> Treebank. The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies.
Current-based testing for <phrase>deep-submicron</phrase> VLSIs - Design & Test of Computers, IEEE recently published articles have questioned the ability to carry out effective current-based testing (I DDX) for deep-micron VLSIs. 1-5 Yet, current-based <phrase>test methods</phrase> for such devices are more relevant than ever. The probability of a defect occurring increases exponentially as its size decreases. As the technology scales, even smaller defects may become potential threats to yield. Furthermore, ensuring gate oxide quality and reliability for a multimillion-transistor device under test (DUT) solely through voltage may become unrealistic. Other techniques, such as burn-in, although particularly successful for memories, might not be economically viable for most commercial digital products. Several <phrase>recent studies</phrase> have raised concerns about new <phrase>failure mechanisms</phrase> in scaled geometries that may be more difficult to detect with conventional means. Nigh et al. reported the existence of many timing-only failures. These failures did not influence the circuit logic functionality; hence, slow-speed SA-based (<phrase>stuck-at fault</phrase>) or functional tests did not detect them. 6 Similarly, for Intel's manufacturing processes, Needham et al. reported an increasing shift toward " soft defects " as technology migrated from 0.35 to 0.25 microns. 7 These defects do not always cause failures at all conditions of temperature and voltage. According to Needham et al., defects correlate with <phrase>long-term</phrase> device reliability. These defects may be due to resistive vias, highly <phrase>resistive bridging</phrase> defects, and so on. I DDQ testing can detect some defects, provided background leakages are under control and circuits are designed to make them <phrase>I DDQ</phrase> testable. Traditionally, voltage testing and I DDQ testing have had complementary objectives. In logic testing, the stress is on DUT logic correctness, <phrase>performance evaluation</phrase>, and detection of catastrophic faults such as <phrase>stuck-at faults</phrase>. In I DDQ testing, on the other hand, the focus is on detecting subtle <phrase>manufacturing-process</phrase> defects and reliability failures. As the technology scales, the roles for these two types of testing will diverge further. Therefore, effective deep-micron current-based testing can play an important role not only in ensuring VLSI quality and reliability but also in arresting the already escalated costs of VLSI testing. The simplified MOS theory assumes a zero drain current for V GS < V T. In fact, drain current (I DS) does not drop abruptly, but decreases exponentially, similar to a bipolar transistor's operation. The <phrase>leakage current</phrase> stems from minority carriers and diffusion currents in the noninverted MOS transistor. In the subthresh-old region, the inverse rate of decrease of I DS in volts per decade, …
Behaviour & <phrase>Information Technology</phrase> <phrase>Information Structure</phrase> and Practice as Facilitators of <phrase>Deaf Users</phrase>' Navigation in Textual Websites <phrase>Information Structure</phrase> and Practice as Facilitators of <phrase>Deaf Users</phrase>' Navigation in Textual Websites Taylor & Francis makes every effort to ensure the accuracy of all the information (the " Content ") contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content. <phrase>Deaf users</phrase> might find it difficult to navigate through websites with textual content which, for many of them, constitutes the written representation of a non-native oral language. With the aim of testing how the <phrase>information structure</phrase> could compensate for this difficulty, 27 prelingual <phrase>deaf users</phrase> of <phrase>sign language</phrase> were asked to search a set of headlines in a web <phrase>newspaper</phrase> where <phrase>information structure</phrase> and practice were manipulated. While practice did not affect <phrase>deep structures</phrase> (<phrase>web content</phrase> distributed through four layers of nodes), wide structures (<phrase>web content</phrase> concentrated in two layers) did facilitate users' performance in the last trial block and compromised it in the first trial block. It is argued that wide structures generate a textual <phrase>information overload</phrase> for <phrase>deaf users</phrase>, which decreases with practice. Thus, wide structures seem preferable for websites requiring frequent use, rather than for those intended for occasional interaction. 1. Introduction International organisations such as the <phrase>International Organization for Standardization</phrase> (<phrase>ISO</phrase> 9241–171) and the <phrase>Web Accessibility</phrase> <phrase>Initiative</phrase> (WAI 1999) offer <phrase>web accessibility</phrase> guidelines aimed at improving the use of the Internet by disabled people. However, accessibility guidelines often lack empirical validation (Ivory and <phrase>Hearst</phrase> 2001) and tend to focus on the physical and sensorial features of their deficiencies (e.g. Seeman 2002), not considering other cognitive, linguistic and cultural aspects. This situation is particularly true for the guidelines developed to improve web use in prelingual <phrase>deaf users</phrase>, generally treated as hearing users who cannot hear which can lead to simplistic and often erroneous <phrase>web design</phrase> solutions like providing text or graphic equivalents to …
Investigation of Speech Separation as a <phrase>Front-End</phrase> for Noise Robust <phrase>Speech Recognition</phrase> Recently, supervised classification has been shown to work well for the task of speech separation. We perform an in-depth evaluation of such techniques as a <phrase>front-end</phrase> for noise-robust <phrase>automatic speech recognition</phrase> (ASR). The proposed separation <phrase>front-end</phrase> consists of two stages. The first stage removes additive noise via time-frequency masking. The second stage addresses channel mismatch and the distortions introduced by the first stage; a non-<phrase>linear function</phrase> is learned that maps the masked spectral features to their clean counterpart. Results show that the proposed <phrase>front-end</phrase> substantially improves ASR performance when the <phrase>acoustic models</phrase> are trained in clean conditions. We also propose a diagonal feature <phrase>discriminant</phrase> <phrase>linear regression</phrase> (dFDLR) adaptation that can be performed on a per-utterance basis for ASR systems employing <phrase>deep neural networks</phrase> and HMM. Results show that dFDLR consistently improves performance in all test conditions. Surprisingly, the best average results are obtained when dFDLR is applied to models trained using noisy log-Mel spectral features from the multi-condition <phrase>training set</phrase>. With no channel mismatch, the best results are obtained when the proposed speech separation <phrase>front-end</phrase> is used along with multi-condition training using log-Mel features followed by dFDLR adaptation. Both these results are among the best on the <phrase>Aurora</phrase>-4 dataset.
Probabilistic analysis of activation volumes generated during <phrase>deep brain stimulation</phrase> <phrase>Deep brain stimulation</phrase> (DBS) is an established therapy for the treatment of <phrase>Parkinson's disease</phrase> (PD) and shows <phrase>great promise</phrase> for the treatment of several other disorders. However, while the clinical analysis of DBS has received great attention, a relative paucity of quantitative techniques exists to define the optimal surgical target and most effective stimulation protocol for a given disorder. In this study we describe a methodology that represents an evolutionary addition to the concept of a probabilistic <phrase>brain atlas</phrase>, which we call a probabilistic stimulation atlas (PSA). We outline steps to combine quantitative clinical outcome measures with advanced computational models of DBS to identify regions where stimulation-induced activation could provide the best therapeutic improvement on a per-symptom basis. While this methodology is relevant to any form of DBS, we present example results from <phrase>subthalamic nucleus</phrase> (STN) DBS for PD. We constructed patient-specific computer models of the volume of tissue activated (<phrase>VTA</phrase>) for 163 different stimulation parameter settings which were tested in six patients. We then assigned clinical outcome scores to each <phrase>VTA</phrase> and compiled all of the VTAs into a PSA to identify stimulation-induced activation targets that maximized therapeutic response with minimal side effects. The results suggest that selection of both electrode placement and clinical stimulation parameter settings could be tailored to the patient's primary symptoms using patient-specific models and PSAs.
FiVaTech: Page-Level <phrase>Web Data</phrase> Extraction from Template Pages —Web <phrase>data extraction</phrase> has been an important part for many Web <phrase>data analysis</phrase> applications. In this paper, we formulate the <phrase>data extraction</phrase> problem as the decoding process of page generation based on <phrase>structured data</phrase> and tree templates. We propose an unsupervised, page-level <phrase>data extraction</phrase> approach to deduce the schema and templates for each individual Deep Website, which contains either singleton or multiple <phrase>data records</phrase> in one Webpage. FiVaTech applies tree matching, tree alignment, and <phrase>mining techniques</phrase> to achieve the challenging task. In experiments, FiVaTech has much higher precision than EXALG and is comparable with other record-level extraction systems like ViPER and MSE. The experiments show an encouraging result for the test pages used in many state-of-the-art Web <phrase>data extraction</phrase> works.
Speech-to-speech Translation: the Project Verbmobil 1 Summary The <phrase>research project</phrase> Verbmobil combines the research branches of <phrase>continuous speech recognition</phrase> and <phrase>machine translation</phrase>. A <phrase>consortium</phrase> of more than 20 <phrase>universities</phrase> and 8 companies in <phrase>Germany</phrase> as well as two partner institutes in <phrase>USA</phrase> and Japan is enganged in the development of a system for translation of spontaneous and <phrase>continuous speech</phrase> input to speech output. In the rst phase the language pair German and Japanese has been chosen with English as a presentation language (to check the translation quality). The domain of discourse is appointment scheduling. The nal application may be an appointment <phrase>negotiation</phrase> among two industrial managers from Japan and <phrase>Germany</phrase>. In the second project phase the domain will be extended to include travel planning discussions. There exists a prototype implementation integrating the modules of the partners. Additional background research has been carried out to investigate prablems e.g. of dialogue and discourse modelling , stochastic parsing and innovative <phrase>software architectures</phrase>. This paper gives an overview of the project, its goals and results. It describes in more details research done on integrative architectures. 2 Aims of the project Verbmobil combines <phrase>research activities</phrase> in <phrase>continuous speech recognition</phrase> and in <phrase>machine translation</phrase>. It tries to develop a speech-to-speech translation prototype as a feasibility study. The methods to achieve this goal vary among the partners and result in rather heterogeneous modules. Especially in syntax, rule <phrase>based approaches</phrase> (mostly \deep" analysis) vs. stochastic approaches (mostly \shal-low" understanding components) are tested and reened in parallel. This fact reeects not only the current situation in <phrase>computational linguistics</phrase> but may also stem from diierent cognitive theories about holistic understanding. To handle such heterogeneous components a lot of research has been done on innovative architectures The technical aim at the end of the rst project phase is: 70 % approximately correct translation (<phrase>end-to-end</phrase>) on unknown examples within the domain of appointment scheduling and within the lexical set of the given 2285 words.
A Low-Power, Process-and- Temperature- Compensated <phrase>Ring Oscillator</phrase> With Addition-Based <phrase>Current Source</phrase> —The design of a 1.8 GHz 3-stage current-starved <phrase>ring oscillator</phrase> with a process-and temperature-compensated <phrase>current source</phrase> is presented. Without post-fabrication calibration or off-chip components, the proposed low variation circuit is able to achieve a 65.1% reduction in the normalized standard deviation of its center frequency at <phrase>room temperature</phrase> and 85 ppm/ C temperature stability with no penalty in the oscillation frequency, the <phrase>phase noise</phrase> or the start-up time. Analysis on the impact of transistor scaling indicates that the same circuit topology can be applied to improve variability as feature size scales beyond the current <phrase>deep submicron technology</phrase>. Measurements taken on 167 test chips from two different lots fabricated in a standard 90 nm <phrase>CMOS process</phrase> show a 3x improvement in frequency variation compared to the baseline case of a conventional current-starved <phrase>ring oscillator</phrase>. The power and area for the proposed circuitry is 87 W and 0.013 mm 2 compared to 54 W and 0.01 mm 2 in the baseline case.
Discrimination ability of individual measures used in <phrase>sleep stages</phrase> classification OBJECTIVE The paper goes through the basic knowledge about classification of <phrase>sleep stages</phrase> from polysomnographic recordings. The next goal was to review and compare a large number of measures to find the suitable candidates for the study of sleep onset and sleep evolution.   METHODS AND MATERIAL A huge number of characteristics, including relevant simple measures in time domain, characteristics of distribution, linear spectral measures, measures of complexity and interdependency measures were computed for polysomnographic recordings of 20 healthy subjects. Summarily, all-night evolutions of 818 measures (73 characteristics for various channels and channel combinations) were analysed and compared with visual scorings of experts (hypnograms). Our tests involved classification of the data into five classes (waking and four <phrase>sleep stages</phrase>) and 10 classification tasks to distinguish between two specific <phrase>sleep stages</phrase>. To discover measures of the best <phrase>decision-making</phrase> ability, <phrase>discriminant analysis</phrase> was done by <phrase>Fisher</phrase> quadratic classifier for one-dimensional case.   RESULTS AND CONCLUSIONS The most difficult <phrase>decision problem</phrase>, between S1 and <phrase>REM sleep</phrase>, were best managed by measures computed from electromyogram led by <phrase>fractal</phrase> exponent (classification error 23%). In the simplest task, distinction between wake and <phrase>deep sleep</phrase>, the power ratio between delta and beta band of electroencephalogram was the most successful measure (classification error 1%). Delta/beta ratio with mean classification error 42.6% was the best single-performing measure also in discrimination between all five stages. However, the error level shows impossibility to satisfactorily separate the five <phrase>sleep stages</phrase> by a single measure. Use of a few additional characteristics is necessary. Some novel measures, especially <phrase>fractal</phrase> exponent and <phrase>fractal dimension</phrase> turned up equally successful or even superior to the conventional scoring methods in discrimination between particular states of sleep. They seem to provide a very promising basis for automatic sleep analysis particularly in conjunction with some of the successful spectral standards.
<phrase>Computational Intelligence</phrase> and AI in Games: A New IEEE Transactions Games have long been seen as an ideal <phrase>test-bed</phrase> for the study of AI. Until recently, most of the academic work in the area focused on traditional <phrase>board games</phrase> and card games, the challenge being to beat expert human players. Following the release of <phrase>Pong</phrase> in the early 1970s, the last few decades have seen a phenomenal increase in the quality, diversity and pervasiveness of <phrase>video games</phrase>. The value of the worldwide computer and <phrase>video games</phrase> market is estimated to be $USD25bn annually 1 , and is predicted to grow rapidly over the next decade. This presents academic researchers [1] and game developers with the challenge of developing <phrase>next generation</phrase> game AI. The future will see games with intelligent empathetic characters who get to know you, and work hard to optimise your experience while playing the game. New titles such as <phrase>Left 4 Dead 2</phrase> have already made important steps in this direction. Superior game AI will lead to more entertaining and immersive games and also add value to the burgeoning serious games market. Traditional games are constrained by the ability of the players to manipulate the game objects such as pieces on a <phrase>chessboard</phrase> or <phrase>cards</phrase> from a deck. The rules of a game specify how the game objects interact and fundamental combinatorics quickly produce enormous game trees; this ensures that complex <phrase>decision-making</phrase> processes are required in order to play such games to a high standard. Implementing computer programs that are able to defeat their human opponents has been the subject of a great deal of AI research, epitomised by outstandingly successful programs such as <phrase>Deep Blue</phrase> for Chess [2] and <phrase>Chinook</phrase> for Checkers [3],[4]. These programs expend most of their effort on <phrase>game-tree</phrase> search, and for this reason are often called brute force approaches. This term although widely used [5] is somewhat inaccurate: there is a great deal of subtlety in the way these algorithms are implemented and fine-tuned. The levels of play achieved demonstrate extraordinary prowess in computer science and engineering for the authors of those systems. On a broader note, they force us to reconsider the nature of intelligence. <phrase>Game-tree</phrase> search has limited applicability when the game has a large branching factor or the state-space is continuous. Hence the techniques applied to chess and checkers do not work well for Go, and are even less applicable to <phrase>video games</phrase> where the state-space is for practical purposes …
Using graphical representation of <phrase>user interfaces</phrase> as visual references Many <phrase>user interfaces</phrase> use indirect references to identify specific objects and devices. My thesis investigates using graphical representations of <phrase>user interfaces</phrase> (i.e. screenshots) as direct visual references to support various kinds of applications. Sikuli Script enables users to programmatically control GUIs without the support from the underlying applications. Sikuli Test lets GUI developers and testers create test scripts without coding. Deep <phrase>Shot</phrase> introduces a framework and interaction techniques to migrate work states across heterogeneous devices in one action, taking a picture. In addition to these pure pixel-based systems, PAX associates the pixel representation with the internal structures and metadata of the <phrase>user interface</phrase>. Based on these building blocks, we propose to develop a visual history system that enables users to search and browse what they have seen on their computer screens. We outline some interesting use cases and discuss the challenges in this ongoing work.
Hybrid <phrase>Concolic Testing</phrase> We present hybrid <phrase>concolic testing</phrase>, an algorithm that interleaves <phrase>random testing</phrase> with concolic execution to obtain both a deep and a wide exploration of program <phrase>state space</phrase>. Our algorithm generates test inputs automatically by interleaving <phrase>random testing</phrase> until saturation with bounded exhaustive symbolic exploration of program points. It thus combines the ability of random search to reach deep program states quickly together with the ability of <phrase>concolic testing</phrase> to explore states in a <phrase>neighborhood</phrase> exhaustively. We have implemented our algorithm on top of CUTE and applied it to obtain better <phrase>branch coverage</phrase> for an <phrase>editor</phrase> implementation (VIM 5.7, 150K lines of code) as well as a <phrase>data structure</phrase> implementation in C. Our experiments suggest that hybrid <phrase>concolic testing</phrase> can handle large programs and provide, for the same testing budget, almost 4× the <phrase>branch coverage</phrase> than <phrase>random testing</phrase> and almost 2× that of <phrase>concolic testing</phrase>.
Regularizing <phrase>Recurrent Networks</phrase> - On Injected Noise and Norm-<phrase>based Methods</phrase> Advancements in <phrase>parallel processing</phrase> have lead to a surge in <phrase>multilayer perceptrons</phrase>' (MLP) applications and <phrase>deep learning</phrase> in the past decades. <phrase>Recurrent Neural Networks</phrase> (RNNs) give additional representa-tional power to feedforward MLPs by providing a way to treat sequential data. However, RNNs are hard to train using conventional error backpropagation methods because of the difficulty in relating inputs over many time-steps. Regularization approaches from MLP sphere, like dropout and noisy <phrase>weight training</phrase>, have been insufficiently applied and tested on simple RNNs. Moreover, solutions have been proposed to improve convergence in RNNs but not enough to improve the <phrase>long term</phrase> dependency remembering capabilities thereof. In this study, we aim to empirically evaluate the remembering and <phrase>generalization ability</phrase> of RNNs on <phrase>polyphonic</phrase> musical datasets. The models are trained with injected noise, random dropout, norm-based regularizers and their respective performances compared to well-initialized plain RNNs and advanced regularization methods like fast-dropout. We conclude with evidence that training with noise does not improve performance as conjectured by a few works in RNN optimization before ours.
Site-Wide <phrase>Wrapper Induction</phrase> for Life Science Deep <phrase>Web Databases</phrase> We present a novel approach to automatic <phrase>information extraction</phrase> from <phrase>Deep Web</phrase> Life Science databases using <phrase>wrapper induction</phrase>. Traditional <phrase>wrapper induction</phrase> techniques focus on learning wrappers based on examples from one class of <phrase>Web pages</phrase>, i.e. from <phrase>Web pages</phrase> that are all similar in structure and content. Thereby, traditional <phrase>wrapper induction</phrase> targets the understanding of <phrase>Web pages</phrase> generated from a database using the same generation template as observed in the example set. However, Life Science <phrase>Web sites</phrase> typically contain structurally diverse <phrase>web pages</phrase> from multiple classes making the problem more challenging. Furthermore, we observed that such Life Science <phrase>Web sites</phrase> do not just provide mere data, but they also tend to provide schema information in terms of data labels – giving further cues for solving the <phrase>Web site</phrase> wrapping task. Our solution to this novel challenge of Site-Wide <phrase>wrapper induction</phrase> consists of a sequence of steps: 1. classification of similar <phrase>Web pages</phrase> into classes, 2. discovery of these classes and 3. <phrase>wrapper induction</phrase> for each class. Our approach thus allows us to perform unsupervised <phrase>information retrieval</phrase> from across an entire <phrase>Web site</phrase>. We test our algorithm against three <phrase>real-world</phrase> biochemical deep <phrase>Web sources</phrase> and report our <phrase>preliminary results</phrase>, which are very promising.
Relation <phrase>Information Extraction</phrase> Using Deep <phrase>Syntactic Analysis</phrase> There has been an increasing need for <phrase>natural language processing</phrase> technology to <phrase>Information Extraction</phrase> (IE), such as relations between entities, which are more informative than mere documents searched by key words. This dissertation proposes a novel method to construct and utilize <phrase>extraction patterns</phrase> for relation extraction based on <phrase>deep syntactic</phrase> relations obtained by full parsing. The process which requires the most amount of manual work in construction of IE systems is construction of <phrase>extraction patterns</phrase> which extract target information from source texts, because the same information can be represented through many kinds of syntactic variations. To reduce this amount of manual work, our approach has two phases: First, we raise representation ability of <phrase>extraction patterns</phrase> and reduce number of necessary patterns by normalizing syntactic variations into <phrase>predicate-argument</phrase> structures (PASs) using a full parser based on <phrase>Head-driven Phrase Structure Grammar</phrase> (HPSG). Then, PASs which connect entities to extract in a small training corpus are considered as <phrase>extraction patterns</phrase>, and we divide them into components and utilize combinations of the components for generalization. As a real world application, we have constructed an IE system for <phrase>protein-protein interactions</phrase>, which are important knowledge in biomedical research. We evaluated the IE system on a small <phrase>test-case</phrase> corpus and a large <phrase>real-world</phrase> corpus, and show its effectiveness. This dissertation also describes aspects that should be considered to ensure effectiveness of full parsers on <phrase>domain-specific</phrase> IE. The first aspect is the ability of <phrase>deep syntactic</phrase> relations obtained by parsing to capture <phrase>syntactic information</phrase>, which is necessary for constructing <phrase>extraction patterns</phrase>. To show enough accuracy of full parsing on a biomed-ical text, we evaluated precision of primitive PASs obtained from a biomedical text by an HPSG parser. And to compare performance of PAS patterns to patterns of part-of-speeches, we also evaluated performance of verb-argument relations obtained from a biomedical text by PAS patterns and by patterns of part-of-speeches. The second aspect is difficulties to apply <phrase>general-purpose</phrase> parsers to <phrase>domain-specific</phrase> domains. To measure <phrase>domain-specific</phrase> coverage of a <phrase>general-purpose</phrase> HPSG, we investigated deficiencies of the grammar on parsing a biomedical text. We also show preliminary investigation on performance of <phrase>general-purpose</phrase> parsers that suggested parsing accuracy on general corpus does not ensure parsing accuracy or IE accuracy on a <phrase>domain-specific</phrase> text. Through all results on this dissertation, we show that full parsing is effective for IE. To obtain more performance of a <phrase>domain-specific</phrase> IE with full parsing, we should use shallow …
Modeling the game of Arimaa with Linguistic Geometry — A computer defeated a chess world champion for the first time in 1997. This event inspired Omar Syed to develop the game of Arimaa. He intended to make it difficult to solve under present search approaches. Linguistic Geometry is a technique that offers a formal method based on the expertise of human chess masters, to make the development of complex heuristics easier. This article introduces a Linguistic Geometry based model for the game of Arimaa. It gives implementation for each of the essential components of Linguistic Geometry: trajectories, zones, translations and searches. A <phrase>test case</phrase> is given and it is used as input for a software implementation of the proposed model. The results given by the software are compared against the analysis made by a human player. I. INTRODUCTION To study games, they can be represented through large trees. These trees include every possible evolution of the game. Searching through the game's tree for a solution of the game is the current approach in computer sciences. To reduce the execution time of game <phrase>search algorithms</phrase>, heuristics are used during the analysis of the game's tree. A heuristic is an estimation mechanism or a rule of thumb. Heuristics were first proposed by <phrase>Claude Shannon</phrase> [7]. The most popular <phrase>search algorithms</phrase> through general trees are depth-first, breath first and best first, while for search in games trees are <phrase>minimax</phrase> and <phrase>alpha-beta</phrase> [4]. Games are an interesting area of study because of their complexity, it is believed that techniques used for solving some games can also be used to solve other kind of problems. In particular, the study of chess has excelled. A new game called Arimaa was created by Omar and Aamir Syed [10]. They were inspired by the defeat of <phrase>Garry Kasparov</phrase> against the <phrase>supercomputer</phrase> <phrase>Deep Blue</phrase>. The game of Arimaa is a two player complete information, <phrase>zero-sum game</phrase> with no random factors. Arimaa was released in 2002 and it was designed to be complex to play well under traditional game <phrase>search algorithms</phrase>. This was done with the purpose of fomenting the development of new, ground breaking techniques. A challenge was published along the rules of the game, it consists of a prize of $10,000 <phrase>USD</phrase> for anyone who creates a computer program capable of defeating a human expert in a competition of six games. Many computer programs have participated in the Ari-maa challenge. The vast majority of them are …
Explaining across contrasting cases for <phrase>deep understanding</phrase> in science: an example using interactive simulations <phrase>Undergraduate students</phrase> used a simulation to learn about electromagnetic flux. They were provided with three simulated cases that illustrate how changes in flux induce current in a coil. In the POE condition, students predicted, observed, and explained the outcome of each case, treating each case separately. In the <phrase>GE</phrase> condition, students were asked to produce a general explanation that would work for all three cases. A second factor crossed whether students had access to a numerical measurement tool. Effects of the measurement tool were less conclusive, but there was a strong effect of instructional method. Compared to POE students, <phrase>GE</phrase> students were better able to induce an underlying principle of electromagnetic flux during instruction and were better able to apply this principle to novel problems at <phrase>post-test</phrase>. Moreover, prior achievement predicted learning in the POE group, while students of all academic levels benefited equally from the <phrase>GE</phrase> condition. <phrase>Science education</phrase> has learning goals that range from basic lab skills to beliefs about the sources of <phrase>scientific knowledge</phrase>. One enduring goal is for students to develop a deep understanding of phenomena so they can engage in the structure of scientific explanation. One way to characterize <phrase>deep understanding</phrase> is the capability and disposition to perceive and explain natural phenomena in terms of general principles. In this study, we show that <phrase>deep understanding</phrase> can depend critically on the way in which multiple instances of phenomena are presented to students and how students are instructed to explain those instances. The research is done in the context of undergraduate physics students learning about <phrase>magnetic flux</phrase> with a computer simulation. It is common in science instruction to ask students to solve or conceptually explain a series of problems. One version of this approach is the Predict-Observe-Explain (POE) cycle (White & Gunstone, 1992). Students receive the setup of an experiment and predict what will happen. They then observe the outcome and develop an explanation for why their prediction did or did not match the expected outcome. For POE and other sequenced formats, a series of questions or examples is carefully selected to help students instantiate a given core principle in multiple contexts, so that they develop a deeper, more abstract sense of the principle and learn the kinds of situations to which it applies. Formats such as POE are considered to be effective in part because they foster deep and often extended engagement with each new question …
Evidence for a motor and a non-motor domain in the human <phrase>dentate nucleus</phrase> - An fMRI study Dum and Strick (J. Neurophysiol. 2003; 89, 634-639) proposed a <phrase>division</phrase> of the cerebellar <phrase>dentate nucleus</phrase> into a "motor" and "non-motor" area based on anatomical data in the <phrase>monkey</phrase>. We asked the question whether motor and non-motor domains of the dentate can be found in humans using <phrase>functional magnetic resonance imaging</phrase> (fMRI). Therefore dentate activation was compared in motor and cognitive tasks. Young, healthy participants were tested in a 1.5 T MRI scanner. Data from 13 participants were included in the <phrase>final analysis</phrase>. A <phrase>block design</phrase> was used for the experimental conditions. Finger tapping of different complexities served as motor tasks, while cognitive testing included a verbal <phrase>working memory</phrase> and a visuospatial task. To further confirm motor-related dentate activation, a simple finger movement task was tested in a supplementary experiment using ultra-highfield (7 T) fMRI in 23 participants. For <phrase>image processing</phrase>, a <phrase>recently developed</phrase> region of interest (ROI) driven normalization method of the deep cerebellar nuclei was used. Dorso-<phrase>rostral</phrase> <phrase>dentate nucleus</phrase> activation was associated with motor function, whereas cognitive tasks led to prominent activation of the caudal nucleus. The visuospatial task evoked activity bilaterally in the caudal <phrase>dentate nucleus</phrase>, whereas verbal <phrase>working memory</phrase> led to activation predominantly in the right caudal dentate. These findings are consistent with Dum and Strick's anatomical findings in the <phrase>monkey</phrase>.
Multimode scan: Test per clock BIST for <phrase>IP cores</phrase> Built-in self-test (BIST) is an attractive <phrase>design-for-test</phrase> methodology for core-based <phrase>SoC design</phrase> because of the minimal need for test access when tests are generated and evaluated within the core itself. However, the <phrase>scan based</phrase> <phrase>logic BIST</phrase> approach being widely considered for this application suffers from two significant weaknesses: slow test-per-scan execution, and a limited capability for detecting realistic timing and <phrase>delay faults</phrase>, critical in deep submicron technologies. The new multimode <phrase>scan based</phrase> approach presented here supports test-per-clock BIST, which runs <phrase>orders of magnitude</phrase> faster, and also provides significantly better delay <phrase>fault coverage</phrase>.
Gates foots a <phrase>malaria</phrase> bill genuine search for the public view, but as a way of getting greater acceptance, " he wrote. " Now they are hoist with their own petard. The people who registered their views were more hostile than the public <phrase>at large</phrase>. " The figures were drawn from over 600 public meetings throughout the <phrase>country</phrase> — which, as The Independent pointed out, were not designed to achieve a cross-sample of the population <phrase>at large</phrase>. " Indeed, there is a case for saying that there is a 'self-selecting' effect at work in such a consultation exercise, because those most likely to turn up to such a meeting might be expected to be those most passionately opposed to GM products. " The Times insisted that the results had particular force because they did not come solely from meetings packed with opponents. " The findings were confirmed by 77 people who were selected randomly as representative of the general public, a grouping named 'Narrow-But-Deep'. The panel was less dogmatic in its opposition to GM but wished the government to delay a decision until there were more tests. " A third surprise was the response of The <phrase>Guardian</phrase>, which in 1999 highlighted Arpad Pusztai's claims about alleged dangers of GM crops, and whose <phrase>editor</phrase> co-authored the anti-GM <phrase>television</phrase> <phrase>drama</phrase> Fields of Gold last year. Though it carried two articles on the survey results, The <phrase>Guardian</phrase> devoted as much space to a piece warning that " public antipathy toward GM crops is driving Britain's leading plant scientists to seek greener pastures abroad ". Prominent among researchers quoted was Richard Flavell, formerly of the <phrase>John Innes Centre</phrase> in <phrase>Norwich</phrase> and now with Ceres in California. " The situation is more disturbing in the <phrase>UK</phrase> than anywhere else in the world, " Flavell was quoted as saying. " The untruths, lies and lack of orchestrated information make it impossible for the average person to make an informed decision. " So " GM Nation? " (who on earth decided to call it that?) did not spawn a uniform, hysterical chorus from the media. True, most journalists and editors decided to amplify its negative verdicts. But a significant minority urged caution. Of these, The Times struck arguably the most appropriate note. Citing both " scaremongering about health effects " and " genuine scientific uncertainty about environmental effects " , it concluded that the <phrase>British</phrase> people " do not want to close …
<phrase>Comparative study</phrase> of <phrase>gene set</phrase> enrichment methods BACKGROUND The analysis of <phrase>high-throughput</phrase> <phrase>gene expression data</phrase> with respect to sets of genes rather than individual genes has many advantages. A variety of methods have been developed for assessing the enrichment of sets of genes with respect to differential expression. In this paper we provide a <phrase>comparative study</phrase> of four of these methods: <phrase>Fisher's exact test</phrase>, <phrase>Gene Set</phrase> Enrichment Analysis (GSEA), Random-Sets (RS), and Gene List Analysis with Prediction Accuracy (GLAPA). The first three methods use associative statistics, while the fourth uses predictive statistics. We first compare all four methods on simulated <phrase>data sets</phrase> to verify that <phrase>Fisher's exact test</phrase> is markedly worse than the other three approaches. We then validate the other three methods on seven real <phrase>data sets</phrase> with known genetic perturbations and then compare the methods on two cancer <phrase>data sets</phrase> where our a priori knowledge is limited.   RESULTS The simulation study highlights that none of the three method outperforms all others consistently. GSEA and RS are able to detect weak signals of <phrase>deregulation</phrase> and they perform differently when genes in a <phrase>gene set</phrase> are both differentially up and down regulated. GLAPA is more conservative and large differences between the two phenotypes are required to allow the method to detect differential <phrase>deregulation</phrase> in gene sets. This is due to the fact that the enrichment statistic in GLAPA is prediction error which is a stronger criteria than classical two sample statistic as used in RS and GSEA. This was reflected in the analysis on real <phrase>data sets</phrase> as GSEA and RS were seen to be significant for particular gene sets while GLAPA was not, suggesting a small <phrase>effect size</phrase>. We find that the rank of <phrase>gene set</phrase> enrichment induced by GLAPA is more similar to RS than GSEA. More importantly, the rankings of the three methods share significant overlap.   CONCLUSION The three methods considered in our study recover relevant gene sets known to be deregulated in the experimental conditions and pathologies analyzed. There are differences between the three methods and GSEA seems to be more consistent in finding enriched gene sets, although no method uniformly dominates over all <phrase>data sets</phrase>. Our analysis highlights the deep difference existing between associative and predictive methods for detecting enrichment and the use of both to better interpret results of <phrase>pathway analysis</phrase>. We close with suggestions for users of <phrase>gene set</phrase> methods.
Temporal Redundancy Based Encoding Technique for <phrase>Peak Power</phrase> and Delay Reduction of On-Chip Buses —<phrase>Power consumption</phrase> and delay are two of the most important constraints in current-day on-chip bus design. The two major sources of dynamic <phrase>power dissipation</phrase> on a bus are the self capacitance and the <phrase>coupling capacitance</phrase>. As technology scales, the interconnect resistance increases due to shrinking wire-width. At the same time, spacing between the interconnects decreases resulting in an increase in the <phrase>coupling capacitance</phrase>. This, in turn, leads to stronger <phrase>crosstalk effects</phrase> between the interconnects. In Deep Sub-Micron technology the <phrase>coupling capacitance</phrase> exceeds the self capacitance, which, in turn, cause more <phrase>power consumption</phrase> and delay on the bus. Recently, the interest has also shifted to minimizing peak <phrase>power dissipation</phrase>. The reason being that higher <phrase>peak power</phrase> leads to an undesired increase in switching noise, metal electromigration problems and operation-induced variations due to non-uniform temperature on the die. Thus, minimizing <phrase>power consumption</phrase> and delay are the most important design objectives for on-chip buses. Several bus encoding schemes have been proposed in the literature for reducing crosstalk. Most of these encoding techniques use spatial redundancy that requires additional transmission wires on the bus. In this paper, a new temporal encoding scheme is proposed, which uses self-shielding memory-less codes to completely eliminate <phrase>worst-case</phrase> <phrase>crosstalk effects</phrase> and hence significantly minimizes <phrase>power consumption</phrase> and delay of the bus. A major advantage of the proposed temporal redundancy based encoding scheme is the reduction in the number of wires of the on-chip bus. This reduction facilitates extra spacing between the bus wires, when compared with the normal bus, for a given area. This, in turn, leads to reduced <phrase>crosstalk effects</phrase> between the wires. The proposed encoding scheme is tested with the SPEC2000 CINT benchmarks. The experimental results, when compared to the transmission over a normal bus, show that on an average the proposed technique leads to a reduction in the peak-<phrase>power consumption</phrase> by 51% (28%), 51% (29%) and 52% (30%) in the data (address) bus for 90nm, 65nm and 45nm technologies, respectively. For a bus length of 10mm the proposed technique also achieves 17%, 31% and 37% reduction in the bus delay for 90nm, 65nm and 45nm technologies, respectively, when compared to what is incurred by the <phrase>data transmission</phrase> on a normal bus.
Evaluation of Effectiveness of Median of Absolute Deviations <phrase>Outlier Rejection</phrase>-based <phrase>IDDQ Testing</phrase> for Burn-in Reduction CMOS chips having high leakage are observed to have high burn-in fallout rate. I DDQ testing has been considered as an alternative to burn-in. However, increased sub-threshold <phrase>leakage current</phrase> in deep sub-micron technologies limits the use of I DDQ testing in its present form. In this work, a statistical <phrase>outlier rejection</phrase> technique known as the median of absolute deviations (MAD) is evaluated as a means to screen early failures using <phrase>I DDQ</phrase> data. MAD is compared with <phrase>delta I DDQ</phrase> and current signature methods. The results of the analysis of the SEMATECH data are presented.
<phrase>Benchmarking</phrase> User Performance by Using <phrase>Virtual Reality</phrase> for Task-based Training <phrase>Conveyor</phrase> belts have a high accident and fatality rate associated with them because of the dangerous environment their constantly moving parts create. Because of this high fatality rate, different methods are being considered to improve current safety training methods. By looking at the principles of cognitive learning and what makes a computer-based <phrase>training program</phrase> successful, a safety <phrase>training program</phrase> using <phrase>virtual reality</phrase> (VR) is being proposed. The <phrase>training program</phrase> structure includes four steps in creating a comprehensive two phase <phrase>training program</phrase> that will train personnel on the required operational processes in an instructional-based phase and then test their abilities through an interactive task-based session that tracks the user's progress and choices, tallies points based on corrective actions taken, and gives immediate feedback and consequences to the user's actions. This paper focus on the task-based phase of the prototype development and what steps are taken in creating an individual exercise. One example exercise is described in detail from choosing the material that is tested, implementing the animations and coding using Right Hemisphere's Deep Creator and <phrase>LISP</phrase> coding, implementing the tracking methods, and the output file that has been designed to keep track of the user's performance.
Microwave Radiometric Measurements of <phrase>Soil Moisture</phrase> in Italy Microwave Radiometric Measurements of <phrase>Soil Moisture</phrase> in Italy Within the framework of the MAP and <phrase>RAPHAEL</phrase> projects, airborne experimental campaigns were carried out by the IFAC group in 1999 and 2000, using a multifrequency <phrase>microwave radiometer</phrase> at L, C and X bands (1.4, 6.8 and 10 GHz). The aim of the experiments was to collect <phrase>soil moisture</phrase> and <phrase>vegetation</phrase> <phrase>biomass</phrase> information on agricultural areas to give reliable inputs to the <phrase>hydrological</phrase> models. It is well known that microwave emission from soil, mainly at L-band (1.4 GHz), is very well correlated to its <phrase>moisture content</phrase>. Two experimental areas in Italy were selected for this project: one was the <phrase>Toce</phrase> <phrase>Valley</phrase>, <phrase>Domodossola</phrase>, in 1999, and the other, the agricultural area of Cerbaia, close to <phrase>Florence</phrase>, where flights were performed in 2000. Measurements were carried out on bare soils, <phrase>corn</phrase> and <phrase>wheat</phrase> fields at different growth stages and on meadows. Ground data of <phrase>soil moisture</phrase> (SMC) were collected by other research teams involved in the experiments. From the analysis of the <phrase>data sets</phrase>, it has been confirmed that L-band is well related to the SMC of a rather deep soil layer, whereas C-band is sensitive to the surface SMC and is more affected by the presence of <phrase>surface roughness</phrase> and <phrase>vegetation</phrase>, especially at high incidence angles. An algorithm for the retrieval of <phrase>soil moisture</phrase>, based on the sensitivity to moisture of the <phrase>brightness temperature</phrase> at C-band, has been tested using the collected <phrase>data set</phrase>. The results of the algorithm, which is able to correct for the effect of <phrase>vegetation</phrase> by means of the polarisation index at X-band, have been compared with <phrase>soil moisture</phrase> measurements in the ground. Finally, the sensitivity of emission at different frequencies to the <phrase>soil moisture</phrase> profile was investigated. Experimental <phrase>data sets</phrase> were interpreted by using the <phrase>Integral Equation</phrase> Model (IEM) and the outputs of the model were used to train an <phrase>artificial neural network</phrase> to reproduce the <phrase>soil moisture</phrase> content at different depths.
Improvement of <phrase>Lagrangian</phrase> Relaxation Convergence for <phrase>Production Scheduling</phrase> It is widely accepted that new <phrase>production scheduling</phrase> tools are playing a key role in flexible <phrase>manufacturing systems</phrase> to improve their performance by avoiding idleness machines while minimizing setup times penalties, reducing penalties for do not delivering orders on time, etc. Since manufacturing <phrase>scheduling problems</phrase> are NP-hard, there is a need of improving scheduling methodologies to get good solutions within low CPU time. La-grangian Relaxation (LR) is known for handling <phrase>large-scale</phrase> separable problems, however, the convergence to the optimal solution can be slow. LR needs customized <phrase>parametrization</phrase>, depending on the <phrase>scheduling problem</phrase>, usually made by an expert user. It would be interesting the use of LR without being and expertise, i.e., without difficult parameters tuning. This paper presents innovative approaches on the LR method to be able to develop a tool capable of solve <phrase>scheduling problems</phrase> applying the LR method without requiring a deep expertise on it. First approach is the improvement of an already existing method which use <phrase>Constraint Programming</phrase> (CP) to obtain better primal cost convergence. Second approach is called Extended Subgradient Information (ESGI) and it speed up the dual cost convergence. Finally, a set of step size rules for the Subgradient (SG) method are compared to choose the most appropriate rule depending on the <phrase>scheduling problem</phrase>. Test <phrase>results demonstrate</phrase> that the application of CP and ESGI approaches, together with LR and the selected step size rule depending on the problem, generates better solutions than the LR method by itself. Note to Practitioners-<phrase>Production scheduling</phrase> tools are one of the keys in flexible mannfacturing systems to improve its performance. These tools are usually based on optimization methods, as could be the <phrase>Lagrangian</phrase> Relaxation. The problems of using optimization methods are the need of time to get the solution, and the need of a high-specialized user to tune them. Therefore, optimization methods must be improved to use less time to obtain solutions and to do not need high-specialized users. This paper was motivated by these needs: reducing the CPU time when scheduling operations in <phrase>production planning</phrase> to permit quick replies to real-time perturbations into production processes; and making easier the use of <phrase>production scheduling</phrase> tools. This paper suggests new approaches for the <phrase>Lagrangian</phrase> Relaxation (LR) method applying Constraint <phrase>Logic Programming</phrase> (<phrase>CLP</phrase>) and improving the multipliers calculation (inside the Subgradient method) during the iterations to speed up the convergence of the LR method and make it easily tuned. Thus, the CPU …
Finite <phrase>State Machines</phrase> : Composition, Verification, Minimization : a Case Study — A deep understanding of circuit behaviour is a prerequisite for any validation process (simulation, <phrase>formal verification</phrase>, <phrase>test generation</phrase>). We propose to use a tool which gives complete and optimized representations of <phrase>sequential circuits</phrase> allowing the designer to understand the accurate behaviour of the circuit. A detailed example is introduced to help reader's understanding. For obvious reasons, we choose a small size circuit. The example comes from our experience ([2]) in computer architecture and <phrase>digital design</phrase> education.
Parameter <phrase>sensitivity analysis</phrase> of <phrase>crop growth models</phrase> based on the extended Fourier Amplitude Sensitivity <phrase>Test method</phrase> Keywords: Extended FAST WOFOST <phrase>crop growth models</phrase> <phrase>Sample size</phrase> Parameter variation range Time-dependent properties Multivariable output a b s t r a c t <phrase>Sensitivity analysis</phrase> (SA) has become a basic tool for the understanding, application and development of models. However, in the past, little attention has been paid to the effects of the parameter <phrase>sample size</phrase> and parameter variation range on the parameter SA and its temporal properties. In this paper, the <phrase>corn</phrase> crop planted in 2008 in the Yingke <phrase>Oasis</phrase> of <phrase>northwest China</phrase> is simulated based on meteorological observation data for the inputs and statistical data for the parameters. Furthermore, using the extended Fourier Amplitude Sensitivity (EFAST) algorithm, SA is performed on the 47 crop parameters of the WOrld FOod STudies (WOFOST) <phrase>crop growth models</phrase>. A deep analysis is conducted, including the effects of the parameter <phrase>sample size</phrase> and variation range on the parameter SA, the temporal properties and the multivariable output issues of SA. The results show that <phrase>sample size</phrase> highly affects the convergence of the sensitivity indices. Two types of parameter variation ranges are used for the analysis, and the results show that the sensitive parameters of the two parameter spaces are distinctly different. In addition, taking the storage <phrase>organ</phrase> biomasses at the different growth stages as the objective output, the time-dependent characteristics of the parameter sensitivity are discussed. The results show that several sensitive parameters exist in the grain <phrase>biomass</phrase> throughout the entire development stage. In addition, analyzing the twelve sensitive parameters has proven that although certain parameters have no effect on the final yield, they play key roles in certain growth stages, and the importance of these parameters gradually increases. Finally, the sensitivity analyses of different state variable outputs are performed, including the <phrase>biomass</phrase>, yield, leaf area index, and <phrase>transpiration</phrase> coefficient. The results suggest that the sensitive parameters of various variable processes differ. This study highlights the importance of considering multiple characteristics of the model parameters and the responses of the models in specific phenological stages. <phrase>Crop growth models</phrase> are a valuable tool for the quantitative analysis of the growth and production of crops and play an important role in crop monitoring, <phrase>crop yield</phrase> prediction, field management recommendations, agricultural production potential evaluation, and <phrase>climate change</phrase> impact evaluation (Batchelor et al., <phrase>Crop growth models</phrase> primarily simulate the growth and development of crops, and they encompass the primary biophysical and biochemical processes in the soilecropeatmosphere system, …
RenderGAN: Generating Realistic <phrase>Labeled Data</phrase> Deep Convolutional Neuronal Networks (DCNN) are showing remarkable performance on many <phrase>computer vision</phrase> tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of <phrase>annotating</phrase> data manually can render the usage of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from <phrase>unlabeled data</phrase> such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of <phrase>barcode</phrase>-like markers that are attached to <phrase>honeybees</phrase>. A DCNN is trained on this data only. It performs better on a test set of <phrase>real data</phrase> than an equal DCNN trained on the limited amounts of <phrase>real data</phrase> available.
Automated Determination of Axonal Orientation in the Deep <phrase>White Matter</phrase> of the <phrase>Human Brain</phrase> The wide-spread utilization of diffusion-weighted imaging in the clinical neurosciences to assess <phrase>white-matter</phrase> (WM) integrity and architecture calls for robust validation strategies applied to the data that are acquired with noninvasive imaging. However, the <phrase>pathology</phrase> and detailed fiber architecture of WM tissue can only be observed postmortem. With these considerations in mind, we designed an automated method for the determination of axonal orientation in <phrase>high-resolution</phrase> <phrase>microscope</phrase> images. The algorithm was tested on tissue that was stained using a <phrase>silver</phrase> impregnation technique that was optimized to resolve axonal fibers against very low levels of background. The orientation of individual nerve fibers was detected using spatial filtering and a template-<phrase>matching algorithm</phrase>, and the results are displayed as color-coded overlays. Quantitative models of WM fiber architecture at the microscopic level can lead to improved interpretation of <phrase>low-resolution</phrase> <phrase>neuroimaging</phrase> data and to more accurate mapping of fiber pathways in the <phrase>human brain</phrase>.
'disappearing Sensor'-<phrase>textile</phrase> Based Sensor for Monitoring Breathing — <phrase>Textile</phrase> based sensors were developed and used for remote monitoring of breathing. The breathing is simulated by using a new cyclic tester device. In the simulated a cyclic force is applied along the length of the <phrase>textile</phrase> sensor. However due to the morphology of <phrase>human body</phrase>, in real situation the sensor is not only under stretching but also under a certain degree of bending. A prototype garment with the sensor situated on the chest area was made. The prototype was worn by 10 persons, and breathing was recorded as the persons were sitting still, walking and <phrase>jogging</phrase>. <phrase>Deep breathing</phrase> in the <phrase>supine position</phrase> and breathing with a <phrase>method called</phrase> athletic breathing were used to evaluate the sensor. A testing circuit and a <phrase>Labview</phrase> program were made for preliminary test. The sensor is wearable, washable and comfortable. Sensor construction is totally 'disappearing' and visualize as printed pattern onto the surface of garment.
Is Quality a Design Constraint for Sub 100nm Designs? Description <phrase>Deep sub-micron</phrase> design (below 100nm) present a number of new design challenges. These include very high masking costs, new interconnect materials and parasitic phenomenon, significant re-engineering at the device level due to changes in basic device performance, very high <phrase>gate count</phrase> and pin count designs, complexity in high pin count packaging & test, and finally reduced product life in the <phrase>marketplace</phrase> do to the rapid rollout of new technologies. One of the trade offs that is taking place in the industry to address these issues is the decision toward " design existence " , which is the selection of the " first functional implementation " of a design, over " design quality " which is the selection of the " optimal implementation " of a design. This panel will discuss the trends in the issue with respect to the re-targeting of the design quality issues from the SOC level to the flow and device levels and the impact on this " shift " on the manufacturability of the resulting designs. Issues discussed will include the use of pre-tested IP as a quality metric, the coverage and quality of the EDA design and validation tools, the correlation of these metrics to the actual <phrase>manufacturing process</phrase> and the impact of post <phrase>fabrication process</phrase> steps (packaging, test, etc) on the yield of the resulting design.
<phrase>Test challenges</phrase> for <phrase>deep sub-micron technologies</phrase> The use of <phrase>deep submicron</phrase> <phrase>process technologies</phrase> presents several new challenges in the area of <phrase>manufacturing test</phrase>. While a significant body of work has been devoted to identifying and investigating design challenges in <phrase>nanometer technologies</phrase>, the impact on test strategies and methodologies is still not well understood. This paper highlights the challenges to current <phrase>test methodologies</phrase> arising from technology driven trends, and will present an overview of emerging techniques that address <phrase>deep submicron</phrase> <phrase>test challenges</phrase>.
Intelligence: what's in a name? Times piece "Nobody's Smart About Intelligence" (March 1, 1998), he offers this lament: "IQs are up. S.A.T.s are down. <phrase>Americans</phrase> flunk math and prosper. Somebody with brains should figure this out." The best anyone can offer, Johnson claims, is a conjecture that the complexity of everyday life (programming small electronic devices or calculating the latest projection of your net worth when you retire) has stretched and exercised our brains into faster and more agile computing engines. This might explain our increasing IQs while <phrase>MTV</phrase> and <phrase>video-game</phrase> overload might explain our increasing ignorance and declining capabilities at the logical plodding <phrase>deductive</phrase> thought of traditional intelligence. So what type of intelligence is AI trying to create? Astro Teller (New ~rk 7qmes, <phrase>Op-Ed</phrase>, March 21, 1998), suggests that no matter the type, building intelligences will make our world better as we learn more about our minds and who we are. But what will we understand? How better to exploit our neighbors or sell them goods and services at ever increasing profits? Will we understand the difference between <phrase>Gandhi</phrase> and <phrase>Saddam</phrase>? <phrase>Mozart</phrase> and <phrase>Madonna</phrase>? Or just what is it that everyone finds funny about Seinfield? The recent <phrase>pinnacle</phrase> of AI achievement has not come from our half century long quest to pass the <phrase>Turing Test</phrase>, but from our fascination at a machine beating a human at the complex task of playing chess. <phrase>Deep Blue</phrase>, a parallel <phrase>supercomputing</phrase> creation from IBM for processing hundreds of millions of chess moves per second, is the hardware and software that realized this • e.,o • oe. eeeoeleQo e*e eolee • oo t • • e#= The recent <phrase>pinnacle</phrase> of AI achievement has come from <phrase>Deep Blue</phrase>, a machine that beat a human at playing chess. But what kind of intelligence is this? impressive accomplishment. An ancient game, long attacked by AI'ers and now empirically conquered. Everyone can honestly admit that <phrase>Deep Blue</phrase> doesn't have a clue about what it is doing, so <phrase>self awareness</phrase> is not an issue. It just "knows" the next best move from an intensive search. So what kind of intelligence is this? It is dearly '~AI Intelligence," a smart machine; honored byAI associations and foundations with a small pile of cash (compared to IBM expenses!) and a <phrase>Newell</phrase> Research Medal. New <phrase>York</phrase> Times piece on <phrase>Deep Blue</phrase>, asked whether <phrase>Deep Blue</phrase> is indeed intelligent. He offered that although human chess grandmasters don't do exactly …
Compression-based classification of biological sequences and structures via the Universal Similarity Metric: experimental assessment BACKGROUND Similarity of sequences is a key mathematical notion for Classification and <phrase>Phylogenetic</phrase> studies in Biology. It is currently primarily handled using alignments. However, the alignment methods seem inadequate for post-genomic studies since they do not scale well with data <phrase>set size</phrase> and they seem to be confined only to genomic and <phrase>proteomic</phrase> sequences. Therefore, alignment-free <phrase>similarity measures</phrase> are actively pursued. Among those, USM (Universal Similarity Metric) has gained prominence. It is based on the deep theory of <phrase>Kolmogorov Complexity</phrase> and universality is its most novel striking feature. Since it can only be approximated via <phrase>data compression</phrase>, USM is a methodology rather than a formula quantifying the similarity of two strings. Three approximations of USM are available, namely <phrase>UCD</phrase> (Universal Compression Dissimilarity), <phrase>NCD</phrase> (Normalized Compression Dissimilarity) and <phrase>CD</phrase> (Compression Dissimilarity). Their applicability and robustness is tested on various <phrase>data sets</phrase> yielding a first massive quantitative estimate that the USM methodology and its approximations are of value. Despite the rich theory developed around USM, its experimental assessment has limitations: only a few data compressors have been tested in conjunction with USM and mostly at a qualitative level, no comparison among <phrase>UCD</phrase>, <phrase>NCD</phrase> and <phrase>CD</phrase> is available and no comparison of USM with <phrase>existing methods</phrase>, both based on alignments and not, seems to be available.   RESULTS We experimentally test the USM methodology by using 25 compressors, all three of its known approximations and six <phrase>data sets</phrase> of relevance to <phrase>Molecular Biology</phrase>. This offers the first systematic and quantitative experimental assessment of this methodology, that naturally complements the many theoretical and the preliminary <phrase>experimental results</phrase> available. Moreover, we compare the USM methodology both with methods based on alignments and not. We may group our experiments into two sets. The first one, performed via <phrase>ROC</phrase> (Receiver Operating Curve) analysis, aims at assessing the intrinsic ability of the methodology to discriminate and classify biological sequences and structures. A second set of experiments aims at assessing how well two commonly available classification algorithms, UPGMA (Unweighted Pair Group Method with Arithmetic Mean) and <phrase>NJ</phrase> (Neighbor Joining), can use the methodology to perform their task, their performance being evaluated against gold standards and with the use of well known statistical indexes, i.e., the F-measure and the partition distance. Based on the experiments, several conclusions can be drawn and, from them, novel valuable guidelines for the use of USM on <phrase>biological data</phrase>. The main ones are reported next.   CONCLUSION <phrase>UCD</phrase> and <phrase>NCD</phrase> are indistinguishable, i.e., they yield nearly the same values of the statistical indexes we have used, accross experiments and <phrase>data sets</phrase>, while <phrase>CD</phrase> is almost always worse than both. UPGMA seems to yield better classification results with respect to <phrase>NJ</phrase>, i.e., better values of the statistical indexes (10% difference or above), on a substantial fraction of experiments, compressors and USM approximation choices. The compression program PPMd, based on PPM (Prediction by Partial Matching), for generic data and Gencompress for DNA, are the best performers among the compression algorithms we have used, although the difference in performance, as measured by statistical indexes, between them and the other algorithms depends critically on the <phrase>data set</phrase> and may not be as large as expected. PPMd used with <phrase>UCD</phrase> or <phrase>NCD</phrase> and UPGMA, on sequence data is very close, although worse, in performance with the alignment methods (less than 2% difference on the F-measure). Yet, it scales well with data <phrase>set size</phrase> and it can work on data other than sequences. In summary, our quantitative analysis naturally complements the rich theory behind USM and supports the conclusion that the methodology is worth using because of its robustness, flexibility, scalability, and competitiveness with existing techniques. In particular, the methodology applies to all <phrase>biological data</phrase> in textual format. The software and <phrase>data sets</phrase> are available under the <phrase>GNU</phrase> <phrase>GPL</phrase> at the supplementary material web page.
Ladd – Laser Assisted <phrase>Deep Drawing</phrase> In the current work, cup drawing experiments with laser assistance are presented where only selected areas of the work piece have been heated up. Since the strongest deformations occur at the outer circumference of the blank, only that area has been heated up by a defocused <phrase>laser beam</phrase>. Selective laser heating of the work pieces was performed by diode as well as Nd:YAG laser radiation. The forming load for the experiments was established by a small <phrase>hydraulic press</phrase> with a maximum drawing force of 630kN and a hydraulic die cushion. During the experiments, the drawing path and all relevant forces have been recorded. <phrase>Experimental results</phrase> clearly demonstrate that this combined laser forming process is enlarging the possibilities of conventional <phrase>deep drawing</phrase>. 1 Introduction In production engineering 3-D parts can be produced by different forming methods. For example, <phrase>deep drawing</phrase> is a well established process in <phrase>automotive industry</phrase>. Fig. 1 shows schematically the setup of a simple cupping test. This process transforms a mainly flat sheet blank into a hollow 3-D part by pressing it into a die. High true strain results from strong plastic deformations which can only be achieved by strong forces applied to the work piece. In some cases limitations arise from the <phrase>mechanical properties</phrase> of the used materials since the material cannot withstand the needed forces. Flow curves (e.g. Fig. 2) of different materials show that plastic deformations can be facilitated by higher temperatures.
Distinctive neuronal firing patterns in subterritories of the subthalamic nucleus. OBJECTIVE <phrase>Deep brain stimulation</phrase> of the subthalamic nucleus (<phrase>STN-DBS</phrase>) is an established treatment for <phrase>Parkinson's disease</phrase> (PD). Anatomical connectivity analyses and task-related physiological studies have divided the STN into different functional domains: <phrase>sensorimotor</phrase>, <phrase>limbic</phrase>, and associative - located in its dorsolateral (dSTN), anteroventral (vSTN) and medial territories, respectively. Targeting <phrase>sensorimotor</phrase> STN is essential for stimulation efficacy and is supported by intraoperative micro-electrode recordings. A different neuronal signature in microelectrode recordings across STN subterritories was explored in this study.   METHODS Stable recordings from 30 <phrase>PD patients</phrase> were assigned to dSTN or vSTN by means of an anatomical method (based on fused <phrase>computed tomography</phrase>/<phrase>magnetic resonance images</phrase>) and through a priori tri-segmented partition of the recording itself. We computed the inter-spike interval (<phrase>ISI</phrase>) and <phrase>ISI</phrase>-characteristics, mean firing rate (MFR), discharge patterns and mean burst rate (<phrase>MBR</phrase>) of each detected single unit activity.   RESULTS We showed a different <phrase>MBR</phrase> between dSTN and vSTN (1.51±0.18 vs. 1.76±0.22events/minute, Wilcoxon rank sum test, p<0.05) and a trend in the difference between their MFR (12.78 vs. 15.05Hz, Wilcoxon rank sum test, p=0.053) only with the anatomically <phrase>based method</phrase>.   CONCLUSION Burst firing differs across STN subterritories.   SIGNIFICANCE Different functions of subthalamic domains might be reflected by distinctive burst signalling of its subterritories.
A Curved-beam Bistable Mechanism —This paper presents a monolithic mechanically -bistable mechanism that does not rely on <phrase>residual stress</phrase> for its bistability. The typical implementation of this mechanism is two curved centrally-clamped parallel beams, hereafter referred to as " double curved beams ". Modal analysis and <phrase>finite element analysis</phrase> (FEA) simulation of the curved beam are used to predict, explain, and design its bistable behavior. Microscale double curved beams are fabricated by <phrase>deep-reactive ion etching</phrase> (DRIE) and their <phrase>test results</phrase> agree well with the analytic predictions. Approaches to <phrase>tailor</phrase> the bistable behavior of the curved beams are also presented. [1079]
Embedded <phrase>Memory Bist</phrase> for Systems-on-a-chip Embedded <phrase>Memory Bist</phrase> for Systems-on-a-chip Title: Embedded <phrase>Memory Bist</phrase> for Systems-on-a-chip <phrase>Embedded memories</phrase> consume an increasing portion of the die area in <phrase>deep submicron</phrase> systems-on-a-chip (SOCs). <phrase>Manufacturing test</phrase> of <phrase>embedded memories</phrase> is an essential step in the SOC production that screens out the defective chips and accelerates the transition from the <phrase>yield learning</phrase> phase to the volume production phase of a new manufacturing technology. Built-in self-test (BIST) is establishing itself as an enabling technology that can effectively tackle the SOC test problem. However, unless consciously implemented, its main limitations lie in elevated <phrase>power dissipation</phrase> and <phrase>area overhead</phrase>, and potential performance penalty and increased testing time, all of which directly influence the cost and quality of <phrase>manufacturing test</phrase>. This thesis introduces two new embedded <phrase>memory BIST</phrase> architectures, whose objective is to reduce the cost of test and increase the <phrase>test quality</phrase> to improve product reliability and yield. A distributed <phrase>memory BIST</phrase> approach with a serial interconnect scheme is first developed. This solution can concurrently support multiple memory test algorithms for heterogeneous memories with low <phrase>power dissipation</phrase> during test and with relatively low gate and routing <phrase>area overhead</phrase>, in addition to facilitating self-diagnosis. The distributed BIST approach is then extended to a hardware/software co-testing <phrase>memory BIST</phrase> architecture for complex SOCs. By reusing the existing on-chip resources (e.g., processor cores and busses), further savings in <phrase>area overhead</phrase> can be achieved and performance penalty for bus-connected memories can be eliminated. This is accomplished using a design <phrase>space exploration</phrase> framework based on a new <phrase>test scheduling algorithm</phrase> that balances the usage of the existing on-chip resources and dedicated <phrase>design for test</phrase> (DFT) hardware such that the functional power constraints are not exceeded during test, while trading-off the testing time against the DFT area. iii Acknowledgments I will begin by thanking my supervisor, Nicola, for his valuable assistance and energetic support during this project. I also wish to thank my colleagues in the <phrase>Computer-Aided Design</phrase> and Test who were of great help when ideas and questions needed to be discussed. In particular, I would like to express my appreciation to Qiang Xu for his help in debugging the <phrase>test scheduling algorithm</phrase>. I wish to acknowledge <phrase>Canadian</phrase> <phrase>Microelectronics</phrase> Corporation (CMC) for their manufacturing grants, as well as the technical support and training they have provided. I am also grateful to the graduate students, faculty, administrative and technical members in De-for their continuous help during my study and research. Members of my family and many more than I can …
The Evaluation of an Effective Out-of-Core <phrase>Run-Time</phrase> System in the Context of Parallel Mesh Generation —We present an out-of-core <phrase>run-time</phrase> system that supports effective parallel computation of large irregular and adaptive problems, in particular unstructured mesh generation (PUMG). PUMG is a highly challenging application due to intensive <phrase>memory accesses</phrase>, unpredictable communication patterns , and variable and irregular data dependencies reflecting the unstructured spatial connectivity of mesh elements. Our runtime system allows to transform the footprint of parallel applications from wide and shallow into narrow and deep by extending the memory utilization to the out-of-core level. It simplifies and streamlines the development of otherwise highly time consuming out-of-core applications as well as the converting of existing applications. It utilizes disk, network and <phrase>memory hierarchy</phrase> to achieve high utilization of computing resources without sacrificing performance with PUMG. The runtime system combines different programming paradigms: multi-threading within the nodes using industrial strength <phrase>software framework</phrase>, one-sided active messages among the nodes, and an out-of-core subsystem for managing large datasets. We performed an evaluation on traditional parallel platforms to stress test all layers of the <phrase>run-time</phrase> system using three different PUMG methods with significantly varying communication and synchronization patterns. We demonstrated high overlap in computation, communication, and disk I/O which results in good performance when computing large out-of-core problems. The runtime system adds very small overhead (up to 18% on most configurations) when computing in-core which means performance is not compromised.
Event-related rTMS at encoding affects differently <phrase>deep and shallow</phrase> memory traces The "level of processing" effect is a classical finding of the <phrase>experimental psychology</phrase> of memory. Actually, the depth of <phrase>information processing</phrase> at encoding predicts the accuracy of the subsequent <phrase>episodic memory</phrase> performance. When the incoming stimuli are analyzed in terms of their meaning (semantic, or deep, encoding), the <phrase>memory performance</phrase> is superior with respect to the case in which the same stimuli are analyzed in terms of their <phrase>perceptual</phrase> features (shallow encoding). As suggested by previous <phrase>neuroimaging</phrase> studies and by some preliminary findings with <phrase>transcranial magnetic stimulation</phrase> (<phrase>TMS</phrase>), the left <phrase>prefrontal cortex</phrase> may play a role in semantic processing requiring the allocation of working memory resources. However, it still remains unclear whether <phrase>deep and shallow</phrase> encoding share or not the same cortical networks, as well as how these networks contribute to the "level of processing" effect. To investigate the brain areas casually involved in this phenomenon, we applied event-related repetitive <phrase>TMS</phrase> (rTMS) during deep (semantic) and shallow (<phrase>perceptual</phrase>) encoding of words. Retrieval was subsequently tested without rTMS interference. RTMS applied to the left <phrase>dorsolateral prefrontal cortex</phrase> (DLPFC) abolished the beneficial effect of deep encoding on <phrase>memory performance</phrase>, both in terms of accuracy (decrease) and reaction times (increase). Neither accuracy nor reaction times were instead affected by rTMS to the right DLPFC or to an additional control site excluded by the memory process (vertex). The fact that online measures of semantic processing at encoding were unaffected suggests that the detrimental effect on <phrase>memory performance</phrase> for semantically encoded items took place in the subsequent consolidation phase. These results highlight the specific causal role of the left DLPFC among the wide left-lateralized cortical network engaged by <phrase>long-term memory</phrase>, suggesting that it probably represents a crucial node responsible for the improved <phrase>memory performance</phrase> induced by semantic processing.
<phrase>Critical path selection</phrase> for <phrase>delay fault testing</phrase> based upon a <phrase>statistical timing</phrase> model —<phrase>Critical path selection</phrase> is an indispensable step for testing of small-size <phrase>delay defects</phrase>. Historically, this step relies on the construction of a set of <phrase>worst-case</phrase> paths, where the timing lengths of the paths are calculated based upon discrete-valued <phrase>timing models</phrase>. The assumption of discrete-valued <phrase>timing models</phrase> may become invalid for modeling delay effects in the deep sub-micron domain, where the effects of <phrase>timing defects</phrase> and <phrase>process variations</phrase> are often statistical in nature. This paper studies the problem of <phrase>critical path selection</phrase> for testing small-size <phrase>delay defects</phrase>, assuming that circuit delays are statistical. We provide theoretical analysis to demonstrate that the new <phrase>path-selection</phrase> problem consists of two computationally intractable subproblems. Then, we discuss practical heuristics and their performance with respect to each subproblem. Using a statistical defect injection and timing-simulation framework, we present <phrase>experimental results</phrase> to support our theoretical analysis.
The University of Texas at <phrase>Austin</phrase> School of Information: deep in the heart of the <phrase>information age</phrase> uated within the thriving yet bohemian Silicon Hills and urban beauty of south <phrase>central Texas</phrase>. The School of Information has a rich tradition of training information professionals of all ilk: archivists, record managers, <phrase>librarians</phrase>, intelligence analysts , and conservators. It is within this context that the <phrase>human-computer interaction</phrase> (HCI)/<phrase>information architecture</phrase> (IA)/usability program has evolved, addressing the profusion of digital information and its impacts on users and communities. editorial board member of <phrase>journals</phrase> such as IJHCS and Interacting with Computers, and is known for his research in human <phrase>information processing</phrase> (e.g., [2] and [3]). He brought an immediate focus on HCI and IA, and has striven to grow that area of expertise, while anchoring it in the context of the strong, historical education of <phrase>librarians</phrase> and attention to information studies. The philosophy of our HCI/IA/usability curriculum in the School of Information is that this is a professional discipline, steeped in science and applied via well-established methods [1]. We have six assistant, associate, and full professors (among our 23) with degrees in <phrase>cognitive psychology</phrase> , <phrase>information science</phrase>, or <phrase>computer science</phrase>, all with <phrase>real-world</phrase> experience providing Usability support (big U-connoting attention to the full lifespan of <phrase>user interfaces</phrase>, from user-requirements gathering, through design and prototype testing, to <phrase>field testing</phrase> and maintenance). We offer a flexible curriculum for someone who wishes to become a professional in usability/IA/HCI design. Such a student would take core courses in <phrase>Research Methods</phrase> and Statistics, and in Understanding and Serving Users. Same student would likely take a six-course series of Intro and Advanced <phrase>Information Architecture</phrase>, Intro and Advanced <phrase>Digital Media</phrase> Design, and Intro and Advanced Usability. He or she would likely take another class in Human <phrase>Information Processing</phrase>, maybe one in <phrase>Information Science</phrase> and Knowledge Systems: Concepts in <phrase>Information Retrieval</phrase>. He or she would complete the <phrase>master's degree</phrase> with a thesis, or maybe a capstone experience where he or she carried out some industrial strength piece of work for a company or <phrase>nonprofit</phrase> or government entity.
<phrase>Numerical Stability</phrase> in <phrase>Linear Programming</phrase> and <phrase>Semidefinite Programming</phrase> I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. ii Abstract We study <phrase>numerical stability</phrase> for interior-point methods applied to <phrase>Linear Programming</phrase>, LP, and <phrase>Semidefinite Programming</phrase>, SDP. We analyze the difficulties inherent in current methods and present robust algorithms. We start with the error bound analysis of the search directions for <phrase>the normal equation</phrase> approach for LP. Our error analysis explains the surprising fact that the ill-conditioning is not a significant problem for <phrase>the normal equation</phrase> system. We also explain why most of the popular LP solvers have a default stop tolerance of only 10 −8 when the machine precision on a 32-bit computer is approximately 10 −16. We then propose a simple alternative approach for <phrase>the normal equation</phrase> based <phrase>interior-point method</phrase>. This approach has better <phrase>numerical stability</phrase> than <phrase>the normal equation</phrase> <phrase>based method</phrase>. Although, our approach is not competitive in terms of CPU time for the <phrase>NETLIB</phrase> problem set, we do obtain <phrase>higher accuracy</phrase>. In addition, we obtain significantly smaller CPU times compared to <phrase>the normal equation</phrase> based direct solver, when we solve well-conditioned, huge, and sparse problems by using our iterative based linear solver. Additional techniques discussed are: crossover; purification step; and no <phrase>backtracking</phrase>. Finally, we present an algorithm to construct SDP <phrase>problem instances</phrase> with prescribed <phrase>strict complementarity gaps</phrase>. We then introduce two measures of <phrase>strict complementarity gaps</phrase>. We empirically show that: (i) these measures can be evaluated accurately; (ii) the size of the <phrase>strict complementarity gaps</phrase> correlate well with the number of iteration for the SDPT3 solver, as well as with the local asymptotic convergence rate; and (iii) large <phrase>strict complementarity gaps</phrase>, coupled with the failure of Slater's condition, correlate well with loss of accuracy in the solutions. In addition, the numerical tests show that there is no correlation between the <phrase>strict complementarity gaps</phrase> and the geometrical measure used in [31], or with Renegar's <phrase>condition number</phrase>. iii Acknowledgments I would like to express my deep thanks to my supervisor, Professor Henry Wolkowicz. Without his continues guidance and support, I could not finish this thesis. I would also like to thank the committee members, Professor Miguel Anjos, Professor Chek Beng Chua, Professor <phrase>Levent</phrase> Tunçel, and Professor Yin Zhang, for their detailed comments and careful …
Modeling and testing of interference faults in the nano NAND <phrase>Flash memory</phrase> Advance of the fabrication technology has enhanced the size and density for the NAND <phrase>Flash memory</phrase> but also brought new types of defects which need to be tested for the quality consideration. This work analyzes three types of <phrase>physical defects</phrase> for the deep nano-meter NAND <phrase>Flash memory</phrase> based on the circuit level simulation and proposes new categories of interference faults (IFs). Testing algorithm is also proposed to test the faults under the worst case condition. The algorithm, in addition to test IFs, can also detect the conventional address faults, disturbance faults and other <phrase>RAM</phrase>-like faults for the NAND Flash.
Perspectives on Chinese <phrase>Question Answering Systems</phrase> <phrase>Question Answering</phrase> (QA) is becoming an <phrase>increasingly important</phrase> research area in <phrase>natural language processing</phrase>. Since 1999, many international <phrase>question answering</phrase> contests have been held at conferences and workshops, such as TREC, <phrase>CLEF</phrase>, and NTCIR. Thus far, eleven languages – Spanish – have been tested on monolingual or cross-lingual <phrase>question answering</phrase> tasks. Although Chinese is the second most popular language in the world, NTCIR only conducted the first QA contest in Chinese this year. The <phrase>results reveal</phrase> that there seems to be a performance gap between Chinese <phrase>question answering systems</phrase> and some systems of other languages. In this paper, we review <phrase>previous works</phrase> on <phrase>Chinese question answering</phrase>, including our two systems on frequently asked questions and factoid questions. Comparing Chinese with other languages, <phrase>word segmentation</phrase> is a key problem in <phrase>Chinese question answering</phrase>. We review studies on <phrase>word segmentation</phrase> and discuss important issues, such as <phrase>part-of-speech tagging</phrase>, <phrase>named entity recognition</phrase>, <phrase>deep and shallow</phrase> parsing, <phrase>semantic role</phrase> and relation labeling etc., which are helpful for building QA systems. <phrase>Machine learning</phrase> approaches currently represent the main stream on many QA research issues, we believe, by efficiently utilizing the above resources, the performance of <phrase>machine learning</phrase> approaches can be improved further in <phrase>Chinese question answering</phrase>.
Blondie24, Playing at the Edge of AI (Book Review) 1 <phrase>Computer science</phrase> has failed abysmally at producing machines which display intelligence. According to Fogel, the last 50 years of effort in <phrase>artificial intelligence</phrase> have been on the wrong track, leaving us no closer to the goal than when we started. The wrong track has been the attempt to make the computer imitate our behavior. Scientists striving to build an <phrase>artificial intelligence</phrase> load the computer with knowledge on a chosen topic, along with an algorithm to do the associated task. The hope is that the computer will equal or surpass our intelligence for that subject. This approach dates back to the <phrase>Turing Test</phrase>, which Fogel points out has been misquoted and misinterpreted almost from day one. Misquoted or not, the <phrase>Turing Test</phrase> has a computer pretend to be human, and this paradigm became a signpost saying that the road to <phrase>artificial intelligence</phrase> is through <phrase>mimicry</phrase> of human behavior. According to Fogel, however , that path leads only to an illusion of intelligence — for example the kind of wooden intelligence exhibited by <phrase>Deep Blue</phrase>. If imitation of <phrase>human intelligence</phrase> has not led to machine intelligence , then what will? What is in-telligence? Fogel is critical in general of researchers in this field for skirting this last question. He believes that if the field of <phrase>artificial intelligence</phrase> had started with a proper definition of intelligence then there would have been a better chance of creating it. That is common sense — knowing what you are trying to build is crucial. Fogel therefore gives the following definition of intelligence. Intelligence is the capacity of a <phrase>decision-making</phrase> system to adapt its behavior to meet goals in a range of environments. Fogel looks to nature for an example of intelligence. He describes the intricate and seemingly clever behavior of a certain species of wasp. However, he points out that an individual wasp is fixed in its behavior. An experiment by Jean Henri Fabré described in the Notes section of the book seems to make this clear. The wasp, Fogel says, is like the 'proverbial robot', an <phrase>automaton</phrase> with no adaptive behavior. Therefore the individual wasp is not intelligent. However, Fogel considers the species of wasp to be intelligent as a group or system, since it evolves to meet a changing environment. The species is what has learned the intricate behavior. The point is made that in general, intelligence requires a <phrase>reservoir</phrase> of knowledge and …

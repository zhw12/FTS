social_media as windows on the social life of the mind this is a programmatic paper , marking out two directions in which the study of social_media can contribute to broader problems of social_science : understanding cultural evolution and understanding collective cogni-tion . under the first heading , i discuss some difficulties with the usual , adaptationist explanations of cultural phenomena , alternative explanations involving network diffusion effects , and some ways these could be tested using social_media data . under the second i describe some of the ways in which social_media could be used to study how the social organization of an epistemic community supports its collective cognitive performance . let me begin by considering two 1 senses in which we might speak of human thought as being " social " , and how they might orient the study of social information_processing and social_media . the first sense is a commonplace of many schools in the social_sciences and humanities : our thought relies on the cultural transmission of cognitive tools . every individual thinker , no matter how innovative or even lonely they may be , depends crucially on a vast array of cognitive tools ( concepts , procedures , languages , assumptions , values , . . . ) which they did not devise themselves , and could not have devised for themselves . instead they inherited these cognitive tools from interacting with other people , who for the most part themselves did not invent them . ( whether this dependence on tradition is a logical necessity , or merely a reflection 1 of course , people think a lot about their own and others' social_interactions , and a big use of social_media is sharing these thoughts . but in this social_media are no different from any other form of human , or for that matter primate , association . 2 " [k]nowledge is a function of association and communication ; it depends upon tradition , upon tools and methods socially transmitted , developed and sanctioned . faculties of effectual observation , reflection and desire are habits acquired under the influence of the culture and institutions of society , not ready-made inherent powers " of our peculiar bounded_rationality and bounded lifespan , is a deep question , fortunately not relevant here . ) while individual thinkers invent and discover , it is nonetheless true that innovations are typically refined , extended and perfected by groups , and that it is very rare indeed for highly developed concepts and ideas to emerge from a single , isolated thinker , rather than from a process of interaction the branches of social_science for which these facts are 
design and implementation of therapeutic ultrasound generating circuit for dental tissue formation and tooth-root healing biological tissue healing has recently attracted a great deal of research interest in various medical fields . trauma to teeth , deep and root caries , and orthodontic treatment can all lead to various degrees of root resorption . in our previous study , we showed that low-intensity pulsed ultrasound ( lipus ) enhances the growth of lower incisor apices and accelerates their rate of eruption in rabbits by inducing dental tissue growth . we also performed clinical studies and demonstrated that lipus facilitates the healing of orthodontically induced teeth-root resorption in humans . however , the available lipus devices are too large to be used comfortably inside the mouth . in this paper , the design and implementation of a low-power lipus generator is presented . the generator is the core of the final intraoral device for preventing tooth root loss and enhancing tooth root tissue healing . the generator consists of a power_supply subsystem , an ultrasonic transducer , an impedance_matching circuit , and an integrated_circuit composed of a digital controller circuitry and the associated driver circuit . most of our efforts focus on the design of the impedance_matching circuit and the integrated system-on-chip circuit . the chip was designed and fabricated using 0 . 8- m high-voltage technology from dalsa semiconductor , inc . the power supply subsystem and its impedance_matching network are implemented using discrete components . the lipus generator was tested and verified to function as designed and is capable of producing ultrasound power_up to 100 mw in the vicinity of the transducer's resonance_frequency at 1 . 5 mhz . the power efficiency of the circuitry , excluding the power supply subsystem , is estimated at 70% . the final products will be tailored to the exact size of teeth or biological tissue , which is needed to be used for stimulating dental tissue ( dentine and cementum ) healing . 
measuring invariances in deep_networks for many pattern_recognition tasks , the ideal input feature would be invariant to multiple confounding properties ( such as illumination and viewing angle , in computer vision applications ) . recently , deep_architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features . however , it is difficult to evaluate the learned features by any means other than using them in a classifier . in this paper , we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different input transformations . we find that stacked autoencoders learn modestly increasingly invariant_features with depth when trained on natural_images . we find that convolutional deep_belief_networks learn substantially more invariant_features in each layer . these results further justify the use of " deep " vs . " shallower " representations , but suggest that mechanisms beyond merely stacking one autoencoder on top of another may be important for achieving invariance . our evaluation met-rics can also be used to evaluate future work in deep_learning , and thus help the development of future algorithms . 
systems_engineering principles for the design of biomedical signal_processing systems systems_engineering aims to produce reliable systems which function according to specification . in this paper we follow a systems_engineering approach to design a biomedical signal_processing system . we discuss requirements capturing , specification definition , implementation and testing of a classification system . these steps are executed as formal as possible . the requirements , which motivate the system design , are based on diabetes research . the main requirement for the classification system is to be a reliable component of a machine which controls diabetes . reliability is very important , because uncontrolled diabetes may lead to hyperglycaemia ( raised blood_sugar ) and over a period of time may cause serious damage to many of the body systems , especially the nerves and blood_vessels . in a second step , these requirements are refined into a formal csp b model . the formal model expresses the system functionality in a clear and semantically strong way . subsequently , the proven system model was translated into an implementation . this implementation was tested with use cases and failure cases . formal modeling and automated model_checking gave us deep insight in the system functionality . this insight enabled us to create a reliable and trustworthy implementation . with extensive_tests we established trust in the reliability of the implementation . 
segmentation and interpretation of mr_brain_images : an improved active_shape_model this paper reports a novel method for fully automated segmentation that is based on description of shape and its variation using point distribution models ( pdm's ) . an improvement of the active shape procedure introduced by cootes and taylor to find new examples of previously learned shapes using pdm's is presented . the new method for segmentation and interpretation of deep neuroanatomic_structures such as thalamus , putamen , ventricular system , etc . incorporates a priori knowledge about shapes of the neuroanatomic_structures to provide their robust segmentation and labeling in magnetic_resonance ( mr ) brain_images . the method was trained in eight mr_brain_images and tested in 19 brain_images by comparison to observer-defined independent standards . neuroanatomic_structures in all testing images were successfully identified . computer-identified and observer-defined neuroanatomic_structures agreed well . the average labeling error was 7%+/-3% . border positioning errors were quite small , with the average border positioning error of 0 . 8+/-0 . 1 pixels in 256 x 256 mr_images . the presented method was specifically developed for segmentation of neuroanatomic_structures in mr_brain_images . however , it is generally applicable to virtually any task involving deformable shape analysis . 
multi-language programming with ada building complex applications often requires putting together pieces of software or requirements that have not been made to work together in the first place . thinking of a project with a high integrity kernel written in ada , using a set of low_level libraries and drivers written in c or c++ , with a graphical_interface done in java and unit tests driven by python is not thinking of sciencefiction anymore . it's actual concrete and day-to-day work . unfortunately , having all of these technologies talking to each other is not straightforward , and often requires a deep knowledge of both sides of the technology and extensive manual work . in this tutorial , we'll first study how to interface directly ada with native languages , such as c or c++ . we'll then have a deep look at communications with languages running on virtual_machines , such as java , python and the . net_framework . finally , we'll see how ada can be interfaced with an arbitrary language using a middleware solution , such as soap or corba we ? ll see how the communication can be manually done using low_level features and apis , and how a substantial part of this process can be automated using high_level binding generators . 
aging in language dynamics human languages evolve continuously , and a puzzling problem is how to reconcile the apparent robustness of most of the deep_linguistic structures we use with the evidence that they undergo possibly slow , yet ceaseless , changes . is the state in which we observe languages today closer to what would be a dynamical attractor with statistically stationary properties or rather closer to a non-steady_state slowly evolving in time ? here we address this question in the framework of the emergence of shared linguistic categories in a population of individuals interacting through language games . the observed emerging asymptotic categorization , which has been previously tested--with success--against experimental_data from human languages , corresponds to a metastable state where global shifts are always possible but progressively more unlikely and the response properties depend on the age of the system . this aging mechanism exhibits striking quantitative analogies to what is observed in the statistical_mechanics of glassy systems . we argue that this can be a general scenario in language dynamics where shared linguistic conventions would not emerge as attractors , but rather as metastable states . 
ultra-low_voltage robust design issues in deep- submicron cmos abstracf-design challenges for operating cmos_circuits fabricated in 0 . 13pm and liner technologies at ultra_low-voltages are analyzed . the design goal consists in minimizing energy by reducing v , , while maintaining delay and yield at acceptable levels in the presence of increasing variability of process parameters . first , an estimation model developed to accurately predict operation of bulk-and soi-cmos in subthreshold is described . the relation between yield , energy , delay and device parameter distributions is examined next along with tradeons necessary to achieve the desired performance point . the main objective of minimizing energy is explored for s u m cells by predicting the minimum v , , based on the data_retention voltage , drv , and , acceptable signal_to_noise margins , snm . experimental_data from a 4kb-sr4m test_chip in 0 . 13pm cmos are presented demonstrating a 90% leakage reduction_potential in standby under reduced bias of 250mv . 
can we really do without the support of formal_methods in the verification of large designs ? 1 . the mystery question from the ic industry's standpoint , the incubation of formal_methods for deployment in eda verification flows has been very long and is still occurring . formal_methods applied at functional verification have interpreted different roles-e . g . are they good for proving correctness or for catching bugs in deep behavioral corner cases-have played with different techniques-e . g . bdd's , sat , atpg , symbolic-and finally have federated with simulation for the purpose of achieving coverage closure . further , in the last 10 years a fair number of start-up's have emerged , that have been acquired by major vendors in the meantime , and new start-up's have been appearing also this year . what are the reasons that make the ic industry to accept an unusually long maturation period of the formal methodology & tools and the eda vendors to put money in the basket of their developments ? 2 . whose affair is this ? i lead a group devoted to investigating new verification techniques for system on chip ( soc ) and system in package ( sip ) . the general approach of the design team leaders is to allocate a significant part of the verification budget to random test_pattern_generation ( rtpg ) driven simulation , by the use of popular testbench automation tools , but then sometimes one design team leader fears exposure to unexpected bugs , for a number of different reasons : the team has pioneered a new design configuration never tried before , i . e . it is not possible to find an existing verification ip ( vip ) already proven and working; the design and the verification teams are distributed in the company organization and geographically far away from each others-multiple design teams and multiple verification teams; total quality commitment is becoming ever more demanding in terms of defectiveness margins and some actions have to be taken to improve the verification flow . very often the budget to formal_verification is not planned in advance : usually formal_verification is asked to clear uncertainty areas but the different usage options with respect to simulation are not clearly perceived . in stmicroelectronics , the verifications of a soc or a sip are becoming everyday more challenging . at the 90nm and 65nm technology_nodes , hundred million transistors will be implemented in few tenth square millimeter of silicon . even considering that a fraction of the chip will be occupied by sparse logic , tens of million gates rtl is foreseeable in the next years : no one is going to design such an rtl from 
genetic fuzzy classifier for sleep stage identification soft_computing techniques are commonly used to detect medical phenomena and help with clinical diagnoses and treatment . in this work , we propose a design for a computerized sleep scoring method , which is based on a fuzzy classifier and a genetic algorithm ( ga ) . we design the fuzzy classifier based on the ga using a single electroencephalogram ( eeg ) signal that detects differences in spectral features . polysomnography was performed on four healthy young adults ( males with a mean age of 27 . 5 years ) . the sleep classifier was designed using a sleep record and tested on the sleep records of the subjects . our results show that the genetic fuzzy classifier ( gfc ) agreed with visual sleep staging approximately 84 . 6% of the time in detection of wakefulness ( wa ) , shallow sleep ( ss ) , deep_sleep ( ds ) , and rapid_eye_movement ( rem ) stages . 
on-chip delay measurement for silicon_debug efficient test and debug techniques are indispensable for performance characterization of large complex integrated_circuits in deep_submicron and nanometer_technologies . performance characterization of such chips requires on-chip hardware and efficient debug schemes in order to reduce time to market and ensure shipping of chips with lower defect levels . in this paper we present an on-chip scheme for delay fault detection and performance characterization . the proposed technique allows for accurate measurement of delays of speed paths for speed binning and facilitates a systematic and efficient test and debug scheme for delay_faults . the area_overhead associated with the proposed technique is very low . 
high_dimensional probability estimation with deep density models one of the fundamental problems in machine_learning is the estimation of a probability_distribution from data . many techniques have been proposed to study the structure of data , most often building around the assumption that observations lie on a lower-dimensional manifold of high probability . it has been more difficult , however , to exploit this insight to build explicit , tractable density models for high_dimensional_data . in this paper , we introduce the deep density model ( ddm ) , a new approach to density_estimation . we exploit insights from deep_learning to construct a bijective map to a representation space , under which the transformation of the distribution of the data is approximately factorized and has identical and known marginal densities . the simplicity of the latent distribution under the model allows us to feasibly explore it , and the invertibility of the map to characterize contraction of measure across it . this enables us to compute normalized densities for out-of-sample data . this combination of tractability and flexibility allows us to tackle a variety of probabilistic tasks on high_dimensional datasets , including : rapid computation of normalized densities at test-time without evaluating a partition function; generation of samples without mcmc; and characterization of the joint entropy of the data . 
deep convolutional_neural_networks for smile recognition declaration i herewith certify that all material in this report which is not my own work has been properly acknowledged . abstract this thesis describes the design and implementation of a smile detector based on deep convolutional_neural_networks . it starts with a summary of neural_networks , the difficulties of training them and new training methods , such as restricted boltzmann machines or autoencoders . it then provides a literature_review of convolutional_neural_networks and recurrent_neural_networks . in order to select databases for smile recognition , comprehensive statistics of databases popular in the field of facial_expression_recognition were generated and are summarized in this thesis . it then proposes a model for smile detection , of which the main part is implemented . the experimental results are discussed in this thesis and justified based on a comprehensive model_selection performed . all experiments were run on a tesla k40c gpu benefiting from a speedup of up to factor 10 over the computations on a cpu . a smile detection test accuracy of 99 . 45% is achieved for the denver intensity of spontaneous facial action ( disfa ) database , significantly outperforming existing approaches with accuracies ranging from 65 . 55% to 79 . 67% . this experiment is rerun under various variations , such as retaining less neutral images or only the low or high intensities , of which the results are extensively compared . 3 first and foremost i offer my sincerest gratitude to my supervisor dr . stavros petridis who has supported me throughout my thesis with his enthusiasm , patience and expertise . i would also like to thank professor maja pantic for her passion , setting the direction of this thesis and valuable regular feedback . 
crosstalk driven routing resource assignment crosstalk_noise is one of the emerging issues in deep sub-micrometer technology which causes many undesired effects on the circuit_performance . in this paper , a cdrra algorithm , which integrates the routing layers and tracks to address the crosstalk_noise issue during the track/layer assignment stage , is proposed . the cdrra problem is formulated as a weighted bipartite matching problem and solved using the linear assignment algorithm . the crosstalk risks between nets are represented by an undirected graph and the maximum number of the concurrent crosstalk risking nets is computed as the max-clique of the graph . then the nets in each max-clique are assigned to disadjacent tracks . thus the crosstalk_noise can be avoided based on the clique concept . the algorithm is tested by a set of bench mark examples and the experimental results show that it can improve the final routing layout a lot with little loss of the completion rate . 
an on-chip esd_protection circuit with low trigger voltage in bicmos technology a novel low-trigger dual-direction on-chip electro-static discharge ( esd ) protection circuit is designed to protect integrated_circuits ( ics ) against esd surges in two opposite directions . the compact esd_protection circuit features low triggering voltage ( 7 . 5 v ) , short response time ( 0 . 18 0 . 4 ns ) , symmetric deep-snap-back characteristics , and low on-resistance ( ) . it passed the 14-kv human_body model ( hbm ) esd test and is very area efficient ( 80 v/ m width ) . the new esd_protection design is particularly suitable for low_voltage or multiple-power_supply ic chips . 
taming complexity at maya design maya design is a full-service product_design consultancy offering services at the intersection of computer_science , psychology , and visual design . we have developed efficient techniques for facilitating interdisciplinary design and for communicating clearly with our clients . delivering usability to consumers there is a significant disparity between the state of the art in academic usability design and the average level of usability delivered to consumers . despite a clear understanding of how to improve usability , hopelessly confusing technology products continue to hit the market daily . there is a solid consensus that interdisciplinary collaboration can vastly improve the usability and marketability of complex devices . there remains , however , a scarcity of commercial venues for interdis-ciplinary , user_centered_design . with the exception of a few superb in-house groups at_large technology firms , there are very few opportunities in industry for careers that focus on meaningful interdisciplinary work . as a result , the best and brightest tend to gravitate toward academic work even if their interests are fundamentally oriented toward professional practice rather than research , maya design group was founded in 1989 to address this problem . a spin-off of carnegie_mellon_university , maya was modeled after the great industrial_design consultancies of the 1930s and 1940s . but it was built from the ground up as a place where engineers , human_factors specialists , graphic designers , and industrial designers could practice their disciplines in deep collaboration ( figure 1 ) . this interdisciplinary collaboration allows maya to deliver a wide range of strategic , user-centered product_design services to the technology industries . cognitive_psychology / / figure 1 : deep collaboration among disciplines is maya's recipe for taming complexity . maya specializes in making complicated computer-based products easier for average people to use . our goal is to " tame complexity , " not necessarily to eliminate it . our 23-member staff is more-or-less equally distributed among the three disciplines of engineering and computer sciencq human_factors and cognitive_psychology; and industrial and graphic_design . we work closely with a diverse group of clients , primarily in the high-technology industries , augmenting their in-house capabilities with our areas of expertise . we encourage clients to view maya as a " virtual employee " whom they can rely on for assistance throughout the whole product development cycle , which includes strategic planning , iterative design , rapid_prototyping , user testing , and transition to manufacturing . even when a project is focused on only one or two of our core competencies , our clients benefit from the 
dropout : a simple way to prevent neural_networks from overfitting deep_neural_nets with a large number of parameters are very powerful machine_learning systems . however , overfitting is a serious problem in such networks . large networks are also slow to use , making it difficult to deal with overfitting by combining the predictions of many different large neural_nets at test time . dropout is a technique for addressing this problem . the key idea is to randomly drop units ( along with their connections ) from the neural network during training . this prevents units from co-adapting too much . during training , dropout samples from an exponential number of different " thinned " networks . at test time , it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights . this significantly reduces overfitting and gives major improvements over other regularization methods . we show that dropout improves the performance of neural_networks on supervised_learning tasks in vision , speech_recognition , document_classification and computational_biology , obtaining state-of-the-art results on many benchmark data_sets . 
iddq_testing for cmos vlsi it is little more than 15-years since the idea of iddq_testing was first proposed . many semiconductor companies now consider iddq_testing as an integral part of the overall testing for all ic's . this paper describes the present status of iddq_testing along with the essential items and necessary data related to iddq_testing . as part of the introduction , a historical background and discussion is given on why this test_method has drawn attention . a section on physical_defects with in-depth discussion and examples is used to illustrate why a test_method outside the voltage environment is required . data with additional_information from case_studies is used to explain the effectiveness of iddq_testing . in section iv , design issues , design styles , iddq_test vector_generation and simulation methods are discussed . the concern of whether iddq_testing will remain useful in deep_submicron_technologies is addressed ( section v ) . the use of iddq_testing for reliability screening is described ( section vi ) . the current measurement methods for iddq_testing are given ( section vii ) followed by comments on the economics of iddq_testing ( section viii ) . in section ix pointers to some recent_research are given and finally , concluding remarks are given in section x . keywords burn-in , current measurement , current sensor , current testing , deep_sub_micron_technology , design_for_test , fault_diagnosis , fault_models , ic testing , iddq_testing , physical_defects , reliability screening , reliability testing , semiconductor testing , simulation , system-on-a-chip testing , test economics , test effectiveness , test_vectors . 
performance analysis of a hybrid branch predictor nowadays , one of the main concerns of computer architects is choosing an accurate branch prediction scheme . the gravity of the issue is due to the deep pipeline architectures of contemporary processors . it is reflected in the range and variety of the related research which contains many studies of various prediction schemes and in depth analysis of their performance . the aim of this paper is to study a new hybrid scheme , pythia , based on a set of tested and popular branch predictors . the main idea is to combine different predictors which perform better for different patterns of branch behaviors , measure the pythia performance and compare it with each involved standalone branch predictor using the same benchmarks . the basic difference of pythia in comparison to other combined branch predictors is that polling for the best predictor is performed , using shared_memory , during a trial period and not on the entire program execution . afterwards , the most accurate scheme is kept and the others are deactivated . in that way , we choose the best scheme for the specific branch pattern and additionally we can gain from the resources released by the rejected predictors . 
promoting vicarious learning of physics using deep questions with explanations two experiments explored the role of vicarious " self " explanations in facilitating student learning gains during computer-presented instruction . in exp . 1 , college students with low or high knowledge on newton's laws were tested in four conditions : ( a ) monologue ( m ) , ( b ) questions ( q ) , ( c ) explanation ( e ) , and ( d ) question explanation ( q e ) . those with low pre-experimental knowledge levels showed marginally significant yet consistently greater gains than those with high_levels and condition q e outperformed the other three ( m , q , e ) . among those with high knowledge , the q e presentations actually inhibited learning . in exp . 2 , high_school physics students in standard and honors classes were studied during their introduction to newton's laws . brief ( 12 min ) computer videos that introduced key newtonian concepts preceded teacher presentations in seven daily sessions . both standard and honors students who received q e presentations prior to regular classroom activities learned more in daily sessions than those who received either m or q presentations . it was concluded that when key concepts are introduced in the context of deep questions along with explanations new learning was facilitated both in vicarious environments and in subsequent standard classroom activities . 
intuitive strategy for parameter setting in video_segmentation in this paper , we propose an original framework for an intuitive tuning of parameters in image and video_segmentation algorithms . the proposed framework is very flexible and generic and does not depend on a specific segmentation_algorithm , a particular evaluation metric , or a specific optimization approach , which are the three main components of its block_diagram . this framework requires a manual segmentation input provided by a human operator as he/she would have performed intuitively . this input allows the framework to search for the optimal set of parameters which will provide results similar to those obtained by manual segmentation . on one hand , this allows researchers and designers to quickly and automatically find the best parameters in the segmentation algorithms they have developed . it helps them to better understand the degree of importance of each parameter's value on the final segmentation result . it also identifies the potential of the segmentation_algorithm under study in terms of best possible performance level . on the other hand , users and operators of systems with segmentation components , can efficiently identify the optimal sets of parameters for different classes of images or video sequences . in a large extent , this optimization can be performed without a deep understanding of the underlying algorithm , which would facilitate the exploitations and optimizations in real applications by non-experts in segmentation . a specific implementation of the proposed framework was obtained by adopting a video_segmentation algorithm invariant to shadows as segmentation component , a full reference segmentation quality metric based on a perceptually motivated spatial context , as the evaluation component , and a downhill simplex method , as optimization component . simulation_results on various test_sequences , covering a representative set of indoor and ourdoor video , show that optimal set of parameters can be obtained efficiently and largely improve the results_obtained when compared to a simple implementation of the same segmentation_algorithm with ad_hoc parameter setting strategy . 
iddt test_methodologies for very deep sub-micron cmos_circuits in this paper , we investigate three i ddt-based test_methodologies , double threshold i ddt , delta i ddt , and delayed i ddt , and we compare their effectiveness in the detection of defects in very deep sub-micron random logic circuits . the target defects are resistive_opens and resistive bridges . we present preliminary simulation_results of 49 defects to study the defect sensitivity of each of the three test_methods . this paper reports our preliminary_results on these three test_methods using a relatively small transistor-level sample circuit , and is not intended to imply any feasibility in a production environment . the test_methods presented herein are the subject of a current invention disclosure . 
critical fault-based pattern_generation for screening sdds testing for small_delay_defects ( sdds ) becomes necessary as technology further scales . traditional timing-unaware transition-delay_fault ( tdf ) atpgs are not adequate for detecting sdds due to sensitization of short paths . timing_aware atpgs suffer from multiple paths sensitization limitation and significant test_cost . in this paper , we present a critical fault-based methodology to generate high_quality sdd patterns . by focusing on critical faults , high_quality original pattern repository could be generated applicably with -detect atpg . novel pattern evaluation and selection method is presented to further minimize pattern_count while maintaining the sdd detection ability . finally , top-off atpg is performed to ensure meeting the target fault_coverage . experimental_results_demonstrate that the proposed critical fault-based_method improves long_path sensitization efficiency by 2 . 5x and saves approximately 80% cpu runtime compared with total fault-based_method . comparing with timing_aware_atpg , our pattern set detects equivalent or even more sdds with significantly reduced pattern_count . i . introduction as manufacturing technology scales , the fabricated chips become more vulnerable to timing_related_defects . semiconductor_industry increasingly relies on delay_fault_test for higher defect_coverage . small_delay defect ( sdd ) only introduces a small amount of extra delay to the design making it very difficult to detect . a sdd detected on a short path may not fail the test but is highly possible to cause a failure when it gets sensitized on a long_path in the filed . therefore , sdds require a serious consideration for ensuring product quality and in-field reliability in very deep-submicron regime [1] . traditional tdf atpg is developed to target gross delay_defects thus has a limited ability in meeting the high sdd test_coverage requirement in practice . commercial timing_aware_atpg tools [2] [3] have been developed to address the deficiencies of the traditional tdf atpgs . with timing information , the timing_aware_atpg can target a fault along the path with minimum timing slack . however , it lacks ability in sensitizing multiple long paths through a fault . in addition , it consumes large storage and runtime limit in practice . -detect atpg can be considered as an alternative for sdd detection [4] [5] . the -detect atpg tries to generate patterns to detect each fault times via different paths . however , it is also limited by large pattern_count . to improve the effectiveness of test patterns for screening sdds , several techniques have been proposed in literature . 1 . path_delay_fault ( pdf ) -based_methods [6] [7] [8] , which concentrate on identifying 
efficient on-line testing of fpgas with provable diagnosabilities we present novel and efficient_methods for on-line testing in fpgas . the testing approach uses a roving tester ( rote ) , which has provable diagnosabilities and is also faster than prior fpga testing_methods . we present 1- and 2-diagnosable built-in self-tester ( bister ) designs that make up the rote , and that avoid expensive adaptive diagnosis . to the best of our knowledge , this is the first time that a bister design with diagnosability greater than one has been developed for fpgas . we also develop functional_testing methods that test plbs in only two circuit functions that will be mapped to them ( as opposed to testing plbs in all their operational modes ) as the rote moves across a functioning fpga . simulation results show that our 1-diagnosable bister and our functional_testing technique leads to significantly more accurate ( 98% fault_coverage at a fault/defect density of 10% ) and faster test-and-diagnosis of fpgas than achieved by previous work . the fault coverage of rote is also expected to be high at fault/defect densities of up to 25% using our 1-diagnosable bister and up to 33% using our 2-diagnosable bister . our methods should thus prove useful for testing current very deep submicron fpgas as well as future nano-cmos and molecular_nanotechnology fpgas in which defect densities are expected to be in the 10% range . 
 case_study : an fpso with dp for deep_water applications in the coming years , there will be a growing demand for floating production and storage units ( fpsos ) for ultra deep_waters ( greater than 2000 m [6 , 000 feet] ) . in the gulf_of_mexico , the technical and economical limitations inherent to other type of concepts , the lack of pipeline infrastructure in such deep areas , and the wide acceptance of the fpso concept by shelf authorities will accelerate this process . one of the most critical issues in the design of fpsos for ultra deep_waters will be the selection of the most cost-efficient station keeping system for the specified operational requirements . standard solutions based on internal turret and thruster assisted mooring systems are already being offered by the industry . however , beyond certain water depths , the technical and economical constraints associated with the use of mooring systems may favour other concepts potentially more attractive and cost-efficient , such as a fully dynamically positioned fpso ( dp-fpso ) . this concept marries state-of-the-art fpso technology and latest generation drill ship technology for dynamic_positioning and operation in ultra deep_waters . this system can either be utilised as an early production system or as a full-fledged field development solution . the areas most suited for this application will be the gulf_of_mexico , brazil and west_africa . the paper describes a joint study undertaken by the various companies represented by the authors to develop a design for a fully dynamically positioned fpso for ultra deep_waters . the various technical challenges and regulatory issues for a fully dp-fpso will be identified and solutions to them provided . detailed design information on the vessel design , the dp thruster , power generation and control systems , and the disconnectable turret and riser system shall be provided for a hypothetical field development in ultra deep_water . the system performance shall be illustrated by the results from a comprehensive study involving state-of-the-art computer simulations and model test_program . results and conclusions from a reliability and safety study performed on the system shall also be presented , as well as those from a thorough power_consumption analysis for the geographical areas of interest . finally , a comparison is made between components associated with the stationkeeping system of the dp-fpso and a conventionally turret moored fpso , to provide input for capex/opex estimates which in turn can be used to identify the range of water depths and field development scenarios for which the dp-fpso is commercially feasible . 
accurate , dense , and robust multi-view stereopsis this paper proposes a novel algorithm for multiview stereopsis that outputs a dense set of small rectangular patches covering the surfaces visible in the images . stereopsis is implemented as a match , expand , and filter procedure , starting from a sparse set of matched keypoints , and repeatedly expanding these before using visibility constraints to filter away false matches . the keys to the performance of the proposed algorithm are effective techniques for enforcing local photometric consistency and global visibility constraints . simple but effective methods are also proposed to turn the resulting patch model into a mesh which can be further refined by an algorithm that enforces both photometric consistency and regularization constraints . the proposed approach automatically detects and discards outliers and obstacles and does not require any initialization in the form of a visual hull , a bounding box , or valid depth ranges . we have tested our algorithm on various data_sets including objects with fine surface details , deep concavities , and thin structures , outdoor_scenes observed from a restricted set of viewpoints , and "crowded" scenes where moving obstacles appear in front of a static structure of interest . a quantitative evaluation on the middlebury benchmark shows that the proposed method outperforms all others submitted so far for four out of the six data_sets . 
randomized directed testing ( redirect ) for simulink/stateflow models the simulink/stateflow ( sl/sf ) environment from math-works is becoming the <i>de facto</i> standard in industry for model_based_development of embedded control systems . many commercial tools are available in the market for test_case generation from sl/sf designs; however , we have observed that these tools do not achieve satisfactory coverage in cases when designs involve nonlinear blocks and stateflow blocks occur deep_inside the simulink blocks . the recent past has seen the emergence of several novel techniques for testing large c , c++ and java programs; prominent among them are directed automated random_testing ( dart ) , hybrid concolic_testing and feedback-directed random_testing . we believe that some of these techniques could be lifted to testing of sl/sf based designs; redirect ( randomized directed testing ) , the proposed testing method of this paper , is an attempt towards this direction . specifically , redirect uses a careful combination of the above techniques , and in addition , the method uses a set of pattern-guided heuristics for tackling nonlinear blocks . a prototype tool has been developed and the tool has been applied to many industrial strength case_studies . our experiments indicate that a careful choice of heuristics and certain combinations of random and directed testing achieve better coverages as compared to the existing commercial tools . <sup>1</sup>
adaptive autonomous_navigation of mobile_robots in unknown environments adaptive autonomous_navigation of mobile_robots in unknown environments autonomous_navigation of a mobile_robot is a challenging task . much work has been done in indoor navigation in the last decade . fewer results have been obtained in outdoor robotics . since the early 90's , the global positioning system ( gps ) has been the main navigation system for ships and aircrafts . in open fields , satellite_navigation gives absolute position accuracy . the absolute heading information is also obtained by satellite_navigation when the mobile_robot is in motion . however , the use of gps satellite_navigation is mainly restricted to open areas where at least three satellites can be seen . for example , mobile_robots working in underground or deep open mines cannot use satellite_navigation at all , and in forest or city areas , there are serious limitations to its use . laser_range finder technology has evolved remarkably over the last decade , and offers a fast_and_accurate method for environment modeling . furthermore , it can be used to define robot position and heading relative to the environment . it is obvious that the use of several alternative sensors according to the environment will make the navigation system more flexible . laser_range finder technology is particularly suitable for indoors or feature rich outdoor environments . the goal of this thesis is to develop a multi sensor navigation system for unknown outdoor environments , and to verify the system with a service robot . navigation should be possible in unstructured outdoors as well as indoor environments . the system should use all available sensor information and emphasize those that best suit the particular environment . the sensors considered in this thesis include a scanning laser_range finder , a gps_receiver , and a heading gyro . the main contribution of the thesis is a flexible navigation system developed and tested with a service robot performing versatile tasks in an outdoor environment . the used range matching method is novel and has not been verified earlier in outdoor environments . no unique solution can be guaranteed in the developed map matching_algorithm , although it seems to work well in the practical tests . position and heading errors grow without bound in successive map matchings , which could be referred to as laser odometry . therefore , the position and heading have been corrected by means of global matching when the robot returns to a place it has previously visited . alternatively , structured landmarks have been used for position and heading correction . in field_tests , tree trunks and walls have been used as structured 
rank_aggregation for similar items the problem of combining the ranked preferences of many experts is an old and surprisingly deep problem that has gained renewed importance in many machine_learning , data_mining , and information_retrieval applications . effective rank_aggregation becomes difficult in real_world situations in which the rankings are noisy , incomplete , or even disjoint . we address these difficulties by extending several standard methods of rank_aggregation to consider similarity between items in the various ranked lists , in addition to their rankings . the intuition is that similar items should receive similar rankings , given an appropriate measure of similarity for the domain of interest . in this paper , we propose several algorithms for merging ranked lists of items with defined similarity . we establish evaluation criteria for these algorithms by extending previous definitions of distance between ranked lists to include the role of similarity between items . finally , we test these new methods on both synthetic and real_world data , including data from an application in keywords expansion for sponsored search advertisers . our results show that incorporating similarity knowledge within rank_aggregation can significantly improve the performance of several standard rank_aggregation methods , especially when used with noisy , incomplete , or disjoint rankings . 
evaluation of global_and_local training techniques over feed_forward_neural_network architecture spaces for computer_aided medical_diagnosis in most cases authors are permitted to post their version of the article ( e . g . in word or tex form ) to their personal website or institutional_repository . authors requiring further information regarding elsevier's archiving and manuscript policies are encouraged to visit : a b s t r a c t in this paper , we investigate the performance of global vs . local techniques applied to the training of neu-ral network classifiers for solving medical_diagnosis problems . the presented methodology of the investigation involves systematic and exhaustive evaluation of the classifier performance over a neural_network_architecture space and with respect to training depth for a particular problem . in this study , the architecture space is defined over feed_forward , fully_connected artificial_neural_networks ( anns ) which have been widely used in computer_aided decision_support_systems in medical domain , and for which two popular neural_network training methods are explored : conventional backpropagation ( bp ) and particle_swarm_optimization ( pso ) . both training techniques are compared in terms of classification performance over three medical_diagnosis problems ( breast_cancer , heart_disease , and diabetes ) from pro-ben1 benchmark dataset and computational and architectural analysis are performed for an extensive assessment . the results clearly demonstrate that it is not possible to compare and evaluate the performance of the two algorithms over a single network and with a fixed set of training parameters , as most of the earlier work in this field has been carried out , since training_and_test classification performances vary significantly and depend directly on the network_architecture , the training depth and method used and the available dataset . we , therefore , show that an extensive evaluation method such as the one proposed in this paper is basically needed to obtain a reliable and detailed performance_assessment , in that , we can conclude that the pso algorithm has usually a better generalization_ability across the architecture space whereas bp can occasionally provide better training and/or test classification performance for some network configurations . furthermore , we can in general say that the pso , as a global training algorithm , is capable of achieving minimum test classification errors regardless of the training depth , i . e . shallow or deep , and its average classification performance shows less variations with respect to network_architecture . in terms of computational_complexity , bp is in general superior to pso for the entire architecture space used . artificial_neural_networks ( anns ) are known as ''universal approximators " and ''computational models " with particular characteristics such as the ability 
udsm ( ultra-deep_sub_micron ) -aware post-layout power_optimization for ultra low_power cmos vlsi in this paper , we propose an efficient approach to minimize total power ( switching , short_circuit , and leakage_power ) without performance loss for ultra-low_power cmos_circuits in nanometer_technologies . we present a framework for combining supply/threshold_voltage scaling , gate sizing , and interconnect scaling techniques for power_optimization and propose an efficient heuristic algorithm which ensures that the total slack budget is maximal and the total power is minimal in the presence of back end ( post-layout-based ) udsm effects . we have tested the proposed algorithms on a set of benchmark_circuits and some building blocks of a synthesizable arm core . the experimental results show that our polynomial-time solvable strategy delivers over an order_of_magnitude savings in total power without compromising performance . 
lazy suspect-set computation : fault_diagnosis for deep electrical bugs current silicon test_methods are highly effective at sensitizing and propagating most electrical faults . unfortunately , with ever increasing chip complexity and shorter time-to-market windows , an increasing number of faults escape undetected . to address this problem , we propose a novel technique to help identify hard-to-find electrical faults that are not detected using conventional test_methods , but manifest themselves as observable functional errors during functional test , system test , or during actual use in the field . these faults are too sequentially deep to be diagnosed using simulation , atpg , or formal tools . our technique relies on repeated full-speed chip runs that witness the functional bug , combined with some additional on-chip functional debug support and off_line analysis , to compute a possible set of suspected faults . the technique quickly prunes the suspect set , and for each suspect , it can provide a short test_vector for further analysis . experiments on the itc'99 benchmarks demonstrate the effectiveness of our approach . 
neural decision forests for semantic image labelling in this work we present neural decision forests , a novel approach to jointly tackle data representation-and dis-criminative learning within randomized decision_trees . recent_advances of deep_learning architectures demonstrate the power of embedding representation learning within the classifier an idea that is intuitively supported by the hierarchical nature of the decision forest model where the input space is typically left unchanged during training_and_testing . we bridge this gap by introducing randomized multi_layer perceptrons ( rmlp ) as new split nodes which are capable of learning non-linear , data-specific representations and taking advantage of them by finding optimal predictions for the emerging child nodes . to prevent overfitting , we i ) randomly select the image_data fed to the input layer , ii ) automatically adapt the rmlp topology to meet the complexity of the data arriving at the node and iii ) introduce an 1-norm based regularization that additionally sparsifies the network . the key findings in our experiments on three different semantic image labelling datasets are consistently improved results and significantly compressed trees compared to conventional classification trees . 
the idiosyncrasy of business cycles across eu countries the idiosyncrasy of business cycles across eu countries * this paper analyses the underlying dynamics of business cycles in the eu-15 . existing literature mainly focuses on the comovement of expansion and contraction phases , while this paper seeks to test the idiosyncrasy of business cycles by studying growth pattern and deepness of industrial production . hypotheses are tested using formal statistical_methods while much existing literature in this field rely on judgements of correlation coefficients . the results_obtained here does not give much rise to concern about the possibility of the ecb to choose an appropriate timing and magnitude of changes in monetary_policy in order to satisfy the economic_development in its member countries . * i owe claus thustrup kreiner many thanks a bunch of useful comments and for good discussions of issues dealt with in this paper . a great deal of the conclusions in this study is obtained using the bry-boschan algorithm for matlab . the effort programming this algorithm is not mine , however; therefore also many thanks to mark_watson for kindly providing the algorithm in gauss . address for correspondence : jesper . linaa@econ . ku . dk . the activities of epru ( economic policy research unit ) are financed through a grant from the danish national research foundation . 
[body symbolics of anorectic_women] . aim body_image and its disorder is an important_issue in anorexia nervosa . the aim of the present paper is to approach this issue by a symbol test and to demonstrate the image what anorectic_women create about their own body through several symbols . method 29 , 13-35 year old anorectic and 29 15-35 year old healthy women were tested with jacqueline royer's metamorphosis test . the test is based on the jungian and bourdonian psychology , and examines the dynamic , altering part of the personality through the motivation and identification system . results among anorectic_women--opposed to healthy women--similar answers , symbols appeared , which differ from the vulger answers of the test , and show a plain or deep analogy with the physical and psychical symptoms of anorexia nervosa . 15 . 7% of the answers given to the figure question by anorectic_women have a symbolic reference to "deformed" quality . in the category of material , 6 . 2% of their answers show the symbolics of "fire" and "burning" . 21 . 0% of their answers given to the category of landscape symbolise "withering" and "dying" parts of nature . conclusions the imaginative expression of the similar symbols given by anorectic_women help us to conceive the psychodynamical background and the subjective somatic-psychical meaning of the disease in a more complex way . 
age and gender estimation of unfiltered faces this paper concerns the estimation of facial attributes namely , age and gender from images of faces acquired in challenging , " in the wild " conditions . this problem has received far less attention than the related problem of face_recognition , and in particular , has not enjoyed the same dramatic improvement in capabilities demonstrated by contemporary face_recognition systems . here we address this problem by making the following contributions . first , ( i ) , in answer to one of the key problems of age estimation research absence of data we offer a unique dataset of face_images , labeled for age and gender , acquired by smart_phones and other mobile_devices , and uploaded without manual filtering to online image repositories . we show the images in our collection to be more challenging than those offered by other face-photo benchmarks . ( ii ) we describe the dropout-svm approach used by our system for face attribute estimation , in order to avoid over-fitting . this method , inspired by the dropout learning_techniques now popular with deep_belief_networks , is applied here for training support_vector_machines , to our knowledge , for the first time . finally , ( iii ) , we present a robust face alignment technique which explicitly considers the uncertainties of facial feature detectors . we report extensive_tests analyzing both the difficulty levels of contemporary benchmarks , as well as the capabilities of our own system . these show our method to outperform state-of-the-art by a wide margin . 
arpia : a high-level evolutionary test signal_generator the integrated_circuits design_flow is rapidly moving towards higher description levels . however , test-related activities are lacking behind this trend , mainly since effective fault_models and test signals generators are still missing . this paper proposes arpia , a new simulation_based evolutionary test generator . arpia adopts an innovative high_level fault_model that enables efficient fault simulation and guarantees good correlation with gate_level results . the approach exploits an evolutionary_algorithm to drive the search of effective patterns within the gigantic space of all possible signal sequences . arpia operates on register_transfer_level vhdl descriptions and generates effective test_patterns . experimental results show that the achieved results are comparable or better than those obtained by high_level similar approaches or even by gate_level ones . 1 background in recent years the application_specific_integrated_circuit ( asic ) design_flow experienced radical changes . deep_sub_micron integrated_circuit ( ic ) manufacturing technology is enabling designers to put millions of transistors on a single integrated_circuit . following moore's_law , design_complexity is doubling every 12-18 months . in addition , there is an ever-increasing demand on reducing time to market . with complexity skyrocketing and such a competitive pressure , designing at high_levels of abstraction has become more of a necessity than an option . at the present time , exploiting design-partitioning techniques , register_transfer_level ( rt_level ) automatic logic_synthesis tools can be successfully adopted in many asic design flows . however , not all activities have already migrated from gate-to rt_level and are not yet mature enough to . high_level design_for_testability , testable synthesis and test_pattern_generation are increasing their industrial relevance [1] . during the development of asic , designers would like to be able to foresee its testability before starting the logic_synthesis process . furthermore , rt_level automatic test signals generators are expected to exploit higher_level compact information about design structure and behavior , and to be able to produce more effective sequences within reduced cpu time . the test signals may possibly be completed after synthesis by ad_hoc gate_level tools . 
cultural historical activity_theory the theoretical framework a long-standing challenge in educational research is to describe and explain the complex_dynamics of learning and development that occur in educational settings . this article summarizes ways in which qualitative methods are essential to this enterprise from the perspective of scholars who approach the issues using the theoretical lens of cultural historical activity_theory ( chat ) . after summarizing basic principles of this theoretical approach we provide four examples involving different levels of analysis and methodologies . ( methodology is used to refer to the ensemble of methods that mediate between theoretical statements and data used to evaluate them . ) to some researchers who employ qualitative methods , the very fact that we enter into this topic guided by a theoretical framework disqualifies our claim to be qualitative researchers . smith argues that ''qualitative approaches in psychology are generally engaged with exploring , describing and interpreting the personal and social experiences of participants . an attempt is usually made to understand a small number of participants' own frames of reference or view of the world rather than trying to test a preconceived hypothesis on a large sample'' ( smith , 2003 : 2 ) . our approach involves small samples , and we are interested in participants' own understandings; however , we do operate from a preconceived theoretical base and in that sense we have preconceived hypotheses . moreover , the approach we espouse does not preclude quantifica-tion . however , such quantification is more likely to be used for purposes of comparative analysis of qualitatively different activities ( cole et al . , 1978 ) or summary evaluations of products than for a deep analysis of the process of change ( cf . , hayes , 1997 ) . chat refers to an interdisciplinary approach to studying human learning and development associated with the names of the soviet russian psychologists , l . there has been a lively debate in recent years about the extent to which these three thinkers represent a single theoretical perspective . according to one line of interpretation , those who follow vygotsky have focused attention on processes of mediation , adopting mediated action in context as a basic unit of analysis ( wertsch et al . , 1995 ) . this line of work is often referred to as sociocultural research . by contrast , followers of leontiev are said to choose activity as a basic unit of analysis ( kaptelinin , 1996 ) . for our present purposes , these distinctions are not central and we will treat the differing formulations as expressions of a single family of theoretical commitments . ) the following are some theoretical principles of this approach : 1 . 
beyond the limits of planning theory : response to my critics 'little things' that re-enchant the world i want to thank the editors for giving me this opportunity to respond to the comments on rationality and power made above what follows i shall give my response to their criticisms . i shall focus less on the many positive things they also say about the book . first let me mention , however , that i was particularly happy to learn that the critics are favourable to the depth of detail in the book's case-study of planning in aalborg . this is especially important to me , because during the years when i was working in the archives , doing interviews , making observations , talking with my informants , etc . a nagging question kept resurfacing in my mind . this is a question bound to haunt many carrying out what peattie calls 'dense data case_studies' : 'who will want to learn about a case like this , and in this kind of detail ? ' i wanted the case_study to be particularly dense because i wished to test the thesis that the most interesting phenomena in planning and policy making , and those of most general import , would be found in the most minute and most concrete of details . or to put the matter differently , i wanted to see whether the dualisms general specic and abstract concrete would metamorphose or vanish if i went into sufciently deep detail . following dewey , rorty has perceptively observed that the way to re-enchant the world is to stick to the concrete . nietzsche similarly advocates a focus on 'little things' if we are to understand the problems of politics and social organization , which , needless to say , include problems of planning . both rorty and nietzsche seem right to me . i saw the aalborg case as being made up of the type of concrete , little things they talk about . indeed , i saw the case itself as such a thing , what nietzsche calls a discreet and apparently insignicant truth , which , when closely examined , would reveal itself to be pregnant with paradigms , metaphors and general signicance . that was my thesis , but theses may be wrong and the study could have fallen at on its face . this has not happened , and it is especially satisfying to me to see that this particular aspect the focus on 'little things' is emphasized by many reviewers , including the present ones , as a strength of the aalborg study . 
enhancement of fault_injection techniques using saboteurs and mutants for modification of vhdl code assistant professor , #4 professor keywords hardware implemented fault_injection ( hwifi ) , single_event upsets ( seus ) , unidirectional serial saboteur ( uss ) , n -bit unidirectional serial saboteur ( nuss ) various fault_modeling methods have been proposed for tackling the problem of increasing test_data_volume of contemporary . the test_pattern_generation by random fault_injection does not produce efficient results . so , the proper fault_modeling becomes important since the deep sub micrometer devices are expected to be increasingly sensitive to physical faults . for this reason , fault_tolerance mechanisms are more and more required in vlsi_circuits . so , validating their dependability is a prior concern in the design process . fault_injection techniques based on the use of hardware_description languages offer important advantages with regard to other techniques . first , as this type of techniques can be applied during the design phase of the system , they permit reducing the time-to-market . second , they present high controllability and reach ability . among the different techniques , those based on the use of saboteurs and mutants are especially attractive due to their high fault_modeling capability . however , implementing automatically these techniques in a fault_injection tool is difficult . especially complex are the insertion of saboteurs and the generation of mutants . in this paper , we present new proposals to implement saboteurs and mutants for models in vhdl which are easy-to-automate , and whose philosophy can be generalized to other hardware_description languages . i . introduction the new deep sub micrometer technologies are increasingly sensitive to physical faults , both to those due to external phenomena ( i . e . , transient_faults such as single_event upsets ( seus ) , single_event transient ( sets ) , etc . ) and to internal defects ( i . e . , intermittent and permanent_faults ) . moreover , this sensitivity implies not only a raise of the fault rate , but also an increment of the likelihood of appearing multiple faults [1] [3] . for this reason , the dependability of systems must be analyzed . this analysis can be either the study of the incidence of faults on the system ( called error syndrome analysis ) or checking the design specifications ( called validation ) . the objective of the error syndrome analysis is to detect those parts of the system which are most sensitive to faults , and eventually , to choose the most suitable fault_tolerance mechanisms ( ftms ) . the aim of the validation is to verify that the system and/or its built-in ftms accomplish the design specifications in presence of faults . if the dependability is analyzed at early phases of the design_cycle , both time and money can be saved in the development
direct measures of path_delays on commercial fpga chips we present a general technique for measuring the propagation_delay on the internal wires of fpga chips . the measure is based on the comparison between the operating_frequencies of two ring oscillators that differ only for the structure under test , that is included ( or not ) in the loop . experimental_results are presented for a device of the xilinx xc4000 family . 1 . introduction achieving the timing closure is one of the main challanges in deep_submicron ( dsm ) design . this is mainly due to the great impact of interconnect delay , that is hard to predict during early design steps , since the actual electrical_parameters depend on physical_design , and require complex procedures to be accurately extracted . as a matter of fact , the timing closure is usually achieved either by highly conservative designs , or by several trial and error design iterations . the regular structure of fpgas makes the propagation_delay more predictable , since : i ) ad_hoc models can be derived for each wire segment , ii ) the electrical_parameters can be pre-characterized once for all , and iii ) the routing options are finite . most design flows for fpgas provide accurate delay estimates that can be used either to meet detailed timing constraints or to effectively optimize performance [1 , 2 , 6 , 7] . however , any delay model is necessarily conservative , since it cannot account for intra/inter-chip process_variations and temperature drifts that may cause the actual parameters to differ from the nominal ( typical ) ones . on the other hand , the short re-configuration time of fpgas makes it possible to perform preliminary measurements on the same device the design will be mapped on . such real_world measurement could even be included in an aggressive trial-and-error design loop . in general , timing measures could be used to refine and/or validate delay models . in this paper we propose a general technique for measuring the actual propagation_delay across an arbitrary path within a fpga chip . the technique is based on the comparison between the operating_frequency of a ring_oscillator that includes the path under test , and that of a reference ring_oscillator that does not . both ring oscillators are realized by ( re ) programming the target device . the proposed technique can be used to characterize ( or refine ) the delay models using device-specific information . moreover , similar test_structures could be integrated with the functional logic to grant delay self-monitoring capabilities . 
physicalism from a probabilistic point_of_view physicalism-like other isms-has meant different things to different people . the main physicalistic thesis i will discuss here is the claim that all occurrences supervene on physical occurrences-that the physical events and states of affairs at a time determine everything that happens at that time . this synchronic claim has been discussed most in the context of the mind/body problem , but it is fruitful to consider as well how the supervenience thesis applies to what might be termed the organism/body problem . how are the biological properties of a system at a time related to its physical properties at that time ? philosophers have discussed the meaning of supervenience claims at considerable length , mainly with an eye to getting clear on modal issues . less ink has been spilled on the question of why anyone should believe this physicalistic thesis . in what follows , i'll discuss both the metaphysics and the epistemology of supervenience from a probabilistic point_of_view . the first half of this paper will explore how supervenience claims are related to other issues; these will include the thesis that physics is causally complete , the claim that there are emergent properties , the idea that mental properties are causally efficacious , and the notion that there are scientific laws about supervenient properties that generalize over systems that deploy different physical realizations of the properties in question . . the second half will examine the question of how observational evidence can lend support to supervenience claims . this problem turns out to raise some surprisingly deep issues about the nature of hypothesis_testing in science . 1 1 . preliminaries what , exactly , is the supervenience thesis supposed to mean ? it adverts to " physical " properties , but what are these ? this question apparently leads to a dilemma ( hempel 1969 , hellman 1985 , crane and mellor 1990 ) . if the supervenience thesis presupposes that the list of fundamental magnitudes countenanced by present physics is true and complete , then the claim is almost certainly false , since physics is probably not over as a subject . alternatively , if we interpret " physical_property " to mean what an ideally complete physics would describe , then the supervenience thesis lapses into vagueness . what does it mean to say of some future scientific_theory that it is part of the discipline we now call " physics ? " papineau ( 1990 , 1991 ) suggests one way to address this dilemma . rather than worry about what " physics " means , we should instead exploit the 
[subthalamic_nucleus stimulation using a fisher zd stereotactic frame , mr-ct fusion guidance and peroperative orthogonal radiographs , in parkinson_disease] . background we present the method and results of an original technique to implant electrodes in the subthalamic nucleus ( stn ) to treat parkinson's_disease , based on adaptations of the fisher zd stereotactic frame . methods targets coordinates were calculated after fusion of stereotactic ct_scan and mri images . stn was localized by its theoretical coordinates according to ac-pc and by its direct visualization on t2 images . electrodes were implanted after local_anesthesia , using peroperative multicanal microrecordings and test stimulation . electrodes location was checked by peroperative perpendicular radiographs . to avoid projection of the frame arm on the area of interest on anteroposterior and lateral radiographs , the arm was fixed at 45 degrees from the usual 90 degrees position . this original fixation needed a trigonometric transformation of the x and y stereotactic coordinates . radiopaque markers , fixed on the frame , were identified on the radiographs , allowing the calculation of the stereotactic coordinates of the electrode tip , which were then entered in the stereotactic mri , to check its location from the defined target . results no problem due to adaptations of the frame occurred in the 60 patients . in all cases , peroperative radiographs allowed to confirm the correct location of electrodes . six months after surgery , updrs iii score without medication was decreased by 52% with stimulation "on" . updrs iv items 32 , 33 and 39 scores were decreased by 75 , 7 , 79 , 5 and 72% . daily dopa-equivalent_dose was decreased by 71% . one asymptomatic thalamic hematoma and two wound infections occurred . conclusion this method was efficient and safe to implant deep electrodes . 
high_throughput asynchronous pipelines for fine-grain dynamic datapaths personal use of this material is permitted . however , permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists , or to reuse any copyrighted component of this work in other works , must be obtained from the ieee . abstract this paper introduces several new asynchronous pipeline designs which offer high_throughput as well as low latency . the designs target dynamic datapaths , both dual-rail as well as single-rail . the new pipelines are latch-free and therefore are particularly well-suited for fine-grain pipelining , i . e . , where each pipeline stage is only a single gate deep . the pipelines employ new control structures and protocols aimed at reducing the handshaking delay , the principal impediment to achieving high_throughput in asyn-chronous pipelines . as a test vehicle , a 4-bit fifo was designed using 0 . 6 micron_technology . the results of careful hspice simulations of the fifo designs are very encouraging . the dual-rail designs deliver a throughput of up to 860 million data items per second . this performance represents an improvement by a factor of 2 over a widely-used comparable approach by williams [16] . the new single-rail designs deliver a throughput of up to 1208 million data items per second . 
efficient web harvesting strategies for monitoring deep web_content web_content changes rapidly [18] . in focused web harvesting [17] which aim it is to achieve a complete harvest for a given topic , this dynamic nature of the web creates problems for users who need to access a set of all the relevant web_data to their topics of interest . whether you are a fan following your favorite idol or a journalist investigating a topic , you may need not only to access all the relevant information but also the recent changes and updates . general search_engines like google apply several techniques to enhance the freshness of their crawled data . however , in focused web harvesting , we lack an efficient approach that detects changes for a given topic over time . in this paper , we focus on techniques that can keep the relevant content to a given query up-to-date . to do so , we test four different approaches to efficiently harvest all the changed documents matching a given entity by querying web_search_engines . we define a document with changed content or a newly created or removed document as a changed document . among the proposed change_detection approaches , the fedweb method outperforms the other approaches in finding the changed content on the web for a given query with 20 percent , on average , better performance . 
intelligent diagnostic tutoring using qualitative symptom information an intelligent diagnostic tutor , diag 1 , selects problems adaptively and generates all tutoring dialogs from its analysis of a model of the target system . the learner performs tests on a graphical representation of the target system and calls on diag for assistance when needed . during exercises , the tutoring functions can advise the learner about the implications of particular test outcomes , the rationality of suspecting particular replaceable units , and advisability of performing various diagnostic actions . after exercises , diag can critique the learner's testing strategy , and it can generate and explain an expert diagnostic strategy for the previous fault . a prototype application of a very complex system demonstrates the range of tutoring capabilities achieved by the system . overview fault_diagnosis is one of the most ubiquitous and difficult tasks performed in everyday life , and it is an unavoidable component of a wide range of medical , commercial , industrial , and military operations . from the broadest viewpoint , the diagnostic process is surprisingly equivalent across domains . while the particular costs , risks , and payoffs of testing actions vary greatly over different applications , the diagnostic process can be characterized in a manner that takes these variables into account . consequently , excellent diagnostic strategies can be artificially generated in highly specific domains , based upon a deep generalized conception of the diagnostic reasoning task . it would seem , therefore , that this class of human performance would be a natural and highly amenable subject area for intelligent_tutoring approaches . in fact , a number of intelligent diagnostic systems have been produced , and several have demonstrated excellent instructional power . those approaches based upon expert_systems ( lesgold , eggan , katz , and rat 1992 ) are capable of generating discussions of impressive verbal and technical content . unfortunately , this instructional depth and facility has come at a very high expense in capturing and structuring the expert knowledge . at least two other methodologies have been explored to combat this discouraging cost-to-power relationship : 1 ) structural ttwdel approach ( johnson , norton , duncan , and hunt 1988 ) , in which the target system is specified in terms of its normal input_output structure , and 2 ) fimctional model approach ( towne , munro , pizzini , surmon , and wogulis 1990 ) , in which the target system is modeled in a manner that it can be executed in various normal and abnormal conditions . the structural model approach specifies the target system as a network of system elements , connected via directed paths , and it then reasons about symptoms by assuming that failures will propagate along the 
towards inverse modeling of turbidity_currents : the inverse lock-exchange problem keywords : turbidite modeling flow inversion turbidity_current surrogate management framework a b s t r a c t a new approach is introduced for turbidite modeling , leveraging the potential of computational_fluid_dynamics methods to simulate the flow processes that led to turbidite formation . the practical use of numerical flow simulation for the purpose of turbidite modeling so far is hindered by the need to specify parameters and initial flow conditions that are a priori unknown . the present study proposes a method to determine optimal simulation parameters via an automated optimization process . an iterative procedure matches deposit predictions from successive flow simulations against available localized reference data , as in practice may be obtained from well logs , and aims at convergence towards the best-fit scenario . the final result is a prediction of the entire deposit thickness and local grain_size distribution . the optimization strategy is based on a derivative-free , surrogate-based technique . direct numerical simulations are performed to compute the flow dynamics . a proof of concept is successfully conducted for the simple test_case of a two-dimensional lock-exchange turbidity_current . the optimization approach is demonstrated to accurately retrieve the initial conditions used in a reference calculation . the modern sea_floor in deep_water is in large part composed of turbidites , the deposits of submarine turbidity_currents . turbidity_currents are believed to constitute the principal mechanism of sediment_transport from shallow_water into the deep sea ( see meiburg and kneller , 2010 and references therein ) . over geological time scales , stacked turbidites may accumulate on abyssal plains and local deep_sea basins , or form submarine fans fed by river deltas . deeply buried , sandy turbidite sheets represent an important class of hydrocarbon reservoirs , many of which are situated in deep_water ( weimer and slatt , 2007 ) . many of these turbidites are essentially sheetlike in nature . there is a large body of experimental work on gravity and turbidity_currents that has established the dimensional relations ( see the review by huppert , 2006 ) , and the geometries and properties of their deposits at laboratory scales ( e . although the results of many of these experiments have been explicitly applied to submarine deposits at natural scales , such extrapolations are subject to considerable scaling uncertainties , as pointed out by parsons et al . ( 2007 ) . the interaction of turbidity_currents with the sea_floor , via deposition and erosion of sediment , leads to complex sedimentary features that can only be understood by investigating the flow processes that created 
effects of light curing method and exposure time on mechanical_properties of resin based dental materials objectives the aim of this study was to investigate microhardness and compressive_strength of composite resin ( tetric-ceram , ivoclar vivadent ) , compomer ( compoglass , ivoclar , vivadent ) , and resin modified glass ionomer cement ( fuji ii lc , gc corp ) polymerized using halogen light ( optilux 501 , demetron , kerr ) and led ( bluephase c5 , ivoclar vivadent ) for different curing times . methods samples were placed in disc shaped plastic molds with uniform size of 5 mm diameter and 2 mm in thickness for surface microhardness test and placed in a diameter of 4 mm and a length of 2 mm teflon cylinders for compressive_strength test . for each subgroup , 20 samples for microhardness ( n=180 ) and 5 samples for compressive_strength were prepared ( n=45 ) . in group 1 , samples were polymerized using halogen light source for 40 seconds; in group 2 and 3 samples were polymerized using led light source for 20 seconds and 40 seconds respectively . all data were analyzed by two way analysis of anova and tukey's post-hoc tests . results same exposure time of 40 seconds with a low intensity led was found similar or more efficient than a high intensity halogen light unit ( p> . 05 ) , however application of led for 20 seconds was found less efficient than 40 seconds curing time ( p= . 03 ) . conclusions it is important to increase the light curing time and use appropriate light curing devices to polymerize resin composite in deep cavities to maximize the hardness and compressive_strength of restorative materials . 
debugging machine_learning extended abstract paste the appropriate copyright statement here . acm now supports three different copyright statements : acm copyright : acm holds the copyright on the work . this is the historical approach . license : the author ( s ) retain copyright , but acm receives an exclusive publication license . open_access : the author ( s ) wish to pay for the work to be open_access . the additional fee must be paid to acm . this text field is large enough to hold the appropriate release statement assuming it is single spaced in a sans_serif 7 point font . every submission will be assigned their own unique doi string to be included here . abstract creating a machine_learning solution for a real world problem often requires multiple iterations of investigation and improvement until it reaches satisfactory performance . even after deployment , it is common to discover limitations of the model or changes in the target concept that necessitate modifications to the training data and parameters . however , as of today , there is no common wisdom about what these iterations consist of , nor what debugging tools are needed to aid the investigative process . in this work we present a novel technique to help model developers find the root causes of prediction error on test items ( henceforth 'bugs' ) and so help the developer to fix them . given an observed bug our method aims to identify the training items most responsible for biasing the model towards giving the wrong prediction on the specific test item . this set of training items can aid in discovery of common errors like faulty training labels or poor training_data coverage . our method is applicable over many different learners , including deep_neural_nets with large and complex model representations , as well as many different data types . 
using a periodic square_wave test signal to detect crosstalk_faults deep_submicron technology's advanced high_density and high_speed vlsi has reduced distances between wires and devices . perhaps because of manufacturing defects , parasitic capacitors become important sources of internal circuit noise . in 0 . 18- m technology , for example , two parallel lines that are 240 m long with a spacing of 0 . 23 m will produce a 25-ff parasitic capac-itance in the normal fabrication condition . 1 a manufacturing defect , such as narrower spacing between two conduction lines , will make the parasitic capacitances even larger . noise induced by these parasitic elements interferes with normal vlsi operation by generating unexpected pulses , speeding up or slowing down the transition speed on interconnecting ( victim ) lines when the nearby aggressor line changes state . 2-4 if unexpected pulses appear , and flip_flops catch these pulses during their sampling time , the system will fall into erroneous states . if the slowed-down transitions exceed the flip_flops' clock period , erroneous states also result . many researchers have focused on analysis , testing , and reduction of crosstalk_faults . 2-10 chen et al . simply used kirchoff's voltage law to analyze crosstalk_noise with a three-step approach . 2 lee et al . analyzed crosstalk in both frequency and time domains to gain insight into effects that cause errors . 3 sabet and ilponse considered crosstalk on clock or reset lines and contrived a fault simulator to estimate the fault effect and the detectability of crosstalk_faults . 4 others used proposed algorithms to study circuit timing characteristics . 5-7 bai et al . proposed an at-speed test technique to detect circuit interconnects' crosstalk_faults by generating a six-vector test_sequence , 8 and lai et al . proposed a software_based_self-test_methodology to detect crosstalk_faults on interconnects . 9 several researchers used a timing_analysis technique to help generate test patterns for crosstalk_faults in the desired time window . testing for crosstalk_faults is difficult because they are pattern dependent and highly unpredictable . generating test_patterns to deterministically detect these faults requires a timing_analysis program of high_precision , and this takes much computation time . previously , we proposed a test scheme based on an oscillation square_wave signal . 11 if a crosstalk_fault between two lines exists , applying the square_wave test signal on the aggressor line induces glitches on the victim line that can be detected . this scheme eliminates the need to consider the fault timing issue because any glitches induced ( usually unexpectedly ) by the crosstalk_fault are detectable at the victim line's output . for the work described 
art expertise reduces influence of visual salience on fixation in viewing abstract-paintings when viewing a painting , artists perceive more information from the painting on the basis of their experience and knowledge than art novices do . this difference can be reflected in eye scan paths during viewing of paintings . distributions of scan paths of artists are different from those of novices even when the paintings contain no figurative object ( i . e . abstract paintings ) . there are two possible explanations for this difference of scan paths . one is that artists have high sensitivity to high_level features such as textures and composition of colors and therefore their fixations are more driven by such features compared with novices . the other is that fixations of artists are more attracted by salient features than those of novices and the fixations are driven by low_level features . to test these , we measured eye fixations of artists and novices during the free viewing of various abstract paintings and compared the distribution of their fixations for each painting with a topological attentional map that quantifies the conspicuity of low_level features in the painting ( i . e . saliency map ) . we found that the fixation distribution of artists was more distinguishable from the saliency map than that of novices . this difference indicates that fixations of artists are less driven by low_level features than those of novices . our result suggests that artists may extract visual_information from paintings based on high_level features . this ability of artists may be associated with artists' deep aesthetic appreciation of paintings . 
process_variation aware multi-temperature test_scheduling chips manufactured with deep_submicron_technologies are prone to large process_variation and temperature-dependent defects . in order to provide high test efficiency , the tests for temperature-dependent defects should be applied at appropriate temperature ranges . existing static scheduling techniques achieve these specified temperatures by scheduling the tests , specially developed heating sequences , and cooling intervals together . because of the temperature uncertainty induced by process_variation , a static test schedule is not capable of applying the tests at intended temperatures in an efficient manner . as a result the test cost will be very high . in this paper , an adaptive test_scheduling method is introduced that utilizes on-chip temperature sensors in order to adapt the test schedule to the actual temperatures . the proposed method generates a low cost schedule tree based on the variation statistics and thermal simulations in the design phase . during the test , a chip selects an appropriate schedule dynamically based on temperature sensor readings . a % decrease in the likelihood that tests are not applied at the intended temperatures is observed in the experimental studies in addition to % reduction in test application time . i . introduction temperature-dependent defects are a challenge for achieving_high test_quality for chips manufactured with modern technologies [1] . this entails the need to apply tests within specified temperature ranges and also the necessity of having a variety of tests applied at different temperatures in order to achieve high defect_coverage . therefore , it is important to develop efficient_methods to apply tests at the specified temperatures with a minimal cost [2] . tests could be performed at specified temperatures using a temperature-aware schedule that adjusts the temperature by introducing cooling and heating intervals [2 , 3] . a heating interval is a period when the chip under test is consuming large amount of power that is achieved by test controls . a cooling interval , on the other hand , corresponds to a period with very small power_consumption . heating could be achieved by applying a section of the normally generated test_pattern that has the maximal power or a sequence of patterns that is especially generated to heat up the chip rapidly; while cooling can be simply done by not applying any patterns . this way , multi-temperature tests are performed without costly extra test_equipment ( such as external heating mechanisms ) . the challenge is that the test application time ( which is already long ) could become excessively long , resulting in an extremely high cost of 
cognitive and motivational consequences of adapting an agent metaphor in multimedia learning : do the benefits outweigh the cognitive and motivational consequences of adapting an agent metaphor in multimedia learning : do the benefits outweigh the costs ? this paper reviews a set of studies designed to test the hypothesis that the presence of animated pedagogical agents in multimedia environments can promote deep_learning . this was done by first comparing the learning and motivational outcomes of students who learned in the context of social-agency to students who learned in a more traditional text and graphics context . second , the particular features of the social-agency environment were manipulated to examine which of its attributes ( e . g . , visual and auditory presence , students' interaction , and agents' learning style ) are most important in the promotion of meaningful learning . the theoretical and practical implications of the findings are'discussed . ( contains 23 references . ) ( mes ) reproductions supplied by edrs are the best that can be made from the original document . educational resources information center ( eric ) this document has been reproduced as received from the person or organization originating it . minor changes have been made to improve reproduction quality . points of view or opinions stated in this document do not necessarily represent official oeri position or policy . abstract : what are the cognitive and motivational consequences of adapting an agent metaphor in multimedia learning ? the present paper reviews a set of studies designed to test the hypothesis that the presence of animated pedagogical agents in multimedia environments can promote deep_learning . this was done by first , comparing the learning and motivational outcomes of students who learned in the context of social-agency was to those of students who learned in a more traditional text and graphics context . second , the particular features of the social agency environment were manipulated to examine which of its attributes are most important in the promotion of meaningful learning . the theoretical and practical implications of the findings are discussed . 
a computational analysis to assess the influence of specimen geometry on cleavage fracture_toughness of metallic materials the fracture response of mild_steel in the domain of brittle behavior , i . e . , the cleavage range , has been carefully evaluated using a weakest_link statistical_model , assuming the existence of a distribution of cracked carbide particles in the microstructures . experiments have provided an evidence of both scatter in test_results and the existence of constraints . statistical-based_model to include micromechanics were developed in an attempt to study and analyze the problem . the weibull stress micro-mechanical model was used in this study to quantify the constraint effects . this was done numerically using a constraint function ( g ( m ) ) derived from the weibull stress model . the non-dimensional function ( g ( m ) ) describes the evolution of the effects of constraint loss on fracture_toughness relative to the reference condition , i . e . , plane-strain , small_scale yielding ( ssy ) ( t-stress = 0 ) . single-edge se ( b ) notched bending specimens having different crack lengths , different cross-sections and side-grooves were modeled and the constraint function ( g ( m ) ) was calculated . in this paper , we compare the loss in constraint for both the deep notch and shallow notch specimens for a given cross_section of the single-edge notched bend specimen ( se ( b ) ) . 
pathway_analysis of genetic_markers associated with a functional mri faces paradigm implicates polymorphisms in calcium responsive pathways several lines of evidence suggest that common polygenic variation influences brain_function in humans . combining high_density genetic_markers with brain_imaging techniques is constricted by the practicalities of collecting sufficiently large brain_imaging samples . pathway_analysis promises to leverage knowledge on function of genes to detect recurring signals of moderate effect . we adapt this approach , exploiting the deep information collected on brain_function by fmri methods , to identify molecular pathways containing genetic variants which influence brain activation during a commonly applied experiment based on a face matching task ( n=246 ) which was developed to study neural processing of faces displaying negative emotions . genetic_markers moderately associated ( p<10 ( -4 ) ) with whole brain activation phenotypes constructed by applying principal components to contrast maps , were tested for pathway enrichment using permutation based_methods . the most significant pathways are related to post nmda receptor activation events , driven by genetic variants in calcium/calmodulin-dependent protein_kinase ii ( camk2g , camk2d ) and a calcium-regulated nucleotide exchange factor ( rasgrf2 ) in which all are activated by intracellular calcium/calmodulin . the most significant effect of the combined polygenic model were localized to the left inferior_frontal_gyrus ( p=1 . 03 10 ( -9 ) ) , a region primarily involved in semantic processing but also involved in processing negative emotions . these findings_suggest that pathway_analysis of gwas results derived from principal_component_analysis of fmri data is a promising method , to our knowledge , not previously described . 
a programmable built-in self-test core for embedded_memories testing embedded_memories is becoming an industry-wide concern with the advent of deep_submicron_technology and system-on-chip applications . we present a prototype chip for a programmable built-in self-test ( bist ) design that is used for testing embedded_memories , especially drams . the bist chip supports various memory test algorithms by a novel controller and sequencer design . the area of the core circuit is about 1 , 020 1 , 020 m 2 using a 0 . 6 m cmos_process , and the clock_speed is over 100mhz . 
an imaging hf gpr using stationary antennas : experimental validation over the antarctic ice_sheet ground_penetrating radars ( gpr ) are commonly used on the earth to probe the subsurface and the moderate mass and power resources they require make them a most useful tool in planetary exploration . in most cases , gpr need to be moved and perform soundings at various locations to retrieve the image of the underground layers or reflectors . yet , tapir ( terrestrial and planetary imaging radar ) is an innovative stationary hf gpr that allows to image the reflectors in the subsurface through the processing of measured electric and magnetic components of the reflected waves . this instrument was originally developed in the frame of the netlander project to perform deep soundings of the martian subsurface and has been tested during a validation campaign on the antarctic ice_sheet . combining the corresponding observations and numerical simulations of the operation of the instrument we demonstrate its imaging capability and evaluate its performances . 
the neural-sift feature_descriptor for visual vocabulary object_recognition in computer vision , one area of research which receives a lot of attention is recognizing the semantic content of an image . it's a challenging problem where varying pose , occlusion , scale and differing light conditions affect the ease of recognition . a common approach is to extract local feature descriptors from images and attach object class labels to them , but choosing the best type of feature to use is still an open_problem . some use deep_learning methods to learn to create features during training . others apply local_image_descriptors to extract features from an image . in most cases these algorithms show good performance , however , the downside of these type of algorithms is that they are not trainable by design . after training there is no feedback_loop to update the type of features to extract , while there possibly could be room for improvement . in this thesis , a continuous deep_neural_network feedback system is proposed , which consists of an adaptive neural_network feature_descriptor , the bag of visual words approach , and a neural classifier . two initialization methods for the neural network feature_descriptor were compared , one where it was trained on the popular scale_invariant_feature_transform ( sift ) descriptor output , and one where it was randomly initialized . after initial training , the system propagates the classification error from the neural network classifier through the entire pipeline , updating not only the classifier itself , but also the type of features to extract . the feature_descriptor , before and after additional training , was also applied using a support_vector_machine ( svm ) classifier to test for generalizability . results show that for both initialization methods the feedback system increased accuracy substantially when regular training was not able to increase it any further . the proposed neural-sift feature_descriptor performs better than the sift_descriptor itself even with limited number of training instances . initializing on an existing feature_descriptor is beneficial when not a lot of training_samples are available . however , when there are a lot of training_samples available the system is able to construct a well-performing feature_descriptor when starting in a random state , solely based on classifier feedback . the improved feature_descriptor did not only show improved performance in the setting in which it was trained , but also while using an svm_classifier . however , the improvements were small and were only demonstrated with one other classifier . therefore , more experiments are needed to get a better grip on 
heuristics for broad-coverage natural_language parsing the slot grammar system is interesting for natural_language applications because it can deliver parses with deep grammatical information on a reasonably broad scale . the paper describes a numerical scoring system used in slot grammar for ambiguity resolution , which not only ranks parses but also contributes to parsing efficiency through a parse space pruning algorithm . details of the method of computing parse scores are given , and test_results for the english slot grammar are presented . 
doppler ultrasound venous mapping of the lower limbs background the study aim was to test the accuracy ( intra and interobserver variability ) , sensitivity , and specificity of a simplified noninvasive ultrasound methodology for mapping superficial and deep veins of the lower limbs . methods 62 consecutive patients , aged 62 11 years , were enrolled . all underwent us-examinations , performed by two different investigators , of both legs , four anatomical parts , and 17 veins , to assess the interobserver variability of evaluation of superficial and deep veins of the lower limbs . results overall the agreement between the second versus the first operator was very high in detecting reflux ( sensitivity 97 . 9 , specificity 99 . 7 , accuracy 99 . 5; p = 0 . 80 at mcnemar test ) . the higher ceap classification stages were significantly associated with reflux ( odds_ratio : 1 . 778 , 95% confidence_interval : 1 . 552-2 . 038; p < 0 . 001 ) as well as with thrombosis ( odds_ratio : 2 . 765 , 95% confidence_interval : 1 . 741-4 . 389; p < 0 . 001 ) . thus , our findings show a strict association between the symptoms of venous disorders and ultrasound evaluation results for thrombosis or reflux . conclusion this study demonstrated that our venous mapping protocol is a reliable method showing a very low interobserver variability , which makes it accurate and reproducible for the assessment of the morphofunctional status of the lower limb veins . 
preferential filtering for gravity_anomaly separation we present the preferential filtering method for gravity_anomaly separation based on green equivalent-layer concept and wiener_filter . compared to the conventional upward continuation and the preferential continuation , the preferential filtering method has the advantage of no requirement of continuation_height . the method was tested both on the synthetic gravity data of a model of multiple rectangular prisms and on the real gravity data from a magnetite area in jilin province , china . the results show that the preferential filtering method produced better separation of gravity_anomaly than both the conventional low_pass filtering and the upward continuation . the observed gravity anomalies are the sum of gravity effects of density differences at various depths in the subsurface half space . in order to study a specific geological problem using gravity data , the target anomalies must first be separated from the observed gravity anomalies . in the literature , there are a variety of methods proposed for separating gravity anomalies , is that the shallow-source short-wavelength signals and deep-source long-wavelength signals are both simultaneously upward continued and consequently are both attenuated . after subtracting the upward continued regional signals from the observed anomalies , the resultant residual signals still contain parts of regional signals . it means that the anomaly separation is not complete . to solve this problem , pawlowski ( 1995 ) proposed the preferential continuation method based on green's equivalent layer concepts and wiener filtering principle . this method attenuates shallow-source short-wavelength signals while minimally attenuating deep-source long-wavelength signals . later , xu and zeng ( 2000 ) and meng et al . ( 2009 ) suggested the algorithm of the difference continuation based on the preferential continuation . the algorithm is used to extract certain signals of a given wavelength band , similar to the conventional band-pass filtering . another problem of the conventional upward continuation , which also occurs in the preferential continuation ( pawlowski , 1995 ) and its difference continuation algorithm ( xu and zeng , 2000; meng et al . , 2009 ) , is that the continuation_height must be known . to overcome this problem , zeng et al . ( 2008 ) presented a practical algorithm , based on model studies , to derive an optimum continuation_height by calculating a series of cross-correlations between the upward continuations at two successive heights . the average height of the maximum deflection of these cross_correlation values yields the optimum continuation_height for regional-residual separation . guo et al . ( 2009 ) and meng et al . ( 2009 ) proposed a similar algorithm for estimating the optimum continuation_height of the preferential upward continuation . in this article we attempted to 
time_series analysis using deep feed_forward_neural_networks time_series analysis using deep feed_forward_neural_networks deep_neural_networks can be used for abstraction and as a preprocessing step for other machine_learning classifiers . our goal was to develop methods for a more accurate automated seizure detection . deep_architectures have been used for classification of events , and shown in this research to be an effective way of classifying multichannel high_resolution medical data . the medical data used in this thesis was gathered from an electroencephalograph ( eeg ) used in a hospital setting on seizure patients . to demonstrate the ability of deep_architectures to learn and abstract from input data , the signals from the eeg that contained both seizure and non seizure data were given both as featurized data and raw data to the deep architecture . in addition to the multiple types of data preparation , a patients eeg data was tested not only against their own eeg signal training_data but other patients as well . this study supports the effectiveness of deep feed_forward_neural_networks for usage in the seizure classification scenario , as well as highlights some of the difficulties associated with training deep_neural_networks , as shown through experimental_results . the sooner i'm done with this thing the sooner we can get a kitten . i love you . ii_acknowledgments i would like to thank all of the people who have helped me , while specifically ensuring that i do so in no particular order , as to not offend someone by accident . to help facilitate this , i generated the list components , and modified their order with a cryptographically secure random_number_generator multiple times , while simultaneously poking the cat . when the cat moved , i used that configuration of acknowledgements . i would like to thank : firstly andrea . she didn't get randomized , because she's the reason i want to get this thesis done asap or sooner . you have heard me complain and stress so much about deep_learning you probably never want to hear it again . thank you for everything you mean to me , and all the help you gave me in this thesis ! i love you . secondly , i would like to thank my family . you've supported me through the years , even when there wasn't much of me left to support . if i could beat any of you at fantasy hockey , it would be the perfect family . lab members of adam and tinoosh that we collaborated with getting published at aaai spring symposia and flairs too ! we all 
distinct syndromes of hemineglect . hemineglect was assessed in 34 patients with right-hemisphere stroke using a letter-cancellation task and a line bisection task . no significant_correlation ( r = . 39 ) was found between scores on the two tests . ten patients who showed neglect on the cancellation task but performed normally on line bisection had frontal or deep lesions . eleven patients with posterior lesions deviated rightward on line bisection; several of these had minimal or no cancellation deficit . a nonmotor task involving judgment of a bisected line was also performed abnormally by six patients with line bisection shift , suggesting that such shift does not result from a motor response asymmetry . we propose that separable components of the neglect syndrome may be associated with damage to discrete areas of the nondominant hemisphere . 
transducer development and characterization for underwater acoustic neutrino detection calibration a short bipolar pressure pulse with "pancake" directivity is produced and propagated when an ultra_high energy ( uhe ) neutrino interacts with a nucleus in water . nowadays , acoustic sensor_networks are being deployed in deep seas to detect this phenomenon as a first step toward building a neutrino_telescope . in order to study the feasibility of the method , it is critical to have a calibrator that is able to mimic the neutrino signature . in previous_works the possibility of using the acoustic parametric technique for this aim was proven . in this study , the array is operated at a high_frequency and , by means of the parametric effect , the emission of the low_frequency acoustic bipolar pulse is generated mimicking the uhe neutrino acoustic pulse . to this end , the development of the transducer to be used in the parametric array is described in all its phases . the transducer design_process , the characterization tests for the bare piezoelectric ceramic , and the addition of backing and matching layers are presented . the efficiencies and directivity patterns obtained for both primary and parametric beams confirm that the design of the proposed calibrator meets all the requirements for the emitter . 
optimization of linear placements for wirelength minimization with free sites we study a type of linear placement problem arising in detailed placement optimization of a given cell row in the presence of white-space ( extra sites ) . in this single-row placement problem , the cell order is fixed within the row; all cells in other rows are also fixed . we give the first solutions to the single-row problem : ( i ) a dynamic_programming technique with time complexity o ( m 2 ) where m is the number of nets incident to cells in the given row , and ( ii ) an o ( m log m ) technique that exploits the convexity of the wirelength objective . we also propose an iterative heuristic for improving cell ordering within a row; this can be run optionally before applying either ( i ) or ( ii ) . experimental results show an average of 6 . 5% wirelength improvement on industry test_cases when our methods are applied to the final output of a leading industry placement tool . 1 introduction the linear placement problem ( lpp ) is well-studied in the vlsi physical_design literature , where it has appeared in various guises [15 , 10 , 11 , 6 , 2 , 9 , 13] . traditionally , linear placement seeks to arrange a number of interconnected circuit modules within a row of locations , to minimize some objective . applications of lpp are reported in , e . g . , [10 , 4 , 5] . the lpp problem is np-hard [8] , and so most existing_methods are heuristics ( see [12] for a comprehensive review ) . in this work , we address a variant of the linear placement problem that arises during detailed placement optimization of standard_cell layouts . our context is a " successive-approximation " placement methodology , e . g . , ( 1 ) mixed analytic ( quadratic ) and partitioning-based top-down global placement ( cf . most leading eda vendor tools ) , ( 2 ) row balancing and legalization , ( 3 ) detailed optimization of row assignments ( cf . , for example , the timberwolf placement tool [14] ) , and ( 4 ) detailed routability optimization ( " white-space " management and pin alignment ) within individual rows . step ( 4 ) in this methodology arises because of routing hot-spots and limited porosity of the cell_library : even with deep_submicron multilayer interconnect processes , placers must leave extra space within cell rows so that the layout is routable . our problem differs from the traditional lpp formulation in that ( i ) cells of a given row can be placed with gaps between them , i . e . , the number of legal locations is more than the number of cells , and the layout has " white-space " ; and ( ii ) fixed cells from other rows participate in the net wirelength ( cost ) 
web_service integration platform for polish linguistic resources this paper presents a robust linguistic web_service framework for polish , combining several mature offline linguistic tools in a common online platform . the toolset comprise paragraph- , sentence-and token-level segmenter , morphological analyser , disambiguating tagger , shallow and deep parser , named entity recognizer and coreference resolver . uniform access to processing results is provided by means of a stand-off packaged adaptation of national corpus of polish tei p5-based representation and interchange format . a concept of asynchronous handling of requests sent to the implemented web_service ( multiservice ) is introduced to enable processing large amounts of text by setting up language_processing chains of desired complexity . apart from a dedicated api , a simple web interface to the service is presented , allowing to compose a chain of annotation services , run it and periodically check for execution results , made available as plain xml or in a simple visualization . usage examples and results from performance and scalability tests are also included . 
experimental and numerical study of pressure drop and heat_transfer in a single-phase micro-channel heat sink the pressure drop and heat_transfer characteristics of a single-phase micro-channel heat sink were investigated both experimentally and numerically . the heat sink was fabricated from oxygen-free copper and fitted with a polycarbonate plastic cover plate . the heat sink consisted of an array of rectangular micro-channels 231 lm wide and 713 lm deep . deionized water was employed as the cooling liquid and two heat_flux levels , q 00 eff 100 w=cm 2 and q 00 eff 200 w=cm 2 , defined relative to the planform area of the heat sink , were tested . the reynolds_number ranged from 139 to 1672 for q 00 eff 100 w =cm 2 , and 385 to 1289 for q 00 eff 200 w =cm 2 . the three-dimensional heat_transfer characteristics of the heat sink were analyzed numerically by solving the conjugate heat_transfer problem involving simultaneous determination of the temperature field in both the solid and liquid regions . also presented and discussed is a detailed description of the local and average heat_transfer characteristics of the heat sink . the measured pressure drop and temperature distributions show good agreement with the corresponding numerical predictions . these findings demonstrate that the conventional navier stokes and energy equations can adequately predict the fluid_flow and heat_transfer characteristics of micro-channel heat sinks . 
wire_length prediction using statistical_techniques we address the classic wire_length estimation problem and propose a new statistical wire_length estimation approach that captures the probability distribution_function of net lengths after placement and before routing . the wire_length prediction model was developed using a combination of paramet-ric and non-parametric statistical_techniques . the model predicts not only the length of the net using input parameters extracted from the floorplan of a design , but also probability_distributions that a net with given characteristics obtained after placement will have a particular length . the model is validated using both learn-and-test and resubsti-tution techniques . the model can be used for a variety of purposes , including the generation of a large number of statistically sound and therefore realistic instances of designs . we applied the net models to the probabilistic buffer_insertion problem and obtained substantial improvement in net delay after routing . introduction wire_length has become one of the most critical metrics in physical_design primarily due to the rise of the deep submi-cron era . there is a large number of different parameters and constraints , such as the bounding box of the net , number of routing grids and the grid capacity , total number of nets routed in the vicinity of the pertinent net , that are all potentially relevant , but are typically very hard to capture into consistent wire_length model . hence , estimating an exact value for the wire_length is a very hard problem . we have developed a new wirelength model that uses data that can be extracted once the placement of the designs is completed . in order to build the model we used a combination of parametric and non-parametric techniques [2 , 3] . statistical models and prediction methodology can be used in many ways . for example , one can use the prediction information to evaluate the suitability of a particular floorplan for obtaining final routing where nets satisfy a particular user specified condition . for instance , the goal can be to determine which among a number of competing floorplans is most likely to result in a final design with few long nets or overall small sum of wirelengths . they are also a natural component of the overall probabilistic design_automation methodology . one such probabilistic algorithm is [5] which performs buffer_insertion assuming wire-lengths which are estimated as distributions . we used our models in the proba-bilistic buffer_insertion approach of [5] and obtained massive improvements in net delay ( 40% ) after routing when compared with traditional bounding box strategies [1] . 
deep neural heart_rate_variability analysis despite of the pain and limited accuracy of blood tests for early recognition of cardiovascular_disease , they dominate risk screening and triage . on the other hand , heart_rate_variability is non-invasive and cheap , but not considered accurate enough for clinical practice . here , we tackle heart beat interval based classification with deep_learning . we introduce an end_to_end differentiable hybrid_architecture , consisting of a layer of biological neuron models of cardiac dynamics ( modified fitzhugh nagumo neurons ) and several layers of a standard feed_forward_neural_network . the proposed model is evaluated on ecgs from 474 stable at-risk ( coronary_artery_disease ) patients , and 1172 chest_pain patients of an emergency_department . we show that it can significantly outperform models based on traditional heart_rate_variability predictors , as well as approaching or in some cases outperforming clinical blood tests , based only on 60 seconds of inter-beat intervals . 
design and performance verification of uhpc piles for deep foundations versity is to develop and implement innovative methods , materials , and technologies for improv ing transportation efficiency , safety , and reliability while improving the learning_environment of students , faculty , and staff in transportation-related fi elds . the contents of this report reflect the views of the authors , who are responsible for the facts and the accuracy of the information presented herein . the opinions , findings and conclusions expressed in this publication are those of the authors and not necessarily those of the sponsors . the sponsors assume no liability for the contents or use of the information contained in this document . this report does not constitute a standard , specification , or regulation . the sponsors do not endorse products or manufacturers . trademarks or manufacturers' names appear in this report only because they are considered essential to the objective of the document . 16 . abstract the strategic plan for bridge engineering issued by aashto in 2005 identified extending the service life and optimizing structural systems of bridges in the united_states as two grand challenges in bridge engineering , with the objective of producing safer bridges that have a minimum service life of 75 years and reduced maintenance cost . material deterioration was identified as one of the primary challenges to achieving the objective of extended life . in substructural applications ( e . g . , deep foundations ) , construction materials such as timber , steel , and concrete are subjected to deterioration due to environmental impacts . using innovative and new materials for foundation applications makes the aashto objective of 75 years service life achievable . ultra high_performance concrete ( uhpc ) with compressive_strength of 180 mpa ( 26 , 000 psi ) and excellent durability has been used in superstructure applications but not in geotechnical and foundation applications . this study explores the use of precast , prestressed uhpc piles in future foundations of bridges and other structures . an h-shaped uhpc section , which is 10-in . ( 250-mm ) deep with weight similar to that of an hp10 57 steel pile , was designed to improve constructability and reduce cost . in this project , instrumented uhpc piles were cast and laboratory and field_tests were conducted . laboratory tests were used to verify the moment-curvature response of uhpc pile section . in the field , two uhpc piles have been successfully driven in glacial_till clay soil and load tested under vertical and lateral loads . this report provides a complete set of results for the field investigation conducted on uhpc h-shaped piles . test_results , durability , drivability , and other material advantages over normal concrete and 
towards information_seeking agents we develop a general problem setting for training and testing the ability of agents to gather information efficiently . specifically , we present a collection of tasks in which success requires searching through a partially-observed environment , for fragments of information which can be pieced together to accomplish various goals . we combine deep_architectures with techniques from reinforcement_learning to develop agents that solve our tasks . we shape the behavior of these agents by combining extrinsic and intrinsic rewards . we empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty , and to exploit information they have already acquired . 
on-line testing symposium issues related to on-line testing are increasingly_important in modern electronic systems . in particular , the huge complexity of electronic systems has led to growth in reliability needs in several application domains as well as pressure for low_cost products . there is a corresponding increasing demand for cost_effective on-line testing techniques . these needs have increased dramatically with the introduction of very deep submicron and nanometer_technologies which adversely impact noise_margins and process parameters variations and make integrating on-line testing and fault tolerance mandatory in many modern ics . the international on-line testing symposium ( iolts ) is an established forum for presenting novel ideas and experimental_data on these areas . the symposium also emphasizes on-line testing in the continuous operation of large applications such as wired , cellular and satellite telecommunication , as well as in secure chips . 
on the computational architecture of the neocortex this paper is a sequel to an earlier paper which proposed an active role for the thalamus , integrating multiple hypotheses formed in the cortex via the thalamo-cortical loop . in this paper , i put forward a hypothesis on the role of the reciprocal , topographic pathways between two cortical areas , one often a 'higher' area dealing with more abstract information about the world , the other 'lower' , dealing with more concrete data . the higher area attempts to fit its abstractions to the data it receives from lower areas by sending back to them from its deep pyramidal cells a template reconstruction best fitting the lower level view . the lower area attempts to reconcile the reconstruction of its view that it receives from higher areas with what it knows , sending back from its superficial pyramidal cells the features in its data which are not predicted by the higher area . the whole calculation is done with all areas working simultaneously , but with order imposed by synchronous activity in the various top-down , bottom up loops . evidence for this theory is reviewed and experimental_tests are proposed . a third part of this paper will deal with extensions of these ideas to the frontal_lobe . 
the impact of diversity on online ensemble learning in the presence of concept drift on-line learning_algorithms often have to operate in the presence of concept drift ( i . e . , the concepts to be learnt can change with time ) . this paper presents a new categorization for concept drift , separating drifts according to different criteria into mutually exclusive and non-heterogeneous categories . moreover , although ensembles of learning machines have been used to learn in the presence of concept drift , there has been no deep study of why they can be helpful for that and which of their features can contribute or not for that . as diversity is one of these features , we present a diversity analysis in the presence of different types of drift . we show that , before the drift , ensembles with less diversity obtain lower test errors . on the other hand , it is a good strategy to maintain highly diverse ensembles to obtain lower test errors shortly after the drift independent on the type of drift , even though high diversity is more important for more severe drifts . longer after the drift , high diversity becomes less important . diversity by itself can help to reduce the initial increase in error caused by a drift , but does not provide a faster recovery from drifts in long_term . 
a randomized exhaustive propositionalization approach for molecule classification drug_discovery is the process of designing compounds that have desirable properties , such as activity and non-toxicity . molecule classification techniques are used along this process to predict the properties of the compounds in order to expedite their testing . ideally , the classification rules found should be accurate and reveal novel chemical properties , but current molecule representation techniques lead to less than adequate accuracy and knowledge_discovery . this work extends the propositionalization approach recently proposed for multi-relational data_mining in two ways : it generates expressive attributes exhaustively and it uses randomization to sample a limited set of complex ( " deep " ) attributes . our experimental_tests show that the procedure is able to generate meaningful and interpretable attributes from molecular structural data , and that these features are effective for classification purposes . 
transcribing code-switched bilingual lectures using deep neural networks with unit merging in acoustic_modeling this paper considers the transcription of the widely observed yet less investigated bilingual code-switched speech : the words or phrases of the guest language are inserted within the utterances of the host language , so the languages are switched back and forth within an utterance , and much less data are available for the guest language . two approaches utilizing the deep_neural_network ( dnn ) were tested and analyzed , including using dnn bottleneck features in hmm/gmm ( bf-hmm/gmm ) and modeling context-dependent hmm senones by dnn ( cd-dnn-hmm ) . in both cases the unit merging ( and recovery ) techniques in acoustic mod-eling were used to handle the data imbalance problem . improved recognition accuracies were observed with unit merging ( and recovery ) for the two approaches under different conditions . 
clustering structured web_sources : a schema-based , model_differentiation approach the web has been rapidly " deepened " with the prevalence of databases on-line . on this " deep_web , " numerous sources are structured , providing schema-rich data their schemas define the object domain and its query capabilities . this paper proposes clustering sources by their query_schemas , which is critical for enabling both source selection and query mediation , by organizing sources of with similar query capabilities . in abstraction , this problem is essentially clustering categorical data ( by viewing each query schema as a transaction ) . our approach hypothesizes that " homogeneous sources " are characterized by the same hidden generative_models for their schemas . to find clusters governed by such statistical distributions , we propose a novel objective_function , model_differentiation , which employs principled hypothesis_testing to maximize statistical heterogeneity among clusters . our evaluation_shows that , on clustering the web query_schemas , the model_differentiation function outperforms existing ones with the hierarchical agglomerative clustering algorithm . 
malaria control in zambia and southern_africa . the purpose of this article is to review the spectrum of image_based diagnostic tools used in the investigation of suspected deep_vein_thrombosis ( dvt ) . summary of the experience gained by the author as well as relevant publications , regarding vein imaging_modalities taken from a computerized database , was reviewed . the imaging_modalities reviewed include phlebography , color doppler duplex ultrasonography ( cddus ) , computerized tomography angiography ( cta ) and venography ( ctv ) , magnetic_resonance venography ( mrv ) , and radionuclide venography ( rnv ) . cddus is recommended as the modality of choice for the diagnosis of dvt . a strategy combining clinical score and d_dimer test refines the selection of patients . phlebography is reserved for discrepant noninvasive studies . 
expectations and memory in link search expectations and memory in link search expectations and memory in link search 2 abstract strategies in searching a link from a web page can rely either on expectations of prototypical locations or on memories of earlier visits to the page . what is the nature of these expectations , how are locations of web objects remembered , and how do the expectations and memories control search ? these questions were investigated in an experiment where , in the experimental group , nine experienced users searched links . to obtain information about expectations , users' eye movements were recorded . memory for locations of web objects was tested immediately afterwards . in the control group , nine matched users had to guess the locations of web objects without seeing the page . eye-movement data and control group's guesses both indicated a robust expectation of links residing on the left side of the page . only the location of task-relevant objects could be recollected , indicating that deep_processing is required for memories to become consciously accessible . a comparison between the experimental and control groups revealed that what is represented in memory was not an individual link's location but the approximate locations of link panels . furthermore , it is argued that practice-related decreases in reaction time reflect savings in reprocessing caused by priming . roles for the types of memory in link search are proposed . 
soft_error_rate testing of deep_submicron integrated_circuits soft_errors induced by radiation pose a major challenge for the reliability of complex chips processed in state-of-the-art technologies . this paper reviews soft_error_rate ( ser ) characterization by real-time system-ser testing and by accelerated testing . additionally , we present scaling trends , simulation approaches , and improvement techniques . special attention is given to soft errors in combinational_logic . 
from requirements to code : issues and learning in is students' systems development projects executive summary the computing curricula ( 2005 ) place information_systems ( is ) at the intersection of exact sciences ( e . g . general systems t heory ) , technology ( e . g . computer_science ) , and behavioral sciences ( e . g . sociology ) . this presents particular challenges for teaching_and_learning , as future is professionals need to be equipped with a wide range of analytical and critical_thinking skills that will enable them to solve business problems . in addition , they require technical , strong interper-sonal communication , and team skills to contribute to the successful delivery of software products . at the university of cape t own ( uct ) the capstone course of the is undergraduate curriculum is structured around three main areas : project_management; people management; and implementation . t he theoretical parts of this course introduce the student to important aspects of managing projects and people in the information communication and t echnology ( ict ) project environment . the practical part comprises a group systems development project , which forms a core part of the course and requires students to apply theoretical skills in a real-world context . although the impact of the issues relating to soft_skills on student learning is neither underestimated nor ignored in the course , this paper mainly focuses on the technical issues that are experienced during the life of the projects . students generally experience difficulty in the areas of problem_solving , coding and testing , all of which are required for successful systems development . is students are often less technically oriented than their counterparts in the other computing disciplines and their courses involve less technical content . as a result , they may be inadequately prepared for the technical demands of the project . is professionals must be able to interact with business experts and apply problem_solving skills in developing possible solutions . it is thus reasonable to argue that the completion of a full life cycle of a project provide is students with invaluable experience in testing the effectiveness of their proposed solution . a reflective approach has been applied to the course design , resulting in the development of a framework to sufficiently address the issues of problem_solving , coding , and testing through an action learning cycle . this approach has proved to lead to improved solutions and to encourage deep_learning . it also shows how teaching practices are shaped by looking back reflexively of student learning and the facilitating environments . this paper describes how the course has evolved through four phases , culminating in an approach that guides students material published as part 
application_layer joint coding for image transmission over deep_space channels in this paper a method to realize the joint application_layer coding for image transmission over deep_space channels has been presented . in more technical detail , both image_compression , based on algorithms such as jpeg2000 and ccsds , and encoding techniques , such as ldpc codes , to protect the sent images are simultaneously applied by the proposed mechanism . it acts on the bases of the deep space channel conditions , in terms of bit_error_rate , and it is based on the multi-attribute decision_making theory . in practice , the proposal is aimed at protecting the essential informative contents of images sent through a deep_space_network and , at the same time , allows minimizing the load offered ( the total amount of data to transmit ) by the overall application_layer coding process to the deep_space_network . the presented mechanism has been tested through simulations . the obtained results show the effectiveness of the proposal and open the door to further developments of the method in real systems . i . introduction the communication impairments in deep_space networks ( dsns ) are due to the hostility of the space_environment and , as a consequence , significant efforts to improve the performance of existing communication_systems [1] are required . nowadays , the technological advances allow connecting heterogeneous terminals separated by thousands of kilometers with satisfactory levels of quality , reliability and flexibility . in fact , by exploiting the transmission capacity of the radio channel , it is possible to achieve radio link for satellite systems ( i . e . , geo , meo and leo ) with performance levels comparable to wired-line technologies . 
removal of impulse_noise using fuzzy genetic_algorithm digital_image_processing plays a key role in medical_diagnosis . medical images are obtained and analyzed to determine the presence or absence of abnormalities such as tumor , which is vital in understanding the type and magnitude of a disease . unfortunately , medical images are susceptible to impulse_noise during acquisition , storage and transmission . hence , image de-noising is a primary precursor for medical image_analysis tasks . noise removal can be done much more efficiently by a combination of image filters or a composite filter , than by a single image filter . determining the appropriate filter combination is a difficult task . in this paper , we propose a technique that uses fuzzy genetic_algorithm to find the optimal composite filters for removing all types of impulse_noise from medical images . here , a fuzzy rule base is used to adaptively change the crossover probability of the genetic_algorithm used to determine the optimal composite filters . we use genetic_algorithm ( ga ) to determine composite filters that remove different levels of impulse_noise from an image . in this method , the ga considers a set of possible filter combinations of a particular length , selects the best combinations among them according to a fitness value assigned to each combination based on a fitness_function , and applies genetic operators such as crossover and mutation on the selected combinations to create the next generation of composite filters . we expect that the results of simulations on a set of standard test images for a wide range of noise corruption levels will show that the proposed method output performs standard procedures for impulse_noise removal both visually and in terms of performance measures such as psnr , iqi and tenengrad values . i . introduction image_denoising refers to the recovery of a digital_image that has been contaminated by additive white gaussian_noise ( awgn ) . many scientific data are contaminated with noise , either because of the data_acquisition process , or because of naturally occurring phenomena . a first pre-processing step in analyzing such data is denoising , that is , estimating the unknown signal of interest from the available noisy_data . there are several different approaches to denoise images . the awgn channel is a good model for many satellite and deep space_communication links . since wideband gaussian_noise comes from many natural sources , such as the thermal vibrations of atoms in conductors ( referred to as thermal noise or johnson-nyquist noise ) , shot_noise , black_body
application of functional delay_tests for testing of transition_faults and vice versa rapid advances of semiconductor_technology lead to higher circuit integration as well as higher operating_frequencies . the statistical variations of the parameters during the manufacturing_process as well as physical_defects in integrated_circuits can sometimes degrade circuit_performance without altering its logic functionality . these faults are called delay_faults . in this paper we consider the quality of the tests generated for two types of delay_faults , namely , functional_delay and transition_faults . we compared the test_quality of functional delay_tests in regard to transition_faults and vice versa . we have performed various comprehensive experiments with combinational benchmark_circuits . the experiments exhibit that the test_sets , which are generated according to the functional delay_fault_model , obtain high fault coverages of transition_faults . however , the functional delay_fault coverages of the test_sets targeted for the transition_faults are low . it is very likely that the test_vectors based on the functional delay_fault_model can cover other kinds of the faults . another advantage of test_set generated at the functional level is that it is independent of and effective for any implementation and , therefore , can be generated at early_stages of the design process . rapid advances of semiconductor_technology lead to higher circuit integration as well as higher operating_frequencies . conventional fault_models like the standard single stuck-at model were developed for gate_level logic circuits . regardless of stuck_at_fault mo-del's efficiency for several decades , alternative models need to account for deep_sub_micron manufacturing process_variations [1] . increasing performance requirements of circuits make it difficult to design them with large timing margins . thus imprecise delay modelling , statistical variations of the parameters during the manufacturing_process as well as physical_defects in integrated_circuits can sometimes degrade circuit_performance without altering its logic func-tionality . these faults are called delay_faults . ensuring that the designs meet the performance specifications requires application of delay_tests . however , delay_fault_testing of deep_submicron_designs is a complex task . it requires application of two-vector patterns at the circuit's intended operating speed . the test_application usually requires high_speed testers or it could also be done through built-in self-test [2] . two general types of delay_fault models , the gate delay_fault_model [3 , 4] and the path_delay_fault model [5] , have been used for modelling delay_defects . although the path_delay_fault model is generally considered to be more realistic and effective in modelling physical 
center pivot design for effluent irrigation of agricultural forage crops center pivot design for effluent irrigation of agricultural forage crops balancing the continuous supply of domestic wastewater from effluent treatment plants with fluctuating crop water demands requires unique irrigation design strategies . a key design consideration when utilizing disinfected secondary treated city water is to maximize the re-use of effluent in winter months , when forage crop water demands are low , yet still produce minimal deep percolation . twenty-seven center pivots in palmdale , california required new custom-designed sprinkler packages to dispose of approximately 7 , 000 gallons per minute of treated wastewater . through innovative design efforts , extensive testing and field experimentation , a standardized package has been adopted by the county sanitation district of los_angeles county that enables a highly efficient application of re-use city wastewater without groundwater degradation throughout the year . many factors influenced the selection of the sprinkler package introduction currently in the united_states , many locations use reclaimed_water . reclaimed_water is treated effluent which is typically for non-potable uses , such as irrigation . historically , treated effluent from wastewater_treatment facilities was discharged directly into a stream , river , or other natural body of water . however , the continued demand for fresh_water supplies has increased need for reuse of treated wastewater . using reclaimed_water for non-potable use saves potable_water for drinking , since less potable_water will be used for non-potable uses . 
molecular event extraction from link grammar parse_trees in the bionlp'09 shared task we present an approach for extracting molecular events from literature based on a deep_parser , using in a query_language for parse_trees . detected events range from gene_expression to protein localization , and cover a multitude of different entity types , including genes/proteins , binding sites , and locations . furthermore , our approach is capable of recognizing negation and the speculative character of extracted statements . we first parse documents using link grammar ( biolg ) and store the parse_trees in a database . events are extracted using a newly_developed query_language with traverses the biolg linkages between trigger terms , arguments , and events . the concrete queries are learnt from an annotated corpus . on the bionlp shared task test_data , we achieve an overall f1-measure of 32 , 29 , and 30% for tasks 1 , 2 , and 3 , respectively . 
f-structure transfer-based statistical_machine_translation in this paper , we describe a statistical deep_syntactic transfer decoder that is trained fully automatically on parsed bilingual corpora . deep_syntactic transfer rules are induced automatically from the f-structures of a lfg parsed bitext corpus by automatically aligning local f-structures , and inducing all rules consistent with the node alignment . the transfer decoder outputs the n-best tl f-structures given a sl f-structure as input by applying large numbers of transfer rules and searching for the best output using a log-linear_model to combine feature scores . the decoder includes a fully integrated dependency-based tri-gram language_model . we include an experimental evaluation of the decoder using different parsing disambiguation resources for the german data to provide a comparison of how the system performs with different german training_and_test parses . 
complex_network perspective on network dynamics : a study on investment network in vc industry in china from the complex_network perspective , using social_network_analysis methods , we aim to systematically conduct in-deep research on the emergence and evolution of the investment network in the venture_capital industry in china to show the network dynamics within a_12-year period . we develop and test 4 alternative logics of attachment - accumulative advantage , follow-the-trend , multiconnectivity and trust-transfer -- to account for both the structure and dynamics of investment in the venture_capital industry . in addition , we map the network dynamics of the field over the years of 1995--2006 and present the results using network visualization . through systematical analysis of the long_term data in the vc field , we can elaborate the action of vcs from the network perspective , supplement the existing literature on investment motivation of vcs , enrich the social meaning of investment behavior and help to better understand the practical condition in this field . 
powerset s natural_language wikipedia search_engine this demonstration shows the capabilities and features of powerset's natural_language search_engine as applied to the english_wikipedia . powerset has assembled scalable document_retrieval technology to construct a semantic index of the world_wide_web . in order to develop and test our technology , we have released a search product ( at http : //www . powerset . com ) that incorporates all the information from the english_wikipedia . the product also integrates community-edited content from metaweb's freebase database of structured information . users may query the index using keywords , natural_language questions or phrases . retrieval latency is comparable to standard keyword based consumer search_engines . powerset semantic indexing is based on the xle , natural_language_processing technology licensed from the palo_alto research_center ( parc ) . during both indexing and querying , we apply our deep natural_language analysis_methods to extract semantic " facts "-relations and semantic connections between words and concepts-from all the sentences in wikipedia . at query time , advanced search-engineering technology makes these facts available for retrieval by matching them against facts or partial facts extracted from the query . in this demonstration , we show how retrieved information is presented as conventional search_results with links to relevant wikipedia pages . we also demonstrate how the distilled semantic_relations are organized in a browsing format that shows relevant subject/relation/object triples related to the user's query . this makes it easy both to find other relevant pages and to use our search-within-the-page feature to localize additional semantic searches to the text of the selected target page . together these features summarize the facts on a page and allow navigation directly to information of interest to individual users . looking ahead beyond continuous improvements to core search and scaling to much larger collections of content , powerset's automatic extraction of semantic facts can be used to create and extend knowledge resources including lexicons , ontologies , and entity profiles . our system is already deployed as a consumer-search web_service , but we also plan to develop an api that will enable programmatic access to our structured representation of text . 
comparing low and high_fidelity prototypes in mobile_phone evaluation this study compared usability_testing results found with low-and high_fidelity prototypes for mobile phones . the main objective is to obtain deep understanding of usability problems found with different prototyping methods . three mobile phones from different manufactures were selected in the experiment . the usability level of the mobile phones was evaluated by participants who completed a questionnaire consisting of 13 usability factors . incorporating the task-based complexity of the three mobile phones , significant differences in the usability_evaluation for each individual factor were found . suggestions on usability_testing with prototyping technique for mobile phones were proposed . this study tries to provide new evidence to the field of mobile_phone usability research and develop a feasible way to quantitatively evaluate the prototype usability with novices . the comparisons of paper-based and fully functional prototypes led us to realize how significantly the unique characteristics of different prototypes affect the usability_evaluation . the experiment took product complexity into account and made suggestions on choosing proper prototyping technique for testing particular aspects of mobile_phone usability . 
a web_based agent challenges human experts on crosswords ames and puzzles reproduce the complexity of the real_world with " the smallest initial structures " ( minsky 1968 ) , thus making their study important for both theoretical and practical issues . since the birth of artificial_intelligence ( ai ) , games and puzzles have received much attention . the game that has captured most of the attention of computer scientists is chess . games play the role of a laboratory where machines can safely be tested by a direct competition with humans . along with the development of successful game-playing programs , the investigation should also include a methodological discussion on how this performance is achieved . deep_blue heavily relied on computational power joined with a search_algorithm based on static evaluation functions to assess the different configurations of the game . instead of relying on a preprogrammed approach , tesauro conceived td- crosswords are very popular and represent a useful domain of investigation for modern artificial_intelligence . in contrast to solving other celebrated games ( such as chess ) , cracking crosswords requires a paradigm_shift towards the ability to handle tasks for which humans require extensive semantic knowledge . this article introduces webcrow , an automatic crossword solver in which the needed knowledge is mined from the web : clues are solved primarily by accessing the web through search_engines and applying natural_language_processing techniques . in competitions at the european conference on artificial_intelligence ( ecai ) in 2006 and other conferences this web_based approach enabled webcrow to out-perform its human challengers . just as chess was once called " the drosophila of artificial_intelligence , " we believe that crossword systems can be useful drosophila of web_based agents . 
syntactic_features for protein_protein interaction extraction background : extracting protein_protein_interactions ( ppi ) from research_papers is a way of translating information from english to the language used by the databases that store this information . with recent advances in automatic ppi detection , it is now possible to speed up this process considerably . syntactic_features from different parsers for biomedical english text are readily available , and can be used to improve the performance of such ppi_extraction systems . results : a complete ppi system was built . it uses a deep_syntactic parser to capture the semantic meaning of the sentences , and a shallow dependency parser to improve the performance further . machine_learning is used to automatically make rules to extract pairs of interacting proteins from the semantics of the sentences . the results have been evaluated using the aimed corpus , and they are better than earlier published_results . the f-score of the current system is 69 . 5% for cross_validation between pairs that may come from the same abstract , and 52 . 0% when complete abstracts are hidden until final testing . automatic 10-fold_cross_validation on the entire aimed corpus can be done in less than 45 minutes on a single server . we also present some previously unpublished statistics about the aimed corpus , and a short analysis of the aimed representation language . conclusions : we present a ppi_extraction system , using different syntactic parsers to extract features for svm with tree kernels , in order to automatically create rules to discover protein_interactions described in the molecular_biology literature . the system performance is better than other published systems , and the implementation is freely available to anyone who is interested in using the system for academic purposes . the system can help researchers quickly discover reported ppis , and thereby increasing the speed at which databases can be populated and novel signaling_pathways can be constructed . 
a hybrid nano-cmos architecture for defect and fault tolerance as the end of the semiconductor roadmap for cmos approaches , architectures based on nanoscale molecular devices are attracting attention . among several alternatives , silicon nanowires and carbon_nanotubes are the two most promising nanotechnologies according to the itrs . these technologies may enable scaling deep into the nanometer regime . however , they suffer from very defect-prone manufacturing processes . although the reconfigurability property of the nanoscale devices can be used to tolerate high defect rates , it may not be possible to locate all defects . with very high device densities , testing each component may not be possible because of time or technology restrictions . this points to a scenario in which even though the devices are tested , the tests are not very comprehensive at locating defects , and hence the shipped chips are still defective . moreover , the devices in the nanometer range will be susceptible to transient_faults which can produce arbitrary soft_errors . despite these drawbacks , it is possible to make nanoscale architectures practical and realistic by introducing defect and fault tolerance . in this article , we propose and evaluate a hybrid nanowire-cmos architecture that addresses all three problems&#8212;namely high defect rates , unlocated defects , and transient_faults&#8212;at the same time . this goal is achieved by using multiple levels of redundancy and majority voters . a key aspect of the architecture is that it contains a judicious balance of both nanoscale and traditional cmos components . a companion to the architecture is a compiler with heuristics to quickly determine if logic can be mapped onto partially defective nanoscale elements . the heuristics make it possible to introduce defect-awareness in placement and routing . the architecture and compiler are evaluated by applying the complete design_flow to several benchmarks . 
field_testing of k10 with hydra at nasa_ames_research_center high_resolution hydrogen surface mapping is essential for locating and characterizing water_ice and other hydrogenous volatile deposits in permanently shadowed lunar craters . this is especially important for potential in-situ resource_utilization . although orbital remote_sensing can provide much information , prospecting for near-subsurface resources can only be performed directly on the surface . the small hydra neutron spectrometer has been successfully integrated onto the k10 black planetary rover , operated by the intelligent robotics group at nasa_ames_research_center [1 , 2] . the system was used to assess hydrogen content in an initial set of field_tests at ames . during these tests , we successfully detected and mapped targets of various hydrogen contents and burial depths . the objectives of the exercise were as follows : integrate and operate the hydra neutron spectrometer with the k10 black rover . acquire hydra data as the rover navigates a grid of gps waypoints chosen without prior_knowledge of the target locations . detect and localize near-surface enhanced hydrogen deposits within the rover test area . the k10/hydra rover tests were carried out from sept . 18 20 , 2007 at an unvegetated pad of fill dirt , north of building ta27b at the nasa_ames_research_center . the area is shown in figure 1 . the extent of the test area was approximately 50 meters in the north-south direction , and 20 meters in the east-west direction . within this area holes were excavated . one set of targets consisted of 3x3 foot polyethylene slabs ( each 0 . 5 in thick , stacked 8 deep for an overall thickness of ~10 cm ) , buried at depths of 0 , 5 , 15 and 30 cm . another set of targets consisted of 4x4 ft , 2x2 ft , and 1x1 ft stacks of cut 0 . 5-inch thick drywall ( gypsum ) , each stacked 12 deep for an overall thickness of 15 cm . finally , some excavations were simply back-filled with the excavated material to act as decoys for the test . the locations of the polyethylene and gypsum targets were known only to three personnel involved in the exercise , in order to provide a single-blind test . 
review of "explorer's guide to the semantic_web by thomas b . passin" a book with a dark-gray hat on its cover and the subtitle " how to break code " makes a strong statement . it does not disappoint . it covers many form of exploiting software that you never dared to explore . the book approaches its problem from many security disciplines . it takes on the reverse_engineering angle to break copyright_protection systems or to find software defects . it takes the pentest ( penetration_test ) view when it explores how to attack server-side software , with local and remote attack options . it describes the botnet ( robot network ) master's options when it targets client software problems . it shows how to hide malware ( malicious_software ) via the rootkit approach , diving deep even into flash_memory and evading forensic analysis . the authors also present more conceptual views , such as the root cause of software security problems , 49 attack_patterns , how to craft malicious input , and buffer overflows in all variations . each topic includes a tutorial , sample systems or code , and known exploits using these techniques . if the topic is unfamiliar , the tutorial may be insufficient , but links to further information are provided . the sample code is clear enough to allow smarter scripters to elaborate on it . there are not many details on the known exploits , but a simple web_search on any of the key terms will provide all that are necessary . the book is about 450 pages , and contains eight chapters . the three longest chapters are on reverse_engineering , buffer_overflow , and rootkits . the others are on software , attack_patterns , exploiting server software , and exploiting client software who should read this book ? the authors start by defending why anyone would write such a book . they show that anything they describe has been exploited already . they spell out how it was done , loud and clear . this takes away ignorance . so , if your job is to build secure software systems , to implement license or copyright_protection systems , to pentest systems , or to do forensic analysis , you will benefit from reading the book . ed felten , princeton_university professor of computer science , is quoted on the cover : " it's hard to protect yourself if you don't know what you're up against . " but having that knowledge , after reading this book , may not improve your peace of mind . a . mari n passin , a principal systems_engineer specializing in data_modeling , web_databases , and xml projects . thus , it is not unexpected to find that he covers 
model_based_development : applications by h . s . lahman i picked up this book because i am always interested in learning about new software_design methodologies . i learned structured design by analyzing pl/1 code and object_oriented_design by creating smalltalk and java applications . this book was my first introduction to model_based_development . fundamentally , model_based_development ( mbd ) is not much different than an object_oriented ( oo ) approach . in mbd the focus is on large_scale re-use instead of object re-use . in mbd a deep understanding of the customer's domain is essential in producing a design which will stand the test of time . the developer should look for modeling invariants , groups that are stable in the customer's problem space . the software structure should model the customer's infrastructure . instead of finding a generalized structure which works across many different domains the goal is to find structures relative to the customer's space being analyzed . design for re-use during the evolution of an organization rather than re-use among different organizations . another difference between object_oriented_design and model_based_development is how they handle implementation hiding . mbd is more restrictive in the accessibility of subsystems than oo applications . i like how mbd approaches this better than oo design . i agree with the author that subsystems should be hidden from one another and only interact through well-defined interfaces . when i develop software i don't like passing around handlers all over my application . this is one part of the book where i think the author really hits it on the nose . code should reduce dependencies instead of adding them through implementation hiding . what i enjoyed about this book is that it is not trying to sell the reader on only model_based_development . instead the book discusses the thought processes behind software_design . there is a lot of theory and not a lot of code . for me this was the only negative aspect of the book . i like to see code accompany the discussion about design . it allows me to apply a new design_methodology faster when i can see how it is used in a programming_language . although , the vast uml examples allow the reader to quickly take any object_oriented_language and apply the design principles described in the book . the book is divided into three parts . the first part discusses the history of object_oriented_design principles . there is a pet care center example which really highlights the use of mbd and how to identify subsystems . this example helps build 
parallel-pipeline-based traversal unit for hardware-accelerated ray_tracing in this work , we propose a novel parallel-pipeline traversal unit for hardware_based ray_tracing , which can reduce latency and increase cache locality . owing to the high memory bandwidth and computation requirements of ray_tracing operations such as traversal and intersection tests , recent_studies have focused on the development of a hardware_based traversal and intersection-test unit[nah et al . 2011][lee et al . 2012] . existing hardware engines are based on a single deep pipeline structure that increases the throughput of ray processing per unit time . however , traversal operations involve non_deterministic changes in the states of a ray . therefore , in some cases , the ray may be unnecessarily transferred between pipeline stages , thereby increasing the overall latency . in order to solve this problem , we propose a parallel traversal unit having a pipeline per state . our results show that the proposed system is up to 30% more efficient than a single-pipeline system because it decreases average latency per ray and increases cache efficiency . 
playfully teaching artificial_intelligence by implementing games to undergraduates using term-work , practical and project based_approach is a common practice for teaching ai course . teaching artificial_intelligence is always a great task for teachers of technical institutes . it is common practice to use computer games to be used as a tool to help introduce basic computer_science concepts to the students . digital games could have a much bigger role in learning than just as a motivational tool . in this paper , it is observed the efforts of teaching artificial_intelligence ( ai ) concepts to undergraduate_students of computer engineering and information_technology with games , because game motivate students , which increase retention and helps to become better computer engineers . paper describes a case study of all types of games and/or puzzles inculcate in teaching ai concepts and other searching algorithms to the students . teaching informed searching techniques like generate-and-test , hill_climbing , a-star , ao-star , constraint_satisfaction problems and means-end analysis is easy , if real_life-problems or puzzles are used for explanation . using water-jug problem , eight-puzzle ( sliding tiles ) , money-banana problem or block-world puzzles helps in understanding these informed searching techniques . students really put more efforts in inventing good heuristics functions for above games . minimax algorithm for two player game is complex algorithm which is used in ibm's deep_blue , who defeated world_chess_champion gary kasparov in 1997 is also implemented in lab . our approach in writing and implementing those games or puzzle in understanding concepts better . 
prediction of parkinson's_disease tremor onset using a radial_basis_function neural_network based on particle_swarm_optimization deep_brain_stimulation ( dbs ) has been successfully used throughout the world for the treatment of parkinson's_disease symptoms . to control abnormal spontaneous electrical activity in target brain areas dbs utilizes a continuous stimulation signal . this continuous power draw means that its implanted battery power source needs to be replaced every 18-24 months . to prolong the life span of the battery , a technique to accurately recognize and predict the onset of the parkinson's_disease tremors in human subjects and thus implement an on-demand stimulator is discussed here . the approach is to use a radial_basis_function neural_network ( rbfnn ) based on particle_swarm_optimization ( pso ) and principal_component_analysis ( pca ) with local_field potential ( lfp ) data recorded via the stimulation electrodes to predict activity related to tremor onset . to test this approach , lfps from the subthalamic nucleus ( stn ) obtained through deep_brain electrodes implanted in a parkinson patient are used to train the network . to validate the network's performance , electromyographic ( emg ) signals from the patient's forearm are recorded in parallel with the lfps to accurately determine occurrences of tremor , and these are compared to the performance of the network . it has been found that detection accuracies of up to 89% are possible . performance comparisons have also been made between a conventional rbfnn and an rbfnn based on pso which show a marginal decrease in performance but with notable reduction in computational overhead . 
mr_brain image_segmentation using gaussian multiresolution analysis and the em_algorithm we present a mr image_segmentation algorithm based on the conventional expectation maximization ( em ) algorithm and the multiresolution analysis of images . although the em_algorithm was used in mri brain segmentation , as well as , image_segmentation in general , it fails to utilize the strong spatial_correlation between neighboring pixels . the multiresolution-based image_segmentation techniques , which have emerged as a powerful method for producing high_quality segmentation of images , are combined here with the em_algorithm to overcome its drawbacks and in the same time take its advantage of simplicity . two data_sets are used to test the performance of the em and the proposed gaussian multiresolution em , gmem , algorithm . the results , which proved more accurate segmentation by the gmem algorithm compared to that of the em_algorithm , are represented statistically and graphically to give deep_understanding . 
imaging using ground_penetrating_radar measurements much work has been done toward reconstructing the electrical_parameters of an unknown buried object . many methods , and accompanying results from simulations , have been presented . most research in the area , however , has been in developing imaging algorithms that have been tested primarily through computer simulation . this paper presents a newly constructed facility for ground_penetrating_radar experiments , as well as the proposed applications for such a facility . the test site consists of a sand-filled volume approximately 15 feet long , 13 feet wide , and 7 feet deep . the experimental setup consists of transmitters and receivers in several common configurations , such as the offset vrp , cross-borehole , and surface-to-surface configurations . the motivations for such an experimental site are presented , as well as the imaging algorithms intended for application . these algorithms include migration techniques , sar processing , born-iterative_method , and diffraction tomography . 
decision_making based on cohort scores for speaker_verification decision_making is an important component in a speaker_verification system . for the conventional gmm-ubm architecture , the decision is usually conducted based on the log likelihood_ratio of the test utterance against the gmm of the claimed speaker and the ubm . this single-score decision is simple but tends to be sensitive to the complex variations in speech signals ( e . g . text content , channel , speaking style , etc . ) . in this paper , we propose a decision_making approach based on multiple scores derived from a set of cohort gmms ( cohort scores ) . importantly , these cohort scores are not simply averaged as in conventional cohort methods; instead , we employ a powerful discriminative model as the decision maker . experimental results show that the proposed method delivers substantial performance_improvement over the baseline system , especially when a deep neural network ( dnn ) is used as the decision maker , and the dnn input involves some statistical features derived from the cohort scores . i . introduction speaker_verification aims to verify claimed identities of speakers , and has gained great popularity in a wide range of applications including access_control , forensic evidence provision and user_authentication . after decades of research , lots of popular speaker_verification approaches have been proposed , such as gaussian mixture_model-universal background model ( gmm-ubm ) [1] , joint factor_analysis ( jfa ) [2] and its 'simplified' version , the i-vector model [3] . accompanied with these models , various back-end techniques have also been proposed to promote the discriminative capability for speakers , such as within-class covariance normalization ( wccn ) [4] , nuisance attribute projection ( nap ) [5] and probabilistic lda ( plda ) [6] , etc . these methods have been demonstrated to be highly successful . recently , deep_learning has been applied to speaker_verification and gained much interest [7] , [8] . within a speaker_verification system , decision_making is an important component [9] . to make a decision , the verification system first determines a score for the test utterance that reflects the confidence that the utterance is from the claimed speaker , and then compares the score with a predefined threshold . in a typical gmm-ubm system , the score is often computed as the log likelihood_ratio that the test utterance being generated from the gmm of the claimed speaker and
dense associative_memory for pattern_recognition a model of associative_memory is studied , which stores and reliably retrieves many more patterns than the number of neurons in the network . we propose a simple duality between this dense associative_memory and neural_networks commonly used in deep_learning . on the associative_memory side of this duality , a family of models that smoothly interpolates between two limiting cases can be constructed . one limit is referred to as the feature-matching mode of pattern_recognition , and the other one as the prototype regime . on the deep_learning side of the duality , this family corresponds to feedforward_neural_networks with one hidden_layer and various activation functions , which transmit the activities of the visible neurons to the hidden_layer . this family of activation functions includes logistics , rectified linear units , and rectified polynomials of higher degrees . the proposed duality makes it possible to apply energy-based intuition from associative_memory to analyze computational properties of neural_networks with unusual activation functions the higher rectified polynomials which until now have not been used in deep_learning . the utility of the dense memories is illustrated for two test_cases : the logical gate xor and the recognition of handwritten digits from the mnist data_set . 
" hug_machine " -deep pressure stimulation this paper is discussing the effects of the_hug_machine on people who have autism , mainly with children who have the medical condtion . studies and experiements have shown that when a patient has regular session with the machine have positive affects and are able to handle their symptoms better . autism is very preverlent problem in our society today . 1 out of every 88 child in america is diagnosed with autism and this disease has seen a tenfold increase over the past 40 years . who knows how far the disease increase in the future . the disorders vary in many differnet ways from difficuttly social_interaction , verbal and nonverbal_communication , and repetitive behaviors . some though with autism excel at school , music , or art even though their social_skills suffer . the_hug_machine provides help for these problems that they have to face everyday . children with autism are more like to be have a lot of stress and anxeity from their environment stressors this stressors are the real reason we have the_hug_machine today . the_hug_machine provides a source of calm and relaxation sesation for those with autism . the device was invented by temple grandin in 1965 . she was an adult fighting the battle of autism and had to suffer with severe stress and anxiety problems . she had the need to be held or touch by another person , but the stress and anxiety when she tried to interact with others was to great for her to handle . then one day she was observing cattle being branded on her aunt's ranch in arizona . she notice once the cattle was in the sqeeze chute it seem to calm down almost instaneously . from this experience , she came to the conclusion that when deep pressure is applied a relaxing sensation overcomes the one having the pressure being applied too . with this sudden realization the idea for the_hug_machine was born . a pilot study was performed with the_hug_machine . this study investigated the effects of deep pressure on arousal and anxiety reduction in autism with gradin's device . they took 12 children with autism were randomly assign to either an experimental group or a placebo group . the placebo group would not receive deep pressure but still be in the disengaged hug machince . every test subject received two 20 min sessionsa week over a 6 week period . the results were good . those in the experimental group 
estimating photometric redshifts using genetic_algorithms photometry is used as a cheap and easy way to estimate redshifts of galaxies , which would otherwise require considerable amounts of expensive telescope time . however , the analysis of photometric redshift datasets is a task where it is sometimes difficult to achieve a high classification_accuracy . this work presents a custom genetic_algorithm ( ga ) for mining the hubble_deep_field north ( hdf-n ) datasets to achieve accurate if-then classification rules . this kind of knowledge_representation has the advantage of being intuitively comprehensible to the user , facilitating astronomers' interpretation of discovered knowledge . the ga is tested against the state of the art decision_tree algorithm c5 . 0 [rulequest , 2005] in two datasets , achieving better classification_accuracy and simpler rule sets in both datasets . 
abnormal deep grey_matter development following preterm_birth detected using deformation-based morphometry . preterm_birth is a leading risk_factor for neurodevelopmental and cognitive impairment in childhood and adolescence . the most common known cerebral abnormality among preterm_infants_at term_equivalent age is a diffuse_white_matter abnormality seen on magnetic_resonance ( mr ) images . it occurs with a similar prevalence to subsequent impairment , but its effect on developing neural systems is unknown . mr_images were obtained at term equivalent age from 62 infants born at 24-33 completed weeks gestation and 12 term born controls . tissue damage was quantified using diffusion-weighted imaging , and deformation-based morphometry was used to make a non-subjective survey of the whole brain to identify significant cerebral morphological alterations associated with preterm_birth and with diffuse_white_matter_injury . preterm_infants_at term_equivalent age had reduced thalamic and lentiform volumes without evidence of acute injury in these regions ( t = 5 . 81 , p < 0 . 05 ) , and these alterations were more marked with increasing prematurity ( t = 7 . 13 , p < 0 . 05 for infants born at less than 28 weeks ) and in infants with diffuse_white_matter_injury ( t = 6 . 43 , p < 0 . 05 ) . the identification of deep grey_matter growth failure in association with diffuse_white_matter_injury suggests that white_matter_injury is not an isolated phenomenon , but rather , it is associated with the maldevelopment of remote structures . this could be mediated by a disturbance to corticothalamic connectivity during a critical_period in cerebral development . deformation-based morphometry is a powerful tool for modelling the developing brain in health and disease , and can be used to test putative aetiological factors for injury . 
statistical_post_processing at wafersort - an alternative to burn-in and a manufacturable solution to test limit setting for sub_micron_technologies in sub_micron_cmos processes , it has become increasingly difficult to identify and separate outliers from the intrinsic distribution at test . this is due to the increasing inadequacy of reliability screens such as burn-in and iddq_testing . statistical_post_processing ( spp ) methods have been developed to run off-tester using the raw data generated from automatic_test_equipment ( ate ) and wafersort maps . post_processing modules include advanced iddq tests such as delta_iddq and the nearest neighbor residual ( nnr ) , as well as other non-iddq based reliability-focused modules . this work presents the application and results of spp at lsi logic on 0 . 18um cmos products . challenges of production implementation have been overcome , which include " user definable " adaptive threshold limits , handling multiple data sources , and data_flow management . burn-in data and customer defects per million units ( dpm ) data show a 30-60% decrease in failure_rate with spp implementation with very acceptable yield_loss . the measure of quiescent_current provides extended fault_coverage due to the inherently wide observability of the power supply rails for static cmos designs . however , the static leakage growth for deep_sub_micron_technologies has obscured the defect resolution of iddq_testing . in order to use iddq effectively , methods of variance_reduction in the iddq data_sets are required . two complementary methods of advanced iddq_testing for deep_sub_micron_technologies include delta_iddq and nnr . delta_iddq and similar intra_die based vector methods have been described in the literature , some examples can be found in [1 , 3 , 5 , 6] . a calculated delta is compared to a threshold that is less difficult to set than that needed for traditional iddq_testing due to the reduction in variance of the delta distributions [2] . small iddq test_sets , however , may cause a defect to escape delta_iddq detection . there must be at least one vector activating and one vector not activating a defect to identify the defect with delta_iddq . using an inter-die approach , such as nnr , in a complementary fashion allows those defective die with all vectors activating the defect to be screened . nnr is an effective variance_reduction technique that uses the parametric data from neighboring die locations for predicting test outcomes of a die [2 , 7] . for this implementation , nnr is used on iddq data . an important concept in this work is the complementary method in which delta_iddq and nnr are used as an improvement to iddq_testing . it is possible that a test set 
deep approximation of set_cover_greedy_algorithm for test_set cui p , liu hj . deep approximation of set_cover_greedy_algorithm for test_set . abstract : test_set problem is a np-hard problem with wide applications . set_cover_greedy_algorithm is one of the commonly used algorithms for the test set problem . it is an open_problem if the approximation_ratio 2ln n+1 directly derived from the set_cover_problem can be improved . the generalization of set_cover_greedy_algorithm is used to solve the redundant test_set problem arising in bioinformatics . this paper analyzes the distribution of the times for which the item pairs are differentiated , and proves that the approximation_ratio of the set_cover_greedy_algorithm for test_set can be 1 . 5lnn+0 . 5lnlnn+2 by derandomization method , thus shrinks the gap in the analysis of the approximation_ratio of this algorithm . in addition , this paper shows the tight lower_bound ( 2 o ( 1 ) ) lnn ( 1 ) of the approximation_ratio of set_cover_greedy_algorithm for the weighted redundant test_set with redundancy n 1 . 
characterization of variability in deeply-scaled fully depleted soi devices characterization of variability in deeply-scaled fully depleted soi devices permission to make digital or hard_copies_of all or part of this work for personal or classroom use is granted_without_fee_provided that copies are not made or distributed for profit_or_commercial_advantage_and that copies_bear this notice and the full citation on the first page . to copy otherwise , to republish , to post on servers or to redistribute to lists , requires_prior_specific_permission . abstract scaling of cmos_technology into the deep submicron regime gives rise to process variability , which in turn compromises circuit yield . one of the main sources of variability is random dopant fluctuation ( rdf ) in the channel . fully depleted silicon on insulator technology has been proposed as a promising alternative to bulk cmos , due to it's undoped channel which reduces rdf , as well as due to its better electrostatic control of the channel . a testchip for measurement and analysis of variability in a 22nm fdsoi process has been designed . among other experiments , the tetstchip features an array of 11x11 tiles with variability measurement structures . each tile contains circuits to measure iv and cv device characteristics , ro frequencies and resistor values . scan_chains and multiplexing are employed to enable analysis of a large number of duts with a limited pad count . the goal of this testchip is to characterize variability in fdsoi . this will be achieved by extracting systematic and random variation data from specifically designed test_structures . the focus of this testchip is to decou-ple different sources of random variation in order to electrically measure line edge roughness and silicon thickness variation due to surface_roughness , characterize the effects of source and drain doping , and quantify the contribution of ground planes and back-biasing in fdsoi variability . 
scaling distributed machine_learning with system and algorithm co-design for a lot of important machine_learning_problems , due to the rapid growth of data and the ever increasing model complexity , which often manifests itself in the large number of model parameters , no single machine can solve them fast enough . therefore , distributed optimization and inference is becoming more and more inevitable for solving large_scale machine_learning_problems in both academia and industry . obtaining an efficient distributed implementation of an algorithm , however , is far from trivial . both intensive computational workloads and the volume of data_communication demand careful design of distributed computation systems and distributed machine_learning_algorithms . in this thesis , we focus on the co-design of distributed_computing systems and distributed optimization algorithms that are specialized for large machine_learning_problems . we propose two distributed_computing frameworks : a parameter server framework which features efficient data_communication , and mxnet , a multi-language library aiming to simplify the development of deep_neural_network algorithms . in less than two years , we have witnessed the wide adoption of the proposed systems . we believe that as we continue to develop these systems , they will enable more people to take advantage of the power of distributed_computing to design efficient machine_learning applications to solve large_scale computational problems . leveraging the two computing platforms , we examine a number of distributed optimization_problems in machine_learning . we present new methods to accelerate the training process , such as data partitioning with better locality properties , communication friendly optimization methods , and more compact statistical models . we implement the new algorithms on the two systems and test on large_scale real data_sets . we successfully demonstrate that careful co-design of computing systems and learning_algorithms can greatly accelerate large_scale distributed machine_learning . 
high_performance_computing systems and applications group objective a position in the field of computer_science with special interests in high_performance_computing , performance_analysis , networking and applying computer technology to new areas . conducted performance analysis of parallel adaptive mesh refinement software by designing , implementing and testing scalable instrumentation library . involved fortran90 and c finite_element codes using mpi on sgi/crayt3e and beowulf systems . avionics system engineering group designed improved scheduling_algorithm used in the software simulation of spacecraft control and data systems . implemented the new algorithm , gained experience in posix threads , c++ , tcl , and concurrency issues of parallel discrete_event_simulation . developed mathematical_models to prove superiority of the new scheduling regimen . designed and implemented intelligent front_end annotation ( ifa ) software used in the design of application specific integrated_circuits ( asics ) . reduced number of design iterations resulting in significant cost savings . ifa subsequently copyrighted by nasa , published in nasa journal , and made available to companies in the private_sector . evaluated fiber_optic switching equipment for use in the deep_space_network . developed user_interface used to control next_generation receiver remotely over tcp/ip network . defined root configuration data format used during initialization of digital receivers prior to tracking interplanetary spacecraft . spearheaded committee to provide windows_95/98 networking support and documentation for users connecting to campus network via ethernet , modem , or ibx data line . participated on committee of senior consultants responsible for interviewing prospective new hires , conducting performance evaluations , designing and administering staff training_program and overseeing maintenance and administration of consulting hardware . 
the vita financial_services sales support environment knowledge_based recommender technologies support customers and sales_representatives in the identification of appropriate products and services . these technologies are especially useful for complex and high involvement products such as cars , computers , or financial_services . in this paper we present the vita ( virtualis tanacsado ) financial_services recommendation environment which has been deployed for the fundamenta building and loan association in hungary . on the basis of knowledge_based recommender technologies , vita supports sales dialogs between fundamenta sales_representatives and customers interested in financial_services ( e . g . , loans ) . vita has been developed and is maintained on the basis of an environment which supports automated testing and debugging of knowledge_bases and recommender process definitions . besides presenting the vita environment we focus on reporting empirical results which clearly show the payoffs of the deployed application in terms of time savings in the conduction of sales dialogs . complex and high-involvement products such as financial_services impose increasing challenges on sales_representatives responsible for identifying appropriate recommendations for customers as well as on software developers responsible for the development and maintenance of sales support environments . on the one hand , sales_representatives have to know which services should be recommended in which context , and how those services should be explained . on the other hand , software_engineering departments are facing the challenge of frequent change requests regarding service assortments and the rules specifying how those assortments are presented and offered to a customer . both aspects are major motivations for the application of knowledge_based recommender technologies ( burke 2000 , felfernig 2007 ) which allow a flexible development and maintenance of sales knowledge_bases . knowledge_based recommender technologies exploit deep knowledge about the product domain in order to derive recommendations . in contrast to the online selling of products such as books or interactive selling of complex products requires the provision of intelligent technologies such as personalized dialogs , explanations , or repair actions for inconsistent customer requirements . an effective organizational embodiment of these technologies allows significant time savings for both sales_representatives in the preparation and conduction of sales dialogs and software engineers in the development of the underlying sales support software . from the business perspective , sales_representatives must be effectively supported in the preparation , conduction , and completion of sales dialogs . the overall goal of financial service_providers is to improve the performance of sales_representatives in terms of an increasing number of sold products per year . achieving significant time savings in all those phases is the 
strategic technology directions jet_propulsion_laboratory national_aeronautics_and_space_administration technologies are deemed strategic if they strive to address nasa's and jpl's grand challenges and aspirations . examples of fundamental challenges that we and our technologies will be called upon to address are : what is the concentration of carbon_dioxide and other greenhouse gases that the atmosphere and oceans can absorb without crossing climatic tipping points ? are there other habitable environments and life on other bodies in the solar system and beyond ? what are the structures and properties of other planetary systems ? what is the nature of dark_matter and dark_energy , and can energy be harnessed on earth from them ? many overarching technical challenges await us beyond the present horizon , if we are to respond to these and other goals : how can we test , place , and operate 10m , 20m , 50m , 100m radar/sub-mm/ir/optical apertures in space ? what is the technology and operations path to provide a_10-fold bandwidth increase per decade for the deep_space_network ? how do we provide a_10-fold increase in spacecraft power ? how do we conduct missions to return samples to earth from mars and other large planets , which we must before sending people there ? it is difficult to resist a sense of appreciation and pride of what has been achieved in the half-century since the space era began . technologies developed to support this development , and the value of the science return , have been incalculable . it allowed us to time-stamp the beginning of the universe and appreciate that the kind of matter we are made of comprises only about 4% of it , to learn more about our solar system than all the knowledge humans had distilled since they began to ponder the sky , to understand more about earth and the dynamics of global change from both natural causes and human_activity . strategic technologies identified in this document also represent technology capabilities that the jet_propulsion_laboratory believes are essential to continuing progress in pursuit of nasa's and jpl's mission , national goals , and in addressing global concerns . advancing these technologies , that often push the theoretical limits of performance , is a major challenge , not least because such advances require sustaining an environment of imagination , creativity , and a culture of innovation for decades of dedicated effort at a time when support is provided on much-shorter time scales and sometimes not at all . the balance of what is invested on near-term projects vs . 
federated_search in the wild : the combined power of over a hundred search_engines federated_search has the potential of improving web_search : the user becomes less dependent on a single search provider and parts of the deep web become available through a unified interface , leading to a wider variety in the retrieved search_results . however , a publicly available dataset for federated_search reflecting an actual web environment has been absent . as a result , it has been difficult to assess whether proposed systems are suitable for the web setting . we introduce a new test_collection containing the results from more than a hundred actual search_engines , ranging from large general web_search_engines such as google and bing to small domain_specific engines . we discuss the design and analyze the effect of several sampling methods . for a set of test queries , we collected relevance judgements for the top 10 results of each search_engine . the dataset is publicly available and is useful for researchers interested in resource selection for web_search collections , result merging and size estimation of uncooperative resources . 
ultra-high_throughput string_matching for deep_packet_inspection deep_packet_inspection ( dpi ) involves searching a packet's header and payload against thousands of rules to detect possible attacks . the increase in internet usage and growing number of attacks which must be searched for has meant hardware_acceleration has become essential in the prevention of dpi becoming a bottleneck to a network if used on an edge or core router . in this paper we present a new multi-pattern_matching algorithm which can search for the fixed strings contained within these rules at a guaranteed rate of one character per cycle independent of the number of strings or their length . our algorithm is based on the aho-corasick string_matching algorithm with our modifications resulting in a memory reduction of over 98% on the strings tested from the snort ruleset . this allows the search structures needed for matching thousands of strings to be small enough to fit in the on-chip memory of an fpga . combined with a simple architecture for hardware , this leads to high_throughput and low power_consumption . our hardware implementation uses multiple string_matching engines working in parallel to search through packets . it can achieve a throughput of over 40 gbps ( oc-768 ) when implemented on a stratix 3 fpga and over 10 gbps ( oc-192 ) when implemented on the lower power cyclone 3 fpga . 
teaching robot_motion planning robot_motion planning is a fairly intuitive and engaging topic , yet it is difficult to teach . the material is taught in undergraduate and graduate robotics classes in computer science , electrical_engineering , mechanical_engineering and aeronautical engineering , but at an abstract level . deep_learning could be achieved by having students implement and test different motion_planning strategies . however , a full implementation of motion_planning algorithms by undergraduates is practically impossible in the context of a single class , even by students proficient in programming . by helping undergraduates grasp motion_planning concepts in series of courses designed for increasing advanced levels , we can open the field to young and enthusiastic talent . this cannot be done by asking students to implement motion_planning algorithms from scratch or access thousands of lines of code and just figure out how things work . we present an ongoing project to develop microworld software and a modeling curriculum that supports undergraduate acquisition of motion_planning knowledge and tool use by computer_science and engineering students . 
deep self-taught learning for handwritten_character_recognition recent theoretical and empirical work in statistical machine_learning has demonstrated the importance of learning_algorithms for deep_architectures , i . e . , function classes obtained by composing multiple non-linear transformations . self-taught learning ( exploiting unla-beled examples or examples from other distributions ) has already been applied to deep learners , but mostly to show the advantage of unlabeled examples . here we explore the advantage brought by out-of-distribution examples . for this purpose we developed a powerful generator of stochastic variations and noise processes for character images , including not only affine transformations but also slant , local elastic deformations , changes in thickness , background images , grey level changes , contrast , occlusion , and various types of noise . the out-of-distribution examples are obtained from these highly distorted images or by including examples of object classes different from those in the target test_set . we show that deep learners benefit more from out-of-distribution examples than a corresponding shallow learner , at least in the area of handwritten_character_recognition . in fact , we show that they beat previously_published results and reach human-level performance on both handwritten digit classification and 62-class handwritten_character_recognition . 
systematic defects in deep_sub_micron_technologies defects due to process-design interaction have a systematic nature . therefore they can have a profound impact on yield . the capability to detect ( and correct ) them is a requirement to continue to follow moore's_law . most of the systematic defects are detected during the process development . these defects are detectable with test_structures or visual inspection tools . however some process marginalities will only show-up in the topology of 'real' designs . moreover , these defects are often not detectable with stuck-at testing . we show two examples of process related_defects which could only be detected with more advanced test_methods such as transition_fault testing and low_voltage testing . to correct systematic problems , however , one should not only have the capability to detect defects but also to identify them . our examples show that other tests would have been far more sensitive in detecting systematic issues . therefore the detection of systematic defects gives new requirements to test_suites and can only be achieved with a shift in the position of manufacturing_test . 
autonomic function tests : some clinical applications modern autonomic function tests can non-invasively evaluate the severity and distribution of autonomic_failure . they have sufficient sensitivity to detect even subclinical dysautonomia . standard laboratory testing evaluates cardiovagal , sudomotor and adrenergic autonomic functions . cardiovagal function is typically evaluated by testing heart_rate response to deep_breathing at a defined rate and to the valsalva_maneuver . sudomotor function can be evaluated with the quantitative sudomotor axon reflex test and the thermoregulatory sweat test . adrenergic function is evaluated by the blood_pressure and heart_rate responses to the valsalva_maneuver and to head-up tilt . tests are useful in defining the presence of autonomic_failure , their natural_history , and response to treatment . they can also define patterns of dysautonomia that are useful in helping the clinician diagnose certain autonomic conditions . for example , the tests are useful in the diagnosis of the autonomic neuropathies and distal small fiber neuropathy . the autonomic neuropathies ( such as those due to diabetes or amyloidosis ) are characterized by severe generalized autonomic_failure . distal small fiber neuropathy is characterized by an absence of autonomic_failure except for distal sudomotor failure . selective autonomic_failure ( which only one system is affected ) can be diagnosed by autonomic testing . an example is chronic idiopathic anhidrosis , where only sudomotor function is affected . among the synucleinopathies , autonomic function tests can distinguish parkinson's_disease ( pd ) from multiple system atrophy ( msa ) . there is a gradation of autonomic_failure . pd is characterized by mild autonomic_failure and a length-dependent pattern of sudomotor involvement . msa and pure autonomic_failure have severe generalized autonomic_failure while dlb is intermediate . 
deep distance metric learning with data summarization we present deep stochastic neighbor compression ( dsnc ) , a framework to compress training_data for instance-based_methods ( such as k-nearest neighbors ) . we accomplish this by inferring a smaller set of pseudo-inputs in a new feature_space learned by a deep neural network . our framework can equivalently be seen as jointly learning a nonlinear distance metric ( induced by the deep feature_space ) and learning a compressed version of the training data . in particular , compressing the data in a deep feature_space makes dsnc robust against label noise and issues such as within-class multi-modal distributions . this leads to dsnc yielding better accuracies and faster predictions at test time , as compared to other competing methods . we conduct comprehensive empirical evaluations , on both quantitative and qualitative tasks , and on several benchmark datasets , to show its effectiveness as compared to several baselines . 
the role of cue information in the outcome_density effect : evidence from neural_network simulations and a causal_learning experiment although normatively irrelevant to the relationship between a cue and an outcome , outcome_density ( i . e . its base-rate probability ) affects people's estimation of causality . by what process causality is incorrectly estimated is of importance to an integrative theory of causal_learning . a potential explanation may be that this happens because outcome_density induces a judgement bias . an alternative explanation is explored here , following which the incorrect estimation of causality is grounded in the processing of cue outcome information during learning . a first neural_network simulation shows that , in the absence of a deep_processing of cue information , cue outcome relationships are acquired but causality is correctly estimated . the second simulation shows how an incorrect estimation of causality may emerge from the active processing of both cue and outcome information . in an experiment inspired by the simulations , the role of a deep_processing of cue information was put to test . in addition to an outcome_density manipulation , a shallow cue manipulation was introduced : cue information was either still displayed ( concurrent ) or no longer displayed ( delayed ) when outcome information was given . behavioural and simulation_results agree : the outcome_density effect was maximal in the concurrent condition . the results are discussed with respect to the extant explanations of the outcome_density effect within the causal_learning framework . 
delay/disruption-tolerant network testing using a leo satellite delay/disruption tolerant networking ( dtn ) " bundles " have been proposed for deep-space_communication in the " interplanetary internet . " this paper describes the first dtn bundle protocol testing from space , using the united_kingdom disaster monitoring constellation ( uk-dmc ) satellite in low_earth_orbit ( leo ) . the mismatch problems between the different conditions of the private dedicated space-to-ground link and the shared , congested , ground-to-ground links are discussed . dtn , with its ability to transfer files on a hop-by-hop basis across different subnets , is presented as a technology that can be used to alleviate this problem . we describe our operational testing , as well as test configurations , goals and results , and lessons learned . i . introduction delay/disruption tolerant networking ( dtn ) has been defined as an end_to_end store-and-forward architecture capable of providing communications in highly-stressed network environments . to provide the store-and-forward service , a " bundle " protocol ( bp ) sits at the application_layer of some number of constituent internets , forming a store-and-forward overlay_network [1] . key capabilities of the bp include : custody-based retransmission the ability to take responsibility for a bundle reaching its final destination ability to cope with intermittent connectivity . ability to cope with long propagation_delays . ability to take advantage of scheduled , predicted , and opportunistic connectivity ( in addition to continuous connectivity ) . 
list of publications ( publikationsf rteckning ) ph-d dissertation ( doktorsavhandling ) publications 1 ) ** nilsson mh , t rnqvist a . l , rehncrona s . deep_brain_stimulation in the subthalamic nuclei improves balance performance in patients with parkinson's disease , when tested without anti-parkinsonian medication . parkinson's_disease after long_term treatment with subthalamic_nucleus high_frequency stimulation . and fear of falling in patients with parkinson's disease treated with high_frequency subthalamic stimulation . fear of falling and falls in people with parkinson's_disease treated with deep_brain_stimulation in the subthalamic nuclei . in press : acta neurol scand . 
a direct inversion scheme for deep resistivity sounding data using artificial_neural_networks initialization of model parameters is crucial in the conventional 1d inversion of dc electrical data , since a poor guess may result in undesired parameter estimations . in the present work , we investigate the performance of neural_networks in the direct inversion of dc sounding data , without the need of a priori information . we introduce a two-step network approach where the first network identifies the curve type , followed by the model parameter_estimation using the second network . this approach provides the flexibility to accommodate all the characteristic sounding curve types with a wide range of resistivity and thickness . here we realize a three layer feed_forward_neural_network with fast back propagation learning_algorithms performing well . the basic data_sets for training and testing were simulated on the basis of available deep resistivity sounding ( drs ) data from the crystalline terrains of south_india . the optimum network parameters and performance were decided as a function of the testing error convergence with respect to the network training error . on adequate training , the final weights simulate faithfully to recover resistivity and thickness on new data . the small discrepancies noticed , however , are well within the resolvability of resistivity sounding curve interpretations . 
deep_semantic embedding we introduce deep_semantic embedding ( dse ) , a supervised learning_algorithm which computes semantic representation for text documents by respecting their similarity to a given query . unlike other methods that use single_layer learning machines , dse maps word inputs into a low-dimensional semantic space with deep_neural_network , and achieves a highly nonlinear embedding to model the human_perception of text semantics . through discriminative fine-tuning of the deep_neural_network , dse is able to encode the relative similarity between relevant/irrelevant document pairs in training_data , and hence learn a reliable ranking score for a query-document pair . we present test_results on datasets including scientific publications and user_generated knowledge_base . 
conception of a prototype to validate a maintenance expert system decision_making is crucial for the life of many production plants . good decision need the right information at the right time , and a deep knowledge of the system . this is especially true for older plants that are more prone to failures . unfortunately , these systems are less rich of monitoring and diagnostics instrumentation , because of their seniority . when the maintenance service is carried out by a group of external companies , they suffer the consequences of backwardness of the equipment , having to deal with problems of poor coordination , delays in interventions and so on . small_and_medium_enterprises are often in trouble facing these issues so they need dedicated resources in order to offer a maintenance service compliant to the costumer requirements . in this context a research_project was funded , aiming at aiding the birth of a virtual enterprise network of maintainers , which should provide a maintenance service of the highest level in the field of legacy industrial plants . one of the most difficult steps of the project , was to design a prototype capable of testing the architecture of the virtual network , the expert system and the overall goodness of the proposal for potential customers . in this paper we describe how it was possible to design , produce and operate a prototype which could fit a virtual enterprise network of maintainers . the method requires , among other things , to fulfil a preliminary selective failure modes and effects criticality analysis ( fmeca ) to be properly used to implement the diagnostic system . any company has to deal with decision_making activities which are a vital part of their functioning . in this ever-competitive world , it is increasingly_important to facilitate the business of taking decision by managers . to facilitate this process , it is necessary to have the right information at the right time , in order to bridge the gap between the needs and the expectations . to facilitate a better flow of information , there is an increasing need for information_management systems , more and more adapted to the needs . for this reason , it is very important to have a clear understanding of their mode of operation and how they are integrated into a company at all levels of management [1] . only in this way you will understand how the decision_support_system can be useful for making appropriate decisions [2] [3] . an information_management system collects and processes all data and provides a organized summary for managers who 
p s-gradua o em ci ncia da computa o " riple-te : a software product lines testing_process " machado , ivan do carmo riple-te : a software product lines testing_process / ivan do carmo machado . to my beloved family . acknowledgements first and foremost , i would like to thank my greatest teacher of all : god , for providing me this opportunity and granting me the capability to proceed successfully . i could never have done this without the faith i have in you . i would like to gratefully acknowledge the supervision of dr . eduardo almeida during this work . he provided me with many helpful suggestions and encouragement during the course of this work as well as the challenging research that lies behind it . i also wish to express my appreciation to dr . silvio meira , for accepting me as m . sc . student . special gratitude goes to the rest of the teaching staff of the cin/ufpe , for their valuable classes . my sincere thanks are extended to dr . manoel mendon a , from dmcc/ufba , and his students , for their help during the execution of the experimental study . i would like to thank the brazilian national research council ( cnpq ) . without their grant , this m . sc . would not have been possible . i cordially thank my friends and colleagues that i have met during my journey in recife . i thank my housemates bruno , jonatas , iuri and leandro for their good friendship and for being the surrogate family during the years i stayed there . i want to express my deep thanks to my colleague paulo anselmo for taking intense academic interest in this study as well as providing valuable suggestions that improved the quality of this study . insightful discussions , offering valuable advice and support . postgraduates of the rise research_group are thanked for numerous stimulating discussions . saturdays will not be the same any longer ! i would like to express my heartiest thanks to edna telma for her understanding , endless patience and encouragement when it was most required , and for never letting me feel that i was away from my family . my cordial thanks to her family members for their kindness and affection , specially to francisco joaquim , for supporting me during these years i lived in recife . i also want to thanks to benedita ( dio ) for opening her wonderful home to me and making me feel so comfortable there . she provided me a perfect place to finish my work while i was in salvador . i cannot finish without thanking my family . i am forever indebted to my parents , serafim and joselice , my brother ivo 
optimized shielding for space radiation_protection . future deep_space mission and international_space_station exposures will be dominated by the high-charge and -energy ( hze ) ions of the galactic cosmic_rays ( gcr ) . a few mammalian systems have been extensively tested over a broad range of ion types and energies . for example , c3h10t1/2 cells , v79 cells , and harderian gland tumors have been described by various track-structure dependent response models . the attenuation of gcr induced biological effects depends strongly on the biological endpoint , response model used , and material composition . optimization of space shielding is then driven by the nature of the response model and the transmission characteristics of the given material . 
modelling temporal_information using discrete_fourier_transform for video classification recently , video classification attracts intensive research efforts . however , most existing works are based on frame-level visual features , which might fail to model the temporal_information , e . g . characteristics accumulated along time . in order to capture video temporal_information , we propose to analyse features in frequency_domain transformed by discrete_fourier_transform ( dft_features ) . frame-level features are firstly extract by a pre-trained deep_convolutional_neural_network ( cnn ) . then , time domain features are transformed and interpolated into dft_features . cnn and dft_features are further encoded by using different pooling methods and fused for video classification . in this way , static image_features extracted from a pre-trained deep cnn and temporal_information represented by dft_features are jointly considered for video classification . we test our method for video emotion classification and action_recognition . experimental_results_demonstrate that combining dft_features can effectively capture temporal_information and therefore improve the performance of both video emotion classification and action_recognition . our approach has achieved a state-of-the-art performance on the largest video emotion dataset ( videoemotion-8 dataset ) and competitive results on ucf-101 . 
deep_architectures for articulatory inversion we implement two deep_architectures for the acoustic-articulatory inversion mapping problem : a deep neural network and a deep trajectory mixture density network . we find that in both cases , deep_architectures produce more accurate predictions than shallow architectures and that this is due to the higher expressive capability of a deep model and not a consequence of adding more adjustable parameters . we also find that a deep trajectory mixture density network is able to obtain better inversion accuracies than smoothing the results of a deep neural network . our best model obtained an average root mean square error of 0 . 885 mm on the mngu0 test dataset . 
drilling automation for subsurface planetary exploration future in-situ lunar/martian resource_utilization and characterization , as well as the scientific search for life on mars , will require access to the subsurface and hence drilling . drilling on earth is hard an art form more than an engineering discipline . the limited mass , energy and manpower in planetary drilling situations makes application of terrestrial drilling techniques problematic . the drilling automation for mars exploration ( dame ) project's purpose is to develop and field-test drilling automation and robotics technologies for projected use in missions to the moon and mars in the 2011-15 period . 1 . introduction space drilling will require intelligent and autonomous systems for robotic exploration and to support future human exploration , as energy , mass and human presence will be scarce . unlike rover navigation problems , most planetary drilling will be blind absent any precursor seismic imaging of substrates , which is common on earth prior to drilling for hydrocarbons . on the moon , eventual in-situ resource_utilization ( isru ) will require deep drilling with probable human-tended operation [1] of large-bore drills , but initial lunar subsurface exploration and near-term isru will be accomplished with lightweight , rover-deployable or standalone drills capable of penetrating a few tens of meters in depth . on the moon or mars , drilling will be initially automated , then later human-tended at best . mass and energy will be scarce . early development and demonstration of automated drilling technologies is necessary otherwise , no exploration mission designer will allow a drill on board their spacecraft . the search for evidence of extant microbial life on mars is expected to require the acquisition of core samples from subsurface depths estimated at hundreds to thousands of meters where , beneath permafrost , the increasing temperature would be consistent with the presence of interstitial water ( as a brine ) in its liquid phase . 
detection of resistive_shorts in deep sub-micron technologies current-based tests are the most effective methods available to detect resistive_shorts . delta_i_ddq testing is the most sensitive variant and can handle off-state currents of 10-100 ma of a single core . nevertheless this is not sufficient to handle the next generations of very deep_sub_micron_technologies . moreover delay_fault_testing and very-low_voltage testing are not a real alternative for the detection of resistive_shorts . the main limitation of i ddq testing is the intra_die variation of the threshold_voltage which results in variations in the off-state current . two methods are investigated that improve the detection capabilities of i ddq testing . the first method reduces the impact of intra_die variation by reducing the amount of logic that switches states . this method can handle very large off-state currents although at the cost of a substantial increase in test time . the second method investigates the correct scaling of the intra_die variations as a function of temperature . we show that both methods improve the detection capabilities of i ddq testing . 
a better way to learn features : technical perspective a typical machine_learning program uses weighted combinations of features to discriminate between classes or to predict real_valued outcomes . the art of machine learning is in constructing the features , and a radically new method of creating features constitutes a major advance . in the 1980s , the new method was backpropagation , which uses the chain_rule to backpropagate error derivatives through a multilayer , feed_forward , neural_network and adjusts the weights between layers by following the gradient of the backpropagated error . this worked well for recognizing simple shapes , such as handwritten digits , especially in convolutional_neural_networks that use local feature detectors replicated across the image . 5 for many tasks , however , it proved extremely difficult to optimize deep_neural_nets with many layers of non-linear features , and a huge number of labeled training cases was required for large neural_networks to generalize well to test data . in the 1990s , support_vector_machines ( svms ) 8 introduced a very different way of creating features : the user defines a kernel function that computes the similarity between two input vectors , then a judiciously chosen subset of the training examples is used to create " landmark " features that measure how similar a test_case is to each training case . svms have a clever way of choosing which training cases to use as landmarks and deciding how to weight them . they work remarkably well on many machine_learning tasks even though the selected features are non-adaptive . the success of svms dampened the earlier enthusiasm for neural_networks . more recently , however , it has been shown that multiple layers of feature detectors can be learned greedily , one layer at a time , by using unsupervised_learning that does not require labeled_data . the features in each layer are designed to model the statistical structure of the patterns of feature activations in the previous layer . after learning several layers of features this way without paying any attention to the final goal , many of the high-level features will be irrelevant for any particular task , but others will be highly relevant because high_order correlations are the signature of the data's true underlying causes and the labels are more directly related to these causes than to the raw inputs . a subsequent stage of fine-tuning using backpropagation then yields neural_networks that work much better than those trained by backpropagation alone and better than svms for important tasks such as object or speech_recognition . the neural 
temperature_gradient based test_scheduling for 3d stacked ics defects that are dependent on temperature_gradients ( e . g . , delay_faults ) introduce a challenge for achieving an effective test_process , in particular for 3d ics . testing for such defects must be performed when the proper temperature_gradients are enforced on the ic , otherwise these defects may escape the test . in this paper , a technique that efficiently heats up the ic during test so that it complies with the specified temperature_gradients is proposed . the specified temperature_gradients are achieved by applying heating sequences to the cores of the ic under test trough test access mechanism; thus no external heating mechanism is required . the scheduling of the test and heating sequences is based on thermal simulations . the schedule generation is guided by functions derived from the ic's temperature equation . experimental_results_demonstrate that the proposed technique offers considerable test time savings . i . introduction a promising technology for fabricating three dimensional integrated_circuits is based on through-silicon vias ( tsv ) [7 , 9 , 13] . the ics fabricated using tsvs are commonly referred to as 3d stacked ic ( 3d-sic ) [13] . 3d-sic and other deep_submicron_technologies suffer from a considerably larger number of delay_faults as compared with previous technologies . the causes for these delay_faults include resistive bridges and vias , power droops , and cross-talk noise effects . therefore , delay_fault_testing is necessary to provide sufficient fault_coverage [4 , 10] . a large number of pre-bond tsv defects are resistive in nature and , moreover , the mechanical stress caused by tsvs contributes to delay_faults [7 , 9] . therefore , the expected number of delay_faults for 3d-sic is larger than that of 2d ics; thus delay_fault_test is , in particular , important for 3d-sic . since temperature has a significant effect on delay , its impact should be taken into account in delay_fault_test . a very important effect of temperature on signal_integrity throughout an ic is its effect on the clock network [6] . delay_faults usually happen because of increased clock_skew and the major sources of skew for 3d-sics are induced by temperature_gradients [15] . since propagation_delays in a clock network depend on temperature , different temperatures at different places on an ic ( i . e . , temperature_gradients ) result in clock_skew . temperature_gradients in an ic may reach up to 50 in adjacent cores for normal_operation and even higher during test [6 , 15] . besides , the temperature_gradients in 3d-sics are much larger than in 2d ics [19] . this will exacerbate temperature_gradient related 
mas meta_models on test : uml vs . opm in the soda case_study in the aose ( agent_oriented software_engineering ) area , several research efforts are underway to develop appropriate meta_models for agent_oriented methodologies . meta_models are meant to check and verify the completeness and expressiveness of methodologies . in this paper , we put to test the well-established standard unified modelling language ( uml ) , and the emergent object process methodology ( opm ) , and compare their meta-modelling power . both uml and opm are used to express the meta-model of soda , an agent_oriented methodology which stresses interaction and social aspects of mass ( multi_agent_systems ) . meta-modelling soda allows us to evaluate the effectiveness of the two approaches over both the structural and dynamics parts . furthermore , this allow us to find out some desirable features that any effective approach to meta-modelling mas methodologies should exhibit . the definition of a methodology is an interactive process , in which a core is defined and then extended to include all the needed concepts . meta-modelling enables checking and verifying the completeness and expressiveness of a methodology by understanding its deep semantics , as well as the relationships among concepts in different languages or methods [1] . according to [2] , the process of designing a system ( object or agent_oriented ) consists of instantiating the system meta-model that the designers have in their mind in order to fulfil the specific problem requirements . in the agent world this means that the meta-model is the critical element ( . . . ) because of the variety of methodology meta_models . in the context of mass , a meta-model should be a structural representation of the elements ( agents , roles , behaviour , ontology , . . . ) that constitute the actual system , along with their composing relationships . several meta_models of aose methodologies can be found in
kasparov versus deep_blue - computer_chess comes of age is misidentified as " andreas nowatzyk " , someone whom newborn obviously did not know or take the time to find out about . also , while the material presented by newborn is sound as far as it goes , it is very slanted toward technical mishaps and other irrelevant trivia when he could have presented games that show the gradual growth of playing strength . it is significant that a coterie of computer_chess achievers have not understood much about chess itself . it is highly commendable that these individuals have achieved all that they have , based principally on generation innovations tested against older versions . however , when such individuals write about their work , it becomes quite clear that they really do not understand what they have or have not achieved . it is as if someone swam the english_channel and his report is that he just kept putting one arm in front of the other until he touched shore . true , yes , but not very informative . related to this schism between the technologists and the chess players is the fact that the 1996 deep_blue was dominated by the technologists , and once kas-parov understood its weaknesses , he had no trouble bringing it to its knees . however , in 1997 chess understanding was invoked in a number of small innovations , and this created an entity that played so well that kasparov became flustered as his tricks were turned aside , and he eventually went off the " deep " end . the real story of computer_chess comes of age should be written about what the deep_blue team did between the matches of 1996 and 1997 . i hope this book will be written some day . in the meantime , i will give newborn's book a_7 on a scale of 1 to 10 and hope that someone who knows more about chess will revise it . this book was published after the 1996 match between kasparov and deep_blue , but before the 1997 match in which kasparov was defeated . given the hype and speculation following that event , i find it difficult to believe that anything i say in this review will persuade anyone to either buy this book or pass on it . however , here goes . monty newborn is a phenomenal raconteur . his stories are truly wonderful , and his introductory tale of a conversation among a group of birds watching early attempts at human flight is probably worth one third the 
fuzzy_logic trajectory_tracking controller for a tanker this paper proposes a fuzzy_logic controller for design of autopilot of a ship . triangular membership functions have been use for fuzzification and the centroid method for defuzzification . a nonlinear mathematical_model of an oil_tanker has been considered whose parameters vary with the depth of water . the performance of proposed controller has been tested under both course changing and trajectory keeping mode of operations . it has been demonstrated that the performance is robust in shallow as well as deep_waters . d esign of ship control system has been great challenge since long time , because the dynamics of ship are not only nonlinear but change with operating_conditions ( depth of water and speed of vehicle ) . these are also highly influenced by unpredictable external environmental disturbances like winds , sea currents and wind generated waves . ifac ( international federation of automatic_control ) has identified the ship control system as one of the benchmark problems , which are difficult to solve [1] . in 1911 elmer sperry constructed first automatic steering mechanism by developing a closed_loop system for a ship [2-3] . minorsky [4] extended the work of sperry by giving detailed analysis of position feedback_control and formulated three term control law which is referred to as pid ( proportional integral derivative ) control . until the 1960s this type of controller was extensively used but after that other linear autopilots like lqg and h have been reported [5-7] . nonlinear_control schemes such as state feedback linearizaton , backstepping , output feedback_control , lyapunov methods and sliding_mode_control [8-12] have also been proposed . applications of nonlinear_control techniques depend on exact knowledge of plant dynamics to be controlled . since the marine vehicle_dynamics are highly nonlinear and are coupled with hydrodynamics therefore it is difficult to obtain exact dynamic model of a marine vehicle . to overcome these difficulties , the model free control strategies like fuzzy_logic and neural_networks are proved considerably useful . yang , et . al . [13-14] discussed the application of fuzzy_control for nonlinear systems and applied takagi_sugeno type autopilot for a ship to maintain its heading . santos , et . al . [15] proposed the fuzzy autopilot along with pid_controller for control of vertical acceleration for fast moving ferries . velagic , et . al . [16] developed ship autopilot for track keeping by using sugeno type fuzzy system . 
phase_transitions : a new paradigm for evaluating complexity in learning and other complex systems the study of the computational_complexity of algorithms takes a central place in computer_science . since its beginning , scientists have studied problems from the complexity point_of_view , categorizing their behaviour into a well-known complexity_class hierarchy . many interesting problems have proved to be intractable . among these , a class of problems that are particularly prone to a dramatic increase in computational_complexity with increasing problem size is the class of combinatorial problems , which include learning_problems . recently , it has been uncovered that computational problems show critical_phenomena , similar to those that emerge in physical many-body systems . in particular , learning_problems have shown to be the subject of the emergence of phase_transitions , which allow a new paradigm for evaluating complexity to be considered . thus , new paradigm is based on the notion of " typical " complexity instead of " worst_case " complexity . for computational systems , the discovery of a phase_transition has important consequences . first of all , the phase_transition region contains the most difficult problem_instances , for which the computational_complexity increases exponentially when the problem size increases . then , the phase_transition can be used as a source of " difficult " test_problems for assessing the properties and the power of algorithms , and to compare them on meaningful problem_instances on a parity base . moreover , very small variations of the problem parameters may induce very large variations in the algorithm's behavior and/or in the types of solution . then , the knowledge of the critical value allows the user to roughly predict the behavior . moreover , by exploiting further the analogy with physical systems , it is possible to enter into the deep_structure of the problem and of its solutions; the system's behavior near the phase_transition allows a microscopic view of the solution space . this fact not only offers the possibility of a deeper understanding of algorithms' properties , but opens the way to the introduction of new effective algorithms . in this talk , we will show the far-reaching effects of the presence of phase_transitions in various learning_approaches . moreover , connections will be established with fundamental problem classes in computer science , such as the satisfiability problem and the constraint_satisfaction_problem classes , and with statistical_physics approaches , which offered some very effective algorithms . finally we will show how phase_transitions are rather ubiquitous in both natural and artificial systems , including human_vision and neural system . 
peak_load transmission pricing for the ieee reliability_test system peak_load transmission pricing for the ieee reliability_test system in order to facilitate the use of inexpensive generation when the existing transmission system creates obstacle to the optimal power transfer , this thesis analyzes the basic trade_off between using expensive generation and investing in transmission enhancement . understanding this trade_off has taken on a new importance as the electric_utility industry undergoes reconstruction from being a regulated monopoly into serving competitive generation . this thesis continues the work started by lecinq [8] , that has introduced the basic notions of an optimal transmission system and a peak_load pricing mechanism capable of recovering the transmission enhancement investments . however , the main contribution of this thesis is an in depth study of transmission provision and peak_load pricing on a relatively large power system , namely , a 24 bus ieee reliability_test system' ( often used as an ieee test standard ) . in addition , matlab-based software was developed to accomplish the objective of economic_efficiency , by valuing trade_offs between the cost of expensive generation and the transmission enhancement cost . in order to understand the implications of the peak_load pricing mechanism , various simulations , regarding the effect of the type of transmission pricing on the overall economic_efficiency , were performed using this software on the ieee rts . finally , the software developed here could be used as a basic transmission enhancement planning tool . i wish to express my deep gratitude to marija ilic , my advisor and foremost supporter . i have learned a lot working with her during my graduate study at mit , and her advice and guidance were invaluable for this work . the financial_support provided by the m . i . t . consortium on transmission provision and pricing is also greatly appreciated . finally , i owe myself to my family , whose confidence and love were with me all these years . without them , my personal and academic accomplishments would be meaningless . 
fuzzy adaptive_control for trajectory_tracking of autonomous_underwater_vehicle in this paper , the problem of the position and attitude tracking of an autonomous_underwater_vehicle ( auv ) in the horizontal plane , under the presence of ocean_current disturbances is discussed . the effect of the gradual variation of the parameters is taken into account . the effectiveness of the adaptive controller is compared with a feedback linearization method and fuzzy gain control approach . the proposed strategy has been tested through simulations . also , the performance of the proposed method is compared with other strategies given in some other studies . the boundedness and asymptotic convergence properties of the control_algorithm and its semi-global stability are analytically proven using lyapunov_stability theory and barbalat's lemma . i . introduction autonomous_underwater_vehicle ( auv ) is a field of increasing interest due to its many interesting applications . underwater_vehicles are extensively employed in the offshore industry , subaquatic scientific investigations and rescue operations , finding sunken ships , searching for lost artifacts . as they are untethered , they may operate under ice , opening up vast , largely unexplored arctic areas that are inaccessible to any other kind of research_vessel , and operate at depths too deep for tethered vehicles . they are also of military interest ( e . g . see [1] ) . many control methods for underwater_vehicles have been discussed in the literature in the past 15 years to handle uncertainties related to the dynamics , hydrodynamics and external disturbances . especially , for developing advanced control strategies for autonomous_vehicles . recent_developments in this area are well summarized in [2 , 3] in which different motion_control algorithms have been developed under various hypotheses . adopting a linearized model , some linear control techniques such as pid_controller [4] and lqr algorithm [5] have been developed with acceptable performance in only special kinds of maneuvering . 
the beauty of simplicity i fully agree with kamp that the decision , conscience or not , was a mistake , and ultimately a very costly one . in my own c programming in the 1970s i found it a frequent source of frustration but believe i understand its motivation : c was designed with the pdp_11 instruction_set very much in mind . the celebrated c one-liner while ( *s++ = *t++ ) ; copies the string at s to the string at t . elegant indeed ! but what may have been lost in the fog of time is the fact that it compiles into a loop of just two pdp_11 instructions , where register r0 contains the location of s and register r1 contains the location of t : a mov ( @r0 ) ++ , ( @r1 ) ++ bne a test result for nonzero and branch such concise code was seductive and , as i recall , mentioned often in discussions of c . a preceding length count would have required at least three instructions . but even at this level of coding , the economy of the code for moving a string was purchased for a high price : having to search for the terminating null in order to determine the length of a string; that price was also paid when concatenating strings . the security_issues resulting from potential buffer overruns could hardly have been anticipated at the time , but , even then , such computational costs were apparent . " x-rays will prove to be a hoax " : lord kelvin , 1883 . even the most brilliant scientists sometimes get it wrong . beware this fatal instruction ? regarding the article " postmortem debugging in dynamic environments " by david pacheco ( dec . 2011 ) , i have a question regarding the broken code example in the section on a s an admirer of the " artistic flare , nuanced style , and technical prowess that separates good code from great code " explored by robert_green and henry ledgard in their article " coding guidelines : finding the art in the science " ( dec . 2011 ) , i was disappointed by the au-thors' emphasis on " alignment , naming , use of white space , use of context , syntax_highlighting , and ide choice . " as effective as these aspects of beautiful code may be , they are at best only skin deep . beauty may indeed be in the eye of the beholder , but there is a more compelling beauty in the deeper semantic properties of code than layout and naming . i also include judicious use of abstraction , deftly balancing 
state pruning for test vector_generation for a multiprocessor cache_coherence_protocol 2 test vector_generation verification is extremely important in designing digital systems such as a cache_coherence_protocol . generating traces for system verification using a model_checker and then using the traces to drive the rtl logic design simulation is an efficient method for debugging . this approach has been called the witness_string method [2] . since depth_first_search ( dfs ) can quickly get deep into the state space , the original witness_string method is based on dfs . we investigate a state pruning method that exploits multiple search heuristics in simultaneous dfs searches to choose the most efficient traces . in this state pruning algorithm each dfs uses a different heuristic ( i . e . , min/max hamming_distance , min/max cache_score ) . we distribute the hash_table of the entire state_space among the simultaneous searches so that they cooperate to avoid redundant state exploration . to evaluate this new search_algorithm for the witness_string method , we implant several protocol bugs in the stanford dash cache_coherence_protocol . using an ibm power4 system with the berkeley active message library , we show an improvement in witness strings generation through the state pruning method over a pure dfs and a guided dfs . 1 introduction system verification in the pre-silicon state of development is crucial for controlling the budget and the time to market . safety_critical systems , such as a cache_coherence_protocol , are not trivial to verify because there are numerous correctness properties which result in a very large state_space . the large state_space causes the state explosion problem when enumerating states for formal_verification . in order to alleviate this problem , the cache coherency models are usually " down-scaled . " verification of the " down-scaled " models cannot guarantee the correctness of the designs , but is useful as a debugging tool [1] . the witness_string approach [2] is such a method that captures bug traces from depth_first_search ( dfs ) of a state_space to help debug the rtl design of a cache_coherence_protocol . the efficiency of the resulting witness strings captured through this dfs process is very important . for instance , in the cray x1 cache_coherence_protocol , a single witness_string can be on the order of a few thousands states . it takes a considerable amount of time to execute all of the necessary witness strings in the rtl logic_simulation [3] . one method to determine efficient witness strings is through a bug-oriented dfs search of the state space . to enhance the 
materializing multi-relational_databases from the web using taxonomic queries recently , much attention has been given to extracting tables from web_data . in this problem , the column definitions and tuples ( such as what "company" is headquartered in what "city , " ) are extracted from web text , structured web_data such as lists , or results of querying the deep web , creating the table of interest . in this paper , we examine the problem of extracting and discovering <i>multiple</i> tables in a given domain , generating a truly multi-relational_database as output . beyond discovering the relations that define single tables , our approach discovers and leverages "within column" set membership relations , and discovers relations across the extracted tables ( e . g . , joins ) . by leveraging within-column relations our method can extract table instances that are ambiguous or rare , and by discovering joins , our method generates truly multi-relational output . further , our approach uses taxonomic queries to bootstrap the extraction , rather than the more traditional "seed instances . " creating seeds often requires more domain_knowledge than taxonomic queries , and previous work has shown that extraction methods may be sensitive to which input seeds they are given . we test our approach on two real_world domains : nba basketball and cancer information . our results_demonstrate that our approach generates databases of relevant tables from disparate web information , and discovers the relations between them . further , we show that by leveraging the "within column" relation our approach can identify a significant number of relevant tuples that would be difficult to do so otherwise . 
comparison of effectiveness of current_ratio and delta_iddq tests i ddq test is a valuable test method for semiconductor manufacturers . however , its effectiveness is reduced for deep_sub_micron_technology chips due to rising background leakage . current two test_methods that promise to extend the life of i ddq test are current_ratio and delta_i_ddq . although several studies have been reported on these methods , their effectiveness in detecting defects has not been contrasted . in this work , we compare these two methods using industrial test_data . 
deep outdoor illumination estimation we present a cnn-based technique to estimate high-dynamic_range outdoor illumination from a single low dynamic_range image . to train the cnn , we leverage a large dataset of outdoor panoramas . we fit a low-dimensional physically-based outdoor illumination model to the skies in these panoramas giving us a compact set of parameters ( including sun position , atmospheric conditions , and camera parameters ) . we extract limited field-of-view images from the panoramas , and train a cnn with this large set of input image output lighting parameter pairs . given a test image , this network can be used to infer illumination parameters that can , in turn , be used to reconstruct an outdoor illumination environment map . we demonstrate that our approach allows the recovery of plausible illumination conditions and enables automatic photorealistic virtual object insertion from a single image . an extensive evaluation on both the panorama dataset and captured hdr environment maps shows that our technique significantly outper-forms previous solutions to this problem . 
finding peer_to_peer_file_sharing using coarse network behaviors a user who wants to use a service forbidden by their site's usage policy can masquerade their packets in order to evade detection . one masquerade technique sends prohibited traffic on tcp ports commonly used by permitted services , such as port 80 . users who hide their traffic in this way pose a special challenge , since filtering by port number risks interfering with legitimate services using the same port . we propose a set of tests for identifying masqueraded peer_to_peer_file_sharing based on traffic summaries ( flows ) . our approach is based on the hypothesis that these applications have observable behavior that can be differentiated without relying on deep packet examination . we develop tests for these behaviors that , when combined , provide an accurate method for identifying these masqueraded services without relying on payload or port number . we test this approach by demonstrating that our integrated detection mechanism can identify bittorrent with a 72% true positive rate and virtually no observed false positives in control services ( ftp-data , http , smtp ) . 
design and field experimentation of a prototype lunar_prospector design and field experimentation of a prototype lunar_prospector scarab is a prototype rover for lunar missions to survey resources in polar craters . it is designed as a prospector that would use a deep coring drill and apply soil analysis instruments to measure the abundance of elements of hydrogen and oxygen and other volatiles including water . scarab's chassis can adjust the wheelbase and height to stabilize its drill in contact with the ground and can also adjust posture to better ascend and descend steep slopes . this enables unique control of posture when moving and introduces new planning issues . scarab has undergone field_testing at lunar-analog sites in washington and hawaii in an effort to quantify and validate its mobility and navigation capabilities . we report on results of the experiments in slope ascent and descent and in autonomous kilometer-distance navigation in darkness . 
automatic computation of electrode trajectories for deep_brain_stimulation : a hybrid symbolic and numerical approach purpose the optimal electrode trajectory is needed to assist surgeons in planning deep_brain_stimulation ( dbs ) . a method for image_based trajectory_planning was developed and tested . methods rules governing the dbs surgical_procedure were defined with geometric constraints . a formal geometric solver using multimodal brain_images and a template built from 15 brain mri scans were used to identify a space of possible solutions and select the optimal one . for validation , a retrospective study of 30 dbs electrode implantations from 18 patients was performed . a trajectory was computed in each case and compared with the trajectories of the electrodes that were actually implanted . results computed trajectories had an average difference of 6 . 45 compared with reference trajectories and achieved a better overall score based on satisfaction of geometric constraints . trajectories were computed in 2 min for each case . conclusion a rule-based solver using pre-operative mr_brain_images can automatically compute relevant and accurate patient-specific dbs electrode trajectories . 
rh : a retro-hybrid parser contemporary parser research is , to a large extent , focused on statistical parsers and deep-unification-based parsers . this paper describes an alternative , hybrid_architecture in which an atn-like parser , augmented by many preference tests , builds on the results of a fast chunker . the combination is as efficient as most stochastic parsers , and accuracy is close and continues to improve . these results raise questions about the practicality of deep unification for symbolic parsing . 
analytic models for crosstalk delay and pulse analysis under non-ideal inputs in this paper 1 we develop a general methodology to analyze crosstalk to obtain insight into effects that are likely to cause errors in deep_submicron high_speed circuits . we focus on crosstalk due to capacitive_coupling between a pair of lines . we first consider the case where crosstalk_noise manifests as a pulse and characterize the maximum amplitude , width , energy and timing of this pulse . closed_form equations quantifying the dependence of these pulse attributes on the values of circuit_parameters and the rise time of the input transition are derived . we also consider how crosstalk causes slowdown ( speedup ) , i . e . increases ( decreases ) the rise/fall times of signals on coupled lines , when their inputs have transitions in the opposite ( same ) directions . expressions relating the slowdown ( speedup ) to circuit_parameters , the rise/fall times of the input transitions , and the skew between the transitions are derived . we show that crosstalk_effects can be significantly aggravated by variations in the fabrication process . new design corners are identified for validation of designs that have significant crosstalk_effects . finally , the results of our analysis provide conditions that must be satisfied by a sequence of vectors used for validation of designs as well as post-manufacturing testing of devices in the presence of significant crosstalk . i . introduction continuous advancements in the field of vlsi has lead to a decrease in device geometries ( deep_sub_micron_technology ) , high device densities , high clock rates and small signal transition times . due to these changes , crosstalk_noise between adjacent interconnects has become an important concern . if not carefully considered during design validation , crosstalk may cause extra signal delay , logic hazards , and even circuit malfunction . accurate modeling_and_simulation of interconnect delay due to crosstalk thus becomes increasingly important in the design of high_performance integrated_circuits . 
effects of multi-cycle sensitization on delay_tests india-building the tall , thin vlsi engineer p . 5 advances in vlsi_design and product development challenges p . 6 high_level modeling and validation methodologies for embedded_systems : bridging the productivity gap p . 9 design of deep_sub_micron_cmos circuits p . 15 testing embedded cores and socs-dft , atpg and bist solutions p . 17 specification and design of multi-million gate socs p . 18 esd reliability challenges of rf/mixed_signal design and processing p . 20 system support for embedded applications p . 22 narrow band noise suppression scheme for improving signal_to_noise_ratio p . 25 a path sensitization technique for testing of switched capacitor circuits p . 30 a novel rf front_end chipset for ism_band wireless applications p . 36 development of 2 . 4 ghz rf transceiver front_end chipset in 0 . 25 [mu]m cmos p . 42 comparison of heuristic algorithms for variable partitioning in circuit implementation p . 51 timing minimization by statistical_timing hmetis-based partitioning p . 58 an efficient practical heuristic for good ration-cut partitioning p . 64 an efficient multi-level partitioning algorithm for vlsi_circuits p . 70 low_power technology mapping for lut based fpga-a genetic algorithm approach p . 79 routability prediction for field programmable gate arrays with a routing hierarchy p . 85 a fast macro based compilation methodology for partially reconfigurable fpga designs p . 91 detailed analysis of fibl in mos transistors with high-k gate dielectrics p . 99 effect of scaling on the non-quasi-static behaviour of the mosfet for rf ic's p . 105 small signal characteristics of thin film single halo soi mosfet for mixed mode applications p . 110 a new approach to analyze a sub_micron_cmos inverter p . 116 optimization of 1 . 8v i/o circuits for performance , reliability at the 100nm technology node p . 122 application of look-up table approach to high-k gate dielectric mos transistor circuits p . 128 effects of multi-cycle sensitization on delay_tests p . 137 exclusive test and its applications to fault_diagnosis p . 143 a fault-independent transitive_closure algorithm for redundancy identification p . 149 exploiting ghost-fsms as a bist structure for sequential machines p . 155 design of a universal bist ( ubist ) structure p . 161 spare : selective partial replication for concurrent fault_detection in fsms p . 167 design of a 2d dct/idct application specific vliw processor supporting scaled and sub-sampled blocks p . 177 design of a high_speed string_matching co-processor for nlp p . 183
a 60 ghz high gain transformer-coupled differential power amplifier in 65nm cmos a fully differential 60 ghz three-stage transformer-coupled amplifier is designed and implemented in 65 nm digital cmos_process . on-chip transformers which offer dc biasing for individual stages , extra stabilization mechanisms , and simultaneous input/inter-stage/output matching networks are used to facilitate a compact circuit_design . with a cascoded circuit configuration , the amplifier is tested with a linear gain of 30 . 5 db centered at 63 . 5 ghz and a-40 db reverse isolation under a 1 v supply . the amplifier delivers 9 dbm and 13 dbm output power under 1 v and 1 . 5 v supplies , respectively and occupies a core chip area of 0 . 05 mm 2 . the measurement results validate a high gain and area efficient power amplifier design_methodology in deep-scaled cmos for millimeter-wave communication system applications . 
learning from mistakes : a comprehensive study on real_world concurrency_bug characteristics the reality of multi_core hardware has made concurrent_programs pervasive . unfortunately , writing correct concurrent_programs is difficult . addressing this challenge requires advances in multiple directions , including concurrency_bug detection , concurrent program testing , concurrent programming model design , etc . designing effective techniques in all these directions will significantly benefit from a deep understanding of real_world concurrency_bug characteristics . this paper provides the first ( to the best of our knowledge ) comprehensive real_world concurrency_bug characteristic study . specifically , we have carefully examined concurrency_bug patterns , manifestation , and fix strategies of 105 randomly selected real_world concurrency_bugs from 4 representative server and client open_source applications ( mysql , apache , mozilla and openoffice ) . our study reveals several interesting findings and provides useful guidance for concurrency_bug detection , testing , and concurrent programming_language design . some of our findings are as follows : ( 1 ) around one third of the examined non-deadlock concurrency_bugs are caused by violation to programmers' order intentions , which may not be easily expressed via synchronization primitives like locks and transactional memories; ( 2 ) around 34% of the examined non-deadlock concurrency_bugs involve multiple variables , which are not well addressed by existing bug detection tools; ( 3 ) about 92% of the examined concurrency_bugs canbe reliably triggered by enforcing certain orders among no more than 4 memory_accesses . this indicates that testing concurrent_programs can target at exploring possible orders among every small groups of memory_accesses , instead of among all memory_accesses; ( 4 ) about 73% of the examinednon-deadlock concurrency_bugs were not fixed by simply adding or changing locks , and many of the fixes were not correct at the first try , indicating the difficulty of reasoning concurrent execution by programmers . 
guaranteed avoidance of unpredictable , dynamically constrained obstacles using velocity_obstacle sets guaranteed avoidance of unpredictable , dynamically constrained obstacles using velocity_obstacle sets dynamic obstacle_avoidance is an important , ubiquitous , and often challenging problem for autonomous mobile_robots . this thesis_presents a new method to guarantee collision avoidance with respect to moving obstacles that have constrained dynamics but move unpredictably . velocity obstacles have been widely used to plan trajectories that avoid collisions with obstacles under the assumption that the path of the objects are either known or can be accurately predicted ahead of time . however , for real systems , this predicted path will typically only be accurate over short time-horizons . to achieve safety over longer time periods , the method introduced here instead considers the set of all reachable points by an obstacle assuming that the dynamics fit the unicycle model , which has known constant forward speed and a maximum turn rate ( sometimes called the dubins car model ) . this thesis extends the velocity_obstacle formulation by using reachability sets in place of a single " known " trajectory to find matching constraints in velocity space , called velocity_obstacle sets . the velocity_obstacle set for each obstacle is equivalent to the union of all velocity obstacles corresponding to any dynamically feasible future trajectory , given the obstacle's current_state . this region remains bounded as the time horizon is increased to infinity , and by choosing control inputs that lie outside of these velocity_obstacle sets , it is guaranteed that the host agent can always actively avoid collisions with the obstacles , even without knowing their exact future paths . it thus follows that , subject to certain initial conditions , an iterative planner under these constraints guarantees safety for all time . finally , the an iterative planner is repeatedly tested and analyzed in simulation under various conditions . if the time horizon is set to some finite value , the guaranteed collision avoidance is lost , but the planned trajectories generally become more direct . this effect of varying this time scale also depends on the presence of static obstacles in the environment and on the dynamic limitations of the host robot . acknowledgments first , i owe deep thanks to my advisor , professor jonathan how , for his guidance over the last two years . his feedback and insights have been greatly beneficial to my research and to my academic career , and his consistent example of hard work and professionalism has demonstrated what it takes to achieve excellence . i also sincerely thank dr . luca bertucelli for the attentive mentorship he has provided me . i would like to thank the members 
positivist single case_study research in information_systems : a critical analysis positivist , single case_study is an important research approach within the information_systems discipline . this paper provides detailed definitions of key concepts in positivist , single case_study research and carefully analyses the conduct and outcomes of the sarker and lee study that examined the role of social enablers in enterprise_resource_planning ( erp ) systems implementation , and was presented at the international conference on information_systems in 2000 . a number of key issues about positivist , single case_studies are identified , including the need for a clear and deep understanding of key concepts including theory , proposition , hypothesis and hypothesis_testing; the need for clearly defined concepts in theories being tested; the need for hypotheses not propositions when undertaking empirical_research; the importance of explicit boundaries for theories; the distinction between single case_studies and single experiments; and the problem of easy refutation of strong hypotheses using specific and unique cases . despite these issues , positivist , single studies provide a sound and systematic approach for conducting research and are an important component of pluralist research programs within information_systems . 
bistatic deep soundings with the hf gpr tapir in the egyptian white desert introduction : in the frame of the netlander mission , the centre d' tude des environnements terrestre et plan taires ( cetp ) has developed an impulse ground_penetrating_radar ( gpr ) operating at very low central frequencies from 2 to 6 mhz [1] . this instrument , named tapir ( terrestrial and planetary investigation radar ) , aims at exploring the geological features in the deep martian subsurface and at the detection of liquid water underground reservoirs . this gpr was designed to retrieve , from a single fixed location , both the distance and the direction of the reflecting structures . this can be achieved by determining the propagation vector of reflected waves , through measurements of two horizontal electrical and three magnetic components of the reflected waves . this direction information is essential not only to characterize the sub-surface structures but also to discriminate between the echoes coming from the subsurface and those due to the surface clutter , in-situ measurements on well documented soils that are analogues of the expected martian soils are crucial to validate the performances of the instrument . in 2004 , ground tests were successfully carried out on the antarctic continent with a mono-static gpr prototype [2] . in no-vember 2005 , an updated version of the instrument working either in monostatic or in bi-static mode was tested in the egyptian white desert . complementary sounding investigations were jointly conducted on the same site by research teams from the lpi ( lunar_and_planetary_institute ) [3] and the swri ( southwest_research_institute ) [4] . they will provide independent information which will help the interpretation of our gpr data . 
dell computer corporation : a zero-time organization duplicated without permission . any use , either electronic or paper-based , must have written permission of dr . pearlson or dr . yeh . the material contained in this case is from company documents , publicly available sources , and personal interviews held with key executives at dell . the authors wish to thank the managers and executives at dell for their time and support of this research . deep in the heart of texas lies a fortune_500 company who exemplifies many of the principles of a zero time organization . dell computer corporation has seen extraordinarily growth : a 58% revenue increase and an 82% profit increase in 1997 , an equally extraordinary short period of time . sales rose to $12 . 3 billion in 1997 , profits to $944 million in 1997 , and the stock_split for the sixth time in 1998 . much of this success is due to management principles and a vision that we describe here . first we provide some background information on the company , and we describe the management principals and philosophies we think make dell a success . finally , we describe dell using the lens of a zero time organization . company background many know the story of michael_dell , his college-based business of building personal_computers with available parts , and his build to order strategy . founded in 1984 as pc's limited , the name was officially changed worldwide to dell computer corporation when the first stock offering took place , in june 1988 . other key turning points , according to michael_dell , were in 1986 , when dell first went outside the us to europe and hit $50 million in sales; 1989 , when the company when from last to first place in their industry on the management of their inventory; and 1993 when the concept of segmenting took shape and allowed the management to regain control of customers . at the core of dell's business was the build-to-order strategy . customers ordered pcs directly , and their order was routed through a credit check , then directly to the manufacturing floor . the order was then built , tested , and shipped to the customer , who received it 5-7 days after placing their order . this strategy afforded dell some impressive results . first , dell eliminated middlemen-the resellers , who were part of the traditional distribution model . as such , dell not only passed the savings to the customers in the form of lower costs , but was also able to understand customer needs first hand and adapt to market changes faster than competitors . second , 
geomagnetic excursions : knowns and unknowns [1] geomagnetic excursions are short-lived episodes when earth's_magnetic_field deviates into an intermediate polarity state . understanding the origin , frequency , amplitude , duration , and field behavior associated with excursions is a forefront research area within solid earth geophysics . recent advances in excursion research are summarized here , and key further research is suggested to resolve major unanswered questions . improving the global distribution of excursion records , particularly from the southern_hemisphere , obtaining high_resolution sedimentary excursion records with good age control from sites with sedimentation rates >10 cm/kyr , obtaining volcanic excursion records coupled with high_precision geochronology , and estimating excursion duration with high chronological precision will all facilitate hypothesis_testing concerning the deep_earth dynamics that generate geomagnetic excursions . 
volumetric layer segmentation using coupled surfaces propagation the problem of segmenting a volumetric layer of-nite thickness is encountered in several important areas within medical image_analysis . key examples include the extraction of the cortical gray_matter of the brain and the left ventricle myocardium of the heart . the coupling between the two bounding surfaces of such a layer provides important information that helps to solve the segmentation problem . here we propose a new approach of coupled surfaces propagation via level_set methods , which takes_into_account coupling as an important constraint . by evolving two embedded surfaces simultaneously , each driven by its own image-derived information while maintaining the coupling , we capture a representation of the two bounding surfaces and achieve automatic segmentation on the layer . characteristic gray level values , instead of image gradient information alone , are incorporated i n deriving the useful image information to drive the surface propagation , which enables our approach to capture the homogeneity inside the layer . the level_set implementation ooers the advantage of easy initializa-tion , computational eeciency and the ability to capture deep folds of the sulci . as a test example , we apply our approach to unedited 3d magnetic reso-nance ( mr ) brain_images . our algorithm automatically isolates the brain from non-brain_structures and recovers the cortical gray_matter . 
hybrid parallel tempering and simulated_annealing method in this paper , we propose a new hybrid scheme of parallel tempering and simulated_annealing ( hybrid_pt/sa ) . within the hybrid pt/sa scheme , a composite system with multiple conformations is evolving in parallel on a temperature ladder with various transition step sizes . the simulated_annealing ( sa ) process uses a cooling scheme to decrease the temperature values in the temperature ladder to the target temperature . the parallel tempering ( pt ) scheme is employed to reduce the equilibration relaxation time of the composite system at a particular temperature ladder configuration in the sa process . the hybrid pt/sa method reduces the waiting time in deep local_minima and thus leads to a more efficient sampling capability on high_dimensional complicated objective_function landscapes . compared to the approaches pt and parallel sa with the same temperature ladder , transition step sizes , and cooling scheme ( parallel sa ) configurations , our preliminary_results obtained with the hybrid pt/sa method confirm the expected improvements in simulations of several test objective functions , including the rosenbrock's function and the ''rug-ged " funnel-like function , and several instantiations of the traveling salesman_problem . the hybrid pt/sa may have slower convergence than genetic_algorithms ( ga ) with good cross-over heuristics , but it has the advantage of tolerating ''bad " initial values and displaying robust sampling capability , even in the absence of additional_information . moreover , the hybrid pt/sa has natural parallelization potential . markov_chain_monte_carlo ( mcmc ) methods have long been recognized as effective tools in difficult statistical sampling and global_optimization problems arising from a wide range of applications , including physics , biology , medical science , chemistry , material_science , computer_science , and economics . in the original metropolis hastings-based mcmc [1 , 2] , a markov_process is built to sample a target probability_distribution : p x z 1 e e x ; where e ( x ) refers to the objective_function and z denotes the normalization constant ( partition function ) . in this scheme , a new state y is generated from the current_state x of the markov_process by drawing y from a proposal transition function q ( x , y ) . the new state y is accepted with the probability min ( 1 , r ) , where r is the metropolis hastings ratio : 0096-3003/$-see front matter
a deep-cutting-plane technique for reverse convex_optimization a large number of problems in engineering design and in many areas of social and physical sciences and technology lend themselves to particular instances of problems studied in this paper . cutting-plane methods have traditionally been used as an effective tool in devising exact algorithms for solving convex and large_scale combinatorial_optimization problems . its utilization in nonconvex optimization has been also promising . a cutting plane , essentially a hyperplane defined by a linear inequality , can be used to effectively reduce the computational efforts in search of a global solution . each cut is generated in order to eliminate a large portion of the search domain . thus , a deep cut is intuitively superior in which it will exclude a larger set of extraneous points from consideration . this paper is concerned with the development of deep-cutting-plane techniques applied to reverse-convex programs . an upper bound and a lower_bound for the optimal value are found , updated , and improved at each iteration . the algorithm terminates when the two bounds collapse or all the generated subdivisions have been fathomed . finally , computational considerations and numerical results on a set of test_problems are discussed . an illustrative example , walking through the steps of the algorithm and explaining the computational process , is presented . 
title of thesis : shape and reflectance recovery from indoor image_sequences shape and reflectance recovery from indoor image_sequences the author reserves all other publication and other rights in association with the copyright in the thesis , and except as herein before provided , neither the thesis nor any substantial portion thereof may be printed or otherwise reproduced in any material form whatever without the author's prior written permission . the undersigned certify that they have read , and recommend to the faculty of graduate studies and research for acceptance , a thesis entitled shape and reflectance recovery from indoor image_sequences submitted by neil aylon charles birkbeck in partial fulfillment of the requirements for the degree of master of science . abstract several successful methods for shape reconstruction from image_sequences have been developed using a variational formulation . in this work we utilize the same framework , which leads to a partial_differential_equation ( pde ) describing the motion of an initial surface to a refined surface that is a better match to the input images . motivated by the primary goals of reconstructing the object and the parameters to a reflectance model , we take advantage of known lighting_conditions in the error measure . in particular , we assume that there is light variation , due to object rotation relative to the light source , allowing the recovery of shape in both textured and textureless regions . additionally , we propose a method to filter out specular highlights , which allows the recovery of surfaces having non-lambertian reflectance . following recent work using an explicit surface parameterization , we apply the pde refinement to a deformable mesh . we test our method using images obtained from an easy to use capture setup . the setup is accessible to the average pc user , because the only hardware requirements are a camera , a light source , and a glossy white sphere . the capture setup provides images , camera calibration , light calibration , and silhouette images to be used in the refinement . the visual hull is used as a starting_point for the pde evolution . at the end of the refinement , our method outputs a triangulated mesh and the parameters of a phong reflectance model represented in texture space . results on real and synthetic images demonstrate that this method is capable of recovering the geometry of textureless surfaces , and moderately textured surfaces , but is unstable in the recovery of deep concavities . several examples on real sequences illustrate the applicability of our models in computer_graphics applications , where the recovered objects are composed and rendered under novel lighting and view conditions . 
a hybrid input_output approach to model metabolic systems : an application to intracellular thiamine kinetics models of the dynamics of complex metabolic systems offer potential benefits to the deep comprehension of the system under study as well as for the performance of certain tasks . unfortunately , dynamic modeling of a great deal of metabolic systems may be problematic due to the incompleteness of the available knowledge about the underlying mechanisms and to the lack of an adequate observational data_set . in theory , a valid alternative to classical structural modeling through ordinary_differential_equations could be represented by input_output approaches . but , in practice , such methods , which learn the nonlinear dynamics of the system from input_output data , fail when the experimental data_set is poor either in size or in quality . such a situation is not rare in the case of metabolic systems . this paper deals with a hybrid approach which aims at overcoming the problems addressed above . more specifically , it allows us to solve the identification problems of the intracellular thiamine kinetics in the intestine tissue . the method , which is half way between the structural and input_output approach , uses the outcomes of the simulation of a qualitative structural model to build a good initialization of a fuzzy system identifier . such an initialization allows us to efficiently cope with both the incompleteness of knowledge and the inadequacy of the available data_set , and to derive an input_output model of the intracellular thiamine kinetics in the intestine tissue . the comparison of the predictions of the intracellular thiamine kinetics obtained by the application of such a model with those obtained by traditional_approaches , namely compartmental models , neural_networks , and fuzzy systems , highlighted a better performance of our model . as the structural assumptions are relaxed , we obtained a model slightly less informative than a purely structural one but robust enough to be used as a simulator . the paper also discusses the interpretative potential offered by such a model , as tested on diabetic subjects . 
multimodal emotion recognition using multimodal deep_learning to enhance the performance of affective models and reduce the cost of acquiring physiological signals for real_world_applications , we adopt mul-timodal deep_learning approach to construct af-fective models from multiple physiological signals . for unimodal enhancement task , we indicate that the best recognition accuracy of 82 . 11% on seed dataset is achieved with shared representations generated by deep autoencoder ( dae ) model . for multimodal facilitation tasks , we demonstrate that the bimodal deep autoencoder ( bdae ) achieves the mean accuracies of 91 . 01% and 83 . 25% on seed and deap datasets , respectively , which are much superior to the state-of-the-art approaches . for cross-modal learning task , our experimental_results_demonstrate that the mean accuracy of 66 . 34% is achieved on seed dataset through shared representations generated by eeg-based dae as training_samples and shared representations generated by eye-based dae as testing sample , and vice versa . 
robust test_generation for power_supply_noise induced path_delay faults in deep_sub_micron designs , the delay caused by power_supply_noise ( psn ) can no longer be ignored . a psn-induced path_delay_fault ( psnpdf ) model is proposed in this paper , and should be tested to enhance chip quality . based on precise timing_analysis , we also propose a robust test_generation technique for psnpdf . concept of timing window is introduced into the psnpdf model . if two devices in the same feed region simultaneously switch in the same direction , the current waveform of the two devices will have an overlap and excessive psn will be produced . experimental results on iscas'89 circuits showed test_generation can be finished in a few seconds . 
far field extrapolation from near field interactions and shielding influence investigations based on a fe-peec coupling method regarding standards , it is well established that common mode currents are the main source of far field emitted by variable_frequency_drive ( vfd ) -cable-motor associations . these currents are generated by the combination of floating potentials with stray capacitances between these floating potential tracks and the mechanical parts connected to the earth ( the heatsink or cables are usual examples ) . nowadays , due to frequency and power increases , the systematic compliance to emc ( electromagnetic_compatibility ) becomes increasingly difficult and costly for industrials . as a consequence , there is a well-identified need to investigate practical and low_cost solutions to reduce the radiated fields of vfd-cable-motor associations . a well-adapted solution is the shielding of wound components well known as the major source of near magnetic_field . however , this solution is not convenient , it is expensive and may not be efficient regarding far field reduction . optimizing the components placement could be a better and cheaper solution . as a consequence , dedicated tools have to be developed to efficiently investigate not easy comprehendible phenomena and finally to control emc disturbances using component placement , layout geometry , shielding design if needed . however , none of the modeling 81 methods usually used in industry complies with large frequency_range and far field models including magnetic materials , multilayer pcbs , and shielding . the contribution of this paper is to show that alternatives regarding modeling solutions exist and can be used to get in-deep analysis of such complex structures . it is shown in this paper that near field investigations can give information on far field behavior . it is illustrated by an investigation of near field interactions and shielding influence using a fe-peec hybrid method . the test_case combining a common mode filter with the floating potentials tracks of an inverter is based on an industrial and commercialized vfd . the near field interactions between the common mode inductance and the tracks with floating potentials are revealed . then , the influence of the common mode inductance shielding is analyzed . 
spider search : an efficient and non-frontier-based real-time search_algorithm real-time search_algorithms are limited to constant-bounded search at each time step . we do not see much difference between standard search_algorithms and good real-time search_algorithms when problem_sizes are small . however , having a good real-time search_algorithm becomes important when problem_sizes are large . in this paper we introduce a simple yet efficient algorithm , spider search , which uses very low constant time and space to solve problems when agents need deep ( but not exhaustive ) path analysis at each step . moreover , we did some experimental_tests to compare spider search with other searches . we expect that spider search is the first in a new class of tree_based rather than frontier-based search_algorithms . 
using computers to describe style uman beings have a poorly_understood ability to recognize patterns in their environment . sometimes this ability i s vital to hsur vival . sometimes it i s the enabling ability that mediates the creation and maintenance of a culture . thus we have art . but always this ability to organize the visual world of art remains largely a mystery that motivates our study both of art making and art looking . despite the mysterious nature of art , people have always attempted to understand both the process and the product of art making . often they are remarkably successful . they have communicated this understanding in several ways . volumes have been written in the attempt to explain an art that has become accessible to a scholar as a consequence of deep and prolonged study . others have expressed their understanding by practicing an art in an established style . this includes the special case of forgers who have demonstated an understanding adequate to fool a-skeptical audience . form i s communicated in such a way as to make i t difficult for others to build upon the knowledge that has been acquired . even in those cases where scholarly understanding i s best communicated , it seldom can be used with the ease that ordinary scientific results can be used . i s inherently not communicable in the sense that scientific results are . we do not believe this . our attempt here i s to show that , at least in one aspect , an understanding of art can be communicated in just the way that scientific results are : it can be tested , validated , and used . the aspect that concerns u s i s style ( schapiro 1953 ) . when we are confronted with a homogeneous collection of art works , the first things that we observe are the formal properties : shape , color , arrangement , in all these cases , however , the achievement of understanding an art in defense of this limiting communication , i t i s often argued that art
on the analysis_tools of traffic fractality the deep changes recently undergone by network_traffic have strongly impacted its statistical characterization . the traditional assumptions on traffic characteristics have been proven to be obsolete and a new formalism has been brought about to properly take into account traffic new properties . accordingly , newly designed analysis_tools have been proposed . our work is focused on the presentation and the test of these tools of analysis , believing that we have first of all to be confident on the methods used to properly characterize network_traffic . 
deterministic built-in self-test using split linear feedback shift register reseeding for low_power testing a new low_power testing methodology to reduce the excessive power_dissipation associated with scan_based_designs in the deterministic test_pattern generated by linear feedback shift_registers ( lfsrs ) in built-in self-test is proposed . this new method utilises two split lfsrs to reduce the amount of the switching_activity . the original test cubes are partitioned into zero-set and one-set cubes according to specified bits in the test cubes , and the split lfsr generates a zero-set or one-set cube in the given test cube . in cases where the current scan shifting value is a do not care bit accounting for the output values of the lfsrs , the last value shifted into the scan_chain is repeatedly shifted into the scan_chain and no transition is produced . experimental_results for the largest iscas'89 benchmark_circuits show that the proposed scheme can reduce the switching_activity by 50% with little hardware_overhead compared with previous schemes . 1 introduction highly developed deep_sub_micron_technology has enabled the implementation of a large system-on-a-chip ( soc ) . the large soc_design includes several intellectual_property ( ip ) cores such as processor cores , embedded_memories and other peripheral cores . traditional test_methods using the automatic_test_equipment ( ate ) have become unsuitable for testing of these large socs . this is because the more ip_cores are used in one soc , the larger test_data volumes are required . therefore in order to apply this large volume of test patterns to the soc , the ate requires large memory and this increases the test cost of the soc . also , since the number of external i/o pins of the soc and the number of ate channels are limited , the soc testing is very time consuming . built-in self-test ( bist ) is widely known as a good solution for testing the individual ip_cores [1 6] . as a test pattern_generator of bist , an lfsr ( linear feedback shift register ) is widely adopted to generate a pseudo_random test_pattern . however , in cases that produce many random pattern resistant ( rpr ) faults in the circuit under test , it is very difficult to get high fault_coverage with a pseudo_random test_pattern . several attractive solutions for this problem have been proposed [7 21] . the weighted random test was presented to reduce the test set size and to guarantee high fault_coverage [7 9] . the weighted random test adds the additional hardware necessary to change the probability of the logic value 1 
comparison of glr and invariant detectors under structured clutter covariance this paper addresses a target detection problem in radar_imaging for which the covariance_matrix of unknown gaussian clutter has block diagonal structure . this block diagonal structure is the consequence of a target lying along a boundary between two statistically independent clutter regions . here , we design adaptive detection algorithms using both the generalized likelihood_ratio ( glr ) and the invariance principles . there has been considerable interest in applying invariant hypothesis_testing as an alternative to the glr test . this interest has been motivated by several attractive properties of invariant tests including : exact robustness to variation of nuisance parameters and possible finite_sample min-max optimality . however , in our deep-hide target detection problem , there are regimes for which neither the glr nor the invariant tests uniformly outperforms the other . we discuss the relative advantages of glr and invariance procedures in the context of this radar_imaging and target detection application . 
metacluster : unsupervised binning of environmental genomic fragments and taxonomic annotation limited by the laboratory technique , traditional microorganism research usually focuses on one single individual species . this significantly limits the deep analysis of intricate biological processes among complex microorganism communities . with the rapid development of genome_sequencing techniques , the traditional research_methods of microorganisms based on the isolation and cultivation are gradually replaced by metagenomics , also known as environmental genomics . the first step , which is also the major_bottleneck of metagenomic data_analysis , is the identification and taxonomic characterization of the dna fragments ( reads ) resulting from sequencing a sample of mixed species . this step is usually referred as "binning" . existing binning methods based on sequence similarity and sequence composition markers rely heavily on the reference genomes of known microorganisms and phylogenetic markers . due to the limited availability of reference genomes and the bias and unstableness of markers , these methods may not be applicable in all cases . not much unsupervised binning methods are reported , but the unsupervised nature of these methods makes them extremely difficult to annotate the clusters with taxonomic labels . in this paper , we present metacluster 2 . 0 , an unsupervised binning method which could bin metagenomic sequencing datasets with high_accuracy , and also identify unknown genomes and annotate them with proper taxonomic labels . the running time of metacluster 2 . 0 is at least 30 times faster than existing binning algorithms . metacluster 2 . 0 , and all the test datasets mentioned in this paper are available at http : //i . cs . hku . hk/~alse/metacluster/ . 
modular sram-based binary content-addressable memories binary content addressable memories ( bcams ) , also known as associative memories , are hardware_based search_engines . bcams employ a massively parallel exhaustive search of the entire memory space , and are capable of matching a specific data within a single cycle . networking , memory_management , pattern_matching , data_compression , dsp , and other applications utilize cams as single-cycle associative search accelerators . due to the increasing amount of processed information , modern bcam applications demand a deep searching space . however , traditional bcam approaches in fpgas suffer from storage inefficiency . in this paper , a novel , efficient and modular technique for constructing bcams out of standard sram blocks in fpgas is proposed . hierarchical search is employed to achieve high storage efficiency . previous hierarchical search approaches cannot be cascaded since they provide a single matching address; this incurs an exponential increase of ram consumption as pattern width increases . our approach , however , efficiently regenerates a match indicator for every single address by storing indirect indices for address match indicators . hence , the proposed method can be cascaded and exponential_growth is alleviated into linear . our method exhibits high storage efficiency and is capable of implementing up to 9 times wider bcams compared to other approaches . a fully parameterized verilog implementation is being released as an open_source library . the library has been extensively tested using altera's quartus and modelsim . 
hardware trojan horse detection using gate_level characterization hardware trojan horses ( hths ) are the malicious altering of hardware specification or implementation in such a way that its functionality is altered under a set of conditions defined by the attacker . there are numerous hths sources including untrusted foundries , synthesis tools and libraries , testing and verification tools , and configuration scripts . hth attacks can greatly comprise security_and_privacy of hardware users either directly or through interaction with pertinent systems and application_software or with data . however , while there has been a huge research_and_development effort for detecting software trojan horses , surprisingly , hths are rarely addressed . hth detection is a particularly difficult_task in modern and pending deep_submicron_technologies due to intrinsic manufacturing variability . our goal is to provide an impetus for hth research by creating a generic and easily applicable set of techniques and tools for hth detection . we start by introducing a technique for recovery of characteristics of gates in terms of leakage_current , switching power , and delay , which utilizes linear_programming to solve a system of equations created using non-destructive measurements of power or delays . this technique is combined with constraint manipulation techniques to detect embedded hths . the effectiveness of the approach is demonstrated on a number of standard benchmarks . 
ambulatory monitoring of activities and motor_symptoms in parkinson's_disease ambulatory monitoring of motor_symptoms in parkinsons disease ( pd ) can improve our therapeutic strategies , especially in patients with motor fluctuations . previously_published monitors usually assess only one or a few basic aspects of the cardinal motor_symptoms in a laboratory setting . we developed a novel ambulatory monitoring system that provides a complete motor assessment by simultaneously analyzing current motor activity of the patient ( e . g . sitting , walking ) and the severity of many aspects related to tremor , bradykinesia , and hypokinesia . the monitor consists of a set of four inertial_sensors . validity of our monitor was established in seven healthy controls and six pd_patients treated with deep_brain_stimulation ( dbs ) of the subthalamic nucleus . patients were tested at three different levels of dbs treatment . subjects were monitored while performing different tasks , including motor tests of the unified parkinsons disease rating scale ( updrs ) . output of the monitor was compared to simultaneously recorded videos . the monitor proved very accurate in discriminating between several motor activities . monitor output correlated well with blinded updrs ratings during different dbs levels . the combined analysis of motor activity and symptom severity by our pd monitor brings true ambulatory monitoring of a wide variety of motor_symptoms one step closer . . 
anthro arm : the design of a seven degree_of_freedom arm with human attributes anthro arm : the design of a seven degree_of_freedom arm with human attributes the author hereby grants to mit permission to reproduce and to distribute publicly paper and electronic copies of this thesis document in whole or in part in any medium now known or hereafter created . abstract studying biological systems has given robotics researchers valuable insight into designing complex systems . this thesis explores one such application of a biomimetic robotic system designed around a human arm . the design of an anthropomorphic arm , an arm that is similar to that of a human's , requires deep insight into the kinematics and physiology of the biological system . investigated here is the design and completion of an arm with 7 degrees of freedom and human-like range of motion in each joint . the comparison of actuation schemes and the determination of proper kinematics enable the arm to be built at a low cost while maintaining high_performance and similarity to the biological analog . complex parts are built by dividing structures into interlocking 2d shapes that can easily be cut out using a waterjet and then welded together with high reliability . the resulting arm will become part of a bionic system when combined with an existing bionic hand platform that is being developed in the intelligent machines laboratory at mit . with a well thought out modular_design , the system will be used as a test_bed for future_research involving data simplification and neurological control . the completion of the anthropomorphic arm reveals that is indeed feasible to use simple dc motors and quick fabrication techniques . the final result is a reliable , modularized , and anthropomorphic arm . 
voltage_drop-constrained optimization of power_distribution network based on reliable maximum current estimates the problem of optimum design of tree-shaped power_distribution networks with respect to the voltage_drop effect is addressed in this paper . an approach for the width adjustment of the power lines supplying the circuit's major functional blocks is formulated , so that the network occupies the minimum possible area under specific voltage_drop constraints at all blocks . the optimization approach is based on precise maximum current estimates derived by statistical means from recent advances in the field of extreme value theory . experimental_tests include the design of power_grid for a choice of different topologies and voltage_drop tolerances in a typical benchmark circuit . i . introduction the massive power_distribution networks of modern deep_submicron_vlsi circuits are particularly susceptible to a number of reliability problems , the biggest one of which is the well-known voltage_drop or ir-drop problem [1]-[2] . this effect characterizes the lowering of the effective voltage level supplied on the active devices of the circuit due to the finite resistance of the power and ground wires , and can have an adverse effect on circuit speed and noise_margins ( fig . 1 ) , degrading performance ( at best ) or causing faulty logic signals and circuit malfunction [3] . traditionally , in order to avoid significant drop of the voltage level the power_distribution lines have been made excessively wide to reduce their resistance . however , the voltage_drop problem becomes even more pronounced with each new generation of integrated_circuits , as the increase in the number of devices and operating_frequency combined with the decrease in feature size induce an increase in the currents and the effective resistance respectively , while the decrease in the supply_voltage lowers its acceptable drop along the power_distribution lines . therefore simply increasing line width without any restriction cannot be maintained since this would result in a significant occupation of valuable silicon_area ( which must also accommodate the interconnection lines of the circuit ) . a method for designing power networks to satisfy certain constraints on ir-drop while occupying the minimum possible silicon_area is required , and this became an important research direction lately . a good deal of work has been done previously for this purpose [4]-[9] . yet , such a design method necessitates the knowledge of the maximum possible currents flowing at any time through the lines of the network , which in all of previous_works were considered as given . however , their specific value is hard to obtain since the instantaneous current 
preservation of microelectrode recordings with non-gabaergic drugs during deep_brain stimulator placement in children . object deep_brain_stimulation ( dbs ) has become accepted therapy for intractable dystonia and other movement disorders . the accurate placement of dbs electrodes into the globus pallidus internus is assisted by unimpaired microelectrode recordings ( mers ) . many anesthetic and sedative drugs interfere with mers , requiring the patient to be awake for target localization and neurological testing during the procedure . in this study , a novel anesthetic technique was investigated in pediatric dbs to preserve mers . methods in this paper , the authors describe a sedative/anesthetic technique using ketamine , remifentanil , dexmedetomidine , and nicardipine in 6 pediatric patients , in whom the avoidance of gabaergic stimulating drugs permitted excellent surgical conditions with no detrimental effects on intraoperative mers . the quality of the mers , and the frequency of its use in making electrode placement decisions , was reviewed . results all 6 patients had good-quality mers . the data were of sufficient quality to make a total of 9 trajectory adjustments . conclusions microelectrode recordings in pediatric dbs can be preserved with a combination of dexmedetomidine and ketamine , remifentanil , and nicardipine . this preservation of mers is particularly crucial in electrode placement in children . 
something of a potemkin village ? acid2 and mozilla's efforts to comply with html4 the real point here is that the acid3 test isn't a broad-spectrum standards-support test . it's a showpiece , and something of a potemkin village at that . which is a shame , because what's really needed right now is exhaustive test_suites for specifications xhtml , css , dom , svg . [2] acid3 is the third of three benchmark tests that have been devised to challenge browsers to comply with internet standards [6] . while firefox developers at mozilla had fully embraced the predecessor to acid3 , acid2 , they showed themselves much more reticent this time around . as the quote above indicates they had come to feel that acid3 would divert attention from the real issues and might actually make it more difficult to achieve " deep compliance " as developers would scramble to come up with quick fixes just to pass the benchmark test . but were these fears justified ? to find out , we retrieved the bug reports for bugs in mozilla's bugzilla bug tracker concerning compliance with the html4 standard and tried to analyze the differences in the process of bug resolution between bugs that were linked to acid2 and bugs that were not . in bugzilla , the bug resolution process passes a number of well-defined stages . based on the transition rates that we observe we conclude that the process of bug resolution is markedly different for bugs associated with acid2 . in particular , bug resolution appears to be much more chaotic in case of acid2 . this might be symptomatic for " scrambling " , which would explain why developers were not so keen to repeat the experience when acid3 came around . further investigations , however , are needed to corroborate this hypothesis . bugs reports in bugzilla are often part of bug report networks [3] . that is , they are part of a network of dependencies as bugs can be declared to depend on , block , or duplicate other bugs . note that the dependencies between bugs are not always purely technical . in fact , an important type of bugs in bugzilla is the " meta-bug " , also known as the " tracker bug " , which is a bug at the root of a dependency_tree whose leafs are bugs that are related to the issue that the meta-bug is trying to address . for instance , meta-bug 7954 is the bug that tracks issues related to the implementation of the html4 standard and the meta-bug 289480 tracks the issues related to acid2 . for our investigation 
deep speech : scaling up end_to_end speech_recognition we present a state-of-the-art speech recognition_system developed using end_to_end deep_learning . our architecture is significantly simpler than traditional speech systems , which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments . in contrast , our system does not need hand-designed components to model background noise , reverberation , or speaker variation , but instead directly learns a function that is robust to such effects . we do not need a phoneme dictionary , nor even the concept of a " phoneme . " key to our approach is a well-optimized rnn training system that uses multiple gpus , as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training . our system , called deep speech , outperforms previously_published results on the widely studied switchboard hub5'00 , achieving 16 . 0% error on the full test_set . deep speech also handles challenging noisy environments better than widely used , state-of-the-art commercial speech systems . 
interacting with the flow mobile_devices offer challenges for ui design . limited screen space leads to deep menus , complex navigation and loss of position . we introduce a new user_interface concept that reverses the traditional navigation paradigm . by utilizing context_awareness and allowing the user to control the ui via filters , objects of interest navigate past the user instead of the user navigating to the object . the user operates on a single_view without the need for deep menu navigation . the new ui is also easy to configure . we implemented the concept on the nokia s60 5th edition touch platform and conducted user testing with 16 users . initially , users felt confused because of new ways of accessing things . however , after a short period of usage , majority of the users found it easy to use . most of the users felt the system to be fun and playful . 
lattice basis reduction : improved practical algorithms and solving subset sum problems we report on improved practical algorithms for lattice basis reduction . we propose a practical oating point v e r s i o n o f t h e l 3 {algorithm of lenstra , lenstra , lovv asz ( 1982 ) . we present a v ariant o f t h e l 3 { algorithm with \deep insertions" and a practical algorithm for block korkin{zolotarev reduction , a concept introduced by s c hnorr ( 1987 ) . empirical tests show that the strongest of these algorithms solves almost all subset sum problems with up to 66 random weights of arbitrary bit length within at most a few hours on a unisys 6000/70 or within a couple of minutes on a sparc 1+ computer . 
evolutionary_computation approach for shape_from_shading a new approach for recovering a 3d shape of an object from a shaded image using a genetic algorithm ( ga ) is proposed . the 3d-shape is recovered through the analysis of the gray levels in a single image of the scene . this problem is ill-posed except if some additional assumptions are made . in the proposed method , shape_from_shading is addressed as an energy minimization problem . the traditional deterministic approach provides efficient algorithms to solve this problem in terms of time but reaches its limits since the energy associated with shape_from_shading can contain multiple deep local_minima . genetic_algorithm is used as an alternative approach which is efficient at exploring the entire search_space . the algorithm is tested in both synthetic and real image and is found to perform accurate and efficient results . . 1 . introduction surface shape recovery of observed objects is one of the main goals in the field of three-dimensional ( 3-d ) computer_vision . marr identified shape_from_shading ( sfs ) as providing one of the key routes to understanding 3d surface structure via the 2 d sketch [1] . the process has been an active area of research for over two decades . it is concerned with recovering 3d surface shape_from_shading patterns . the subject has been tackled in a variety of ways since the pioneering work of horn and his coworker's in the 1970s [2] . they are classified into direct and indirect methods . direct or active methods recover depth information directly using range finder and structure light . indirect methods determine the relative depth by cues extracted from gray level images of observed object . this class contains shape_from_shading . since the direct or active techniques usually involve many complex external component setup , they may not be suitable for instance in object shape estimation . hence many researchers have focused on the latter class of method . the shape_from_shading methods make use of the change in image brightness of an object to determine the relative surface depth and can be effectively applied to smooth surface regions . most of the shading techniques employ variational approaches which impose constraints such as smoothness , imaging equation and light conditions . as a result these techniques can only be applied to the object surface that satisfies the constraints . also the computational_complexity of the resulting algorithms is high and increases as the number of these constraints increases . useful alternatives for variational shading are local shading and linear 
ic outlier identification using multiple test metrics with advances in semiconductor_manufacturing processes , ics become faster , more powerful , and more densely packed . this in turn makes testing difficult , especially for deep_submicron chips ( http : //public . itrs . net ) . engineers use parametric tests to evaluate different ic parameters like speed or leakage_current ( i_ddq ) . if a parameter is outside a predetermined limit or threshold , the chip is considered defective . for example , a chip can operate too slowly ( a delay failure ) or consume excessive i_ddq . engineers usually determine the pass/fail threshold by taking a sample of chips from the initial production for characterization . alternatively , they can build device models and determine a threshold through simulation . chips that fail only parametric tests do not meet their specifications completely , but are functional . however , manufacturers can reject such chips for reliability reasons . with the reduction in transistor geometries , precise process_control of manufacturing becomes more difficult . this leads to variation in transistor parameters , both within a chip and within a wafer ( across chips ) . this results in a large spread in fault_free parameter values . for example , i_ddq increases exponentially as you scale down transistors . as a result , setting the pass/fail threshold becomes difficult because the overlap between fault_free and faulty distributions invariably results in yield_loss and/or test escapes . if you plot a distribution of test parameters for different chips , the defective chips appear as outliers in the tail of the distribution . screening these chips is therefore , in principle , similar to statistical outlier_rejection . since statistical outlier_rejection is a well-researched topic , this opens up a wide variety of statistical_methods for application to vlsi testing . this article presents an overview of the application of outlier identification methodology for vlsi test , using industrial test_data . variability is inevitable in semiconductor_manufacturing . variations occur because of changes in processing conditions such as temperature , implant dose , and gas turbulence . transistor parameters such as effective channel length and width , and gate oxide thickness change from chip to chip . generally , test engineers observe four levels of variations : within chip , within wafer , between wafers , and between lots , in order of increasing magnitude . these variations have a tremendous impact on test parameters . for example , figure_1_shows 20 i_ddq measurements for each of five chips . chips a and b are adjacent to each other on a wafer . chip c is from a different region of the same wafer . chip d is from a different wafer from the same lot 
on optimizing scan testing power and routing cost in scan_chain design with advanced vlsi manufacturing technology in deep_submicron ( dsm ) regime , we can integrate entire electronic systems on a single chip ( soc ) . due to the complexity in soc_design , circuit testability becomes one of the most challenging works . without careful design in scan cell placement and chain ordering , circuits consume much more power in test_mode operation than that in normal functional mode . this elevated testing power may cause problems including overall yield lost and instant circuit damage . in this paper , we present an approach to simultaneously minimizing power and routing cost in scan_chain reordering after cell placement . we formulate the problem as a traveling salesman_problem ( tsp ) , different cost evaluation from [3] , [5] , and apply an efficient heuristic to solve it . the experimental results are encouraging . compared with a recent result in [3] , which uses the approach with clustering overhead , we obtain up to 10% average power_saving under the same low routing cost . furthermore , we obtain 57% routing cost improvement under the same test power_consumption in s9234 , one of iscas'89 benchmarks . we collaborate multiple scan_chains architecture with our methodology and obtain good results as well . 
multi_objective ai planning : evaluating dae yahsp on a tunable benchmark all standard artifical intelligence ( ai ) planners to-date can only handle a single objective , and the only way for them to take into account multiple objectives is by aggregation of the objectives . furthermore , and in deep contrast with the single objective case , there exists no benchmark problems on which to test the algorithms for multi_objective planning . divide-and-evolve ( dae ) is an evolutionary planner that won the ( single-objective ) deterministic temporal satisficing track in the last international planning competition . even though it uses intensively the classical ( and hence single-objective ) planner yahsp ( yet another heuristic search planner ) , it is possible to turn daeyahsp into a multi_objective evolutionary planner . a tunable benchmark suite for multi_objective planning is first proposed , and the performances of several variants of multi_objective daeyahsp are compared on different instances of this benchmark , hopefully paving the road to further multi_objective competitions in ai planning . 
processing wearable sensor data to optimize deep_brain_stimulation r ecent advances in miniature-sensor technology have enabled the development of wearable systems 1 that physicians could use to monitor motor behavior in individuals with parkinson's_disease . parkin-son's disease occurs when neurons in a part of the midbrain referred to as substantia_nigra pars compacta degenerate and stop producing dopamine , a neurotransmitter involved in motor and cognitive functions . symptoms include tremors , bradykinesia ( slowed movement ) , rigidity , and impaired balance . therapy is based on augmenting or replacing dopamine using drugs that activate dopamine receptors . these therapies are often successful for some time , but patients eventually develop motor complications such as wearing off ( that is , the benefits wear off within a few hours of taking the medication ) and dyskinesias ( involuntary and sometimes violent writhing movements ) . when pharmacological interventions can't sufficiently manage symptoms , stimulating the subthalamic nucleus or deep_brain_stimulation can help . physicians determine the optimal settings for deep_brain_stimulation by clinically testing different combinations of various stimulation parameters . however , target symptoms respond to parameter changes at different times some within seconds , others not for days or weeks . another complication is how various medications work when combined with deep_brain_stimulation . choosing optimal stimulator parameters is thus very challenging . gathering accelerometer data from patient-worn sensors following the adjustment of stimulation settings could be key in optimizing deep_brain_stimulation . here , we present the results of a pilot study we conducted to evaluate this approach . our work is a first step toward implementing advanced strategies for optimizing clinical outcomes using systematic data capture and analysis . data_collection and analysis deep_brain_stimulation requires quadripolar electrodes , extension cables , and an internal pulse generator . stimulators deliver pulses through cylindrical electrode contacts that are 1 . 27 mm in diameter and 1 . 5 mm long . the relevant stimulation parameters , which the physicians control using an external console , are electrode polarity , contact location , and the pulse amplitude , duration , and frequency . generally , the pulse amplitude ranges from 1 to 3 . 5 volts , the pulse width from 60 to 90 s , and the frequency from 110 to 150 hz . the stimulation across the quadripolar electrode can be either monopolar or bipolar . 2 clearly , numerous combinations of stimulation parameter values exist . our goal was to determine whether accelerometer data could help physicians predict clinical scores for the severity of tremor , bradykinesia , and dyskinesia; identify distinct movement patterns that mark transitory behaviors once stimulation has stopped; and estimate the rate of change in the this is a 
believable social and emotional agents one of the key steps in creating quality interactive drama is the ability to create quality interactive characters ( or believable agents ) . two important aspects of such characters will be that they appear emotional and that they can engage in social_interactions . my basic approach to these problems has been to use a broad agent architecture and minimal amounts of modeling of other agent in the environment . this approach is based on an understanding of the artistic nature of the problem . to enable agent-builders ( artists ) to create emotional agents , i provide a general framework for building emotional agents , default emotion-processing rules , and discussion about how to create quality , emotional characters . my framework gets a lot of its power from being part of a broad agent architecture . the concept is simple : the agent will be emotionally richer if there are more things to have emotions about and more ways to express them . this reliance on breadth has also meant that i have been able to create simple emotion models that rely on perception and motivation instead of deep modeling of other agents and complex cognitive processing . to enable agent builders to create social behaviors for believable agents , i have designed a methodology that provides heuristics for incorporating personality into social behaviors and suggests how to model other agents in the environment . i propose an approach to modeling other agents that calls for limiting the amount of modeling of other agents to that which is sufficient to create the desired behavior . using this technique , i have been able to build robust social behaviors that use surprisingly little representation . i have used this methodology to build a number of social behaviors , like negotiation and making friends . i have built three simulations containing seven agents to drive and test this work . i have also conducted user studies to demonstrate that these agents appear to be emotional and can engage in non-trivial social_interactions while also being good characters with distinct personalities . 
evaluation of residue-residue contact_prediction in casp10 . we present the results of the assessment of the intramolecular residue-residue contact predictions from 26 prediction groups participating in the 10th round of the casp experiment . the most recently_developed direct coupling analysis_methods did not take part in the experiment likely because they require a very deep sequence_alignment not available for any of the 114 casp10 targets . the performance of contact_prediction methods was evaluated with the measures used in previous casps ( i . e . , prediction accuracy and the difference between the distribution of the predicted contacts and that of all pairs of residues in the target protein ) , as well as new measures , such as the matthews correlation_coefficient , the area under the precision-recall curve and the ranks of the first correctly and incorrectly predicted contact . we also evaluated the ability to detect interdomain contacts and tested whether the difficulty of predicting contacts depends upon the protein length and the depth of the family sequence_alignment . the analyses were carried out on the target domains for which structural homologs did not exist or were difficult to identify . the evaluation was performed for all types of contacts ( short , medium , and long_range ) , with emphasis placed on long_range contacts , i . e . those involving residues separated by at least 24 residues along the sequence . the assessment suggests that the best casp10 contact_prediction methods perform at approximately the same level , and comparably to those participating in casp9 . 
a 2-d processor array for massively parallel image_processing the concept of introducing image_processing logic within the spatial gaps of an array of photodiodes is the key factor behind the presented work . a two-dimensional massively parallel image_processing paradigm based on 8 8 pixel neighborhood digital processors has been designed . a low_complexity processor array architecture along with its instruction_set has been designed and fully verified on a fpga platform . various image_processing tests have been run on the fpga platform to demonstrate the functionality of a design that uses 12 parallel processors . the test results indicate that the architecture is scalable to support high frame rates while allowing for flexible processing due to inherent pro-grammability at a high level . the gate level logic_synthesis results of the processor targeting a 0 . 13 m cmos_technology indicates a low silicon_area complexity , allowing for image_sensor integration . iii dedication this thesis is dedicated to my beloved parents jagdish nelliparthi and radha rani nelliparthi . a special dedication to my maternal great-grandmother late srimathi durgamma thirumareddy , my paternal great-grandfather late sri lak-shmiah naidu nelliparthi , my maternal grandfather late sri appa rao thiru-mareddy , my paternal grandfather sri lakshmiah venkata rao nelliparthi and my sister late kumari swapna rani chirugudu . iv acknowledgments firstly , i would like to express my deep gratitude to my adviser dr . sina balkir and i am indebted for his continuous support , valuable guidance and encouragement throughout my masters program . also , i am indebted to my co-adviser dr . michael w . hoffman for his continued guidance , support and invaluable inputs throughout my thesis work . i would like to thank committee member dr . khalid sayood for providing critical review and recommendations for my thesis . a special thanks to mr . nathan schemm and mr . daniel j . white for their valuable assistance for this work . also , i would like to acknowledge the support of the faculty and staff of electrical_engineering department . finally , i am grateful to my parents , friends and well-wishers for their love , support , patience and encouragement they provided me with , in achieving my masters degree . 
algorithm 825 : a deep-cut bisection envelope algorithm for fixed points we present the bedfix ( bisection envelope deep-cut fixed_point ) algorithm for the problem of approximating a fixed_point of a function of two variables . the function must be lipschitz continuous with constant 1 with respect to the infinity norm; such functions are commonly found in economics and game_theory . the computed approximation satisfies a residual criterion given a specified error tolerance . the bedfix algorithm improves the befix algorithm presented in shellman and sikorski [2002] by utilizing "deep cuts , " that is , eliminating additional segments of the feasible domain which cannot contain a fixed_point . the upper bound on the number of required function evaluations is the same for bedfix and befix , but our numerical tests indicate that bedfix significantly improves the average-case performance . in addition , we show how bedfix may be used to solve the absolute criterion fixed_point problem with significantly better performance than the simple iteration method , when the lipschitz constant is less than but close to 1 . bedfix is highly efficient when used to compute residual solutions for bivariate functions , having a bound on function evaluations that is twice the logarithm of the reciprocal of the tolerance . in the tests described in this article , the number of evaluations performed by the method averaged 31 percent of this worst_case bound . bedfix works for nonsmooth continuous functions , unlike methods that require gradient information; also , it handles functions with minimum lipschitz constants equal to 1 , whereas the complexity of simple iteration approaches infinity as the minimum lipschitz constant approaches 1 . when bedfix is used to compute absolute criterion solutions , the worst-case complexity depends on the logarithm of the reciprocal of 1-q , where q is the lipschitz constant , as well as on the logarithm of the reciprocal of the tolerance . 
the deep versus the shallow : effects of co-speech gestures in learning from discourse this study concerned the role of gestures that accompany discourse in deep_learning processes . we assumed that co-speech gestures favor the construction of a complete mental_representation of the discourse content , and we tested the predictions that a discourse accompanied by gestures , as compared with a discourse not accompanied by gestures , should result in better recollection of conceptual information , a greater number of discourse-based inferences drawn from the information explicitly stated in the discourse , and poorer recognition of verbatim of the discourse . the results of three experiments confirmed these predictions . 
a general and multi-lingual phrase chunking model based on masking method several phrase chunkers have been proposed over the past few years . some state-of-the-art chunkers achieved better performance via integrating external resources , e . g . , parsers and additional training_data , or combining multiple learners . however , in many languages and domains , such external materials are not easily available and the combination of multiple learners will increase the cost of training_and_testing . in this paper , we propose a mask method to improve the chunking accuracy . the experimental results show that our chunker achieves better performance in comparison with other deep parsers and chunkers . for conll-2000 data_set , our system achieves 94 . 12 in f rate . for the base-chunking task , our system reaches 92 . 95 in f rate . when porting to chinese , the performance of the base-chunking task is 92 . 36 in f rate . also , our chunker is quite efficient . the complete chunking time of a 50k words document is about 50 seconds . 1 introduction automatic text chunking aims to determine non-overlap phrases structures ( chunks ) in a given sentence . these phrases are non-recursive , i . e . , they cannot be included in other chunks [1] . generally speaking , there are two phrase chunking tasks , including text chunking ( shallow parsing ) [15] , and noun_phrase ( np ) chuncking [16] . the former aims to find the chunks that perform partial analysis of the syntactic structures in texts [15] , while the later aims to identify the initial portions of non-recursive noun_phrase , i . e . , the first level noun_phrase structures of the parsing trees [17] [19] . in this paper , we extend the np chunking task to arbitrary phrase chunking , i . e . , base-chunking . in comparison , shallow parsing extracts not only the first level but also the other level phrase structures of the parsing tree into the flat non-overlap chunks . chunk information of a sentence is usually used to present syntactic relations in texts . in many natural_language_processing ( nlp ) areas , e . g . , chunking-based full parsing [1] [17] [24] , clause identification [3] [19] , semantic_role labeling ( srl ) [4] , text_categorization [15] and machine_translation , the phrase structures provide downstream syntactic_features for further analysis . in many cases , an efficient and pdf created with pdffactory pro trial version www . pdffactory . com
modelling and identification of yaw motion of an open-frame underwater robot a semi-autonomous unmanned underwater_vehicle ( uuv ) , named vsor , is being developed at the laboratory of sensors and actuators at the university of sao_paulo . the vehicle has been designed to provide inspection and intervention capabilities in specific missions in deep_water oil fields . with the aim of identifying the hydrodynamic motion_parameters , such as , drag_coefficient and added mass inertia , experimental trials with the vehicle prototype in a test tank have been performed . the methodology is based on the utilisation of an uncoupled 1-dof ( degree_of_freedom ) dynamic system equation of an underwater_vehicle and the application of the integral method which is the classical least_squares algorithm applied to the integral form of the system dynamic equations . an assessment of the feasibility of the method is presented . 
unexpected universality in disordered systems and modeling perpendicular recording_media in this thesis , i study the random_field ising_model ( rfim ) in both theoretical and applied perspectives . for theoretical interests , i compare the avalanche behavior in equilibrium and non-equilibrium and find an unexpected universality . the application part focuses on the reliability_test of the h ( m , m ) methodology , which has been used to measure microscopic properties of magnetic recording_media . based on rfim , an interacting random hysteron model has been developed and used to systematically test the reliability of the h ( m , m ) methodology . avalanche behavior in response to slowly changing external conditions is ubiquitous in a remarkably wide variety of dynamical_systems . when driven far from equilibrium , those systems display impulsive cascade of dynamic avalanches spanning a broad range of sizes . independent of their microscopic details , many non-equilibrium systems have been shown to have exactly the same dynamic avalanche behavior on many scales . this fact is called universality . so far , non-equilibrium systems were believed to be completely different from equilibrium ones . however , here we show that the zero-temperature rfim exhibits surprisingly similar avalanche behavior in equilibrium and out of equilibrium . this finding solves a highly controversial question , i . e . whether the equilibrium and non-equilibrium disorder-induced phase_transitions of rfim belong to the same universality class . our finding also indicates that generally equilibrium systems and their non-equilibrium counterparts may have deep connections . in state-of-the-art storage applications such as hard_disk drives , the intrinsic switching field distribution of the media grains is one of the most crucial properties defining the recording quality . however , this piece of microscopic information is very hard to measure macroscopically , especially for the perpendicular recording_media . using the interacting random hysteron model , we have studied the reliability of the recently_developed h ( m , m ) method . we demonstrated that this method does have several advantages over comparable methods . first , it has a well-defined reliability range and it allows for self-consistency checks . second , the presence of dipolar interactions in the range of typical recording_media substantially enhances the reliability of this method . third , it is robust even in the presence of randomness in exchange or magneto-static coupling within the range of a typical recording_media . 
evidence for asymmetric intra substantia_nigra functional connectivity - application to basal_ganglia processing the growing uses of deep_brain_stimulation for various basal_ganglia ( bg ) abnormalities have reinforced the need to better understand its functional circuitry and organization . here we focus on cortico-basal_ganglia pathways to test the "parallel , segregated" versus "funneling , integrated" theories . using manganese-enhanced mri ( memri ) together with principal_component spatiotemporal analysis , we previously described two patterns of caudomedial striatum efferent connectivity to the substantia_nigra pars reticulata ( snr ) that were hypothesized to represent the coexistence of integrated and segregated processes . these patterns corresponded to a direct mono-synaptic projection to the dorsolateral core of the sn and to a di-synaptic projection covering the entire nucleus . in the current study , memri of the rostrolateral striatum was carried out to test whether this coexistence remains in the mirror pathway , by measuring rostrolateral striatum efferent connectivity that is known to connect to the ventromedial snr . only one spatiotemporal pattern of manganese accumulation , corresponding to projections from the striatum , was observed . it corresponds to a mono-synaptic projection to the ventromedial snr covering snr laminas , but no manganese was observed at the dorsolateral snr core . together with our previous findings , this suggests functional asymmetry along the snr which is consistent with the known anatomical organization of dendrite and axonal 3d arborization . consequently , the polarized connectivity along the dorsolateral-ventromedial axis implies that funneling and integration occur in the core ( dorsolateral snr ) to the lamina ( ventromedial snr ) direction , whereas in the other direction , and within other parts of the snr , segregation predominates . 
constraint_based type_inference for guarded_algebraic_data_types constraint_based type_inference for guarded_algebraic_data_types guarded_algebraic_data_types subsume the concepts known in the literature as indexed types , guarded recursive datatype constructors , and first-class phantom types , and are closely_related to inductive types . they have the distinguishing feature that , when typechecking a function defined by cases , every branch may be checked under different assumptions about the type variables in scope . this mechanism allows exploiting the presence of dynamic tests in the code to produce extra static type information . we propose an extension of the constraint_based type system hm ( x ) with deep pattern_matching , guarded_algebraic_data_types , and polymorphic recursion . we prove that the type system is sound and that , provided recursive function definitions carry a type annotation , type_inference may be reduced to constraint solving . then , because solving arbitrary constraints is expensive , we further restrict the form of type annotations and prove that this allows producing so-called tractable constraints . last , in the specific setting of equality , we explain how to solve tractable constraints . to the best of our knowledge , this is the first generic and comprehensive account of type_inference in the presence of guarded_algebraic_data_types . inf rence de types base de contraintes pour les types de donn es alg briques gard s r_sum : les types de donn es alg briques gard s g n ralisent les concepts connus dans la litt rature sous les noms de types index s , constructeurs de types de donn es r cursifs gard s et types fant mes deprem ere classe , et sont intimement li s aux types inductifs . ils ont pour trait caract ristique le fait que , lors du typage d'une fonction d finie par cas , chaque branche peut tre typ e sous des hypoth ses diff rentes propos des variables de types connues . ce m canisme permet d'exploiter la pr sence de tests dynamiques dans le code pour produire une information de typage statique suppl mentaire . nous proposons une extension du syst me de types base de contraintes hm ( x ) avec filtrage profond , types de donn es alg briques gard s et r cursivit polymorphe . nous d montrons que ce syst me de types est s r et que , pourvu que les d finitions de fonctions r cursives soient annot es par un sch ma de types , l'inf rence de types se r duit la r solution de contraintes . ensuite , parce que la r solution de contraintes arbitraires est co teuse , nous restreignons la forme des annotations autoris es et d montrons que cela permet de produire des contraintes dites g rables . enfin , dans le cas particulier de l' egalit , nous expliquons comment r soudre les contraintes g rables . ` a notre connaissance , ceci est le premier trait g n rique et 
modeling the habitat suitability for deep_water gorgonian corals based on terrain variables a r t i c l e i n f o the coral species paragorgia arborea and primnoa resedaeformis are abundant and widely distributed gorgonians in north_atlantic waters . both species add significant habitat complexity to the benthic environment , and support a host of invertebrate species . mapping their distribution is an essential step in conservation and resource management , but challenging as a result of their remoteness . in this study , three predictive models ecological niche factor_analysis , genetic_algorithm for rule-set production and maximum entropy modeling ( maxent ) were applied to predict the distribution of species' suitable habitat across a region of r_st reef ( norwegian margin ) based on multiscale terrain variables . all three models were successful in predicting the habitat suitability for both gorgonian species across the study area , and the maxent predictions were shown to outperform other predictions . all three models predicted the most suitable habitats for both species to mainly occur along the ridges and on the upper section of the large slide , suggesting both species preferentially colonize topographic highs . jackknife tests for maxent predictions highlighted the seabed aspect in relation to p . arborea distribution , and the seabed relative position ( curvature ) in relation to the distribution of both species . given the vulnerability of deep_water corals to anthropogenic impacts , further comparative_study over a wider study area would be particularly beneficial for the management of the species . paragorgia arborea and primnoa resedaeformis are abundant and widely distributed gorgonian coral species observed in north_atlantic waters ( mortensen and buhl-mortensen , 2004; tendal , 1992 ) . the two species are among the largest deep_sea gorgonians , with p . arborea colonies reaching heights above the seafloor of 0 . 5 2 . 5 m and p . resedaeformis colony lengths of 0 . 5 1 m , with both providing complex biotic habitats for numerous invertebrate species ( buhl-mortensen et al . , 2010; mortensen et al . , 2008; roberts et al . , 2009 ) . the deep_water gorgonians are long lived and slow growing species , and are fragile and vulnerable to damage which may result from anthropogenic activity such as bottom trawling and oil exploration ( mortensen and buhl-mortensen , 2005 ) . mapping the distribution of deep_water coral species is fundamental in assessing the potential risks to these ecosystems posed by these activities , and for developing management plans , particularly as species protection legislation such as the ec habitats_directive stipulates protection of these habitats for all member states . however , such distribution mapping is challenging given the remoteness of these habitats , and predictive modeling techniques address 
generalization of figure-ground segmentation from binocular to monocular vision in an embodied biological brain model humans have the remarkable ability to generalize from binocular to monocular figure-ground segmentation of complex_scenes . this is clearly evident anytime we look at a photograph , computer monitor or simply close one eye . we hypothesized that this skill is due to of the ability of our brains to use rich embodied signals , such as disparity , to train up depth_perception when only the information from one eye is available . in order to test this hypothesis we enhanced our virtual robot , emer , who is already capable of performing robust , state-of-the-art , invariant 3d object_recognition [1] , with the ability to learn figure-ground segmen-tation , allowing him to recognize objects against complex backgrounds . continued development of this skill holds great_promise for efforts , like emer , that aim to create an artificial_general_intelligence ( agi ) . for example , it promises to unlock vast sets of training_data , such as google_images , which have previously been inaccessible to agi models due to their lack of embodied , deep_learning . more immediately practical implications , such as achieving human performance on the caltech101 object_recognition dataset [2] , are discussed . ment is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation thereon . disclaimer : the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements , either expressed or implied , 
a logic base tool set for real-time ada software_development this is a report on work conducted privately that explores the use of predicate_logic to reason about real-time design . safety and reliability issues associated with embedded real-time systems make greater demands on development engineers than non-real-time systems whose functional complexity is of similar degree . the effort undertaken here attempts to provide methods_and_tools for dealing with both the functional and temporal aspects of software_engineering . the goal is to improve the effectiveness of engineering and thus the productivity of a development team . the problem was approached using a variant of the idefo ( sadt ) modeling methodology , an adaptation equipped with primitives for expressing data types , time constraints , and process activation rules . the sadt methodology serves as the interface between an engineer and a &#8220;deep_structure&#8221; representation of a system expressed using logic predicates . the sadt framework supports the modeling of a system with an arbitrary number of processing resources using hierarchical specifications of time constraints and resource_utilization limitations . the concepts of resource_utilization , period , as well as &#8220;hard&#8221; and &#8220;soft&#8221; deadline are built into the approach . this particular report describes a tool set that has been built using these concepts . the tools illustrate that the logic based models are sufficient for conduction control path analysis and schedulability analysis . an example is provided of tool capability in which schedulable entities are identified in the sadt model assigned priorities , and then are shown to be schedulable by application of the critical instant test . the associated pdl , annotated with time constraints is produced as well . all steps are automatic . rate monotonic prioritization is selected for the example . experience with the tools indicates that this is a practical cost_effective approach to real-time systems_engineering . 
impact of wireless channel temporal variation on mac design for body area networks we investigate the impact of wireless channel temporal variations on the design of medium access_control ( mac ) protocols for body area networks ( bans ) . our measurements-based channel model captures large and small time-scale signal correlations , giving an accurate picture of the signal variation , specifically , the deep fades which are the features that mostly affect the behavior of the mac . we test the effect of the channel model on the performance of the 802 . 15 . 4 mac both in contention access mode and tdma access mode . we show that there are considerable differences in the performance of the mac compared to simulations that do not model channel temporal variation . furthermore , explaining the behavior of the mac under a temporal varying channel , we can suggest specific design choices for the emerging ban mac standard . 
real-time slam with octree evidence grids for exploration in underwater tunnels at 110m in diameter and over 350m in depth , the cenote zacat n in central mexico is a unique flooded sinkhole . a platform for conducting preliminary sonar tests is tethered in place . abstract we describe a simultaneous_localization_and_mapping ( slam ) method for a hovering underwater_vehicle that will explore underwater caves and tunnels , a true three dimensional ( 3d ) environment . our method consists of a rao-blackwellized particle_filter with a 3d evidence grid map representation . we describe a procedure for dynamically adjusting the number of particles to provide real-time performance . we also describe how we adjust the particle_filter prediction step to accommodate sensor degradation or failure . we present an efficient oc-tree data_structure which makes it feasible to maintain the hundreds of maps needed by the particle_filter to accurately model large environments . this octree structure can exploit spatial locality and temporal shared ancestry between particles to reduce the processing and storage requirements . to test our slam method , we utilize data_collected with manually-deployed sonar mapping vehicles in the wakulla_springs cave system in florida and the sistema zacat n in mexico , as well as data collected by the depthx vehicle in the test tank at the austin applied research_laboratory . we demonstrate our mapping and localization approach with these real_world datasets . zacat n is a flooded cenote ( sinkhole ) in tamaulipas , mexico , that has been measured at over 350m deep ( gary 2002 ) . the depths of the cenote remain unexplored , but during a preliminary expedition in may 2005 , we discovered that the upper 200m of zacat n is roughly a cylinder 110m wide that tapers slightly with depth ( figure 1 ) . zacat n is the deepest of a series of similar water-filled formations , which are thought to have formed as hydrothermal groundwater dissolved through a layer of limestone ( gary 2002 ) . zacat n has a small river flowing out through a tunnel near the surface , which indicates that water is flowing in from somewhere below the mapped regions , perhaps through navigable tunnels . the mineral-rich water in zacat n supports colorful microbial mats in the photic_zone and has exotic geo-chemical features which make it an excellent match 1
automated micro_architectural test_generation for validation of modern processors design_complexity of todays microprocessors is increasing at an alarming rate to cope up with the required performance_improvement by adopting complicated micro_architectural features such as deep pipelines , dynamic scheduling , out-of-order and superscalar execution , and dynamic speculation . since verification complexity is directly proportional to the design_complexity , considerable amount of time and resources are spent on design validation . in the current industrial practice , billions of random test programs generated at instruction_set_architecture ( isa ) level are used during simulation_based validation . however , architectural test_generation techniques have limitations in terms of exercising intricate micro_architectural artifacts . therefore , it is necessary to use micro_architectural details during test_generation . furthermore , there is a lack of automated techniques for directed test_generation targeting micro_architectural faults . to address these challenges , we present a directed test_generation technique at micro_architectural level for functional validation of microprocessors . a processor model is described in a temporal specification_language at micro-architecture level . the desired behaviors of micro-architecture mechanisms are expressed as temporal_logic properties . we use decompositional model_checking for systematic test_generation . our experiments using a processor based on the power_architecture t m technology 1 shows very promising_results in terms of test_generation time as well as test_program length . 
test_scheduling for circuits in micron to deep_submicron_technologies we discuss the test scheduling problem in this paper . we first provide a historical perspective of the original test_scheduling formulation that dealt only with resource conflicts , followed by the consideration of power constraint test_scheduling . we then move on to the recent formulations which include dealing with thermal constraint . we explain solutions , their limitations and the challenges that remain . with the emergence of on-chip sensors , in future it may be possible to leverage the use of such sensors to arrive at more efficient schedules . the paper explains these new opportunities and suggests research directions . this paper also contains an exhaustive list of references that may help the researchers and practitioners dealing with this problem . 
dependability analysis of a very large volume neutrino_telescope the srisl performed the dependability analysis of a very large volume neutrino_telescope in the km3net project ( eu fp6 km3net design study , contract no . 011937 ) , as part of the work group on risk_assessment and quality_assurance . the km3net is an over m 300 european project involving 40 institutes or university groups from 10 countries , to design , install and operate a deep sea research infrastructure hosting a neutrino_telescope with a volume of at least one cubic kilometer at the bottom of the mediterranean_sea . the neutrino detector will consist of several thousand optical sensor modules placed on mechanical structures connecting them in vertical assemblies . the sensors will be inter connected via watertight and pressure resistant connectors . their measured data will travel through an underwater network of specially designed multiplexed passive optical or active electronic equipment and optical_fibers to the onshore base . challenges in the availability features of this system rise from the extreme deployment , operation and maintenance conditions at depths of 3500 to 5500 meters ( depending on the site location ) . the dependability analysis performed in the srisl was a first order steady_state approximation and consisted of the following steps : treat the neutrino_telescope as a complex system; identify the system components and their operational interdependencies , and the required function of the telescope system . develop an appropriate mathematical_model to estimate the telescope unavailability based on the unavailabilities of its components and a set of steady_state unavailability evaluation correlations depending on possible component repair/test characteristics . obtain results for a variety of alternative detector network configurations , and distances of the detector from the onshore facilities . the final study presented ranges and combinations of possible component unavailability values that satisfied a fixed unavailability requirement for the telescope system . it also developed dependability requirements for major components and/or subsystems consistent with an overall system performance target . the results depicted the dependence of the system unavailability on the number of optical modules and the alternative deep_sea infrastructure configurations for transferring the measured signals . this work has been presented as a detailed report on the dependability of the telescope , and will be included in the telescope technical design report which is still in preparatory phase . further research is under way to establish a realistic perspective on the practically attainable component reliability ranges , and develop suitable cost reliability correlations to depict the trade offs between component cost and reliability . these correlations will collaborate with the 
leakage_current in sub-quarter micron mosfet : a perspective on stressed delta_iddq testing the effectiveness of single threshold i_ddq measurement for defect_detection is eroded owing to higher and more variable background leakage_current in modern vlsis . delta_i_ddq is identified as one alternative for deep_submicron current measurements . often delta_i_ddq is coupled with voltage and thermal stress in order to accelerate the failure_mechanisms . a major concern is the i ddq limit setting under normal and stressed conditions . in this article , we investigate the impact of voltage and thermal stress on the background leakage . we calculate i_ddq limits for normal and stressed operating_conditions of 0 . 18 m n-mosfets using a device simulator . intrinsic leakage_current components of transistor are analyzed and the impact of technology scaling on effectiveness of stressed i ddq testing is also investigated . 
analysis of mos cross-coupled lc-tank oscillators using short-channel device equations new analytical techniques for estimating the large-signal periodic steady_state solution of mos lc-tank oscillators using short-channel device equations are presented . these techniques allow us to make quantitative estimates of the oscillator steady_state performance without the need for time-consuming transient simulations using simulators such as spice . further , our engineering techniques provide insight and quantitative understanding on the design of current-day , deep_submicron mos lc-tank os-cillators and serve as a starting_point in a design strategy that includes complete phase_noise/timing jitter analysis and optimization . our analytical results for a cross-coupled lc-tank oscillator that was previously fabricated and tested are in good agreement with simulations using hspice . 
a scalable built-in self-recovery ( bisr ) vlsi architecture and design_methodology for 2d-mesh based on-chip_networks on-chip networks ( ocns ) have been proposed to solve the complex on-chip communication problems . in very deep_submicron_era , ocn will also be affected by faults in chip due to technologies shrinking . many researches focused on fault detection and diagnosis in ocn systems . however , these approaches didn't consider faulty ocn system recovery . this paper proposes a scalable built-in self-recovery ( bisr ) design_methodology and corresponding surrounding test ring ( str ) architecture for 2d-mesh based ocns to extend the work of diagnosis . the bisr design_methodology consists of str architecture generation , faulty system recovery , and system correctness maintenance . for an n n mesh , str architecture contains one controller and 4n test modules which are formed as a ring-like connection surrounding the ocn . moreover , these test modules generate test patterns for fault_diagnosis during warm-up time . according to these diagnosis results , the faulty system is recovered . finally , this paper proposes a fault-tolerant routing algorithm , through-path fault_tolerant ( tp-ft ) routing , to maintain the correctness of this faulty system . in our experiments , the proposed approach can reduce 68 . 33 79 . 31% unreachable packets and 4 . 86 23 . 6% latency in comparison with traditional approach with 8 . 48 13 . 3% area_overhead . 
phase_space and power spectral approaches for eeg-based automatic sleep-wake classification in humans : a comparative_study using short and standard epoch lengths sleep_disorders in humans have become a public_health issue in recent years . sleep can be analysed by studying the electroencephalogram ( eeg ) recorded during a night's sleep . alternating between sleep-wake stages gives information related to the sleep quality and quantity since this alternating pattern is highly affected during sleep_disorders . spectral composition of eeg_signals varies according to sleep_stages , alternating phases of high energy associated to low_frequency ( deep_sleep ) with periods of low energy associated to high_frequency ( wake and light sleep ) . the analysis of sleep in humans is usually made on periods ( epochs ) of 30-s length according to the original rechtschaffen and kales sleep scoring manual . in this work , we propose a new phase_space-based ( mainly based on poincar plot ) algorithm for automatic_classification of sleep-wake states in humans using eeg data gathered over relatively short-time periods . the effectiveness of our approach is demonstrated through a series of experiments involving eeg data from seven healthy adult female subjects and was tested on epoch lengths ranging from 3-s to 30-s . the performance of our phase_space approach was compared to a 2-dimensional state_space approach using the power spectral ( ps ) in two selected human-specific frequency bands . these powers were calculated by dividing integrated spectral amplitudes at selected human-specific frequency bands . the comparison demonstrated that the phase_space approach gives better performance in the case of short as well as standard 30-s epoch lengths . 
an application of nonlinear resistive networks in computer_vision an application of nonlinear resistive networks in computer_vision in this thesis , an improved optical_flow algorithm is presented , as well as a hardware called "tanh" component . the new approach performs an optimization that reduces the error at spatial discontinuities , and increases the computational speed using analog circuit implementation . simple simulation of this design is tested using hspice . we also build a simulator for a complicated circuit using c , which focuses more on speed , and less on transient . for image smoothing and segmentation problems and optical_flow problems , a series of test images are fed to both the resistor network and the so-called "tanh" network to determine how effective the tanh network is in image_analysis . acknowledgments i would like to express my deepest gratitude to my supervisor , professor berthold k . horn , for giving me the opportunity and support to work on this thesis , for providing me with crucial guidance and encouragement throughout years , and for serving as a model of an enthusiastic teacher and a dedicated scientist . i am also grateful to dr . richard lanza , my thesis co-supervisor , for providing invaluable insight , ideas , and support . i have learned much from their expertise and am grateful for the time they have devoted to me . this work has benefited from many discussions with professor john l . wyatt about non-linear networks , professor hae-seung lee about analog circuit_design , professor rahul sarpeshkar about non-linear analog circuit_design and dr . christopher terman about non-linear circuit_simulation . i thank all of them for their enthusiasm , patience , and advice . love and yajun fang , for their help and invaluable advice in the past three years . i would also like to thank all my friends in mit and elsewhere for the great moment we have shared in the past years , especially thank benjamin walter , for his advice and help in thesis revising . i would like to express my deep love to my family , especially my parents , who have granted me unconditional love throughout my life; my sisters bizhou and jin , my brothers bihui and juhui , who have given me all kinds of support in both my career and my life . finally and most importantly , i would like to thank my girl friend , xiang xian , for her love and encourage . 
artificial_intelligence as the year 2000 approaches approaches as a human being and defying an interrogator to i am aware that i have acquired a reputation for being critical of the claims made for artificial_intelligence ( ai ) . it is true that i am repelled by some of the hype that i hear and by the lack of self_criticism that it implies . however , the underlying philosophical questions have long been of deep interest to me . i can even claim to be the first ai professional; my definition of a professional being someone who performs a service and accepts a fee in return . i had read alan turing's paper "calculating machinery and intelligence" when it appeared in mind in 1950 , and was deeply impressed . without attempting to emulate turing's erudition-or the wit in which he clothed it-i wrote a short article on the same subject and offered it to the editor of the spectator , a british journal devoted to contemporary affairs . the editor accepted it and i duly received a fee . apart from his mathematical papers , i still consider the paper in o ~ mind to be the best thing turing o ever wrote . he began with the z question "can machines think ? " in z that form , he found the question to be unsatisfactorily formulated . an attempt to extract the essential o underlying point of interest led him-to propose the famous turing_test . p-instead of asking whether a partic-~ ular machine could think , he suggested that one should instead ask whether it could pass this test . the test involved the machine posing determine whether it was a man or a woman . turing admitted that he ]c strong arguments to put form favor of the view that a digita puter would one day be able his test , although he was incli believe that it would . he ma interesting suggestion that t/c chine might be equipped learning program and then like a child . the way he put that the machine might bq grammed to simulate a child' ! rather than an adult's brain brings out the point that th is intimately connected learning . turing was quite aware that the education of a machine would , like that of a child , be a long , drawn-out process . he also saw practical difficulties . ti computer would not have 1 ( and could not be sent to s& like a normal child . even ifth ciency could be overcome by ' engineering" he was afrai other children might make sive 
performance analysis of ccsds file delivery protocol and erasure coding techniques in deep_space environments the rising demand for multimedia services even in hazardous environments , such as space_missions and military theatres , and the consequent need of proper internetworking technologies have revealed the performance limits experienced by tcp protocol over long-delay and lossy links and highlighted the importance of the communication features provided by the protocol architectures proposed by the consultative committee for space data systems ( ccsds ) . this paper proposes a ccsds file delivery protocol ( cfdp ) extension , based on the implementation of erasure coding schemes , within the cfdp itself , in order to assure high reliability to the data_communication even in presence of very critical conditions , such as hard shadowing , deep-fading periods and intermittent links . different encoding techniques are considered and various channel conditions , in terms of bit error ratio and bandwidth values , are tested . 
transformations to automate model change evolution as models are elevated to first-class artifacts within the software_development lifecycle , new approaches are needed to address the accidental complexities associated with current modeling practice ( e . g . , manually evolving the deep_hierarchical structures of large system models can be error prone and labor intensive ) . this research abstract presents a model transformation approach to automate model evolution and testing tools to improve the quality of model transformation . 
automated legislative drafting : generating paraphrases of legislation in this paper , we describe which roles deep_structures of law play in drafting legislation . deep_structures contain a formal description of the intended normative eeects of a new regulation . we will discuss mechanisms that can be used to generate diierent paraphrases of regulations automatically . since it is possible to test the paraphrases on legal knowledge_based_systems , we have provided two extra design steps in legislative drafting which can be supported by automated tools . deep_structures are straightforward descriptions of the normative eeects of regulations . each deep_structure distinguishes desired and undesired behaviour , and has no further internal structure such as paragraphs or exception structures . this paper describes methods to translate a deep_structure to representations of diierent types of codes , i . e . paraphrases . each representation of a code has a diierent structure , according to the choice we make during the translation regarding : a ) the initial assumptions of the regulation , i . e . modelling from desired or undesired behaviour , b ) the level_of_abstraction , c ) the viewpoint of the law , i . e . the category of norm subjects , and d ) the type of deontic modalities the regulation largely uses . all paraphrases have the same eeects as the deep_structure , but with diierent features , and are suitable for diierent goals . 
rvab : rational varied-depth search in siguo game game playing is one of the classic problems of artificial_intelligence . the siguo game is an emerging field of research in the area of game-playing programs . it provides a new test_bed for artificial_intelligence with imperfect information . to improve search efficiency for siguo with more branches and the uncertain payoff in the game_tree , this paper presents a modified alpha_beta search_algorithm , which is called rational varied-depth alpha_beta ( rvab ) . the rvab the basic ideas of rvab algorithm is : if player get much information about opponent during playing game , they can do more deep thinking about strategies of game . if player can only get few , very uncertain information about opponent during playing game , they don't think more deep about their strategy because it is worthless for player to speculate the strategies of game under very uncertain . experiments show that rvab achieves the goals of the improvability of visited nodes efficiency , although it costs a little more memory . 
influence of manufacturing variations in iddq measurements : a new test criterion this work presents a new -based test criterion supported by the characteristics of a set of experimental testing measurements realized over different samples of industrial ics and by the definition of the corresponding simulation model . comparing the current consumptions of a specific circuit a significant correlation between measurements can be observed . the current behaviour can be divided into two parts : ( 1 ) a circuit dependent one , which has a major contribution , and affects equally all the devices in a given die , and ( 2 ) a smaller die dependent fraction due to variations , defective and non-defective , of each of the devices of a specific die . in this paper , a current model is defined , introducing the effects of manufacturing variations in the basic equations of the sub-threshold current to explain that double behaviour . the results show how it is possible to obtain a lot of information from measurements and how other test selection criteria can be applied to increase the testing sensitivity and quality . testing is performed by measuring the quiescent_current of the power supply and comparing this obtained value with a fixed limit . in the last years this technique has proven to be very useful and has been an important contribution to improve the quality of cmos_ics [1] . the selection of the current limit is one of the open and key questions in the utilization of testing . the selection criterion is based on a pass/fail limit and it is just efficient with failures which provoke high increments of consumption . however , the sensitivity of the is insufficient in order to determine the correctness of ics with consumptions close to the current limit . the current limit has to be fixed as a tradeoff between the yield and the required quality , that is , it must be selected to reduce the cost impact of yield_loss , without imposing a penalty on defect_detection [2] . some approaches have been proposed for a better limit selection : current estimation methodologies [3] , statistical_analysis of data [4][5] or the consideration of global process_variations [6] . in addition , the forecast for deep_sub_micron_technologies shows an abrupt increment of the background current in several orders_of_magnitude and a decrement of some defect effects [7] . the separation between defective and non-defective currents and , therefore , the sensitivity will 
microprocessor software__based_self-testing sbst in the microprocessor test_process microprocessor test and validation ever-increasing market demands for higher computational performance at lower cost and power_consumption continually drive processor vendors to develop new microprocessor generations . every new generation incorporates technology innovations from different research domains , such as microelectronics , digital_circuit design , and computer architecture . all these technology advancements , however , impose new challenges on microprocessor testing . as device geometries shrink , deep_submicron delay_defects become more prominent , thereby increasing the need for at-speed tests . also , increases in the core operating_frequency and speed of i/o interfaces necessitate more-expensive external test_equipment . in addition , as multicore processor architectures become more popular , the time needed to test the chip scales with the number of cores , unless the inherent execution parallelism is exploited during testing . these test technology challenges prompted the semiconductor industry during the past decade to consider new testing_methods that can be incorporated in an established microprocessor test flow . the purpose of such methods is to achieve the target dppm ( defective parts per million ) rate that high_quality product development demands , but without imposing excessive overhead in the test budget or interfering with the well-optimized , high_performance processor design . such a testing method , which was introduced in the 1980s and has garnered renewed interest in the past decade , is functional self-testing , 1 also called instruction-based self-testing , or software_based_self-testing . sbst has become more accepted for microprocessor testing , and it already forms an integral part of the manufacturing_test flow for major processor vendors . 2 , 3 this article is the first attempt at classifying sbst approaches according to the way they generate self-test programs . hence , we propose a taxonomy for the most representative sbst approaches according to their test_program development philosophy . we also summarize research approaches for optimizing other key sbst aspects , and we highlight the potential of sbst for detecting and diagnosing faults at different stages of the microprocessor test and validation process . the key idea of sbst is to exploit on-chip program-mable resources to run normal programs that test the processor itself . the processor generates and applies functional-test_patterns using its native instruction_set , virtually eliminating the need for additional test-specific hardware . ( test-specific hardware such as scan_chains might exist in the chip , but sbst normally does not use this hardware . ) also , the test is applied at the processor's actual operating_frequency . a typical flow for an sbst application in a microprocessor chip comprises the following three steps ( see figure 1 ) : 1 . test code and test_data 
validation of experts versus atlas-based and automatic registration methods for subthalamic_nucleus targeting on mri objects in functional stereotactic neurosurgery , one of the cornerstones upon which the success and the operating time depends is an accurate targeting . the subthalamic nucleus ( stn ) is the usual target involved when applying deep_brain_stimulation for parkinson's_disease ( pd ) . unfortunately , stn is usually not clearly visible in common medical imaging_modalities , which justifies the use of atlas-based segmentation techniques to infer the_stn_location . materials and methods eight bilaterally implanted pd_patients were included in this study . a three-dimensional t1_weighted sequence and inversion recovery t2-weighted coronal slices were acquired pre-operatively . we propose a methodology for the construction of a ground_truth of the_stn_location and a scheme that allows both , to perform a comparison between different non_rigid_registration algorithms and to evaluate their usability to locate the stn automatically . results the intra-expert variability in identifying the_stn_location is 1 . 06 0 . 61 mm while the best non_rigid_registration method gives an error of 1 . 80 0 . 62 mm . on the other hand , statistical_tests show that an affine registration with only 12 degrees of freedom is not enough for this application . conclusions using our validation evaluation scheme , we demonstrate that automatic stn localization is possible and accurate with non_rigid_registration algorithms . 
auto-gopher -a wire-line rotary-hammer ultrasonic drill developing technologies that would enable nasa to sample rock , soil , and ice by coring , drilling or abrading at a significant depth is of great importance for a large number of in-situ exploration missions as well as for earth applications . proven techniques to sample mars subsurface will be critical for future nasa astrobiology missions that will search for records of past and present life on the planet , as well as the search of water and other resources . a deep corer , called auto-gopher , is currently being developed as a joint effort of the jpl's ndeaa laboratory and honeybee robotics corp . the auto-gopher is a wire-line rotary-hammer drill that combines rock breaking by hammering using an ultrasonic actuator and cuttings removal by rotating a fluted bit . the hammering mechanism is based on the ultrasonic/sonic drill/corer ( usdc ) that has been developed as an adaptable tool for many of drilling and coring applications . the usdc uses an intermediate free-flying mass to transform the high_frequency vibrations of the horn tip into a sonic hammering of a drill_bit . the usdc concept was used in a previous task to develop an ultrasonic/sonic ice gopher . the lessons learned from testing the ice gopher were implemented into the design of the auto-gopher by inducing a rotary motion onto the fluted coring bit . a wire-line version of such a system would allow penetration of significant depth without a large increase in mass . a laboratory version of the corer was developed in the ndeaa lab to determine the design and drive parameters of the integrated system . the design configuration lab version of the design and fabrication and preliminary testing results are presented in this paper . 
wordless sounds : robust speaker_diarization using privacy-preserving audio representations this paper investigates robust privacy_sensitive_audio_features for speaker diarization in multiparty conversations : ie . , a set of audio_features having low linguistic_information for speaker diarization in a single and multiple distant microphone scenarios . we systematically investigate linear_prediction ( lp ) residual . issues such as prediction order and choice of representation of lp residual are studied . additionally , we explore the combination of lp residual with subband information from 2 . 5 khz to 3 . 5 khz and spectral slope . next , we propose a supervised framework using deep neural architecture for deriving privacy_sensitive_audio_features . we benchmark these approaches against the traditional mel frequency cepstral coefficients ( mfcc ) features for speaker diarization in both the microphone scenarios . experiments on the rt07 evaluation dataset show that the proposed approaches yield diarization performance close to the mfcc features on the single distant microphone dataset . to objectively evaluate the notion of privacy in terms of linguistic_information , we perform human and automatic_speech_recognition tests , showing that the proposed approaches to privacy_sensitive_audio_features yield much lower recognition accuracies compared to mfcc features . 
nonlinear low_dimensional regression using auxiliary coordinates when doing regression with inputs and outputs that are high_dimensional , it often makes sense to reduce the dimensionality of the inputs before mapping to the outputs . much work in statistics and machine_learning , such as reduced-rank regression , sliced inverse regression and their variants , has focused on linear dimensionality_reduction , or on estimating the dimensionality_reduction first and then the mapping . we propose a method where both the dimensionality_reduction and the mapping can be nonlinear and are estimated jointly . our key idea is to define an objective_function where the low_dimensional coordinates are free parameters , in addition to the dimensionality_reduction and the mapping . this has the effect of decoupling many groups of parameters from each other , affording a far more effective optimization than if using a deep network with nested mappings , and to use a good initialization from sliced inverse regression or spectral methods . our experiments with image and robot applications show our approach to improve over direct regression and various existing approaches . we consider the problem of low_dimensional regression , where we want to estimate a mapping between inputs x r dx and outputs y r dy that are both continuous and high_dimensional ( such as images , or control commands for a robot ) , but going through a low-dimensional , or latent , space z r dz : y = g ( f ( x ) ) , where z = f ( x ) , y = g ( z ) and d z < d x , d y . in some situations , this can be preferable to a direct ( full-dimensional ) regression y = g ( x ) , for example if , in addition to the regression , we are interested in obtaining a low-dimensional representation of x for its own sake ( e . g . visualization or feature_extraction ) . even when the true mapping g is not low_dimensional , using a direct regression requires many parameters ( d x d y in linear_regression ) and their estimation may be unreliable with small sample sizes . using a low-dimensional composite mapping g f with fewer parameters can be seen as a form of regularization and lead to better generalization with test_data . finally , a common practical approach is to reduce the dimension of x independently of y , say with principal_component_analysis ( pca ) , and then solve the regression . however , the latent coordinates z obtained in this way do not necessarily preserve the information that is needed to predict y . this is the same reason why one would use linear_discriminant_analysis 
measuring cortical thickness using an image domain local surface model and topology preserving segmentation we present a measure of gray_matter ( gm ) thickness based on local surface models in the image domain . thickness is measured by integrating gm probability maps along the white_matter ( wm ) surface normal direction . the method is simple to implement and allows statistical_tests to be performed in the gray_matter volume . a novel topol-ogy preserving segmentation method is introduced that is able to accurately recover gm in deep sulci . we apply this methodology to a longitudinal_study of gray_matter atrophy in a patient cohort diagnosed with frontotemporal demen-tia ( ftd ) spectrum disorders . following image_based nor-malization of gm thickness maps , results show significant reduction in cortical thickness in several brodmann areas spanning temporal , parietal and frontal lobes across subjects . 
digital modeling and testing research on digging mechanism of deep_rootstalk_crops the digital model of the laboratory bench parts of digging deep_rootstalk_crops were established through adopting the parametric model technology based on feature . the virtual assembly of the laboratory bench of digging deep_rootstalk_crops was done and the digital model of the laboratory bench parts of digging deep_rootstalk_crops was gained . the vibrospade , which is the key part of the laboratory bench of digging deep_rootstalk_crops was simulated and the movement parametric curves of spear on the vibrospade were obtained . the results show that the spear was accorded with design requirements . it is propitious to the deep rootstalk . 
synthesizing geometry constructions in this paper , we study the problem of automatically solving ruler/compass based geometry construction problems . we first introduce a logic and a programming_language for describing such constructions and then phrase the automation problem as a program synthesis problem . we then describe a new program synthesis technique based on three key insights : ( i ) reduction of symbolic reasoning to concrete reasoning ( based on a deep theoretical result that reduces verification to random_testing ) , ( ii ) extending the instruction_set of the programming_language with higher_level primitives ( representing basic constructions found in textbook chapters , inspired by how humans use their experience and knowledge gained from chapters to perform complicated constructions ) , and ( iii ) pruning the forward exhaustive search using a goal_directed heuristic ( simulating backward reasoning performed by humans ) . our tool can successfully synthesize constructions for various geometry problems picked up from high_school textbooks and examination papers in a reasonable amount of time . this opens up an amazing set of possibilities in the context of making classroom teaching interactive . 
a self-applicable supercompiler a supercompiler is a program which can perform a deep transformation of programs using a principle which is similar to partial_evaluation , and can be referred to as metacomputation . supercompilers that have been in existence up to now ( see 12] , 13] ) were not self-applicable : this is a more diicult problem than self-application of a partial evaluator , because of the more intricate logic of supercompilation . in the present paper we describe the rst self-applicable model of a supercompiler and present some tests . three features distinguish it from the previous models and make self-application possible : ( 1 ) the input language is a subset of refal which w e refer to as at refal . ( 2 ) the process of driving is performed as a transformation of pattern_matching graphs . ( 3 ) metasystem jumps are implemented , which a l l o ws the supercom-piler to avoid interpretation whenever direct computation is possible . 
efficient_methods for unsupervised learning of probabilistic_models efficient_methods for unsupervised learning of probabilistic_models high_dimensional probabilistic_models are used for many modern scientific and engineering data_analysis tasks . interpreting neural spike trains , compressing video , identifying features in dna microarrays , and recognizing particles in high_energy_physics all rely upon the ability to find and model complex structure in a high_dimensional_space . despite their great_promise , high_dimensional probabilistic_models are frequently computationally intractable to work with in practice . in this thesis i develop solutions to overcome this intractability , primarily in the context of energy based models . a common cause of intractability is that model distributions cannot be analytically normalized . probabilities can only be computed up to a constant , making training exceedingly difficult . to solve this problem i propose 'minimum probability flow learning' , a variational technique for parameter_estimation in such models . the utility of this training technique is demonstrated in the case of an ising_model , a hopfield auto-associative_memory , an independent_component_analysis model of natural_images , and a deep_belief_network . a second common difficulty in training probabilistic_models arises when the parameter space is ill-conditioned . this makes gradient_descent optimization slow and impractical , but can be alleviated using the natural gradient . i show here that the natural gradient can be related to signal whitening , and provide specific prescriptions for applying it to learning_problems . it is also difficult to evaluate the performance of models that cannot be analytically normalized , providing a particular challenge to hypothesis_testing and model comparison . to overcome this , i introduce a method termed 'hamiltonian annealed importance_sampling , ' which more efficiently estimates the normalization constant of non-analytically-normalizable models . this method is then used to calculate and compare the log likelihoods of several state of the art probabilistic_models of natural image patches . 1 finally , many tasks performed with a trained probabilistic model ( for instance , image_denoising or inpainting and speech_recognition ) involve generating samples from the model distribution , which is typically a very computationally expensive process . i introduce a modification to hamiltonian monte_carlo sampling that reduces the tendency of sampling trajectories to double back on themselves , and enables statistically independent samples to be generated more rapidly . taken together , it is my hope that these contributions will help scientists and engineers to build and manipulate probabilistic_models . 2 acknowledgements
symbolic model_checking composite_web_services using operational and control behaviors this paper addresses the issue of verifying if composite_web_services design meets some desirable properties in terms of deadlock freedom , safety ( something bad never happens ) , and reachability ( something good will eventually happen ) . composite_web_services are modeled based on a separation of concerns between business and control aspects of web_services . this separation is achieved through the design of an operational behavior , which defines the composition functioning according to the web_services' business_logic , and a control behavior , which identifies the valid sequences of actions that the operational behavior should follow . these two behaviors are formally defined using automata-based_techniques . the proposed approach is model_checking-based where the operational behavior is the model to be checked against properties defined in the control behavior . the paper proves that the proposed technique allows checking the soundness and completeness of the design model with respect to the operational and control behaviors . moreover , automatic translation procedures from the design models to the nusmv model checker's code and a verification tool are reported in the paper . web_services are widely used for developing business_to_business applications whose performance spreads by default over orga-nizations' boundaries . indeed , web_services rely on a set of platform-independent and vendor-neutral specifications that offer the necessary means for their description , discovery , invocation , and mainly composition ( yang , zhang , & lan , 2007 ) . despite the great interest of the research and industry communities in web_services , some challenging issues remain pending and hence , hinder the adoption of web_services to develop robust , dynamic , and safe business applications . examples of issues include verifica-the severity of these issues intensifies when several component web_services are put together to form composite_web_services ( bensli-in this paper , we address the ''thorny'' issue of verifying the design of composite_web_services . developing business applications that end_users trust requires a deep investigation of the different and independent operations and behaviors that the component web_services in a composition execute and exhibit , respectively . for instance , having a deadlock in a web_service-based business transaction is a simply disaster for all stakeholders . although software vendors can guarantee the safety of their web_services , the development , testing , and verification of these web_services are done independently from other vendors' peers , which means a serious lack of how these web_services behave when put together . to tackle this problem , we use model_checking , a powerful formal and fully automatic technique for the verification of 
protein_protein binding_affinity prediction from amino_acid sequence motivation protein_protein_interactions play crucial roles in many biological processes and are responsible for smooth functioning of the machinery in living_organisms . predicting the binding_affinity of protein_protein complexes provides deep insights to understand the recognition mechanism and identify the strong binding partners in protein_protein interaction networks . results in this work , we have collected the experimental binding_affinity data for a set of 135 protein_protein complexes and analyzed the relationship between binding_affinity and 642 properties obtained from amino_acid sequence . we noticed that the overall correlation is poor , and the factors influencing affinity depends on the type of the complex based on their function , molecular weight and binding_site residues . based on the results , we have developed a novel methodology for predicting the binding_affinity of protein_protein complexes using sequence-based features by classifying the complexes with respect to their function and predicted percentage of binding_site residues . we have developed regression models for the complexes belonging to different classes with three to five properties , which showed a correlation in the range of 0 . 739-0 . 992 using jack-knife test . we suggest that our approach adds a new aspect of biological significance in terms of classifying the protein_protein complexes for affinity prediction . 
syntactically annotated corpora of estonian syntactically annotated corpora are needed 1 ) to train and test parsers and various language technological products-grammar checkers , information retrievers and extractors , machine translators etc; 2 ) to check the agreement of existing linguistic theories with the real language usage . the corpora can be annotated on different levels of depth . in shallow syntactically annotated corpora a syntactic function is determined for every wordform; in deep syntactically annotated corpora ( treebanks ) also the dependency structure is determined for every sentence ( graphically represented as a tree ) . there exists a constraint grammar shallow syntactic parser for estonian , developed by k . m risep and t . puolakainen . to train the parser , we have annotated texts of written estonian ( 20 000 words of fiction , 6 000 words of legal text and 10 000 words of newspaper texts ) . by now we have extended the size of the corpus up to 200 000 words . we have also started to build two versions of estonian treebank . a parallel_corpus of 50 sentences from j . gaarder's novel "sophie's_world" has been annotated and aligned and a constraint grammar plus phrase_structure hybrid treebank is being developed , currently consisting of 2400 automatically_generated trees , 149 of them manually revised . 
infaunal marsh foraminifera from the outer_banks , north_carolina , usa the distribution and abundance of live ( rose bengal stained ) and dead , shallow infaunal ( 0 1_cm_depth ) and deep_infaunal ( >1_cm_depth ) benthic foraminifera have been documented at three locations representing different salinity settings on the fringing marshes along the pamlico_sound and currituck_sound coasts of north carolina's outer_banks . two cores taken at each site represent the lower and higher marsh . twenty-two taxa were recorded as live . of these , eight taxa were found only at shallow infaunal depths; the other 14 taxa occur at deep_infaunal depths in one or more cores . only jadammina macrescens and tiphotrocha comprimata were recorded as living in all six cores . the distributions of the other taxa were restricted by combinations of infaunal depth , salinity regime and location on the marsh . the tests of infaunal foraminifera were generally more likely to be preserved in the lower marsh than the higher marsh at low-and intermediate-salinity sites . the opposite pattern was evident at the high-salinity site but this may be due to the low numbers of deep_infaunal specimens recovered . arenoparrella mexicana , haplophragmoides wilberti , jadammina macrescens and trochammina inflata are the most resistant taxa , whereas miliammina fusca is the species whose tests are most likely to be lost to post-mortem degradation . in five of the six cores , foraminiferal assemblages and populations do not differ significantly with depth which suggests that the foraminifera of the 0 1_cm_depth interval provide an adequate model upon which paleoenvironmental ( including former sea_level ) reconstructions can be based . aibstract north_carolina . the deepest records of living marsh fora-the distribution and abundance of live ( rose ben[ , -li stained ) and dead . shallow infaunal ( 0-1_cm_depth ) and deep_infaunal ( >1_cm_depth ) benthic foraminifera have been documented ;rt three locations representing different salinity settings on the fringing marshes along the pamlico_sound' and currituck_sound coasts of north carolina's outer_banks . two cores taken a t each site represent the lowcr . and higher marsh . twenty-two tax; ! were recorded as live . of these , eight h x a were found only a t shallow infaunal depths; the other 14 taxa occur at deep_infaunal depths in one or more cores . only jadammina ntacrescerts and tipholro-cha compriinata were recorded as living in all six cores . the distributions of the other taxa were restricted by combinations of irrfaunal depth , salinity regime and location on the marsh . the tests of infar~nal foraminifera were generally more likely to be 
using interactive multimedia for teaching_and_learning object_oriented software_design object_oriented ( oo ) design and programming is an abstract and complex domain , and students have problems with understanding the concepts and applying them to the design of software systems . at napier_university , approximately 400 undergraduate_students per year take object_oriented software_design ( oosd ) . there is a growing need to find a way to support students' learning . the question was what we could do to support large number o f students with an abstract domain . the solution we came up with was using interactive multimedia ( imm ) for learning and teaching the subject . key strengths of imm are interactivity and visualisation . imm can help students develop clear understanding of oo concepts such as objects , classes , and message_passing through interactivity and visualisation . learning requires active thinking . although the imm materials will be initiated from a lecture or a tutorial , they are aimed to be self-directed learning materials . the materials should be able to encourage students to think actively in order to promote deep_learning . hyperlinks have been used to prompt internal question and reflection . graphical representation is used to visualise oo design_process from real_world physical objects to software system built . research_into students' learning using these features is needed in order to explore new design aspects with imm to improve learning in higher_education . two different types of learning materials have been developed to support this research . one is a resource-oriented material , which is similar to primary courseware [1] and will be initiated by a lecturer in a lecture . the other is a task-oriented material with embedded hyperlinks to the resource-oriented ones , and will be used in a tutorial . to investigate the effectiveness of hyperlinks in promoting cognitive interactivity , we test three types of hyperlinks , which are no hyperlink , static hyperlink presented as default and dynamic hyperlink appearing with tips when there is a mistake or incorrect answer made . this poster will describe trials conducted at bmnel and napier universities . the results and comparison made from the trials in terms of students' attitudes to imm assisted learning and their performance will be presented along with findings about hyperlinks and visualisation in learning . ada is still a popular language for introductory programming courses in many university computer_science departments . it is often used as a 'super pascal' for teaching basic algorithmic constructs before moving on to 'big picture' object_oriented languages . it is a good educational language because of its clear , 
novel speech_processing mechanism derived from auditory neocortical circuit analysis analysis of the prominent anatomical and physiological features of auditory thalamus and neocortex has enabled construction of models designed to identify functionality emergent from these biological circuits . these models have recently been shown to provide powerful computational mechanisms for processing of continuous time-varying sequences such as speech; testing on speech databases has yielded positive initial results that are reported here . the model constitutes a novel hypothesis of underlying functions of auditory neocortex , and also represents a novel approach to speech_processing . research in our laboratory has been concentrating on the phenomenon of long_term_potentiation ( ltp ) [3] , which is the most likely candidate for a substrate of neocortical learning and memory . a set of simple learning rules was formulated based on physiological properties of ltp-i ) synaptic weight can only increase , ii ) every increase is small fixed change , and iii ) low saturation threshold permits only 5-10 weight increases over the whole period of training [2] [10] . a series of models were constructed based on the known anatomical cortical features-sparse-random connec-tivity in the superficial cortical layers , emergence of the cortical patches defined by the radius of the local inhibition , and feedback inhibition and masking . the above variety of neocortical features specify a biologically constrained class of microcircuits , which typically perform pattern_recognition or classification via competitive_learning and lateral_inhibition [6] [5] . simulations of those circuits lead to efficient hardware implementations , with a proven utility for pattern_recognition via efficient approximation of statistical pattern_recognition methods ( e . g . bayes classifiers ) [5] . key anatomical properties of the auditory model being reviewed in [16] ( see also table 1 ) include i ) topographic ( mgv ) versus broadly-tuned ( mgm ) thalamic nuclei , convergently projecting to primary auditory_cortex; local cortical circuits composed of roughly 100 : 1 excitatory to inhibitory cells with lateral_inhibition; and iii ) vertical columnar organization projecting from middle to superficial to deep layers . key physiological properties include : i ) plastic ( nmda-dependent ) synapses from broadly tuned mgm afferents versus non-plastic synapses from topographic mgv affer-ents; ii ) plasticity via long_term_potentiation ( ltp ) ; and iii ) time courses for excitation versus inhibition of roughly 1 : 100 . learning in the model is based on the physiological induction and expression rules for synaptic long_term_potentiation or ltp [12] [11] which have been shown in previous modeling efforts to give rise to useful computational properties [2] [10] [9] [5] [7] [8] . superficial cortical layer broadly tuned afferents from non-specific thalamic nucleus sparse connectivity ( p ~ . 1 ) ltp of 
boussinesq green naghdi rotational water wave theory a r t i c l e i n f o using boussinesq scaling for water waves while imposing no constraints on rotationality , we derive and test model equations for nonlinear water wave transformation over varying depth . these use polynomial basis functions to create velocity profiles which are inserted into the basic equations of motion keeping terms up to the desired boussinesq scaling order , and solved in a weighted residual sense . the models show rapid convergence to exact solutions for linear dispersion , shoaling , and orbital velocities; however , properties may be substantially improved for a given order of approximation using asymptotic rearrangements . this improvement is accomplished using the large numbers of degrees of freedom inherent in the definitions of the polynomial basis functions either to match additional terms in a taylor_series , or to minimize errors over a range . explicit coefficients are given at o ( 2 ) and o ( 4 ) , while more generalized basis functions are given at higher_order . nonlinear performance is somewhat more limited as , for reasons of complexity , we only provide explicitly lower order nonlinear terms . still , second order harmonics may remain good to kh 10 for o ( 4 ) equations . numerical tests for wave transformation over a shoal show good agreement with experiments . future work will harness the full rotational performance of these systems by incorporating turbulent and viscous stresses into the equations , making them into surf_zone models . modern boussinesq water wave theory began in the 1960s as moderate computing power became more available to researchers . papers by peregrine ( 1967 ) , and madsen and mei ( 1969 ) extended the shallow_water_equations asymptotically into deeper water to arrive at inviscid , nonlinear , wave evolution equations with leading order dispersive effects . these were confined to relatively shallow_water , with k 0 h 0 b 1 . 5 , where k 0 is a typical wavenumber and h 0 is a typical water_depth and so had a limited range of application . however , even at these early_stages it was realized that entire families of equations could be developed that were asymptotically identical but had differing properties . with exceptions ( witting , 1984 ) , this finding was largely ignored until the early 1990s when several groups of researchers ( madsen and s rensen , 1992; madsen et al . , 1991; nwogu 1993 ) used various methods of asymptotic rearrangement to improve properties of boussinesq equations so that dispersion relations were accurate to the nominal deep_water limit of k 0 h 
beyond the short answer question with research_methods tutor research_methods tutor is a new intelligent_tutoring_system created by porting the existing implementation of the autotutor system to a new domain , research_methods in behavioural_sciences , which allows more interactive dialogues . the procedure of porting allowed for an evaluation of the domain independence of the autotutor framework and for the identification of domain related requirements . specific recommendations for the development of other dialogue-based tutors were derived from our experience . recent advances in intelligent_tutoring_system technology focus on developing dialogue-based tutors , which act as conversational partners in learning . autotutor [5 , 7] , one of the prevalent systems in this field , claims to simulate naturalistic tutoring sessions in the domain of computer literacy . an innovative characteristic of autotutor is the use of a talking head as the primary interface with the user . the system is also claimed to be domain independent and to be capable of supporting deep reasoning in the tutorial dialogue . one goal of the current project is to test these claims . another motivation was the fact that the domain of autotutor , computer literacy , provides limited potential for activating deep reasoning mechanisms . by porting the tutor to a new domain , which requires in-depth qualitative reasoning , we can address issues of domain independence and framework usability in a concrete manner . the new tutor , based on the autotutor framework , is built on the domain of research_methods in behavioural_sciences and thus was named research_methods tutor ( rmt ) . 
acm src poster : gem : a formal dynamic environment for hpc pedagogy computing is undergoing a dramatic shift from sequential to parallel_processing . with this shift comes new challenges : how to debug code with multiple processes and threads , and how to effectively teach these programming concepts . traditional testing tools are ineffective and inefficient when it comes to detecting deep seated logical bugs in parallel code , and often lack a gui in popular ide's . no support exists for teaching actual courses based on these tools either . in previous work , my research_group provided an mpi testing tool called isp and integrated it into eclipse's ptp via the gem plug-in . i expand on these with enhanced graphical interactions , interception of threaded behavior , and providing a tool for hpc pedagogy . 
a perspective on regional and global strategies of multinational enterprises multinational enterprises ( mnes ) are the key drivers of globalization , as they foster increased economic interdependence among national markets . the ultimate test to assess whether these mnes are global themselves is their actual penetration level of markets across the globe , especially in the broad 'triad' markets of nafta , the european_union and asia . yet , data on the activities of the 500 largest mnes reveal that very few are successful globally . for 320 of the 380 firms for which geographic sales data are available , an average of 80 . 3% of total sales are in their home region of the triad . this means that many of the world's largest firms are not global but regionally based , in terms of breadth and depth of market coverage . globalization , in terms of a balanced geographic distribution of sales across the triad , thus reflects a special , and rather unusual , outcome of doing international business ( ib ) . the regional concentration of sales has important implications for various strands of mainstream ib research , as well as for the broader managerial debate on the design of optimal strategies and governance structures for mnes . introduction globalization , in the sense of increased economic interdependence among nations , is a poorly_understood phenomenon . in this paper , we focus on the key actors in the globalization process , namely the firms that drive this process . a relatively small set of multinational enterprises ( mnes ) accounts for most of the world's trade and investment . indeed , the largest 500 mnes account for over 90% of the world's stock of foreign_direct_investment ( fdi ) and they , themselves , conduct about half the world's trade ( rugman , 2000 ) . yet , this paper demonstrates that most of these firms are not 'global' companies , in the sense of having a broad and deep penetration of foreign markets across the world . instead , most of them have the vast majority of their sales within their home leg of the 'triad' , namely in north_america , the european_union ( eu ) or asia . this new view on 'globalization' is very different from the conventional , mainstream perspective . the latter perspective focuses primarily on macro-level growth patterns in trade and fdi , and compares these data with national gdp growth rates , but without ever analyzing the equivalent micro-level growth data for the mnes responsible for the trade and fdi flows ( united_nations , 2002 ) . 
automatic verification of hybrid systems with large discrete state_space we address the problem of model_checking hybrid systems which exhibit nontrivial discrete behavior and thus cannot be treated by considering the discrete states one by one , as most currently available verification tools do . our procedure relies on a deep integration of several techniques and tools . a first-order extension of and-inverter-graphs ( aigs ) serves as a compact representation format for sets of configurations which are composed of continuous regions and discrete states . boolean reasoning on the aigs is complemented by first-order reasoning in various forms and on various levels . these include subsumption checks for simple constraints , test vector_generation for fast inequality checks of boolean combinations of constraints , and an exact implication check . these techniques are integrated within a model_checker for universal ctl . technically , it deals with discrete_time hybrid systems with linear differentials . the paper presents the approach , the architecture of a prototype implementation , and first experimental_data . 
situated simplification testing satisfaction of guards is the essential operation of concurrent constraint_programming ( ccp ) systems . we present and prove correct , for the first time , an incremental algorithm for the simultaneous tests of entailment and disentailment of rational tree constraints to be used in ccp systems with deep guards ( e . g . , in akl or in oz ) . the algorithm is presented as the simplz+ztion of the constraints which form the ( possibly deep ) guards and which are situated at different nodes in a tree ( of arbitrary depth ) . the nodes correspond to local computation spaces . in this algorithm , a variable may have multiple bindings ( which each represent a constraint on that same variable in a different node ) . these may be realized in various ways . we give a simple fixed_point algorithm and use it for proving that the tests implemented by another , practical algorithm are correct and complete for entailment and disentailment . we formulate the results in this paper for rational tree constraints; they can be adapted to finite and feature trees . 
delay_testing considering power_supply_noise eeects we propose a new delay test_generation technique that can take into account the impact of the power_supply_noise on the signal propagation_delays . this is diierent from existing delay_fault models and test_generation techniques that ignore the dependence of path_delays on the applied test_patterns and cannot capture the worst-case timing scenarios in deep_submicron_designs . in addition to sensitizing the fault and propagating the fault eeects to the primary outputs , our new tests also produce the worst-case power_supply_noise on the nodes in the target path . thus , the tests also cause the worst-case propagation_delay for the nodes along the target path . our experimental results on benchmark_circuits show that the new delay_tests produce signiicantly longer delays on the tested paths compared to the tests derived using existing delay_testing methods . 
layer-wise learning of deep generative_models when using deep , multi-layered architectures to build generative_models of data , it is difficult to train all layers at once . we propose a layer-wise training procedure admitting a performance guarantee compared to the global optimum . it is based on an optimistic proxy of future performance , the best latent marginal . we interpret auto-encoders in this setting as generative_models , by showing that they train a lower_bound of this criterion . we test the new learning procedure against a state of the art method ( stacked rbms ) , and find it to improve performance . both theory and experiments highlight the importance , when training deep_architectures , of using an inference model ( from data to hidden variables ) richer than the generative model ( from hidden variables to data ) . 
parkinson's_disease : a motor_control study using a wrist robot deep_brain_stimulation ( dbs ) is the most common surgical_procedure for patients_with_parkinson's_disease ( pd ) . dbs has been shown to have a positive effect on pd symptoms; however , its specific effects on motor_control are not yet understood . we introduce the novel use of a wrist robot in studying the effects of stimulation on motor performance and learning . we present results from patients performing reaching movements in a null field and in a force field with and without stimulation . we discuss special cases where robotic testing reveals otherwise undiagnosed impairments , and where clinical scores and robot-based scores display opposing trends . 
gmdh algorithm for optimal model choice by the external error criterion with the extension of definition by model bias and its applications to the committees and neural_networks in the case of substantial noise , i . e . , for inaccurate and incomplete data , the use of the group method of data handling ( gmdh ) algorithm leads to sharp and rather deep minimums of dependency of external criterion of accuracy measured on testing sample on the complexity of model structure . this minimum indicates the optimal model . in practice , however , if the noise is just noticeable , i . e . , if data are accurate , the minimum becomes indefinite or does not exist at all . in this case , an extension of definition is needed based on a new criterion such as , e . g . , the value of a model bias measured on the two identical data samples . the combi-natorial gmdh algorithm with an extension of definition by the model bias can be used as a neuron in committees and in repeatedly multilayered neural networks for solving the problems of medical monitoring . 
onboard classifiers for science event detection on a remote_sensing spacecraft typically , data collected by a spacecraft is downlinked to earth and preprocessed before any analysis is performed . we have developed classifiers that can be used onboard a spacecraft to identify high priority data for downlink to earth , providing a method for maximizing the use of a potentially bandwidth limited downlink channel . onboard analysis can also enable rapid reaction to dynamic events , such as flooding , volcanic_eruptions or sea_ice break-up . four classifiers were developed to identify cryosphere events using hyperspectral images . these classifiers include a manually constructed classifier , a support_vector_machine ( svm ) , a decision_tree and a classifier derived by searching over combinations of thresholded band ratios . each of the classifiers was designed to run in the computationally constrained operating environment of the spacecraft . a set of scenes was hand-labeled to provide training_and_testing data . performance results on the test data indicate that the svm and manual classifiers outperformed the decision_tree and band-ratio classifiers with the svm yielding slightly better classifications than the manual classifier . the manual and svm_classifiers have been uploaded to the eo-1 spacecraft and have been running onboard the spacecraft for over a year . results of the onboard analysis are used by the autonomous sciencecraft experiment ( ase ) of nasa's new millennium program onboard eo-1 to automatically target the spacecraft to collect follow-on imagery . the software demonstrates the potential for future deep space_missions to use onboard decision_making to capture short-lived science events . 
autonomous mobile_robot research at louisiana_state university's robotics research_laboratory need for specific capabilities in amrs , including rapid multimodal sensing and integration , real-time response , real-time interruptability , and fault tolerance . research_laboratory ( rrl ) is conducted with the help of a specially modified denning mrv ( mobile_robot vehicle ) iii research robot ( figure 1 ) . mrv iii is an excellent test_bed for robotics research : it has a straightforward user_interface , it is easily modified to accept new features , and it s the department of computer_science at louisiana_state_university ( lsu ) has been involved in robotics research since 1985 when the robotics research_laboratory ( rrl ) was established as a research and teaching program specializing in autonomous mobile_robots ( amrs ) . researchers at rrl are conducting high_quality research in amrs with the goal of identifying the computational problems and the types of knowledge that are fundamental to the design and implementation of autonomous_mobile robotic systems . in this article , we overview the projects that are currently under way at lsu's rrl . autonomous mobile_robots ( amrs ) are synthetic operational systems that can govern themselves while they accomplish given objectives and simultaneously manage their resources and maintain their integrity . the practical importance of research in amrs has steadily increased over the last several years because of developments in technology that have brought such systems closer to reality . already , industrial robots are being used in applications that involve monotonous or tedious tasks as well as applications in hazardous environments such as nuclear_power_plants . next_generation robots are being planned for applications such as deep_sea mining and salvage operations , servicing-assembly tasks in space , and maintenance activities in toxic environments such as nuclear plants . continued research in this area is certain to pay large dividends in the near future . the various unstructured environments in which amrs are expected to operate can change rapidly . amr can unexpectedly encounter environmental threats that are hazardous to itself or its environment ( for example , the outbreak of a fire or the failure of one of the robot's internal systems ) . dynamic and complex environments such as these impose a also be operated as a slave to another computer through its external ports . aside from mrv iii , the lab is equipped with various robotic manipulators , several macintosh iis and pcs , and a hero robot . rrl is continually upgrading its facilities to expand research capabilities . this upgrading is especially true of the denning mrv iii robot . rrl is currently adding several features to mrv iii , most 
novel design_methodology for high_performance xor_xnor circuit_design as we scale down to deep_submicron ( dsm ) technology , noise is becoming a metric of equal importance as power , speed , and area . smaller feature_sizes , low_voltage , and high_frequency are some of the characteristics for dsm circuits . a novel design methodology for the design of energy_efficient noise-tolerant xor_xnor circuits that can operate at low voltages is proposed . the proposed circuits are characterized and compared with previously_published circuits for reliability and energy efficiency . to test their driving capability , the proposed gates are implanted in an existing 5-2 compressor design and is shown to provide superior performance . the average noise threshold energy is used for quantifying the noise immunity . simulation results show that the proposed circuits are more noise-immune and displays better power_consumption results as well as power-delay product characteristics . also , the circuits prove to be faster and successfully works at all ranges of supply_voltage starting from 0 . 6v to 3 . 3v . 
type checking xml transformations acknowledgments i wish to thank helmut seidl for inspiring me to conduct my research in the field of xml transformations , and i feel deep gratitude towards him for guiding me like virgil through the circles of tree transducers . he employed me on a project position providing ideal conditions for conducting my research . it was funded by the " deutsche forschungs gemeinschaft " under the headword " mttcheck " . i am grateful to many colleagues who in one way or another contributed to this thesis . special thanks to thomas gawlitza , ingo scholtes , and peter ziewer for careful reading of the manuscript and for their helpful comments_and_suggestions . last but not least , i want to thank jennifer for her love and constant_support on the stony path of writing this work . abstract xml_documents are often generated by some application in order to be processed by a another program . for a correct exchange of information it has to be guaranteed that for correct inputs only correct outputs are produced . the shape of correct documents , i . e . , their type , is usually specified by means of schema languages . this thesis is concerned with methods for statically guaranteeing that transformations are correct with respect to pre_defined types . therefore , we consider the xml transformation language tl abstracting the key_features of common xml transformation languages . we show that each tl program can be decomposed into at most three stay macro tree transducers . by means of classical results for stay macro tree transducers , this decomposition allows to formulate a type checking algorithm for tl . this method , however , has even for small programs an exorbi-tant run_time . therefore , we develop an alternative approach , which allows at least for a large class of transformations that they can be type checked in polynomial time . the developed algorithms have been implemented and tested for practical examples . 1 introduction when axel_thue published his article about logical problems [thu10; st00] in 1910 , he certainly would not have expected that his ideas of trees and their rewriting will become as popular as_90 years later with the introduction of the extensible_markup_language ( xml ) . the w3c 1 , an international consortium to develop web_standards , describes this language as " a simple , very flexible text format derived from sgml . originally designed to meet the challenges of large_scale electronic_publishing , xml is also playing an increasingly important role in the exchange of the variety of 
web-prospector - an automatic , site-wide wrapper_induction approach for scientific deep-web_databases wrapper_induction techniques traditionally focus on learning wrappers based on examples from one class of web_pages , i . e . from web_pages that are all similar in structure and content . thereby , traditional wrapper_induction targets the understanding of web_pages generated from a database using the same generation template as observed in the example set . applying such techniques to web_sites generated from biological databases , however , we found that there is a need for wrapping of structurally diverse web_pages from multiple classes making the problem more challenging . furthermore , we observed that such scientific web_sites do not just provide mere data , but they also tend to provide schema information in terms of data labels giving further cues for solving the web_site wrapping task . in this paper we present a novel approach to automatic information_extraction from whole web_sites that considers the novel challenge and takes advantage of the additional clues commonly available in scientific deep web_databases . the solution consists of a sequence of steps : 1 . classification of similar web_pages into classes , 2 . discovery of these classes and 3 . wrapper_induction for each class . our approach thus allows us to perform unsupervised information_retrieval from across an entire web_site . we test our algorithm against three real_world biochemical deep web_sources and report our preliminary_results , which are very promising . 
brain functional integration decreases during propofol-induced loss of consciousness consciousness has been related to the amount of integrated information that the brain is able to generate . in this paper , we tested the hypothesis that the loss of consciousness caused by propofol anesthesia is associated with a significant reduction in the capacity of the brain to integrate information . to assess the functional structure of the whole brain , functional integration and partial correlations were computed from fmri data acquired from 18 healthy volunteers during resting wakefulness and propofol-induced deep sedation . total integration was significantly reduced from wakefulness to deep sedation in the whole brain as well as within and between its constituent networks ( or systems ) . integration was systematically reduced within each system ( i . e . , brain or networks ) , as well as between networks . however , the ventral attentional network maintained interactions with most other networks during deep sedation . partial correlations further suggested that functional connectivity was particularly affected between parietal areas and frontal or temporal regions during deep sedation . our findings_suggest that the breakdown in brain integration is the neural correlate of the loss of consciousness induced by propofol . they stress the important_role played by parietal and frontal areas in the generation of consciousness . 
equivalent circuit modeling of guard_ring structures for evaluation of substrate crosstalk isolation a substrate_coupling equivalent circuit can be derived for an arbitrary guard_ring test structure by way of f-matrix computation . the derived netlist represents a unified impedance network among multiple sites on a chip surface and allows circuit_simulation for evaluation of isolation effects provided by guard rings . geometry dependency of guard_ring effects attributes to layout patterns of a test structure , including such as area of a guard_ring as well as location distance from the circuit to be isolated by the guard_ring . in addition , structural dependency arises from vertical impurity concentrations such as p<sup>+</sup> , n<sup>+</sup> , and deep n-well , which are generally available in a deep_submicron cmos_technology . the proposed simulation_based prototyping technique of guard_ring structures can include all these dependences and thus can be strongly helpful to establish isolation strategy against substrate_coupling in a given technology , in an early stage of soc developments . 
i ic diagnosis using multiple supply pad i_ddq s ddq has been used extensively as a reliability screen for shorting defects in digital integrated_circuits . unfortunately , single-threshold i_ddq methods applied to devices fabricated in deep_submicron_technologies result in unacceptably high_levels of yield_loss . the significant increase in subthresh-old leakage_currents in these technologies makes it difficult to set an absolute pass/fail threshold to fail only defective devices . 1 there have been proposed solutions to the subthreshold leakage_current problem and , more recently , to process_variation issues . current signatures , delta_i_ddq and ratio i_ddq are based on a self-relative analysis , in which the average i_ddq of each device is factored into the pass/fail threshold value . 2 4 we refer to the technology dependency of subthreshold leakage_current as a technology-related variation effect to contrast it with the chip-to-chip variation effects caused by changes in process parameters ( process_variation ) . we base our approach on a previous v ddt-based method_called transient_signal_analysis ( tsa ) . 5 tsa uses regression_analysis to calibrate for process and technology-related variation effects by cross-correlating multiple supply pin transient_signals measured under each test_sequence . the i ddq signal analysis , or qsa , method proposed uses a set of i_ddq measurements instead , each obtained from the individual supply pins of the device under test ( dut ) . the process and technology calibration procedure used in qsa is based on a regression_analysis procedure similar to tsa . in tsa , we referred to signal variation resulting from defects as regional variation , to contrast it with the global variations introduced by process and technology-related effects . for transient_signals , the supply rail's resistance-capacitance ( rc ) network modifies signal characteristics , such as phase and magnitude , at different supply pin test points . in qsa , only the resistive component of the supply rail network introduces variation in the i ddq values at different supply pins . in either case , the position of the defect in the layout with respect to any given power_supply pin is related to the amount of regional defect variation observed at that pin . for qsa , the variation is directly related to the resistance between the defect site and the pin . for example , a larger value of i_ddq is expected on supply pins closer to the defect site because of the smaller resistance . therefore , the multiple i_ddq measurements can be used to detect the defect as well as triangulate the physical position of the defect in the layout . 
for problems sufficiently hard . . . ai needs cogsci the view sketched is cognitive_science relevant to ai problems ? yes but only when these problems are sufficiently hard . when they qualify as such , the best move for the clever ai researcher is to turn not to yet another faster machine bestowed by moore's_law , and not to some souped-up version of an instrument in the ai toolbox , but rather to the human mind's approach to the problem in question . despite turing's ( turing 1950 ) prediction , made over half a century ago , that by now his test ( the tur-ing test , of course ) would be passed by machines , the best conversational computer can't out-debate a sharp toddler . the mind is still the most powerful thinking thing in the known universe; a brute fact , this . but what's a " sufficiently hard " problem ? well , one possibility is that it's a problem in a set of those which are turing-solvable , but which takes a lot of time to solve . as you know , this set can be analyzed into various subsets; complexity theorists do that for us . some of these subsets contain only problems that most would say are very hard . for example , most would say that an np_complete problem is very hard . but is it sufficiently hard , in our sense ? no . let p be such a problem , a decision_problem for f associated with some finite alphabet a , say . we have an algorithm a that solves p . 1 and odds are , a doesn't correspond to human_cognition . the best way to proceed in an attempt to get a computer to decide particular members of a is to rely on computational horsepower , and some form of pruning to allow decisions to be returned in relatively short order . what we have just described structurally , maps with surprising accuracy onto what was done in ai specifically for the problem of chess . in his famous " 20 questions " paper , written in the very early days of ai and cogsci ( and arguably at the very dawn of a sub-field very relevant , for reasons touched upon later , to issues dealt with herein : computational cognitive model-ing and cognitive architectures ) , newell ( newell 1973 ) suggested that perhaps the nature of human_cognition could be revealed by building a machine able to play good chess . but deep_blue was assuredly not what newell had in mind . deep_blue was an experiment in harnessing horsepower to muscle through a turing-1 i . e . , for every u a 
magnetic_field-based 3d etree modelling for multi-frequency eddy_current inspection more and more solid_state_magnetic field sensors such as hall devices are used in eddy_current inspection ( ec ) to acquire magnetic_field signals . this work extends the previous analytical model , i . e . 2d extended truncated region eigenfunction expansion ( etree ) of ec , and focuses on establishment of 3d etree of multi-frequency eddy_current inspection ( mfec ) on stratified conductors , while taking into account the solid_state_magnetic field sensors for field quantification and rectangular coils for field excitation . 3d finite_element modelling ( fem ) and a hybrid modelling are adopted for verification of the established model . it has been noticed that the 3d etree implements the fast_and_accurate computation of magnetic_field signals of mfec . following that , the directional characteristics of ec with rectangular excitation coils are investigated , which reveals that the coil width contributes more to the measurement sensitivity than the coil length , and benefits the evaluation of defects and anisotropic conductivity profile of conductors in the follow-up study . 1 introduction to evaluate the integrity and structural health of metallic structures such as stratified conductors , electromagnetic non-destructive evaluation ( ende ) techniques , especially eddy_current ( ec ) and transient eddy_current , are preferred and used in real-time inspections [1] . eddy_current testing traditionally relies on the detection of impedance changes in a pickup coil as it moves across the inspected specimen . accordingly , the theoretical modelling for ec previously was implemented merely to predict : ( 1 ) the impedance signals from the stranded induction coils for time-harmonic field [2] , [3]; and ( 2 ) the electromotive_force ( emf ) signals from coils for transient field [4] . however , it is formidable for traditional ec to detect deep flaws in conductive materials , because
whanausip : a secure peer_to_peer communications platform whanausip : a secure peer_to_peer communications platform this thesis_presents a novel mechanism for achieving secure and reliable peer_to_peer communications on the internet . whanausip merges a sybil-proof distributed_hash_table with mature sip technology to enable instant_messaging , audio chat , and video_conferencing that is resilient to censoring , eavesdropping , and forgery . performance and security evaluations performed on the planetlab network demonstrate that the majority of resource lookups return within 5 seconds . these results indicate that whanausip delivers practical performance with respect to call session initialization latency for voip telephony . furthermore , the tests demonstrated that lookup performance was minimally affected during a sybil cluster id attack , illustrating the network's resilience to malicious adversaries . the thesis delivers three software packages for public use : a general whanau distributed_hash_table implementation , a whanausip gateway , and a desktop im/voip client . acknowledgments this project began a few years_ago , when i had a " there must be a better way " moment . the truth is that we live in a digital world where there is a growing ability and willingness to invade privacy . additionally , countries and organizations can use their leverage over centralized services to monitor and censor their citizens . i knew information should be free , and i knew people had a right to free_speech , but i didn't know how to make it so . it was really in the world of pdos that i gained the skills i needed to fulfill my vision . i owe a great amount of gratitude to chris lesniewski-laas for his deep insights , novel ideas , and continuing support . every meeting with him not only brought clarity to the project , but also clarity to my vision . you've helped me grow tremendously and i am lucky to have had you as an advisor . i would also like to thank professor kaashoek . you have a magical ability to turn complex ideas into a flowing story . your comments_and_suggestions have been wonderfully useful and it has truly been an honor to work with you . last but not least , i need to thank my pea . serena , you have helped me stay focused . you have helped me with my writing and presentation skills . you have inspired , motivated , and kept me excited . you have supported me day in and day out , and i could not thank you enough for just being by my side . 
subthalamic deep_brain_stimulation and impulse control in parkinson's_disease . background and purpose experimental studies suggest that deep_brain_stimulation ( dbs ) of the subthalamic nucleus ( stn ) induces impulsivity in patients with parkinson's disease ( pd ) . the purpose of this study was to assess various measures of impulse control in pd_patients with stn_dbs in comparison to patients receiving medical therapy . methods in a cross-sectional evaluation , 53 consecutively eligible patients were assessed for impulsivity with the barratt impulsiveness scale , for impulse control disorders ( icds ) using the minnesota impulsive disorders interview , and for obsessive-compulsive symptoms using the maudsley obsessional-compulsive inventory . results independent samples t-tests revealed that compulsivity scores were not different between dbs patients and patients without dbs . however , impulsivity scores were significantly_higher in dbs patients . additionally , icds were observed in 3 of 16 ( 19% ) dbs patients and in 3 of 37 ( 8% ) medically treated patients . no association was found between the use of dopamine agonists and impulsivity in dbs patients . conclusions our data suggest that screening for impulsivity and icds should be performed prior to dbs , and that patients should be monitored for these problems during follow-up . prospective trials are needed to confirm the findings of this exploratory study and to elucidate the reasons of a possible induction of impulsivity by stn_dbs . 
heart_rate analysis in 24 patients treated with 150 mg amitriptyline per day . twenty-four patients treated with 150 mg amitriptyline per day for an episode of major_depression underwent a standardized heart_rate analysis ( hra ) before therapy and after 14 days . the battery of cardiovascular reflex tests included the determination of the coefficient of variation ( cv ) while resting and during deep respiration , a spectral analysis of heart_rate , the heart_rate response to standing , and the valsalva manoeuvre . the results of the initial hra did not differ from a group of 24 normal control subjects matched for age and sex . on day 14 of treatment the patients showed significantly reduced values of heart_rate_variability in all tests ( p < 0 . 0001 ) , probably due to the anticholinergic side effects of amitriptyline . heart_rate increased from 78 . 1 to 93 . 6 bpm on average ( p < 0 . 0001 ) . abnormal cv at rest was registered in 96% of the patients; during deep respiration 29% showed abnormal cv results . an abnormal spectral analysis was found in 100% of the cases ( low_frequency peak : 42% , mid-frequency peak : 100% , high_frequency peak : 79% ) . the heart_rate response to standing was abnormal in 75% and the valsalva test in 33% of the cases . eighty-eight percent of the patients fulfilled the criteria of a cardiovascular autonomic neuropathy under the conditions of amitriptyline therapy . as yet , the consequences of these changes for the patients have not been sufficiently elucidated . 
thermo-mechanical behaviour of a compacted swelling clay compacted unsaturated swelling clay is often considered as a possible buffer material for deep nuclear_waste disposal . an isotropic cell permitting simultaneous control of suction , temperature and pressure was used to study the thermo-mechanical behaviour of this clay . tests were performed at total suctions ranging from 9 to 110 mpa , temperature from 25 to 80 c , isotropic pressure from 0 . 1 to 60 mpa . it was observed that heating at constant suction and pressure induces either swelling or contraction . the results from compression tests at constant suction and temperature evidenced that at lower suction , the yield pressure was lower , the elastic compressibility parameter and the plastic compressibility parameter were higher . on the other hand , at a similar suction , the yield pressure was slightly influenced by the temperature; and the compressibility parameters were insensitive to temperature changes . the thermal hardening phenomenon was equally evidenced by following a thermo-mechanical path of loading heating-cooling-reloading . 
eigenvector centrality in industrial sat instances despite the success of modern sat solvers on industrial instances , most of the progress relies on intensive experimental testing of improvements or new ideas . in most cases , the behavior of cdcl solvers cannot be predicted and even small changes may have a dramatic positive or negative effect . in this paper , we do not try to improve the performance of sat solvers , but rather try to improve our understanding of their behavior . more precisely , we identify an essential structural property of industrial instances , based on the eigenvector centrality of a graphical representation of the formula . we show how this static value , computed only once over the initial formula casts new light on the behavior of cdcl solvers . we also advocate for a better partitionning of industrial problems . our experiments clearly suggest deep discrepancies among the families of benchmarks used in the last sat competitions . 
investigations on linear transformations for speaker adaptation and normalization diese dissertation ist auf den internetseiten der hochschulbibliothek online verf gbar . zwei dinge sind zu unserer arbeit n tig : unerm dliche ausdauer und die bereitschaft , etwas , in das man viel zeit und arbeit gesteckt hat , wieder wegzuwerfen . acknowledgments first i would like to thank my supervisor prof . dr . -ing . hermann ney , head of the lehrstuhl f r informatik vi at the rwth_aachen , for the opportunity to realize this work as part of your team . you introduced me to the exciting field of pattern_recognition in general and speech_recognition in particular . you allowed me great latitude to pursue my ideas and followed them with great interest . i would also like to thank you for the numerous interesting and enlightening discussions we had . i am also grateful to my second supervisor prof . christian wellekens , who is with the multimedia communications department of institut eur com , france , for your interest in my work , the in-depth reading of this thesis and the valuable comments . stephan kanthak , you have been an enormous help in many computer problems and difficult debugging sessions . i always admired your deep insight in computer technology , linux and c++ . besides that , we had many funny talks about the world and his brother . ralf schl ter , i am grateful for our discussions and numerous sessions at the whiteboard , which gave me a deeper insight into speech_recognition and helped to solve a couple of problems . you kept the computers running and patiently dealt with all my requests . i always enjoyed very much the relaxing time at lunch and coffee breaks with the to all current and former colleagues of the lehrstuhl f r informatik vi for the motivating atmosphere , many interesting discussions and also many laughter . i want to express a very special thank to my girlfriend beate . you had an important part in the success of this thesis . without you , life would be less wonderful . nicht zuletzt m chte ich besonders meinen eltern danken . ihr habt meinen weg immer verfolgt , mich ermutigt und unterst tzt . abstract this thesis deals with linear transformations at various stages of the automatic_speech_recognition process . in current_state-of-the-art speech_recognition systems linear transformations are widely used to care for a potential mismatch of the training_and_testing data and thus enhance the recognition performance . a large number of approaches has been proposed in literature , though the connections between them have been disregarded so far . by developing a unified mathematical framework , close relationships between 
stand diameter distribution modelling and prediction based on richards function the objective of this study was to introduce application of the richards equation on modelling and prediction of stand diameter distribution . the long_term repeated measurement data_sets , consisted of 309 diameter frequency distributions from chinese fir ( cunninghamia lanceolata ) plantations in the southern china , were used . also , 150 stands were used as fitting data , the other 159 stands were used for testing . nonlinear regression method ( nrm ) or maximum_likelihood estimates method ( mlem ) were applied to estimate the parameters of models , and the parameter prediction method ( ppm ) and parameter recovery method ( prm ) were used to predict the diameter distributions of unknown stands . four main conclusions were obtained : ( 1 ) r distribution presented a more accurate simulation than three-parametric weibull function; ( 2 ) the parameters p , q and r of r distribution proved to be its scale , location and shape parameters , and have a deep relationship with stand characteristics , which means the parameters of r distribution have good theoretical interpretation; ( 3 ) the ordinate of inflection_point of r distribution has significant relativity with its skewness and kurtosis , and the fitted main distribution range for the cumulative diameter distribution of chinese fir plantations was 0 . 4 0 . 6; ( 4 ) the goodness-of-fit test showed diameter distributions of unknown stands can be well estimated by applying r distribution based on prm or the combination of ppm and prm under the condition that only quadratic mean dbh or plus stand age are known , and the non-rejection rates were near 80% , which are higher than the 72 . 33% non-rejection rate of three-parametric weibull function based on the combination of ppm and prm . 
latent feature_learning in social_media network the current trend in social_media analysis and application is to use the pre_defined features and devoted to the later model development modules to meet the end tasks . in this work , we claim that representation is critical to the end tasks and contributes much to the model development module . we provide evidence that specially learned feature well addresses the diverse , heterogeneous and collective characteristics of social_media data . therefore , we propose to transfer the focus from the model development to latent feature_learning , and present a general feature_learning framework based on the popular deep architecture . in particular , following the proposed framework , we design a novel relational generative deep_learning model to test the idea on link_analysis tasks in the social_media networks . we show that the derived latent features well embed both the media content and their observed links , leading to improvement in social_media tasks of user recommendation and social image_annotation . 
general formulas for capacity of classical-quantum channels coding problems of classical-quantum channels are considered in the most general setting , where no structural assumptions such as the stationary mem-oryless property are made on a channel . the channel_capacity as well as the characterization of the strong converse property is given just in parallel with the corresponding classical results of verd -han based on the so-called information-spectrum method . the general results are applied to the stationary memoryless case with or without cost constraint , whereby a deep relation between the channel coding_theory and the hypothesis_testing for two quantum states is elucidated . 
model predictive control of remotely operated underwater_vehicles this paper describes the implementation of a model predictive controller novel in an underwater robot vehicle . this work also shows the development of an underwater_vehicle model that accounts for physical , hydrodynamic and restorative effects , while the damping coefficients are neglected in the prediction of the vehicle position and orientation . the vehicle kinematic and dynamic models are linearized and arranged into the state space form inside the predictive controller . the model helps to determine the future position and orientation of the vehicle to track a predefined underwater trajectory in an optimal way . the results show that the predictive controller offered significant benefits compared to pid controllers by reducing the mse and rms by 40% and 76% respectively . i . introduction a remote operated vehicle ( rov ) is a type of unmanned underwater_vehicle ( uuv ) connected to the surface through a cable or umbilical line . rovs can perform important underwater tasks that include assisting the offshore exploration and production of oil and gas [1] and studying marine life and collecting deep_water samples [2] . improving rovs involves not only researching their design , but also the reliability of their operation and maneuverability [3] . the design , implementation and testing of the guidance and control systems for rovs have been addressed by several researchers during the last decade [4] , [5] . the design of robust_tracking controllers using proportional and derivative action with nonlinear compensation has proved to be stable , converging the tracking error exponentially [6] . model_based closed_loop trajectory_tracking control has been successfully deployed in several rovs in the united_states and the united_kingdom [7] . soylu , buckham and podhorodesky have proposed the use of chattering-free sliding_mode and l controllers for the trajectory control of rovs to incorporate the thruster saturation limits as part of the controller design [8] . the control techniques mentioned above have significantly improved the operation reliability and task accuracy of rovs [5] . nevertheless , these control algorithms do not consider the effect of forecast perturbation and out-coming tracking maneuvers that can be well predicted by a dynamic model . model predictive control ( mpc ) [9] , [10] is a model_based control_algorithm that solves a finite_horizon optimal
arabic goal_oriented conversational agent based on pattern_matching and knowledge trees conversational agents ( ca's ) are computer agents used in applications to converse with humans using natural_language dialogues . they are widely used in different fields like industry , education , marketing , health , and other services . goal_oriented conversational agents ( go-cas ) are agents having a deep strategic purpose which enables them to direct conversations to achieve a certain goal using a specific domain . typically ( ca's ) are programmed to have a set of rules that guide the conversation with the user . one technique used to script ca's is through pattern_matching algorithms . such algorithms are used to match the user's dialogue and instigate the conversation through writing a series of scripts that contains the rules and patterns relevant to the domain . throughout the conversation , values can be extracted from the user's dialogue which allows the ca to respond with the correct answer . ca's have been mainly developed for the english_language and very limited work has been carried out in arabic . this is mainly due to the complexity of the language and the lack of resources supporting the arabic_language . this paper proposes a new ca architecture based on a pattern_matching algorithm for the development of a goal orientated arabic conversational agents ( aca ) . the aca incorporates a new scripting_language and knowledge_engineering is used to construct the domain . a prototype aca was developed and the iraqi passport system was used as a domain to evaluate the new aca . the aca was tested and evaluated by experts within the iraq consulate with encouraging results and received positive_feedback . 
validation of tissue modelization and classification techniques in t1_weighted mr_brain_images we present a deep study of the performance of tissue modelization and classification techniques for t1_weighted mr_images . it is assumed that only t1_weighted mr image modality is available . the methods presented here were selected to represent the whole range of prior_information that can be used in the classification , i . e . intensity , spatial and anatomical priors . first , we consider the finite gaussian mixture_model ( a-fgmm ) with a bayes classification . the second method is closely_related to a-fgmm but also considers a hidden markov_random_field ( hmrf ) model to account for spatial prior_information . for this model , the maximum a posteriori ( map ) criterion is used as the classification decision rule . third , we study a tissue model that assumes the mixture tissues to be probablistically modeled by the linear_combination of their correspondent pure gaussian tissue densities ( c-gpv ) . here again , bayes classification is used for the final classification . the fourth method , d-gpv-hmrf , uses the same image model as method c-gpv , but it also encodes spatial information by a hidden markov_random_field as done in method b-hmrf . the fifth algorithm does not model the tissue classes by parametric probability densities , but rather by non-parametric models . as a result , the probabilistic tissue model and the classification criterion can not be distinguished anymore , but are directly interdependent . the resulting algorithm minimizes an information theortic quantity , called the error probability ( e-ep ) . the final method is also non-parametric , but again adds a hmrf to model spatial prior_information ( f-nphmrf ) . all methods have been tested on digital brain phantom images for which the classification ground truths were known . noise and intensity non-uniformities were added to simulate real imaging conditions . no enhancement of the image quality is considered either before or during the classification process . this way robustness and accuracy of the methods is tested in front of the image artifacts . results_demonstrate clearly that methods relying on both , intensity and spatial information , are in more robust to noise and field inhomogeneities . we demonstrate also that partial volume ( pv ) is still not perfectly modeled , even though methods that account for mixture classes outperform methods that just consider pure classes . 
on comparison of ncr effectiveness with a reduced i{ddq} vector set i ddq test-based outlier_rejection becomes difficult for deep_sub_micron_technology chips due to increased leakage and process_variations . the use of neighbor current_ratio ( ncr ) that uses wafer_level spatial_correlation for identifying outlier chips has been proposed earlier as a means of coping with these issues . due to the slow speed of i ddq test , there is a strong motivation to reduce the number of test_vectors without compromising the fault coverage . in this paper , we examine the effectiveness of neighbor current_ratio using a reduced i_ddq vector set and industrial test_data . 
flash : fast length adjustment of short reads to improve genome assemblies motivation next_generation sequencing technologies generate very large numbers of short reads . even with very deep genome coverage , short read lengths cause problems in de novo assemblies . the use of paired-end libraries with a fragment size shorter than twice the read length provides an opportunity to generate much longer reads by overlapping and merging read pairs before assembling a genome . results we present flash , a fast computational tool to extend the length of short reads by overlapping paired-end reads from fragment libraries that are sufficiently short . we tested the correctness of the tool on one million simulated read pairs , and we then applied it as a pre-processor for genome assemblies of illumina reads from the bacterium staphylococcus aureus and human chromosome 14 . flash correctly extended and merged reads >99% of the time on simulated reads with an error_rate of <1% . with adequately set parameters , flash correctly merged reads over 90% of the time even when the reads contained up to 5% errors . when flash was used to extend reads prior to assembly , the resulting assemblies had substantially greater n50 lengths for both contigs and scaffolds . availability and implementation the flash system is implemented in c and is freely available as open-source_code at http : //www . cbcb . umd . edu/software/flash . contact t . magoc@gmail . com . 
patterns in benthic biodiversity link lake trophic status to structure and potential function of three large , deep lakes relative to their scarcity , large , deep lakes support a large proportion of the world's freshwater species . this biodiversity is threatened by human_development and is in need of conservation . direct comparison of biodiversity is the basis of biological monitoring for conservation but is difficult to conduct between large , insular ecosystems . the objective of our study was to conduct such a comparison of benthic biodiversity between three of the world's largest lakes : lake_tahoe , usa; lake_h_vsg_l , mongolia; and crater_lake , usa . we examined biodiversity of common benthic organism , the non-biting midges ( chironomidae ) and determined lake trophic status using chironomid-based lake typology , tested whether community_structure was similar between the three lakes despite geographic distance; and tested whether chironomid diversity would show significant variation within and between lakes . typology analysis indicated that lake_h_vsg_l was ultra-oligotrophic , crater_lake was oligotrophic , and lake_tahoe was borderline oligotrophic/mesotrophic . these results were similar to traditional pelagic measures of lake trophic status for lake_h_vsg_l and crater_lake but differed for lake_tahoe , which has been designated as ultra-oligotrophic by traditional pelagic measures such as transparency found in the literature . analysis of similarity showed that lake_tahoe and lake_h_vsg_l chironomid_communities were more similar to each other than either was to crater_lake communities . diversity varied between the three lakes and spatially within each lake . this research shows that chironomid_communities from these large lakes were sensitive to trophic conditions . chironomid_communities were similar between the deep environments of lake_h_vsg_l and lake_tahoe , indicating that chironomid_communities from these lakes may be useful in comparing trophic state changes in large lakes . spatial variation in lake tahoe's diversity is indicative of differential response of chironomid_communities to nutrient enrichment which may be an indication of changes in trophic state within and across habitats . 
efficient monte_carlo device modeling a single-particle approach to full-band monte_carlo device simulation is presented which allows an efficient computation of drain , substrate and gate currents in deep_submicron mosfets . in this approach , phase_space elements are visited according to the distribution of real electrons . this scheme is well adapted to a test-function evaluation of the drain current , which emphasizes regions with large drift velocities ( i . e . , in the inversion channel ) , a substrate current evaluation via the impact ionization generation rate ( i . e . , in the ldd region with relatively high electron temperature and density ) and a computation of the gate current in the dominant direct-tunneling regime caused by relatively cold electrons ( i . e . , directly under the gate at the source well of the inversion channel ) . other important features are an efficient treatment of impurity scattering , a phase_space steplike propagation of the electron allowing to minimize self-scattering , just-before-scattering gathering of statistics , and the use of a frozen electric_field obtained from a drift-diffusion simulation . as an example an 0 . 1-m n-mosfet is simulated where typically 30 minutes of cpu time are necessary per bias point for practically sufficient accuracy . 
power flow analysis on cuda-based gpu this major qualifying project investigates the algorithm and the performance of using the cuda-based graphics_processing_unit for power flow analysis . the accomplished work includes the design , implementation and testing of the power flow solver . comprehensive analysis shows that the execution time of the parallel_algorithm outperforms that of the sequential algorithm by several factors . iii acknowledgements i would like to express my deep and sincere gratitude to my academic and mqp advisor , professor xinming huang , for giving me the professional and insightful comments_and_suggestions on this project . 
deep read : a reading_comprehension system this paper describes initial work on deep read , an automated reading_comprehension system that accepts arbitrary text input ( a story ) and answers questions about it . we have acquired a corpus of 60 development and 60 test stories of 3 rd to 6 th grade material; each story is followed by short-answer questions ( an answer key was also provided ) . we used these to construct and evaluate a baseline system that uses pattern_matching ( bag-of-words ) techniques augmented with additional automated linguistic processing ( stemming , name identification , semantic class identification , and pronoun resolution ) . this simple system retrieves the sentence containing the answer 30 40% of the time . 
wheels around the world : windows_live mobile interface_design we present a unique interface_design for mobile_devices that addresses major user pain points with deep menu systems and page scrolling . using a series of 1-5 wheels of content , arranged in a combination_lock style on a single mobile screen , this design enables a user to consume a multitude of personalized internet and web_content without ever scrolling up/down or selecting from a menu . additionally , the wheels are easily mapped to a personalized pc experience such as those from my msn , live . com , and myyahoo ! , enabling users to access their pc content from anywhere . results from iterative testing across us , japan , and china show the model to be an effective and desirable mode of consuming personal and internet content on the mobile_device , despite very different navigation paradigms and cultural expectations in each of the countries . 
improved reconstruction of 4d-mr_images by motion predictions the reconstruction of 4d images from 2d navigator and data slices requires sufficient observations per motion state to avoid blurred images and motion artifacts between slices . especially images from rare motion states , like deep inhalations during free-breathing , suffer from too few observations . to address this problem , we propose to actively generate more suitable images instead of only selecting from the available images . the method is based on learning the relationship between navigator and data-slice motion by linear_regression after dimensionality_reduction . this can then be used to predict new data slices for a given navigator by warping existing data slices by their predicted displacement field . the method was evaluated for 4d-mris of the liver under free-breathing , where sliding boundaries pose an additional challenge for image_registration . leave-one-out tests for five short sequences of ten volunteers showed that the proposed prediction method improved on average the residual mean ( 95% ) motion between the ground_truth and predicted data slice from 0 . 9mm ( 1 . 9mm ) to 0 . 8mm ( 1 . 6mm ) in comparison to the best selection method . the approach was particularly suited for unusual motion states , where the mean error was reduced by 40% ( 2 . 2mm vs . 1 . 3mm ) . 
dynamic analysis of an electrostatic energy_harvesting system dynamic analysis of an electrostatic energy_harvesting system traditional small_scale vibration energy harvesters have typically low efficiency of energy_harvesting from low_frequency vibrations . several recent_studies have indicated that introduction of nonlinearity can significantly improve the efficiency of such systems . motivated by these observations we have studied the nonlinear electrostatic energy harvester using a combination of analytical and numerical approaches . the analytical approach was based on the normal vibration mode analysis around an equilibrium_point . the numerical model was implemented and tested using modelica language . it was found that the efficiency of energy transfer strongly depends on three parameters : the ratio between the maximal electrical and mechanical energies in the system and ratio of natural frequencies of electric and mechanical modes , and finally the dimensionless degree of nonlinearity in the system . the dependence of the transfer factor on these three parameters was studied and characterized both theoretically and numerically . it was found that the transfer factor tr has a sharply pronounced peak as a function of e providing a possibility of efficient energy conversion between modes with highly different normal frequencies . 3 4 acknowledgments i would like to extend my gratitude to the many people who helped to bring this research_project to fruition . first , i would like to thank professor kostya turitsyn for providing me the opportunity of taking part in this research . i am so deeply grateful for his help , professionalism , valuable guidance and financial_support throughout this project and through my entire program of study that i do not have enough words to express my deep and sincere appreciation . am gratefully indebted to him for his valuable comments for this thesis . i would also like to thank mr . petr vorobev and my roommate daniela miao , who have willingly proof read my thesis . finally , i must express my very profound gratitude to my parents and my friends sha miao and xin xu for providing me with unfailing support and continuous encouragement throughout my years of study and through the process of researching and writing this thesis . this accomplishment would not have been possible without them . thank you . 
automated synthesis of distributed controllers synthesis is a particularly challenging problem for concurrent_programs . at the same time it is a very promising approach , since concurrent_programs are difficult to get right , or to analyze with traditional verification techniques . this paper gives an introduction to distributed synthesis in the setting of mazurkiewicz traces , and its applications to decentralized runtime monitoring . modern computing systems are increasingly distributed and heterogeneous . software needs to be able to exploit these advances , providing means for applications to be more performant . traditional concurrent programming paradigms , as in java , are based on threads , shared_memory , and locking mechanisms that guard access to common data . more recent paradigms like the reactive programming model of erlang [4] and scala [35 , 36] replace shared_memory by asynchronous message_passing , where sending a message is non-blocking . in all these concurrent frameworks , writing reliable software is a serious challenge . programmers tend to think about code mostly in a sequential way , and it is hard to grasp all possible schedulings of events in a concurrent execution . for similar reasons , verification and analysis of concurrent_programs is a difficult task . testing , which is still the main method for error_detection in software , has low coverage for concurrent_programs . the reason is that bugs in such programs are difficult to reproduce : they may happen under very specific thread schedules and the likelihood of taking such corner-case schedules is very low . automated verification , such as model_checking and other traditional exploration techniques , can handle very limited instances of concurrent_programs , mostly because of the very large number of possible states and of possible interleavings of executions . formal_analysis of programs requires as a prerequisite a clean mathematical_model for programs . verification of sequential programs starts usually with an abstraction step reducing the value domains of variables to finite domains , viewing conditional branching as non-determinism , etc . another major simplification consists in disallowing recursion . this leads to a very robust computational model , namely finite-state automata and regular languages . regular languages of words ( and trees ) are particularly well understood notions . the deep connections between logic and automata revealed by the foundational work of b chi , rabin , and others , are the main ingredients in automata-based verification . 
iitp : a supervised approach for disorder mention detection and disambiguation in this paper we briefly describe our supervised machine_learning approach for disorder mention detection system that we submitted as part of our participation in the semeval-2014 shared task . the main goal of this task is to build a system that automatically identifies mentions of clinical conditions from the clinical texts . the main challenge lies due in the fact that the same mention of concept may be represented in many surface forms . we develop the system based on the supervised machine_learning_algorithms , namely conditional_random_field and support_vector_machine . one appealing characteristics of our system is that most of the features for learning are extracted automatically from the given training or test datasets without using deep domain_specific resources and/or tools . we submitted three runs , and best performing system is based on conditional_random_field . for task a , it shows the precision , recall and f-measure values of 50 . 00% , 47 . 90% and 48 . 90% , respectively under the strict matching criterion . when the matching criterion is relaxed , it shows the precision , recall and f-measure of 81 . 50% , 79 . 70% and 80 . 60% , respectively . for task b , we obtain the accuracies of 33 . 30% and 69 . 60% for the relaxed and strict matches , respectively . 
effects of subthalamic_nucleus ( stn ) stimulation on motor_cortex excitability . background deep_brain_stimulation of the internal global pallidus ( gpi ) and the subthalamic nucleus ( stn ) has become a treatment alternative in advanced pd . although the effects of gpi stimulation have been examined recently , little is known about stn stimulation effects on motor_cortex excitability . methods the effects of stn stimulation were studied in eight patients with advanced pd using paired-pulse transcranial_magnetic_stimulation ( tms ) in comparison with healthy control subjects . motor evoked potentials following paired-pulse tms ( interstimulus interval 3 ms to test for corticocortical inhibition vs 13 ms for facilitation ) have been recorded from the extensor carpi radialis and its functional antagonist , the flexor carpi radialis muscle . silent period ( sp ) was also determined . patients were examined under four conditions : medication "off"/stimulator "off" vs medication "on"/stimulator "off" vs medication "off"/stimulator "on" vs medication "on"/stimulator "on . " results although the mean values for intracortical inhibition ( ici ) were not significantly different , data variation was smaller and levels of significance higher with the stn stimulator switched "on , " suggesting that ici was more consistent . sp during stimulator "on"/medication "on" was longer than during stimulator "off"/medication "off . " motor performance as indicated by a finger-tapping test and unified pd rating scale iii was significantly better with dopaminergic medication and further improved with stimulator "on . " conclusions results_suggest an effect of subthalamic_nucleus stimulation on intracortical inhibitory mechanisms . this hypothesis could at least partially explain a more consistent depression of motor evoked potentials following inhibiting paired-pulse transcranial_magnetic_stimulation , a longer silent period ( under stimulator "on"/medication "on" ) , and a reduction of akinesia and rigidity leading to a better motor performance in subthalamic_nucleus-stimulated patients . 
automatic data aggregation for software distributed_shared_memory systems automatic data aggregation for software distributed_shared_memory systems software distributed_shared_memory ( dsm ) provides a shared_memory abstraction on distributed_memory hardware , making a parallel programmer's task easier . unfortunately , software_dsm is less eecient than the direct use of the underlying message_passing hardware . the chief reason for this is that hand-coded and compiler-generated message_passing programs typically achieve better data aggregation in their messages than programs using software_dsm . software_dsm has poorer data aggre-gation because the system lacks the knowledge of the application's behavior that a programmer or compiler analysis can provide . we propose four new techniques to perform automatic data aggregation in software_dsm . our techniques use run_time analysis of past data-fetch accesses made by a processor , to aggregate_data movement for future accesses . they do not need any additional compiler support . we implemented our techniques in the treadmarks software_dsm system . we used a test suite of four applications-3d-fft , barnes-hut , ilink and shallow . for these applications we obtained 40% to 66% reduction in message counts which resulted in 6% to 19% improvement in execution times . acknowledgements i express my deep gratitude to dr . alan_cox for his continuous guidance and advise without which i could not have completed this thesis . i thank the other members of my committee dr . willy zwaenepoel , dr . sarita adve and dr . peter druschel for their invaluable comments_and_suggestions . i also thank my friends ramakrishnan rajamony and parthasarathy ranganathan for their continuous help throughout the course of my work . contents abstract ii_acknowledgments iii list of tables vi list of illustrations vii 1 introduction 1
the ( non ) utility of predicate_argument frequencies for pronoun interpretation state-of-the-art pronoun interpretation systems rely predominantly on morphosyntac-tic contextual features . while the use of deep_knowledge and inference to improve these models would appear technically in-feasible , previous work has suggested that predicate_argument statistics mined from naturally-occurring data could provide a useful approximation to such knowledge . we test this idea in several system configurations , and conclude from our results and subsequent error analysis that such statistics offer little or no predictive information above that provided by morphosyntax . 
3d positioning issues in virtual_reality environments in last few years a big deal of work has been done to introduce virtual_reality tools in engineering design , mechanical and aeronautical assembly and many other fields , like professional training for civil and military purpose . many interfaces has been developed and used particularly in assembly environment , where 3d part positioning can be considered the most important issue . bi-dimensional grids and snaps ( or object-snap ) have been replaced with " magnetic " handles , 3d grids or assembly wizards without a deep study and evaluation of interface impact and results , either in terms of modeling and components assembly speed , either in terms of positioning precision . in this paper a original test and comparison between different positioning methods in virtual_reality has been provided in order to increase user-handling capabilities and interactivity . furthermore , a virtual_reality desk for semi-immersive engineering applications ( vrdd ) has been designed and manufactured , targeting a more comfortable and user-friendly environment for modeling and assembly of 3d components . a open inventor vr interface with polhemus tracking system and shuttered stereo_vision on a virtual desk has been used to test several 3d positioning solutions . 
collocational knowledge versus general linguistic_knowledge among iranian efl learners this study has a twofold purpose . the first and foremost is to see whether there exists any correlation between the collocational knowledge and general linguistic_knowledge of efl learners . the second is to reveal which type ( s ) of collocation is or are more difficult for efl learners . to this end , 35 subjects , screened by a proficiency test , were given a 90-item multiple_choice test including lexical collocations ( noun+noun , noun+verb , verb+noun , and adjective+noun ) , and grammatical collocations ( noun+preposition and preposition+noun ) . a native speaker checked the final version of the data and necessary corrections were made . the results_showed that a ) there was no significant_correlation between general linguistic_knowledge and collocational knowledge of efl learners , and b ) the grammatical collocations were more difficult than the lexical collocations for learners and from among all subcategories , noun+preposition was the most difficult and noun+verb was the easiest . introduction one of the most important aspects of learning a language is learning the vocabulary of that language and its appropriate use . since traditional techniques of learning vocabulary the learning of individual words or memorizing bilingual vocabulary lists appeared to be no longer tenable , researchers suggested ways for learning multiword phrases , chunks as well as association between lexical items . anderson and nagy ( 1991 ) , for instance , accentuate the importance of deep meanings including collocational properties in words . students need to know which words go with which other words , how words go together normally , and how we can manipulate these
a yield improvement methodology using pre- and post-silicon statistical clock_scheduling in deep sub-micron technologies , process_variations can cause significant path_delay and clock_skew uncertainties thereby lead to timing failure and yield_loss . in this paper , we propose a comprehensive clock_scheduling methodology that improves timing and yield through both pre-silicon clock_scheduling and post-silicon clock tuning . first , an optimal clock_scheduling algorithm has been developed to allocate the slack for each path according to its timing uncertainty . to balance the skew that can be caused by process_variations , programmable delay elements are inserted at the clock inputs of a small set of flip_flops on the timing critical paths . a delay_fault_testing scheme combined with linear_programming is used to identify and eliminate timing violations in the manufactured chips . experimental results show that our methodology achieves substantial yield improvement over a traditional clock_scheduling algorithm in many of the iscas89 benchmark_circuits , and obtain an average yield improvement of 13 . 6% . i . introduction the performance and yield of sequential_circuits can be improved significantly by carefully assigning clock arrival times to each flip-flop . this technique is usually called clock_scheduling [1] , [2] or clock_skew optimization [3] , [4] , [5] , [6] . because of process_variations , the path_delays in each manufactured chip are different , and each chip requires a different clock schedule to achieve its best performance . however , the clock trees in existing designs usually do not have tuning capabilities . moreover , process_variations also change clock arrival times and cause unintentional clock skews , which aggravate the timing yield problem . previous_works attempt to improve the timing yield of a sequential circuit by finding a good clock schedule that maximizes the slacks for all paths in different ways . the authors in [5] , [6] formulate the clock_skew optimization_problem as a least square error problem : where the error is defined as the difference between the assigned skew and the middle point of the permissible range . held et al . [1] and albrecht et al [2] adopt the minimum balance algorithm [7] to find the clock schedule that yields a lexicographically maximum slack vector when the slacks are sorted in nondecreasing order . both approaches do not take into consideration the statistical behavior of process_variation . recent clock tree designs have started to incorporate tuning capabilities in order to remove unintentional clock skews or speedup timing convergence . intel's itanium t m processors
on-chip traffic modeling and synthesis for mpeg_2 video applications the objective of this paper is to introduce self_similarity as a fundamental property exhibited by the bursty traffic between on-chip modules in typical mpeg_2 video applications . statistical_tests performed on relevant traces extracted from common video clips establish unequivocally the existence of self_similarity in video traffic . using a generic tile-based communication architecture , we discuss the implications of our findings on on-chip buffer space allocation and present quantitative evaluations for typical video streams . we also describe a technique for synthetically generating traces having statistical properties similar to those obtained from real video clips . our proposed technique speeds up buffer simulations , allows media system designers to explore architectures rapidly and use large media data benchmarks more efficiently . we believe that our findings open new directions of research with deep_implications on some fundamental issues in on-chip networks design for multimedia applications . 
vision-aided imu for handheld pedestrian navigation he has been working with gnss and integrated systems for over 10 years . tom botterill obtained his ma degree in maths and computer_science at cambridge_university in 2005 and is and is currently studying towards a phd with the he has over 18 years r&d experience in the area of hardware and software_development in signal_processing and sensor integration . he is currently holding the position of senior research scientist at the geospatial research centre in new zealand , where his main area of research is on signal_processing methods to enable gnss positioning in indoor environments , together with general electronics and communication development . abstract low_cost inertial_sensors are often promoted as the solution to indoor navigation . however , in reality , the quality of the measurements is poor , and as a result , the sensors can only be used to navigate for a few seconds at a time before the drift becomes too large to be useful . therefore , it is necessary to regularly update the sensors with measurements from external systems such as gps or other sensors useful for navigation . one such sensor is provided by the computer_vision community where a camera can be used to obtain information about the relative translation and rotation between successive images . this paper describes the use of a camera attached to a low cost imu for navigation in areas where gps is unavailable such as indoors or deep urban_canyons . it is assumed that a pedestrian user is walking with the mobile_device held out in front of them with the camera pointing approximately towards the ground . features are matched between successive frames , and the robust ransac framework is used to identify which of these lie on the ground_plane , while estimating the camera's orientation and 3 dimensional body frame translation relative to its previous position . this information is used to aid the imu using a kalman_filter to reduce the position drift . this paper describes the implementation of the combined computer_vision and inertial_navigation approach . a tactical grade imu is used for initial testing since it provides more reliable measurements and enables us to provide a reference by which to compare the measurements obtained from the computer_vision algorithm . it is demonstrated that even with a good quality imu , the algorithm is able to significantly improve the performance of ins navigation when gps measurements are unavailable . 
the benefits of using guyton's model in a hypotensive control system in order to improve the intraoperative applications , this paper presents the advantages of using guyton's model in hypotensive control system development . in this system , the mean arterial pressure is decreased and maintained at a low_level during anaesthesia by controlling sodium nitroprusside infusion rate . the key of the study is to develop a physiological model of cardiovascular dynamics to present the mean arterial pressure response to sodium nitroprusside , which was considered as a linear_model in most of known blood_pressure control systems . being linear , the previous models cannot accurately mimic a physiological system of human circulation , especially at deep hypotensive control with strong reaction of the body . the enhanced model in this study was modified based on guyton's model of human circulation . it is useful to design a pid_controller , which allows studying and handling the wide range of the body sensitivities . this model is also helpful for studying the behaviors of patients under anaesthesia conditions , such as the perfusion of organs and the reaction of the body at hypotensive state . a fuzzy gain scheduler and a supervising algorithm were also developed for online tuning the controller to handle the behavior of the body . the control system was tested on 25 experiments on seven pigs in the animal laboratory . simulation and experiment_results proved the usefulness of guyton's model in control system design which can present the dynamical response of blood_pressure in the circulation under and after hypotensive control . the results also indicated the safety and stability of the controller . 
the physiological microphone ( pmic ) : a competitive alternative for speaker assessment in stress detection and speaker_verification interactive speech system scenarios exist which require the user to perform tasks which exert limitations on speech_production , thereby causing speaker variability and reduced speech performance . in noisy stressful scenarios , even if noise could be completely eliminated , the production variability brought on by stress , including lombard effect , has a more pronounced impact on speech system performance . thus , in this study we focus on the use of a silent speech interface ( pmic ) , with a corresponding experimental assessment to illustrate its utility in the tasks of stress detection and speaker_verification . this study focuses on the suitability of pmic versus close-talk microphone ( ctm ) , and reports that the pmic achieves as good performance as ctm or better for a number of test conditions . pmic reflects both stress-related information and speaker-dependent information to a far greater extent than the ctm . for stress detection performance ( which is reported in % accuracy ) , pmic performs at least on par or about 2% better than the ctm-based system . for a speaker_verification application , the pmic outperforms ctm for all matched stress_conditions . the performance reported in terms of %eer is 0 . 91% ( as compared to 1 . 69% ) , 0 . 45% ( as compared to 1 . 49% ) , and 1 . 42% ( as compared to 1 . 80% ) for pmic . this indicates that pmic reflects speaker-dependent information . also , another advantage of the pmic is its ability to record the user physiology traits/state . our experiments illustrate that pmic can be an attractive alternative for stress detection as well as speaker_verification tasks along with an advantage of its ability to record physiological information , in situations where the use of ctm may hinder operations ( deep_sea divers , fire-fighters in rescue operations , etc . ) . 
experimental study of cognitive_radio test_bed using usrp acknowledgments first of all , i would like to express sincere gratitude to my adviser dr . shuangqing wei for his constant_support in building my thesis . i would like to thank him for introducing me to the field of wireless_communication and cognitive_radio . his throughout guidance and motivation in solving complex problems has provided encouragement , enthusiasm and support , while the knowledge acquired by working with him has provided deep understanding of issues that are apart from classroom work . also , i would like to thank my co-adviser dr . rajgopal kannan , whose support has provided valuable experience . i would thank dr . xue-bin liang for being a member of thesis committee . apart from all , my parents and elder sister have provided the emotional and financial_support to build my confidence and without which this wouldn't be possible . i would like to thank my friends and roommates for their support and morale boost during the hard times . also , i would like to mention my lab-mates for giving their resources and time in enabling to complete my thesis . 
performance-impact limited area fill synthesis chemical-mechanical planarization ( cmp ) and other manufacturing steps in very deep-submicron vlsi have varying effects on device and interconnect features , depending on the local layout density . to improve manufacturability and performance predictability , area fill features are inserted into the layout to improve uniformity with respect to density criteria . however , the performance impact of area fill insertion is not considered by any fill method in the literature . in this paper , we first review and develop estimates for capacitance and timing overhead of area fill insertions . we then give the first formulations of the performance impact limited fill ( pil_fill ) problem with the objective of either minimizing total delay impact ( mdfc ) or maximizing the minimum slack of all nets ( msfc ) , subject to inserting a given prescribed amount of fill . for the mdfc pil_fill problem , we describe three practical solution approaches based on integer linear_programming ( ilp-i and ilp-ii ) and the greedy method . for the msfc pil_fill problem , we describe an iterated greedy method that integrates call to an industry static_timing_analysis tool . we test our methods on layout testcases obtained from industry . compared with the normal fill method [3] , our ilp-ii method for mdfc pil_fill problem achieves between 25-% and 90% reduction in terms of total <i>weighted edge delay</i> ( roughly , a measure of sum of node slacks ) impact while maintaining identical quality of the layout density control; and our iterated greedy method for msfc pil_fill problem also shows significant advantage with respect to the minimum slack of nets on post-fill layout . 
an efficient design-for-verification technique for hdls due to the high complexity of modern circuit designs , verification has become the major_bottleneck of the entire design_process . there is an emerging need for a practical solution to reduce the verification time . in manufacturing_test , a well-known technique , "design_for_testability" , is often used to reduce the testing time . by inserting some extra circuits on the hard-to-test points , the testability can be improved and the testing time can be reduced . in this paper , we apply the similar idea to functional verification and propose an efficient "design-for-verification" ( dfv ) technique to help users reduce the verification time . the conditions for hard-to-control ( htc ) codes in a hdl design are clearly defined , and an efficient algorithm to detect them automatically is proposed . besides the htc detection , we also propose an algorithm that can eliminate those htc points with minimum number of dfv points . by the help of those dfv points , the number of required test_patterns to reach the same coverage can be greatly reduced especially for deep-sequential designs . 
design of asynchronous controllers with delay insensitive interface deep_submicron_technology calls for new design techniques , in which wire and gate delays are accounted to have equal or nearly equal effect on circuit behavior . asynchronous speed-independent ( si ) circuits , whose behavior is only robust to gate delay_variations , may be too optimistic . on the other hand , building circuits totally delay-insensitive ( di ) , for both gates and wires , is impractical . the paper presents a new approach for synthesis of globally di and locally si circuits suggested in [7] . the method starts from a speed-independent implementation and locally modifies gate functions to ensure their independence from delays in communication wires . the suggested approach was successfully tested on a set of benchmarks . 
physic_ist : cleaning source_trees to infer more informative supertrees background supertree methods combine phylogenies with overlapping sets of taxa into a larger one . topological conflicts frequently arise among source_trees for methodological or biological reasons , such as long_branch attraction , lateral gene transfers , gene_duplication/loss or deep gene coalescence . when topological conflicts occur among source_trees , liberal methods infer supertrees containing the most frequent alternative , while veto methods infer supertrees not contradicting any source tree , i . e . discard all conflicting resolutions . when the source trees host a significant number of topological conflicts or have a small taxon overlap , supertree methods of both kinds can propose poorly resolved , hence uninformative , supertrees . results to overcome this problem , we propose to infer non-plenary supertrees , i . e . supertrees that do not necessarily contain all the taxa present in the source trees , discarding those whose position greatly differs among source_trees or for which insufficient information is provided . we detail a variant of the physic veto method_called physic_ist that can infer non-plenary supertrees . physic_ist aims at inferring supertrees that satisfy the same appealing theoretical properties as with physic , while being as informative as possible under this constraint . the informativeness of a supertree is estimated using a variation of the cic ( cladistic information content ) criterion , that takes_into_account both the presence of multifurcations and the absence of some taxa . additionally , we propose a statistical preprocessing step called stc ( source_trees correction ) to correct the source trees prior to the supertree inference . stc is a liberal step that removes the parts of each source tree that significantly conflict with other source_trees . combining stc with a veto method allows an explicit trade_off between veto and liberal approaches , tuned by a single parameter . performing large_scale simulations , we observe that stc+physic_ist infers much more informative supertrees than physic , while preserving low type i error compared to the well-known mrp method . two biological case_studies on animals confirm that the stc preprocess successfully detects anomalies in the source trees while stc+physic_ist provides well-resolved supertrees agreeing with current knowledge in systematics . conclusion the paper_introduces and tests two new methodologies , physic_ist and stc , that demonstrate the interest in inferring non-plenary supertrees as well as preprocessing the source trees . an implementation of the methods is available at : http : //www . atgc-montpellier . fr/physic_ist/ . 
the new face of design for manufacturability 232 deep_submicron beol yield challenges infrastructure for successful beol yield_ramp , transfer to manufacturing , and dfm characterization at 65 nm and below the challenges presented by deep_submicron interconnect back-end-of-line ( beol ) integration continue to grow in number , complexity , and required resolution at 90 nm and 65 nm . these challenges are causing industry-wide delays in technology deployment as well as low and often unstable yields . the historically observed improvements in time to successful yield_ramp and final manufacturing_yield as the industry deploys new technology_nodes disappeared at 90 nm . such improvements have been significant factors in fueling the semiconductor industry's growth . in this article , we describe an infrastructure developed to specifically address beol deep_submicron yield_learning needs . these include the need to reduce the overall time to results and to provide information that manufacturers can successfully use in process and yield debug , and in higher-level_design models . this infrastructure establishes a needed foundation for deep_submicron technology_nodes where design and manufacturing share yield entitlement . by building on this foundation , manufacturers can accelerate yield issue detection and correction , and realize yield-aware design flows . the international_technology_roadmap_for_semiconductors 1 provides an excellent summary of the confounding combination of increased process complexity , reduced yield_learning cycles , and reduced defect visibility that confronts the industry . accordingly , a yield_ramp infrastructure must be available that can provide the following characteristics : parts-per-billion sensitivity to key yield-limiting topologies ( not just to purely random defects ) ; identification of nonvisual defects , such as defects in high-aspect_ratio features and interfacial defects; direct identification of defect location and layer; ability to generate vastly more data than has been possible with traditional technology characterization vehicles , to provide sufficient coverage of ever-expanding design rule sets , interactions with resolution enhancement technologies ( rets ) , and so on; editor's note : optimized test_structures are necessary to measure and analyze the causes for systematic yield_loss . this article introduces a novel test structure for beol an infrastructure_ip for process monitoring . it also describes a method for characterizing and measuring yield_ramp issues and solutions for improving silicon_debug and dfm . 
goodness of fit tests for generalized linear mixed_models goodness of fit tests for generalized linear mixed_models generalized linear mixed_models ( glmms ) are widely used for regression_analysis of data , continuous or discrete , that are assumed to be clustered or correlated . assessing model fit is important for valid inference . we therefore propose a class of chi_squared goodness-of-fit tests for glmms . our test_statistic is a quadratic_form in the differences between observed values and the values expected under the estimated model in cells defined by a partition of the covariate space . we show that this test_statistic has an asymptotic chi_squared_distribution . we study the power of the test through simulations for two special cases of glmms , linear mixed_models ( lmms ) and logistic mixed_models . for lmms , we further derive the analytical power of the test under contiguous local alternatives and compare it with simulated empirical power . three examples are used to illustrate the proposed test . dedication to my parents and jun . ii_acknowledgments it is a great honor for me to thank all the people who made this thesis possible . first and foremost , i owe my deepest gratitude to my two coadvisors , slud has been a significant presence in my life . his perpetual energy and his rigor and enthusiasm in research have been motivating me through my whole doctoral study . his insights have strengthened this study significantly . i will always be thankful for his wisdom , knowledge , deep concern and constant encouragement . i would like to show my greatest gratitude and sincere thanks to dr . ruth m . pfeiffer who offered me the predoctoral training at national_cancer_institute ( nci ) which led me to the world of biostatistics and cancer research . her rigor and passion on research influenced me a lot . i am deeply indebted to her for her being always ready to help and her precious time on guiding me through my research . i could not have finished this thesis within four years without her invaluable help . she has also made available her support in a number of other ways , such as revising my cv , giving suggestions to my job talk rehearsal . she is the best mentor i have ever met . it has been an honor for me to work with her . 
cold_regions issues for off_road_autonomous_vehicles cold_regions issues for off_road_autonomous_vehicles cold_regions issues for off_road_autonomous_vehicles front cover : regions of the world considered severely cold ( a ) and moderately cold ( b ) . lines are based on bates and bilello ( 1966 ) . world map from di cartography center ( 1999 ) . abstract about half of earth's land mass experiences mean temperatures below 0c during the coldest month . attendant conditions pose major challenges to the operation of off_road_autonomous_vehicles . low-temperature effects on lubricants , materials , and batteries can impair a robot's ability to operate at all . cold starting will be a serious problem if missions require long periods of engine shutdown . deep snow can easily immobilize vehicles on terrain that would otherwise pose no problems . blowing snow and icing can also degrade the performance of sensors needed for navigation and target detection . winter operation of passenger vehicles and construction equipment provides guidance to surmount cold_regions effects on robotic vehicles . this report identifies problems likely to be encountered , simple preventative measures , and references for additional_information . conditions are sufficiently demanding that off_road_autonomous_vehicles must be designed for and tested in cold_regions if they are expected to operate there successfully . disclaimer : the contents of this report are not to be used for advertising , publication , or promotional purposes . citation of trade names does not constitute an official endorsement or approval of the use of such commercial products . all product names and trademarks cited are the property of their respective owners . the findings of this report are not to be construed as an official department of the army position unless so designated by other authorized documents . 
collaborative_learning with the cognitive_tutor algebra 1 collaborative_learning with the cognitive_tutor algebra . an experimental classroom study . collaborative_learning with the cognitive_tutor algebra 2 interest in developing improved methods for mathematics instruction has increased since timss and pisa . in our project , we investigate a new way to promote learning in mathematics : we enhanced the cognitive_tutor algebra , a computer-based intelligent_tutoring_system for mathematics at the high_school level , to a collaborative_learning setting . although the algebra tutor has shown to increase learning substantially , there are also several shortcomings . for instance , learning with the algebra tutor places emphasis on improving students' problem_solving skills , yet a deep understanding of underlying mathematical concepts is not necessarily achieved . to reduce these shortcomings , we extended the learning_environment to a dyadic setting , thus adding new learning opportunities such as the possibility to mutually elaborate on the learning content . a script was developed to guide students' interaction and to ensure that students profit from these new learning opportunities . our script followed a general jigsaw-scheme , i . e . it distributed expertise for the problem between partners and allowed them to prepare individually for the following interaction , thus laying the grounds for effective collaboration . during the collaboration , the script provided additional support : it prompted fruitful interaction and adaptively supported students as they encountered difficulties . following each problem , it guided the dyads to reflect on their interaction in order to improve subsequent collaboration . in an experimental classroom study taking place over the course of one week , we compared three conditions to evaluate the impact of collaboration in a cognitive tutoring setting : individual learning , unscripted collaborative_learning , and scripted collaborative_learning . after a two-day learning phase , several post tests were administered to assess learning on three levels : reproduction , transfer and future learning . in the collaborative reproduction test , we found a higher need for cognitive_tutor assistance in scripted students with low prior_knowledge ( note that this was the first post_test after script support had been removed ) . in the subsequent individual reproduction test , however , differences between conditions were no longer observed . in fact , the learning_outcomes of students in the two collaborative conditions were comparable to those in the individual condition even though collaborative students had solved fewer problems , i . e . had had less practice , during the learning phase . analysis of the transfer test did not reveal differences between conditions . finally , scripted collaboration better prepared students for future collaborative_learning situations as compared to the unscripted collaboration condition . in addition to presenting the study and its results , this paper_discusses the advantages and methodological challenges 
bacterioplankton distribution and production in the bathypelagic ocean : directly coupled to particulate organic carbon export ? a recently published evaluation of bacterioplankton abundance and productivity in the bathypelagic north_pacific suggests that these properties are generally coupled with particulate organic carbon ( poc ) fluxes . in that analysis , bacterial biomass and productivity were several-fold greater in subarctic than subtropical waters , consistent with the basin-scale distribution of poc flux and suggestive of a sinking poc-e doc-x bacteria transformation of the carbon . to test this hypothesis , we sought to determine whether the very strong spatial and temporal gradients in poc flux in the arabian_sea would force similar deep-ocean gradients in bacterial variables . on both a within-and between-cruise basis , there was variability in bacterial_abundance and thymidine incorporation in the deep arabian_sea , but correspondence was equivocal between these variables and several correlates to export : flux of biogenic carbon from the euphotic zone , state of the monsoon , and proximity to productive coastal upwelling zones . however , when annual mean bacterial_abundance at 2 , 000 m was compared with annual poc flux at that depth , a strong correspondence emerged : high annual flux supported high bacterial_abundance ( such a correspondence was not found for bacterial productivity ) . this finding suggests that bathypelagic bacterial_abundance responds to the long_term mean input of organic_matter and less to episodic inputs . a comparative evaluation of the north_pacific revealed that although the bathypelagic bacteria there showed correspondence to deep poc flux , that variable alone would not account for the wide meridional variations in bacterial_abundance that have been reported . the nutrition and sustenance of organisms in the deep sea has been a classic problem in oceanography since the discovery of life on the sea_floor by the challenger_expedition in 1872-1876 ( mills 1983 ) . with the advent of deep-ocean sediment traps , sedimentation of particulate organic_matter became recognized as the principal agent of deep_sea food supply ( moseley 1880; tyler 1988 ) . subsidy with dissolved organic_matter has long been invoked as a potential nutritional and energy source ( j0rgensen 1976 ) . bacteria dominate the metabolism of deep_waters below the euphotic zone ( pomeroy and johannes 1968 ) , but the mechanism of their nutrition is not clear . the proximal sources of organic_matter for bacteria are dissolved , low-molecular-weight substances because bacterial uptake systems are restricted to transporting molecules <500 da across cell membranes ( williams 2000 ) . however , the ultimate sources of the substances actually transported into bacterial cells are not well characterized , even at a crude operational level . karl et al . ( 1988 ) showed that 
micro gas_bearings fabricated by deep x_ray lithography micro bearing systems for micro electrome-chanical systems ( mems ) have drawn attention for several decades as critical components for micro rotating machinery . ideally , frictionless bearings are needed , and in practice , micro gas_bearings approach the ideal . typically , bearings function as a separate component , assembled onto sliding counterparts . however , in micro scale devices , assembly procedures are known to be very tedious and time consuming . this leads to the pursuit of single material monolithic structures . critical issues arising from these approaches include : limitation of materials , friction , and reliability , among others . in this paper , new approaches have been pursued . micro gas_bearings were fabricated as a single component through x_ray lithography . a stainless_steel gauge pin , machined to ultra precision , was used as a journal shaft . simple and very easy assembly processes using self-aligning concepts were developed as an alternative method to conventional assembly . this article presents the design , fabrication , assembly , and testing of micro gas_bearings . 1 introduction micro fabrication technology based on integrated_circuit-processes is mature in the various fields of mems such as optical devices , mechanical rf circuit , sensors , micro fluidics , and bio-devices . however , micro actuators fabricated through conventional micro machining processes have several limitations : sparse selection of materials , limited functionality , reliability , only a few processes , and structures that are principally 2-d [1 2] . for micro rotating devices such as micro motors , micro engines , and micro turbines to provide or generate meaningful power to external systems , their structures should be truly 3-dimensional . in addition , long_term reliability and assembly/packaging are critical issues . l . g . frechette , et al . [3] demonstrated an electrostatic induction_motor supported by externally pressurized hydrostatic gas_bearings . however , due to limitations arising from deep_reactive_ion_etching ( drie ) to thick silicon wafers , the nominal gas film thickness and the diameter to length ratio were out of normal design ranges for typical macro scale gas_bearings . this article presents a deep x_ray lithography fabrication_process for newly_developed micro_gas journal bearings . using the process , bearings with diameter ( 500 lm ) to length ( 320 lm ) ratio of 0 . 64 were fabricated . a stainless_steel gauge pin machined to ultra high_precision was the journal shaft; the high_precision maintained a nominal bearing clearance of 1 lm . the journal bearings have several evenly distributed recesses of 2 lm deep along the circumferential direction , to overcome an inherent instability in the hydrodynamic gas_bearings [4] . thrust bearings with 4 thrust pads , 3 lm high with 
generating optimization-based decision_support_systems this paper_discusses the implementation of opti-mizaiion based dsss . a n approach is proposed that will enable or/ms analysts t o develop this kind of system much more eociently than is possible today . the aim is to develop systems with modern guis , that interact with dbmss , and that can be built with little programming effort . many of the tools required for this approach were developed to support sml , which was chosen to represent the models because it has ample expressive power , completely specified syntax and semantics , total structure/data independence , and lends itself to the surface/deep_structure disiinc-tion , which makes the approach feasible . the approach is being implemented in a research_project funded by the chilean government and several firms and is being tested on problems taken from these firms . 
dt - an automated theorem prover for multiple-valued first-order predicate logics we describe the automated theorem prover \deep thought" ( d dt ) . the prover can be used for arbitrary multiple-valued rst-order logics , provided the connec-tives can be deened by truth tables and the quantiiers are generalizations of the classical universal resp . ex-istential quantiiers . d dt has been tested with many interesting multiple-valued logics as well as classical rst-order predicate_logic . d dt uses a free-variable semantic tableau calculus with generalized signs . for the existential tableau-rules two liberalized versions are implemented . the system utilizes a static index to control the application of axioms as wells as the search for applicable rules . a dynamic lemma generation strategy and various heuristics to control the tableau expansion and branch closure are integrated into d dt . theoretically , contradiction sets of arbitrary size can be discovered to close a branch . 
on the performance of evolutionary_algorithms in biomedical keyword clustering in the field of life_sciences it often turns out to be a challenge to quickly find the desired information due to the huge amount of available data . the research area of information_retrieval ( ir ) addresses this problem and tries to provide suitable solutions . one of the approaches used in ir is query extension based on keyword or document clusters . in this paper we present a deep analysis of a keyword clustering approach using four different kinds of evolutionary_algorithms , namely evolution_strategy ( es ) , genetic_algorithm ( ga ) , genetic_algorithm with strict offspring selection ( osga ) , and the multi_objective elitist non-dominated sorting genetic_algorithm ( nsga-ii ) . we have identified features that characterize solution candidates for the keyword clustering problem , e . g . , the number of documents covered and how well the identified clusters of keywords match with the occurrence of keywords in the given set of documents . the use of these features and how evolutionary_algorithms can be used to solve the optimization of keyword clusters is shown in this paper . to test the here presented approach we used a real world data_set provided within the trec-9 conference; this data_collection includes information about approximately 36 , 000 documents collected from the pubmed database . in the results section we compare the performance of the here tested evolutionary_algorithms and see that especially es and nsga-ii produce meaningful results for this documents collection . this approach based on evolutionary_algorithms shall be used further on in automated query extension for biomedical information_retrieval in pubmed . 
quantification via probability estimators quantification is the name given to a novel machine_learning task which deals with correctly estimating the number of elements of one class in a set of examples . the output of a quantifier is a real value; since training instances are the same as a classification problem , a natural approach is to train a classifier and to derive a quantifier from it . some previous_works have shown that just classifying the instances and counting the examples belonging to the class of interest ( classify & count ) typically yields bad quantifiers , especially when the class distribution may vary between training_and_test . hence , adjusted versions of classify & count have been developed by using modified thresholds . however , previous_works have explicitly discarded ( without a deep analysis ) any possible approach based on the probability estimations of the classifier . in this paper , we present a method based on averaging the probability estimations of a classifier with a very simple scaling that does perform reasonably well , showing that probability estimators for quantification capture a richer view of the problem than methods based on a threshold . 
peer knowledge modeling in computer_supported collaborative_learning learners benefit from collaboration because it triggers effective interaction processes such as externalization , elicitation and negotiation of knowledge . in order to communicate effectively , learners need to have a certain representation of their peers' knowledge . we refer to the process of building and maintaining a representation of the peers' knowledge as peer knowledge modeling . the present thesis aim at making contributions to the fields of computer support collaborative_learning and work ( cscl , cscw ) on three main levels . first , as an empirical contribution , we investigate the process of peer knowledge modeling in the context of cscl . our main research question inquires the effects of a socio-cognitive support , providing co-learners with cues about their peer's prior_knowledge , on collaborative_learning outcomes and processes . in an empirical study ( the kat experiment ) , university students ( n=64 ) participated in a remote computer-mediated dyadic learning scenario . they were provided ( or not ) with a visual representation of their partner's prior_knowledge level through a knowledge awareness tool ( kat ) . results_showed that the kat enhances co-learners' collaborative_learning gain . this effect appears to be mediated by the positive effect of the kat on participants' accuracy in estimating their peer's knowledge . analyses on the process level showed that participants of the kat condition produce more elaborated utterances . kat condition dyads' interactions are more focused on knowledge negotiation , whereas the control condition dyads are mainly focused on task completion . the kat seems to provide a sensitizing metacognitive support , structuring and regulating the collaboration by helping co-learners to cope with their knowledge gaps and discrepancies . second , as a methodological contribution , we examine the affordance of dual eye_tracking techniques as an innovative methodology to investigate , on a deeper level , the socio-cognitive_processes underlying collaboration . we introduce duet ( dual eye_tracking ) , a method using a multimodal technique to collect rich data featuring peers' synchronized gaze patterns , verbal interaction and potentially activities . we examine the main research applications of duet and exemplify them with analyses conducted in the context of the kat experiment . duet method appears to be a promising technique to investigate collaborative processes on a deep_level . finally , as a third computational contribution , we built upon micro-level analyses of the verbal referencing process to introduce and test regard , a computational model allowing to automatically detect verbal references and locate the specific object of reference . the results of the test show a reasonably good accuracy of the regard algorithm to detect and associate verbal references to 
some new results in model_integration model_integration has been an active field of research . ideally one would like to apply the same results from software_engineering to construct new models from previously defined and tested ones . as in software_engineering many algorithms and/or procedures written in diflerent languages are assembled together , so there are many models expressed in diflerent modeling languages which we would like to integmte when necessary . our aim is to show some new results in model_integration , achieved in the framework of structured modeling ( sm ) , by loo&g only at the model scheme and to inuestigate the possibility of constructing a unified fmmeworh to integrate algebmic and structured models . 1 introduction in a recent survey paper geoffrion , [17] , states that : " model_integration means difierent things to difier-ent people " . we would like to add that " it means at least one common thing for many people " , that is , a methodology to construct new models by assembling correlated sub-models or to extract sub-models from models already defined . clearly , there is a strict similarity between model_integration and one of the aspects from software_engineering concerning the production of new code : software_programs are often obtained by assembling well tested routines written in different programming_languages . the computer programmer has to know how the input/output communication works for the parameters of the routines , how the data_structures are stored , the compilers' options , etc . however sometimes he doea not need to call the whole routine , because it computes more than what he needs , but only a part of it . turning to model_integration , we can say that a model builder would like to follow more or less the same steps taken by a computer programmer . in fact , he would like to assemble models defined in the same or different definitional frameworks . many authors have distinguished between two types of integration . in this paper we adopt the convention proposed by geoffrion in [17] , and call " deep " integration that for which the models to integrate and their results have to belong to the same definitional framework; " functional " integration is when such a constraint does not hold . examples and results for " deep " integration can be found in geoffrion and tsai [15] , [25] . in a previous paper , [9] , we attempted to see how " deep " integration for structured models could be automated . in our approach there is only one concern : 
a c/c++-based functional verification framework using the systemc verification library this paper describes socbase-vl , which is a c/c++ based integrated framework for soc functional verification . it has a layered architecture which provides easier test-bench description , automatic verification of bus interfaces and seamless testbench migration . this framework does not require verification engineers to learn other verification languages as long as they have sufficient knowledge on both c/c++ and systemc . we have confirmed its usefulness by applying it to a tft_lcd controller verification . 1 introduction in the dynamic verification , a set of stimuli is applied to a design and then , its responses are compared to the corresponding correct outputs to check its equivalence or cor-rectness . this verification approach requires a testbench that generates stimuli and checks correct outputs . thus , the quality of verification depends on the quality of the test-bench . as the designs are getting more complex , however , the difficulty of authoring the testbenches is continuously growing even more rapidly . the difficulties related to the testbench design can be summarized as follows : as the number of the state in a component increases linearly , the number of test_cases increases exponentially . therefore , manual enumeration of each test_case is not feasible . several models for a component may be required at different abstraction levels . a testbench for each model should be redesigned to verify the model . describing a testbench often requires a deep and thorough understanding on domain_specific knowledge . e . g . bus specification . a quantitative measure of the quality of verification is needed . otherwise , the quality of verification tends to depend on that of verification engineers . to alleviate those problems , many researchers and eda vendors offer tools for testbench authoring [1-5] . the sys-temc verification library ( scv ) is an extension of sys-temc for easier testbench authoring which provides constrained randomization and transaction level tracing[1] . socbase-vl is another extension of the scv , which additionally provides a layered architecture for easier test-bench description , seamless testbench migration , and an automatic verification of bus interfaces . it also provides the coverage monitor modeling library ( cml ) for functional coverage monitoring . in this paper , we explain our layered testbench architecture in section 2 and the cml in section 3 . in section 4 , we briefly introduce how to use our framework through a practical example . the summary and future works are given in section 5 . a h/w component ( or a system ) can have several abstraction level models : transaction level model , rt_level model , fpga prototype and silicon . we propose a layered 
npoess soil_moisture satellite data_assimilation : using windsat data a four dimensional coupled atmospheric/land data_assimilation framework is developed using the regional atmospheric mesoscale data_assimilation system ( ramdas ) to retrieve deep soil_moisture profiles . passive microwave data from coriolis windsat is used as a surrogate for future national polar-orbiting operational environmental satellite system ( npoess ) microwave sensors . current efforts are focused on the use of the system for a case study occurring in september 2003 . the polarization ratio was found to be the most useful function for the observational soil_moisture sensitivity_analysis . simple brightness_temperature differences indicated radiative_transfer model biases on the order of 5-8 k . additional radiative_transfer model debiasing studies are thus needed; however , windsat polarization ratio results are able to demonstrate a strong soil_moisture signal . the calibration and validation approaches for both the output soil_moisture product and related model input data_sets were also tested using the windsat data_sets . in particular , model output from the usaf agricultural meteorological model ( agrmet ) was analyzed and compared to in situ and satellite data_sets . the performance characteristics of agrmet were determined both temporally and spatially . objective methods for performing quality_control have been developed to assure that the first-guess information used within the satellite data_assimilation system is of the highest possible quality . the outcome of this work will be to extend satellite soil_moisture information from the surface to deeper soil levels to more accurately determine its effect upon dod-related trafficability , off-road mobility , counter-mine operations , and hydrological streamflow estimation . 
underwater photogrammetry and object modeling : a case study of xlendi wreck in malta in this paper we present a photogrammetry-based approach for deep_sea underwater surveys conducted from a submarine and guided by knowledge_representation combined with a logical approach ( ontology ) . two major issues are discussed in this paper . the first concerns deep_sea surveys using photogrammetry from a submarine . here the goal was to obtain a set of images that completely covered the selected site . subsequently and based on these images , a low_resolution 3d model is obtained in real-time , followed by a very high_resolution model produced back in the laboratory . the second issue involves the extraction of known artefacts present on the site . this aspect of the research is based on an a priori representation of the knowledge involved using systematic reasoning . two parallel processes were developed to represent the photogrammetric process used for surveying as well as for identifying archaeological artefacts visible on the sea_floor . mapping involved the use of the cidoc-crm system ( international committee for documentation ( cidoc ) -conceptual reference model ) -this is a system that has been previously utilised to in the heritage sector and is largely available to the established scientific community . the proposed theoretical representation is based on procedural attachment; moreover , a strong link is maintained between the ontological description of the modelled concepts and the java_programming_language which permitted 3d structure estimation and modelling based on a set of oriented images . a very recently discovered shipwreck acted as a testing ground for this project; the xelendi phoenician shipwreck , found off the maltese coast , is probably the oldest known shipwreck in the western_mediterranean . the approach presented in this paper was developed in the scope of the groplan project ( g n ralisation du relev , avec ontologies et photogramm trie , pour l'arch ologie navale et sous-marine ) . financed by the french national research agency ( anr ) for four years , this project associates two french research laboratories , an industrial partner , the university of malta , and texas a &amp; m university . 
a kind of low_cost non-intrusive autonomous fault emulation system sram-filed programmable gate arrays ( fpga ) have become one of the most important carriers of digital electronic system because of its many inborn advantages . however , as manufacture of integrated_circuit evolves towards very deep_sub_micron_technology , fpga designers must be careful of circuit's single_event_upset ( seu ) susceptibility when used in hostile environment , such as avionics and space applications where reliability is vital . we proposed a seu-fault emulation platform to evaluate circuit's seu mitigation performance . the platform does not need any external circuit or micro controller to manage fault emulation process compared with existing approach . source codes of circuit_under_test ( cut ) do not need to be modified or intruded with any component . it is a non-intrusive testing . communication between host-computer and emulation board is minimized to accelerate fault_injection speed . experimental result shows that a single fault injecting ( including multi-bits-upset ) only costs 29us . a circuit state reloading technology is exploited to increase emulation efficiency . moreover , in the field of evolvable hardware , genetic operations can be reconfigured and its fitness can be evaluated on-line using the proposed fast dynamic reconfiguration method , which is useful for implementing self-repair and self-evolutionary hardware . 
a situation_awareness assistant for human deep_space_exploration this paper presents the development_and_testing of a virtual camera ( vc ) system to improve astronaut and mission operations situation_awareness while exploring other planetary bodies . in this embodiment , the vc is implemented using a tablet-based computer system to navigate through interactive database application . it is claimed that the advanced interaction media capability of the vc can improve situation_awareness as the distribution of human space_exploration roles change in deep_space_exploration . the vc is being developed and tested for usability and capability to improve situation_awareness . work completed thus far as well as what is needed to complete the project will be described . planned testing will also be described . 1 introduction the virtual camera ( vc ) for human deep_space_exploration is a virtual assistant that is being developed for use by astronauts and other associated mission operations personnel as they use human-piloted rovers to explore the surface of other planets [1] . the vc concept is based on incremental upgrading of a suboptimal 3-d geographical and geological database . it is an interactive window on the world as we know it at the time it is being used . such an interactive window enables the user to maintain better situational awareness , to navigate and further explore space . it can be used for training , data_analysis and augmentation of actual surface exploration . the vc for human space_exploration was originally described for use by astronauts navigating a surface exploration rover on a remote body such as the moon [1] . further analysis has indicated that such an interactive window to data in multiple dimensions has many other applications in domains such as aviation , medicine , control systems and finance . this paper presents a tablet version of the vc for deep_space_exploration
model_checking large network_protocol implementations network protocols must work . the effects of protocol specification or implementation errors range from reduced performance , to security breaches , to bringing down entire networks . however , network protocols are difficult to test due to the exponential size of the state space they define . ideally , a protocol implementation must be validated against all possible events ( packet arrivals , packet losses , timeouts , etc . ) in all possible protocol states . conventional means of testing can explore only a minute fraction of these possible combinations . this paper focuses on how to effectively find errors in large network_protocol implementations using model_checking , a formal_verification technique . model_checking involves a systematic exploration of the possible states of a system , and is well-suited to finding intricate errors lurking deep in exponential state_spaces . its primary limitation has been the effort needed to use it on software . the primary contribution of this paper are novel techniques that allow us to model check complex , real_world , well-tested protocol implementations with reasonable effort . we have implemented these techniques in cmc , a c model_checker [30] and applied the result to the linux tcp/ip implementation , finding four errors in the protocol implementation . 
transformation-invariant convolutional jungles many computer_vision problems arise from information_processing of data sources with nuisance variances like scale , orientation , contrast , perspective foreshortening or in medical_imaging staining and local warping . in most cases these variances can be stated a priori and can be used to improve the generalization of recognition algorithms . we propose a novel supervised feature_learning approach , which efficiently extracts information from these constraints to produce interpretable , transformation-invariant_features . the proposed method can incorporate a large class of transformations , e . g . , shifts , rotations , change of scale , morphological operations , non-linear distortions , photometric transformations , etc . these features boost the discrimination power of a novel image_classification and segmentation method , which we call transformation-invariant convolu-tional jungles ( ticj ) . we test the algorithm on two benchmarks in face_recognition and medical_imaging , where it achieves state of the art results , while being computation-ally significantly more efficient than deep_neural_networks . 
meeting medical terminology needs-the ontology-enhanced medical concept mapper this paper describes the development_and_testing of the medical concept mapper , a tool designed to facilitate access to online medical information sources by providing users with appropriate medical search terms for their personal queries . our system is valuable for patients whose knowledge of medical vocabularies is inadequate to find the desired information , and for medical experts who search for information outside their field of expertise . the medical concept mapper maps synonyms and semantically related concepts to a user's query . the system is unique because it integrates our natural_language_processing tool , i . e . , the arizona ( az ) noun phraser , with human-created ontologies , the unified medical language system ( umls ) and wordnet , and our computer generated concept space , into one system . our unique contribution results from combining the umls semantic net with concept space in our deep_semantic parsing ( dsp ) algorithm . this algorithm establishes a medical query context based on the umls semantic net , which allows concept space terms to be filtered so as to isolate related terms relevant to the query . we performed two user studies in which medical concept mapper terms were compared against human experts' terms . we conclude that the az noun phraser is well suited to extract medical phrases from user queries , that wordnet is not well suited to provide strictly medical synonyms , that the umls metathesaurus is well suited to provide medical synonyms , and that concept space is well suited to provide related medical terms , especially when these terms are limited by our dsp algorithm . 
dependable computing in the context of mobility , nomadicity , ubiquity , and pervasiveness why do software projects fail ? : reasons and a solution using a bayesian classifier to predict potential risk ( pdf ) p . 4 sigma : a fault-tolerant mutual_exclusion algorithm in dynamic distributed_systems subject to process crashes and memory losses p . 7 intersecting sets : a basic abstraction for asynchronous agreement problems p . 15 decision optimal early-stopping k-set agreement in synchronous systems prone to send omission failures p . 23 privacy-preserving bayesian_network structure learning on distributed_heterogeneous data p . 31 simultaneous simulation of alternative system configurations shravan gaonkar p . 41 availability assessment of sunos/solaris unix systems based on syslogd and wtmpx log files : a case study p . 49 on-chip debugging-based fault emulation for robustness evaluation of embedded_software components p . 57 bayesian_networks modeling for software inspection effectiveness p . 65 application-based metrics for strategic placement of detectors p . 75 a hardware approach to concurrent error_detection capability enhancement in cots processors p . 83 optimal fault_tolerant routing scheme for generalized hypercube p . 91 high_order syndrome testing for vlsi_circuits p . 101 a new bist solution for system-on-chip p . 109 a failure-aware model for estimating and analyzing the efficiency of web_services compositions p . 114 code design and decoding methods for burst error locating codes p . 125 an evaluation of the virtual router redundancy protocol extension with load_balancing p . 133 formal development of software for tolerating transient_faults p . 140 on the fully-informed communication-induced checkpointing protocol p . 151 optimal choice of checkpointing interval for high_availability p . 159 an improved scheme of index-based checkpointing p . 167 compression/scan co-design for reducing test_data_volume , scan-in power_dissipation and test application time p . 175 partitioned cache shadowing for deep_sub_micron ( dsm ) regime p . 183 proxy cryptography for secure inter-domain information exchanges p . 193 anomaly_detection with high deviations for system security p . 200 a multi-faceted approach towards spam-resistible mail p . 208 a virtual modeling and a fast algorithm for grid service reliability p . 219 research on architecture and design principles of cots components based generic fault_tolerant computer p . 227 bi-objective model for test_suite reduction based on modified condition/decision coverage p . 235 a reliable routing algorithm based on fuzzy applicability of f sets in manet p . 245 an efficient approach to tolerating route errors in mobile_ad_hoc_networks p . 250 a novel approach to kernel construction of china bridge ca p . 258
test-pattern_selection for screening small_delay_defects in very-deep submicron integrated_circuits timing_related_defects are major contributors to test escapes and in-field reliability problems for very-deep submicron integrated_circuits . small delay_variations induced by crosstalk , process_variations , power_supply_noise , as well as resistive_opens and shorts can potentially cause timing_failures in a design , thereby leading to quality and reliability concerns . we present a test-grading technique that uses the method of output deviations for screening small_delay_defects ( sdds ) . a new gate-delay defect probability_measure is defined to model delay_variations for nanometer_technologies . the proposed technique intelligently selects the best set of patterns for sdd detection from an n-detect pattern set generated using timing-unaware automatic_test_pattern_generation ( atpg ) . it offers significantly lower computational_complexity and it excites a larger number of long paths compared to a commercial timing_aware_atpg tool . our results also show that , for the same pattern_count , the selected patterns provide more effective coverage ramp-up than timing_aware_atpg and a recent pattern_selection method for random small_delay_defects potentially caused by resistive_shorts , resistive_opens , and process_variations . 
vip - an input pattern_generator for indentifying critical voltage_drop for deep_sub_micron designs we present a novel input pattern_generator for dynamic power network simulation . the obtained patterns successfully identia critical voltage_drop areas for a set of industrial designs , which are dificult to be found using functional vectors . the search_engine of the pattern_generator for worst_case ir voltage_drop is based on the multi_objective genetic_algorithm . to achieve high coverage for critical voltage_drop cells , we propose to model the search criteria into the maximum weighted matching of a bipartite_graph , and guide the search direction according to the matching results . experimental results show that , compared with the other approaches , our patterns give a higher coverage of critical voltage_drop cells . 1 . introduction for designers of today's high_performance and complexity ics , the accurate and efficient analysis for power net voltage_drop is very important . excessive voltage_drop increases the transistor and gate delays , which results in unpredictable performance or performance failing to meet original design goal . to identify this problem , dynamic simulation is needed for providing the profile of the voltage_drop . therefore , generating high_quality input patterns for simulation of the voltage drops has become a necessary step in the entire design_cycle . recently , several genetic_algorithm-based ( ga-based ) techniques have been proposed to generate the input patterns for identifying the maximum instantaneous current [4] , maximum power_dissipation [ 11 , and maximum voltage_drop [5] . through iteratively generating the new test patterns for simulation based on the " good " property of the current patterns , they produce tight lower bounds for these problems . in such a way , however , certain functional blocks whose current has little contribution to the maximum total current or maximum
evaluating remotely_sensed rainfall estimates using nonlinear mixed_models and geographically weighted regression this article evaluates an infrared-based satellite algorithm for rainfall estimation , the convective strat-iform technique , over mediterranean . unlike a large number of works that evaluate remotely_sensed estimates concentrating on global measures of accuracy , this work examines the relationship between ground_truth and satellit0e derived data in a local scale . hence , we examine the fit of ground_truth and remotely_sensed data on a widely adopted probability_distribution for rainfall totals the mixed log_normal_distribution per measurement location . moreover , we test for spatial nonstationarity in the relationship between in situ observed and satellite-estimated rainfall totals . the former investigation takes place via using recent algorithms that estimate nonlinear mixed_models whereas the latter uses geographically weighted regression . remotely_sensed information from satellites , having a high spatial coverage and high temporal sampling , can play a key role in monitoring precipitation in flood-prone regions , sea precipitation , and other extreme_weather events . such information is of particular importance in areas with sparse rain_gauge and precipitation radar networks from which reliable real time assessments of precipitation can be obtained . an area where the latter argument applies is the mediterranean_basin; its climate is known for its variety and variability , due to the surrounding orography , to the relative high_temperature of the sea and to the different origin and physical_characteristics of the air masses . there is a paucity of dense rain_gauge networks or precipitation radar networks due to the presence of the mediterranean_sea and to the complex topography . in addition , rain_gauge observations are not generally available in real time in many regions , and missing reports and grossly erroneous reports occur in cases of extremely heavy rainfall and in regions of steep terrain . a number of techniques have been developed to indirectly estimate rainfall using visible ( vis ) and infrared ( ir ) satellite data . most of these methods are based on the notion that deep convec-tive clouds might produce more rain and on operational findings , which show that regions of rainfall tend to be correlated with bright ( vis ) , cold ( ir ) clouds . when ir satellite data of high spatial and temporal_resolution became available , precipitation was correlated to the cloud-top temperature ( ctt ) and the relationships used in the satellite techniques were redefined as a function of ctt . numerous new precipitation estimation algorithms have been developed that use ir data as the only data input; i . one disadvantage of vis/ir techniques for estimating precipitation rates 
on the complexity of nonrecursive xquery and functional query languages on complex values this article studies the complexity of evaluating functional query languages for complex values such as monad_algebra and the recursion-free fragment of xquery . we show that monad_algebra , with equality restricted to atomic values , is complete for the class ta[2<sup><i>o</i> ( <i>n</i> ) </sup> , <i>o</i> ( <i>n</i> ) ] of problems solvable in linear exponential time with a linear number of alternations if the query is assumed to be part of the input . the monotone fragment of monad_algebra with atomic value equality but without negation is nexptime-complete . for monad_algebra with deep value equality , that is , equality of complex values , we establish ta[2<sup><i>o</i> ( <i>n</i> ) </sup> , <i>o</i> ( <i>n</i> ) ] lower and exponential-space upper bounds . we also study a fragment of xquery , core_xquery , that seems to incorporate all the features of a query_language on complex values that are traditionally deemed essential . a close connection between monad_algebra on lists and core_xquery ( with &#8220;child&#8221; as the only axis ) is exhibited . the two languages are shown expressively equivalent up to representation issues . we show that core_xquery is just as hard as monad_algebra with respect to query and combined complexity . as core_xquery is nexptime-hard , the best-known techniques for processing such problems require exponential amounts of working_memory and doubly exponential time in the worst case . we present a property of queries---the lack of a certain form of composition---that virtually all real_world xqueries have and that allows for query evaluation in pspace and thus singly exponential time . still , we are able to show for an important special case---core_xquery with equality testing restricted to atomic values---that the composition-free language is just as expressive as the language with composition . thus , under widely-held complexity-theoretic assumptions , the language with composition is an exponentially more succinct version of the composition-free language . 
ion_thruster plume plasma interactions in the solar wind the ion_thruster plasma near-spacecraft plume environment 3 . far-field plume-solar_wind interactions the solar wind plasma formulation and approach ion propulsion will be used for the first time on an in-terplanetary spacecraft , deep_space one ( ds1 ) , scheduled for launch in july 1998 . a primary objective of new millennium ds 1 is to flight validate solar electric propulsion ( sep ) for interplanetary missions . the cruise phase of the mission will characterize the life and performance of a 30 cm xenon ion_thruster and determine how its operation may affect spacecraft payloads and critical subsystem . . . effects introduced by the operation of the ion_thruster have long raised both technology and science concerns . the technology concerns include plutne backflow contamination and spacecraft interactions with the induced plasma environment . backflow contamination can lead to effluent deposition that can affect thermal control surfaces , optical sensors , solar arrays , science instru-mentation , and communications . the induced plasma environment will modify spacecraft charging characteristics , and can lead to plasma interactions with the solar array . the science concerns relate to plasma nleasrrre-rnents . the plume will modify the properties of the solar wind flowing around the spacecraft and may contaminate measurements of the ambient plasma and magnetic_fields . as ion thrusters are designed to operate for long periods of time , these effects need to be carefully assessed . the interactions induced by ion_thruster plumes have been studied for some time . due to the complexity of the problem , the difhculty of matching space conditions in a laboratory , and the lack of opportunities to flight test ion thrusters , computer particle simulations have recently become the best means to study this problem . samania roy et al[1996a , 1996b] used hybrid pic sinl-ulations to model the far-downstream region and study charge exchange ion backflow . wang and brophy[1995] developed full particle and hybrid pic-mcc models of single and multiple thruster plumes and studied the effects of ambient environment on plasma plumes . wang et al . [1996] have carried out 3-d simulations of ion_thruster plume environments using parameters similar to those of the ns'i'ar ( nasa solar-electric propulsion technology application readiness ) thruster to be used on ds1 . all studies on this subject so far have concentrated on charge-exchange ion interactions near the spacecraft . there have been no studies concerning other aspects of plume interactions , such as ion_thruster operation in the solar wind environment and plume-solar_wind interactions . for an interplanetary spacecraft such w the ds-1 , the ion_thruster operates in the solar wind , which is a tenuous , relatively hot plasma with a high flow speed and a frozen-in 
decoding the perioperative process breakdowns : a theoretical model and implications for system design background breakdowns in communication and coordination are situations of mismatch between actual and expected conditions in joint activities . breakdowns have been identified as the leading cause of adverse events in healthcare , especially in the operating room environment . as a result , researchers have started to examine breakdowns in healthcare as emergent dynamics of teamwork . however , the occurrence and consequences of breakdowns related to inter-team processes are yet to be addressed at a fine level of detail . in this paper we seek understanding of breakdowns at the systemic level , and its relevance to design . objectives the objective of this study is to bring forward an in-depth understanding of the impact of breakdowns on the surgical process by expanding the focus of analysis beyond teamwork dynamics , to the level of hospital system processes . this study also aims to examine the implications of such understanding of breakdowns for the design of clinical systems . methods properties of breakdowns and repairs were inductively derived , and developed into a formal coding scheme , which was applied over a set of observed breakdowns from an elective surgery unit in a north_american hospital . systematic content_analysis was employed to quantify qualitative data spanning 79 h of observations , followed by statistical hypotheses testing for relationships between variables of breakdowns and repairs . measures breakdown type , theme , tangibility , coordination scale , breakdown lifetime , repair strategy , and repair cost . results the results_reveal that properties of breakdowns determine properties of repairs . the majority of breakdowns were outside the scope of teamwork--at the inter-team coordination level . the results also demonstrate that breakdowns usually propagate downstream in the surgical process , affecting the work of multiple teams , and the longer they propagate the higher the communication cost associated with the respective repair . the implications are two-fold : in terms of theory we develop a conceptual_framework of breakdowns in perioperative work , and in terms of system design we propose a design framework informed by the acquired understanding of breakdowns . conclusions this study achieved an initial understanding of the deep features of breakdowns from a process-oriented perspective , which allowed us to build the groundwork for a theoretical model of breakdowns in perioperative activities and to propose a design approach that tackles breakdowns during early_stages of system development . the direct association between breakdowns and repairs can be exploited in both it-system design and organizational design . the patterns of repair work can inform design so as to provide clinicians with the types of information that will prevent breakdowns from occurring or to mitigate the impact of breakdowns . the results_reveal that preventing breakdown propagation should be a prime target in surgical applications design . 
experience in critical_path_selection for deep_sub_micron delay test and timing validation critical_path_selection is an indispensable step for ac delay test and timing validation . traditionally , this step relies on the construction of a set of worse-case paths based upon discrete timing_models . however , the assumption of discrete timing_models can be invalidated by timing_defects and process_variation in the deep sub-micron domain , which are often continuous in nature . as a result , critical paths defined in a traditional timing_analysis approach may not be truly critical in reality . in this paper , we propose using a statistical delay evaluation framework for estimating the quality of a path set . based upon the new framework , we demonstrate how the traditional definition of a critical_path set may deviate from the true critical_path set in the deep sub-micron domain . to remedy the problem , we discuss improvements to the existing path_selection strategies by including new objectives . we then compare statistical approaches with traditional_approaches based upon experimental analysis of both defect_free and defect-injected cases . 
[feeling for affective psychosis] . the paper presents new research results on the pathogenesis of depression and mania and future perspectives on diagnosis and treatment . on the one hand , we know that depression most often develops in the interplay between genes and the environment; and , on the other hand , that depression and mania may be induced suddenly by deep_brain_stimulation of the subthalamic areas . ongoing studies aim to identify biomarkers for depression and mania , and results from recent_research suggest that neuroticism , abnormal response to the dex-crh test , increased frontotemporal serotonine 2a binding , abnormal emotional processing and deceased executive_function may be candidates for such biomarkers . these biomarkers may help to improve diagnosis and treatment in the future . 
experimental_evaluation of automatic hint generation for a logic tutor we have augmented the deep thought logic tutor with a hint factory that generates data-driven , context-specific hints for an existing computer_aided instructional tool . we investigate the impact of the hint factory's automatically_generated hints on educational outcomes in a switching replications experiment that shows that hints help students persist in a deductive logic proofs tutor . three instructors taught two semester-long courses , each teaching one semester using a logic tutor with hints , and one semester using the tutor without hints , controlling for the impact of different instructors on course outcomes . our results show that students in the courses using a logic tutor augmented with automatically_generated hints attempted and completed significantly more logic proof problems , were less likely to abandon the tutor , performed significantly better on a post_test implemented within the tutor , and achieved higher grades in the course . 
a 40 nm 0 . 32 v 3 . 5 mhz 11t single-ended bit-interleaving subthreshold sram with data-aware write-assist this paper presents a new bit-interleaving 11t subthreshold sram cell with data-aware power-cutoff ( dapc ) write-assist to mitigate the leakage and variation and improve the write-ability in deep sub-100nm technology . measurement results from a 4 kb test_chip implemented in 40 nm general_purpose ( 40gp ) cmos_technology operates for v<sub>dd</sub> down to 0 . 32 v ( ~0 . 69x of threshold_voltage ) with v<sub>ddmin</sub> limited by read operation . the measured maximum operation frequency is 3 . 5 mhz ( 16 . 5 mhz ) at 0 . 32 v ( 0 . 38 v ) with total power_consumption of 15 . 2 &mu;w ( 27 . 2 &mu;w ) at 25 &#176;c . 
teaching data_structures with besocratic ( abstract only ) data_structures are one of the fundamental concepts that all computer scientist students must learn if they are to succeed in their careers . therefore , it is important to develop and assess questions targeted at improving the teaching of data_structures . unfortunately , research suggests that multiple_choice or matching questions cannot be used to properly assess deep_knowledge on a subject [1 , 2 , 3 , 4] . students can often guess their way to the correct answer . we believe that students must construct these structures instead of simply identifying them . however , analyzing many hand-drawn data_structures is time-consuming for large class sizes . this poster describes a web_based software tool , <i>besocratic</i> , designed to facilitate interactivity in a data_structures course . <i>besocratic</i> allows students to build data_structures intuitively using a combination of handwriting_recognition and gestures . using <i>besocratic</i> , instructors can create intelligent tutors that teach students to construct various data_structures . these tutors are able to identify problems and provide multi-tiered feedback to students . furthermore , <i>besocratic</i> records each action a student makes , so it may be replayed and visualized to gain deeper insights into how students construct data_structures and complete algorithms . we have created and pilot-tested a <i>besocratic</i> activity , which teaches students how to construct splay trees . 
automated segmentation of basal_ganglia and deep_brain_structures in mri of parkinson's_disease purpose template_based segmentation techniques have been developed to facilitate the accurate targeting of deep_brain_structures in patients with movement disorders . three template_based brain mri segmentation techniques were compared to determine the best strategy for segmenting the deep_brain_structures of patients_with_parkinson's_disease . methods t1_weighted and t2-weighted magnetic_resonance ( mr ) image templates were created by averaging mr_images of 57 patients_with_parkinson's_disease . twenty-four deep_brain_structures were manually segmented on the templates . to validate the template_based segmentation , 14 of the 24 deep_brain_structures from the templates were manually segmented on 10 mr scans of parkinson's patients as a gold_standard . we compared the manual segmentations with three methods of automated segmentation : two registration-based_approaches , automatic nonlinear image_matching and anatomical labeling ( animal ) and symmetric image normalization ( syn ) , and one patch-label fusion technique . the automated labels were then compared with the manual labels using a dice-kappa metric and center of gravity . a friedman test was used to compare the dice-kappa values and paired t tests for the center of gravity . results the friedman test showed a significant difference between the three methods for both thalami ( p < 0 . 05 ) and not for the subthalamic nuclei . registration with animal was better than with syn for the left thalamus and was better than the patch-based_method for the right thalamus . conclusion although template_based approaches are the most used techniques to segment basal_ganglia by warping onto mr_images , we found that the patch-based_method provided similar results and was less time-consuming . patch-based_method may be preferable for the subthalamic nucleus segmentation in patients with parkinson's disease . 
deep cerebellar neurons mirror the spinal cord's gain to implement an inverse controller smooth and coordinated motion requires precisely timed muscle activation_patterns , which due to biophysical limitations , must be predictive and executed in a feed_forward manner . in a previous study , we tested kawato's original proposition , that the cerebellum implements an inverse controller , by mapping a multizonal microcomplex's ( mzmc ) biophysics to a joint's inverse transfer_function and showing that inferior olivary neuron may use their intrinsic oscillations to mirror a joint's oscillatory dynamics . here , to continue to validate our mapping , we propose that climbing fiber input into the deep cerebellar nucleus ( dcn ) triggers rebounds , primed by purkinje_cell inhibition , implementing gain on io's signal to mirror the spinal_cord reflex's gain thereby achieving inverse control . we used biophysical modeling to show that purkinje_cell inhibition and climbing fiber excitation interact in a multiplicative fashion to set dcn's rebound strength; where the former primes the cell for rebound by deinactivating its t-type ca2 ( + ) channels and the latter triggers the channels by rapidly depolarizing the cell . we combined this result with our control_theory mapping to predict how experimentally injecting current into dcn will affect overall motor output performance , and found that injecting current will proportionally scale the output and unmask the joint's natural response as observed by motor output ringing at the joint's natural_frequency . experimental verification of this prediction will lend support to a mzmc as a joint's inverse controller and the role we assigned underlying biophysical principles that enable it . 
modeling_language vagueness in privacy policies using deep neural networks website privacy policies are too long to read and difficult to understand . the over-sophisticated language undermines the effectiveness of privacy notices . people become less willing to share their personal_information when they perceive the privacy_policy as vague . the goal of this paper is to decode vagueness from a natural_language_processing perspective . while thoroughly identifying the vague terms and their linguistic scope remains an elusive challenge , in this work we seek to learn vector representations of words in privacy policies using deep neural networks . the vector representations are fed to an interactive visualization tool ( lstmvis ) to test on their ability to discover syntactically and semantically related terms . the approach holds promise for modeling and understanding language vagueness . 
ocean_drilling_program leg 191 scientific prospectus northwest pacific seismic observatory and hammer drill engineering tests material in this publication may be copied without restraint for library , abstract service , educational , or personal research purposes; however , republication of any portion requires the written consent of the director , any opinions , findings , and conclusions or recommendations expressed in this publication are those of the author ( s ) and do not necessarily reflect the views of the national_science_foundation , the participating agencies , this scientific prospectus is based on precruise joides panel discussions and scientific input from the designated co-chief scientists on behalf of the drilling proponents . the operational plans within reflect joides planning committee and thematic panel priorities . during the course of the cruise , actual site operations may indicate to the co-chief scientists and the operations manager that it would be scientifically or operationally advantageous to amend the plan detailed in this prospectus . it should be understood that any proposed changes to the plan presented here are contingent upon approval of the director of the ocean_drilling_program in consultation with the science and operations committees ( successors to the planning committee ) and the pollution prevention and safety panel . abstract ocean_drilling_program leg 191 consists of two parts : ( 1 ) a science segment devoted to drilling and casing a hole on the northwest pacific abyssal seafloor ( site wp-2 ) coupled with the installation of a broadband seismometer for a long_term sub-seafloor borehole observatory and ( 2 ) engineering tests of the hard_rock reentry system ( hrrs ) and other equipment . the seismic observatory is an important component of the international ocean network seismometer net . by filling a large gap in the global station grid , it will help increase the resolution of global tomographic studies , which have revolutionized understanding of mantle dynamics and structure . moreover , it will allow more precise studies of the seismic structure of old pacific_ocean crust and lithosphere , as well as better resolution of earthquake locations and mechanisms in the northwest pacific subduction_zone . approximately 400 m of sediments and 100 m of basalt will be cored at site wp-2 . studies of these cores will add to existing knowledge of cretaceous pacific mid_ocean_ridge basalt chemistry , construction of the ocean crust , paleolatitude , the age of magnetic lineations , basalt physical properties , and the deep biosphere . engineering tests will be conducted to test the performance of hammers and bits for the hrrs that will eventually allow hard_rock spudding of holes at mid-ocean ridges and other locales where holes must be started on hard outcrops . the 
excavation-parallel laser scanning of a medieval cesspit in the archaeological zone cologne , germany during the construction of an underground museum in the historic city center of cologne , germany , large parts of the roman and medieval city are being excavated . the newly excavated remains as well as remains of the roman city , which had already been excavated in 1954 , exhibit structural damages . while at first deficiencies in the construction were assumed to be the cause of the damages , in 2003 a seismogenic origin was suggested . to further test this hypothesis of seismically induced slope movements and other possible causes , a multidisciplinary project was started . one step in this project is the documentation of the damages using a 3d laser scanner , followed by a quantitative damage analysis . this article presents the 3d documentation and the quantitative damage analysis of a recently excavated medieval cesspit . the 8 . 3m-deep cesspit was mapped during 11 campaigns using a phase-based 3d laser scanner . due to the static conditions of the cesspit , the structure could not be excavated in its entirety . after the excavation of every 1-2m-section , restoration work had to be done to avoid a collapse of the construction . the laser scanning technique offered the possibility of working parallel to the excavation so the original conditions of each section could be documented before the restoration . the resulting models were used to identify , classify , and quantify the structural damages of the cesspit . 
3d reconstruction of subsurface geological_bodies : methods and applications an original approach for the 3d visualisation and modeling of buried deep_and_shallow subsurface geological_bodies by means of gocad is presented in this paper . cartographic data and structural surface observations have been used , establishing a link between the geographic_information_systems , where the data are stored , and the gocad environment . four main sources of information are needed for the development of a 3d structural model : 1-topographic data represented by contour lines and quoted points; 2-geological , geomorphological and tectonic boundaries consisting of 2d linear elements; 3-mesoscopic structural measurements including attitude of planar and linear elements ( bedding , thrusts , strike-slip , normal faults , lineations , etc . ) ; 4-geological cross_section reconstructed through the analysis of surface geological data; other sources of geological information as wells data , seismic sections , etc , can be also introduced into the model . the analysed geological_bodies consist of a deep landslide developed in the sedimentary cover of the southern_alps ( lombardia , northern_italy ) , and of the sedimentary successions of the sant'arcangelo basin , a recent piggy-back basin located in the southern apennines ( basilicata , southern_italy ) . the geometric features of the reconstructed geological_bodies can be used to design preliminary monitoring plans or subsurface investigations through seismic surveys and drilling . the characterisation of the shallow subsurface is important for civil_engineering and environmental applications that depend upon precise definitions of the geometrical , geomechanical and hydrological properties of rock bodies . 1 . introduction this paper describes a complete framework for the 3d representation and modeling of deep_and_shallow subsurface geological_bodies . three dimensional reconstruction is developed in the gocad environment , basing on digital information stored in geographic_information_systems ( gis ) and related databases . these procedures intend to visualise geological information and to establish topological relationships among the analysed objects , coupling the data_processing capabilities of gis ( arcinfo , arcview ) with 3d modeling in gocad . a simple link among different software is established through a set of conversion programs that make the information , stored in a gis and related database_management system , available on demand . the proposed procedures have been tested on different geological problems exploiting the same types of data , consisting of geological and structural surface observations directly surveyed in the field . two examples of 3d reconstruction are here shortly
spanish freeling dependency_grammar this paper presents the development of an open_source spanish dependency_grammar implemented in freeling environment . this grammar was designed as a resource for nlp applications that require a step further in natural_language automatic analysis , as is the case of spanish-to-basque translation . the development of wide-coverage rule_based grammars using linguistic_knowledge contributes to extend the existing spanish deep parsers collection , which sometimes is limited . spanish freeling dependency_grammar , named estxala , provides deep and robust parse_trees , solving attachments for any structure and assigning syntactic functions to dependencies . these steps are dealt with hand written rules based on linguistic_knowledge . as a result , freeling dependency parser gives a unique analysis as a dependency_tree for each sentence analyzed . since it is a resource open to the scientific community , exhaustive grammar evaluation is being done to determine its accuracy as well as strategies for its manteinance and improvement . in this paper , we show the results of an experimental_evaluation carried out over estxala in order to test our evaluation methodology . 
an ic manufacturing_yield model considering intra_die variations in deep_submicron feature_sizes continue to shrink aggressively beyond the natural capabilities of the 193 nm lithography used to produce those features thanks to all the innovations in the field of resolution enhancement techniques ( ret ) . with reduced feature_sizes and tighter pitches die level variations become an increasingly dominant factor in determining manufacturing_yield . thus a prediction of design-specific features that impact intra_die variability and correspondingly its yield is extremely valuable as it allows for altering such features in a manner that reduces intra_die variability and improves yield . in this paper , a manufacturing_yield model which takes_into_account both physical layout features and manufacturing fluctuations is proposed . the intra_die systematic variations are evaluated using a physics-based_model as a function of a design's physical layout . the random variations and their across-die spatial correlations are obtained from data harvested from manufactured test_structures . an efficient algorithm is proposed to reduce the order of the numerical_integration in the yield model . the model can be used to ( i ) predict manufacturing yields at the design stage and ( ii ) enhance the layout of a design for higher manufacturing_yield . 
global refinement of random forest y x training_samples rf 2k leaf_nodes rf 10k leaf_nodes ours 2k leaf_nodes rf 100k leaf_nodes random_forest is well known as one of the best learning methods . in spite of its great success , it also has certain drawbacks : the heuristic learning rule does not effectively minimize the global training loss; the model size is usually too large for many real applications . to address the issues , we propose two techniques , global refinement and global pruning , to improve a pre-trained random_forest . the proposed global refinement jointly relearns the leaf_nodes of all trees under a global objective_function so that the complementary information between multiple trees is well exploited . in this way , the fitting power of the forest is significantly enhanced . the global pruning is developed to reduce the model size as well as the over-fitting risk . the refined model has better performance and smaller storage cost , as verified in extensive experiments . random_forest [2] is one of the most popular learning methods and has many ideal properties : 1 ) it is simple to understand and implement; 2 ) it is strong in handling non-linearity and outliers; 3 ) it is friendly to parallel training and large data; and 4 ) it is fast in testing . recently , it has proven extremely successful on important applications in data_mining [12] and computer_vision [6 , 11] . in spite of its great success , random_forest has certain insufficiency from both theoretical and practical viewpoints . theoretically , the heuristic learning of random forest is suboptimal in terms of minimizing training error . specifically , each individual tree is learnt independently and greedily . such learning does not fully utilize complementary information among different trees . practically , for complex real problems [3 , 4 , 6 , 11] , deep trees are usually required to fit the training data well . this results in high storage cost , which is a serious issue especially for embedded_devices such as mobile_phone or kinect . while tree pruning can reduce the tree size , existing_methods [7 , 8] are independently performed on individual trees and could degrade the performance of random_forest . to address the above problems , we propose a simple and effective method to refine a pre-trained random_forest . we notice that the learning and prediction of random_forest is inconsistent : the learning of individual trees is independent but the prediction averages all trees' outputs . the loss functions implied from these two processes are actually different . this limits the fitting power of random forest . to alleviate such inconsistency , we discard the old values stored in all tree leaves 
design and analysis of a head-mounted parallel kinematic device for skull surgery purpose precision skull surgery requires specialized instrumentation to satisfy demanding requirements in cochlear array implantation , deep_brain_stimulation electrode placement , and related applications . a miniaturized reconfigurable parallel kinematic mechanism which can be directly mounted on a patient's skull was designed , built , and tested for precision skull surgery . methods a stewart-gough platform is attached to a patient's skull so no optical tracking affecting the overall accuracy in keyhole surgery is required . six bone anchors comprising the mechanism base joints are implanted at positions with sufficient skull thickness . since no fixation frame is required , intervention planning flexibility is increased . the centers of the spherical shaped bone anchors can be localized accurately in the image space . an implicit registration to the physical space is performed by assembling the kinematics . registration error is minimized compared to common optical tracker-based_approaches . due to the reconfigurable mechanism , an optimization of the hexapod's configuration is needed to maximize accuracy and mechanical stability during the incision . mathematical simulation was conducted to estimate system performance . results simulation_results revealed significant improvement in accuracy and stability when exploiting the redundant degrees of freedom and the implemented reconfigurability . inaccurate localization of base points in the image data_set was identified as the main source of error . a first prototype with passive prismatic actuators , i . e . micrometer calipers , was successfully built . conclusions a head-mounted parallel kinematic device for high_precision skull surgery was developed that provides submillimetric accuracy in straight-line incisions . the system offers enhanced flexibility due to the absence of a rigid fixation frame . 
an efficient algorithm for calculating the worst-case delay due to crosstalk analyzing the effect of crosstalk on delay is critical for high_performance circuits . the major_bottleneck in performing crosstalk-induced delay analysis is the high computational cost of simulating the coupled interconnect and the nonlinear drivers . in this work , we propose an efficient iterative algorithm that avoids time-consuming nonlinear driver simulations and performs node-specific crosstalk delay analysis . the proposed algorithm has been tested over circuits in two deep_submicron_technologies with varying driver sizes , interconnect parasitics , signal transition times and it has been found to predict the worst-case delay to within 10 % of the actual delay . 1 introduction due to scaling in process_technology , coupling_capacitance has become dominant and crosstalk issues have become highly critical . crosstalk leads to two significant problems . firstly , coupling effects may inject noise into a circuit leading to discharge of the capacitance at the output of a gate , and thereby altering functionality . this effect has been extensively secondly , crosstalk-induced delay can critically affect circuit_performance . this problem is more serious and this paper is directed towards analyzing this effect . the coupling between interconnect lines makes it difficult to consider the effect of different aggressor drivers in an independent fashion . the traditional method of interconnect analysis that considers only one line at a time is no longer valid since the behavior of each line can depend on that of its transitive neighbors . this implies that either a large number of lines must be concurrently simulated , or that an intelligent iterative approach must be used , simulating only one line at a time . moreover , nonlinear driver simulation greatly increases the computation time needed for analysis in either of these scenarios . ignoring nonlinear drivers completely or modeling them with a simple linear resistance is generally known to give large errors . early approaches to incorporate the effects of coupling in calculating delays made use of a switch factor of [0 , 2] or [-1 , 3] for the coupling_capacitance [kah00] , [che00] , [sap00] . the worst-case bound has been found to predict overly pessimistic delay values for some cases and underestimate others [dar97] . the exact value of the switch factor is dependent on signal polarity , driver strengths , interconnect parasitics , slew rates and arrival times and cannot be determined a priori . switch factor methods do not consider many of these parameters and their accuracy for deep_submicron_designs is questionable . a relative window based_approach is proposed in [sas00] . a look-up table is used to capture 
two discourse driven language models for semantics natural_language_understanding often requires deep_semantic knowledge . expanding on previous proposals , we suggest that some important aspects of semantic knowledge can be modeled as a language_model if done at an appropriate level_of_abstraction . we develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities . for each model , we investigate four implementations : a " standard " n_gram language_model and three discriminatively trained " neural " language models that generate embeddings for semantic frames . the quality of the semantic language models ( semlm ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically we show that our semlm helps improve performance on semantic natural_language_processing tasks such as co-reference resolution and discourse parsing . 
assistance to planning in deep_brain_stimulation : data_fusion method for locating anatomical targets in mri . symptoms of parkinson's_disease can be relieved through deep_brain_stimulation . this neurosurgical technique relies on high_precision positioning of electrodes in specific areas of the basal_ganglia and the thalamus . in order to identify these anatomical targets , which are located deep within the brain , we developed a semi_automated method of image_analysis , based on data_fusion . information provided by both anatomical magnetic_resonance_images and expert knowledge is managed in a common possibilistic frame , using a fuzzy_logic approach . more specifically , a graph_based virtual atlas modeling theoretical anatomical knowledge is matched to the image_data from each patient , through a research algorithm ( or strategy ) which simultaneously computes an estimation of the location of every structures , thus assisting the neurosurgeon in defining the optimal target . the method was tested on 10 images , with promising_results . location and segmentation results were statistically assessed , opening perspectives for enhancements . 
fault_coverage analysis techniques of crosstalk in chip interconnects this paper addresses the problem of evaluating the effectiveness of test_sets to detect crosstalk_defects in system-level interconnects and buses of deep_submicron ( dsm ) chips . the fast_and_accurate estimation technique will enable : 1 ) evaluation of different existing tests , like functional , scan , logic built-in self-test ( bist ) , and delay_tests , for effective testing of crosstalk_defects in core-to-core interconnects and 2 ) development of crosstalk tests if the existing tests are not sufficient , thereby minimizing the cost of interconnect testing . based on a covering relationship we distinguish between transition tests in detecting crosstalk_defects and develop an abstract crosstalk_fault model for chip interconnects . with this fault_model and the covering relationship , we develop a fast and efficient method to estimate the fault coverage of any general test_set . we also develop a simulation_based technique to calculate the probability of occurrence of the defects corresponding to each fault , which enables the fault-coverage analysis technique to produce accurate estimates of the actual crosstalk_defect_coverage of a given test_set . the crosstalk test and fault properties , as well as the accuracy of the proposed crosstalk coverage analysis techniques , have been validated through extensive simulation experiments . the experiments also demonstrate that the proposed crosstalk techniques are orders_of_magnitude faster than the alternative method of spice-level simulation . finally , we demonstrate the practical applicability of the proposed fault_coverage analysis technique by using it to evaluate the crosstalk fault_coverage of logic_bist tests for the system-level interconnects and buses in a digital_signal_processor core . 
social_computing for educational knowledge_building research_and_development activities in social_computing , although still in exploratory stages , have already inspired a wide array of undertakings aimed at fostering the collective engagement of small groups and larger collectivities . social_computing research combines theory building and design work; experiments supporting small social groups or large online_communities create new interaction settings that allow testing and expanding theoretical perspectives . we are interested in harnessing social_computing for educational purposes , facilitating the building of knowledge by distributed small groups of students working collaboratively . we want to understand how knowledge_building social activity can be accomplished and how effective social_computing environments can be designed and deployed to promote collaborative_learning . our project , the virtual math teams ( vmt ) project at mathforum . org , is an attempt to develop a social_computing environment that promotes scaffolded discourse about mathematics among teens and to study the forms of collective interaction that take place there . the vmt project is based upon an evolving theory of group cognition . this theory hypothesizes that " small groups are the engines of knowledge_building . the knowing that groups build up in manifold forms is what becomes internalized by their members as individual learning and externalized in their communities as certifiable knowledge " ( stahl , 2006 , p . 16 ) . the vmt project is an ongoing effort to catalyze and promote a math discourse community . starting very simply in 2003 from a successful online math problem-of-the-week service ( mathforum . org/pow/ ) and taking advantage of popular off-the-shelf chat software to make it collaborative , we have since then gradually evolved a more sophisticated environment involving carefully scripted pedagogical interventions , open-ended math issues and custom software guided by extensive analysis of student behaviors through cycles of trials . we now want to strengthen its social_networking supports to facilitate wider adoption and to further our research agenda . while the ubiquity of networked computers connected through the internet from homes and schools creates an exciting opportunity for students around the world to explore math together , the practical difficulties are enormous . we are interested in facilitating the development of high_level thinking skills and the deep_understanding that comes from engaging in effective dialog and merging personal perspectives , but we find that students are accustomed to using text chat and the internet for superficial socializing . furthermore , their habits of learning are overwhelmingly skewed toward passive acquisition of knowledge from authority sources like teachers and books , rather than from self-regulated or collaborative inquiry . finally , attempts 
a new methodology for concurrent technology_development and cell_library optimization to minimize the time to market and cost of new sub 0 . 25um process_technologies and products , pdf solutions , inc . , has developed a new comprehensive approach based on the use of predictive simulation tools combined with highly efficient experimental design techniques and special test_structures . this paper focuses on our approach for concurrent development of new technologies and optimization of cell libraries for these technologies . we present a software system called circuit surfer which performs this library optimization in a highly automated fashion and with guaranteed correctness in silicon . we demonstrate several examples of circuit surfer applications to cell_library design to optimize such objective functions as performance , cell area or yield . 1 . introduction with each new generation of technology and products , semiconductor_manufacturing becomes more complex . the increase in ic functionality has been made possible by a continuous drive towards smaller feature_sizes . this decrease in dimensions of semiconductor structures has given rise to a new set of problems as manufacturing sensitivity to critical design and processing parameters has risen dramatically . while ic manufacturing becomes more complex , market windows for new products are shrinking . success in today's marketplace requires effective technology integration as dictated by consumer demand . against the backdrop of changing market conditions , the overall design_cycle time and yield_ramp have become the key drivers for product profitability . technology independent design_methodology , popularized by mead and conway[14] and used ever since to address the growing complexity problem , no longer applies to deep_submicron_designs . unfortunately , this failure is happening at a time when it is more crucial than ever to design products concurrently with new technology_development and its transfer to high volume manufacturing these changes require a redefinition of the interfaces between design , test and manufacturing . in the following section , we present a comprehensive view of the yield problem and a " holistic " yield improvement methodology specifically designed to overcome yield detrac-tors in state-of-the-art technologies . while elimination of systematic and design/process matching issues is critical to yield_ramp , and hence profitability , there exist organizational barriers that reinforce traditional_approaches . one barrier present in virtually all companies , even vertically_integrated manufacturers ( idms ) , exists between the design and manufacturing groups . 
educational_technology in introductory college physics teaching_and_learning : the importance of students perception and performance e ed du uc ca at ti io on na al l t te ec ch hn no ol lo og gy y i in n i in nt tr ro od du uc ct to or ry y c co ol ll le eg ge e p ph hy ys si ic cs s t te ea ac ch hi in ng g a an nd d l le ea ar rn ni in ng g : : t th he e i im mp po or rt ta an nc ce e o of f s st tu ud de en nt ts s' ' p pe er rc ce ep pt ti io on n a an nd d p pe er rf fo or rm ma an nc ce e abstract in this paper the researcher explored how introductory physics student perceptions about learning physics and their perspectives about physics instructors' presentational formats might be developed . within a constructivist framework , it is of fundamental importance that the educators understand and address student expectations about effective instructional methods and educational_technology-integrated curricula in particular . the researcher also investigated the likely impact of student expectations of learning_outcomes as part of the implications of improving the approach towards teaching . introduction physics is a science composed of well-founded expectations of how the natural world should behave , and it uses the tool of mathematics to describe these behaviors ( foster , 2000 ) . physics learning therefore involves observing phenomena , quantifying the observations , and synthesizing the results into theories ( williams , 1999 ) . the traditional approach to instruction is to teach physics through solving_problems . students of physics are expected to learn both the descriptive or conceptual side of physics and its predictive , problem_solving , and logical reasoning aspects . because learning logical thinking is difficult , many students try to memorize formulas and recall results from the homework problems at test time . hammer ( 1994 ) reported that many students_learn by rote because they have a naive conception of what it means to understand physics . formulas and equations are important for physics because physical quantities have to be calculated by using them . however , if students cannot understand the physics behind the formulas , they usually will not be able to solve the problem . elby ( 1999 ) focused on another cause of these study habits because many students believed that a deep understanding of physics is not necessary to obtain high grades in 
formal software analysis emerging trends in software model_checking formal software analysis emerging trends in software model_checking * 1995 through 2004 . he worked for six years as a senior engineer with intermetrics inc . developing compilers and software for safety_critical embedded_systems before returning to graduate school at the university of massachusetts at amherst where he completed his phd in 1995 . his interests cover a wide range of topics in software dependability including : specification methods , static_analysis and verification , run_time monitoring and testing . he is program co-chair of fase'07 and icse'08 and currently serves as secretary and treasurer of acm sigsoft . interests_include formal_methods , software specifications , model_checking , static_analysis , language-based security , and software_architecture . together with colleagues in the laboratory for static_analysis and transformation of software ( santos ) at kansas_state , he has developed several formal_analysis tools including the indus static_analysis and slicing tool and the bandera and bogor model_checking frameworks . his current efforts are focused on transferring these tools into industrial use in collaboration with engineers from lockheed_martin and rockwell_collins . he has served on a number of program committees for major meetings in the formal_analysis area including fse , popl , pldi , cav and tacas . where he completed his ph . d . in 2004 . he received the national_science_foundation faculty early career award in 2007 for his research on an integrated , formal_analysis framework for modular reasoning_about deep_semantic properties of open object_oriented systems . he leads the development of the bogor software model_checking framework , which have been supported by lockheed_martin and ibm . his collaborative_work on high-assurance multiagent_systems has been funded by the air_force office of scientific research . his broad interests_include software specification and verification using various forms of static and dynamic analyses . interests are in using abstraction and symbolic techniques as a base for efficient software verification . her other interests_include automated assume-guarantee style compositional verification and programming_language design . she received her phd degree from kansas_state_university , the department of computing and information sciences . she has served on program committees for many of the major meetings in the formal_analysis area including center . during his time at nasa he was one of the pioneers of the software model_checking field and the lead for the java pathfinder model_checker project . his interests_include advanced testing and static_analysis techniques using symbolic_execution . he received his phd from the university of manchester in 1998 . abstract the study of methodologies and techniques to produce correct software 
a usability study on the use of multi-context visualization this material is posted here with permission of the ieee . such ermission of the ieee does not in any way imply ieee endorsement of any of the university of technology , sydney's products or services . internal or personal use of this material is permitted . however , permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the ieee by writing to pubs-permissions@ieee . org . by choosing to view this document , you agree to all provisions of the copyright laws protecting it abstract graph visualization has been widely used in real_world_applications , as it provides better presentation of overall data_structure . however , there are navigation problems existing in deep and large relational datasets . to address these challenges , a new technique called multi-context visualization , which provides users with rich contextual information , has been proposed as the solution to the navigation in large_scale datasets . this paper evaluates the multi-context visualization by conducting an experiment-based user study . to answer whether the more contextual information positively assist in making more accurate and easier decisions , it aims to evaluate the effectiveness and efficiency of the multi-context visualization , by measuring the user performance . specifically , this usability test was designed to test if the use of multiple context views can improve navigation problems for deep and large relational data_sets . 
optimized hybrid verification of embedded_software the verification of embedded_software has become an important subject over the last years . however , neither stand-alone verification approaches , like simulation_based or formal_verification , nor state-of-the-art hybrid/semiformal verification approaches are able to verify large and complex embedded_software with hardware dependencies . this work presents an optimized scalable hybrid verification approach for the verification of embedded_software with hardware dependencies using a mixed bottom-up/top-down algorithm with optimized static parameter assignment ( spa ) . these algorithms and methodologies like spa and counterexample guided simulation are used to combine simulation_based and formal_verification in a new way . spa offers a way to interact between dynamic and static verification approaches based on an automated ranking heuristic of possible function parameters according to the impact on the model size . furthermore , spa inserts initialization code for specific function parameters into the source_code under test and supports model building and optimization algorithms to reduce the state space . we have successfully_applied this optimized hybrid verification methodology to an embedded_software application : motorola's powerstone benchmark suite . the results show that our approach scales better than stand-alone software model_checkers to reach deep_state spaces . i . introduction embedded_software ( esw ) is omnipresent in our daily life . it plays a key role in overcoming the time-to-market pressure and providing new functionalities . therefore , a high number of users are dependent on its functionality [1] . esw is often used in safety_critical applications ( automotive , medical , avionic ) where correctness is of fundamental importance . thus , verification and validation approaches are an important part of the development process . the most commonly used approaches to verify embedded_software are based on simulation or formal_verification ( fv ) approaches . testing , co-debugging and/or co-simulation techniques result in a tremendous effort to create test_vectors . furthermore , critical corner case scenarios might remain un-noticed . an extension of simulation is the assertion-based ver-ication ( abv ) methodology that captures a design's intended
special issue on recent_advances on brain-inspired innovative computing brain requires the most important scientific endeavor of the 21st century , and the international conference on neural information_processing ( iconip2007 ) looks for a clue to reveal and implement the brain functions with an integrated approach . to make an archive on the innovative results presented and discussed at iconip2007 , this special issue includes extended_versions of six selected papers on " recent_advances on brain-inspired innovative computing " from iconip2007 , held in kitakyushu , japan , 13-16 november , 2007 . the papers in this issue have been thoroughly reviewed and revised to give the readers a variety of recent findings on the brain-inspired innovative computing . 1 . introduction . this special issue is composed of six excellent papers which present new insight and models based on the brain_function and verify the effectiveness with a variety of interesting real problems . the first three papers mainly focus on the modeling part and tend to be rather theoretical , and the remaining three papers highlight the challenging applications of the new models , ranging from task segmentation in a mobile_robot to every day load forecasting for an electric power company . in the paper entitled bifurcation-based_model construction of a pyramidal_cell of the primary_visual_cortex , ishiki et al . proposed a physiologically plausible pyramidal_cell model of the primary_visual_cortex which contains many regulating systems of the intra-cellular calcium ions concentration , and shows the usefulness of the nonlinear dynamical system theory such as global bifurcation and slow/fast decomposition analyses for a construction of the neuron model . in the paper , classifying deep_brain neuronal activities by bursting parameters , chao et al . developed a method for classifying neuronal activities from the deep_brain nuclei , subthalamic_nucleus ( stn ) and subtantia nigra ( snr ) . analyzing 54 trials of data from parkinson's patients with seven bursting relevant parameters inducing a classifier of support_vector_machine with principal_component_analysis that improves the classification_accuracy for 22% on average . in the work novel models for hourly solar radiation using a 2-d approach , hocaoglu et al . analyzed one year hourly solar radiation data and modeled the general behavior of the solar radiation in a year using a 2-dimensional surface fitting approach . gaussian surface model with proper model parameters is found to be the most accurate model among the tested analytical models for data characterization . 
teaching patterns and software_design in this paper we describe our experiences with reengi-neering an undergraduate course in software_design . the course's learning_outcomes require that students can model , design and implement software . these are inherently practical skills and rely on functioning knowledge . to facilitate a learning_environment in which students can acquire the necessary deep_level of understanding , we have designed the course by applying the educational theory of constructive alignment and a number of proven techniques for teaching , learning , and assessment . fundamentally , we have embraced the active_learning paradigm that recog-nises that student activity is critical to the learning_process . in this paper , we describe several active_learning techniques that we have used including role play , problem_solving and peer learning . we also describe two novel assessment techniques we have developed , holistic assessment and formative examination . in addition we describe how students work with ju-nit , a popular unit_testing tool , not as users but as developers by applying design_patterns to extend it with new functionality . finally , we report on student assessment results and relay student feedback . 
filtering out deep_brain_stimulation artifacts using a nonlinear oscillatory model this letter is devoted to the suppression of spurious signals ( artifacts ) in records of neural activity during deep_brain_stimulation . an approach based on nonlinear adaptive model with self-oscillations is proposed . we developed an algorithm of adaptive filtering based on this approach . the proposed algorithm was tested using recordings collected from patients during the stimulation . this was then compared to existing_methods and showed the best performance . 
attribute based lattice rescoring in spontaneous speech_recognition in this paper we extend attribute-based lattice rescoring to spontaneous speech_recognition . this technique is based on two key_features : ( i ) an attribute-based frontend , which consists of a bank of speech attribute detectors followed up by an evidence merger that generates confidence scores ( e . g . , sub-word posterior probabilities ) , and ( ii ) a rescoring module that integrates information generated by the frontend into an existing asr engine through lattice rescoring . the speech attributes used in this work are phonetic features , such as frication and palatalization . experimental results on the switchboard part of the nist 2000 hub5 data_set demonstrate that the proposed approach outperforms lvcsr systems based on gaussian mixture_model/ hidden_markov_model ( gmm/hmm ) that does not use attribute related information . furthermore , a small yet promising improvement is also observed when rescoring word-lattices generated by a state-of-the-art asr system using deep neural networks . different frontend configuration are investigated and tested . 
neural_network_based symbol recognition using a few labeled samples the recognition of pen-based visual patterns such as sketched symbols is amenable to supervised machine_learning models such as neural_networks . however , a sizable , labeled training corpus is often required to learn the high variations of freehand sketches . to circumvent the costs associated with creating a large training corpus , improve the recognition accuracy with only a limited amount of training_samples and accelerate the development of sketch recognition_system for novel sketch domains , we present a neural network training protocol that consists of three steps . first , a large pool of unlabeled , synthetic samples are generated from a small set of existing , labeled training_samples . then , a deep_belief_network ( dbn ) is pre_trained with those synthetic , unlabeled samples . finally , the pre_trained dbn is fine-tuned using the limited amount of labeled samples for classification . the training protocol is evaluated against supervised baseline approaches such as the nearest neighbor classifier and the neural network classifier . the benchmark data_sets used are partitioned such that there are only a few labeled samples for training , yet a large number of labeled test_cases featuring rich variations . results_suggest that our training protocol leads to a significant error reduction compared to the baseline approaches . sketch understanding [1 , 2] aims to enable the computers to interpret man-made , freehand sketches and extract the intended information underlying the input strokes . fig . 1 shows two exemplary sketches depicting two engineering systems and the corresponding engineering model . if successful , sketch understanding could provide a natural human-computer interface for scenarios in which physical , pen-and-paper sketches have been routinely used , such as the early ideation process or the classroom instruction . moreover , sketch understanding could automate the mining , organization , search and critique of the information embedded in freehand sketches , potentially resulting in a myriad of intelligent_agents , such as a web spider that crawls through the drawings in online textbooks and lecture notes to learn the design rules of electrical systems , an archiver that indexes brainstorming sketches for later retrieval and reuse , and a computer grader for the free-body diagrams that students draw in their statics homework . one of the core problems in sketch understanding is to devise a symbol recognizer to compute a categorical label for each segment of the input sketch . used in conjunction with a sketch parser that divides the input sketch into segments and possibly a post-processor that ensures the consistency of the recognition , an interpretation of the input sketch can 
a built-in i ddq testing circuit * although i ddq testing has become a widely accepted defect_detection technique for cmos_ics , its effectiveness in very deep_submicron_technologies is threatened by the increased transistor leakage_current . in this paper , a built-in i ddq testing circuit is presented , that aims to extend the viability of i ddq testing in future technologies and first experimental_results are discussed . 
transparent-test_methodologies for random_access_memories without/with ecc an extended tanh law mosfet_model for high_temperature circuit_simulation , " j . alpha-power_law mosfet_model and its application to cmos inverter delay and other formulas , " j . experimental characterization and mod-eling of electron saturation velocity in mosfet's inversion layer from 90 to 350 k , " ieee electron device lett . a temperature dependent model for the saturation velocity in semiconductor materials , " mater . age current mechanisms and leakage reduction techniques in deep-submicrometer cmos_circuits , " paradigm of predictive mosfet and interconnect modeling for early circuit_design , " in [15] c . park et al . , " reversal of temperature_dependence of integrated_circuits operating at very low voltages , " in iedm tech . supply_voltage scaling for temperature insensitive cmos circuit operation , " ieee trans . design impact of positive temperature_dependence on drain current in sub-1-v cmos vlsis , " in analysis of substrate thermal gradient effects on optimal buffer_insertion , " in proc . iccad , 2001 , pp . 44 48 . [21] r . dennard et al . , " design of ion-implanted mosfets with very small physical dimension , " in j . full chip leakage estimation considering power_supply and temperature variations , " in proc . abstract this paper presents a systematic procedure for transforming a bit-oriented march test into a transparent word-oriented march test . the test-time complexity of the transparent word-oriented march tests converted by the proposed method is only ( p + 5 log 2 b + 2 ) n for an n b-bit random_access_memory , and the test-time complexity of the corresponding signature-prediction test is qn . here , p and q denote the number of total read/write and read test operations of the original bit-oriented march test . a transparent-test_methodology for memories with error-correction code ( ecc ) is also proposed . this methodology can test and locate faulty cells , and no signature prediction is needed . the test-time complexity of the proposed transparent-test_methodology for memories with ecc is only ( p + 5 log 2 b + 2 ) n . 
implementation and evaluation of mpi-based parallel md program the message_passing_interface ( mpi ) -based object_oriented particle particle interactions ( ppi ) library is implemented and evaluated . the library can be used in the n-particle simulation algorithm designed for a ring of p interconnected processors . the parallel simulation is scalable with the number of processors , and has the time requirement proportional to n 2 /p if n/p is large enough , which guarantees optimal speedup . in a certain range of problem_sizes , the speedup becomes superlinear because enough cache memory is available in the system . the library is used in a simple way by any potential user , even with no deep programming knowledge . different simulations using particles can be implemented on a wide spectrum of different computer platforms . the main purpose of this article is to test the ppi library on well-known methods , e . g . , the parallel molecular_dynamics ( md ) simulation of the monoatomic system by the second-order leapfrog verlet algorithm . the performances of the parallel simulation program implemented with the proposed library are competitive with a custom-designed simulation code . also , the implementation of the split integration symplectic method , based on the analytical calculation of the harmonic part of the particle interactions , is shown , and its expected performances are predicted . 
learning to remove multipath_distortions in time-of-flight range images for a robotic_arm setup range images captured by time-of-flight ( tof ) cameras are corrupted with multipath_distortions due to interaction between modulated light signals and scenes . the interaction is often complicated , which makes a model_based solution elusive . we propose a learning-based approach for removing the multipath_distortions for a tof camera in a robotic_arm setup . our approach is based on deep_learning . we use the robotic_arm to automatically collect a large amount of tof range images containing various multipath_distortions . the training images are automatically labeled by leveraging a high_precision structured light sensor available only in the training time . in the test time , we apply the learned model to remove the multipath_distortions . this allows our robotic_arm setup to enjoy the speed and compact form of the tof camera without compromising with its range measurement errors . we conduct extensive experimental validations and compare the proposed method to several baseline algorithms . the experiment results show that our method achieves 55% error reduction in range estimation and largely outperforms the baseline algorithms . 
mapping water_constituents in lake_constance using chris/proba chris-proba data were acquired at 3 days in 2003 and one day in 2004 at the eastern part of lake_constance . field campaigns were organised at these days in order to measure optical parameters of the water and the atmosphere and concentrations of water_constituents from ship for validation . five deep_water sampling stations were chosen . the modular inversion program ( mip ) developed by heege , miksa and kisselev was used for the coupled retrieval of water_constituents and aerosol concentrations , for atmospheric and water surface corrections and for optical closure and sensor calibration tests . different viewing angles of chris images of 2003 were used to test the atmospheric correction algorithm . spectra were calculated and adjusted to the measured ones by fitting the concentrations of 3 aerosol types ( rural , maritime , urban ) and 3 water_constituents ( suspended matter , chlorophyll , gelbstoff ) . the resulting mean relative error for suspended matter is 17% , for chlorophyll it is 28% which is within the error margin of the measurements . maps of chlorophyll , suspended matter and gelbstoff were generated for all viewing angles of chris/proba where sunglint contamination is absent . by comparing these maps the accuracies of the resulting water_constituents are determined . looking at one pixel from different angles , the water constituent concentration is supposed to be the same . the mean relative error for each sampling station is 37% for chlorophyll and 24% for suspended matter . a more detailed statistical_analysis is in preparation . 
chinese text_retrieval without using a dictionary it is generafly believed that words , rather than characters , should be the smallest indexing unit for chinese text_retrieval systems , and that it is essential to have a comprehensive chinese_dictionary or lexicon for chhmse text_retrieval systems to do well . chinese_text has no delimiters to mark woni boundaries . as a result , any text_retrieval systems that build word-based indexes need to segment text into words . we implemented several statistical and dictionary-hazed word_segmentation methods to study the effect on retrieval_effectiveness of different segmentation methods using the trec-s chinese test_collection and topics . the results show that , for all three sets of queries , the simple bigram indexing and the purely statistical word_segmentation perform better than the popular dictionary-based maximum matching method with a dictionary of 138 , 955 entries . 1 introduction the written_chinese text has no delimitem to mark word boundaries , it consists of a string of characters and punctuation . the first step toward word-based indexing is to break a sequence of characters into words . the process of breaking a string of character into words is called word_segmentation . word_segmentation is known to be a difficult task because accurate segmentation of written_chinese text may require deep analysis of the sentences . even chhese speakers may d~ree over how a sentence should be segmented because of the lack of a clear-cut definition on what constitutes a chinese word . some practical and popular word_segmentation methods use dictionaries ( lexicon ) , the simplest one being just a list of chinese words . the dictionary coverage of words can have a significant impact on the accuracy of word_segmentation . it is virtually impossible to list all the chinese words in a dictionary because the set of words is open-ended . the construction of a comprehensive dictionary is itself a difficult task . another group of word_segmentation methuds uses the lexical statistics of the chinese_characters in corpora to mark the word boundaries . the lexical statistics may include the occurrence frequency of a character in text corpora , and the co-occurrence frequency of two or more charectem in text corpora . what makes the statistical word_segmentation approaches appealing is that they do not require a comprehensive dictionary to mark word boundaries . it is generally believed that a comprehensive chinese_dictionary or lexicon is needed for a chinese_text retrieval_system to perform well . we want to know if chinese text_retrieval sys-permisaionto make digitallhardcopies of 
people detection through quantified fuzzy temporal rules the knowledge about the position and movement of people is of great importance in mobile_robotics for implementing tasks such as navigation , mapping , localization , or human robot_interaction . this knowledge enhances the robustness , reliability and performance of the robot control architecture . in this paper , a pattern classifier system for the detection of people using laser_range finders data is presented . the approach is based on the quantified fuzzy temporal rules ( qftrs ) knowledge_representation_and_reasoning paradigm , that is able to analyze the spatio-temporal patterns that are associated to people . the pattern classifier system is a knowledge_base made up of qftrs that were learned with an evolutionary_algorithm based on the cooperative-competitive approach together with token competition . a deep experimental study with a pioneer ii robot involving a five-fold_cross_validation and several runs of the genetic_algorithm has been done , showing a classification rate over 80% . moreover , the characteristics of the tests represent complex and realistic conditions ( people moving in groups , the robot moving in part of the experiments , and the existence of static and moving people ) . the operation of mobile_robots in real environments , like supermarkets , railway stations , hospitals , etc . , is generally characterized by the existence of people and moving_objects in the surrounding . this fact needs to be considered when implementing tasks such as mapping or path_planning , since discarding moving_objects usually leads to errors and poor performance . the detection of people is particularly important for service_robots and , fundamentally , for human robot_interaction , where both moving people and also static people have to be detected . the detection of people is highly influenced by the type of sensor being used . the two types of sensors most widely employed for this purpose are cameras [1 4] and range finders ( generally , laser_range finders ) [5 7] . the advantages of laser_range finders are that they can directly measure objects geometry , distances information is accurate , the field of view is large , and information of the probability of occupancy of each area of the environment can be easily obtained . on the contrary , the quantity of information that can be extracted is lower than with a camera and , therefore , distinguishing among objects with similar geometric properties becomes much more difficult . several proposals have been done for the detection of people with laser_range finders . they can be grouped into three categories : those that are based on the difference of occupancy between consecutive range scans [7 16] 
parallel_algorithms for neuronal spike_sorting state-of-the-art algorithms will be studied for sorting spike data recorded via tetrodes from living rats . parallel_algorithms will be implemented on state-of-the-art parallel cpu hardware in order to improve performance of existing algorithms , and increase the range of tractable algorithms . emphasis will be placed on tuning the algorithms to achieve maximum performance from the hardware platform , and grasping a deep understanding of programming practices needed to achieve high performance on parallel hardware . detailed analysis will be performed to understand the trade-offs and quality of the algorithms , with regards to clustering quality and execution time . the project should culminate in a working application with a graphical user interface which will be used to perform spike_sorting , and to experiment with the different implemented algorithms . abstract neurons communicate through electrophysiological signals , which may be recorded using electrodes inserted into living tissue . when a neuron emits a signal , it is referred to as a spike , and an electrode can detect these from multiple neurons . neuronal spike_sorting is the process of classifying the spike activity based on which neuron each spike signal is emitted from . advances in technology have introduced better recording equipment , which allows the recording of many neurons at the same time . however , clustering software is lagging behind . currently , spike_sorting is often performed semi-manually by experts , with computer assistance , in a drastically reduced feature_space . this makes the clustering prone to subjectivity . automating the process will make classification much more efficient , and may produce better results . implementing accurate and efficient spike_sorting algorithms is therefore increasingly_important . we have developed parallel implementations of superparamagnetic clustering , a novel clustering algorithm , as well as k_means_clustering , serving as a useful comparison . several feature_extraction methods have been implemented to test various input distributions with the clustering algorithms . to assess the quality of the results from the algorithms , we have also implemented different cluster quality algorithms . our implementations have been benchmarked , and found to scale well both with increased problem_sizes and when run on multi_core processors . the results from our cluster quality measurements are inconclusive , and we identify this as a problem related to the subjectivity in the manually classified datasets . to better assess the utility of the algorithms , comparisons with intracellular recordings should be performed . iii iv acknowledgements this thesis was written during spring 2011 , as part of the course tdt4900 computer and information_science , master thesis . the project 
coma-boost : co-operative multi_agent adaboost multi feature_space representation is a common practise in computer vision applications . traditional features such as hog , sift , surf etc . , individually encapsulates certain discriminative cues for visual classification . on the other hand , each layer of a deep neural network generates multi ordered representations . in this paper we present a novel approach for such multi feature representation learning using adaptive boosting ( adaboost ) . general practise in adaboost [8] is to concatenate components of feature spaces and train base learners to classify examples as correctly/incorrectly classified . we posit that multi feature_space learning should be viewed as a derivative of cooperative multi_agent learning . to this end , we propose a mathematical framework to leverage performance of base learners over each feature_space , gauge a measure of "difficulty" of training space and finally make soft weight updates rather than strict binary weight updates prevalent in regular adaboost . this is made possible by periodically sharing of response states by our learner agents in the boosting framework . theoretically , such soft weight update policy allows infinite combinations of weight updates on training space compared to only two possibilities in adaboost . this opens up the opportunity to identify 'more difficult' examples compared to 'less difficult' examples . we test our model on traditional multi feature representation of mnist handwritten_character dataset and 100-leaves classification challenge . we consistently outperform traditional and variants of multi view boosting in terms of accuracy while margin analysis reveals that proposed_method fosters formation of more confident ensemble of learner agents . as an application of using our model in conjecture with deep_neural_network , we test our model on the challenging task of retinal blood_vessel segmentation from fundus images of drive dataset by using kernel dictionaries from layers of unsupervised trained stacked autoencoder network . our work opens a new avenue of research for combining a popular statistical machine_learning paradigm with deep network architectures . 
editorial : alan_turing and artiicial intelligence his turing's] point was that we should not be species-chauvinistic , or anthropocentric , about the insides of an intelligent being , for there might be inhuman ways of being intelligent . {daniel c . dennett alan mathison turing ( 23 june 1912{7 june 1954 ) was one of the most eminent scientists of the 20th century ( figure 1 ) . his research was a central catalyst of the computer revolution . the concept of a turing_machine , which he developed in the 1930s , is still one of the most widely used models of computation in theoretical computer_science , but this monumental contribution was only the rst of many . and maintainer of the \alan_turing home_page" 1 | divides turing's publications into ve areas : mathematical_logic , mechanical intelligence , pure_mathematics , morphogenesis , and crypt-analysis . moreover , in sir roger penrose's words , turing was also \a deep and innuential philosopher in addition to his having made contributions to mathematics , technology and code-breaking that profoundly contribute to our present-day well being" ( a . hodges , 1998 ) . in a landmark article published in october 1950 in the philosophy journal mind ( figure 2 ) turing made a famous assertion . he predicted that by the year 2000 it would be feasible to write a program that would , after ve minutes of questioning , have at least a 30% chance of fooling an average conversational partner into believing it was a human being ( turing , 1950 ) . 2 as charniak and mcdermott ( 1985 , p . 10 ) remark \actually , the mind] paper makes it sound as if turing had in mind the computer pretending to be a woman in the man/woman game , but the point is not completely clear , and most have assumed that he intended the test to be a person/computer one , and not woman/computer . " see ( saygin et al . , 1999 ) for an attempt at clariication . 
a new maximal diagnosis algorithm for bus-structured systems complex interconnects in highly integrated system chips are implemented with the bus structure . from testing point_of_view , the bus structure system needs more complicated consideration than simple wiring networks since a bus line is received data from many drivers . therefore , some faults are detected all the time and others are detected only at the particular time . we propose a new interconnect_test algorithm for the bus structure . the md+ algorithm supports maximal diagnosis for the bus-structured system and its test period is shorter than the previous algorithms . moreover , the md+ algorithm is easy to apply since it is based on the complete diagnosis algorithm for wiring networks . the effectiveness of the md+ algorithm is confirmed by comparing the test length with previous bus based interconnect test algorithms . 1 . introduction deep_submicron_technology makes it possible to integrate a system in a single chip , called soc ( system on a chip ) . the various modules are integrated in a chip and their complex interconnects are implemented with the bus-structured system . as a point_of_view for soc testing , the defects on interconnects are shown as defects of a system because the interconnect_test is only done through the i/o pins of the soc . as ieee 1149 . 1[1] supports the testing environments for the board level test , ieee p1500 does the same role for soc . the interconnect_test procedure is executed by applying various interconnect test algorithms through p1500 interface . many researches [2]-[12] have been made for the diagnosis of interconnect . they assume the three general fault_models named stuck_at_fault , stuck-open fault , and short fault . they assume the multiple faults on a net but it does not mean the complete diagnosis . it's because the complete diagnosis is physically impossible , so we focus on the maximal diagnosis . most of the previous
a rule-based question_answering system for reading_comprehension_tests we have developed a rule-based system , quarc , that can reada short_story and find the sentence in the story that best answers a given question . quarc uses heuristic rules that look for lexical and semantic clues in the question and the story . we have tested quarc on reading_comprehension_tests typically given to children in grades 3-6 . overall , quarc found the correct sentence 40% of the time , which is encouraging given the simplicity of its rules . 1 introduction in the united_states , we evaluate the reading ability of children by giving them reading_comprehension_tests . these test typically consist of a short_story followed by questions . presumably , the tests are designed so that the reader must understand important aspects of the story to answer the questions correctly . for this reason , we believe that reading_comprehension_tests can be a valuable tool to assess the state of the art in natural_language_understanding . these tests are especially challenging because they can discuss virtually any topic . consequently , broad-coverage natural_language_processing ( nlp ) techniques must be used . but the reading_comprehension_tests also require semantic understanding , which is difficult to achieve with broad-coverage techniques . we have developed a system called quarc that "takes" reading_comprehension_tests . given a story and a question , quarc finds the sentence in the story that best answers the question . quarc does not use deep language_understanding or sophisticated techniques , yet it achieved 40% accuracy in our experiments . quarc uses hand-crafted heuristic rules that look for lexical and semantic clues in the question and the story . in the next section , we de
improving structural testing of object_oriented_programs via integrating evolutionary_testing and symbolic_execution achieving_high structural coverage such as branch_coverage in object_oriented_programs is an important and yet challenging goal due to two main challenges . first , some branches involve complex program logics and generating tests to cover them requires deep knowledge of the program structure and semantics . second , covering some branches requires special method sequences to lead the receiver object or non-primitive arguments to specific desirable states . previous work has developed the symbolic_execution technique and the evolutionary_testing technique to address these two challenges , respectively . however , neither technique was designed to address both challenges at the same time . to address the respective weaknesses of these two previous techniques , we propose a novel framework called evacon that integrates evolutionary_testing ( used to search for desirable method sequences ) and symbolic_execution ( used to generate desirable method arguments ) . we have implemented our framework and applied it to test 13 classes previously used in evaluating white-box test_generation tools . the experimental results show that the tests generated using our framework can achieve higher branch_coverage than the ones generated by evolutionary_testing , symbolic_execution , or random_testing within the same amount of time . 
efficacy of a new charge-balanced biphasic electrical stimulus in the isolated sciatic_nerve and the hippocampal slice most deep_brain stimulators apply rectangular monophasic voltage pulses . by modifying the stimulus shape , it is possible to optimize stimulus efficacy and find the best compromise between clinical effect , minimal side effects and power_consumption of the stimulus generator . in this study , we compared the efficacy of three types of charge-balanced biphasic pulses ( cbbps , nominal duration 100 s ) in isolated sciatic nerves and in in vitro hippocampal brain slices of the rat . using these two models , we tested the efficacy of several stimulus shapes exclusively on axons ( in the sciatic_nerve ) and compared the effect with that of stimuli in the more complex neuronal network of the hippocampal slice by considering the stimulus-response relation . we showed that ( i ) adding an interphase gap ( ipg , range 100-500 s ) to the cbbp enhances stimulus efficacy in the rat sciatic_nerve and ( ii ) that this type of stimuli ( cbbp with ipg ) is also more effective in hippocampal slices . this benefit was similar for both models of voltage and current stimulation . in our two models , asymmetric cbbps were less beneficial . therefore , cbbps with ipg appear to be well suited for application to dbs , since they enhance efficacy , extend battery life and potentially reduce harmful side effects . 
mirdeep-p : a computational tool for analyzing the microrna transcriptome in plants motivation ultra-deep sampling of small_rna libraries by next_generation sequencing has provided rich information on the microrna ( mirna ) transcriptome of various plant species . however , few computational tools have been developed to effectively deconvolute the complex information . results we sought to employ the signature distribution of small_rna reads along the mirna precursor as a model in plants to profile expression of known mirna genes and to identify novel ones . a freely available package , mirdeep-p , was developed by modifying mirdeep , which is based on a probabilistic model of mirna biogenesis in animals , with a plant-specific scoring system and filtering criteria . we have tested mirdeep-p on eight small_rna libraries derived from three plants . our results_demonstrate mirdeep-p as an effective and easy-to-use tool for characterizing the mirna transcriptome in plants . availability http : //faculty . virginia . edu/lilab/mirdp/ contact : ll4jn@virginia . edu supplementary information supplementary data are available at bioinformatics online . 
fast simulated diffusion : an optimization algorithm for multiminimum problems and its application to mosfet_model parameter extraction a new algorithm , namely a fast simulated diffusion ( fsd ) , is proposed to solve a multiminimal optimization_problem on multidimensional continuous space . the algorithm performs a greedy search and a random search alternately and can find the global minimum with a practical success rate . a new , efficient hill-decending method employed as the greedy search in the fsd is proposed . when the fsd is applied to a set of standard test functions , it shows an order_of_magnitude faster speed than the conventional simulated diffusion . some of the optimization_problems encountered in system and vlsi_designs are classified into multioptimal problems . a mosfet parameter extraction problem is one of them and the proposed fsd is successfully_applied to the problem with a deep_submicron mosfet . 
an old friend revisited : countable models of _stable_theories we work in the context of _stable_theories . we obtain a natural , algebraic equivalent of eni-ndop and discuss recent joint proofs with s . shelah that if an _stable theory has either eni-dop or is eni-ndop and is eni-deep , then the set of models of t with universe is borel complete . in 1983 shelah , harrington , and makkai proved vaught's conjecture for _stable_theories [11] . in that paper they determined which _stable_theories have fewer than 2 0 countable models and proved a strong structure theorem for models of such a theory . as in most verifications of vaught's conjecture for specific classes , little attention was paid to countable models of _stable_theories have 'many' models . it is curious that following the publication of [11] in 1984 , the investigation of the class of countable models of an arbitrary _stable theory lay fallow for many years . 1 one explanation for this hiatus may have been a lack of test questions . how could one describe the complexity of a class of countable structures beyond asserting that there are 2 0 nonisomorphic ones ? a remedy was provided by the collective 1 we understand that recently martin koerwien has been working independently on similar problems . 
deep_sub_micron sram design for ultra_low leakage standby operation deep_sub_micron sram design for ultra_low leakage standby operation contents permission to make digital or hard_copies_of all or part of this work for personal or classroom use is granted_without_fee_provided that copies are not made or distributed for profit_or_commercial_advantage_and that copies_bear this notice and the full citation on the first page . to copy otherwise , to republish , to post on servers or to redistribute to lists , requires_prior_specific_permission . suppressing the standby current in memories is critical in low_power_design . by lowering the supply_voltage ( v_dd ) to its standby limit , the data_retention voltage ( drv ) , sram leakage_power can be reduced substantially . the drv theoretical limit is derived to be 52mv for a 90nm technology at room_temperature . the drv increases with transistor mismatches . based on sub-threshold circuit analysis , a practical drv model is developed and verified with measurement data from several test chips in 130nm and 90nm technologies . by reducing the standby v_dd of a 32k-bit 130nm industrial ip sram module to 490 mv ( 390 mv worst_case drv + 100 mv electrical-noise guard-band ) , an 85% leakage power_saving is measured , compared to the standby_power at 1v . since the drv is a strong function of both process and design parameters , the sram cell can be optimized to reduce drv . it is shown that the body bias and device channel length are the most effective knobs in minimizing drv . this is confirmed with measurement data from a 90nm sram test_chip . building on these results , feasibility of a 270mv standby v_dd is demonstrated for an optimized 4k-bit sram in a 90nm technology , leading to a 97% leakage_power reduction . by dynamically configuring the body bias during read and write operations , the active operation noise_margins and data access speed are also improved according to simulation_results . correcting the low_voltage retention errors with error correction code ( ecc ) provides another opportunity to further reduce the sram standby v_dd . to establish a power-per-bit metric , the sram leakage_power is modeled as a function of the ecc parameters , drv distribution and the standby v_dd . this power metric is optimized based on ecc theory to obtain fundamental bounds of the power_saving enabled by error-tolerant design . taking into account the practical design requirements , an error-tolerant sram design with a ( 31 , 26 ) hamming_code is proposed introducing a further power_reduction of 33% . both the circuit optimization and the error-tolerant architecture are 
high_resolution hurricane forecasts problem overview widely varying scales of atmospheric motion make it extremely difficult to predict hurricane intensity , even after decades of research . a new model capable of resolving a hurricane's deep convection motions was tested on a large sample of atlantic tropical_cyclones . results show that using finer resolution can improve storm intensity predictions . p redicting a hurricane's intensity remains a daunting challenge even after four decades of research . the intrinsic difficulties lie in the vast range of spatial and temporal scales of atmospheric motions that affect tropical_cyclone intensity . the range of spatial scales is literally millimeters to 1 , 000 kilometers or more . atmospheric dynamical models must account for all these scales simultaneously . being a non-linear system , these scales can interact . although forecasters must make approximations to keep computations finite , there's a continued push for finer resolution to capture as many of these scales as possible . from the present standpoint of computational feasibility , a model's minimum grid lengths that still allow modeling of the storm and its near environment are a few hundred horizontal meters and approximately 50 to 100 vertical meters . most current weather prediction uses grid-based rather than spectral-based models ( such as fourier or some other basis_function ) . 1 statistical_analysis of energy spectra reveal that motions with scales smaller than approximately six to seven grid points aren't well resolved . 2 therefore , the minimum resolvable physical length scales are nearly 1 km horizontally and perhaps 300 m vertically . given current computing capability , however , timely numerical forecasts must be run on much coarser grids . what does this mean for hurricane forecasts ? we believe that it's important to resolve clouds at least the largest cumulonimbus-producing thunderstorms . these clouds have a horizontal scale of at most a few kilometers and thus can be resolved only with a 1 km or less horizontal grid spacing . these clouds span the troposphere's vertical extent 12 to 16 km and so are relatively easy to resolve in the vertical if 30 to 40 layers are used . nevertheless , to cover the region affected by a hurricane in a five-day forecast requires a horizontal domain of perhaps 5 , 000 km . a volume with a grid increment of 500 m horizontally and 250 m vertically over a domain of depth 25 km contains roughly 10 10 grid points . because the equations of motion are first-order in time , knowing the model state at one time lets us , in principle , predict the state a short time 
extending the embedded system e-tddunit test_driven_development tool for development of a real time video security system prototype despite the existence of 75 " different " xunit frameworks , their domain of application differs only in the programming_language , compiler or operating system supported . if one is working in the embedded world , unit_testing is still needed , but now our " testing requirements " differ significantly from the testing framework needed for the desktop world . embedded_systems often have significant non-functional_requirements , which demand validation at the unit level . in addition , they interact intimately with hardware resources and often have only very limited input/output capabilities imagine a xunit framework where printing to the screen is a technical challenge ! there have been a number of notable efforts in migrating agile ideas into the embedded environment but only one or two intrepid practitioners have braved this new domain deep down towards and into the " plumbing " layer of small embedded_systems . the purpose of this abstract is to demonstrate extensions of an embedded system test_driven_development tool ( e-tddunit [1] ) to permit the development of a real-time security system prototype ( fig . 1 ) around an analog_devices adsp-bf533 blackfin processor ez-kit lite evaluation board . this initial solution and tests were successfully ported to a newly available bf537 system ( with both video and internet connection ) ; demonstrating the practicality of the approach . e-tddunit is a custom-ized cppunitlite version [2] adopted and modified so that the tests could run on the real embedded system where the timing relationships were not just seen through a simulated environment . the code development for this project can be recognized as having two distinct stages , commonly found in embedded applications involving video and telecommunications . an attempt to run the same set of e-tddunit tests for ( 1 ) the double_precision floating_point matlab prototyping phase ( develop and test signal_processing algorithms ) and ( 2 ) a code migration phase , where the algorithm implementation must meet the time and precision requirements of running on a fixed_point processor , was abandoned because of the impractical c++ / matlab interface . 
fast simulation_based testing of anti-tearing mechanisms for small embedded_systems small embedded_systems are often powered by unreliable power supplies like energy_harvesting systems ( e . g . , for sensor nodes ) or external power supplies ( for smart_cards ) . for secure embedded_systems a sudden loss of power can violate data_integrity . the power has just to drop when data is written to non_volatile_memory . thinking about a byte array in a smart_card representing some digital money of an e-purse , this becomes obvious . in order to guarantee data_integrity a secure embedded system has to provide an anti-tearing mechanism . testing this mechanism is very difficult , extensive , and requires deep_inside knowledge into its implementation_details . in this work we show how a simulation of an embedded system can be used to test the anti-tearing mechanism . high_level test_cases are used to generate test_vectors automatically . the proposed approach allows a fast and comprehensive test of the anti-tearing mechanism . we 1 explain our proposed mechanism on the basis of a case study of a smart_card system . however , the mechanism is general enough to be used for secure embedded_systems of any kind . 
characterization of optical_interconnects characterization of optical_interconnects title : associate professor of electrical_engineering and computer sciences interconnect has become a major issue in deep_sub_micron_technology . even with copper and low-k dielectrics , parasitic eeects of interconnects will eventually impede advances in integrated electronics . one technique that has the potential to provide a paradigm_shift is optics . this project evaluates the feasibility of optical_interconnects for distributing data and clock signals . in adopting this scheme , variation is introduced by the detector , the waveguides , and the optoelectronic circuit , which includes device , power_supply and temperature variations . we attempt to characterize the eeects of the aforementioned sources of variation by designing a baseline optoelectronic circuitry and fabricating a test chip which consists of the circuitry and detectors . simulations are also performed to supplement the eeort . the results are compared with the performance of traditional metal interconnects . the feasibility of optical_interconnects is found to besensitive to the optoelectronic circuitry used . variation eeects from the devices and operating_conditions have profound impact on the performance of optical_interconnects since they introduce substantial skew and delay in the otherwise ideal system . 2 3 acknowledgments i would like to thank professor boning and professor chandrakasan for their time and eeort in guiding and motivating this work . their encouragement and trust were crucial to the completion of the project . i w ould also like to thank kush for his willingness and sellessness in providing insights and enlightening suggestions whenever i encountered problems along the way . i am grateful to desmond and andy for imparting their knowledge of device physics and for assisting in the design and testing of the detector . in addition , i want to thank paul-peter for his assistance in building the prototype . heartfelt appreciation is extended to jim and illiana for oering invaluable help during the layout of the chip . i w ant to thank rong-wei for his constant_support , be it emotional or technical , and for his belief in my ability , m o r e s o than myself . lastly , i would like to thank my family for their love , support , and sacriices . my father , for ooering insights and inspirationss my mother , for the loving care and gentle encouragementss my little brother , for the mysteriously timed phone calls and laughter that we share . 
a 2d unknown contour recovery method immune to system non-linear effects a method to recover general 2d a priori unknown contours using a kind of special optic sensor is described . contour recovery is an important task for exploratory operations in unknown environments as well as for more practical applications such as grinding or deburring . it is not an easy task since the recovered contour ( generally obtained using encoder data ) is severely distorted due to errors in the kinematic model of the robot and to the non-linearities of its actuators . some mathematical_models have been presented to partially compensate for those effects , but they require a deep knowledge of both the robot and sensor models which are difficult to obtain accurately , and normally imply an adaptive non-linear control to estimate some of the unknown parameters of the model . our approach , in despite of its simplicity , is intrinsically immune to non-linearities , which allows us to eliminate most of the distortions added to the sensor data . a simple algorithm to follow unknown planar contours is presented and used to test the performance of this approach in comparison to the one using encoder data . experimental_results and practical problems are also discussed . 
a method-based ahead-of-time compiler for android applications the execution environment of android system is based on a virtual_machine called dalvik virtual_machine ( dvm ) in which the execution of an application program is in interpret-mode . to reduce the interpretation overhead of dvm , google has included a trace-based just-in-time compiler ( jitc ) in the latest version of android . due to limited resources and the requirement for reasonable response time , the jitc is unable to apply deep optimizations to generate high_quality code . in this paper , we propose a method-based ahead-of-time compiler ( aotc ) , called icing , to speed up the execution of android applications without the modification of any components of android framework . the main idea of icing is to convert the hot methods of an application program from dex code to c code and uses the gcc compiler to translate the c code to the corresponding native code . with the java_native_interface ( jni ) library , the translated native code can be called by dvm . both aotc and jitc have their strength and weakness . in order to combine the strength and avoid the weakness of aotc and jitc , in icing , we have proposed a cost model to determine whether a method should be handled by aotc or jitc during profiling . to evaluate the performance of icing , four benchmarks used by google jitc are used as test_cases . the performance results show that , with icing , the execution time of an application is two to three times faster than that without jitc , and 25% to 110% faster than that with jitc . 
effects of sequential and discrete rapid_naming on reading in japanese children with reading difficulty . to clarify whether rapid_naming ability itself is a main underpinning factor of rapid automatized naming tests ( ran ) and how deep an influence the discrete decoding process has on reading , we performed discrete_naming tasks and discrete hiragana reading tasks as well as sequential naming tasks and sequential hiragana reading tasks with 38 japanese schoolchildren with reading difficulty . there were high correlations between both discrete and sequential hiragana reading and sentence reading , suggesting that some mechanism which automatizes hiragana reading makes sentence reading fluent . in object and color tasks , there were moderate correlations between sentence reading and sequential naming , and between sequential naming and discrete naming . but no correlation was found between reading tasks and discrete naming tasks . the influence of rapid_naming ability of objects and colors upon reading seemed relatively small , and multi-item processing may work in relation to these . in contrast , in the digit naming task there was moderate correlation between sentence reading and discrete naming , while no correlation was seen between sequential naming and discrete naming . there was moderate correlation between reading tasks and sequential digit naming tasks . digit rapid_naming ability has more direct effect on reading while its effect on ran is relatively limited . the ratio of how rapid_naming ability influences ran and reading seems to vary according to kind of the stimuli used . an assumption about components in ran which influence reading is discussed in the context of both sequential processing and discrete naming speed . 
deep_neural_networks segment neuronal membranes in electron_microscopy images we address a central problem of neuroanatomy , namely , the automatic segmen-tation of neuronal structures depicted in stacks of electron_microscopy ( em ) images . this is necessary to efficiently map 3d brain structure and connectivity . to segment biological neuron membranes , we use a special type of deep artificial_neural_network as a pixel classifier . the label of each pixel ( membrane or non-membrane ) is predicted from raw pixel values in a square window centered on it . the input layer maps each window pixel to a neuron . it is followed by a succession of convolutional and max_pooling layers which preserve 2d information and extract features with increasing levels of abstraction . the output layer produces a calibrated probability for each class . the classifier is trained by plain gradient_descent on a 512 512 30 stack with known ground_truth , and tested on a stack of the same size ( ground_truth unknown to the authors ) by the organizers of the isbi 2012 em segmentation challenge . even without problem-specific post_processing , our approach outperforms competing techniques by a large margin in all three considered metrics , i . e . rand error , warping error and pixel error . for pixel error , our approach is the only one outperforming a second human observer . 
early voltage and saturation voltage improvement in deep_sub_micron_technologies using associations of transistors the design of analog integrated_circuits together with mixed_signal applications in deep sub-micron technologies is a difficult task , since state-of-the-art technologies and minimum channel length transistors , suitable for digital_circuits , are very rarely optimized for analog block_design . non-desired effects are present shortest transistors , leading mainly to a high output conductance , which is disadvantageous for gain in the stages . in this work , we present measurement results supporting the associations of transistors concept to be used in such applications : the t-shaped transistor ( tst ) . the main characteristic of this association is its trapezoidal nature , with no limit on the sizes of the unit composite transistors , providing lower output conductance and saturation voltage in comparison to regular configurations . such electrical characteristics are demonstrated by means of electrical simulations and electrical measurements of a test chip fabricated by mosis in an ibm 0 . 18&#956;m cmos_process . 
modeling freeway lane_changing behavior modeling freeway lane_changing behavior drivers continuously evaluate the surrounding traffic and the roadway environment , and make decisions about lanes and travel speed . the objective of this thesis is to develop a lane_changing model that can be used in microscopic traffic simulation models to capture drivers' lane_changing behavior . lane_change is modeled as a sequence of four steps : decision to consider a lane_change , choice of left or right lane , search for an acceptable gap to execute the decision , and performing the lane_change maneuver . first , a decision is made whether a driver will consider changing_lanes . if a decision to consider changing_lanes is made , a lane is chosen from the alternatives . finally , the gap acceptance model determines whether the available gap in the target lane is sufficient for a safe merging and lane_change can be completed . a discrete choice framework is used to model the lane_changing behavior . the framework allows for modeling the impact of different elements of the traffic and roadway environment on driver behavior . the model is applied in the special case of merging from an on-ramp . results from the estimation of the parameters show that in addition to the gap length , other important factors that affect drivers gap acceptance behavior are relative speed , distance remaining to the point at which lane_change must be complete , and delay in completing merging . finally , the estimated model is tested in a micro-simulation environment . acknowledgement i acknowledge with deep gratitude the guidance and constant inspiration provided by my was a privilege to work with them . i have learned a lot from them during the course of this research . my special thanks goes to qi yang-an extremely helpful and always encouraging friend . sometimes he came out of his way to help me that made me feel guilty lot of times . i wish him success in life . grateful to the center for transportation studies at mit for awarding me admission with financial aid without which mit would have been a dream place to me . i would like to thank all my friends that made my stay at mit an enjoyable experience department for the friendship and their support . and finally , i wish i knew the exact words to express my indebtedness to lubna , my wife , for her constant_support , care , and endless love , to my parents for having faith in me and their love , encouragement , and constant inspiration that helped me outgrow 
multiple-description wavelet based image_coding we consider the problem of coding images for transmission over error-prone channels . the impairments we target are transient channel shutdowns , as would occur in a packet network when a packet is lost , or in a wireless system during a deep fade : when data is delivered it is assumed to be error-free , but some of the data may never reach the receiver . the proposed algorithms are based on a combination of multiple description scalar quantizers with techniques successfully_applied to the construction of some of the most efficient subband coders . a given image is encoded into multiple independent packets of roughly equal length . when packets are lost , the quality of the approximation computed at the receiver depends only on the number of packets received , but does not depend on exactly which packets are actually received . when compared with previously reported results on the performance of robust image coders based on multiple descriptions , on standard test images , our coders attain similar psnr values using typically about 50-60% of the bit_rate required by these other state-of-the-art coders , while at the same time providing significantly more freedom in the mechanism for allocation of redundancy among descriptions . 
a parallel circuit-partitioned algorithm for timing driven cell placement simulated_annealing based standard_cell placement for vlsi_designs has long been acknowledged as a compute-intensive process . all previous work in parallel simulated_annealing based placement has minimized area , but with deep_submicron design , minimizing wirelength delay is also needed . the algorithm discussed in this paper is the first parallel_algorithm for timing driven placement . we have used a very accurate elmore delay model which is more compute intensive and hence the need for parallel placement is more apparent . parallel placement is also needed for very large circuits that may not fit in the memory of a single processor . therefore , our algorithm is circuit partitioned and can handle arbitrary large circuits on distributed_memory multiprocessors . the algorithm , called mpiplace , has been tested on several large benchmarks on a variety of parallel architectures . 
edge_detection of noisy images based on cellular neural_networks this paper studies a technique employing both cellular neural_networks ( cnns ) and linear matrix inequality ( lmi ) for edge_detection of noisy images . our main work focuses on training templates of noise_reduction and edge_detection cnns . based on the lyapunov_stability theorem , we derive a criterion for global asymptotical stability of a unique equilibrium of the noise_reduction cnn . then we design an approach to train edge_detection templates , and this approach can detect the edge precisely and efficiently , i . e . , by only one iteration . finally , we illustrate performance of the proposed methodology from the aspect of peak signal_to_noise_ratio ( psnr ) through computer simulations . moreover , some comparisons are also given to prove that our method outperforms classical operators in gray image edge_detection . it is well known that the hopfield neural_network ( hnn ) requires fully_connected and grows exponentially with the size of the array . thus it is very difficult to implement , even in modest array sizes , as vsli circuits [1 , 2] . a novel class of information_processing system called cellular neural_network ( cnn ) was proposed by chua and yang in 1988 , which came from the hnn and cellular_automata as an effective combination of both characteristics [3 , 4] . moreover , the cnn has two prominent features : real-time signal_processing capability and local connection . on one hand , the characteristic of real-time signal_processing has been extensively exploited in various applications such as parallel signal_processing , image edge_detection , connected component detection and various morphology operations ( dilation , erosion and hole filling , etc . ) . on the other hand , the characteristic of local connection makes it applicable to vlsi implementation and allows to operate at a very high_speed in real time . with deep_submicron_technology ( 0 . 25 um-0 . 33 um ) , an array of 100 100 large analog processors array can be implemented on a single chip , whose theoretical computation speed can be at least a thousand times faster than the current digital processor [5] . some smaller operational test chips have also been designed [6 8] . as a result of this rapid development , the cnns have been widely studied for practical applications in image and video signal_processing , robotic and biological visions and higher brain functions [9 12] . the most important key point of cnns applications is how to find the satisfactory feedback template ''a'' , control template ''b'' and bias ''i'' . in recent years , the problem of cnn design for image_processing has attracted considerable attention [13 20] and the 
pattern_matching using layered strifa for intrusion_detection with the advent and explosive growth of the global internet adaptive/automatic network intrusion and anomaly_detection in wide area data networks is fast gaining critical research and practical importance . in order to detect intrusions in a network , need efficient ids . deep_packet_inspection ( dpi ) has the ability to inspect both packet headers and payloads to identify the attack signatures in order to protect internet systems . regular_expression matching , despite its flexibility and efficiency in attack detection , brings high computation and storage complexities to nidss , making packet processing a bottleneck . stride finite_automata ( strifa ) , a new family of finite_automata , to accelerate both string_matching and regular_expression matching with reduced memory consumption . to increase the efficiency of strifa , a layered approach of attack detection by using kdd 99 darpa dataset is integrated with strifa . we demonstrate that attack detection accuracy can be achieved by using strifa and high efficiency by implementing the layered approach . i . introduction intrusions are the abnormal events happening in the computer system or network which attempts to compromise the confidentiality and availability of data or a system or a network . intrusions are caused by attackers who seek to gain extra prerogatives by getting at a system from the internet; however they may be unauthorized user or the authorized users misusing their prerogatives . intrusion_detection is the mechanism of supervising events occurring in the networks to detect the abnormal behaviours of events i . e . intrusions . the most common approaches in intrusion_detection_system are anomaly_detection and misuse detection . anomaly_detection can identify the activities that vary from the common behaviour , and thus have the potential to detect novel attacks . an approach for detecting intrusions is to conceptualize both the normal and the known attack_patterns for training a system , then performing classification of the test data . it integrates the advantages of both the signature-based and the anomaly-based detections , known as the hybrid system . hybrid systems are effective , subject to the categorization method used . they can be used to classify the unseen or new instances when occur , and then they assign one of the known classes to every test instance , because during training the system learns patterns and features from all the classes . but the only problem with the hybrid systems is the availability of labelled data . however , data requirement is also a concern for the signature , and the anomaly-based
challenges and opportunities for fpga programmable system platforms process_technology and architecture innovation are the two engines that have fueled a spectacular advancement in fpgas over the past 10 years . during this period , the price of fpgas has been reduced with 2 orders_of_magnitude , the logic capacity of fpgas has been increased with 2 orders_of_magnitude and the performance has been increased with one order_of_magnitude . whereas asics buck the tide of processing technology , fpgas ride the tide . deep_submicron effects are breaking the traditional modular_design flow of traditional soc architectures . a growing portion of the design time is spent on dealing with deep_submicron effects , at the expense of the creative process of design authoring . surveys show that , today , less than 20% of the design time , for complex socs , is spent on design authoring . programmable fpga platforms give designers the benefits of deep_submicron but rather than focusing on getting the silicon to work , you can focus on getting the design to work . today , several million logic_gates can be implemented in fpgas , as such covering the sweet spot of the asic market . by combining the programmable nature of the fpga with advanced testing methodologies , the fitness of a specific piece of silicon can be guaranteed for a given application . the increased yield associated with the custom testing approach allows to further reduce price of large fpgas with more than 80% . as a consequence , fpgas are being used for ever higher product volumes . fpgas , through their regular , parallel architecture and distributed_memory organization can continue to take benefit of scaling dimensions by adding more parallel hardware and distributed_memory . von_neumann architectures , originating from the days when silicon_area was very limited , have been dictating sequential programming models . spatial computing , exploiting massive resources of parallel hardware will change the way we program future system platforms . we predict that fpgas will become the heart of most systems over the next 5 years , replacing asics and processors as the fabric of choice . 
pooling faces : template_based face_recognition with pooled face_images we propose a novel approach to template_based face_recognition . our dual goal is to both increase recognition accuracy and reduce the computational and storage costs of template matching . to do this , we leverage on an approach which was proven effective in many other domains , but , to our knowledge , never fully explored for face_images : average pooling of face photos . we show how ( and why ! ) the space of a template's images can be partitioned and then pooled based on image quality and head pose and the effect this has on accuracy and template size . we perform extensive_tests on the ijb-a and janus cs2 template_based face_identification and verification benchmarks . these show that not only does our approach outperform published state of the art despite requiring far fewer cross template comparisons , but also , surprisingly , that image pooling performs on par with deep feature pooling . 
understanding the salience of cognitive_diversity in face-to-face and computer-mediated teams in the group decision_making literature , the effects of diversity on group interaction and performance have also been well investigated . however , with the increasing reliance of organizations on collaborative_technology , it is not clear whether cognitive_diversity affects the decision_making processes and group performance across cultural and geographic boundaries in a similar fashion as it does in traditional face-to-face teams . our goal of this study , thus , is to compare the effects of cognitive_diversity on group interaction and decision outcomes in traditional teams and computer-mediated teams . considering the two dimensions of cognitive_diversity , we suggest that in traditional teams , surface-level cognitive_diversity reduces team members' satisfaction with group interaction; while deep_level cognitive_diversity improves team members' satisfaction with group decisions . in comparison , the negative effect of surface-level cognitive_diversity will be mitigated in virtual_teams , whereas the positive effect of deep_level cognitive_diversity will be strengthened . the study is empirically tested using an intellective decision_making task . results provide partial support for our hypotheses , and shed light on both research and practice involving technology-mediated teams . 
neural_network_based underwater image_classification for autonomous underwater_vehicles image_processing has been one of hot issues for real_world robot applications such as navigation and visual servoing . in case of underwater robot application , however , conventional optical camera-based images have many limitations for real application due to visibility in turbid water , image saturation under underwater light in the deep_water , and short visible range in the water . thus , most of underwater image applications use high_frequency sonar to get precise acoustic image . there have been some approaches to apply optical image_processing methods to acoustic image , but performance is still not good enough for automatic_classification/recognition . in this paper , a neural_network_based image_processing algorithm is proposed for acoustic image_classification . especially , shadow of an acoustic object is mainly used as a cue of the classification . the neural network classifies a pre-taught image from noisy and/or occlude object images . in order to get fast learning and retrieving , a bidirectional associative_memory ( bam ) is used . it is remarked that the bam doesn't need many learning trials , but just simple multiplication of two vectors for generating a correlation matrix . however , because of the simple calculation , it is not guaranteed to learn and recall all data_set . thus , it is needed to modify the bam for improving its performance . in this paper , complement data_set and weighted learning factor are used to improve the bam performance . the test results show that the proposed method successfully classified 4 pre-taught object images from various underwater object images with up to 50% of b/w noise . 
oracle_big_data_connectors big_data for the enterprise key_features tight integration with oracle_database oracle_big_data_connectors oracle sql connector for hadoop distributed file system leverage hadoop compute resources for data in hdfs enable oracle sql to access and load hadoop data fast and very efficient load from hadoop into oracle_database partition pruning of hive tables during load and query graphical_user_interfaces of oracle data integrator drive data_transformation workflows on hadoop automatically transform r programs into hadoop jobs process large volumes of xml files in parallel and load xquery results into the database access data in hdfs securely with kerberos authentication key benefits quickly deliver data discovery applications to business users query data in-place in hadoop with oracle sql extremely fast data loading between hadoop and oracle_database while minimizing database cpu utilization during load enable data scientists to use r on data in hadoop and combine with advanced analytics in the database process extremely large volumes of xml_data in hadoop reduce the complexities of hadoop through graphical tooling integrated and tested on big_data appliance easy-to-use for hadoop and oracle developers oracle_big_data_connectors is a software_suite that integrates processing in hadoop with operations in a data_warehouse . designed to leverage the latest features of apache_hadoop , big_data_connectors connect hadoop clusters with database infrastructure to harness massive volumes of structured and unstructured_data for critical business insights . big_data_connectors greatly simplify development and are optimized for efficient connectivity and high_performance between oracle_big_data appliance and oracle exadata . oracle_big_data_connectors 3 . 0 delivers a rich set of new features , increased connectivity , enhanced performance , and security for big_data applications . large volumes of data are increasingly collected and processed in hadoop , while enterprise it systems are centered on relational data_warehouses . oracle_big_data_connectors bridges data_processing in hadoop with oracle_database , providing the crucial ability to unify data across these systems . combining pre-processing of large data volumes of raw and unstructured_data in hadoop with the advanced analytics , complex data_management , and real-time query capabilities of oracle_database , oracle_big_data_connectors deliver features that support information discovery , deep analytics and fast integration of all data in the enterprise . the components of this software_suite are : oracle sql connector for hadoop distributed file system oracle loader for hadoop oracle data integrator application adapter for hadoop oracle r advanced analytics for hadoop oracle xquery 
exploratory engineering in ai we regularly see examples of new artificial_intelligence ( ai ) capabilities . google's self-driving car has safely traversed thousands of miles . watson beat the jeopardy ! champions , and deep_blue beat the chess champion . boston dynamics' big dog can walk over uneven terrain and right itself when it falls over . from many angles , software can recognize faces as well as people can . as their capabilities improve , ai systems will become increasingly independent of humans . we will be no more able to monitor their decisions than we are now able to check all the math done by today's computers . no doubt such automation will produce tremendous economic value , but will we be able to trust these advanced autonomous systems with so much capability ? for example , consider the autonomous trading programs which lost knight capital $440 million ( pre-tax ) on august 1st , 2012 , requiring the firm to quickly raise $400 million to avoid bankruptcy . 1 this event undermines a common view that ai systems cannot cause much harm because they will only ever be tools of human masters . autonomous trading programs make millions of trading decisions per day , and they were given sufficient capability to nearly bankrupt one of the largest traders in u . s . equities . today , ai safety_engineering mostly consists in a combination of formal_methods and testing . though powerful , these methods lack foresight : they can be applied only to particular extant systems . we describe a third , complementary approach which aims to predict the ( potentially hazardous ) properties and behaviors of broad classes of future ai agents , based on their mathematical_structure ( e . g . reinforcement_learning ) . such projects hope to discover methods "for determining whether the behavior of learning agents [will remain] within the bounds of pre-specified constraints . . . after learning . " 2 we call this approach "exploratory engineering in ai . "
development of an improved gui automation test system based on event-flow graph a more automated graphic user_interface ( gui ) test model , which is based on the event-flow graph , is proposed . in the model , a user_interface automation api tool is first used to carry out reverse_engineering for a gui_test sample so as to obtain the event-flow graph . then two approaches are adopted to create gui_test sample cases . that is to say , an improved ant_colony_optimization ( aco ) algorithm is employed to establish a sequence of testing cases in the course of the daily smoke test . the sequence goes through all object event points in the event-flow graph . on the other hand , the spanning_tree obtained by deep breadth_first_search ( bfs ) approach is utilized to obtain the testing cases from goal point to outset point in the course of the deep regression test . finally , these cases are applied to test the new gui . moreover , according to the above-mentioned model , a corresponding prototype system based on microsoft ui automation framework is developed , thus giving a more effective way to improve the gui automation test in windows os . 
web_based personalised system of instruction : an effective approach for diverse cohorts with virtual learning environments ? computer_mediated_communication improving classroom teaching media in education post-secondary_education teaching/learning strategies the personalised system of instruction is a form of mastery learning which , though it has been proven to be educationally effective , has never seriously challenged the dominant lecture-tutorial teaching method in higher_education and has largely fallen into disuse . an information_and_communications_technology assisted version of the personalised system of instruction using a virtual_learning_environment is promoted here based on the authors " longitudinal design research_into this pedagogy . the particular elements of the virtual_learning_environment which are promoted are short video clips , online formative tests and an assessment management system . the authors present their experiences of developing and deploying this pedagogy for the teaching of introductory discrete_mathematics to large classes of computer_science students at two uk higher_education institutions both with whole cohorts and " at risk " groups of students . in particular , this method is promoted as particularly helpful to students who do not adopt a deep approach to learning as many students fail to do . moreover " at risk " students using this method ( n = 71 ) demonstrated an average glass effect_size of 0 . 83 compared with other " at risk " students who did not ( n = 35 ) . based on these experiences , this pedagogy is promoted as an effective approach to teaching in higher_education , especially the teaching of cognitive skills to diverse cohorts of students on foundation level modules . 
the use of ict in the public_sector and its influence on communication with citizens in slovenia the internet has become one of the most important means of communication in all areas of our life . in the paper we focused on central and local government bodies and their attitude towards information and communication technology . by analysing web_pages , inquiring public servants and testing the responses on citizens' questions we tried to discover influences of internet on better informing of citizens , their participation in making decisions of public interest and communication between citizens and central and local government bodies . 1 . introduction over the last few years the internet has become one of the most important means of communication in all social areas and the public_sector is no exception . however , the expectations of experts from the administrative field and those who are engaged in public_sector from a more organisational , sociological or political perspective are enormous [schalken , 2000] . taking into account the nature of the public_sector where efficient collection , processing , storing , distribution and exchange of information between administrative bodies and between administrative bodies and citizens is one of the fundamental activities , expectations of deep changes and advantages being ushered in by the use of the internet are completely legitimate . furthermore , the more the number of internet users in the population of an area approaches the degree of penetration of other mass_media ( particularly television ) , the more realistic considerations about the use of internet in political processes and enforcement of democracy will become . the influence ( and use ) of the internet on democratic processes in a certain environment can be direct or indirect . by direct influence we can understand the use of internet for the realisation of public opinion polling , different referenda and in finally , even elections . literature on the topic reveals a series of pilot projects and experiments with internet usage for direct_democracy introduction ( the city of amsterdam , american elections , etc ) . unfortunately in slovenia the political elites would not think in this way yet , and also the penetration of internet is not so high ( the latest information suggests 15% of households have access to internet [ris , 1999] ) that would allow similar experiments . however , internet and its intensive use in public_sector can indirectly influence on the democratisation of public life and democratic processes in a particular area . intensive , creative and stimulating use of the internet in public_sector can essentially contribute to better communication between political and administrative bodies and citizens . it can also contribute to better-924 
is optical imaging spectroscopy a viable measurement technique for the investigation of the negative bold phenomenon ? a concurrent optical imaging spectroscopy and fmri study at high field ( 7 t ) traditionally functional_magnetic_resonance_imaging ( fmri ) has been used to map activity in the human_brain by measuring increases in the blood oxygenation level dependent ( bold ) signal . often accompanying positive bold fmri signal changes are sustained negative signal changes . previous_studies investigating the neurovascular coupling mechanisms of the negative bold phenomenon have used concurrent 2d-optical imaging spectroscopy ( 2d-ois ) and electrophysiology ( boorman et al . , 2010 ) . these experiments suggested that the negative bold signal in response to whisker stimulation was a result of an increase in deoxy-haemoglobin and reduced multi-unit activity in the deep cortical layers . however , boorman et al . ( 2010 ) did not measure the bold and haemodynamic response concurrently and so could not quantitatively compare either the spatial maps or the 2d-ois and fmri time_series directly . furthermore their study utilised a homogeneous tissue model in which is predominantly sensitive to haemodynamic changes in more superficial layers . here we test whether the 2d-ois technique is appropriate for studies of negative_bold . we used concurrent fmri with 2d-ois techniques for the investigation of the haemodynamics underlying the negative bold at 7 tesla . we investigated whether optical methods could be used to accurately map and measure the negative bold phenomenon by using 2d-ois haemodynamic data to derive predictions from a biophysical model of bold_signal_changes . we showed that despite the deep cortical origin of the negative bold response , if an appropriate heterogeneous tissue model is used in the spectroscopic analysis then 2d-ois can be used to investigate the negative bold phenomenon . 
question-answer approach to human_computer_interaction in collaborative designing question-answer approach to human_computer_interaction in collaborative designing background of qa-approach introduction one of problematic kinds of a human-computer activity is a collective creating of software intensive systems ( siss ) in any of which the software plays an essential role in the system functionality , cost , development risk , and development time ( software , 2006 ) . a very low degree of success ( about 35% ) in the activity of such type indicates that the problem of failures is connected with an absence of very important means accessible to both developers and users of the sis . from the general point_of_view , the unsuccess-fulness of the sis development is being discovered via users interactions with the sis that essentially differ from reactions expected by users . similar events indicate that corresponding units of the programmed behavior have not been tested by developers or were being understood incorrectly . usually any definite unit of the behavior has not been tested when this unit was not qualified by developer as an essential case . abstract the chapter presents a question-answer approach to the programming of human-computer interactions ( hci ) during collaborative development of software intensive systems . efficiency of the general work can be essentially increased if the human part of the work is informed by precedents and executed with a special kind of pseudo-program by " intellectual processors . " the role of any processor of such type is fulfilled by a designer . the suggested approach was investigated and evolved until an instrumental system providing the pseudo-code programming of intellectual processors combined with computer processors . interactions between processors are based on question-answer reasoning . pseudo-code programs and their corresponding instrumental means can be combined easily with traditional hci . therefore , the developers of siss need the effective means for adequate defining of the essential behavior units , their modeling for achieving the necessary understanding and also for testing the units in appropriate conditions of designing and using . first of all , the essential units are to be distinguished and such actions can be fulfilled experimentally by interacting with the developing system in real time of designing . let us notice that interactions used in experiments with the chosen behavior unit can play for this unit the integrative and others helpful roles . on a deep_belief of the author , the named behavioral units are to be distinguished , defined , modeled , understood , coded , and tested as precedents . " precedents are actions or decisions that have already happened in the past and which can be referred to and justified as an example that can 
assessment of gps signal quality in_urban_environments using deeply integrated gps/imu research interests_include various aspects of inertial_navigation , gps/ins integration , gps software receiver development , gps carrier_phase positioning , digital_signal_processing , laser radar ( ladar ) localization technologies , and joint time-frequency data_analysis . he received the ion early achievement award in 2006 . research has included the local area augmentation system ( laas ) , with emphasis on the analysis of gps signal anomalies , and the use of integrated gps systems for vehicle navigation in_urban_environments . abstract this research evaluates the quality of gps signals and their usability for localization in_urban_environments using gps data_collected in urban_canyons . gps signals collected on a software_defined_radio ( sdr ) platform in urban_canyons in downtown athens , ohio and columbus , ohio are processed using a deeply integrated gps/ins scheme . this deep_integration architecture allows for coherent signal integration over time intervals as long as 1 second . the deep_integration mode that provides continuous carrier_phase tracking is used herein . performance of the deep_integration scheme in urban_canyons is compared to performance characteristics of commercially available low-sensitivity gps receivers . results characterize the received signals in_urban_environments in terms of signal_strength , tracking continuity and multipath influence on signal tracking performance . results attained show that signals from 5 to 6 space vehicles ( svs ) are available for processing , even in dense urban_canyons . deep gps/ins integration enables continuous carrier_phase tracking , thus allowing for accuracies on the cm/s level in velocity in_urban_environments . in contrast , velocity performance of the commercial low-sensitivity gps receivers considered yielded errors on the level of 1 m/s . additionally , the results_demonstrate that continuous carrier_phase tracking is possible , even for those cases where buildings block the satellite line of sight ( los ) . further , consistent carrier_phase tracking is performed for at least 2 svs for a test scenario where all los vectors are blocked by buildings , and up to 6 svs for all other urban canyon scenarios . tracking remains consistent for weak signals with carrier-to-noise ratios ( cnrs ) down to 12 db-hz . 
tri-scan : a novel dft technique for cmos path_delay_fault_testing we propose a novel design_for_testability technique to apply two pattern_tests for path_delay_fault_testing . due to stringent timing requirements of deep-submicron vlsi chips , design_for_test schemes have to be tailored for detecting stuck-at as well as delay_faults quickly and efficiently . existing techniques such as enhanced scan add substantial hardware_overhead , whereas techniques such as scan-shifting or functional justification make the test_generation process complex and produce lower coverage for scan_based_designs as compared to non-scan designs . we exploit the characteristics of cmos circuitry to enable the application of two-pattern_tests . the proposed technique reduces the problem of path_delay_fault_testing for scan_based_designs to that of path_delay_fault_testing with complete accessibility to the combinational_logic , and has minimal area_overhead . the scheme also provides significant reduction in power during scan operation . 
more than skin deep : measuring effects of the underlying model on access_control system usability in access_control systems , policy rules <i>conflict</i> when they prescribe different decisions ( allow or deny ) for the same access . we present the results of a user study that demonstrates the significant impact of conflict_resolution method on policy-authoring usability . in our study of 54 participants , varying the conflict_resolution method yielded statistically_significant differences in accuracy in five of the six tasks we tested , including differences in accuracy rates of up to 78% . our results_suggest that a conflict_resolution method favoring rules of smaller scope over rules of larger scope is more usable than the microsoft_windows operating system's method of favoring deny rules over allow rules . perhaps more importantly , our results_demonstrate that even seemingly small changes to a system's semantics can fundamentally affect the system's usability in ways that are beyond the power of user_interfaces to correct . 
swat-based streamflow and embayment modeling of karst-affected chapel branch watershed , south_carolina swat is a gis-based basin-scale_model widely used for the characterization of hydrology and water_quality of large , complex watersheds; however , swat has not been fully tested in watersheds with karst geomorphology and downstream reservoir-like embayment . in this study , swat was applied to test its ability to predict monthly streamflow dynamics for a 1 , 555 ha karst watershed , chapel branch creek , which drains to a large embayment and is comprised of highly diverse land uses . swat was able to accurately simulate the monthly streamflow at a cave spring ( cs ) outlet draining mostly agricultural and forested lands and a golf course plus an unknown groundwater discharging area , only after adding known monthly subsurface inputs as a point_source at that location . monthly streamflows at two other locations , both with multiple land uses , were overpredicted when lower lake levels were prevalent as a result of surface_water flow to groundwater ( losing streams ) . the model underpredicted the flows during rising lake levels , likely due to high conductivity and also a deep percolation coefficient representing flow lost to shallow and deep groundwater . at the main watershed outlet , a wide section performing as a reservoir embayment ( re ) , the model was able to more accurately simulate the measured mean monthly outflows . the re storage was estimated by using a daily water_balance approach with upstream inflows , rainfall , and pet as inputs and using parameters obtained by bathymetric survey , lidar , and downstream lake level data . results demonstrated the substantial influence of the karst features in the water_balance , with conduit and diffuse flow as an explanation for the missing upstream flows appearing via subsurface conveyance to the downstream cave spring , thus providing a more accurate simulation at the embayment outlet . results also highlighted the influences of downstream lake levels and karst voids/conduits on the watershed hydrologic balance . simulation performance of hydrology could be improved with more accurate dems obtained from lidar for karst feature identification and related modification of swat parameters . this swat modeling effort may have implications on nutrient and sediment loading estimates for tmdl development and implementation in karst watersheds with large downstream embayments that have significant changes in water level due to adjoining lakes . nderstanding watershed hydrology is critical , as it is often a primary driving force for nutrient cycl ing and loading dynamics and subsequent down stream water_quality impacts as a result of rapid urbanization and other land use changes . for this purpose , 
model_checking for autonomic systems specified with assl autonomic computing augurs great_promise for deep_space_exploration missions , bringing on-board intelligence and less reliance on control links . as part of our research on the assl ( autonomic system specification_language ) framework , we have successfully specified autonomic properties , verified their consistency , and generated prototype models for both the nasa ants ( autonomous nano-technology swarm ) concept mission and the nasa voyager mission . the first release of assl provides built-in consistency checking and functional_testing as the only means of software verification . we discuss our work on model_checking autonomic systems specified with assl . in our approach , an assl specification is translated into a state-transition model , over which model_checking is performed to verify whether the assl specification satisfies correctness properties . the latter are expressed as temporal_logic formulae expressed over sets of assl constructs . we also discuss possible solutions to the state-explosion problem in terms of state graph abstraction and probability weights assigned to states . moreover , we present an example case_study involving checking liveness properties of autonomic systems with assl . 
pervasive parallel_computing : an historic opportunity for innovation in programming and architecture parallel_programming has been the subject of deep research for decades -- and renowned in the software community as a difficult challenge to the degree that many companies have teams of parallelism and concurrency experts . further , many isv's explicitly design their software_architectures so as to ensure that the majority of the development effort , including of course debug and test , can be done without consideration of parallelism . what makes parallelism so difficult , are the knotty and coupled problems of correctness , performance -- particularly data locality , and software modularity . in terascale ( manycore ) chip-level multiprocessors , we are facing a pervasive and critical parallel_programming challenge . core counts on a single chip are expected to increase rapidly , progressing with moore's_law , and quad-core systems are already available today in mainstream volume client and server platforms . to continue the rapid performance scaling to which we have become accustomed , applications will need to exhibit ample parallelism ( and increasing amounts of it ) for successive generations of hardware . further , because the move to multiple-core parallelism as the primary basis for performance_improvement is pervasive , this requirement falls on a wide range of applications including traditional large_scale commercial and hpc server , desktop , laptop , and even those running on small mobile_devices . that breadth has numerous implications for the types of solutions that are required . we will discuss some of the requirements for terascale parallel_programming solutions , and point out several potentially fruitful directions . a number of these solutions will build on mainstream programming approaches ( objects , modularity , imperative ) , particularly introducing parallelism with modest disruption to both large_scale and local-scale program structure . however , there is an opportunity for radically different approaches to take hold in the mainstream ( e . g . functional ) . on the hardware front , there are several reasons why the parallel_programming problem for terascale ( manycore ) systems is easier than previous generations of multiprocessors ( and can be much easier ) . the basic hardware characteristics of chip-multiprocessors provide much greater opportunity for efficient coupling and coordination , and a tightly-coupled memory system , simplifying a wealth of sophisticated scheduling and sharing structures . further , the diminishing performance returns for larger single cores releases innovation to support both parallel_programming , and higher_level programming in general . this is a huge opportunity to pioneer new approaches and solutions that are radically better than those widely-used today . we will close with some speculation on the rate of progress of parallel_programming into the mainstream software community and some implications of such proliferation . 
graph_based path_planning for mobile_robots graph_based path_planning for mobile_robots for mobile_robots , whose capricious perceptions are such a bother . iii acknowledgements when i entered the graduate ece program at georgia tech in 2003 , i had accepted a position working on correlating gene interaction based on time_series expression data . an unexpected set of events later led me to work on compressing digital signals for low-bandwidth communication , ostensibly for image transmission between mobile_robots . then , before i knew what hit me , i found myself neck_deep in a multi-million dollar competitive robotics project funded by darpa . i consider myself extremely lucky to have ended up in a position that allows me such extraordinary experience with field robotics , especially given the precious little programming and practical experience i had when i started . the grits lab has been a stimulating place , and having the trio of irobot magellan " trashcans " , the nrec lagr pair of robots , and the porsche cayenne " sting " robot has left me unusually spoiled . there aren't many places where there a fresh young student can dive right into top-of-the-line robot hardware . my advisor , professor magnus egerstedt , has been a motivating and ever animated mentor , and to whom i offer my deepest thanks . he gave me the freedom to explore whatever kinds of solutions intrigued me and was an irreplaceable support in guiding me along the way . i cannot say enough good things about these past few years in his lab . in a similar way , professor tucker balch has been a great pleasure to work with ( and to work for ) . he has made a lasting influence on me , particularly in how i look at field robotics and tackle complex projects . the white board discussions and the field_tests have certainly helped shape this thesis and contributed enormously to my understanding of robotics . i would also like to thank all my cohorts in the grits and borg labs , especially matt powers , without whose partnership on the lagr project much of the practical application of this thesis might literally not have ever gotten rolling . our discussions which ranged from basic programming to esoteric control architectures to how to make cheesecake from scratch have been very valuable to me , and his suggestions have greatly improved the iv quality of the implemented versions of the work presented below . much of my work is a direct result of problems that needed to be solved for the lagr project , so i 
prediction of novel precursor mirnas using a context-sensitive hidden_markov_model ( cshmm ) background it has been apparent in the last few years that small non coding rnas ( ncrna ) play a very significant role in biological regulation . among these micrornas ( mirnas ) , 22-23 nucleotide small regulatory rnas , have been a major object of study as these have been found to be involved in some basic biological processes . so far about 706 mirnas have been identified in humans alone . however , it is expected that there may be many more mirnas encoded in the human_genome . in this report , a "context-sensitive" hidden_markov_model ( cshmm ) to represent mirna structures has been proposed and tested extensively . we also demonstrate how this model can be used in conjunction with filters as an ab_initio method for mirna identification . results the probabilities of the cshmm model were estimated using known human mirna sequences . a classifier for mirnas based on the likelihood score of this "trained" cshmm was evaluated by : ( a ) cross_validation estimates using known human sequences , ( b ) predictions on a dataset of known mirnas , and ( c ) prediction on a dataset of non coding rnas . the cshmm is compared with two recently_developed methods , mipred and cid-mirna . the results suggest that the cshmm performs better than these methods . in addition , the cshmm was used in a pipeline that includes filters that check for the presence of est matches and the presence of drosha cutting sites . this pipeline was used to scan and identify potential mirnas from the human chromosome 19 . it was also used to identify novel mirnas from small_rna sequences of human normal leukocytes obtained by the deep_sequencing ( solexa ) methodology . a total of 49 and 308 novel mirnas were predicted from chromosome 19 and from the small_rna sequences respectively . conclusion the results suggest that the cshmm is likely to be a useful tool for mirna discovery either for analysis of individual sequences or for genome scan . our pipeline , consisting of a cshmm and filters to reduce false positives shows promise as an approach for ab_initio identification of novel mirnas . 
the influence of modality on deep_reasoning_questions this study investigated the influence that modality ( print versus spoken text ) had on learning with deep_reasoning_questions . half the participants were randomly assigned to receive deep_reasoning_questions during the learning session . the other half received the same information in the absence of deep_reasoning_questions . the participants who received deep_reasoning_questions were randomly assigned to one of two different groups . one group received deep_reasoning_questions as on-screen printed text while the other group received deep_reasoning_questions in a spoken modality via a text to speech engine . participants who received deep_reasoning_questions had higher post_test scores than those who did not , a finding that replicated previous_research . additionally , learning was better for the learners who received printed text than spoken messages , a finding that is not compatible with a number of theoretical and empirical claims in the literature . ( 2010 ) 'the influence of modality on deep_reasoning_questions' , int . interests_include intelligent_tutoring environments , student generated questions , and emotions . scotty d . craig is a research scientist in the institute for intelligent_systems located at the university of memphis . to date , he has worked on projects in such areas as affect and learning , discourse processing , mechanical reasoning , multimedia learning , vicarious learning environments and intelligent_tutoring_systems . 
haptic discrimination of object shape in humans : two-dimensional angle discrimination . the human ability to recognize objects on the basis of their shape , as defined by active exploratory movements , is dependent on sensory feedback from mechanoreceptors located both in the skin and in deep_structures ( haptic feedback ) . surprisingly , we have little information about the mechanisms for integrating these different signals into a single sensory percept . with the eventual aim of studying the underlying central neural mechanisms , we developed a shape discrimination test that required active exploration of objects , but was restricted to one component of shape , two-dimensional ( 2d ) angles . the angles were machined from 1-cm-thick plexiglas , and consisted of two 8-cm-long arms that met to form an angle of 90 degrees ( standard ) or 91 degrees to 103 degrees ( comparison angles ) . subjects scanned pairs of angles with the index_finger of the outstretched arm and identified the larger angle of each pair explored . discrimination threshold ( 75% correct ) was 4 . 7 degrees ( range 0 . 7 degrees to 12 . 1 degrees ) , giving a precision of 5 . 2% ( 0 . 8-13 . 4% : difference/standard ) . repeated blocks of trials , either in the same session or on different days , had no effect on discrimination threshold . in contrast , the motor strategy was partly modified : scanning speed increased but dwell-time at the intersection did not change . finally , 2d angle discrimination was not significantly modified by rotating the orientation of one of the angles in the pair ( 0 degrees , 4 degrees or 8 degrees rotation towards the midline , in the vertical plane ) , providing evidence that subjects evaluated each angle independently in each trial . subject reports indicated that they relied on cutaneous feedback from the exploring digit ( amount of compression of the finger at the angle ) and mental images of the angles , most likely arising from proprioceptive information ( from the shoulder ) generated during the to-and-fro scans over the angle . in terms of shoulder angles , the mean discrimination threshold here was 0 . 54 degrees ( range 0 . 08 degrees to 1 . 36 degrees ) . these values are lower than previous estimates of position sense at the shoulder . in light of the subjects' strategies , it therefore seems likely that both cutaneous and proprioceptive ( including both dynamic and static position-related signals ) feedback contributed to the haptic discrimination of 2d angles . 
deep_learning design for sustainable innovation within shifting learning landscapes changes in the underpinning technologies for tel is occurring at a pace that we have never before experienced , and this is unlikely to slow down . this necessitates a broader and more profound understanding of design that needs to be more future-proof than relying on the latest or emerging technologies and yet embraces the collaborative , multimodal and ubiquitous nature of learning in 21c . in addressing this challenge this article develops , exemplifies and tests the approach of deep_learning design ( dld ) , which has led to relatively large_scale and sustainable innovations and also outlined clear directions for near-future developments . specifically , in this article we : justify why dld is necessary and describe its key principles; exemplify these principles through four tel initiatives; and , draw some implications and conclusions from across these projects about dld and future learning . 
hydroacoustic signal classification using support_vector_machines kernel-based algorithms such as support_vector_machines ( svms ) are state-of-the-art in machine_learning for pattern_recognition . this chapter introduces svms and describes a specific application to hydroacoustic signal classification . long_range , passive-acoustic monitoring in the oceans is facilitated by propagation properties for underwater sound . in particular , the deep sound ( sofar , sound fixing and ranging ) channel can act as a waveguide for underwater signals . in this chapter , svms are employed for classifying hydroacoustic signals recorded by the sensor_network for verification of the comprehensive_nuclear_test_ban_treaty . constraints in the early signal_processing chain and limited data require tailored kernel functions and careful svm model_selection . we demonstrate how problem-specific kernel functions can increase classifier performance when combined with efficient gradient_based approaches for optimizing kernel and svm regularization parameters . 
iddq data_analysis using neighbor current_ratios i ddq test loses its effectiveness for deep_sub_micron chips since it cannot distinguish between faulty and fault_free currents . the concept of current_ratios , in which the ratio of maximum to minimum i_ddq is used to screen faulty chips , has been previously_proposed . however , it is incapable of screening some defects . the neighboring chips on a wafer have similar fault_free properties and are correlated . in this paper , the use of spatial_correlation in combination with current_ratios is investigated . by differentiating chips based on their non-conformance to local i_ddq variation , outliers are identified . the analysis of sematech test_data is presented . 
modeling bus load on can modeling bus load on can title : modeling bus load on can ii preface this titled master thesis has been done at the research_and_development/system i take this opportunity to thank several people who gave me their support during my thesis preparation days at s_dert_lje and during my post graduation at halmstad . deep and special thanks go first to jan lindman , my supervisor at scania , for his intensive comments , instructions , guidance , testing and tracking . to prof . tony larsson , my supervisor and examiner at halmstad university for his patience and unlimited explanations , valuable instructions and support . to david holmgren , the resa group manager for his instructions and comments . to my father , mother and wife , i have received intensive and unlimited support from them . abstract the existence of high load and latency in the can bus network would indeed lead to a situation where a given message crosses its deadline; this situation would disturb the continuity of the required service as well as activating fault codes due to delay of message delivery , which might lead to system failure . the outcome and goal of this thesis is to research and formulate methods to determine and model busload and latencies , by determining parameters such as alpha and breakdown utilization , which are considered as indications to the start of network breakdown when a given message in a dataset start to introduce latency by crossing its deadline which are totally prohibited in critical real time communications . the final goal of this master thesis is to develop a tool for calculating , modeling , determining and visualizing worst_case busload , throughput , networks' breakdown points and worst_case latency in scania can bus networks which is based on the j1939 protocol . scanla ( the developed can busload analyzer tool in this thesis ) is running as an executable application and uses a graphical user interface as a human-computer interface ( i . e . , a way for humans to interact with the tool ) that uses windows , icons and menus and which can be manipulated by a mouse . associated databases and accompanying documents , such as this thesis report . this tool is hoped that it will be useful but without any warranty . for a description and instructions on how to use the tool , read intensively the report . please let the producer of the mentioned tool know of any improvements , changes or modifications that might be made . 
mastering use cases : capturing functional_requirements for interactive applications use cases were introduced in the early 90s by jacobson . he defined a use case as a "specific way of using the system by using some part of the functionality . " use case modeling is making its way into mainstream practice as a key activity in the software_development_process ( e . g . , unified process ) . there is accumulating evidence of significant benefits to customers and developers . the use case model is the artifact of choice for capturing functional_requirements and as such , serves as a contract of the envisioned system behavior between stakeholders . it drives the architecture of the application , it can be used to generate functional test_cases and often serves as a reference point for maintenance and documentation purposes . writing effective and well-structured use cases is a difficult task which requires a deep understanding of the surrounding techniques and best practices . current practice has shown that it is easy to misuse them or make mistakes that can unintentionally turn them into "abuse cases" . 
multiprocessor implementation of transitive_closure the matter embodied herein has not been submitted to any other university for the award of any other degree . 8023119 this is to certify that above statement made by the candidate is correct and true to best of my knowledge . acknowledgement to discover , analyze and to present something new is to venture on an untrodden path towards an unexplored destination is an arduous adventure unless one gets a true torchbearer to show the way . this enlightening guidance , i found in my revered guide mrs patronization it was never possible to give final shape to this thesis . i express my heartfelt gratitude towards her for her valuable guidance , encouragement , constant involvement , inspiration and the enthusiasm with which she solved my difficulties . i shall be failing in my duties if i do not express my deep sense of gratitude ( deemed university ) , patiala , for his valuable advices and suggestions . i would like to thank all the faculty and staff members of computer_science & engineering department for providing me all the facilities required for the completion of this work . last but not the least , i express my heartfelt thanks to my parents , my friends for cooperation , which they were always ready to extend . abstract graph_theoretic algorithms are found quite effective for solving complex real_life problems . computing the transitive_closure in directed graphs is a fundamental graph problem . transitive_closure can be thought of as establishing a data_structure that makes it possible to solve reachability questions ( can i get to x from y ? ) efficiently . after the preprocessing of constructing the transitive_closure , all reachability queries can be answered in constant time by simply reporting a matrix entry . transitive_closure is fundamental in propagating the consequences of modified attributes of a graph g . for efficient system utilization and fast response to the user , it is necessary to use parallel_algorithms for solving a single problem . transitive_closure is a highly parallelizable problem; it belongs to the class nc of problems that can be solved in polylogarithmic time ( i . e . o ( log c n ) for some constant c ) with polynomial number of processors . this thesis proposes an efficient scalable multiprocessor algorithm for transitive_closure computation in a distributed_computing environment . it focuses on the role of transitive_closure in the graph_theory , very large databases , relational database_management_systems , and vlsi test_generation . serious efforts have been made in investing new parallel and /or 
open object_oriented modelling and validation framework for modular industrial_automation systems this paper introduces a framework for formal modelling and validation of automation systems intended to be used by control engineers . the framework is based on a graphical , modular , and typed formalism of net condition/event systems . this allows for modelling of realistic hierarchically organized industrial_automation systems in a closed_loop . the framework consists of methodologies and tools which enable formal_analysis of automation systems . the framework will be used to improve safety , reliability and robustness of automation systems predicting potential faults and deadlocks . 1 introduction modern production_systems need to be more flexible and re-configurable . for this reason they are built from standardized processing modules . their software is also organized in a modular_form and is executed on distributed control devices . when new configurations of production_systems are formed from the modular components , the testing becomes a bottleneck for quick commissioning . formal validation can reduce the time-consuming testing and commissioning phases of system's development and deployment . as the functionality of such systems is determined by cooperation of entities of heterogeneous domains , e . g . mechanical , electric , automation hardware and software , the validation has to take into account the relevant properties from all these domains . formal modelling of automation systems proved to be helpful for validation of automation systems by simulation or by formal_verification of static and dynamic properties . in automation systems the software represents a variable part , while the models of equipment can be reused through the engineering cycle . once developed by a machine vendor , the models may follow the equipment , enabling the machine users ( e . g . system integrators ) to validate new configurations of the machines re-using the models of their components . this vision , however , requires a more systematic approach to the modelling , than that can be seen now . models in most of the formalisms such as petri_nets or finite_automata lack integrating capabilities : while they may cope well with the modelling of a particular process , building the overall model of a system comprising several processes is difficult . an opposite example make the modelling techniques based on the unified modelling language ( uml ) . the uml is getting increasingly popular also in automation for its ability to describe systems in object_oriented form . however , the uml lacks a formal background and can hardly be used for deep analysis of the systems . this paper tries to sketch another approach for systematic modelling of systems by means of a modular modelling formalism . the paper is organized 
impact of well edge proximity effect on timing this paper studies impact of the well edge proximity effect on digital_circuit delay , based on model parameters extracted from test_structures in an industrial 65nm wafer process . the experimental results show that up to 10% of delay increase arises by the well edge proximity effect in the 65nm technology , and it depends on interconnect length . furthermore , due to asymmetric increase in pmos and nmos threshold voltages , delay may decrease in spite of the threshold_voltage increase . from these results , we conclude that considering wpe is indispensable to cell characterization in the 65nm technology . i . introduction at front_end stages of cmos wafer process , p and n types of ions are implanted to form wells . during the implantations , lateral scattering of the ions nearby edges of the photo-resist causes well doping concentration , as shown_in_fig . 1 [1][2][3] . the well doping concentration mainly drifts the threshold_voltage of mos transistors . we call it " well edge proximity effect " ( wpe ) . as advanced deep well implants with high-energy implanters are introduced to suppress parasitic bipolar gain for latch-up protection , wpe becomes severer . in circuit_design , wpe can be suppressed by making a large separation between gate poly and enclosing well edge . on the other hand , as shown_in_fig . 2 , common cell based designs intrinsically involve wpe , whereas analog circuit_design can make use of the large separation . the figure depicts a part of standard cells placement . since wells are continuous along the cell rows and then dummy cells are placed at the both ends of the cell rows for lithographical reasons , horizontal proximity between gate and well edges can be negligible . as for the vertical direction , the inner and the outer spacing shown in the figure are uniquely determined for cell by cell . therefore , we should take care of wpe only for the vertical direction in the digital cell based design . during circuit_design , a lot of effort is put into timing convergence[4] . from a standpoint of circuit_design , delay variation due to wpe is a matter of utmost concern . we evaluate the impact of wpe on circuit timing as a case study in an industrial 65nm technology , and demonstrate the delay increase quantitatively . we also point out that wpe decreases cell delay in some conditions of input waveform and output load . 
pgo : a parallel computing_platform for global_optimization based on genetic_algorithm this paper presents the design , architecture and implementation of a general parallel computing_platform , termed pgo , based on the genetic_algorithm for global_optimization . pgo provides an efficient and easy-to-use framework for paralleliz-ing the global_optimization procedure for general scientific modeling_and_simulation processes . along with a core optimization kernel built on a genetic algorithm , pgo also includes a general input generator and an output extractor that can facilitate its easy integration with various scientific computing tasks . in this paper , we demonstrate the efficiency and versatility of pgo with two different applications : ( 1 ) the parallelization of a large scale_parameter estimation problem associated with mod-eling water flow in a heterogeneous deep vadose_zone; ( 2 ) the parallelization of a complex simulation-optimization procedure for searching for an optimal groundwater remediation design . pgo is developed as an open source_code , and is independent of the computer operating system . it has been tested in a heterogeneous computing environment consisting of solaris 9 , fedora core_2 linux , and microsoft_windows machines , and is freely available for download from
problem_based_learning in mathematics - a tool for developing students' conceptual knowledge mathematics teachers must teach students not only to solve problems but also to learn about mathematics through problem_solving . 1 while " many students may develop procedural fluency they often lack the deep conceptual understanding necessary to solve new problems or make connections between mathematical ideas . " 2 this presents a challenge for teachers : problem_based_learning ( pbl ) provides opportunities for teachers to meet this challenge . pbl exists as a teaching method grounded in the ideals of constructivism and student-centred learning . when using pbl , teachers help students to focus on solving_problems within a real_life context , encouraging them to consider the situation in which the problem exists when trying to find solutions . 3 the majority of research examining pbl focuses on its use in medical_schools , with the key_features being ( a ) the use of collaborative small-group work , ( b ) a student-centred approach , ( c ) the teacher as facilitator and ( d ) the use of real_life problems as the organizing focus . 4 in the medical arena , groups of students are given a set of realistic patient symptoms and expected to research possible diagnoses and courses of treatment; groups work independently , developing and answering their own questions . if , during this diagnostic phase , a group is unsuccessful in addressing key issues , the instructor notes this on their assessment but does not provide the solution . 4 in the classroom setting , it is this aspect of pbl which presents the most signifcant challenge , requiring teachers to shift from direct instruction to supporting students organize their own learning . 5 what kind of mathematics problems help students develop deep , conceptual understanding ? research tells us many students lack a deep understanding of mathematical concepts . classroom teachers find it difficult both to develop a real_life hook for students and to allow students to work through problem_solving independently . pbl is a promising approach not only to build mathematics understanding but also to test students' conceptual knowledge . pbl requires teachers to present students with multifaceted , real_life problems and to act as facilitators supporting students in organizing their own learning . 
induction of integrated view for xml_data with heterogeneous dtds this paper proposes a novel approach to integrating heterogeneous xml dtds . with this approach , an information agent can be easily extended to integrate heterogeneous xml-based contents and perform federated_search . based on a tree grammar inference technique , this approach derives an integrated view of xml dtds in an information integration framework . the derivation takes advantages of naming and structural similarities among dtds in similar domains . the complete approach consists of three main steps . ( 1 ) <i>dtd clustering</i> clusters dtds in similar domains into classes . ( 2 ) <i>schema learning</i> applies a tree grammar inference technique to generate a set of tree grammar rules from the dtds in a class from the previous step . ( 3 ) <i>minimization</i> optimizes the rules generated in the previous step and transforms them into an integrated view . we have implemented the proposed approach into a system called <i>deep</i> and tested the system on artificial and real domains . the experimental results reveal that this system can effectively and efficiently integrate radically different dtds . 
a development environment for an mtt-based sentence generator with the rising standard of the state of the art in text generation and the increase of the number of practical generation applications , it becomes more and more important t o p r o vide means for the maintenance of the generator , i . e . its extension , modiication , and monitoring by gram-marians who are not familiar with its internals . however , only a few sentence and text generators developed to date actually provide these means . one of these generators is kpml ( bate-man , 1997 ) . kpml comes with a development environment and there is no doubt about the contribution of this environment to the popularity of the systemic approach in generation . in the generation project at stuttgart , the realization of a high_quality d e v elopment e n-vironment ( henceforth , de ) has been a central topic from the beginning . the de provides support to the user with respect to writing , modifying , testing , and debugging of ( i ) grammar rules , ( ii ) lexical information , and ( iii ) linguistic structures at diierent l e v els of abstraction . furthermore , it automatically generalizes the organization of the lexica and the grammar . in what follows , we brieey describe de's main features . the theoretical linguistic background of the de is the meaning-text theory ( mel' cuk , 19888 polgu ere , 1998 ) . however , its introduction is beyond the scope of this notee the interested reader is asked to consult the above r e f-erences as well as further literature on the use of mtt in text generation|for instance , ( ior-in mtt , seven levels ( or strata ) of linguistic description are distinguished , of which ve are relevant for generation : semantic ( sem ) , deep_syntactic ( dsynt ) , surface-syntactic ( ssynt ) , deep-morphological ( dmorph ) and surface-morphological ( smorph ) . in order to be able to generate starting from the data in a data base , we i n troduce an additional , the conceptual ( con ) stratum . the input structure to de is thus a conceptual structure ( constr ) derived from the data in the db . the generation process consists of a series of structure mappings between adjacent s t r a t a u n til the smorph stratum is reached . at the smorph stratum , the structure is a string of linearized word forms . the central module of the de is a compiler that maps a structure speciied at one of the ve rst of the above strata on a structure at the adjacent stratum . to support the user in the examination of the internal information gathered during the 
interactive dirt : increasing mobile work performance with a wearable projector-camera system mobile teamwork requires people to maintain good situational awareness ( sa ) about their real_world environments . current mobile_devices are highly portable , but their user_interfaces ( uis ) require too deep of focus of attention to allow their users to use them and simultaneously maintain sa . as a result , some mobile practitioners have little or no access to useful computer-based interactive services . inspired by existing projector-camera systems , this paper studies the feasibility of developing a wearable projector-camera system that enables users to access human_computer_interaction ( hci ) services without negatively affecting their sa . a functional prototype of the "interactive dirt" system was developed using inexpensive commercial_off_the_shelf technologies . a field experiment was conducted as a formative evaluation to test the utility of the prototype under extreme mobile teamwork requirements for sa--military stability and support operations ( saso ) . results show strong potential to increase performance of mobile teams . 
the role of diversity and technology in global virtual_teams this study is an attempt to develop and test a comprehensive model for global_virtual team ( gvt ) effectiveness based on development of collaborative partnership among diverse team members and the moderating role of collaborative_technology and task . the research is an ongoing dissertation work . the conceptual_model is based on traditional i-p-o framework for understanding gvt effectiveness . team diversity in terms of surface level , functional , and deep_level are treated as the central tenet of team inputs . collaborative partnership elements are at the process level , moderated by task and collaborative_technology . at the outcome level , this study is more interested in gvt effectiveness as measured by team performance and individual team member satisfaction . 
road-testing the english resource grammar over the british_national_corpus this paper addresses two questions : ( 1 ) when a large deep_processing resource developed for relatively closed domains is run over open text , what coverage does it have , and ( 2 ) what are the most effective and time-efficient ways of consolidating gaps in the coverage of such as resource ? 
an efficient method to detect periodic_behavior in botnet traffic by analyzing control plane traffic botnets are large networks of bots ( compromised machines ) that are under the control of a small number of bot masters . they pose a significant threat to internet's communications and applications . a botnet relies on command and control ( c2 ) communications channels traffic between its members for its attack execution . c2_traffic occurs prior to any attack; hence , the detection of botnet's c2_traffic enables the detection of members of the botnet before any real harm happens . we analyze c2_traffic and find that it exhibits a periodic_behavior . this is due to the pre-programmed behavior of bots that check for updates to download them every t seconds . we exploit this periodic_behavior to detect c2_traffic . the detection involves evaluating the periodogram of the monitored traffic . then applying walker's large sample test to the periodogram's maximum ordinate in order to determine if it is due to a periodic component or not . if the periodogram of the monitored traffic contains a periodic component , then it is highly likely that it is due to a bot's c2_traffic . the test looks only at aggregate control plane traffic behavior , which makes it more scalable than techniques that involve deep_packet_inspection ( dpi ) or tracking the communication flows of different hosts . we apply the test to two types of botnet , tinyp2p and irc that are generated by slingbot . we verify the periodic_behavior of their c2_traffic and compare it to the results we get on real traffic that is obtained from a secured enterprise network . we further study the characteristics of the test in the presence of injected http background traffic and the effect of the duty_cycle on the periodic_behavior . 
digital techniques for etruscan graves : the etruscanning project etruscanning is a project founded by the european_commission and it focuses on the investigation of new digitization and presentation techniques , in order to recreate the original context of the etruscan graves . several digital techniques have been applied for the stages of digitization , virtual restoration and reconstruction and communication . the possibility of working on two different tombs allows us to deep two specific approaches and to diversify the final real-time applications . this project represents an interesting opportunity to create a concrete link between research and communication in the field of virtual museums , testing the effective impact in terms of cultural transmission , learning and appreciation both in non-linear narrative plots conception and in novel metaphors of interaction . from a technological point_of_view the most innovative result of the project is the implementation of natural interaction interfaces , allowing the public to move and interact with objects inside the virtual_environment . 
responsive neuromodulators based on artificial_neural_networks used to control seizure-like events in a computational model of epilepsy deep_brain_stimulation ( dbs ) has been noted for its potential to suppress epileptic seizures . to date , dbs has achieved mixed results as a therapeutic approach to seizure control . using a computational model , we demonstrate that high-complexity , biologically-inspired responsive neuromodulation is superior to periodic forms of neuromodulation ( responsive and non-responsive ) such as those implemented in dbs , as well as neuromodulation using random and random repetitive-interval stimulation . we configured radial_basis_function ( rbf ) networks to generate outputs modeling interictal time_series recorded from rodent hippocampal slices that were perfused with low mg /high k solution . we then compared the performance of rbf-based interictal modulation , periodic biphasic-pulse modulation , random modulation and random repetitive modulation on a cognitive rhythm generator ( crg ) model of spontaneous seizure-like events ( sles ) , testing efficacy of sle control . a statistically_significant improvement in sle mitigation for the rbf interictal modulation case versus the periodic and random cases was observed , suggesting that the use of biologically-inspired neuromodulators may achieve better results for the purpose of electrical control of seizures in a clinical setting . 
development of multi-stack process on wafer-on-wafer ( wow ) the multi-stack process on wafer-on-wafer ( wow ) has been developed . in order to realize the multi-stacked wafer with ultra thinned wafer of less than 10 m with adhesive polymer , several processes have been optimized . the wafer thickness after back-grinding was controlled within the total thickness variation ( ttv ) of 1 . 2 m on wafer_level of 8inch . for the side wall of though silicon vias ( tsv ) , sin film with low deposition temperature of 150 c has been developed and applied for tsv process without degradation for electrical characteristics . the uniformity of cu electro-plating has been improved that the overburdened cu from the sueface was decreased from 13 . 3 m to 0 . 7 m by optimizing plating solution . the cmp process following cu electro-plating has been customized for the high rate of 5 m/min . finally , the stacked wafer has been evaluated for thermal cycle test ( tct ) of 100 cycles with-65 to 150 c . the result showed that there was no degradation for packaging process . introduction although there are various methods reported for three dimensional integration ( 3di ) of semiconductor devices , the production cost remains as a big issue . therefore , it is necessary to use not the state-of-the art technology but the conventional facilities and technology . in addition , the production yield must be so high that technological difficulty for production processes has to be set as low as possible . in order to reduce the issues shown above , the aspect ratio of tsv has to be as small as possible . in this paper , the thickness of stack-wafer was thinned down to less than 10 m . the thinned wafer was stacked on a base wafer with an adhesive polymer following tsv formation with cu filling for interconnection . the tsv was formed by deep_reactive_ion_etching ( drie ) process and the via diameter was 10 m . the aspect ratio of the tsv was just 1 . 5 that was not difficult for production . the side-wall dielectric film was deposited and formed by pe-cvd and rie respectively . the cu filling and redistribution layer ( rdl ) were formed simultaneously by damascene process . this tsv formation process was very close to beol process , which contributed to both production cost and yield . we call the process " wafer-on-wafer ( wow ) " . by repeating the wow process , multi-wafer-stack was finally achieved , which lead to realize high_density 3d integration with production worthy process [1-3] . the key technologies for wow process were categorized into four parts; 1 ) wafer-thinning , 2 ) wafer stacking [4] , 3 ) tsv formation with 
cell-level temperature distributions in skeletal_muscle post spinal_cord_injury as related to deep tissue injury deep tissue injury ( dti ) is a severe pressure ulcer , which initiates in skeletal_muscle tissue under intact skin . patients with spinal_cord_injury ( sci ) are especially vulnerable to dti , due to their impaired motosensory capacities . the underlying mechanisms that lead to dti are , however , still poorly_understood . this study focuses on cell-level temperature distributions in muscles of patients with sci , which typically contain thinner muscle_fibers and fewer capillaries . it has been shown previously by our group that ischemic muscles of rat models of dti cool down mildly and locally , which is very likely to slow the diffusivity of metabolites in the ischemic regions . however , it is unclear how these temperature decreases affect diffusivity at the scale of individual muscle_cells in the microanatomy of sci patients . we hypothesize that a 2 degrees c drop in the temperature of inflowing capillary blood , as shown in our animal studies , has a substantial effect on lowering the diffusivity of metabolites in skeletal_muscle , but the pathological microanatomy in the chronic phase of sci is less dominant in affecting the local temperatures in and around muscle_cells . in order to test this hypothesis , two-dimensional finite_element ( fe ) models of cross sections through the microanatomy of muscle_tissue were developed using comsol multiphysics software for normal and sci muscles . the models included muscle_cells , extracellular_matrix ( ecm ) , and capillaries , each with its own geometrical , thermal , and heat production properties . the sci model configuration specifically included reduced cross_section of myofibrils in favor of more ecm , less capillaries , and decreased blood inflow rate . after a_20-s heat_transfer simulation , it was found that temperatures around the cells of the sci muscle were approximately 2 degrees c lower than that in the normal muscle , that is , heat production from the muscle cell metabolism did not compensate for the lower inflowing blood temperature in the sci model . we conclude that the temperature and rate of inflowing capillary blood are the dominant factors determining the localized temperatures in the microarchitecture of an ischemic sci muscle_tissue . the altered sci microanatomy was shown to be less influential . taken together with the stokes-einstein theory , our results indicate that diffusivity of metabolites would be approximately 50% less around the cells of sci muscle due to local cooling , which is yet another factor compromising tissue viability in the patients with sci . 
an adaptive and predictive respiratory motion model for image-guided interventions : theory and first clinical application this paper describes a predictive and adaptive single parameter motion model for updating roadmaps to correct for respiratory motion in image-guided interventions . the model can adapt its motion estimates to respond to changes in breathing pattern , such as deep or fast breathing , which normally would result in a decrease in the accuracy of the motion estimates . the adaptation is made possible by interpolating between the motion estimates of multiple submodels , each of which describes the motion of the target organ during cycles of different amplitudes . we describe a predictive technique which can predict the amplitude of a breathing cycle before it has finished . the predicted amplitude is used to interpolate between the motion estimates of the submodels to tune the adaptive model to the current breathing pattern . the proposed technique is validated on affine motion models formed from cardiac magnetic_resonance_imaging ( mri ) datasets acquired from seven volunteers and one patient . the amplitude prediction technique showed errors of 1 . 9-6 . 5 mm . the combined predictive and adaptive technique showed 3-d motion prediction errors of 1 . 0-2 . 8 mm , which represents an improvement in modelling performance of up to 40% over a standard nonadaptive single parameter motion model . we also applied the combined technique in a clinical setting to test the feasibility of using it for respiratory motion correction of roadmaps in image-guided cardiac catheterisations . in this clinical case we show that 2-d registration errors due to respiratory motion are reduced from 7 . 7 to 2 . 8 mm using the proposed technique . 
decision-theoretic reenement planning : principles and application we present a general theory of action abstraction for reducing the complexity of decision-theoretic planning . we develop projection rules for abstract actions and prove our abstraction techniques to be correct . we present a planning algorithm that uses the abstraction theory to eeciently explore the space of possible plans by eliminating suboptimal classes of plans without explicitly examining all plans in those classes . an instance of the algorithm has been implemented as the drips decision-theoretic reenement planning_system . we apply the planner to the problem of selecting the optimal test/treat strategy for managing patients suspected of having deep_vein_thrombosis of the lower extremities . we show that drips signiicantly outperforms a standard branch_and_bound decision_tree evaluation algorithm on this domain . we would like to thank charles kahn for pointing us to the dvt application . 
an inexact newton_method combined with hestenes multipliers' scheme for the solution of karush-kuhn-tucker systems in this work a newton interior point method for the solution of karush kuhn tucker systems is presented . a crucial feature of this iterative_method is the solution , at each iteration , of the inner subproblem . this subproblem is a linear quadratic_programming problem , that can solved approximately by an inner iterative_method such as the hestenes multipliers' method . a deep analysis on the choices of the parameters of the method ( perturbation and damping parameters ) has been done . the global convergence of the newton interior point method is proved when it is viewed as an inexact newton_method for the solution of non-linear systems with restriction on the sign of some variables . the newton interior point method is numerically evaluated on large_scale test_problems arising from elliptic optimal_control problems which show the effectiveness of the approach . 
web_based medical teaching using a multi_agent_system web_based teaching via intelligent_tutoring_systems ( itss ) is considered as one of the most successful enterprises in artificial_intelligence . indeed , there is a long list of itss that have been tested on humans and have proven to facilitate learning , among which we may find the well-tested and known tutors of algebra , geometry , and computer languages . these itss use a variety of computational paradigms , as production_systems , bayesian_networks , schema-templates , theorem proving , and explanatory reasoning . the next generation of itss are expected to go one step further by adopting not only more intelligent interfaces but will focus on integration . this article will describe some particularities of a tutoring system that we are developing to simulate conversational dialogue in the area of medicine , that enables the integration of highly heterogeneous sources of information into a coherent knowledge_base , either from the tutor's point_of_view or the development of the discipline in itself , i . e . the system's content is created automatically by the physicians as their daily work goes on . this will encourage students to articulate lengthier answers that exhibit deep reasoning , rather than to deliver straight tips of shallow knowledge . the goal is to take advantage of the normal functioning of the health_care units to build on the fly a knowledge_base of cases and data for teaching and research purposes . 
monte_carlo tree_search : long_term versus short_term planning in this paper we investigate the use of monte_carlo tree_search ( mcts ) on the physical travelling_salesman_problem ( ptsp ) , a real-time game where the player navigates a ship across a map full of obstacles in order to visit a series of waypoints as quickly as possible . in particular , we assess the algorithm's ability to plan ahead and subsequently solve the two major constituents of the ptsp : the order of waypoints ( long_term planning ) and driving the ship ( short_term planning ) . we show that mcts can provide better results when these problems are treated separately : the optimal order of cities is found using branch & bound and the ship is navigated to collect the waypoints using mcts . we also demonstrate that the physics of the ptsp game impose a challenge regarding the optimal order of cities and propose a solution that obtains better results than following the tsp route of minimum euclidean_distance . i . introduction games haven always been a popular benchmark for testing new techniques in computational_intelligence . real-time ( video ) games have become increasingly popular in recent years and many competitions are held at international conferences every year where competitors from different areas of research compete to be the best . video_games tend to be very complex and players must solve a wide range of problems , often in very little time , to make progress . to better understand these requirements , it is useful to examine some of the characteristics of such games in a simplified framework . in this paper we focus on a simple single-player real-time game : the physical travelling_salesman_problem ( ptsp ) requires the player to navigate a ship in real-time across a map filled with obstacles to collect a series of waypoints as quickly as possible . despite its simplicity , the ptsp is representative of the numerous challenges a player faces in more complex video_games , such as real-time constraints , continuous state_spaces and open-endedness . the ptsp lacks the presence of an opponent and hence one is able to plan ahead without having to worry about the actions carried out by the adversary . however , this does not make it trivial : the ptsp is a real-time game where the action to execute must be chosen quickly . hence , it is usually not possible to plan the entire gameplay at the early_stages of the game . furthermore , the search_space may simply be too big to perform deep searches . in many cases one has 
ship_squat in non-uniform water_depth the problem of predicting ship_squat in non-uniform water_depth is studied in this paper . for transverse depth variations , calculations are done using slender-body shallow_water theory , as implemented in the code " shallowflow " . examples are given for realistic ships transiting dredged channels , and the effect of channel width on ship_squat is discussed . further examples are given for ships transiting canals such as the new panama canal . it is found that in a typical dredged channel , midship squat can be in the order of 20% larger than in open water of the same depth , while dynamic trim is essentially unchanged . in canals such as the new panama canal , midship sinkage can be 100% larger than in open water of the same depth . 1 . introduction many port approaches utilize dredged channels , with shallow_water either side of a deep channel . this has implications for ship under-keel clearance , as transverse depth restrictions tend to increase ship_squat over its uniform-depth value . the effect is magnified further in wall-sided canals . as an example , model tests of guliev [1] showed an increase of 20% for squat in a dredged channel , and an increase of 150% for squat in a canal . in this article , we discuss the mechanisms of increased ship_squat in confined water , and give example calculations for realistic test_cases . 2 . theory the theory used here is based on the slender-body shallow_water theory of tuck [2] for open water; tuck [3] for canals and beck et al . [4] for dredged channels . minor changes to these theories have been made to make them more applicable to modern transom-stern ships , and the methods have been extended to cater to arbitrary transverse bathymetry , as described in [5] . the ship inputs are simply the waterline breadth and section area curve , so there is no need to mesh the hull . for commercial ships whose lines plan is confidential , the required inputs may be estimated based on representative standard series ships , modified based on information from the ship's trim and stability book , as described in [6] . validation has been done using containership model tests in rectangular and non-rectangular canals [7]; bulk_carrier model tests in wide canals [8]; containerships at full-scale [9] , [10]; and bulk carriers at full-scale [11] . the available experimental_data has been used to develop empirical corrections to the theoretical methods , following the icorels procedure adopted by pianc [12] . the resulting methods are implemented 
identifying and discriminating seismic patterns leading ank eruptions at mt . etna volcano during 1981 1996 seismicity affecting mt . etna volcano ( italy ) has been investigated in order to identify and discriminate seismic patterns precursory to ank eruptions . an intense period ( 1981 1996 ) of seismicity and volcanism , during which eight ank eruptions occurred has been considered . two statistical_methods are used : mean hypothesis_testing and entropic decision_trees . the results of the two methods are consistent and reveal a pattern of`deep' and`western' events , prior to the ank eruptions that can be used as a predictive tool as well as a physical modeling constraint . 
a task_specific approach for crawling the deep web there is a great amount of valuable information on the web that cannot be accessed by conventional crawler engines . this portion of the web is usually known as the deep web or the hidden_web . most probably , the information of highest value contained in the deep web , is that behind web forms . in this paper , we describe a prototype hidden-web_crawler able to access such content . our approach is based on providing the crawler with a set of domain definitions , each one describing a specific data-collecting task . the crawler uses these descriptions to identify relevant query forms and to learn to execute queries on them . we have tested our techniques for several real_world tasks , obtaining a high degree of effectiveness . i . introduction crawlers are software_programs that automatically traverse the web , retrieving pages to build a searchable index of their content . conventional crawlers receive as input a set of "seed" pages and recursively obtain new ones by locating and traversing their outbound links . crawling techniques have led the construction of highly successful commercial web_search_engines . nevertheless , conventional web crawlers cannot access to a significant fraction of the web , which is usually called the " hidden_web " or the " deep_web " . the problem of crawling the " hidden_web " can be divided into two challenges : -crawling the " server-side " hidden_web . many websites offer query forms to access the contents of an underlying database . conventional crawlers cannot access these pages because they do not know how to execute queries on those forms . -crawling the " client_side " hidden_web . many websites use techniques such as client_side scripting languages and session maintenance mechanisms . most conventional crawlers are unable to handle this kind of pages . several works have tried to characterize the hidden_web [4] , 
a comparison of broad versus deep auditory menu structures objective the primary purpose of this experiment was to gain a greater understanding of the utilization of working_memory when interacting with a speech-enabled interactive_voice_response ( ivr ) system . background a widely promoted guideline advises limiting ivr menus to five or fewer items because of constraints of the human memory system , commonly citing miller's ( 1956 ) paper . the authors argue that miller's paper does not , in fact , support this guideline . furthermore , applying modern theories of working memory leads to the opposite conclusion--that reducing menu length by creating a deeper structure is actually more demanding of users' working memories and leads to poorer performance and satisfaction . method participants took a working_memory capacity test and then attempted to complete a series of e-mail tasks using one of two ivr designs ( functionally equivalent , but one with a broad menu structure and the other with a deep_structure ) . results users of the broad-structure ivr performed better and were more satisfied than users of the deep_structure ivr . furthermore , this effect was more pronounced for those with low working_memory capacity . conclusion results indicate that creating a deeper structure is more demanding of working memory resource than the alternative of longer , shallower menus . application this experiment has important practical implications for all systems with auditory menus ( particularly ivrs ) because it provides empirical_evidence refuting a widely promoted design practice . 
precision passive alignment of wafers precision passive alignment of wafers several macro-scale bench level experiments were carried out to evaluate the alignment repeatability that can be obtained through the elastic averaging principle . based on these results , a precision passive alignment technique for wafer_bonding application was developed . wafer integral features that allow two stacked wafers to self-align were designed , fabricated and tested for wafer alignment repeatability and accuracy . testing has demonstrated sub-micrometer repeatability and accuracy can be held using the proposed technique on 4 inch wafers . passive alignment of the wafers is achieved when convex pyramids , supported on flexural cantlievers , and concave v-grooves patterned on the edges of the wafer engage and are preloaded . a silicon cantilever beam flexure between one of the wafers and the pyramid provides compliance to the coupling to avoid strain on the wafers and allows the surfaces of the wafers to mate . both the concave coupling features and the convex coupling features are bulk microma-chined through wet anisotropic etch ( koh ) . the convex features are then release etched through a backside deep_reactive_ion etch ( drie ) . as part of the fabrication process development , tests were performed to optimize the convex corner compensating mask structures needed to create the pyramid shaped convex coupling structures . testing has shown that patterning two pairs of features on each of the four sides of the wafer is enough to achieve sub-micrometer repeatability . 
building predictive models on complex symbolic sequences with a second-order recurrent bcm network with lateral_inhibition activation_patterns across recurrent units in recurrent_neural_networks ( rnns ) can be thought of as spatial codes of the history of inputs seen so far . when trained on symbolic sequences to perform the next-symbol prediction , rnns tend to organize their state_space so that \close" recurrent activation vectors correspond to histories of symbols yielding similar next-symbol distributions 1] . this leads to simple nite-context predictive models built on top of recurrent activations by grouping close activation_patterns via a vector_quantization . in this paper we investigate an unsu-pervised alternative to the state space organization . in particular , we use a recurrent version of the bienenstock , cooper and munro ( bcm ) network with lateral_inhibition 2] to map histories of symbols into activations of the recurrent layer . recurrent bcm networks perform a kind of time-conditional projection pursuit . we compare the nite-context models built on top of bcm recurrent activations with those constructed on top of rnn recurrent activation vectors . as a test_bed we use two complex symbolic sequences with rather deep memory structures . surprisingly , the bcm-based_model has a comparable or better performance than its rnn-based counterpart . this can be explained by the familiar information latching problem in recurrent_networks when longer time spans are to be latched 3 , 4] . 
bio-inspired grasp control in a robotic hand with massive sensorial input the capability of grasping and lifting an object in a suitable , stable and controlled way is an outstanding feature for a robot , and thus far , one of the major problems to be solved in robotics . no robotic tools able to perform an advanced control of the grasp as , for instance , the human hand does , have been demonstrated to date . due to its capital importance in science and in many applications , namely from biomedics to manufacturing , the issue has been matter of deep scientific investigations in both the field of neurophysiology and robotics . while the former is contributing with a profound understanding of the dynamics of real-time control of the slippage and grasp force in the human hand , the latter tries more and more to reproduce , or take inspiration by , the nature's approach , by means of hardware and software technology . on this regard , one of the major constraints robotics has to overcome is the real-time processing of a large amounts of data generated by the tactile sensors while grasping , which poses serious problems to the available computational power . in this paper a bio-inspired approach to tactile data_processing has been followed in order to design and test a hardware-software robotic architecture that works on the parallel_processing of a large amount of tactile_sensing signals . the working principle of the architecture bases on the cellular nonlinear/neural_network ( cnn ) paradigm , while using both hand shape and spatial-temporal features obtained from an array of microfabricated force sensors , in order to control the sensory-motor_coordination of the robotic system . prototypical grasping tasks were selected to measure the system performances applied to a computer-interfaced robotic hand . successful grasps of several objects , completely unknown to the robot , e . g . soft and deformable objects like plastic bottles , soft balls , and japanese tofu , have been demonstrated . 
diagnostics and a qualitative model first generation expert_systems were using shallow knowledge based on heuristic information to solve a diagnostic problem . this approach has many disadvantages , which can be avoided by using deep_knowledge . diagnostic reasoning based on deep_knowledge is called model_based diagnostics . recently , the use of qualitative modeling in relation to deep_knowledge in expert_systems has become increasingly_important . the main purpose of our contribution is to present the model_based diagnostic approach at a formal level . the originality of the presented formalization is the concept of the diagnostic space , the characterization of the minimal diagnoses , and the measurement . the formalization serves as the theoretical background to prove our view to the design of qualitative system models and to establish the diagnostic architecture called disy . the qualitative system model in our diagnostic approach needs not to be specially adopted for use in the diagnostic domain . the only requirement is that it must simulate the system behavior expressed by normal or abnormal functioning of its components . proposed disy architecture is not complex and simply takes into an account the previous diagnostic result to obtain a new one from the additional observation-measurement ( medical tests or examinations ) of the system . 
an extended metastability simulation_method for synchronizer characterization synchronizers play a key role in multi-clock domain systems on chip . designing reliable synchronizers requires estimating and evaluating synchro-nizer parameters ( resolution time constant ) and ( metastability window ) . typically , evaluation of these parameters has been done by empirical rules of thumb or simple circuit simulations to ensure that the synchronizer mtbf is sufficiently long . this paper shows that those rules of thumb and some common simulation_method are unable to predict correct synchronizer parameters in deep sub-micron technologies . we propose an extended simulation_method to estimate synchronizer characteristics more reliably and compare the results_obtained with other state-of-the-art simulation methods and with measurements of a 65nm lp cmos test_chip . 1 introduction multiple-clock domain system on chip ( soc ) designs require synchronization when transferring signals and data among clock domains and when receiving asynchronous inputs . such synchronizations are susceptible to metastability effects which can cause malfunction in a receiving circuit . in critical designs , this risk must be mitigated . to assess the risk and to design reliable synchronizers , models describing the failure_mechanisms for latches and flip_flops have been developed [ 1][ 2] . most models express the risk of not resolving metastability in terms of the meantime between failures ( mtbf ) of the circuit , eq . ( 1 ) , where s is the time allotted for resolution , and are the receiver and sender clock frequencies , respectively , is the resolution time constant , and is a parameter related to the effective setup-and-hold time window during which the synchronizer is vulnerable to metastability . over the years , techniques have been developed for obtaining an arbitrarily long mtbf . these techniques have been translated into convenient rules of thumb for designers . as digital_circuits have become more complex , denser and faster with
the impact of organization , project and governance variables on software_quality and project success in this paper we present a statistically tested evidence about how quality and success rate are correlated with variables reflecting the organization and aspects of its project's governance , namely retrospectives and metrics . the results presented in this paper are based on the agile projects governance survey that collected 129 responses . this paper discuss the deep analysis of this survey , and the main findings_suggest that when applying agile_software_development , the quality of software improves as the organization measures customer_satisfaction more frequently , and as the impact of retrospective increases . project success improves as quality , frequency of measuring customer_satisfaction , organization experience in agile development , retrospective impact , the team participation in retrospective and the team contribution to retrospective , increases . 
tracking real-time user_experience ( true ) : a comprehensive instrumentation solution for complex systems automatic recording of user_behavior within a system ( instrumentation ) to develop and test theories has a rich history in psychology and system design . often , researchers analyze instrumented behavior in isolation from other data . the problem with collecting instrumented behaviors without attitudinal , demographic , and contextual data is that researchers have no way to answer the 'why' behind the 'what' . we have combined the collection and analysis of behavioral instrumentation with other hci methods to develop a system for tracking real-time user_experience ( true ) . using two case_studies as examples , we demonstrate how we have evolved instrumentation methodology and analysis to extensively improve the design of video_games . it is our hope that true is adopted and adapted by the broader hci community , becoming a useful tool for gaining deep insights into user_behavior and improvement of design for other complex systems . 
deformations of ic structure in test and yield_learning this paper argues that the existing approaches to modeling and characterization of ic malfunctions are inadequate for test and yield_learning of deep_sub_micron ( dsm ) products . traditional notions of a spot defect and local and global process_variations are analyzed and their shortcomings are exposed . a detailed taxonomy of process-induced deformations of dsm ic structures , enabling modeling and characterization of ic malfunctions , is proposed . the blueprint of a roadmap enabling such a characterization is suggested . 
partition based soc test_scheduling with thermal and power constraints under deep_submicron_technologies for core-based system-on-chip ( soc ) testing , conventional power_constrained test_scheduling methods do not guarantee a thermal-safe solution . also , most of the test scheduling schemes make poor assumptions about power_consumption . in deep_submicron_era , leakage_power and wake-up power_consumption can not be neglected . in this paper , we propose a partition based thermal-aware test_scheduling_algorithm with more realistic assumptions of recent socs . in our test_scheduling_algorithm , each test is partitioned and the earliest starting time of each partition is searched . to reduce the execution time of thermal simulation , we also exploit superposition_principle to compute the power and thermal profile rapidly and accurately . we apply our test_scheduling_algorithm to itc'02 soc benchmarks and the results show improvements in the total test time over scheduling schemes without partitioning . 
local optimization method with global multidimensional search for descent this paper presents a new method for solving global_optimization problems . we use a local technique based on the notion of discrete gradients for finding a cone of descent directions and then we use a global cutting angle algorithm for finding global minimum within the intersection of the cone and the feasible region . we present results of numerical experiments with well-known test_problems and with the so-called cluster function . these results confirm that the proposed algorithm allows one to find a global minimizer or at least a deep local minimizer of a function with a huge amount of shallow local_minima . 
delay_testing considering power_supply_noise effects we propose a new delay test_generation technique that can take into account the impact of the power_supply_noise on the signal propagation_delays . this is diierent from existing delay_fault models and test_generation techniques that ignore the dependence of path_delays on the applied test_patterns and cannot capture the worst-case timing scenarios in deep_submicron_designs . in addition to sensitizing the fault and propagating the fault eeects to the primary outputs , our new tests also produce the worst-case power_supply_noise on the nodes in the target path . thus , the tests also cause the worst-case propagation_delay for the nodes along the target path . our experimental results on benchmark_circuits show that the new delay_tests produce signiicantly longer delays on the tested paths compared to the tests derived using existing delay_testing methods . 
deciding confluence of ground term rewrite systems in cubic time it is well known that the confluence property of ground term rewrite systems ( ground trss ) is decidable in polynomial time . for an efficient implementation , the degree of this polynomial is of great interest . the best complexity bound in the literature is given by comon , godoy and nieuwenhuis ( 2001 ) , who describe an o ( n 5 ) algorithm , where n is the size of the ground trs . in this paper we improve this bound to o ( n 3 ) . the algorithm has been implemented in the confluence tool csi . 1 introduction it is well known that confluence of ground trss can be decided in polynomial time . in this paper , we are interested in the degree of the associated polynomial . to derive a polynomial time decision procedure for confluence of ground trss , comon et al . [3] use an approach based on a transformation by plaisted [9] that flattens the trs . then they test deep joinability of sides of rules . the authors sketch an implementation with complexity o ( n 5 ) , where n is the size of the given trs . tiwari [10] and godoy et al . [6] base their approach on a rewrite closure that constructs tree transducers the given trs r is converted into two trss f and b such that f and b 1 are left-flat , right-constant , f is terminating , and * r = * f * b . they then consider top-stabilizable terms to derive conditions for confluence . tiwari obtains a bound of o ( n 9 ) ( but a more careful implementation would end up with o ( n 6 ) ) , while godoy et al . obtain a bound of o ( n 6 ) . the algorithm of [3] is limited to ground trss , but [10] extends the algorithm to certain shallow , linear systems , and [5] treats shallow , linear systems in full generality . 1 in these extensions , however , the exponent depends on the maximum arity of the function symbols of the given trs . in our work we combine ideas from [3 , 10 , 6] in order to improve the complexity bound to o ( n 3 ) . the key ingredients are a plaisted-style rewrite closure , which results in trss f and b of only quadratic size , and top-stabilizability , which is cheaper to test than deep joinability . 1 the same claim can be found in [6] . however , rule splitting , a key step in the proof of their lemma 3 . 1 , only works if left-hand side and right-hand side variables are disjoint for every rule . 
a novel algorithm for color_constancy introduction ' specific clients , is ht clearance center lid directly to ccc or general distribu-ale . abstract color_constancy is the skill by which it is possible to tell the color of an object even under a colored light . i interpret the color of an object as its color under a fixed canonical light , rather than as a surface reflectance function . this leads to an analysis that shows two distinct sets of circumstances under which color_constancy is possible . in this framework , color_constancy requires estimating the illuminant under which the image was taken . the estimate is then used to choose one of a set of linear maps , which is applied to the image to yield a color descriptor at each point . this set of maps is computed in advance . the illuminant can be estimated using image measurements alone , because , given a number of weak assumptions detailed in the text , the color of the illuminant is constrained by the colors observed in the image . this constraint arises from the fact that surfaces can reflect no more light than is cast on them . for example , if one observes a patch that excites the red receptor strongly , the illuminant cannot have been deep_blue . two algorithms are possible using this constraint , corresponding to different assumptions about the world . the first algorithm , crule will work for any surface reflectance . crule corresponds to a form of coefficient rule , but obtains the coefficients by using constraints on illuminant color . the set of illuminants for which crule will be successful depends strongly on the choice of photoreceptors : for narrowband photoreceptors , crule will work in an unrestricted world . the second algorithm , mwext , requires that both surface reflectances and illuminants be chosen from finite dimensional spaces; but under these restrictive conditions it can recover a large number of parameters in the illuminant , and is not an attractive model of human color_constancy . crule has been tested on real images of mondriaans , and works well . i show results for crule and for the retinex algorithm of land ( land 1971; land 1983; land 1985 ) operating on a number of real images . the experimental work shows that for good constancy , a color_constancy system will need to adjust the gain of the receptors it employs in a fashion analagous to adaptation in humans . people experience color as a surface property that is largely unaffected by the color of the illuminating light . this phenomenon 
recent advances in ai planning the past five years have seen dramatic advances in planning_algorithms , with an emphasis on propo-sitional methods such as graphplan and compilers that convert planning_problems into propositional conjunctive_normal_form formulas for solution using systematic or stochastic sat methods . related work , in the context of spacecraft control , advances our understanding of interleaved planning and execution . in this survey , i explain the latest techniques and suggest areas for future_research . t he field of ai planning seeks to build control algorithms that enable an agent to synthesize a course of action that will achieve its goals . although researchers have studied planning since the early days of ai , recent_developments have revolutionized the field . two approaches , in particular , have attracted much attention : ( 1 ) the two-phase graphplan ( blum and furst 1997 ) planning algorithm and ( 2 ) methods for compiling planning_problems into propositional formulas for solution using the latest , speedy systematic and stochastic sat algorithms . these approaches have much in common , and both are affected by recent progress in constraint_satisfaction and search technology . the current level of performance is quite impressive , with several planners quickly solving_problems that are orders_of_magnitude harder than the test pieces of only two years_ago . as a single , representative example , the blackbox planner ( kautz and selman 1998a ) requires only 6 minutes to find a 105-action logistics plan in a world with 10 16 possible states . furthermore , work on propositional planning is closely_related to the algorithms used in the autonomous controller for the national_aeronautics_and_space_administration ( nasa ) deep_space one spacecraft , launched in october 1998 . as a result , our understanding of inter-leaved planning and execution has advanced as well as the speed with which we can solve classical planning_problems . the goal of this survey is to explain these recent_advances and suggest new directions for research . because this article requires minimal ai background ( for example , simple logic and basic search_algorithms ) , it's suitable for a wide audience , but my treatment is not exhaustive because i don't have the space to discuss every active topic of planning research . 1 i progress as follows : the remainder of the introduction defines the planning problem and surveys freely downloadable planner implementations . the next sections discuss graphplan , sat compilation , and interleaved planning and execution . i conclude by quickly mentioning other recent_advances and suggestion topics for future_research . preliminaries a simple " classical " formulation 
tribica : trie bitmap content analyzer for high_speed network intrusion_detection deep_packet_inspection ( dpi ) is often used in network intrusion_detection and prevention systems ( nidps ) , where incoming packet payloads are compared against known attack signatures . processing every single byte in the incoming packet payload has a very stringent time constraint , e . g . , 200 ps for a 40-gbps line . traditional dpi systems either need a large memory space or use special memory such as ternary content_addressable_memory ( tcam ) , limiting parallelism , or yielding high cost/power_consumption . in this paper , we present a high_speed , single_chip dpi scheme that is scalable and configurable through memory updates . the scheme is based on a novel data_structure called tribica ( trie bitmap content analyzer ) , which provides minimal perfect hashing functionality . it uses a trie structure with a hash_function performed at each layer . branching is determined by the hashing results with an objective to evenly partition attack signatures into multiple groups at each layer . during a query , as an input traverses the trie , an address to a table in the memory that stores all attack signatures is formed and is used to access the signature for an exact match . due to the small space required , multiple copies of tribica can be implemented on a single chip to perform pipelining and parallelism simultaneously , thus achieving high_throughput . we have designed the tribica on a modest fpga chip , xilinx virtex ii pro , achieving 10-gbps throughput without using any external memory . a proof-of-concept design is implemented and tested with 1-gbps packet streams . by using today's state-of-the-art fpgas , a throughput of 40 gbps is believed to be achievable . 
automating defects simulation and fault_modeling for srams the continuos improvement in manufacturing_process density for very deep_sub_micron_technologies constantly leads to new classes of defects in memory devices . exploring the effect of fabrication defects in future technologies , and identifying new classes of realistic functional fault_models with their corresponding test_sequences , is a time consuming task up to now mainly performed by hand . this paper proposes a new approach to automate this procedure . the proposed method exploits the capabilities of evolutionary_algorithms to automatically identify faulty behaviors into defective memories and to define the corresponding fault_models and relevant test_sequences . target defects are modeled at the electrical level in order to optimize the results to the specific technology and memory architecture . 
measurement and analysis of variability in cmos_circuits measurement and analysis of variability in cmos_circuits measurement and analysis of variability in cmos_circuits permission to make digital or hard_copies_of all or part of this work for personal or classroom use is granted_without_fee_provided that copies are not made or distributed for profit_or_commercial_advantage_and that copies_bear this notice and the full citation on the first page . to copy otherwise , to republish , to post on servers or to redistribute to lists , requires_prior_specific_permission . abstract measurement and analysis of variability in cmos_circuits the scaling of cmos_technology into the deep sub-micron regime has resulted in increased impact of process variability on circuits , to the point where it is considered a major_bottleneck to further scaling . in order to continue scaling , there is a need to reduce margins in the design by classifying process_variations as systematic or random . in this work , a methodology to characterize variability through measurement and analysis has been developed . systematic and random , die-to-die ( d2d ) and within-die ( wid ) components of variability are quantified and corresponding sources of variability are identified . this methodology was developed for an early 90nm cmos_process and further refined for an early 45nm cmos_process . test-chips have been designed to study the effects of layout , and characterize variability of delay and leakage_current using an array of test_structures . delay is obtained through the measurement of ring_oscillator frequencies , and transistor leakage_current is measured by an on-chip analog_to_digital_converter ( adc ) . 2 in 90nm , it has been found that transistor performance depends strongly on polysil-icon ( poly-si ) gate density and that spatial_correlation depends on gate orientation and the direction of gate spacing . wid variation is small with three standard deviations over mean ( 3 / ) 3 . 5% , whereas d2d and systematic layout-induced variations are significant , with 3 / d2d variation of 15% and a maximum layout-induced frequency shift of 10% . in 45nm , a process which features immersion_lithography , strained-si and more restrictive design rules for gate spacing , it has been found that systematic layout-induced variability has decreased . however , new sources of variability due to the dependence of stress on layout were found . wid has increased to 3 / 6 . 6% and can be attributed to a smaller transistor area whereas d2d variation has remained at 3 / 15% . this methodology is effective in characterizing variability . it improves the accuracy of statistical models and allows process corners to be set up for wid or d2d variations . in addition , sources of systematic variations are 
learning_styles and approaches to learning among medical undergraduates and postgraduates background the challenge of imparting a large amount of knowledge within a limited time period in a way it is retained , remembered and effectively interpreted by a student is considerable . this has resulted in crucial changes in the field of medical education , with a shift from didactic teacher centered and subject based teaching to the use of interactive , problem based , student centered learning . this study tested the hypothesis that learning_styles ( visual , auditory , read/write and kinesthetic ) and approaches to learning ( deep , strategic and superficial ) differ among first and final year undergraduate medical students , and postgraduates medical trainees . methods we used self administered vark and assist questionnaires to assess the differences in learning_styles and approaches to learning among medical undergraduates of the university of colombo and postgraduate trainees of the postgraduate institute of medicine , colombo . results a total of 147 participated : 73 ( 49 . 7% ) first year students , 40 ( 27 . 2% ) final year students and 34 ( 23 . 1% ) postgraduate students . the majority ( 69 . 9% ) of first year students had multimodal learning_styles . among final year students , the majority ( 67 . 5% ) had multimodal learning_styles , and among postgraduates , the majority were unimodal ( 52 . 9% ) learners . among all three groups , the predominant approach to learning was strategic . postgraduates had significant higher mean scores for deep and strategic approaches than first years or final years ( p < 0 . 05 ) . mean scores for the superficial approach did not differ significantly between groups . conclusions the learning_approaches suggest a positive shift towards deep and strategic learning in postgraduate students . however a similar difference was not observed in undergraduate_students from first year to final year , suggesting that their curriculum may not have influenced learning methodology over a five year period . 
minimally supervised domain-adaptive parse reranking for relation extraction the paper demonstrates how the generic parser of a minimally supervised information_extraction framework can be adapted to a given task and domain for relation extraction ( re ) . for the experiments a generic deep_linguistic parser was employed that works with a largely hand-crafted head_driven_phrase_structure_grammar ( hpsg ) for english . the output of this parser is a list of n best parses selected and ranked by a maxent parse-ranking component , which had been trained on a more or less generic hpsg treebank . it will be shown how the estimated confidence of re rules learned from the n best parses can be exploited for parse reranking . the acquired rerank-ing model improves the performance of re in both training_and_test phases with the new first parses . the obtained significant boost of recall does not come from an overall gain in parsing performance but from an application-driven selection of parses that are best suited for the re task . since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation , generic parsing accuracy actually decreases . the novel method for task_specific parse reranking does not require any annotated data beyond the semantic seed , which is needed anyway for the re task . 
hybrid renewable energy systems for a dynamically_positioned buoy hybrid renewable energy systems for a dynamically_positioned buoy hybrid renewable energy systems for a dynamically_positioned buoy ii we the undersigned committee hereby approve the attached thesis to ease the burdens associated with deep ocean buoy moorings , a relatively recent technological development known as dynamic_positioning ( dp ) could be employed . this method , which is being used on some oil drilling ships and semi_submersible platforms , provides pinpoint positioning for precision drilling and other operations with the use of multiple , multi-axis , thrusters below the waterline of a vessel to counter the effects of winds and ocean currents . this eliminates the need for anchoring in deep oceans , but depending on the characteristics of the vessel and environmental conditions , power requirements for dp tend to be quite substantial and costly . a theoretical design of a hybrid wind and solar_energy system on an ocean surface buoy is made for the purpose of powering a low cost , simple , dynamic_positioning system . this system was implemented on a dynamically positioned buoy ( dpb ) intended for sea keeping using renewable_energy_sources . iv some prototypes of autonomous surface vehicles have experimented with renewable energies as a source of supplemental power , but these vehicles are typically designed as transient surface vehicles with station keeping capability as a secondary function . a combination of design requirements set forth by 2004 defense_advanced_research_projects_agency ( darpa ) solicitation number baa04-33 and 2008 solicitation number darpasn08-45 are used as a basis for dpb design parameters . the aims of the dpb design thesis are to develop and test a low cost , dynamic_positioning system that will continuously maintain a 250 m watch radius and to present a theoretical hybrid renewable energy system to power it , thereby improving on the station keeping buoy ( skb ) energy balance problem . global positioning system ( gps ) technology , combined with an 8-bit embedded microcontroller and circuitry provide sufficient autonomous control signals to independent thrusters below the waterline . these correct for position offsets caused by sea and air currents in the open ocean . the results of the system in a 2 . 5 m s-1 wind validated the feasibility of mounting a horizontal axis wind_turbine on a buoy without a necessary counter balancing device . a hybrid wind and solar renewable energy system was designed using [54] . the 100% power load of about 1280 total watts proved too substantial to warrant the practicality of this renewable energy system in three out of four simulations . one optimization , however , was able to produce an annual capacity v 
bootstrapping from game_tree search in this paper we introduce a new algorithm for updating the parameters of a heuris-tic evaluation function , by updating the heuristic towards the values computed by an alpha_beta search . our algorithm differs from previous approaches to learning from search , such as samuel's checkers player and the td-leaf algorithm , in two key ways . first , we update all nodes in the search tree , rather than a single node . second , we use the outcome of a deep search , instead of the outcome of a subsequent search , as the training signal for the evaluation function . we implemented our algorithm in a chess program meep , using a linear heuristic function . after initialising its weight vector to small random values , meep was able to learn high_quality weights from self-play alone . when tested online against human opponents , meep played at a master level , the best performance of any chess program with a heuristic learned entirely from self-play . 
modeling and analysis of crosstalk_coupling effect on the victim interconnect using the abcd network model the paper describes an abcd modeling approach of a victim interconnect that takes_into_account the crosstalk_coupling effect due to aggressor line in deep sub-micron chips . after the order reduction the crosstalk model is utilized for the analysis of crosstalk_coupling effect on the victim's output signal . various timing issues related to signal waveform such as , delay time , overshoot and undershoot occurrence time etc . , that in effect help to ensure in prior the desired signal_integrity ( si ) and performance reliability of the socs , can be estimated analytically using the reduced order crosstalk model . it has been observed that the crosstalk_coupling effect introduces the delay in the victim's output signal which can be significant enough or even unacceptable if many aggressors simultaneously couple energy to the victim line , or the line spacing between the aggressor and victim is reduced due to under-etching or even , length of the victim interconnect is increased because of improper layouts / routing . influences of other interconnect parasitics on the victim s output signal can also be tested using the same model . simulation_results obtained with our reduced order model is found to be quite good and comparable to the accuracy of the pspice simulation . 
cpa and cca-secure encryption systems that are not 2-circular secure traditional definitions of encryption guarantee security for plaintexts which can be derived by the adversary . in some settings , such as anonymous credential or disk encryption systems , one may need to reason about the security of messages potentially unknown to the adversary , such as secret keys encrypted in a self-loop or a cycle . a public_key cryptosystem is n-circular secure if it remains secure when the ciphertexts e are revealed , for independent key pairs . a natural question to ask is what does it take to realize circular security in the standard_model ? are all cpa-secure ( or cca-secure ) cryptosystems also n-circular secure for n > 1 ? one way to resolve this question is to produce a cpa-secure ( or cca-secure ) cryptosystem which is demonstrably insecure for key cycles larger than self-loops . recently and independently , acar , belenkiy , bellare and cash provided a cpa-secure cryptosystem , under the sxdh assumption , that is not 2-circular secure . in this paper , we present a different cpa-secure counterexample ( under sxdh ) as well as the first cca-secure counterexample ( under sxdh and the existence of certain nizk proof systems ) for n > 1 . moreover , our 2-circular attacks recover the secret keys of both parties and thus exhibit a catastrophic_failure of the system whereas the attack in acar et al . provides a test whereby the adversary can distinguish whether it is given a 2-cycle or two random ciphertexts . these negative results are an important step in answering deep questions about which attacks are prevented by commonly-used definitions and systems of encryption . 
system test evaluation and review technique stochastic modeling and optimal_control for system testing of software system test execution process test manager system test execution model system test evaluation and review technique|3 system test evaluation and review technique|4 ' h * [ ( ) ] arg max lim i t e c t t system test evaluation and review technique|2 preface this work is part of my master's_degree program at vu university , where each student is required to perform a research regarding a specific problem motivated by practice . i would like to extend my gratitude to dr . sandjai bhulai whose course in stochastic_optimization instilled in me the belief that it was mathematically feasible to build a model for system test execution and test management . his research , ideas , efforts and enthusiasm have helped me enormously in writing this paper . i am grateful to shunji osaki and hisashi mine for their paper on semi-markov_decision_processes which helped me gain deep insight into the subject . also , i would like to extend my gratitude to salah e elmaghraby for his work on gert and smp's which was enormously helpful for writing this paper . finally , to james a whittaker who pioneered the work on markov_chain modeling of software_testing which in turn became the foundation on which i could perform my research in this area . this paper is my humble effort to extend the research in the field of modeling of software_testing and test management . executive summary system testing of software is defined as the " investigation conducted to evaluate whether a complete and integrated software system complies with its specified requirements " 1 . thus system testing is a process that requires creation of test_cases for every function point of the software and execution of the test_cases to validate whether the function point conforms to the specified requirements . in case of failure of a test , a defect is logged that is fixed by the development team and again re-tested . the system testing_process can therefore be described by the following steps : ( a ) requirements_analysis , ( b ) test estimation and strategy ( c ) test planning ( d ) creation of test scripts based on requirements ( e ) execution of test scripts on the software product ( f ) reporting of defects ( g ) retesting of fixed defects and ( h ) test closure the system testing_process described in steps ( e ) - ( g ) is a cyclic process and requires allocation of resources ( software testers ) to complete testing in allocated time . however , as businesses increasingly tend to reduce the time to market of their products and services , coupled with 
analytical semi-empirical model for ser sensitivity estimation of deep_submicron cmos_circuits an analytical expression is proposed for the estimation of the soft_error_rate ( ser ) sensitivity of circuits designed in deep-submicron cmos technologies . the model parameters for a given technology and for a specific radiation type have been determined by combining experimental accelerated ser test_results with critical charge data obtained from circuit simulations . the resulting analytical models are discussed for the cases of the alpha-induced ser of a 0 . 18 m process and for both the alpha-and neutron-induced ser of a 0 . 13 m process . the results indicate that the approach provides an efficient means to predict the contributions of individual nodes to the ser of a circuit . the method is shown to be effective in the evaluation of the impact of design modifications on the circuit ser . 
i for cosmic_ray soft errors in semiconductor memories this paper presents a review of experiments performed by ibm to investigate the causes of soft errors in semiconductor_memory chips under field test conditions . the effects of alpha_particles and cosmic_rays are separated by comparing multiple measurements of the soft_error_rate ( ser ) of samples of memory chips deep underground and at various altitudes above the earth . the results of case_studies on four different memory chips show that cosmic_rays are an important source of the ionizing_radiation that causes soft_errors . the results of field_testing are used to confirm the accuracy of the modeling and the accelerated testing of chips . 
on-chip cache hierarchy-aware tile_scheduling for multicore machines iteration space tiling and scheduling is an important technique for optimizing loops that constitute a large fraction of execution times in computation kernels of both scientific codes and embedded applications . while tiling has been studied extensively in the context of both uniprocessor and multiprocessor platforms , prior research has paid less attention to tile_scheduling , especially when targeting multicore machines with deep on-chip cache hierarchies . in this paper , we propose a cache hierarchy-aware tile_scheduling algorithm for multicore machines , with the purpose of maximizing both horizontal and vertical data reuses in on-chip caches , and balancing the workloads across different cores . this scheduling_algorithm is one of the key components in a source-to-source translation tool that we developed for automatic loop parallelization and multithreaded code generation from sequential codes . to the best of our knowledge , this is the first effort that develops a fully-automated tile_scheduling strategy customized for on-chip cache topologies of multicore machines . the experimental results collected by executing twelve application programs on three commercial intel machines ( nehalem , dunnington , and harpertown ) reveal that our cache-aware tile_scheduling brings about 27 . 9% reduction in cache misses , and on average , 13 . 5% improvement in execution times over an alternate method tested . 
prediction of pharmacologically induced baroreflex sensitivity from local time and frequency_domain indices of r-r interval and systolic_blood_pressure signals obtained during deep_breathing pharmacological measurement of baroreflex sensitivity ( brs ) is widely accepted and used in clinical practice . following the introduction of pharmacologically induced brs ( p-brs ) , alternative assessment methods eliminating the use of drugs were in the center of interest of the cardiovascular research_community . in this study we investigated whether p-brs using phenylephrine injection can be predicted from non-pharmacological time and frequency_domain indices computed from electrocardiogram ( ecg ) and blood_pressure ( bp ) data acquired during deep_breathing . in this scheme , ecg and bp data were recorded from 16 subjects in a two-phase experiment . in the first phase the subjects performed irregular deep breaths and in the second phase the subjects received phenylephrine injection . from the first phase of the experiment , a large pool of predictors describing the local characteristic of beat-to-beat interval tachogram ( rr ) and systolic_blood_pressure ( sbp ) were extracted in time and frequency domains . a subset of these indices was selected using twelve subjects with an exhaustive search fused with a leave one subject out cross_validation procedure . the selected indices were used to predict the p-brs on the remaining four test subjects . a multivariate regression was used in all prediction steps . the algorithm achieved best prediction accuracy with only two features extracted from the deep_breathing data , one from the frequency and the other from the time domain . the normalized l2-norm error was computed as 22 . 9% and the correlation_coefficient was 0 . 97 ( p=0 . 03 ) . these results_suggest that the p-brs can be estimated from non-pharmacological indices computed from ecg and invasive bp data related to deep_breathing . 
memory testing under different stress_conditions : an industrial evaluation this paper presents the effectiveness of various stress_conditions ( mainly voltage and frequency ) on detecting the resistive_shorts and open defects in deep_sub_micron embedded_memories in an industrial environment . simulation studies on very-low_voltage , high voltage and at-speed testing show the need of the stress_conditions for high_quality products; i . e . , low defect-per-million ( dpm ) level , which is driving the semiconductor market today . the above test conditions have been validated to screen out bad devices on real silicon ( a test-chip ) built on cmos 0 . 18 um technology . ifa ( inductive fault analysis ) based simulation technique leads to an efficient fault coverage and dpm estimator , which helps the customers upfront to make decisions on test algorithm implementations under different stress_conditions in order to reduce the number of test escapes . 
recurrent_neural_networks with limited numerical precision recurrent_neural_networks ( rnns ) produce state-of-art performance on many machine_learning tasks but their demand on resources in terms of memory and computational power are often high . therefore , there is a great interest in optimizing the computations performed with these models especially when considering development of specialized low_power hardware for deep_networks . one way of reducing the computational needs is to limit the numerical precision of the network weights and biases , and this will be addressed for the case of rnns . we present results from the use of different stochastic and deterministic reduced precision training methods applied to two major rnn types , which are then tested on three datasets . the results show that the stochastic and deterministic ternarization , pow2-ternarization , and exponential quantization methods gave rise to low-precision rnns that produce similar and even higher_accuracy on certain datasets , therefore providing a path towards training more efficient implementations of rnns in specialized hardware . 
scan design and ac test we propose a novel design_for_testability technique to apply two pattern_tests for path_delay_fault_testing . due to stringent timing requirements of deep-submicron vlsi chips , design_for_test schemes have to be tailored for detecting stuck-at as well as delay_faults quickly and efficiently . existing techniques such as enhanced scan add substantial hardware_overhead , whereas techniques such as scan-shifting or functional justification make the test_generation process complex and produce lower coverage for scan_based_designs as compared to non-scan designs . we exploit the characteristics of cmos circuitry to enable the application of two-pattern_tests . the proposed technique reduces the problem of path_delay_fault_testing for scan_based_designs to that of path_delay_fault_testing with complete accessibility to the combinational_logic , and has minimal area_overhead . the scheme also provides significant reduction in power during scan operation . 
svd-based ghost circuitry detection ghost circuitry ( gc ) insertion is the malicious addition of hardware in the specification and/or implementation of an ic by an attacker intending to change circuit functionality . there are numerous gc insertion sources , including untrusted foundries , synthesis tools and libraries , testing and verification tools , and configuration scripts . moreover , gc attacks can greatly compromise the security_and_privacy of hardware users , either directly or through interaction with pertinent systems , application_software , or with data . gc detection is a particularly difficult_task in modern and pending deep_submicron_technologies due to intrinsic manufacturing variability . here , we provide algebraic and statistical approaches for the detection of ghost circuitry . a singular value decomposition ( svd ) -based technique for gate characteristic recovery is applied to solve a system of equations created using fast and non-destructive measurements of leakage_power and/or delay . this is then combined with statistical constraint manipulation techniques to detect embedded ghost circuitry . the effectiveness of the approach is demonstrated on the iscas 85 benchmarks . 
website identification : revisiting the online consumer purchasing intent research every business dreams of having committed , loyal and enthusiastic customers . however , many challenges stand in the way . this is especially true of electronic vendors who must not only grapple with the traditional product issues but also technology issues related to competition in the digital_economy . prior studies in information_systems have emphasized technology and relational factors such as trust and stickiness in their modeling . this study proposes website identification as a concept that can help turn customers into " super customers " and provide lasting and deep relationships between the electronic vendor and the customer leading to creation of economic value . a structural equation model with website identification as a mediating variable was tested using a sample of 406 individuals . the results support the model hypotheses . 
modeling users' powertrain preferences leslie pack kaelbling professor thesis supervisor modeling users' powertrain preferences our goal is to construct a system that can determine a drivers preferences and goals and perform appropriate actions to aid the driver achieving his goals and improve the quality of his road behavior . because the recommendation problem could be achieved effectively once we know the driver's intention , in this thesis , we are going to solve the problem to determine the driver's preferences . a supervised_learning approach has already been applied to this problem . however , because the approach locally classify a small interval at a time and is memory-less , the supervised_learning does not perform well on our goal . instead , we need to introduce new approach which has following characteristics . first , it should consider the entire stream of measurements . second , it should be tolerant to the environment . third , it should be able to distinguish various intentions . in this thesis , two different approaches , bayesian hypothesis_testing and inverse reinforcement_learning , will be used to classify and estimate the user's preferences . bayesian hypothesis_testing classifies the driver as one of several driving_types . assuming that the probability_distributions of the features ( i . e . average , standard_deviation ) for a short period of measurement are different among the driving_types , bayesian hypothesis_testing classifies the driver as one of driving_types by maintaining a belief distribution for each driving type and updating it online as more measurements are available . on the other hand , inverse reinforcement_learning estimates the users' preferences as a linear_combination of driving_types . the inverse reinforcement_learning approach assumes that the driver maximizes a reward function while driving , and his reward function is a linear_combination of raw / expert features . based on the observed tra-jectories of representative drivers , apprenticeship learning first calculates the reward function of each driving type with raw features , and these reward functions serve as expert features . after , with observed trajectories of a new driver , the same algorithm calculates the reward function of him , not with raw features , but with expert features , and estimates the preferences of any driver in a space of driving_types . acknowledgments i owe my deepest gratitude to professor leslie pack kaelbling , my academic , urop , and thesis advisor , for her unending support , guidance , and inspiration , which have enabled me to develop a deep understanding in the field of computer_science and machine_learning . i would also like to thank professor tomas lozano-perez , for his guidance and insightful comments that enabled me to 
malacoda : towards high_level compilation of network_security applications on reconfigurable hardware while the use of reconfigurable_computing for tasks such as packet header processing or deep_packet_inspection in high_speed networks has been widely studied , efforts to extend the technology to application-level processing have only recently been made . one issue that has prevented wider use of reconfigurable platforms in that context is the unfamiliar programming environment : such systems commonly require expertise in computer architecture and digital logic design generally foreign to networking experts . to make the technology more accessible to potential users , we present the high-level domain_specific_language malacoda for application-level network processing and an associated compiler that automatically translates malacoda descriptions into high_performance hardware blocks for insertion into an fpga-based processing platform . we evaluate our approach on the use-case of a hardware-accelerated secure honeypot-in-a-box , programmed in malacoda , and implemented on the netfpga 10g board . results from a live-test of the system connected to a 10g internet uplink complete the evaluation . 
reasoning_about complexity of object_oriented_programs modern imperative object_oriented_design methods and languages take a rigorous approach to compatibility and reusability mainly from an interface and speciication point_of_view | if at all . beside functional speciication , however , users select classes from libraries based on performance characteristics , too . this report develops an appropriate fundamental approach towards performance estimation , measurement and metering in oo approaches . we use examples written in the sather language to demonstrate the concepts of so-called oo-machines , which lend themselves to performance metrics , and a calculus for reasoning_about performance . a language_binding of these concepts is then sketched in the form of cost annotations that allow programmers to le classes in libraries well-documented with cost related speciica-tions . these annotations can optionally be used for instrumenting code that meters cost and checks whether the taken measurements are consistent with the given speciication . in this way programmers can beneet from cost annotations by means of documentation and rigorous testing without requiring a deep familiarity with the theoretical underpinnings . 
deep_linguistic processing for spoken_dialogue systems we describe a framework for deep_linguistic processing for natural_language_understanding in task-oriented spoken_dialogue systems . the goal is to create domain-general processing techniques that can be shared across all domains and dialogue tasks , combined with domain_specific optimization based on an ontology mapping from the generic lf to the application on-tology . this framework has been tested in six domains that involve tasks such as interactive planning , coordination operations , tutoring , and learning . 
the impact of aerosols on cloud and precipitation processes : cloud-resolving model simulations 1 . introduction aerosols and especially their effect on clouds are one of the key components of the climate system and the hydrological cycle [ramanathan et al . , 2001] . yet , the aerosol effect on clouds remains largely unknown and the processes involved not well understood . a recent report published by the national_academy_of_science states "the greatest uncertainty about the aerosol climate forcing-indeed , the largest of all the uncertainties about global climate forcing-is probably the indirect effect of aerosols on clouds [nrc , 2001] . " the aerosol effect on clouds is often categorized into the traditional "first indirect ( i . e . , twomey ) " effect on the cloud droplet sizes for a constant liquid water path [twomey , 1977] and the "semi-direct" effect on cloud coverage [e . g . , ackerman et al . , 2000] . enhanced aerosol concentrations can also suppress warm rain processes by producing a narrow droplet spectrum that inhibits collision and coalescence processes [e . the aerosol effect on precipitation processes , also known as the second type of aerosol indirect effect [albrecht , 1989] , is even more complex , especially for mixed-phase convective clouds . table 1 summarizes the key observational studies identifying the microphysical properties , cloud characteristics , thermodynamics and dynamics associated with cloud systems from high-aerosol continental environments . for example , atmospheric aerosol concentrations can influence cloud droplet size distributions , warm-rain process , cold-rain process , cloud-top height , the depth of the mixed phase region , and occurrence of lightning . in addition , high aerosol concentrations in_urban_environments could affect precipitation variability by providing an enhanced source of cloud_condensation_nuclei ( ccn ) . hypotheses have been developed to explain the effect of urban regions on convection and precipitation [van den heever and cotton , 2007 and shepherd , 2005] . please see tao et al . ( 2007 ) for more detailed description on aerosol impact on precipitation . recently , a detailed spectral-bin microphysical scheme was implemented into the goddard cumulus ensemble ( gce ) model . atmospheric aerosols are also described using number_density size-distribution functions . a spectral-bin microphysical model is very expensive from a computational point_of_view and has only been implemented into the 2d _________________________________________ version of the gce at the present time . the model is tested by studying the evolution of deep tropical clouds in the west pacific warm pool region and summertime convection over a mid-latitude continent with different concentrations of ccn : a low "clean" concentration and a high "dirty" concentration . the impact of atmospheric aerosol concentration on cloud and precipitation will be investigated . 
the " global drifter program " drifter measurements of surface velocity , sst , sss , winds and atmospheric_pressure 1 . project summary 1 . 1 . rationale the principal scientific questions of the role of the ocean in climate_change are how well can we describe or model the ocean_circulation today and how well can these descriptions or models predict the evolution of future climates . climate time scale changes in the sea_surface_temperature ( sst ) directly force changes in the air_temperature and habitability conditions very large parts of the globe . on these interannual time scales sst depends on ocean_circulation as well as air-sea interaction . a global array of drifters provide the operational instrumental data_sets describing sst and ocean near surface circulation and evolution and these data are used for testing climate models and enhancing long_range weather prediction and interannual climate_change . sensors that measure sea_surface salinity ( sss ) are now added to drifters and these sss data are critical to determining the oceans' fresh_water cycle and onset of deep_water renewals . air pressures measured on drifters are assimilated into weather prediction models and are used by operational meteorological agencies to discern severe_weather conditions over the oceans . drifter pressure data also contribute significantly to the calculation of the inverted barometer effect on global sea_level_rise as measured from altimeters . wind sensor and subsurface temperature chain data are used to improve prediction of tropical_storms and hurricanes . drifters designed and built within the " global drifter program " ( gdp ) have proven to be reliable , autonomous platforms for obtaining climate and operational weather data from the global oceans . 1 . 2 . objectives of the " global drifter program " the " global drifter program " ( gdp ) is the principal international component of the joint_commission of marine measurements ( jcomm ) " global surface drifting buoy array " . it is a " scientific project " of the data buoy cooperation panel ( dbcp ) of world_meteorological_organization ( wmo ) /international ocean commission ( ioc ) . it is a near-operational ocean-observing network that , through the argos satellite system and the global telecommunication system ( gts ) , returns real time data on ocean near-surface currents , sst and air pressure ( and winds , subsurface temperature-t ( z ) , and sss ) and provides a data_processing system for scientific utilization of these data . in addition to gdp , drifters are deployed by operational oceanographic and meteorological agencies and individual scientific research projects , whose data are utilized by gdp . in turn , gdp data are made available to operational users and scientists at_large . wind
system for deep venous_thrombosis detection using objective compression measures a system for objective vessel compression assessment for deep venous_thrombosis characterization using ultrasound image_data and a sensorized ultrasound probe is presented . two new objective measures calculated from applied force and transverse vessel area are also presented and used to describe vessel compressibility . a modified star-kalman algorithm is used for feature detection in acquired ultrasound images , and objective measures of vessel compressibility are calculated from the detected features and acquired force and location data from the sensorized probe . a three_dimensional_shape model of the examined vessel that includes compressibility measures mapped as colors to its surface is presented on the user_interface , as well as a virtual representation of the image plane . the compressibility measures were validated using expert segmentation of healthy and diseased vessels and compared using paired t-tests , which showed a significant difference between healthy and diseased cases for both measures . 100% sensitivity and specificity were obtained for both measures . the system was implemented in real-time ( 16 hz ) and evaluated using a tissue phantom and on healthy human subjects . sensitivity was 100% and 60% , while specificity was 97% for both measures when implemented . the initial results for the system and its components are promising . 
a test suite for inference involving adjectives recently , most of the research in nlp has concentrated on the creation of applications coping with textual entailment . however , there still exist very few resources for the evaluation of such applications . we argue that the reason for this resides not only in the novelty of the research field but also and mainly in the difficulty of defining the linguistic phenomena which are responsible for inference . as the tsnlp project has shown test_suites provide optimal diagnostic and evaluation tools for nlp applications , as contrary to text corpora they provide a deep insight in the linguistic phenomena allowing control over the data . thus in this paper , we present a test suite specifically developed for studying inference problems shown by english adjectives . the construction of the test_suite is based on the deep_linguistic analysis and following classification of entailment patterns of adjectives and follows the tsnlp guidelines on linguistic databases providing a clear coverage , systematic annotation of inference tasks , large reusability and simple maintenance . with the design of this test_suite we aim at creating a resource supporting the evaluation of computational systems handling natural_language inference and in particular at providing a benchmark against which to evaluate and compare existing semantic analysers . 
effects of link annotations on search performance in layered and unlayered hierarchically organized information spaces the effects of link annotations on user search performance in hypertext environments having deep ( layered ) and shallow link structures were investigated in this study . four environments were tested layered-annotated , layered-unannotated , shallow-annotated , and shallow-unannotated . a single document was divided into 48 sections , and layered and unlayered versions were created . additional versions were created by adding annotations to the links in the layered and unlayered versions . subjects were given three queries of varying difficulty and then asked to find the answers to the queries that were contained within the hypertext environment to which they were randomly assigned . correspondence between the wording links and queries was used to define difficulty level . the results of the study confirmed previous_research that shallow link structures are better than deep ( layered ) link structures . annotations had virtually no effect on the search performance of the subjects . the subjects performed similarly in the annotated and unannotated environments , regardless of whether the link structures were shallow or deep . an analysis of question difficulty suggests that the wording in links has primacy over the wording in annotations in influencing user search behavior . introduction the internet and the world_wide_web have grown to the point where they have become a major resource for access-ing information . web-accessible information is composed largely of hypertext documents . these documents have links to subunits of information within them , or to information in other documents . the earliest hypertext systems predated the world_wide_web , and one problem with hypertext since its inception has been navigation and way finding ( hammond , 1988; kerr , 1986 ) . because information is presented in interlinked fragments that can be accessed nonlinearly , many users become disoriented ( marchionini & shneiderman , 1993 ) . the problem becomes magnified when users attempt to find information in a global hypertext environment as vast as the web . the way information is represented and linked in hypertext can either alleviate or exacerbate the way finding problem . this study builds on previous_research related to information linking and representation by testing the effects of link annotations on user ability to search hierarchically organized hypertext under conditions where the hierarchical structure has intervening link layers and conditions where it does not . 
practical detection of spammers and content promoters in online video sharing systems a number of online video sharing systems , out of which youtube is the most popular , provide features that allow users to post a video as a response to a discussion topic . these features open opportunities for users to introduce polluted content , or simply pollution , into the system . for instance , spammers may post an unrelated video as response to a popular one , aiming at increasing the likelihood of the response being viewed by a larger number of users . moreover , content promoters may try to gain visibility to a specific video by posting a large number of ( potentially unrelated ) responses to boost the rank of the responded video , making it appear in the top lists maintained by the system . content pollution may jeopardize the trust of users on the system , thus compromising its success in promoting social_interactions . in spite of that , the available literature is very limited in providing a deep understanding of this problem . in this paper , we address the issue of detecting video spammers_and_promoters . towards that end , we first manually build a test_collection of real youtube users , classifying them as spammers , promoters , and legitimate users . using our test_collection , we provide a characterization of content , individual , and social attributes that help distinguish each user class . we then investigate the feasibility of using supervised classification algorithms to automatically detect spammers_and_promoters , and assess their effectiveness in our test_collection . while our classification approach succeeds at separating spammers_and_promoters from legitimate users , the high cost of manually labeling vast amounts of examples compromises its full potential in realistic scenarios . for this reason , we further propose an active_learning approach that automatically chooses a set of examples to label , which is likely to provide the highest amount of information , drastically reducing the amount of required training_data while maintaining comparable classification effectiveness . 
licklider transmission protocol ( ltp ) -based dtn for cislunar communications delay/disruption-tolerant networking ( dtn ) technology offers a new solution to highly stressed communications in space environments , especially those with long link delay and frequent link disruptions in deep-space_missions . to date , little work has been done in evaluating the performance of the available "convergence layer" protocols of dtn , especially the licklider transmission protocol ( ltp ) , when they are applied to an interplanetary internet ( ipn ) . in this paper , we present an experimental evaluation of the bundle protocol ( bp ) running over various "convergence layer" protocols in a simulated cislunar communications environment characterized by varying degrees of signal propagation_delay and data_loss . we focus on the ltp convergence layer ( ltpcl ) adapter running on top of udp/ip ( i . e . , bp/ltpcl/udp/ip ) . the performance of bp/ltpcl/udp/ip in realistic file transfers over a pc-based network test_bed is compared to that of two other dtn protocol_stack options , bp/tcpcl/tcp/ip and bp/udpcl/udp/ip . a statistical method of <i>t</i>-test is also used for analysis of the experimental results . the experiment_results showthat ltpcl has a significant performance advantage over transmission_control_protocol convergence layer ( tcpcl ) for link delays longer than 4000 ms regardless of the bit_error_rate ( ber ) . for a very lossy channel with a ber of around 10<sup>-5</sup> , ltpcl has a significant goodput advantage over tcpcl at all the link delay levels studied , with an advantage of around 3000 b/s for delays longer than 1500 ms . ltpcl has a consistently significant goodput advantage over udpcl , around 2500-3000 b/s , at all levels of link delays and bers . 
neural mechanisms for voice_recognition we investigated neural mechanisms that support voice_recognition in a training paradigm with fmri . the same listeners were trained on different weeks to categorize the mid-regions of voice-morph continua as an individual's voice . stimuli implicitly defined a voice-acoustics space , and training explicitly defined a voice-identity space . the pre_defined centre of the voice category was shifted from the acoustic centre each week in opposite directions , so the same stimuli had different training histories on different tests . cortical sensitivity to voice similarity appeared over different time-scales and at different representational stages . first , there were short_term adaptation effects : increasing acoustic similarity to the directly preceding stimulus led to haemodynamic response reduction in the middle/posterior sts and in right ventrolateral prefrontal regions . second , there were longer-term effects : response reduction was found in the orbital/insular_cortex for stimuli that were most versus least similar to the acoustic mean of all preceding stimuli , and , in the anterior temporal pole , the deep posterior sts and the amygdala , for stimuli that were most versus least similar to the trained voice-identity category mean . these findings are interpreted as effects of neural sharpening of long_term stored typical acoustic and category-internal values . the analyses also reveal anatomically separable voice representations : one in a voice-acoustics space and one in a voice-identity space . voice-identity representations flexibly followed the trained identity shift , and listeners with a greater identity effect were more accurate at recognizing familiar voices . voice_recognition is thus supported by neural voice spaces that are organized around flexible 'mean voice' representations . 
test_quality/cost optimization using output-deviation-based reordering of test_patterns at-speed functional_testing , delay_testing , and n-detection test_sets are being used today to detect deep submi-crometer defects . however , the resulting test_data volumes are too high; the 2005 international roadmap_for_semiconductors predicts that test_application times will be 30 times larger in 2010 than they are today . in addition , many new types of defects cannot be accurately modeled using existing fault_models . therefore , there is a need to model the quality of test_patterns such that they can be quickly assessed for defect screening . test selection is required to choose the most effective pattern sequences from large test_sets . current industry practice for test selection is based on fault grading , which is computationally expensive and must also be repeated for every fault_model . moreover , although efficient_methods exist today , for fault-oriented test_generation , there is a lack of understanding on how best to combine the test_sets thus obtained , i . e . , derive the most effective union of the individual test_sets without simply taking all the patterns for each fault_model . this paper presents the use of the output deviation as a surrogate coverage-metric for pattern modeling and test grading . a flexible , but general , probabilistic-fault_model is used to generate a probability map for the circuit , which can subsequently be used for test_pattern reordering . the output deviations resulting from the probability map ( s ) are used as a coverage-metric to model test_patterns; the higher the deviation , the better the quality of the test_pattern . we show that , for the iscas benchmark_circuits and as compared to other reordering methods , the proposed method provides " steeper " coverage curves for different fault_models . 
imagenet classification with deep convolutional_neural_networks we trained a large , deep_convolutional_neural_network to classify the 1 . 2 million high_resolution images in the imagenet lsvrc-2010 contest into the 1000 different classes . on the test data , we achieved top-1 and top-5 error_rates of 37 . 5% and 17 . 0% which is considerably better than the previous state-of-the-art . the neural network , which has 60 million parameters and 650 , 000 neurons , consists of five convolutional layers , some of which are followed by max_pooling layers , and three fully_connected layers with a final 1000-way softmax . to make training faster , we used non-saturating neurons and a very efficient gpu implementation of the convolution operation . to reduce overfitting in the fully_connected layers we employed a recently_developed regularization method_called " dropout " that proved to be very effective . we also entered a variant of this model in the ilsvrc-2012 competition and achieved a winning top-5 test error_rate of 15 . 3% , compared to 26 . 2% achieved by the second-best entry . 
a u t h o r s shopping_orientations , product types and internet shopping intentions background and research questions a b s t r a c t shopping_orientations are useful in the study of patronage behaviour including store loyalty , brand loyalty , in-home_shopping , and out-shopping . this paper describes an empirical study that examined the relationship between shopping_orientations , product types , and consumer intentions to use the internet for shopping . analyses of data collected from over 750 survey respondents reveal that home , economic , and local shopping_orientations are related to online_shopping intentions . product types , based on cost and tangibility , do not have a moderating influence on the relationship between shopping_orientations and intentions to shop using the internet , but do have a direct effect on the latter . and , the incremental contribution of demographic indicators in predicting online_shopping intentions is minimal . implications of the findings and the association between shopping_orientations and the more easily ascertainable demographic indicators are discussed . his research examines the adoption , use , and consquences of systems that facilitate electronic_commerce . introduction although the grandiose predictions for business-to-consumer internet commerce are yet to be realized , the continued success of a few online merchants such as amazon and ebay offer evidence of the virtual medium's potential as a retail channel . therefore , in spite of the rapid decline in the fortunes of dot-com companies that has justifiably triggered deep scepticism about its future , dismissing the digital channel as a fad may be premature . haste to stake a claim on the digital frontier may be largely to blame for the recent spate of failures among online commercial ventures . it is not surprising that ill-conceived business_models that bypassed careful scrutiny from overly eager investors are proving to be unsustainable . however , rashness alone may not explain the rapid exit of many online businesses . even companies that have proven their viability with respectable sales are discovering that there are some major roadblocks to harnessing the power of the internet to serve as a channel for communications , transactions and distribution . managing the unpredictability of technological glitches , solving the logistical puzzle of order fulfilment and delivery , allaying consumer fears about security_and_privacy breaches , and building and nurturing trust are all major problems facing online retailers . even more of a challenge is identifying , attracting and retaining consumers who would embrace the new medium as a shopping channel . research to address the above issues in electronic retailing is becoming increasingly sophisticated . moving beyond demographics , researchers are building and testing more complex models to 
climate quality broadband and narrowband solar reflected radiance calibration between sensors in orbit as the potential impacts of global_climate_change become more clear [1] , the need to determine the accuracy of climate prediction over decade-to-century time scales has become an urgent and critical challenge . the most critical tests of climate_model predictions will occur using observations of decadal changes in climate forcing , response , and feedback variables . many of these key climate variables are observed by remotely sensing the global distribution of reflected solar spectral and broadband radiance . these "reflected solar" variables include aerosols , clouds , radiative fluxes , snow , ice , vegetation , ocean color , and land cover . achieving sufficient satellite instrument accuracy , stability , and overlap to rigorously observe decadal change signals has proven very difficult in most cases and has not yet been achieved in others [2] . one of the earliest efforts to make climate quality observations was for earth radiation budget : nimbus 6/7 in the late 1970s , erbe in the 1980s/90s , and ceres in 2000s are examples of the most complete global records . the recent ceres data products have carried out the most extensive intercomparisons because if the need to merge data from up to 11 instruments ( ceres , modis , geostationary imagers ) on 7 spacecraft ( terra , aqua , and 5 geostationary ) for any given month . in order to achieve climate calibration for cloud feedbacks , the radiative effect of clear-sky , all-sky , and cloud radiative effect must all be made with very high stability and accuracy . for shortwave solar reflected flux , even the 1% ceres broadband absolute accuracy ( 1- confidence bound ) is not sufficient to allow gaps in the radiation record for decadal climate_change . typical absolute accuracy for the best narrowband sensors like seawifs , misr , and modis range from 2 to 4% ( 1- ) . ipcc greenhouse_gas radiative_forcing is ~ 0 . 6 wm-2 per decade or 0 . 6% of the global mean shortwave reflected flux , so that a 50% cloud feedback would change the global reflected flux by ~ 0 . 3 wm-2 or 0 . 3% per decade in broadband sw calibration change . recent results comparing ceres reflected flux changes with modis , misr , and seawifs narrowband changes concluded that only seawifs and ceres were approaching sufficient stability in calibration for decadal climate_change [3] . results using deep convective clouds in the optically thick limit as a stability target may prove very effective for improving past data_sets like isccp . results for intercalibration of geostationary imagers to ceres using an entire month of regional nearly coincident data demonstrates new approaches to constraining the 
estimating depth of anesthesia from eeg_signals using wavelet_transform electroencephalogram ( eeg ) is the brain signal containing valuable information about the conscious and unconscious states of the brain , which may provide a useful tool to measure depth of anesthesia . however , raw eeg_signals received in various states of consciousness cannot be distinguished visually . in this paper an approach is presented to find out difference between eeg_signals in fully awake and in deep_sleep conditions with respect to the coefficients of wavelet_transform . continuous wavelet_transform of the raw eeg signal obtained at different conscious state of a human subject have been performed . statistical analyses were then performed on coefficient values to determine the differences between the sleep state and the awake state . from statistical t-test analysis significant difference of the two state of consciousness was found . 
wishart deep stacking network for fast polsar_image_classification inspired by the popular deep_learning architecture - deep stacking network ( dsn ) , a specific deep model for polarimetric synthetic_aperture_radar ( polsar ) image_classification is proposed in this paper , which is named as wishart deep stacking network ( w-dsn ) . first of all , a fast implementation of wishart distance is achieved by a special linear_transformation , which speeds up the classification of polsar_image and makes it possible to use this polarimetric information in the following neural_network ( nn ) . then a single-hidden_layer neural_network based on the fast wishart distance is defined for polsar_image_classification , which is named as wishart network ( wn ) and improves the classification_accuracy . finally , a multi_layer neural_network is formed by stacking wns , which is in fact the proposed deep_learning architecture w-dsn for polsar_image_classification and improves the classification_accuracy further . in addition , the structure of wn can be expanded in a straightforward way by adding hidden_units if necessary , as well as the structure of the w-dsn . as a preliminary exploration on formulating specific deep_learning architecture for polsar_image_classification , the proposed methods may establish a simple but clever connection between polsar_image interpretation and deep_learning . the experiment_results tested on real polsar_image show that the fast implementation of wishart distance is very efficient ( a polsar_image with 768000 pixels can be classified in 0 . 53s ) , and both the single-hidden_layer architecture wn and the deep_learning architecture w-dsn for polsar_image_classification perform well and work efficiently . 
diagnosis for scan_based bist : reaching deep into the signatures for partitioning-based diagnosis in a scan_based bist environment , an exact analysis scheme , capable of identifying all scan_cells that receive incorrect data , is proposed . in contrast to previously suggested approaches , the scheme we propose identifies all failing scan_cells with no ambiguity whatsoever . not only do we resolve failing scan_cells unambiguously , but we do so at the earliest possible instance through reexamination of already computed signatures . intensive utilization of this highly precise diagnostic state information leads to prognostic information regarding the usefulness of running upcoming tests which in turn leads to reductions in diagnosis time in excess of 30% compared to previous approaches . 
characterizing pedophile conversations on the internet using online grooming cyber-crime targeting children such as online pe-dophile activity are a major and a growing concern to the society . a deep understanding of predatory chat conversations on the internet has implications in designing effective solutions to automatically identify malicious conversations from regular conversations . we believe that a deeper understanding of the pedophile conversation can result in more sophisticated and robust surveillance_systems than majority of the current systems relying only on shallow processing such as simple word-counting or keyword spotting . in this paper , we study pedophile conversations from the perspective of online grooming theory and perform a series of linguistic-based empirical analysis on several pedophile chat conversations to gain useful insights and patterns . we manually annotated 75 pedophile chat conversations with six stages of online grooming and test several hypothesis on it . the results of our experiments reveal that relationship forming is the most dominant online grooming stage in contrast to the sexual stage . we use a widely used word-counting program ( liwc ) to create psycho-linguistic profiles for each of the six online grooming stages to discover interesting textual patterns useful to improve our understanding of the online pedophile phenomenon . furthermore , we present empirical results that throw light on various aspects of a pedophile conversation such as probability of state transitions from one stage to another , distribution of a pedophile chat conversation across various online grooming stages and correlations between pre_defined word categories and online grooming stages . 
a path_planning and obstacle_avoidance algorithm for an autonomous robotic vehicle abstract sharayu yogesh ghangrekar . a path_planning and obstacle_avoidance path_planning in robotics is concerned with developing the logic for navigation of a robot . path_planning still has a long way to go considering its deep impact on any robot's functionality . various path_planning techniques have been tried and tested earlier , including probabilistic , integral and genetic approaches . the implementation_details of most of these algorithms are proprietary to specific organizations . the requirement of a customized strategy for collision free and concerted navigation of an all_terrain_vehicle ( atv ) led to the activities of this research . as a part of this research an algorithm has been developed and simulated to give a visual effect . the algorithm presented is evolutionary and capable of path_planning for atvs in the presence of completely known and newly-discovered obstacles . this algorithm helps the atv to maneuver in an open field in a specific pattern and avoid the obstacles , if any , along its path . as part of the research the actual algorithm is implemented and simulated using c and winapi . as a result , given the data of known obstacles and the field , the atv can maneuver in a systematic and optimum manner towards its goal by avoiding all the obstacles in its path . this algorithm can also be deployed on an atv using real time data from lidar and gps . the logic of the algorithm can be extended for path_planning in a completely dynamic environment . iv acknowledgments
li_bist : a low-cost self-test scheme for soc logic cores and interconnects for system-on-chips ( soc ) using deep_submicron ( dsm ) technologies , interconnects are becoming critical determinants for performance , reliability and power . buses and long interconnects being susceptible to crosstalk_noise , may lead to functional and timing_failures . existing at-speed interconnect crosstalk test_methods propose inserting dedicated interconnect self-test_structures in the soc to generate vectors which have high crosstalk_defect_coverage . however , these methods may have a prohibitively high area_overhead . to reduce this overhead , existing logic_bist structures like lfsrs could be reused to deliver interconnect tests . but , as shown by our experiments , use of lfsr tests achieve poor crosstalk_defect_coverage . additionally , it has been shown that the power consumed during testing can potentially become a significant concern . in this paper , we present logic-interconnect bist ( li_bist ) , a comprehensive self-test solution for both the logic of the cores and the soc interconnects . li_bist reuses existing logic_bist structures but generates high_quality tests for interconnect crosstalk_defects , while minimizing the area_overhead and interconnect power_consumption . the application of the li_bist methodology on example socs indicates that li_bist is a viable , low_cost , yet comprehensive solution for testing socs . 
fast regular_expression matching using small tcams for network intrusion_detection and prevention systems regular_expression ( re ) matching is a core component of deep_packet_inspection in modern networking and security devices . in this paper , we propose the first hardware_based re matching approach that uses ternary content addressable memories ( tcams ) , which are off-the-shelf chips and have been widely deployed in modern networking devices for packet classification . we propose three novel techniques to reduce tcam space and improve re matching speed : transition sharing , table consolidation , and variable striding . we tested our techniques on 8 real_world re sets , and our results show that small tcams can be used to store large dfas and achieve potentially high re matching throughtput . for space , we were able to store each of the corresponding 8 dfas with as many as 25 , 000 states in a 0 . 59mb tcam chip where the number of tcam bits required per dfa a different tcam encoding scheme that facilitates processing multiple characters per transition , we were able to achieve potential re matching throughputs of between 10 and 19 gbps for each of the 8 dfas using only a single 2 . 36 mb tcam chip . 
a theory of defocus via fourier_analysis in this paper we present a novel theory to analyze defo-cused images of a volume density by exploiting well-known results in fourier_analysis and the singular value decomposition . this analysis is fundamental in two respects : first , it gives a deep insight into the basic mechanisms of image formation of defocused images , and second , it shows how to incorporate additional a-priori knowledge about the geometry and photometry of the scene in restoration algorithms . for instance , we show that the case of a scene made of a single surface results in a simple constraint in the fourier domain . we derive two basic types of algorithms for vol-umetric reconstruction : one based on a dense set of defo-cused images , and one based on a sparse set of defocused images . while the first one excels in simplicity , the second one is of more practical use . both algorithms are tested on real and synthetic data . 
a hybrid finite automaton for practical deep_packet_inspection deterministic finite_automata ( dfas ) are widely used to perform regular_expression matching in linear time . several techniques have been proposed to compress dfas in order to reduce memory requirements . unfortunately , many real_world ids regular_expressions include complex terms that result in an exponential increase in number of dfa states . since all recent proposals use an initial dfa as a starting_point , they cannot be used as comprehensive regular_expression representations in an ids . in this work we propose a hybrid automaton which addresses this issue by combining the benefits of deterministic and non_deterministic finite_automata . we test our proposal on snort rule-sets and we validate it on real traffic traces . finally , we address and analyze the worst case behavior of our scheme and compare it to traditional ones . 
phototaxic foraging of the archaepaddler , a hypothetical deep_sea species an autonomous_agent ( animat , hypothetical animal ) , called the ( archae ) paddler , is simulated in sufficient detail to regard its simulated aquatic locomotion ( paddling ) as physically possible . the paddler is supposed to be a model of an animal that might exist , although it is perfectly possible to view it as a model of a robot that might be built . the agent is assumed to navigate in a simulated deep_sea environment , where it forages for autoluminescent prey . it uses a biologically inspired phototaxic foraging strategy , while paddling in a layer just above the bottom . the advantage of this living space is that the navigation problem--and hence our model--is essentially two-dimensional . moreover , the deep-sea environment is physically simple ( and hence easy to simulate ) : no significant currents , constant temperature , completely dark . a foraging performance metric is developed that circumvents the necessity to solve the traveling salesman_problem . a parametric simulation study then quantifies the influence of habitat factors , such as the density of prey , and body geometry ( e . g . , placement , direction and directional selectivity of the eyes ) on foraging success . adequate performance proves to require a specific body geometry adapted to the habitat characteristics . in general , performance degrades gracefully for modest changes of the geometric and habitat parameters , indicating that we work in a stable region of "design space . " the parameters have to strike a compromise between , on the one hand , to "see" as many targets at the same time as possible . one important conclusion is that simple reflex-based navigation can be surprisingly efficient . additionally , performance in a global task ( foraging ) depends strongly on local parameters such as visual direction tuning , position of the eyes and paddles , and so forth . behavior and habitat "mold" the body , and the body geometry strongly influences performance . the resulting platform enables further testing of foraging strategies or vision and locomotion theories stemming either from biology or from robotics . 
self-x ran : autonomous self organizing radio access networks current situation for radio access network_management deployment and maintenance become more and more complex and cost extensive trend to smaller cells , multi-band operation , heterogeneous mobile_networks high manual intervention for configuration , capacity upgrade or in failure cases required high effort required for optimisation of system performance deep system expertise required high effort necessary for measurement campaigns ( drive tests ) different tools for planning , configuration , measurement/kpi acquisition and optimisation involved increasing effort for network_management and optimisation new concepts for simplified network operation required
comparing bacterial_communities inferred from 16s_rrna_gene sequencing and shotgun metagenomics 16s_rrna_gene sequencing has been widely used for probing the species structure of a variety of environmental bacterial_communities . alternatively , 16s_rrna_gene fragments can be retrieved from shotgun metagenomic sequences ( metagenomes ) and used for species profiling . both approaches have their limitations-16s_rrna sequencing may be biased because of unequal amplification of species' 16s_rrna genes , whereas shotgun metagenomic sequencing may not be deep enough to detect the 16s_rrna genes of rare species in a community . however , previous_studies showed that these two approaches give largely similar species profiles for a few bacterial_communities . to investigate this problem in greater detail , we conducted a systematic comparison of these two approaches . we developed phyloshop , a pipeline that predicts 16s_rrna_gene fragments in metagenomes , reports the taxonomic assignment of these fragments , and visualizes their taxonomy distribution . using phyloshop , we analyzed 33 metagenomic datasets of human-associated bacterial_communities , and compared the bacterial community structures derived from these metagenomic datasets with the community_structure derived from 16s_rrna_gene sequencing ( 71 datasets ) . based on several statistical_tests ( including a statistical test proposed here that takes into consideration differences in sample_size ) , we observed that these two approaches give significantly different community structures for nearly all the bacterial_communities collected from different locations on and in human_body , and that these differences cannot be be explained by differences in sample_size and are likely to be attributed by experimental method . 
improving the accuracy of rf alternate_test using multi-vdd conditions : application to envelope-based test of lnas this work demonstrates that multi-vdd conditions may be used to improve the accuracy of machine_learning models , significantly decreasing the prediction error . the proposed technique has been successfully_applied to a previous alternate_test strategy for lnas based on response envelope detection . a prototype has been developed to show its feasibility . the prototype consists of a low-power 2 . 4ghz lna and a simple envelope detector , integrated in a 90nm cmos_technology . post-layout simulation_results are provided to verify the functionality of the approach . i . introduction nowadays , advances in rf cmos_technologies have enabled the integration of complete transceivers in a single chip , which provides a significant reduction in production cost . however there is a simultaneous increase in the cost of testing and diagnosing these devices . their diverse specifications and high operating_frequency , as well as the large impact of process_variations in current deep_submicron_technologies , make necessary extensive_tests and dedicated high_frequency test_equipment . rf testing exhibits the same difficulties present in analog testing , but adding the problem of handling high_frequency signals . that is , rf testing is based on functional characterization , while fault_model-based tests , very successful in the digital test domain , are difficult to standardize in the rf field since each circuit type demands its own custom fault_model . reducing rf test complexity and cost is still an open research topic that has been addressed in a number of different approaches . recent work in this area includes defect modeling and failure diagnosis [1] , [2] , alternate_test [2] [5] , dft and bist_techniques [6] [8] , etc . in particular , the combination of bist_techniques with the statistical_analysis of alternate_test seems to be a promising solution to mitigate most rf test drawbacks . on one hand , moving some of the testing functions to the device under test ( dut ) would reduce test_equipment cost , and eliminate the problem of transporting high_frequency test signals . on the other hand , alternate_test strategies take advantage of advanced statistical tools to find correlations between a reduced number of observables ( signatures ) , and the diverse dut specifications , thus reducing the number of necessary test measurements and configurations . 
a logic for vagueness in dummett's important paper [1] on the sorites_paradox it is suggested that the vagueness of observational predicates such as ' . . . is red' or more obviously ' . . . looks red' generates an apparent incoherence : their use resembles a game governed by inconsistent rules . a similar incoherence is seen by wright [18 , 17] as a real and serious threat to very ordinary ideas of how language works . wright argues not that the use of vague predicates is incoherent but that it would be if the use of language were a practice in which the admissibility of moves were determined by rules whose general properties are discoverable by appeal to non-behavioural notions . but unless we do move from anecdotes about behaviour to just such rules , how are we to reason at all ? the incoherence in question is an outcome of the vagueness or tolerance of observational locutions , and would seem if established for them to spread to non-observational vague expressions like ' . . . is water' or ' . . . is a test_tube' , 1 thus vitiating almost all of our attempts to use language consistently , even in science . on the face of it , vagueness is everywhere , whence such deep-seated incoherence would upset even such fragile understanding of semantics as we have gleaned from a century's work . the argument connecting vagueness to incoherence , therefore , strikes at the heart of logic : every philosophical logi-cian is called upon to respond to it . the sorites_paradox , the " slippery_slope argument " or the " paradox of the heap " , is old and famous and wears an air of sophistry . one feels that the problem will vanish on exposure of the trivial trick involved . what is surprising is that it is so deep and difficult after all . an example or two will help focus the discussion . 1 how long and thin must a piece of glassware be , or how polluted may a liquid be , before it no longer counts as a test_tube or as water ? 
complete ioco test_cases : a case study input/output transition systems ( iotss ) have been widely used as test models in model_based_testing . traditionally , input_output conformance testing ( ioco ) has been used to generate random test_cases from iotss . a recent test_case generation method for iotss , called complete ioco , applies fault_models to obtain complete test_suites with guaranteed fault_coverage for iotss . this paper measures the efficiency of complete ioco in comparison with the traditional ioco test_case generation implemented in the jtorx tool . to this end , we use a case study involving five specification models from the automotive and the railway domains . faulty mutations of the specifications were produced in order to compare the efficiency of both test_generation methods in killing them . the results indicate that complete ioco is more efficient in detecting deep faults in large state_spaces while ioco is more efficient in detecting shallow faults in small state_spaces . 
conference paper ggscrs : ggnmos triggered silicon controlled rectifiers for esd_protection in deep_sub__micron_cmos processes ggscrs : ggnmos triggered silicon controlled rectifiers for esd_protection in deep sub-micron cmos_processes eos/esd symposium 2001 in this paper , design aspects , operation , protection capability and applications of scrs in deep submicron cmos are addressed . a novel grounded gate nmos triggered scr device ( ggscr ) is introduced and compared to the lvtscr . experimental verification , including endurance testing , demonstrates that ggscrs can fulfill all esd_protection requirements for today's ic applications in different 0 . 25um , 0 . 18um and 0 . 13um cmos_processes . abstract in this paper , design aspects , operation , protection capability and applications of scrs in deep_sub_micron_cmos are addressed . a novel grounded-gate nmos triggered scr device ( ggscr ) is introduced and compared to the lvtscr . experimental verification , including endurance testing , demonstrates that ggscrs can fulfill all esd_protection requirements for todays ic applications in different 0 . 25um , 0 . 18um and 0 . 13um cmos_processes . 
a reflective framework for configurable workflow processes and tools 1 abstract this paper describes some key issues in implementing a reflective object_oriented framework . we use the framework to build applications in administrative environments . these applications share a common business_model , and require a mix of database , document_management and workflow functionality . so far , we have focused our development efforts on the central administration of schools in the flemish_community of belgium , with increasing emphasis on access to the central applications from within the schools and local boards through the internet . in addition we implemented a few test_cases to verify the validity of our approach . we rewrote a rather large application of the belgian police department in a few weeks; this includes training the developer . the framework uses a repository to store meta-information about end_user applications . this includes object model , object behavior , constraints , specifications of application environments , query screens , layout definitions of overview lists and forms , authorization rules , workflow process templates and event-condition-action rules . fully operational end_user and development_tools consult this meta-information at run_time , and adapt themselves dynamically to the application specifications in the database . thus we separate specifications of a particular organization's business_model from the generic functionality of the tools . rather than coding or generating code , we develop end_user applications and most of the development_tools themselves by building increasingly complete specifications of the business_model . these specifications are available for immediate execution . one of the main objectives of the framework is a high-degree of end_user configurability . configuration by end_users is often just skin-deep . in our case it involves all aspects of end_user application development : knowledgeable users adapt the business rules and workflow processes , whereas regular end_users adapt the form and overview list layouts and query screens to their own needs . the business rules ensure consistency in all cases , because their specifications are de-coupled from the application functionality . users are becoming increasingly aware that change is a constant factor and that applications are never truly finished . thus we give them the necessary tools to build and configure their applications . end_user application developers are also users of the system , with there own requirements . these may change over time , or depend on the kind of applications they are building . given the generic functionality of the end_user tools and the configuration options , another major goal of our framework consists of replacing as many dedicated development_tools as possible by applications configured in the system itself . thus any enhancement of 
how much can part_of_speech_tagging help parsing ? folk wisdom holds that incorporating a part-of-speech tagger into a system that performs deep_linguistic analysis will improve the speed and accuracy of the system . previous_studies of tagging have tested this belief by incorporating an existing tagger into a parsing system and observing the effect on the speed of the parser and accuracy of the results . however , not much work has been done to determine in a fine_grained manner exactly how much tagging can help to disambiguate or reduce ambiguity in parser output . we take a new approach to this issue by examining the full parse-forest output of a large-scale lfg-based english_grammar ( riezler et al . , 2002 ) running on the xle grammar development platform ( maxwell and kaplan , 1993 , 1996 ) , and partitioning the parse outputs into equivalence classes based on the tag sequences for each parse . if we find a large number of tag-sequence equivalence classes for each sentence , we can conclude that different parses tend to be distinguished by their tags; a small number means that tagging would not help much in reducing ambiguity . in this way , we can determine how much tagging would help us in the best case , if we had the " perfect tagger " to give us the correct tag sequence for each sentence . we show that if a perfect tagger were available , a reduction in ambiguity of about 50% would be available . somewhat surprisingly , about 30% of the sentences in the corpus that was examined would not be disambiguated , even by the perfect tagger , since all of the parses for these sentences shared the same tag sequence . our study also helps to inform research on tagging by providing a targeted determination of exactly which tags can help the most in disambiguation . 
tiled convolutional_neural_networks deep_belief_networks for scalable unsupervised learning of hierarchical representations . in icml , 2009 . what is the best multi-stage architecture for object_recognition ? in iccv , 2009 . [4] a . hyvarinen and p . hoyer . topographic independent_component_analysis as a model of v1 organization and receptive fields . algorithm convolutional_neural_networks [1] work well for many recognition tasks : -local receptive fields for computational reasons-weight sharing gives translational invariance however , weight sharing can be restrictive because it prevents us from learning other kinds of invariances . abstract convolutional_neural_networks ( cnns ) have been successfully_applied to many tasks such as digit and object_recognition . using convolutional ( tied ) weights significantly reduces the number of parameters that have to be learned , and also allows translational invariance to be hard-coded into the architecture . in this paper , we consider the problem of learning invariances , rather than relying on hard-coding . we propose tiled convolutional_neural_networks ( tiled cnns ) , which use a regular " tiled " pattern of tied weights that does not require that adjacent hidden_units share identical weights , but instead requires only that hidden_units k steps away from each other to have tied weights . by pooling over neighboring units , this architecture is able to learn complex invariances ( such as scale and rotational invariance ) beyond translational invariance . further , it also enjoys much of cnns' advantage of having a relatively small number of learned parameters ( such as ease of learning and greater scalability ) . we provide an efficient learning algorithm for tiled cnns based on topographic ica , and show that learning complex invariant_features allows us to achieve highly competitive results for both the norb and cifar-10 datasets . tica network_architecture evaluating benefits of convolutional training training on 8x8 samples and using these weights in a tiled cnn obtains only 51 . 54% on the test set compared to 58 . 66% using our proposed_method . networks learn concepts like edge detectors , corner detectors invariant to translation , rotation and scaling algorithms for pretraining convolutional_neural_networks [2 , 3] do not use untied weights to learn invariances . tica can be used to pretrain tiled cnns because it can learn invariances even when trained only on unlabeled_data [4 , 5] . tiled cnns are more flexible and usually better than fully convolutional_neural_networks . pretraining with tica finds invariant and discriminative features and works well with finetuning . 
hierarchical extreme_learning machine for unsupervised representation learning learning representations from massive unlabelled data is a topic for high_level tasks in many applications . the recent great improvements on benchmark data_sets , which are achieved by increasingly complex unsupervised_learning methods and deep_learning models with many parameters , usually requiring many tedious tricks and much expertise to tune . additionally , the filters learned by these complex architectures are quite similar to standard hand-crafted visual features , and training to fine-tune the weights of deep_architectures requires a long time . in this paper , the extreme_learning machine-auto encoder ( elm-ae ) is employed as the learning unit to learn local receptive fields at each layer , and the lower layer responses are transferred to the last layer ( trans-layer ) to form a more complete representation to retain more information . in addition , some beneficial methods in deep_learning architectures such as local contrast normalization and whitening are added to the implemented hierarchical extreme_learning machine networks to further boost the performance . the resulting trans-layer representations are processed into block histograms with binary hashing to produce translation and rotation_invariant representations , which are utilized to do high_level tasks such as recognition and detection . the proposed trans-layer representation method with elm-ae based learning of local receptive filters was tested on the mnist digit recognition data_set , including mnist variations , and on the caltech 101 object_recognition database . compared to traditional deep_learning methods , the proposed elm-ae based system has a much faster learning speed and attains 65 . 97% accuracy on the caltech 101 task ( 15 samples per class ) and 99 . 45% on the standard mnist data_set . 
laminar patterns of local excitatory input to layer_5_neurons in macaque primary_visual_cortex . layer_5_neurons in primary_visual_cortex make putative reciprocal feedback connections to the superficial layers . to test this hypothesis , we employed scanning laser photostimulation combined with intracellular dye injection to examine local functional excitatory inputs to and axonal projections from individual layer_5_neurons in brain slices from monkey v1 . in contrast with previous_studies of other v1 neurons , layer_5_neurons received significant input from nearly all of the cortical layers , suggesting individual layer 5 cells integrate information from a broad range of input sources . nevertheless relative strengths of laminar inputs varied across neurons . cluster_analysis of relative strength of laminar inputs to individual layer_5_neurons revealed four discrete clusters representing recurring input patterns; each cluster included both excitatory and inhibitory neurons . twenty-five of 40 layer_5_neurons fell into two clusters , both characterized by very strong input from superficial layers . these input patterns are consistent with layer_5_neurons providing feedback to superficial layers . the remaining 15 neurons received stronger input from deep layers . differences in input from layer 4calpha versus 4cbeta also suggest specific associations of the magnocellular and parvocellular visual pathways , with populations receiving stronger input from deep versus superficial cortical layers . 
convolutional-recursive deep_learning for 3d object_classification recent advances in 3d sensing technologies make it possible to easily record color and depth images which together can improve object_recognition . most current methods rely on very well-designed features for this new 3d modality . we introduce a model based on a combination of convolutional and recursive neural_networks ( cnn and rnn ) for learning features and classifying rgb-d images . the cnn layer learns low_level translationally invariant_features which are then given as inputs to multiple , fixed-tree rnns in order to compose higher_order features . rnns can be seen as combining convolution and pooling into one efficient , hierarchical operation . our main result is that even rnns with random weights compose powerful features . our model obtains state of the art performance on a standard rgb-d object dataset while being more accurate and faster during training_and_testing than comparable architectures such as two-layer cnns . 
fifteen compilers in fifteen days traditional_approaches to semester-long projects in compiler courses force students to implement the early_stages of a compiler in depth; since many students fall behind , they have little opportunity to implement the back end . consequently , students have a deep knowledge of early material and no knowledge of latter material . we propose an approach based on incremental development and test_driven_development; this approach solves the emphasis problem , provides experience with useful tools , and allows for such a course to be taught in a three or four weeks . 
on "deep" knowledge extraction from documents syndikate comprises a family of natural_language_understanding systems for automatically acquiring knowledge from real_world texts ( e . g . , information_technology test reports , medical finding reports ) , and for transferring their content to formal representation structures which constitute a corresponding text knowledge_base . we present a general system architecture which integrates requirements from the analysis of single sentences , as well as those of referentially linked sentences forming cohesive texts . properly accounting for text cohesion phenomena is a prerequisite for the soundness and validity of the generated text representation structures . it is also crucial for any information system application making use of automatically_generated text knowledge_bases in a reliable way . 
analytical models for crosstalk excitation and propagation in vlsi_circuits we develop a general methodology to analyze crosstalk_effects that are likely to cause errors in deep_submicron high_speed circuits . we focus on crosstalk due to capacitive_coupling between a pair of lines . closed_form equations are derived that quantify the severity of these effects and describe qualitatively the dependence of these effects on the values of circuit_parameters , the rise/fall times of the input transitions , and the skew between the transitions . for noise propagation , we present a new way for predicting the output waveform produced by an inverter due to a non-square_wave pulse at its input . to expedite the computation of the response of a logic_gate to an input pulse , we have developed a novel way of modeling such gates by an equivalent inverter . the results of our analysis provide conditions that must be satisfied by a sequence of vectors used for validation of designs as well as post-manufacturing testing of devices in the presence of significant crosstalk . we present data to demonstrate accuracy of our results , including example runs of a test generator that uses these results . 
a multi-mode power gating structure for low_voltage deep_submicron cmos_ics most existing power gating structures provide only one power_saving mode . we propose a novel power gating structure that supports both a cutoff mode and an intermediate power_saving and data-retaining mode . experiments with test_structures fabricated in 0 . 13-m cmos bulk technology show that our power gating structure yields an expanded design space with more power-performance tradeoff alternatives . 
a semantic-modal view on ramsey's test we present a semantic_analysis of the ramsey test , pointing out its deep underlying flaw : the tension between the " static " nature of agm revision ( which was originally tailored for revision of only purely ontic beliefs , and can be applied to higher_order beliefs only if given a " backwards-looking " interpretation ) and the fact that , semantically speaking , any ramsey conditional must be a modal operator ( more precisely , a dynamic-epistemic one ) . thus , a belief about a ramsey conditional is in fact a higher_order belief , hence the agm revision postulates are not applicable to it , except in their " backwards-looking " interpretation . but that interpretation is consistent only with a restricted ( weak ) version of ramsey's test ( in-applicable to already revised theories ) . the solution out of the conundrum is twofold : either accept only the weak ramsey test; or replace the agm revision operator * by a truly " dynamic " revision operator , which will not satisfy the agm axioms , but will do something better : it will " keep up with reality " , correctly describing revision with higher_order beliefs . 
countermeasures against high_order fault_injection attacks on crt-rsa in this paper we study the existing crt-rsa countermeasures against fault_injection attacks . in an attempt to classify them we get to achieve deep understanding of how they work . we show that the many countermeasures that we study ( and their variations ) actually share a number of common features , but optimize them in different ways . we also show that there is no conceptual distinction between test-based and infective countermeasures and how either one can be transformed into the other . furthermore , we show that faults on the code ( skipping instructions ) can be captured by considering only faults on the data . these intermediate results allow us to improve the state of the art in several ways : ( a ) we fix an existing and that was known to be broken countermeasure ( namely the one from shamir ) ; ( b ) we drastically optimize an existing countermeasure ( namely the one from vigilant ) which we reduce to 3 tests instead of 9 in its original version , and prove that it resists not only one fault but also an arbitrary number of randomizing faults; ( c ) we also show how to upgrade countermeasures to resist any given number of faults : given a correct first-order countermeasure , we present a way to design a prov-able high_order countermeasure ( for a well-defined and reasonable fault_model ) . finally , we pave the way for a generic approach against fault_attacks for any modular_arithmetic computations , and thus for the automatic insertion of countermeasures . 
a methodology for deep sub-0 . 25 m cmos_technology prediction we present a novel methodology for characterization of sub-quartermicron cmos_technologies . it involves process calibration , device calibration employing two-dimensional device simulation and automated technology computer_aided_design ( tcad ) optimization and , finally , transient mixed-mode de-vice/circuit_simulation . the proposed methodology was tested on 0 . 25 m technology and applied to 0 . 13 m technology in order to estimate ring_oscillator speed . the simulation results show an excellent agreement with available experimental_data . 
speech_synthesis for portable devices a text-to-speech ( tts ) synthesiser is a computer-based system that should be able to read any text aloud , whether it was directly introduced in the computer by an operator or scanned and submitted to an optical_character_recognition ( ocr ) system . this project presents the design and implementation of a speech synthesiser for portable devices . the tts implementation was developed for j2me . this development involved the conversion of a desktop tts program by sun microsystems inc . called freetts . the implementation also involved the partial development of java sound api's for j2me on windows_ce . the tts implementation on a pda , apart from being platform independent produces the same sound quality with a far less powerful processor than its desktop counterparts . the implementation is written for j2me , and is also compatible with jdk 1 . 3 on desktop computers . the program has been tested on a dell axim x5 pda running windows_ce , and has been tested on a wide range of desktop computers and operating_systems including : windows_2000 , windows_xp , freebsd and linux . i acknowledgements " . . . very little do we have and enclose which we can call our own in the deep sense of the word . we all have to accept and learn , either from our predecessors or from our contemporaries . even the greatest genius would not have achieved much if he had wished to extract everything from inside himself . but there are many good people , who do not understand this , and spend half their lives wondering in darkness with their dreams of originality . i have known artists who were proud of not having followed any teacher and of owing everything only to their own genius . such fools ! " [goethe , conversations with eckermann , 17 feb 1832] first of all , i would like to thank all of the staff in dit kevin street , in particular dr . fred mtenzi , the project supervisor for all the guidance he provided throughout this work . i appreciate the input of the computer_science classes , ft228 , ft225 and dt226 for their continued discussion , support and assisting with software_testing . thanks to nsicom who issued a license for the use of their j2me virtual_machine in this project , and also for answering questions relating to the hardware and processing limitations of portable devices . great thanks are expressed to both donald_knuth [31] , the author of t e x , and also to leslie_lamport [34] for writing l a t e x , 
advanced code_coverage analysis using substring holes code_coverage is a common aid in the testing_process . it is generally used for marking the source_code segments that were executed and , more importantly , those that were not executed . many code_coverage tools exist , supporting a variety of languages and operating_systems . unfortunately , these tools provide little or no assistance when code_coverage data is voluminous . such quantities are typical of system tests and even for earlier testing phases . drill-down capabilities that look at different granularities of the data , starting with directories and going through files to functions and lines of source_code , are insufficient . such capabilities make the assumption that the coverage issues themselves follow the code hierarchy . we argue that this is not the case for much of the uncovered code . two notable examples are error handling code and platform-specific constructs . both tend to be spread throughout the source in many files , even though the related coverage , or lack thereof , is highly dependent . to make the task more manageable , and therefore more likely to be performed by users , we developed a hole analysis algorithm and tool that is based on common substrings in the names of functions . we tested its effectiveness using two large ibm software systems . in both of them , we asked domain experts to judge the results of several hole-ranking heuristics . they found that 57% - 87% of the 30 top-ranked holes identified by the effective heuristics are relevant . moreover , these holes are often unexpected . this is especially impressive because substring hole analysis relies only on the names of functions , whereas domain experts have a broad and deep understanding of the system . we grounded our results in a theoretical framework that states desirable mathematical properties of hole ranking heuristics . the empirical results show that heuristics with these properties tend to perform better , and do so more consistently , than heuristics lacking them . 
international journal of computer_science and mobile_computing high_speed fsm_based programmable memory built-in self-test ( mbist ) controller this paper proposed a high_speed fsm_based controller for programmable memory built-in self-test for testing memory devices . this technique is popular because of its flexibility of new test algorithms . the architecture of controller is designed to implement a new test algorithm has less number of operations and this algorithm emphasis testing of high_density memory ics either faulty or good . the components of controller is studied and designed using verilog hdl . the analysis of the timing , logic area usage and speed are presented . i . introduction memories are the most universal component today almost all system chips contain some type of embedded_memory , such as rom , sram , dram , and flash_memory . in the embedded domain , embedded rams of the strongarmsa110 occupy 90% of the total area . the projection is , by 2014 , memory will represent more than 94% of the chip area in average soc environment , according to the international_technology_roadmap_for_semiconductors 2007[7] the percentage of chip area occupied by memories in a design and the increasing trend predicted for the next decade , with the advent of deep-submicron vlsi technology , the memory density and capacity is growing but the clock frequency is never higher . the dominant use of embedded memory_cores along with emerging new architectures and technologies make providing a low area_overhead and high_speed test solution for these on-chip memories a very challenging task . built-in self-test ( bist ) [5] has been proven to be one of the most cost_effective and widely used solutions for memory testing because the tests can run at circuit speed to yield a more realistic test time , no external test_equipment , reduced development efforts and on-chip test_pattern_generation to provide higher controllability and observability . there are several fsm_based controllers proposed in [1-10] . in fsm_based memory_bist controller , counters are the key component especially in fsm_based memory_bist controller but some fsm_based bist_controller [2] excluded counter from its design . this type of architecture has optimum area_overhead however less flexible to allow any changes in the test algorithm . usually , different counters [3] , [4] are used to generate the address , test_data and read/write sequences . two types of fsm_based bist_controller architectures are proposed in [5] . both are designed by using a counter for the test pattern_generator and test controller but one is using misr which is a part of the bist_controller block for output response analyzer ( ora ) while another one is 
on-line leakage-aware energy minimization scheduling for hard real-time systems as the semiconductor_technology proceeds into the deep sub-micron era , leakage and its dependency with the temperature become critical in dealing with the power/energy minimization problem . in this paper , we develop an analytical method to estimate energy consumption on-line with the leakage/temperature dependency taken into consideration . based on this method , we develop an on-line scheduling_algorithm to reduce the overall energy consumption for a hard real-time system scheduled according to the earliest deadline first ( edf ) policy . our experimental results show that the proposed energy estimation method can achieve up to 210x speedup compared with an existing approach while still maintaining high_accuracy . in addition , with a large number of different test_cases , the proposed energy saving scheduling method consistently outperforms two closely_related researches in average by 10% and 14% respectively . 
exploring continued online_service usage behavior : the roles of self_image_congruity and regret the expectation confirmation model ( ecm ) of continued information_systems ( is ) use has proven to be successful across online_service contexts . previous_studies based on ecm have focused on a referent ( i . e . , comparison standard ) that is centered on the target is ( i . e . , target online_service ) . the effect of this ref-erent , captured through confirmation , has been strongly demonstrated . yet , few studies have explored the saliency of two additional reference effects , captured through self_image_congruity and regret , in online_service continuance . to fill this knowledge gap , this paper attempts to develop a research model that extends the ecm perspective in view of the additional contributions of regret and self-image congru-ity on two post-adoption beliefs ( perceived usefulness and perceived enjoyment ) and continuance intention . for this extension , we synthesized the extant literature on continued is use , self_image_congruity , and regret . the model was empirically tested within the context of a social_network_service . our analysis result shows that self_image_congruity plays a key role in forming the two post-adoption beliefs . it is also found that the absolute effect of regret on continuance intention is larger than the effects of other antecedents identified in is . overall , this study preliminarily confirms the salience of self_image_congruity and regret in online_service continuance . although initial use is an important measure of online_service success , it does not necessarily result in the desired managerial performance unless the use continues ( bhattacherjee , 2001b ) . specifically , how to promote continued online_service usage or , alternatively , how to prevent discontinuance is a critical issue for online service_providers to consider ( parthasarathy & bhattacherjee , 1998 ) . deep down , online_service managers know that achieving strong and sustained customers is crucial . therefore , research_into this online_service continuance has recently emerged as an important_issue in the is literature individuals' information_systems ( is ) continuous usage decisions are congruent with consumers' repeat purchase decisions . the expectancy-confirmation paradigm has been strongly confirmed across a wide range of product repurchase and service con-tinuance contexts ( e . from the paradigm , bhattacherjee ( 2001b ) developed the expecta-tion confirmation model ( ecm ) of continued is use . the model explicitly focuses on a user's psychological motivations that emerge after initial adoption of is . furthermore , it has proven to be successful across consumer-oriented online_service contexts what are the distinguishing characteristics that influence people to continue is usage in a consistent fashion ? the original ecm hypothesizes that an individual's intention to continue is usage depends on three variables : 
a computationally efficient stereo algorithm for adaptive cruise_control vision a computationally efficient stereo_vision algorithm for adaptive cruise_control a major obstacle in the application of stereo_vision to intelligent_transportation_systems is high computational cost . in this thesis , we present an edge based , subpixel stereo algorithm which is adapted to permit accurate distance measurements to objects in the field of view in real-time using a compact camera assembly . once computed , the 3-d scene information may be directly applied to a number of in-vehicle applications , such as adaptive cruise_control , obstacle_detection , and lane tracking . on-road applications , such as vehicle counting and incident detection , are also possible . a pc based three-camera stereo_vision system , constructed with off-the-shelf components , was built to serve as a tool for testing and refining algorithms which approach real-time performance . in-vehicle road trial results are presented and discussed . acknowledgments i wish to thank professor berthold horn for agreeing to supervise this thesis , and for generously sharing his expertise in machine_vision over the course of the research_project . i also extend my deep gratitude to dr . ichiro masaki for proposing this graduate work , and for his continued support and advice as the project evolved . the students of the artificial_intelligence lab provided lots of technical advice and equally abundant friendship . i wish to extend special thanks to marcelo mizuki for his unbounded generosity in helping out at various phases of the project . he safely piloted the test vehicle during numerous road trials while i provided a steady stream of distractions from the passenger's seat . thanks to gideon stein for helping me get started with this project , and for a number of stimulating discussions . many thanks the staff members in the ai lab were also very helpful . i'd especially like to thank inglese for helping me to navigate the labyrinth of purchasing , and ron wiken for his help with miscellaneous supplies and for giving me a crash course on the milling_machine . i'd like to thank eric hopkins at metrovideo in wellesley , ma , for his assistance in technical matters regarding video and image_processing hardware . kudos to the staff at imaging technology inc . in bedford , ma , for their patient technical support . above all , i thank my family for their love and support in all that i have done , from the beginning . 
voice based biometric security system project details acknowledgment we express our gratitude and deep-felt thanks towards our esteemed guide , dr . sudip sanyal for his able guidance , which enabled us to complete our project . we are extremely grateful to prof . asr murthy for providing us with valuable suggestions and feedback . we take this opportunity to thank our colleagues who provided us with valuable suggestions , and also lent their voices for analysis and testing during the course of the project , thus resulting in the successful completion . aim : develop a biometric security system , which used the human_voice as a distinguishing feature between various users . 
an effective scheduler for ip routers at the communications and telematics laboratory of the university of coimbra is being developed a router prototype with the aim to provide qos to different traffic classes . one of the most important mechanisms of this router is the ip packet scheduler . it is well known that the common scheduling discipline of current routers ( first come first serve ) turns them useless when qos is needed-a different type of scheduler must be used . our first idea to overcome this problem was to use a simple , open , and available scheduler , easy to adapt to the system we wanted to implement . we thought of the wfq discipline , and , as we are using a testbed of intel machines running freebsd os , we admitted that the wfq/altq implementation would be an interesting choice . nevertheless , a broad set of tests carried out at our laboratory proved the contrary . most important , these tests guided us to a deep knowledge about the problems , and causes , that can weaken the effectiveness of ip schedulers . given the importance of that surplus information , we decided to implement our own scheduler . the idea was to take advantage of a most pragmatic view of scheduling activities to construct a scheduler with the best possible characteristics , but also very simple , thus , able to reach very good performance levels . this paper presents the scheduler that resulted from our attempts . the proposed scheduler was subject to a set of tests that proved its ability to effectively differentiate traffic classes . the results of these tests are also presented and analyzed . 
cicerobot , a cognitive robot for museum tours the paper describes cicerobot , a robot based on a cognitive_architecture for robot vision and action . the aim of the architecture is to integrate visual_perception and actions with knowledge_representation , in order to let the robot to generate a deep inner understanding of its environment . the principled integration of perception , action and of symbolic knowledge is based on the introduction of an intermediate representation based on g rdenfors conceptual spaces . the architecture has been tested on a rwi b21 autonomous_robot on tasks related with guided tours in the archaeological museum of agrigento . experimental_results are presented . 
faster parameterized algorithms for minor_containment the theory of graph minors by robertson and seymour is one of the deepest and significant theories in modern combinatorics . this theory has also a strong impact on the recent development of algorithms , and several areas , like parameterized complexity , have roots in graph minors . until very recently it was a common belief that graph minors theory is mainly of theoretical importance . however , it appears that many deep results from robertson and seymour's theory can be also used in the design of practical algorithms . minor_containment testing is one of algorithmically most important and technical parts of the theory , and minor containment in graphs of bounded branchwidth is a basic ingredient of this algorithm . in order to implement minor_containment testing on graphs of bounded branchwidth , hicks [networks 04] described an algorithm , that in time o ( 3 k 2 ( h + k 1 ) ! m ) decides if a graph g with m edges and branchwidth k , contains a fixed graph h on h vertices as a minor . that algorithm follows the ideas introduced by robertson and seymour in [j'ctsb 95] . in this work we improve the dependence on k of hicks' result by showing that checking if h is a minor of g can be done in time o ( 2 ( 2k+1 ) log k h 2k 2 2h 2 m ) . our approach is based on a combinatorial object called rooted packing , which captures the properties of the potential models of subgraphs of h that we seek in our dynamic_programming algorithm . this formulation with rooted packings allows us to speed up the algorithm when g is embedded in a fixed surface , obtaining the first single-exponential algorithm for minor_containment testing . namely , it runs in time 2 o ( k ) h 2k 2 o ( h ) n , with n = |v ( g ) | . finally , we show that slight modifications of our algorithm permit to solve some related problems within the same time bounds , like induced minor or contraction minor_containment . 
learning through inquiry : student difficulties with online course-based material this study investigates the case-based learning experience of 133 undergraduate veterinarian science students . using qualitative methodologies from relational student learning research , variation in the quality of the learning experience was identified , ranging from coherent , deep , quality experiences of the cases , to experiences that separated significant aspects , such as the online case_histories , laboratory test_results , and annotated images emphasizing symptoms , from the meaning of the experience . a key outcome of this study was that a significant percentage of the students surveyed adopted a poor approach to learning with online resources in a blended experience even when their overall learning experience was related to cohesive conceptions of veterinary science , and that the difference was even more marked for less successful students . the outcomes from the study suggest that many students are unsure of how to approach the use of online resources in ways that are likely to maximise benefits for learning in blended experiences , and that the benefits from case-based learning such as authenticity and active_learning can be threatened if issues closely associated with qualitative variation arising from incoherence in the experience are not addressed . 
hidden conditional neural fields for continuous phoneme speech_recognition summary in this paper , we propose hidden conditional neural fields ( hcnf ) for continuous phoneme speech_recognition , which are a combination of hidden conditional random fields ( hcrf ) and a multi_layer perceptron ( mlp ) , and inherit their merits , namely , the discrimina-tive property for sequences from hcrf and the ability to extract non-linear features from an mlp . hcnf can incorporate many types of features from which non-linear features can be extracted , and is trained by sequential criteria . we first present the formulation of hcnf and then examine three methods to further improve automatic_speech_recognition using hcnf , which is an objective_function that explicitly considers training errors , provides a hierarchical tandem-style feature and includes a deep non-linear feature extractor for the observation function . we show that hcnf can be trained realistically without any initial model and outperforms hcrf and the triphone hidden_markov_model trained by the minimum phone error ( mpe ) manner using experimental_results for continuous english phoneme recognition on the timit core test_set and japanese phoneme recognition on the ipa 100 test_set . 
czech syntactic_analysis constraint_based - xdg : one possible start this article describes an attempt to implement a constraint_based dependency_grammar for czech , a language with rich morphology and free word_order , in the formalism extensible dependency_grammar ( xdg ) . the grammar rules are automatically inferred from the prague dependency treebank ( pdt ) and constrain dependency relations , modification frames and word_order , including non-projectivity . although these simple constraints are adequate from the linguistic point_of_view , their combination is still too weak and allows an exponential number of solutions for a sentence of n words . 1 motivation current approaches to syntactic_analysis of czech and other languages with a high degree of free word_order have limitations that are important from the theoretical point_of_view . first , all the available parsers are restricted to the surface syntactic_analysis and there is no simple way of extending them to include a deep_syntactic ( for instance tectogrammatical ) level of representation . second , the available statistical parsers produce only one solution for a given sentence , ignoring the possibility of the syntactic ambiguity of a sentence . and last but not least , the available parsers 1 are by nature statistical and do not contribute to the explanation of syntactic phenomena very much . several declarative ( relational ) approaches to syntax analysis overcoming these problems are available , including well known formalisms such as hpsg or lfg , or the robust constraint_based dependency parsing by ( foth , menzel , and schr_der , 2004 ) . another promising formalism is the extensible dependency_grammar 2 ( xdg , ( debusmann et al . , 2004 ) ) . none of these approaches has ever been tested on a language with rich morphology and freer word_order in a large scale . dependency_grammar is a formalism that excellently fits our needs : xdg is dependency based , as fgd is . xdg distinguishes between immediate dominance ( id , dependency ) relations and linear precedence ( lp ) ; constraints are allowed to speak about these two dimensions separately as well as simultaneously and the dimensions are mutually constraining each other . it is easy to handle non-projective constructions in xdg . both these issues are important with respect to the relatively free word_order of czech . xdg allows for handling such dimensions of language description as the deep_syntactic ( tec-togrammatical ) level . fgd's main objective is deep syntactic_structure . 1 rare exceptions include an unpublished parser for czech by zden k abokrtsk . 2 the term grammar is used here in the sense of a set of rules underlying syntactic or syntactico-semantic_analysis . 
need for undergraduate and graduate-level education in testing of microelectronic circuits and systems as deep_sub_micron and beyond technology emerges , quality_assurance of microelectronic circuits and systems becomes more important than ever . consequentially , ( 1 ) a strong need for well-educated microelectronic circuits and systems test engineers is desired by the industry , ( 2 ) graduate-level research efforts are also called to overcome numerous micro-electronic circuits and systems test issues . this paper is to address issues related to increasing impact of the electronic circuits and systems test field on education in electrical and computer engineering and to propose suitable educational topics for undergraduate and graduate-level electrical and computer engineering courses . 
deep_architectures for protein contact map prediction motivation residue-residue contact_prediction is important for protein_structure_prediction and other applications . however , the accuracy of current contact predictors often barely exceeds 20% on long_range contacts , falling short of the level required for ab_initio structure prediction . results here , we develop a novel machine_learning approach for contact map prediction using three steps of increasing resolution . first , we use 2d recursive neural_networks to predict coarse contacts and orientations between secondary_structure elements . second , we use an energy-based_method to align secondary_structure elements and predict contact probabilities between residues in contacting alpha_helices or strands . third , we use a deep neural network architecture to organize and progressively refine the prediction of contacts , integrating information over both space and time . we train the architecture on a large set of non-redundant proteins and test it on a large set of non-homologous domains , as well as on the set of protein_domains used for contact_prediction in the two most recent casp8 and casp9 experiments . for long_range contacts , the accuracy of the new cmappro predictor is close to 30% , a significant increase over existing approaches . availability cmappro is available as part of the scratch suite at http : //scratch . proteomics . ics . uci . edu/ . contact pfbaldi@uci . edu supplementary information supplementary data are available at bioinformatics online . 
high_performance traffic workload architecture for testing dpi systems traffic identification and classification are essential tasks performed by internet_service_providers ( isps ) administrators . deep_packet_inspection ( dpi ) is currently playing a key role in traffic identification and classification due to its increased expressive power . to allow fair comparison among different dpi techniques and system , workload generators should have the following characteristics : ( i ) synthetic packets with meaningful payloads; ( ii ) tcp and udp traffic generation; ( iii ) configurable network_traffic profile , and ( iv ) high_speed sending rate . filling this niche of interest , this paper proposes a workload generator framework which inherits all of the above characteristics . performance_evaluation shows that our flexible workload generator system achieves very high sending rates over a 10gbps network , using a commodity linux machine . additionally , we have configured and tested our workload generator following a real application traffic profile . we then have analyzed its results within a dpi system , proving its accuracy and efficiency . 
a radiation tolerant phase_locked_loop design for digital_electronics with decreasing feature_sizes , lowered supply_voltages and increasing operating_frequencies , the radiation tolerance of digital_circuits is becoming an increasingly_important problem . many radiation_hardening techniques have been presented in the literature for combinational as well as sequential_logic . however , the radiation tolerance of clock generation circuitry has received scant attention to date . recently , it has been shown that in the deep submicron regime , the clock network contributes significantly to the chip level soft_error_rate ( ser ) . the on-chip phase_locked_loop ( pll ) is particularly vulnerable to radiation strikes . in this paper , we present a radiation hardened pll design . each of the components of this design the voltage_controlled_oscillator ( vco ) , the phase frequency detector ( pfd ) and the loop filter are designed in a radiation tolerant manner . whenever possible , the circuit elements used in our pll exploit the fact that if a gate is implemented using only pmos ( nmos ) transistors then a radiation particle strike can result only in a logic 0 to 1 ( 1 to 0 ) flip . by separating the pmos and nmos devices , and splitting the gate output into two signals , extreme high_levels of radiation tolerance are obtained . our pll is tested for radiation immunity for critical charge values up to 250fc . our results_demonstrate that over a large number of radiation strikes on a number of sensitive nodes in our design , the worst case jitter is just 18% . in the worst case , our pll returns to the locked state in 16 cycles of the vco clock , after a radiation strike . i . introduction with relentless device scaling , lowered supply_voltages and higher operating_frequencies , the noise_margins of vlsi_designs are reducing . thus vlsi_circuits are becoming more vulnerable to noise due to crosstalk , power_supply variations and single_event_effects ( see ) or soft_errors . sees are caused when radiation particles such as protons , neutrons , alpha_particles , or heavy ions strike sensitive diffusion regions in vlsi_designs . these radiation particle strikes can deposit a charge , resulting in a voltage glitch on the affected node . this is particularly problematic for memories , since it can directly flip the stored state of a memory element , resulting in a single_event_upset ( seu ) [1] , [2] . although seu induced errors in sequential elements continue to be problematic , it is expected that soft errors in combinational_logic will become problematic in future technologies [3] , [4] , [5] . in a combinational circuit , 
inference of low_dimensional latent structure in high_dimensional_data ( ee ) abstract the problem of learning a latent model for sparse or low_dimensional representation of high_dimensional_data has attracted significant attention for many years . this thesis focuses on learning latent models for sparse or low_dimensional representation of images , dynamic data , and documents with bayesian nonparametrics . the thesis consists of three parts . first , nonparametric bayesian methods are considered for recovery of imagery based upon compressive measurements . a truncated beta-bernoulli_process is employed to infer an appropriate dictionary for the test data , and also for image recovery . in the context of compressive sensing , significant improvements in image recovery are manifested using learned dictionaries , relative to using standard orthonormal image expansions . the compressive-measurement projections are also optimized for the learned dictionary . spatial interrelationships within imagery are exploited through use of the dirichlet and probit stick-breaking processes . several example results are presented , with comparisons to other state-of-the-art methods in the literature . second , hierarchical bayesian methods are employed to learn a reversible statistical embedding . the proposed embedding procedure is connected to spectral embedding methods ( e . g . , diffusion maps and isomap ) , yielding a new statistical spectral framework . the proposed approach allows one to discard the training data when embedding new data , allows synthesis of high_dimensional_data from the embedding space , and provides accurate estimation of the latent-space dimensionality . hier-iv archical bayesian methods are also developed to learn a nonlinear dynamic model in the low_dimensional embedding space , allowing joint analysis of multiple types of dynamic data , sharing strength and inferring interrelationships . in addition to analyzing dynamic data , the learned model also yields effective synthesis . example results are presented for statistical embedding , latent-space dimensionality estimation , and analysis and synthesis of high_dimensional ( dynamic ) motion_capture data . third , a new hierarchical tree_based topic model is developed , based on nonpara-metric bayesian techniques . the model has two unique attributes : ( i ) a child node in the tree may have more than one parent , with the goal of eliminating redundant sub-topics deep in the tree; and ( ii ) parsimonious sub-topics are manifested , by removing redundant usage of words at multiple scales . the depth and width of the tree are unbounded within the prior , with a retrospective sampler employed to adap-tively infer the appropriate tree size based upon the corpus under study . excellent quantitative results are manifested on five standard data_sets , and the inferred tree_structure is also found to be highly interpretable . 
bilingual generation of job from quasi-conceptual descriptions forms the exclass system ( expert job evaluation assistant ) is intended to provide intelligent support for job description and classification in the canadian public service . the job description module ( jdm ) of exclass is used to create conceptual representations of job_descriptions , which are used for job evaluation and bilingual generation of textual job_descriptions . the design of these representations was subject to two opposing constiaints : ( 1 ) that they be deep enough to resolve the ambiguities present in textual job_descriptions , and ( 2 ) that they be close enough to surface linguistic forms that they can be conveniently manipulated by users with little specialized training . the close correspondence of concepts to surface words and phrases , as well as properties of the job description sublanguage , permit a simplified generator design , whereby phrases are prepackaged with a certain amount of linguistic structure , and combined according to a small set of mostly language-independent rules . text planning , consisting mainly of grouping and ordering of conjoined phrases , is performed manually by the user , and composition of conceptual forms is supported by a "continuous text feedback" function . 1 . goals of exclass the exclass system ( described on a more general level in korelsky & caldwell 1993 ) is intended to provide intelligent support for the process of describing and evaluating jobs in the canadian public service . the job description module ( jdm ) of exclass , developed by cogentex for the canadian treasury_board , provides resources for the user to compose conceptual representations of job_descriptions . the jdm generates textual job_descriptions in both english and french from these representations; a job evaluation module ( jem ) also reasons on them to produce a classification and rating of a job , according to the government's evolving universal classification standard . the first phase of the exclass project resulted in a proof-of-concept prototype , based on a sample of some 30 job_descriptions in the domain of procurement and asset_management , in which the jdm and jem are linked through a common graphical_interface . the second phase , concluded in the spring of 1994 , involved r&d in preparation for fielding and site testing of the system in a selected government department . exclass is intended to eventually be used by thousands of managers across canada , thus decreasing reliance on classification experts , while at the same time increasing the standardization , objectivity and comparability of job classifications across diverse occupational and organizational groupings . the principal task of the jdm is to produce an unam-biguous 
an energy_efficient triple_channel_uwb_based_cognitive_radio an energy_efficient triple_channel_uwb_based_cognitive_radio an energy_efficient triple_channel_uwb_based_cognitive_radio an energy_efficient triple_channel_uwb_based_cognitive_radio permission to make digital or hard_copies_of all or part of this work for personal or classroom use is granted_without_fee_provided that copies are not made or distributed for profit_or_commercial_advantage_and that copies_bear this notice and the full citation on the first page . to copy otherwise , to republish , to post on servers or to redistribute to lists , requires_prior_specific_permission . acknowledgement working and studying with brilliant people in the superb environment at uc_berkeley was one of the most fortunate and honorable opportunities in my whole life . most of all my gratitude to my research advisor , prof . jan m . rabaey is so sincere and deep that i can't even find proper words to express it . without the dedication , insight , and tolerance he showed me during my stay , i could not have reached this point , much less conducted proper research . i also would like to convey my deepest thanks to prof . ali m . ninejad for the discussion and his devotion to the field of research . it was also my honor to have prof . paul k . wright as my qualification committee . the proposed triple_channel_uwb_based_cognitive_radio exploits spectral crowding and coexistence of other wireless devices as the number of sensors and wearable computing devices increases in 3 . 1ghz to 10 . 6ghz ism_band to achieve energy_efficient 1gb/s short-range wireless_communication . a dual-resolution analog wavelet-based spectrum performs bandwidth-and frequency-agile band_pass_filter ( bpf ) to detect narrowband and wideband interferers with low power_consumption . a charge-pump-based triangular waveform generators and a source follower type low_pass_filter ( lpf ) generates basis_function for the spectrum sensing with 132mhz sensing resolution . a low power integer-n qpll with reduced reference spur by digital calibration on mismatch of the charge pump current supports the tuning frequencies with a linear tuned wide_range two stage ring-vco and a low power programmable true-single-phase-clock ( tspc ) divider . the proposed triple_channel_uwb_based_cognitive_radio was fabricated in 1v 65nm cmos gp process . the test_chip size is 2 . 3 2 mm 2 , and the active area is 2 . 1mm 2 . the data rate by using triangular shaped bpsk data is 1gb/s at 1m communication . the lowest fom of the energy/bit is 61pj/bit , and the highest fom is 102pj/bit . it achieves ber from 9 . 2 10-7 to 1 . 1 10-4 according to frequency_allocation of the triple-channels . the triple_channel_uwb_based_cognitive_radio can provide energy_efficient high-data rate wireless_communication even with 
dallas eeprom equipment profile for rapid integration and system modeling one definition of responsive space is the ability for mission-specific payloads and support systems to be rapidly integrated within a short period . however , as components are added to the spacecraft , the complex interactions between subsystems must be noted and , if possible , modeled . this process is extremely time consuming and , when done poorly ( or not at all ) , is a major contributor to spacecraft failure . a new paradigm is needed for rapid integration and system modeling . and testing by functionally combining their respective satellites , akoya and onyx . both vehicles were connected via a common power and data wiring harness , allowing one spacecraft to operate any device on either vehicle . despite possessing minimal prior_knowledge of the other school's subsystems , functional integration was achieved in less than thirty minutes . each satellite uses a distributed_computing architecture with a standardized interface and communication protocol . this architecture allows each subsystem to be developed separately and rapidly integrated into the spacecraft . the success of this experience led to an improved design for subsystem-level embedded operational intelligence . the dallas eeprom equipment profile ( deep ) architecture extends this standardized bus to include improved support for rapid integration and system modeling . deep is a protocol standard using the maxim/dallas 1-wire bus allowing for low_level control and monitoring of the spacecraft using commercial_off_the_shelf devices including memory and sensor devices . deep specifies a standard with which a representation of subsystem functionality is encoded within the subsystem itself , allowing for the creation of a satellite-wide model paralleling the physical integration of the spacecraft . this allows a stockpile of flight deep enabled subsystems , ready to be rapidly composed into a functional spacecraft . each subsystem includes a subsystem model , with parameters such as thermal and power characteristics , allowing an anomaly management system to identify off-nominal conditions through model-based_reasoning . additional functionality includes , automated ground operations and ground integration and test software generation , standard command planning , resource_allocation , and other areas of command and control . nanosatellite competition operated by the air_force_research_laboratory . this paper describes the current success of both universities with rapid integration , current development of the deep architecture , and future advances regarding responsive space . 
a readability checker with supervised_learning using deep_syntactic_and_semantic_indicators checking for readability or simplicity of texts is important for many institutional and individual users . formulas for approximately measuring text readability have a long tradition . usually , they exploit surface-oriented indicators like sentence length , word length , word frequency , etc . however , in many cases , this information is not adequate to realistically approximate the cognitive difficulties a person can have to understand a text . therefore we use deep_syntactic_and_semantic_indicators in addition . the syntactic_information is represented by a dependency_tree , the semantic_information by a semantic_network . both representations are automatically_generated by a deep syntactico-semantic_analysis . a global readability score is determined by applying a nearest neighbor algorithm on 3 , 000 ratings of 300 test persons . the evaluation showed , that the deep_syntactic_and_semantic_indicators lead to quite comparable results to most surface-based indicators . finally , a graphical user interface has been developed which highlights difficult-to-read text passages , depending on the individual indicator values , and displays a global readability score . 
iddq test_challenges in nanotechnologies : a manufacturing_test strategy the implementation of iddq_test is increasingly challenging with the shrinking of process geometry in nanotechnologies . this paper presents a case study of the test_challenges that the industry is facing in deep_submicron process . an iddq manufacturing_test strategy is discussed to address the challenges . 
crosstalk_fault testing by using oscillation ring testing methodology for soc interconnection lines the advance in ic processing technology rapidly reduces spacing between adjacent wires; which renders crosstalk_fault an important source of anomaly in deep subcicrom vlsi . as a result , crosstalk fault_detection should be an essential part in soc testing . although ieee p1500 has been developed to test interconnects in soc . this standard is not suitable for coupling fault testing . in this paper , we propose a new testing scheme , namely the oscillation ring testing . this method is very efficient for crosstalk fault_detection , which is otherwise very difficult under traditional test scheme . we also propose a systematic way to find out oscillation rings and test_patterns . we have conducted experiments on the proposed scheme with several test circuits consisting of iscas benchmarks , and the results show that crosstalk_faults can be detected with a very small number of test_patterns . 
force-detecting gripper and force_feedback system for neurosurgery applications purpose for the application of less invasive robotic neurosurgery to the resection of deep-seated tumors , a prototype system of a force-detecting gripper with a flexible micromanipulator and force_feedback to the operating unit will be developed . methods gripping force applied on the gripper is detected by strain gauges attached to the gripper clip . the signal is transmitted to the amplifier by wires running through the inner tube of the manipulator . proportional force is applied on the finger lever of the operating unit by the surgeon using a bilateral control program . a pulling_force experienced by the gripper is also detected at the gripper clip . the signal for the pulling_force is transmitted in a manner identical to that mentioned previously , and the proportional torque is applied on the touching roller of the finger lever of the operating unit . the surgeon can feel the gripping force as the resistance of the operating force of the finger and can feel the pulling_force as the friction at the finger surface . results a basic operation test showed that both the gripping force and pulling_force were clearly detected in the gripping of soft material and that the operator could feel the gripping force and pulling_force at the finger lever of the operating unit . conclusions a prototype of the force_feedback in the microgripping manipulator system has been developed . the system will be useful for removing deep-seated brain tumors in future master-slave-type robotic neurosurgery . 
cover_free_families and topology-transparent communication imagine that tens or hundreds of thousands of sensors are deployed in a person's circulatory system to monitor blood chemistry , travelling on currents in the blood_stream . our task is to enable these sensors to communicate with one another effectively , to provide multihop paths to fixed monitoring stations , and to provide clinical personnel with the critical data needed to respond . the sensors can move rapidly and unpredictably . they must operate at very low transmission power to avoid tissue damage . in this extreme case any knowledge of the network_topology is at best severely limited . indeed , topology discovery has limited value in the face of such rapid changes . protocols are needed that do not require a sensor to know the identity of its neighbours topology-transparent communication . with violet syrotiuk and others , we have recently shown that topology-transparent communication can be achieved using cover_free_families of sets . these have a deep mathematical history in group_testing . combinatorial group_testing has been the main tool to isolate defectives , by identifying pools of members with one or more defectives . uses include disease screening , identifying clones in dna libraries , and communications . in satellite_communications , group_testing is used to schedule polling of ground stations in groups . instead our goal is to schedule transmissions so that every node pair has a collision-free transmission opportunity . such topology-transparency has been , until now , a theoretical curiosity . we have made major inroads in changing this , by establishing that the substantial theory of cover_free_families provides the necessary mathematical basis , and by overcoming many of the obstacles to implementation . the basic_research to bring these into application in the large sensor_networks to come requires generalization of the mathematical foundations from cover_free_families , and their experimentation in practical scenarios . in this talk , an introduction to cover_free_families and to topology-tranparency is given . then we describe a three-state generalization of cover_free_families for application in sensor_networks . since sensors are energy-limited , any transmission schedule must address energy savings . this requires the scheduling not only of times for transmission and reception , but also for sleep . the resulting combinato-rial problem yields a new , and largely unexplored , extension of cover_free_families to permit three states rather than two . while focussing on the combinatorial problem , experimental_results from simulations in ad_hoc and sensor_networks will also be presented . 
warm deep_drawing of aluminium sheet aluminium sheet drawing processes can be improved by manipulating local flow behaviour by means of elevated temperatures and temperature_gradients in the tooling . forming tests showed that a substantial improvement is possible not only for 5xxx but also for 6xxx series alloys . finite_element_method simulations can be a powerful tool for the design of warm forming processes and tooling . their accuracy will depend on the availability of materials models that are capable of describing the influence of temperature and strain_rate on the flow stresses . two models , an adapted nadai power_law and a dislocation based bergstr m type model , are compared by means of simulations of a cup drawing process . experimental drawing test_data are used to validate the modelling approaches , whereas the model parameters follow from tensile tests . 1 introduction the need for lighter car bodies on the one hand side and the complicated shapes of car parts on the other hand side result in a quest for improving the formability of aluminium sheet . aluminium has a large potential for weight reduction , but press operations are more critical than with steel . the alloys used for automotive sheet components are mostly 5xxx and 6xxx alloys . the 5xxx alloys have the best formability , but cannot be used for outer panels because of stretcher strains . these are mostly made from 6xxx alloys , which are however less suitable for complicated inner parts because of a lesser formability . the formability can be improved by using elevated temperatures and temperature_gradients in the tooling and blank , which make it possible to manipulate local flow [1-3] . an extra benefit of warm forming is that stretcher strains do not occur in 5xxx series alloys at elevated temperatures . in this paper the effect of warm drawing ( in the temperature range up to 250 o c ) on the process limits of a 1 . 2 mm gauge 5754-o and 6016-t4 alloy sheet and on the mechanical_properties of the formed material are demonstrated . the introduction of warm drawing technology will be greatly helped if finite_element_method simulations are available for process and tooling design . hence fem simulations , including material models of warm flow behaviour , are developed and validated . 
functional and structural reasoning in configuration tasks a configuration problem within the technical domain is the problem of putting together different parts into a complex system , given a description of the possible parts and the functionality of the required system . while workin~ with our master thesis ( leitgeb , pernler 95 ) at sics ~ during 93 and 94 , we found only a few configuration systems extracting the functional reasoning from the structural . we herein motivate the need for functional reasoning as a basis to find meaningful control structures separated from the application domain_knowledge , and that this , together with a separated structural approach to describe the possible parts of the artefact , might be a neat solution . we have defined a model for representing configuration problems in two dimensions : functional knowledge of the function of the artefact . structural knowledge about structure and possible parts of the artefact . we have named this model the fast model-the functional and structural model , and tested it on a real world application . when we create an object through synthesis 2 , what do we strive for ? in reality , we are not just looking for an object-no , we search for an object providing a certain number of functions in its environment . the object is characterised both by the functions it provides to its environment , and the internal functions it uses to fulfil the external functionality-without its internal functions , the object can not perform the external ones . in classical science , when talking about a physical object , we can characterise it both through its structure ( its parts ) and its functions ( pirsig 74 ) . we claim that the most important characterisation is the functional in synthesis , we search for parts because of 2such as configuration , design etc . their functions . we are not primarily concerned with aesthetical objects-we do not want amy structural part not motivated by a function . each part realizes one or several necessary functions , or gives the overall system a required quality . our thesis is that no structural description within the synthesis domain is justified by itself . our willingness to use a structural description and a structural reasoning as the basis for synthesis , comes from our view of the object-we cam see its structure , touch its parts , divide it into pieces . you can hardly see all the external and internal functions of a motorcycle , for example . to divide its functions into sub-functions requires deep knowledge of the function of the motorcycle . still , ff you want to repair 
function extraction ( fx ) technology : computing the behavior of software the idea of software behavior computation modern society is dependent on software systems of ever-increasing scope and complexity . these systems continue to experience errors and vulnerabilities despite best efforts in their development . programmers today have no practical means to determine the full functional behavior of software . this technology gap has been the source of problems and frustrations in software for decades . software engineers must understand all of the behavior of software , intended or unintended , benign or malicious . while current software_development and testing tools can help analyze specific properties and cases of behavior , what is needed is an " all cases of behavior " understanding of what software does . the cert ( computer emergency_response team ) organization of the software_engineering_institute at cmu is developing the emerging technology of function extraction ( fx ) to automate the calculation of software behavior with mathematical precision to the maximum extent possible [1] . fx is a new type of engineering automation . the objective is to reduce dependence on slow , fallible , manual methods of software analysis by substituting fast , correct computation of behavior . computing the precise behavior of software requires deriving its net functional effect; that is , how it transforms inputs into outputs in all circumstances of use , without heuristics or approximations . that information can be presented to analysts in non-procedural behavior displays that define all the possible effects a program can have , essentially , the " all cases of behavior " view . the ultimate objective is to move from an uncertain understanding of software derived in human time scale to a precise understanding computed in machine time scale . theory-based function extraction operates on the deep functional semantics of software , and is not subject to the limitations of traditional syntactic methods [2] , [3] , [4] , [5] . controlled experimentation showed that users of an fx prototype were orders_of_magnitude faster than users of manual methods in determining software functionality , and were much more productive when computed behavior was available [6] . a miniature example of behavior computation in notional illustration of behavior computation , consider the following sequence structure of three instructions ( " : = " is the assignment operator ) operating on integer variables x and y ( machine precision aside ) , and the question of what it does : do x : = x y y : = y + x x : = y x enddo the function extraction process computes a procedure-free_expression of what this structure does from beginning to end 
cheops : a compact explorer for complex hierarchies as the amount of electronic information explodes , hierarchies to handle this information become huge and complex . visualizing and interacting with these hierarchies become daunting tasks . the problem is exacerbated if the visualization is to be done on mass-market personal_computers , with limited processing power and visual resolution . many of the current visualization techniques work effectively for hierarchies of 1000 nodes . but as the number of nodes increases toward 5000 nodes , these techniques tend to break down . hierarchies above 5000 nodes usually require special modifications such as clustering , which can affect visual stability . this paper introduces cheops . a novel approach to the representation , browsing and exploration of huge , complex information hierarchies such as the dewey decimal system , which can contain between i million and i billion nodes . the cheops approach maintains context within a huge hierarchy , while simultaneously providing easy access to details . this paper will also present some preliminary_results from usability tests performed on an 8 wide by 9 deep classification hierarchy , which if fully populated would contain over i9 million nodes . 
wireless_network deployment configurations : dwesa marginalized area as a case study several technological initiatives have been , and will continue to be implemented across the world to tackle the major barriers in bridging the digital_divide . these include the use of internet and other icts . this is seen as a gigantic step in the alleviation of the different paramount social-economic problems , at the same time , in the transformation of the society and realization of a truly free and democratic world society . the use of technology in bridging this gap is a fundamental advancement because it also brings with it a mutual understanding and elimination of differential powers within communities in both developing and developed countries worldwide . this paper describes how different wireless access technologies can be combined and deployed to facilitate a continuous flow of information , in and out of the marginalized areas in developing_countries . it further explains again , the major role of having internet connectivity in these areas and how it's seen as an improvement in successfully deployment of ict4d activities . different technologies such as wifi , wimax and vsat will be explored and discussed for the best wireless deployment configurations for the siyakhula living lab ( sll ) . sll was chosen as our test_bed and it is located in dwesa community . this is one of the deep marginalized rural communities in south_africa found in the eastern_cape region . 
measurement-based deep venous_thrombosis screening system an experimental system and interface that indicate the likelihood of deep venous_thrombosis using objective measures was developed , based on ultrasound image_processing using a modified star-kalman algorithm and a sensorized ultrasound probe . force , location and image_data is used to assess a vessel segment for compression . a user_interface displays the results using a 3-d representation . a tissue phantom was developed for testing and validation . initial results with this phantom and healthy volunteers are presented . 
classifying and visualizing motion_capture sequences using deep neural networks the gesture_recognition using motion_capture data and depth sensors has recently drawn more attention in vision recognition . currently most systems only classify dataset with a couple of dozens different actions . moreover , feature_extraction from the data is often computational complex . in this paper , we propose a novel system to recognize the actions from skeleton data with simple , but effective , features using deep neural networks . features are extracted for each frame based on the relative positions of joints ( po ) , temporal differences ( td ) , and normalized trajectories of motion ( nt ) . given these features a hybrid multi_layer perceptron is trained , which simultaneously classifies and reconstructs input data . we use deep autoencoder to visualize learnt features , and the experiments show that deep_neural_networks can capture more discriminative information than , for instance , principal_component_analysis can . we test our system on a public database with 65 classes and more than 2 , 000 motion sequences . we obtain an accuracy above 95% which is , to our knowledge , the state of the art result for such a large dataset . 
circuit_design for logic_automata circuit_design for logic_automata the logic_automata model is a universal distributed_computing structure which pushes parallelism to the bit_level extreme . this new model drastically differs from conventional computer architectures in that it exposes , rather than hides , the physics underlying the computation by accomodating data_processing and storage in a local and distributed manner . based on logic_automata , highly scalable computing struc-trues for digital_and_analog processing have been developed; and they are verified at the transistor level in this thesis . the asynchronous logic_automata ( ala ) model is derived by adding the temporal locality , i . e . , the asynchrony in data exchanges , in addition to the spacial locality of the logic_automata model . as a demonstration of this incrementally extensible , clockless structure , we designed an ala cell_library in 90 nm cmos_technology and established a " pick-and-place " design_flow for fast ala circuit layout . the work flow gracefully aligns the description of computer programs and circuit realizations , providing a simpler and more scalable solution for application_specific_integrated_circuit ( asic ) designs , which are currently limited by global contraints such as the clock and long interconnects . the potential of the ala circuit design_flow is tested with example applications for mathematical operations . the same logic_automata model can also be augmented by relaxing the digital states into analog ones for interesting analog computations . the analog logic_automata ( anla ) model is a merge of the analog logic principle and the logic_automata arhitecture , in which efficient processing is embedded onto a scalable construction . in order to study the unique property of this mixed_signal computing structure , we designed and fabricated an anla test_chip in ami 0 . 5 m cmos_technology . chip tests of an anla noise-locked loop ( nll ) circuit as well as application tests of anla image_processing and error_correcting_code ( ecc ) decoding , show large potential of the anla structure . acknowledgments i would like to acknowledge the support of mit's center for bits and atoms and its sponsors . thank you to my thesis supervisor , neil_gershenfeld , for his intriguing guidance and encouragement over the past two years . his wide knowledge span and deep insight have been inspiring me to keep learning and thinking; and his openness to ideas has encouraged me to always seek better solutions to problems i encounter . thank you for directing the center for bits and atoms and the physics and media research_group , which provide me great intellectual_freedom and wide vision . he has 
fault clustering in deep-submicron cmos processes the fraction of ics that pass all production tests but fail in the application is called the defect level . defect levels depend on the average number of defects per ic , and also on the clustering of these defects . high clustering leads to a higher yield and a lower defect level . this paper compiles the coefficients for defect clustering using research findings from 1970 until 2001 . because recent data for deep_submicron processes are missing in the literature , the clustering_coefficient has been calculated using scan fail distributions of ics in a 180 nm process . clustering coefficients show a steady trend towards higher defect clustering . this is beneficial , but it is probably not sufficient to achieve today's ambitious target of 'zero defects' . 
generating efficient test_sets with a model_checker it is well-known that counterexamples produced by model_checkers can provide a basis for automated generation of test_cases . however , when this approach is used to meet a coverage criterion , it generally results in very inefficient test_sets having many tests and much redundancy . we describe an improved approach that uses model_checkers to generate efficient test_sets . furthermore , the generation is itself efficient , and is able to reach deep regions of the statespace . we have prototyped the approach using the model_checkers of our sal system and have applied it to model_based designs developed in stateflow . in one example , our method achieves complete state and transition coverage in a stateflow model for the shift sched-uler of a 4-speed automatic_transmission with a single test_case . 
slope : a quick and accurate method for locating non-snp structural variation from targeted next_generation sequence data motivation targeted 'deep' sequencing of specific genes or regions is of great interest in clinical cancer diagnostics where some sequence variants , particularly translocations and indels , have known prognostic or diagnostic significance . in this setting , it is unnecessary to sequence an entire genome , and target capture methods can be applied to limit sequencing to important regions , thereby reducing costs and the time required to complete testing . existing 'next-gen' sequencing analysis packages are optimized for efficiency in whole-genome studies and are unable to benefit from the particular structure of targeted sequence data . results we developed slope to detect structural variants from targeted short-dna reads . we use both real and simulated data to demonstrate slope's ability to rapidly detect insertion/deletion events of various sizes as well as translocations and viral integration sites with high sensitivity and low false_discovery_rate . availability binary_code available at http : //www-genepi . med . utah . edu/suppl/slope/index . html
computer_chess championship fter twenty years of travel-ing from city to city across the united_states , the acm north_american computer_chess championship came back to the place of its birth , the new york hilton hotel , where the competitions began in 1970 . this latest five-round event ended in a two-way tie for first place between mephisto and deep_thought/88 . finishing in a two-way tie for third place were hitech and m chess . a total of 10 teams participated , and the level of play was at the low grand-master level . a special three-round end-game championship was won by mephisto , who also captured the prize for the best small computing system . a total of $8000 in prizes was divided up among the winners . deep_thought/88 , currently under development at ibm by researchers feng-hsiung hsu , murray campbell , and thomas anantharaman along with two former associates at carnegie_mellon_university , peter jensen and andreas nowat-zyk , outplayed mephisto in the third round but lost to hitech in the next round . it entered the final round of play one-half point behind hitech , who had won all of its games with the exception of a first-round draw with zarkov . deep_thought/88 defeated zarkov in the final round while hitech lost on time to mephisto in a dead-drawn game . mephisto won all of its games with the exception of its third-round loss to deep_thought/88 . mephisto played solid chess throughout the event , but was fortunate to win its game against hitech in the final round . mephisto , developed by richard lang of great_britain , is marketed by the ger-man company of hegener & glaser a . g . the rules of the tournament required each side to play all of its moves within a two-hour period ensuring that the games would last at most four hours . mephisto played slightly faster than hitech in the middle game and entered the endgame with approximately five more minutes on its clock . hitech , who played even with mephisto , was unable to regain the lost time and eventually lost a dead-drawn game on move 145 . the tournament was marred by difficulties . this was the first time the tournament was played during the day , and bob hyatt was unable to make the necessary arrangements . deep_thought/88 was used by hsu and company when they found they did not have sufficient time to test out their latest version . zerker , a new entry developed by james testa at the university of california , 
temperature-aware soc test_scheduling considering inter-chip process_variation systems on chip implemented with deep_submicron_technologies suffer from two undesirable effects , high power_density , thus high_temperature , and high process_variation , which must be addressed in the test_process . this paper presents two temperature-aware scheduling approaches to maximize the test throughput in the presence of inter-chip process_variation . the first approach , an off_line technique , improves the test throughput by extending the traditional scheduling method . the second approach , a hybrid one , improves further the test throughput with a chip classification scheme at test time based on the reading of a temperature sensor . experimental_results have demonstrated the efficiency of the proposed methods . 
efficient electronic integrals and their generalized derivatives for object_oriented implementations of electronic_structure calculations for the new parallel implementation of electronic_structure methods in aces iii ( lotrich et al . , in preparation ) the present state-of-the-art algorithms for the evaluation of electronic integrals and their generalized derivatives were implemented in new object_oriented codes with attention paid to efficient execution on modern processors with a deep hierarchy of data storage including multiple caches and memory banks . particular attention has been paid to define proper integral blocks as basic building objects . these objects are stand-alone units and are no longer tied to any specific software . they can hence be used by any quantum_chemistry code without modification . the integral blocks can be called at any time and in any sequence during the execution of an electronic_structure program . evaluation efficiency of these integral objects has been carefully tested and it compares well with other fast integral programs in the community . correctness of the objects has been demonstrated by several application runs on real systems using the aces iii program . 
use of patterns for detection of likely answer strings : a systematic approach the paper describes the question_answering approach applied first at trec-10 qa track and developed systematically in trec 2002 experiments . the approach is based on the assumption that answers can be identified by their correspondence to formulas describing the structure of strings carrying certain ( generalized ) semantics , supposed by the question type . these formulas , or patterns , are like regular_expressions but include elements corresponding to predefined lists of terms . complex patterns can be constructed from blocks corresponding to such semantic entities as persons' or organizations' names , posts , dates , locations , etc . using various combinations of blocks and intermediate syntactic elements allows to build a great variety of patterns . exact position of elements corresponding to the "exact answer" was localized within the structure of each pattern . each pattern is characterized by a generalized semantics , thus the pattern_matching string must be checked for correlation with the question terms and/or their synonyms/substitutes . in 2002 trec qa track tests we have further developed the approach described in [soubbotin , 2001] . in general , our method lies in the domain of approaches examining the potential of information_extraction for question_answering tasks ( mucs ) , shows a certain shift from deep text analysis based on computational linguistic and nlp methods to surface techniques [eagles , 1998] . our approach can be considered as being in line with this tendency . more specifically , our approach is based on the use of formulas describing the structure of strings likely bearing certain semantic_information . for example , string "fbi director louis_freeh" can be recognized , according to one of such formulas , as likely bearing the following information : a person represented by his/her first and last names occupies a ( leading ) post in an organization . the formula for this string is : a word composed of capital letters; an item from the list of posts in an organization; an item from the list of first names; a capitalized word . we can mark two first items in this formula as "exact answer" , if we want to get answer to the question "who is louis_freeh ? " , and two last items , if the question is "who is fbi head ? " ( question 1583 at trec 2002 ) . first used at trec-10 qa track , formulas of such kind were called "patterns" [soubbotin m . m and soubbotin s . m , 2001] . the term "pattern" is widely used in the field of information_extraction . our concept of patterns as structural formulas for strings is obviously different from that in "traditional" ie field , but 
web_analytics 2 . 0 : empowering customer centricity in this two-part article , we start by describing the most standard practices of web_analytics; the first steps required to analyze a website and understand the behavior of its surfers . for this purpose we present a web_analytics process created by the authors based on industry best practices . the paper details each step of the process , going from defining goals and kpis to collect , analyze , and take action using website data . instead of presenting a single case_study , we chose to spread real_life examples throughout the article , enabling readers to connect each section to its practical application more easily . following the hands-on process , part ii proposes a pioneering concept of the next generation web_analytics or , as we call it , web_analytics 2 . 0 . this concept advocates a holistic approach to website analysis in which we consider several sources of knowledge : website data , multichannel analysis , testing , competitive analysis , and customers' voice . the papers are especially valuable to people managing , maintaining or optimizing websites because they provide the tools to analyze and improve online customer_experience and website profitability . introduction web_analytics is the science and the art of improving websites to increase their profitability by improving the customer's website experience . it is a science because it uses statistics , data_mining techniques , and a methodological process . it is an art because , like a brilliant painter , the analyst or marketer has to draw from a diverse pallet of colors ( data sources ) to find the perfect mix that will yield actionable insights . it is also an art because improving websites requires a deep level of creativity , balancing user-centric design , promotions , content , images , and more . besides , the analyst is always walking the fine line among website designers , it personnel , marketers , senior management and customers . by now , website managers are aware that visitor acquisition is a multi-faceted endeavor , which makes use of the following techniques : email , mail , affiliate_marketing and of course search . with each option they have become better at finding the right visitor to bring to their websites . for example , every website now has a search_engine_optimization ( seo ) strategy that will help them rank highly on search_engine organic results . likewise they are also aware that pay-per-click ( ppc ) campaigns can be effective at driving relevant visitors . acquiring visitors is only the start of the process rather than , as many marketers believe , the end . jim sterne and matt cuttler provide an excellent explanation 
deep_transfer via second-order markov logic standard inductive learning requires that training_and_test instances come from the same distribution . transfer_learning seeks to remove this restriction . in shallow transfer , test instances are from the same domain , but have a different distribution . in deep_transfer , test instances are from a different domain entirely ( i . e . , described by different predicates ) . humans routinely perform deep_transfer , but few learning systems , if any , are capable of it . in this paper we propose an approach based on a form of second-order markov logic . our algorithm discovers structural regularities in the source domain in the form of markov logic formulas with predicate variables , and instantiates these formulas with predicates from the target domain . using this approach , we have successfully transferred learned knowledge among molecular_biology , social_network and web domains . the discovered patterns include broadly useful properties of predicates , like symmetry and transitivity , and relations among predicates , such as various forms of homophily . 
generating and prioritizing optimal paths using ant_colony_optimization the assurance of software reliability partially depends on testing . numbers of approaches for software_testing are available with their proclaimed advantages and limitations , but accessibility of any one of them is a subject dependent . time is a critical factor in deciding cost of any project . a deep insight has shown that executing test_cases are time consuming and tedious activity . thus stress has been given to develop algorithms which can suggest better pathways for testing . one such algorithm called path prioritization ant_colony_optimization ( pp-aco ) has been suggested in this paper which is inspired by real ant's foraging behavior to generate optimal paths sequence of a decision to decision ( dd ) path of a graph . the algorithm does full path coverage and suggests the best optimal sequences of path in path testing and prioritizes them according to path strength . 1 introduction testing is an important aspect of the software_development life cycle . it focuses on the process of testing the newly_developed / under development software system , prior to its use . the program is executed with desired input ( s ) and the output ( s ) is / are observed accordingly . observed/actual output ( s ) is compared with the expected output ( s ) , if they are same then the program under test is said to be correct as per its specification ( s ) , otherwise there is something wrong somewhere in the program . testing is the process of executing a program with the intent of finding faults ( mayers , 1977 ) . the growing importance of software systems for small and big organization results in more complex software systems . thus it is one of the logic that more stress has been given on quality of developed software ( s ) . history has shown that software faults not only caused a loss of money but also precious human lives . thus if we don't improve the quality while system is under development , these losses may only get bigger ( beizer , 1990 ) . in this paper we have proposed a novel algorithm for automatic generation and prioritization of optimal
a method for studying jaw_muscle activity during standardized jaw movements under experimental jaw muscle_pain . this paper describes a method for studying superficial and deep jaw_muscle activity during standardized jaw movements under experimental jaw muscle_pain . in 22 healthy adults , pain was elicited in the right masseter_muscle via tonic infusion of 4 . 5% hypertonic saline and which resulted in scores of 30-60 mm on a 100-mm visual analogue scale . subjects performed tasks in five sessions in a repeated measures design , i . e . , control 1 , test 1 ( during hypertonic or isotonic saline infusion ) , control 2 ( without infusion ) , test 2 ( during isotonic or hypertonic saline infusion ) , control 3 ( without infusion ) . during each session , subjects performed maximal clenching and standardized jaw tasks , i . e . , protrusion , lateral excursion , open/close , chewing . mandibular movement was recorded with a 6-degree_of_freedom tracking system simultaneously with electromyographic ( emg ) activity from the inferior head of the lateral pterygoid muscle with fine-wire electrodes ( verified by computer tomography ) , and from posterior temporalis , the submandibular muscle group and bilateral masseter muscles with surface electrodes . emg root mean square values were calculated at each 0 . 5 mm increment of mandibular incisor movement for all tasks under each experimental session . this establishes an experimental model for testing the effects of pain on jaw_muscle activity where the jaw motor system is required to perform goal_directed tasks , and therefore should extend our understanding of the effects of pain on the jaw motor system . 
the integrated or efficiency and effectiveness evaluation after two years use , a pilot study objectives technology evaluation of integrated/digital or is needed since very little literature has been published on the subject . the integrated or is a technological solution intended for minimally invasive surgery where the surgeons have complete control of the environment , devices and image distribution . before such an investment , health technology_assessment can be used as a method to evaluate what vendors' state , i . e . the fact that the integrated or is a very effective and efficient solution . then a follow-up evaluation could be useful after the installation to test the users' satisfaction and give suggestions to the community about real- experienced integrated or advantage . methods a multiple answer questionnaire has been handed to 17 surgeons and 9 scrub nurses form varese town and university hospital to evaluate the degree of satisfaction after 2 years of use of integrated ors . results surgeons and scrub nurses agree that the integrated or can be very effective in increasing quality , risk reduction and surgery time reduction through the use of digitalized video acquisition system , boom-mounted devices and multiple displays . scrub nurses are a little bit more confident than surgeons that medical_device control could reduce the confusion inside the or and reduce the number of setting errors . a very positive judgment was given to the system's teaching capabilities , but both surgeons and scrub nurses agree that a great degree of education and a cultural change are needed to use the system in a correct and complete way . conclusions results show that there is a deep appreciation of the system which proved to be efficient ( reducing surgery time and enhancing surgical quality ) and effective . this is a pilot study based on few collected data , but the questionnaire could be handed to many hospitals where integrated ors are present , in order to achieve a significant degree of assessment and find common topics to be considered fundamental especially in the evaluating phase . 
contrast limited adaptive histogram equalization image_processing to improve the detection of simulated spiculations in dense mammograms the purpose of this project was to determine whether contrast limited adaptive histogram equalization ( clahe ) improves detection of simulated spiculations in dense mammograms . lines simulating the appearance of spiculations , a common marker of malignancy when visualized with masses , were embedded in dense mammograms digitized at 50 micron pixels , 12 bits deep . film images with no clahe applied were compared to film images with nine different combinations of clip levels and region sizes applied . a simulated spiculation was embedded in a background of dense breast tissue , with the orientation of the spiculation varied . the key variables involved in each trial included the orientation of the spiculation , contrast level of the spiculation and the clahe settings applied to the image . combining the 10 clahe conditions , 4 contrast levels and 4 orientations gave 160 combinations . the trials were constructed by pairing 160 combinations of key variables with 40 backgrounds . twenty student observers were asked to detect the orientation of the spiculation in the image . there was a statistically_significant improvement in detection performance for spiculations with clahe over unenhanced images when the region size was set at 32 with a clip level of 2 , and when the region size was set at 32 with a clip level of 4 . the selected clahe settings should be tested in the clinic with digital mammograms to determine whether detection of spiculations associated with masses detected at mammography can be improved . 
monotone systems under negative_feedback 1 oscillations in i/o monotone systems under negative_feedback oscillatory behavior is a key property of many biological systems . the small-gain theorem ( sgt ) for input/output monotone systems provides a sufficient condition for global asymptotic stability of an equilibrium and hence its violation is a necessary condition for the existence of periodic solutions . one advantage of the use of the monotone sgt technique is its robustness with respect to all perturbations that preserve monotonicity and stability properties of a very low_dimensional ( in many interesting examples , just one-dimensional ) model reduction . this robustness makes the technique useful in the analysis of molecular biological models in which there is large uncertainty regarding the values of kinetic and other parameters . however , verifying the conditions needed in order to apply the sgt is not always easy . this paper provides an approach to the verification of the needed properties , and illustrates the approach through an application to a classical model of circadian oscillations , as a nontrivial " case_study , " and also provides a theorem in the converse direction of predicting oscillations when the sgt conditions fail . i . introduction motivated by applications to cell_signaling , our previous paper [1] introduced the class of monotone input/output systems , and provided a technique for the analysis of negative feedback_loops around such systems . the main theorem gave a simple graphical test which may be interpreted as a monotone small gain theorem ( " sgt " from now on ) for establishing the global asymptotic stability of a unique equilibrium , a stability that persists even under arbitrary transmission delays in the feedback_loop . since that paper , various papers have followed-up on these ideas , see for example the first purpose is to develop explicit conditions so as to make it easier to apply the sgt theorem , for a class of systems of biological significance , a subset of the class of tridiagonal systems with inputs and outputs . tridiagonal systems ( with no inputs and outputs ) were introduced largely for the study of gene networks and population models , and many results are known for them , see for instance [32] , [34] . deep achievements of the theory include the generalization of the poincar-bendixson theorem , from planar systems to tridiagonal systems of arbitrary dimension , due to mallet-paret and smith [29] as well as a later generalization to
internet repositories for collaborative_learning : supporting both students and teachers most efforts to create computer_supported collaborative_learning environments have been focused on students . however , without providing appropriate integration of collaborative activities into curricula , these efforts will have little widespread impact on educational practices . to improve education through technology , learning environments for students must be integrated with curriculum development_tools for teachers to create an integrated collaboration-oriented classroom . this paper describes how software tools for internet repositories can aid fundamental collaboration activities locating , using , adapting , and sharing at both the teacher level ( with the teacher's curriculum assistant ) and the student level ( with the remote exploratorium ) . it illustrates how tools for educators and tools for students can be orchestrated into integrated classroom support . 1 . collaborative activities require support the goal of encouraging groups of learners to engage collaboratively in problem_solving activities has much merit . social_interaction fosters deep_learning in which students develop intellectual structures that allow them to create their own knowledge [27] . it promotes social_skills that help people participate in the social construction of their shared reality [3] . it increases student engagement and brings out the relevance of learning [16] . it allows the educational process to be more student-centered , less disciplinary , and more exciting [14 , 15] . the use of technology to foster collaborative_learning is often seen as a key to reforming science_education on the principle that the best way to learn science is to engage in the practice of science [10] . the practices of modern science involve the use of technologic tools for : observing and measuring interesting phenomena in the world , generating representations and visualizations of the data , and creating simulations to understand observed processes and to test hypotheses . importantly , the practice of modern science is highly collaborative . scientists work together to incrementally design experiments and simulations , to convergently develop hypotheses and theories , and to test and evaluate their work [17 , 22] . many projects have successfully combined these elements to foster innovative forms of collaborative science_education among students [8 , 12 , 24 , 26] . however , research projects have often been unable to transfer their successful results to other sites or schools because they did not replicate the initial teacher learning that occurred implicitly in the teacher-researcher and teacher-teacher collaborations [21] . for educational change to succeed , teachers too must be supported in changing from an isolated teaching model to one of collaborative_learning with other educators [4] . we believe that for collaborative_learning to succeed in the classroom , collaborative 
experiences with cicerobot , a museum guide cognitive robot the paper describes cicerobot , a robot based on a cognitive_architecture for robot vision and action . the aim of the architecture is to integrate visual_perception and actions with knowledge_representation , in order to let the robot to generate a deep inner understanding of its environment . the principled integration of perception , action and of symbolic knowledge is based on the introduction of an intermediate representation based on g rdenfors conceptual spaces . the architecture has been tested on a rwi b21 autonomous_robot on tasks related with guided tours in the archaeological museum of agrigento . experimental_results are presented . 
assessing inquiry learning inquiry and river_city in this paper , we provide an overview of the design of an inquiry_based curriculum project , and then offer a comparative analysis of the outcomes of two methods for assessing student understanding of the inquiry process . our findings indicate that the complex nature of scientific_inquiry is better captured using an alternative method of assessment in addition to a more traditional multiple_choice test . recently issued a position statement recommending the use of science inquiry as a method to help students understand the processes and content of science ( national_science_teachers_association , 2004 ) . however , currently , there is a competing push in science for coverage of material found on state and national standardized_tests; in many situations , this competing push forces the emphasis in science classrooms to change from inquiry_based instruction to test_preparation ( falk & drayton , 2004 ) . could this dilemma of teaching scientific process versus covering test content be resolved via the inclusion of more inquiry_based questions on these standardized_tests ? while this may provide teachers and schools with incentives to cover inquiry skills as well as factual content , this solution raises a different concern : can learning from good inquiry_based projects be adequately assessed using a standardized_test format ? what kind of assessments will allow valid inferences about whether a student has learned how to engage in inquiry , particularly in the " front_end " inquiry processes used to derive a strategy for making sense out of complexity : problem finding , hypothesis formation , experimental design ? using an nsf-funded multiuser virtual_environment ( muve ) as a pedagogical vehicle , our research team is exploring how a technology-intensive learning experience that immerses participants in a virtual " world " whose citizens face chronic illnesses can help middle_school students_learn both deep inquiry skills and science knowledge . in this paper , we provide an overview of the design of this inquiry_based curriculum project . we then offer a comparative analysis of the outcomes of two methods of assessing student understanding of the inquiry process in order to clarify the extent to which typical forms of test items can validly measure students' inquiry skills . theoretical underpinnings inquiry what is " inquiry ? " the range of possible responses to this question is large . some refer to inquiry as a set of process skills that include questioning , hypothesizing and testing while others equate it to " hands-on " learning . the national science_education standards ( nses ) define scientific_inquiry as " the diverse ways 
testing symmetric properties of distributions testing symmetric properties of distributions we introduce the notion of a canonical tester for a class of properties on distributions , that is , a tester strong and general enough that " a distribution property in the class is testable if and only if the canonical tester tests it " . we construct a canon-ical tester for the class of properties of one or two distributions that are symmetric and satisfy a certain weak continuity condition . analyzing the performance of the canonical tester on specific properties resolves several open problems , establishing lower bounds that match known upper bounds : we show that distinguishing between entropy < or > on distributions over [n] requires n / o ( 1 ) samples , and distinguishing whether a pair of distributions has statistical distance < or > requires n 1 o ( 1 ) samples . our techniques also resolve a conjecture about a property that our canonical tester does not apply to : distinguishing identical distributions from those with statistical distance > requires ( n 2/3 ) samples . acknowledgments i am indebted to each member of my thesis committee silvio micali , ronitt rubin-feld and madhu sudan for their longstanding guidance , support , and friendship . silvio micali , my advisor , has been a tireless source of inspiration , ideas , and raw energy since i first arrived at mit . our interactions are distinguished by his knack for recasting seemingly intractable problems into ones whose solutions are simple , natural , and deep . where once i might have seen an insurmountable problem , i now realize that there are always many more ways around an obstacle than over it; nothing has broadened my view of the research landscape more . and despite his protestations that " perfection is the enemy of the good " , he has kept my eyes firmly directed towards that elusive goal , despite all cross-currents , obstacles , and distractions in the way . his dedication to speaking and writing with flair serves as a continuing example by which to improve my own work . ronitt rubinfeld is responsible for directing me to the beautiful problems with which this thesis is concerned . for this , and her inexhaustible enthusiasm about both the field of property testing and my efforts in it , i am deeply grateful . madhu sudan was my first advisor at mit , and to him go my thanks for a smooth and effective introduction to the art of computer science research . to him i also owe the occasional temptation to regard all of computer science as 
large_scale brain functional modularity is reflected in slow electroencephalographic rhythms across the human non-rapid_eye_movement_sleep cycle large_scale brain functional networks ( measured with functional_magnetic_resonance_imaging , fmri ) are organized into separated but interacting modules , an architecture supporting the integration of distinct dynamical processes . in this work we study how the aforementioned modular architecture changes with the progressive loss of vigilance occurring in the descent to deep_sleep and we examine the relationship between the ensuing slow electroencephalographic rhythms and large_scale network modularity as measured with fmri . graph theoretical methods are used to analyze functional connectivity graphs obtained from fifty-five participants at wakefulness , light and deep_sleep . network modularity ( a measure of functional segregation ) was found to increase during deeper sleep_stages but not in light sleep . by endowing functional networks with dynamical properties , we found a direct link between increased electroencephalographic ( eeg ) delta power ( 1-4 hz ) and a breakdown of inter-modular connectivity . both eeg slowing and increased network modularity were found to quickly decrease during awakenings from deep_sleep to wakefulness , in a highly coordinated fashion . studying the modular structure itself by means of a permutation test , we revealed different module memberships when deep_sleep was compared to wakefulness . analysis of node roles in the modular structure revealed an increase in the number of locally well-connected nodes and a decrease in the number of globally well-connected hubs , which hinders interactions between separated functional modules . our results_reveal a well-defined sequence of changes in brain modular organization occurring during the descent to sleep and establish a close parallel between modularity alterations in large_scale functional networks ( accessible through whole brain fmri recordings ) and the slowing of scalp oscillations ( visible on eeg ) . the observed re-arrangement of connectivity might play an important role in the processes underlying loss of vigilance and sensory awareness during deep_sleep . 
learning detectors from large datasets for object retrieval in video_surveillance we address the problem of learning robust and efficient multi-view object detectors for surveillance video indexing and retrieval . our philosophy is that effective solutions for this problem can be obtained by learning detectors from huge amounts of training_data . along this research direction , we propose a novel approach that consists of strategically partitioning the training_set and learning a large array of complementary , compact , deep cascade detectors . at test time , given a video sequence captured by a fixed camera , a small number of detectors is automatically selected per image location . we demonstrate our approach on the problem of vehicle detection in challenging surveillance scenarios , using a large training dataset composed of around one million images . our system runs at an impressive average rate of 125 frames per second on a conventional laptop computer . 
an effect of dopamine depletion on decision_making : the temporal coupling of deliberation and execution when a decision between alternative actions has to be made , the primate brain is able to uncouple motor execution from mental deliberation , providing time for higher cognitive_processes such as remembering and reasoning . the mental deliberation leading to the decision and the motor execution applying the decision are likely to involve different neuronal circuits linking the basal_ganglia and the frontal_cortex . behavioral and physiological studies in monkeys indicate that dopamine depletion may result in a loss of functional segregation between these circuits , hence , in interference between the deliberation and execution processes . to test this hypothesis in humans , we analyzed the movements of parkinsonian patients in a go/no-go task , contrasting periods of uncertainty with periods of knowledge about the rule to be applied . two groups of patients were compared to healthy subjects : one group was treated with dopaminergic medication and the other with deep_brain_stimulation; both groups were also tested without any treatment . in healthy subjects , the movement time was unaffected by uncertainty . in untreated patients , the movement time increased with uncertainty , reflecting interference between deliberation and execution processes . this interference was fully corrected with dopaminergic medication but was unchanged with deep_brain_stimulation . moreover , decision-related hesitations were detectable in the movements of dopamine-depleted patients , revealing a temporal coupling of deliberation and execution . we suggest that such coupling may be related to the loss of dopamine-mediated functional segregation between basal_ganglia circuits processing different stages of goal_directed_behavior . 
automated software test tool 1 objective the objective of the graduate project described below is to introduce the development of a software tool which will be used to automate the testing_process of a jpl-specific set of software_programs . 2 introduction the multi-mission ground system office ( mgso ) , which is part of the jet_propulsion_laboratory ( jpl ) organization , produces a multiple set of core software_programs to assist in the generation of flight sequences that are uplinked to spacecraft through the deep_space_network ( dsn ) . mgso develops these software_programs b y collecting all the common requirements from different jpl/nasa projects . upon delivery of the mgso core software , each project modifies the program and tailors it to their specific needs and requirements by manipulating the necessary files . these programs are inter-linked together . for instance , the output of one software program is an input to another , in addition to passing initialization files such as the command database and flight rules . a sequence integration engineer ( sie ) may generate the initial input file by using one of these software tools . one can look at these programs as an " operating system " of the spacecraft , but with some differences . consider the following : when a unix command directive such as " 1s " is entered at the command_line , the result is a list of the current working directory . however , the steps involved in executing this command directive occur within the operating system and are transparent to the user . the " 1s " command , after some translations , is converted into binary , loaded into cpu memory , and then executed . a similar process occurs on the spacecraft which carries the computer system ( spacecraft brain ) onboard . due to size and weight limitations , there is <a limited storage ( hard_disk ) on board the spacecraft computer s ystern . therefore , only a portion of the operating system is installed onboard , while the remaining portion remains in the ground system . in other words , the command translations and binary conversions remain in the ground system , and then command bits are uplinked to the spacecraft for command processing and execution . these programs are large in size and complexity . many files could be manipulated during the process of adaptation and , therefore , each software program must be tested at the unit level . also , other initialization files are created during the adaptation phase either manually , by software or combination of both . these files must also be tested for completeness and correctness . 
low_frequency perturbation_theory in eddy_current non-destructive evaluation a method is presented by which series solutions for the impedance change in an eddy_current test probe due to closed cracks in a non-magnetic , conducting half-space can be derived at low_frequency . the series solution is applicable for flaws whose dimensions are much smaller than the electromagnetic skin-depth . the problem is formulated using an approach in which the flaw is represented by an equivalent distribution of current dipoles . the electric_field scattered by the flaw is then written as an integral , over the flaw , of the product of the dipole density distribution and an appropriate green's_function . terms in the series expansion for the dipole density are calculated by solving the integral_equation at each order in the chosen small parameter , using perturbation_theory and a dual integral_equation method . the impedance change due to the crack is then calculated from the dipole distribution using the reciprocity theorem . example solutions are given for semi-circular surface-breaking cracks and for long , uniformly deep surface-breaking cracks . results are compared with other analytical solutions and the predictions of an independent numerical scheme , and very good agreement is observed . 
a heuristic for thermal-safe soc test_scheduling 1 high_temperature has become a technological barrier to the testing of high_performance systems-on-chip , especially when deep_submicron_technologies are employed . in order to reduce test time while keeping the temperature of the cores under test within a safe range , thermal-aware test_scheduling techniques are required . in this paper , we address the test time minimization problem as how to generate the shortest test schedule such that the temperature limits of individual cores and the limit on the test-bus bandwidth are satisfied . in order to avoid overheating during the test , we partition test_sets into shorter test sub-sequences and add cooling periods in between , such that continuously applying a test sub-sequence will not drive the core temperature going beyond the limit . further more , based on the test partitioning scheme , we interleave the test sub-sequences from different test_sets in such a manner that a cooling period reserved for one core is utilized for the test transportation and application of another core . we have proposed a heuristic to minimize the test application time by exploring alternative test partitioning and interleaving schemes with variable length of test sub-sequences and cooling periods . experimental_results have shown the efficiency of the proposed heuristic . 
transfer of conflict and cooperation from experienced games to new games : a connectionist model of learning the question of whether , and if so how , learning can be transfered from previously experienced games to novel games has recently attracted the attention of the experimental game_theory literature . existing research presumes that learning operates over actions , beliefs or decision rules . this study instead uses a connectionist approach that learns a direct mapping from game payoffs to a probability_distribution over own actions . learning is operationalized as a backpropagation rule that adjusts the weights of feedforward_neural_networks in the direction of increasing the probability of an agent playing a myopic best response to the last game played . one advantage of this approach is that it expands the scope of the model to any possible n n normal-form game allowing for a comprehensive model of transfer of learning . agents are exposed to games drawn from one of seven classes of games with significantly different strategic characteristics and then forced to play games from previously unseen classes . i find significant transfer of learning , i . e . , behavior that is path-dependent , or conditional on the previously seen games . cooperation is more pronounced in new games when agents are previously exposed to games where the incentive to cooperate is stronger than the incentive to compete , i . e . , when individual incentives are aligned . prior exposure to prisoner's_dilemma , zero-sum and discoordination games led to a significant decrease in realized payoffs for all the game classes under investigation . a distinction is made between superficial and deep_transfer of learning both-the former is driven by superficial payoff similarities between games , the latter by differences in the incentive structures or strategic implications of the games . i examine whether agents learn to play the nash_equilibria of games , how they select amongst multiple equilibria , and whether they transfer nash_equilibrium behavior to unseen games . sufficient exposure to a strategically heterogeneous set of games is found to be a necessary condition for deep_learning ( and transfer ) across game classes . paradoxically , superficial transfer of learning is shown to lead to better outcomes than deep_transfer for a wide range of game classes . the simulation_results corroborate important experimental findings with human subjects , and make several novel predictions that can be tested experimentally . 
mass detection in digital breast tomosynthesis : deep_convolutional_neural_network with transfer_learning from mammography . purpose develop a computer_aided detection ( cad ) system for masses in digital breast tomosynthesis ( dbt ) volume using a deep_convolutional_neural_network ( dcnn ) with transfer_learning from mammograms . methods a data_set containing 2282 digitized film and digital mammograms and 324 dbt volumes were collected with irb approval . the mass of interest on the images was marked by an experienced breast radiologist as reference standard . the data_set was partitioned into a training_set ( 2282 mammograms with 2461 masses and 230 dbt views with 228 masses ) and an independent test_set ( 94 dbt views with 89 masses ) . for dcnn training , the region of interest ( roi ) containing the mass ( true positive ) was extracted from each image . false_positive ( fp ) rois were identified at prescreening by their previously developed cad systems . after data augmentation , a total of 45 072 mammographic rois and 37 450 dbt rois were obtained . data normalization and reduction of non-uniformity in the rois across heterogeneous data was achieved using a background correction method applied to each roi . a dcnn with four convolutional layers and three fully_connected ( fc ) layers was first trained on the mammography data . jittering and dropout techniques were used to reduce overfitting . after training with the mammographic rois , all weights in the first three convolutional layers were frozen , and only the last convolution layer and the fc layers were randomly initialized again and trained using the dbt training rois . the authors compared the performances of two cad systems for mass detection in dbt : one used the dcnn-based_approach and the other used their previously developed feature-based approach for fp reduction . the prescreening stage was identical in both systems , passing the same set of mass candidates to the fp reduction stage . for the feature-based cad system , 3d clustering and active_contour method was used for segmentation; morphological , gray level , and texture_features were extracted and merged with a linear discriminant classifier to score the detected masses . for the dcnn-based cad system , rois from five consecutive slices centered at each candidate were passed through the trained dcnn and a mass likelihood score was generated . the performances of the cad systems were evaluated using free-response roc curves and the performance difference was analyzed using a non-parametric method . results before transfer_learning , the dcnn trained only on mammograms with an auc of 0 . 99 classified dbt masses with an auc of 0 . 81 in the dbt training_set . after transfer_learning with dbt , the auc improved to 0 . 90 . for breast-based cad detection in the test set , the sensitivity for the feature-based and the dcnn-based cad systems was 83% and 91% , respectively , at 1 fp/dbt volume . the difference between the performances for the two systems was statistically_significant ( p_value < 0 . 05 ) . conclusions the image patterns learned from the mammograms were transferred to the mass detection on dbt slices through the dcnn . this study demonstrated that large data_sets collected from mammography are useful for developing new cad systems for dbt , alleviating the problem and effort of collecting entirely new large data_sets for the new modality . 
testing in nanometer_technologies the last 25 years have been a very exciting time for the people involved in testing . as an industry we had a very difficult time generating tests for boards which had only 1000 logic_gates on them with packages which had only a few logic_gates per module . because of these difficulties a number of people started to look for different approaches to testing . it was clear to many that automatic test_generation for sequential networks could not keep up with the rate of increasing network size . this resulted in many changes in the way designs were done . this increase in gate_count resulted in the development of the area of design_for_testability . test in these 25 years was driven by the increase in gate_count coupled with the inability of automatic sequential test_generation to keep pace . today we are looking at an era before us which also has the gate_count increasing at virtually the same rate . the design_for_testability techniques such as full scan , lssd , etc . seems to be well in place . however , there is a significant difference brought on by the technology developments facing us . the onset of deep_sub_micron ( now currently alluded to as nanometer technology ) is changing the way chips are being designed and manufactured . because of the large capacity of these new chips , plus the expense of new designs , embedded_systems are setting the pace for today and the future . new problems are arising that are driving design_automation to integrate all the tools that are needed to successfully take a design from concept to reality in this new design environment . test is one part of this process that is getting significant attention . an area once classified as a "back end" process in the design_flow is moving closer to the "front_end" . design methodologies are incorporating test-related structures in the beginning of the design_cycle . in addition , standards to manage the test complexity of these large designs are being proposed . for example , ieee p1500 is working towards defining a structure for embedded cores such that tests can be delivered to these cores . this alone is a strong challenge for the test community . it is clear that design and testing of embedded_systems is the key challenge to the test community as we face these new technologies . this includes both the tools that are required to test these designs and the 
the new ccsds image_compression recommendation 1 process both frame and non-frame ( push-broom ) data 2 offer adjustable coded data rate or image quality ( up to lossless ) 3 accommodate from 4-bit to 16-bit input pixels 4 provide real-time processing with space qualified electronics ( 20 msamples/sec , 1 watt/msamples/sec , based on year 2000 space electronics technology ) 5 require minimal ground operation 6 limit the effects of a packet_loss to a small region of the image . abstract the consultative committee for space data systems ( ccsds ) data_compression working group has recently adopted a recommendation for image data_compression , with a final release expected in 2005 . the algorithm adopted in the recommendation consists of a two-dimensional discrete_wavelet_transform of the image , followed by progressive bit-plane coding of the transformed data . the algorithm can provide both lossless and lossy_compression , and allows a user to directly control the compressed data volume or the fidelity with which the wavelet-transformed data can be reconstructed . the algorithm is suitable for both frame-based image_data and scan_based sensor data , and has applications for near-earth and deep-space_missions . the standard will be accompanied by free_software sources on a future web_site . an application_specific_integrated_circuit ( asic ) implementation of the compressor is currently under development . this paper describes the compression algorithm along with the requirements that drove the selection of the algorithm . performance results and comparisons with other compressors are given for a test set of space images . 
machine_learning paradigms for speech_recognition : an overview automatic_speech_recognition ( asr ) has historically been a driving force behind many machine_learning ( ml ) techniques , including the ubiquitously used hidden_markov_model , discriminative learning , structured sequence learning , bayesian learning , and adaptive learning . moreover , ml can and occasionally does use asr as a large-scale , realistic application to rigorously test the effectiveness of a given technique , and to inspire new problems arising from the inherently sequential and dynamic nature of speech . on the other hand , even though asr is available commercially for some applications , it is largely an unsolved problem for almost all applications , the performance of asr is not on par with human performance . new insight from modern ml methodology shows great_promise to advance the state-of-the-art in asr technology . this overview article provides readers with an overview of modern ml techniques as utilized in the current and as relevant to future asr research and systems . the intent is to foster further cross-pollination between the ml and asr communities than has occurred in the past . the article is organized according to the major ml paradigms that are either popular already or have potential for making significant contributions to asr technology . the paradigms presented and elaborated in this overview include : generative and discriminative learning; supervised , unsupervised , semi_supervised , and active_learning; adaptive and multi-task learning; and bayesian learning . these learning paradigms are motivated and discussed in the context of asr technology and applications . we finally present and analyze recent_developments of deep_learning and learning with sparse representations , focusing on their direct relevance to advancing asr technology . 
adaptive robust_tracking control of deep_sea manipulator : theory and experiments click here to enter text . abstract the underwater hydraulic manipulator's challenging working condition , such as the unknown payload , varying speed , and hydraulic actuator's dynamics , makes common controller invalid . in this paper , based on the nonlinear_control theory , the adaptive robust_control method is proposed to enhance the joint tracking accuracy , the controller can also compensate the interactive dynamic effects between manipulator links . the deep-sea manipulators are just equipped with angular sensors , so an observer which can provide the smooth angular_velocity estimation is designed . by using the lyapunov approach , the proposed controller can be proved asymptotically stable for trajectory_tracking . the comparative experiments are conducted on a deep_sea hydraulic manipulator , experiment results show the control_algorithm could provide a fast , high_accuracy tracking , and guarantee the tracking performance when subjected to payload change or different reference speed . introduction in the deep sea exploration , hydraulic manipulators are commonly used , especially in work-class rovs , hydraulic manipulators have much higher power_density , stiffness and don't require thick protection shell . most of the deep-sea manipulators are master-slave type , operator on the deck manipulates the master arm to guide the slave arm performing some task , such as connecting the oil pipeline , underwater equipment maintenance . so the accuracy of slave arm is a key indicator of system performance , a fast response and high_accuracy hydraulic manipulator could greatly promote operator's work efficiency and reduce the operating mistakes . but the manipulator's working conditions are full of challenges , for example , when a manipulator connects the oil pipeline , the connectors are heavy and commonly in different weight , and desired joint speed is unpredictable , the common controller's accuracy couldn't meet requirements , so researches in this field is critical . hydraulic system is nonlinear and difficult to control due to the fluid compressibility , control valve's dead band property . to guarantee the tracking performance , many control schemes were brought up . adaptive_control greatly enhances the system's robustness , which has been tested in many research fields , this algorithm can estimate the unknown parameter , and eventually , achieve zero steady_state tracking . but it's designed to deal with a system with just parameter uncertainty . to overcome the shortcoming of adaptive_control , the adaptive robust_control ( arc ) of hydraulic manipulator was brought up . this control_algorithm preserves the high tracking accuracy , parameter adaptability of ac , and disturbance rejection property of drc . when parameters change , the parameter_estimation part of arc controller will detect the change , and make 
identifying defects in deep_submicron cmos_ics given the oft-cited difficulty of testing modern integrated_circuits , the fact that cmos_ics lend themselves to iddq_testing is a piece of good fortune . but that valuable advantage is threatened by the rush of semiconductor_technology to smaller feature_sizes and faster , denser circuits , in line with the semiconductor industry association's ( sia ) roadmap-its forecast for the cmos ic industry . with safety margins for reliability , test , failure_analysis , and design_verification shrinking , it would be a shame to give up the iddq technique-and luckily , we may not have to . steps can be taken to maintain its applicability as we rush deeper into the submicron regime . before discussing them , however , a brief discussion of iddq_testing seems to be in order . we will first examine why the iddq_test serves several interests , then describe the challenge posed by 0 . 35-0 . 07- m transistor geometries , and finally propose several solutions . cmos ic power_supply current can be amperes during logic state transitions , but only nanoamperes during the steady_state , or quiescent , portion of the clock_cycle . this low quiescent power_supply current , known as iddq , is what gives cmos its traditional low_power edge over its technology competitors . but it does more than that . engineers in design , fabrication , and test have learned to use this low quiescent_current as a sensitive test to identify defects , which often prove to be the reason for customer returns , whether as test escapes or reliability failures . in fact , test escape levels below 200 parts per million have recently been attainable only by adding iddq_testing . the technique has also eliminated the need for burn-in for some mature product lines . more , iddq measurements speed failure_analysis by providing current-voltage signatures and temporal characteristics . detecting defects with current current is a more effective parameter than voltage for defect_detection in cmos_ics , although both are necessary for complete testing . the simple logic circuit with a bridging defect shown_in_fig . 1 illustrates how iddq increases in the presence of a flaw . bridging defects and certain open-circuit defects typically elevate the nanoampere levels of a normal circuit by two to seven orders_of_magnitude , providing very sensitive defect_detection . if ics are correctly designed and fabricated for low background current , then iddq is a relatively simple measurement with many benefits . but as background iddq rises , for whatever the reason , the effectiveness of iddq_testing diminishes . for optimum detection of manufacturing defects , the defect_free 
assessing computational methods for transcription_factor target gene identification based on chip_seq data chromatin immunoprecipitation coupled with deep_sequencing ( chip_seq ) has great potential for elucidating transcriptional networks , by measuring genome_wide binding of transcription factors ( tfs ) at high_resolution . despite the precision of these experiments , identification of genes directly regulated by a tf ( target genes ) is not trivial . numerous target gene scoring methods have been used in the past . however , their suitability for the task and their performance remain unclear , because a thorough comparative assessment of these methods is still lacking . here we present a systematic evaluation of computational methods for defining tf targets based on chip_seq data . we validated predictions based on 68 chip_seq studies using a wide range of genomic expression data and functional information . we demonstrate that peak-to-gene assignment is the most crucial step for correct target gene prediction and propose a parameter-free method performing most consistently across the evaluation tests . 
convolutional_neural_network and convex_optimization this report shows that the performance of deep_convolutional_neural_network can be improved by incorporating convex_optimization techniques . first , we find that the sub-models learned by dropout can be more effectively combined by solving a convex problem . also , we generalize this idea to models that are not trained by dropout . compared to traditional methods , we get an improvement of 0 . 22% and 0 . 76% test accuracy on cifar10 dataset . second , we investigate the performance for different loss functions borrowed from the convex_optimization community and find that selecting loss functions matters a lot . we also implement a novel loss based on the idea of one-versus-one svm , which has never been explored in the literature . experiment shows that it can give performance comparable to the standard cross-entropy loss , without being fully tuned . 
utilizing fault cases for supporting fault_diagnosis tasks when building large and complex systems , like satellites , all sorts of risks have to be managed if it were to be successful . although there have been various techniques for fault_diagnosis , applying them requires both deep domain_knowledge and extensive effort of domain experts . in this paper , we present an approach to support fault_diagnosis at relatively low_cost . the approach utilizes fault cases experienced while testing of the system . we show the effectiveness of the approach by applying it to a case taken from a satellite development project . 
large corpus-based semantic feature_extraction for pronoun coreference semantic_information is a very important factor in coreference resolution . the combination of large corpora and 'deep' analysis procedures has made it possible to acquire a range of semantic_information and apply it to this task . in this paper , we generate two statistically-based semantic features from a large corpus and measure their influence on pronoun coreference . one is contextual compatibility , which decides if the antecedent can be used in the anaphor's context; the other is role pair , which decides if the actions asserted of the antecedent and the anaphor are likely to apply to the same entity . we apply a semantic labeling system and a baseline coreference system to a large corpus to generate semantic patterns and convert them into features in a maxent model . these features produce an absolute gain of 1 . 5% to 1 . 7% in resolution accuracy ( a 6% reduction in errors ) . to understand the limitations of these features , we also extract patterns from the test corpus , use these patterns to train a coreference model , and examine some of the cases where coreference still fails . we also compare the performance of patterns extracted from semantic_role labeling and syntax . 
bottleneck features based on gammatone frequency cepstral coefficients recent work demonstrates impressive success of the bottleneck ( bn ) feature in speech_recognition , particularly with deep_networks plus appropriate pre-training . a widely admitted advantage associated with the bn_feature is that the network structure can learn multiple environmental conditions with abundant training_data . for tasks with limited training_data , however , this multi-condition training is unavailable , and so the networks tend to be over-fitted and sensitive to acoustic condition changes . a possible solution is to base the bn features on a channel-robust primary feature . in this paper , we propose to derive the bn_feature based on gammatone frequency cepstral coefficients ( gfccs ) . the gfcc feature has shown nice robustness against acoustic change , due to its capability of simulating the auditory system of humans . the idea is to integrate the advantage of the gfcc feature in acoustic robustness and the advantage of the bn_feature in signal representation , so that the bn_feature can be improved in the condition of mismatched training/test channels . this is particularly useful for small_scale tasks for which the training data are often limited . the experiments are conducted on the wsjcam0 database , where the test utterances are mixed with noises at various snr levels to simulate the channel change . the results confirm that the gfcc-based bn_feature is much more robust than the bn features based on the mfcc and the plp . furthermore , the primary gfcc feature and the gfcc-based bn_feature can be concatenated , leading to a more robust combined feature which provides considerable performance gains in all the tested noise conditions . 
generalized conflict learning for hybrid discrete/linear optimization generalized conflict learning for hybrid discrete/linear optimization conflict-directed search_algorithms have formed the core of practical , model-based_reasoning systems for the last three decades . in many of these applications there is a series of discrete constraint optimization_problems and a conflict-directed search_algorithm , which uses conflicts in the forward search step to focus search away from known infeasibilities and towards the optimal solution . in the arena of model_based autonomy , discrete systems , like deep_space probes , have given way to more agile systems , such as coordinated vehicle control , which must robustly control their continuous dynamics . controlling these systems requires optimizing over continuous , as well as discrete variables , using linear and non-linear as well as logical constraints . this paper explores the development of algorithms for solving hybrid discrete/linear optimization_problems that use conflicts in the forward search direction , generalizing from the conflict-directed search_algorithms of model-based_reasoning . we introduce a novel algorithm called generalized conflict-directed branch_and_bound ( gcd-bb ) . gcd-bb extends traditional branch_and_bound ( b&b ) , by first constructing conflicts from nodes of the search tree that are found to be infeasible or sub-optimal , and then by using these conflicts to guide the forward search away from known infeasible and sub-optimal states . we evaluate gcd-bb empirically on a range of test_problems of coordinated air vehicle control . gcd-bb demonstrates a substantial improvement in performance compared to a traditional b&b algorithm , applied to either disjunctive linear programs or an equivalent binary integer program encoding . acknowledgments first of all , i would like to thank my advisor , brian_williams , for his guidance and encouragement on my research and working so hard with me to make the thesis deadline . i would like to thank my caring roommates , caroline maier and jit kee chin , especially caroline , for feeding and taking care of me when i was overwhelmed by work , and cheering me up when i was down . they are not just my roommates; they are my family . i would like to thank my parents , for their unconditional love and support , and for their care and patience during the time when i was stuck in china for 9 months . i would like to thank mers group , for making our lab a comfortable and stimulating part of my life . especially , thomas l aut , for providing test_problems for my algorithm , and giving immediate and helpful comments on my thesis , lars blackmore , for the insightful discussions we had , the comments he gave on the early draft of my 
deep_level acoustic-to-articulatory mapping for dbn-hmm_based phone recognition in this paper we experiment with methods based on deep_belief_networks ( dbns ) to recover measured articulatory data from speech acoustics . our acoustic-to-articulatory mapping ( aam ) processes go through multi-layered and hierarchical ( i . e . , deep ) representations of the acoustic and the articulatory domains obtained through unsupervised learning of dbns . the unsupervised learning of dbns can serve two purposes : ( i ) pre-training of the multi_layer perceptrons that perform aam; ( ii ) transformation of the articulatory domain that is recovered from acoustics through aam . the recovered artic-ulatory features are combined with mfccs to compute phone posteriors for phone recognition . tested on the mocha-timit corpus , the recovered articulatory features , when combined with mfccs , lead to up to a remarkable 16 . 6% relative phone error reduction w . r . t . a phone recognizer that only uses mfccs . 
a corpus of clinical narratives annotated with temporal_information clinical reports often include descriptions of events in the patient's medical_history , as well as explicit or implicit temporal information about these events . we are working towards applying deep natural_language_processing tools towards understanding such narratives . this requires both the extraction and classification of the relevant events , and the placing of those events in time , or at least in relation to one another . although several corpora of news data exist that have been annotated using the timeml schema , similar corpora of clinical reports are not readily available . in this paper we report on the design of a small corpus and the annotation schema we developed , based on data from the fourth i2b2/va challenge . these data include , among others , annotations for medical problems , tests , and treatments in clinical reports from several healthcare institutions . we have selected a subset of clinical reports and added annotations similar to those used in the tempeval tasks for the annotation of events , time expressions and temporal relations for the news domain . the annotations have been made freely available to the research_community . 
calmsystem : a conversational agent for learner modelling this paper describes a system which incorporates natural_language technologies , database manipulation and educational theories in order to offer learners a negotiated learner_model , for integration into an intelligent_tutoring_system . the system presents the learner with their learner_model , offering them the opportunity to compare their own beliefs regarding their capabilities with those inferred by the system . a conversational agent , or " chatbot " has been developed to allow the learner to negotiate over the representations held about them using natural_language . the system aims to support the metacognitive goals of self-assessment and reflection , which are increasingly seen as key to learning and are being incorporated into uk educational policy . the paper describes the design of the system , and reports a user trial , in which the chatbot was found to support users in increasing the accuracy of their self-assessments , and in reducing the number of discrepancies between system and user beliefs in the learner_model . some lessons learned in the development have been highlighted and future_research and experimentation directions are outlined . intelligent_tutoring_systems ( its ) provide their users with an adaptive learning_environment , with personalized tutoring and testing customised to meet the needs of the individual student . this adaptation is based on the contents of the learner_model , a representation of the student's knowledge , gaps in understanding and misconceptions . traditional itss have not made the contents of the learner_model visible to the learner . however , it has been argued that an open learner_model ( i . e . one that can be inspected by the student ) can offer opportunities for learner reflection , metacognition and deep_learning , which may enhance learning ( e . g . [1] , [2] , [3] , [4] and [5] ) , as well as improving the accuracy of the learner_model . educational theorists have emphasised the importance of learner reflection ( [6] , [7] and [8] ) . some researchers have developed open learner models ( olm ) that
model_based performance_assessment model_based performance_assessment model_based performance_assessment performance_assessment learning models problem_solving self-regulation communication content understanding collaboration learning the findings and opinions expressed in this report do not reflect the position or policies of t h e there is concern in many quarters about the type and level of knowledge our children are acquiring in schools . study ) assessments , many students do poorly . further , there is a growing concern in american business and industry ( u . s . 1992 ) that young people entering the workforce are not adequately prepared for the world of work . these concerns are a major source of what is now almost a decade of effort to restructure what students are taught and the ways in which we can accurately assess their learning . for example , government policy makers have attempted to address these issues , beginning with the 1989 national achievement towards the goals ( see goals 2000 ) on a national and state level , also advised that standards of learning be set so that students progress towards the goals could be determined . goal 3 , which states that all students will leave grades 4 , 8 , and 12 having demonstrated competency over challenging subject matter . . . , was a particular focal point for the development of standards . the national education goals panel also pointed to the need to define more specifically what constitutes challenging subject matter and competency i n it for instance , the kind of learning characterized by higher_order thinking skills , deep content knowledge within and across subject areas , problem_solving ability and the need to determine how that competency would be measured . standards now have been developed for eight of the nine subject areas listed but goals for national educational achievement and resulting standards to describe those goals will not tell us how well our children are doing unless we also measure their progress in learning the content of the standards . to do this , new kinds of tests are being created , called performance assessments , in which students engage in tasks that may require significant amounts of time and i n which they are asked to communicate their understanding of content , of process performance_assessment also has been described by its proponents as a major strategy to assist teachers to improve the learning of their students . this piece will describe both the values ascribed to performance_assessment and the major criticisms of assessment that have developed in the last few years of exploration . one approach , model_based performance_assessment , will be described as a way to remedy and to avoid criticisms of performance_assessment . part 
the inevitable future of the starless core barnard_68 dense , small molecular_cloud cores have been identified as the direct progenitors of stars . one of the best studied examples is barnard_68 which is considered a prototype stable , spherical gas core , confined by a diffuse high-pressure environment . observations of its radial density structure however indicate that barnard_68 should be gravitationally unstable and collapsing which appears to be inconsistent with its inferred long lifetime and stability . we argue that barnard_68 is currently experiencing a fatal collision with another small core which will lead to gravitational collapse . despite the fact that this system is still in an early phase of interaction , our numerical simulations imply that the future gravitational collapse is already detectable in the outer surface density structure of the globule which mimicks the profile of a gravitationally unstable bonnor-ebert sphere . within the next 2 10 5 years barnard_68 will condense into a low_mass solar-type star ( s ) , formed in isolation , and surrounded by diffuse , hot interstellar_gas . as witnessed in situ for barnard_68 , core mergers might in general play an important role in triggering star_formation and shaping the molecular core mass distribution and by that also the stellar initial mass function . subject headings : ism : globules ism : clouds ism individual ( barnard_68 ) stars : formation hydrodynamics 1 . introduction barnard_68 is considered an excellent test_case and the prototype of a dense molecular_cloud core ( alves et al . 2001b ) . because of its small distance ( 125 pc ) , this so called bok_globule /bok & reilly 1947 ) with a mass of m=2 . 1 m , contained within a region of r = 12 , 500 au has been observed with unprecedented accuracy ( alves et al . 2001b; lada et al . 2003; bergin et al . 2006; redman et al . 2006; maret et al . 2007 ) . deep near-infrared dust extinction measurements of starlight toward individual background stars observed through the cloud provided a detailed 2-dimensional map of its projected surface density distribution ( fig . 1 ) from which a high_resolution density profile was derived over the entire extent of the system . the striking agreement of the inferred density structure with the theoretical solution of an isothermal , pressure confined , hydrostatic gas sphere ( so called bonnor-ebert sphere ) was interpreted as a signature that the globule is old , thermally supported and stable , with the pressure_gradient balancing the gravita-tional force . this conclusion has received additional support from molecular line observations ( lada et al . 2003 ) that 
influencing the perceived emotions of music with intent 2 . introduction 1 . abstract music is an immensely powerful affective medium that pervades our everyday life . with ever advancing technology , the reproduction and application of music for emotive and information transfer purposes has never been more prevalent . in this paper we introduce a rule-based engine for influencing the perceived emotions of music . based on empirical music_psychology , we attempt to formalise the relationship between musical elements and their perceived emotion . we examine the modification to structural aspects of music to allow for a graduated transition between perceived emotive states . this mechanism is intended to provide music reproduction systems with a finer grained control over this affective medium; where perceived musical emotion can be influenced with intent . this intent comes from both an external application and the audience . using a series of affective_computing technologies , an audience's response metrics and attitudes can be incorporated to model this intent . a generative feedback_loop is set up between the external application , the influencing process and the audience's response to this , which together shape the modification of musical structure . the effectiveness of influencing perceived musical emotion was examined in earlier work , with a small test study providing generally encouraging results . no one is sure for what end music came about , be it a biological urge [1] , an offshoot of our evolving language faculty [2] or simply another mechanism for the expression of self brought about by mankind's cultural explosion some 100 , 000 odd years_ago [3] . few however would argue that it is our intense and diverse emotional capacity that allows us to create music; the at times transcendental inexplicable . given this deep emotional connection , and the ease with which sound be reproduced with modern technology , music is now a pervasive medium found throughout everyday life . it is used in almost every form of human communication and can be heard at the cinema , on television , on radio , in commercials , at the ballet , in shopping centres , on public and private transport , in waiting rooms and restaurants , to name but a few . one study found that within any waking 2 hour period , a person had on average a 44% chance of experiencing a
efficient parsing of spoken inputs for human_robot_interaction the use of deep parsers in spoken_dialogue systems is usually subject to strong performance requirements . this is particularly the case in human_robot_interaction , where the computing resources are limited and must be shared by many components in parallel . a real-time dialogue system must be capable of responding quickly to any given utterance , even in the presence of noisy , ambiguous or distorted input . the parser must therefore ensure that the number of analyses remains bounded at every processing step . the paper presents a practical approach to addressing this issue in the context of deep parsers designed for spoken_dialogue . the approach is based on a word lattice parser combined with a statistical_model for parse selection . each word lattice is parsed incrementally , word by word , and a discriminative model is applied at each incremental step to prune the set of resulting partial analyses . the model incorporates a wide range of linguistic and contextual features and can be trained with a simple perceptron . the approach is fully implemented as part of a spoken_dialogue system for human_robot_interaction . evaluation results on a wizard-of-oz test_suite demonstrate significant improvements in parsing time . 
0 th world congress on structural and multidisciplinary optimization probability collectives for solving truss structure problems 1 . abstract the approach of probability collectives ( pc ) in the collective_intelligence ( coin ) framework is one of the emerging artificial_intelligence approaches dealing with the complex problems in a distributed way . it decomposes the entire system into subsystems and treats them as a group of learning , rational and self interested agents or a multi_agent_system ( mas ) . these agents iteratively select their strategies to optimize their individual local goal which also makes the system to achieve the global optimum . the approach of pc has been tested and validated by solving a variety of practical problems in continuous domain . this paper demonstrates the ability of pc solving 2-d space truss structure and 3-d truss structure design problems with discrete as well as continuous variables . the approach is shown to be producing competent and sufficiently robust results . the associated strengths , weaknesses are also discussed . the solution to these problems indicates that the approach of pc can be further efficiently applied to solve a variety of practical/real_world problems . 2 . 3 . introduction in the framework of collective_intelligence ( coin ) , the artificial_intelligence ( ai ) tool referred to as probability collectives ( pc ) is becoming popular for modeling and controlling distributed multi_agent_system ( mas ) [1-15] . it was inspired from a sociophysics viewpoint , with deep connections to game_theory , statistical_physics , and optimization [1 , 2] . according to [1 , 2 , 9-11] , the key characteristics of the pc methodology such as its ability to accommodate discrete and continuous variables as well as irregular and noisy functions , tolerance to subsystem/agent failure , ability to provide sensitivity information and ability to handle uncertainty in terms of probability , use of homotopy function to make the solution jump out of possible local_minima , ability to avoid the tragedy of commons , high scalability , ability to achieve unique nash_equilibrium , etc . makes it a very competitive choice over other contemporary algorithms . the approach of pc has been applied in variegated areas such as airplane fleet assignment problem [12] and various cases of the multiple traveling salesmen problems ( mtsps ) [4 , 7] , continuous constrained problems such as benchmark test_problems [7 , 9 , 13-15] , two variations of the circle packing problem ( cpp ) [5] , sensor_network coverage problem [10] as well as fault_tolerant system in association with the cpp [11] . furthermore , the segmented beam problem [8] , multimodal , nonlinear and non-separable test_problems comparing the performance with genetic_algorithm ( ga ) [24] as well as joint optimization of the routing and resource_allocation 
model neural prostheses with integrated microfluidics : a potential intervention strategy for controlling reactive cell and tissue responses model silicon intracortical probes with microfluidic channels were fabricated and tested to examine the feasibility of using diffusion-mediated delivery to deliver therapeutic agents into the volume of tissue exhibiting reactive responses to implanted devices . three-dimensional probe structures with microfluidic channels were fabricated using surface micromachining and deep_reactive_ion_etching ( drie ) techniques . in vitro functional tests of devices were performed using fluorescence microscopy to record the transient release of texas red labeled transferrin ( tr-transferrin ) and dextran ( tr-dextran ) from the microchannels into 1% w/v agarose gel . in vivo performance was characterized by inserting devices loaded with tr-transferrin into the premotor_cortex of adult male rats . brain sections were imaged using confocal microscopy . diffusion of tr-transferrin into the extracellular_space and uptake by cells up to 400 microm from the implantation site was observed in brain slices taken 1 h postinsertion . the reactive tissue volume , as indicated by the presence of phosphorylated mitogen-activated protein kinases ( mapks ) , was characterized using immunohistochemistry and confocal microscopy . the reactive tissue volume extended 600 , 800 , and 400 microm radially from the implantation site at 1 h , 24 h , and 6 weeks following insertion , respectively . these results indicate that diffusion-mediated delivery can be part of an effective intervention strategy for the treatment of reactive tissue responses around chronically implanted intracortical probes . 
evolution of deep_brain_stimulation : human electrometer and smart devices supporting the next generation of therapy . deep_brain_stimulation ( dbs ) provides therapeutic benefit for several neuropathologies including parkinson's_disease ( pd ) , epilepsy , chronic_pain , and depression . despite well established clinical efficacy , the mechanism ( s ) of dbs remains poorly_understood . in this review we begin by summarizing the current understanding of the dbs mechanism . using this knowledge as a framework , we then explore a specific hypothesis regarding dbs of the subthalamic nucleus ( stn ) for the treatment of pd . this hypothesis states that therapeutic benefit is provided , at least in part , by activation of surviving nigrostriatal dopaminergic neurons , subsequent striatal dopamine release , and resumption of striatal target cell control by dopamine . while highly controversial , we present preliminary data that are consistent with specific predications testing this hypothesis . we additionally propose that developing new technologies , e . g . , human electrometer and closed_loop smart devices , for monitoring dopaminergic neurotransmission during stn_dbs will further advance this treatment approach . 
autonomous onboard science data_analysis for comet missions coming years will bring several comet rendezvous missions . the rosetta spacecraft arrives at comet 67p/churyumov gerasimenko in 2014 . subsequent rendezvous might include a mission such as the proposed comet hopper with multiple surface landings , as well as comet_nucleus sample return ( cnsr ) and coma rendezvous and sample return ( crsr ) . these encounters will begin to shed light on a population that , despite several previous flybys , remains mysterious and poorly_understood . scientists still have little direct knowledge of interactions between the nucleus and coma , their variation across different comets or their evolution over time . activity may change on short timescales so it is challenging to characterize with scripted data_acquisition . here we investigate automatic onboard image_analysis that could act faster than round-trip light time to capture unexpected outbursts and plume activity . we describe one edge-based_method for detect comet nuclei and plumes , and test the approach on an existing catalog of comet images . finally , we quantify benefits to specific measurement objectives by simulating a basic plume monitoring campaign . previous comet encounters include international flybys of 1p/halley , the flyby of 81p/wild by stardust , the deep impact and next encounters with 9p/tempel_1 , and a deep_space_1 flyby of 19p/borrelly . these few encounters have already revealed a very diverse population . comets vary in size by orders_of_magnitude , with most having heterogeneous texture , albedo and composition . each new visit reveals features not seen in previous cases . tempel_1 has morphological evidence of active geologic processes including scarps and outflows [1] . its surface undergoes continuous modification , with visible change during the years between two flybys . the epoxi flyby of comet hartley 2 shows skyscraper-size spires , flat featureless plains that outgas h 2 o , regions of rough and mottled texture , bands of various shapes , and diverse surface albedo . comets' active areas range from 10-90% , changing over time and distance to the sun . they manifest as both localized jets and diffuse regions ( figure 1 ) . still more exotic , recently discovered " active asteroids " suggest that primitive ice could survive for billions of years in the inner solar system . this challenges the fundamental distinction between comets and asteroids [2] . a fundamental underlying question is the temporal evolution and driving mechanisms of comet surface activity . it is likely that comet activity changes on timescales faster than ground control's traditional command cycle and uplink interval . faster reaction time will be important to characterize the dynamic activity profile . additionally , future 
static_timing_analysis considering power_supply variations power_supply integrity verification has become a key concern in high_performance designs . in deep submicron technologies , power_supply_noise can significantly increase the circuit_delay and lead to performance failures . traditional static_timing_analysis which applies worst_case voltage margins to compute circuit_delay leads to a very conservative analysis because the worst-case drop is localized to a small area of the die . in this paper , we propose a new approach for analyzing the impact of power_supply variations on circuit_delay . the circuit_delay maximization problem is formulated as a constrained non-linear optimization_problem which takes both ir and ldi/dt drops into account the proposed approach does not require apriori knowledge of critical paths in the circuit and can be effectively incorporated in an existing static_timing_analysis framework . the proposed method has been implemented and tested on iscas85 benchmark_circuits and compared with the traditional methods for computing worst_case circuit_delay under supply variations . 
circuit_design for embedded_memory in low_power integrated_circuits circuit_design for embedded_memory in low_power integrated_circuits this thesis explores the challenges for integrating embedded static_random_access_memory ( sram ) and non_volatile_memory based on ferroelectric capacitor technology into low_power integrated_circuits . first considered is the impact of process_variation in deep-submicron technologies on sram , which must exhibit higher density and performance at increased levels of integration with every new semiconductor generation . techniques to speed up the statistical_analysis of physical memory designs by a factor of 100 to 10 , 000 relative to the conventional monte_carlo_method are developed . the proposed methods build upon the importance_sampling simulation algorithm and efficiently explore the sample_space of transistor parameter fluctuation . process_variation in sram at low_voltage is further investigated experimentally with a 512kb 8t sram test_chip in 45nm soi cmos_technology . for active operation , an ac coupled sense amplifier and regenerative global bitline scheme are designed to operate at the limit of on current and off current separation on a single-ended sram bitline . the sram operates from 1 . 2 v down to 0 . 57 v with access times from 400ps to 3 . 4ns . for standby_power , a data_retention voltage sensor predicts the mismatch-limited minimum supply_voltage without corrupting the contents of the memory . the leakage_power of sram forces the chip designer to seek non_volatile_memory in applications such as portable electronics that retain significant quantities of data over long durations . in this scenario , the energy cost of accessing data must be minimized . this thesis_presents a ferroelectric random_access_memory ( fram ) prototype that addresses the challenges of sensing diminishingly small charge under conditions favorable to low access energy with a time-to-digital sensing scheme . the 1 mb 1t1c fram fabricated in 130 nm cmos operates from 1 . 5 v to 1 . 0 v with corresponding access energy from 19 . 2 pj to 9 . 8 pj per bit . finally , the computational state of sequential elements interspersed in cmos logic , also restricts the ability to power gate . to enable simple and fast turn-on , ferroelectric capacitors are integrated into the design of a standard_cell register , whose non_volatile operation is made compatible with the digital design_flow . a test_case circuit containing ferroelectric registers exhibits non_volatile operation and consumes less than 1 . 3 pj per bit of state information and less than 10 clock cycles to save or restore with no minimum standby_power requirement in-between active periods . acknowledgments i would like to acknowledge the following people and entities that have been involved in my graduate experience : colleagues from 
test-time , run_time , and simulation-time temporal assertions in rsp for cost_effective prototyping , system designers should have a clear understanding of the intended use of the prototype under development . this paper describes a classification of formal_specification ( temporal ) assertions used during system prototyping . the classification introduces two new classes of assertions in addition to the well-known class of test-time assertions : ( i ) assertions used only during simulation , and ( ii ) deployable assertions integrated with run_time control_flow . separating the formal_specification into three distinct classes allows system designers to develop more effective prototypes to evaluate the different system behaviors and constraints . a prototype of a naval torpedo system is used to illustrate the concept . 1 introduction the analysis and design of complex safety_critical embedded_systems pose many challenges . feasible timing and safety requirements for these systems are difficult to formulate , understand , and meet without extensive prototyping . traditional timing_analysis techniques are not effective in evaluating time_series temporal behaviors ( e . g . the maximum duration between consecutive missed deadlines must be greater than 5 seconds ) . this kind of requirements can only be evaluated through execution of the real-time systems or their prototypes . rapid_prototyping also helps system designers formulate and evaluate safety requirements of the system under development , by building two separate models ( one for the system under development and the other for the environment ( or equipment ) under its control ) and then exercising the two models in tandem to see if the simulation ends up in known hazardous states under normal operating_conditions and under various failure conditions [al] . run_time execution monitoring of formal_specification assertions ( rem ) is class of methods of tracking the temporal behavior of an underlying application . rem methods range from simple print-statement logging methods to run_time tracking of complex formal requirements ( e . g . , written in temporal_logic ) for verification purposes [d3] . recently , nasa used rem for the verification of flight code for the deep impact project [dw] . also recently , the u . s . ballistic_missile_defense system has adopted rem as the primary verification method for the new ballistic_missile battle manager because of its ability to scale , and its support for temporal assertions that include real-time and time_series constraints [cdmss] . in [ds] , we showed that the use of run_time monitoring and verification of temporal assertions , in tandem with rapid_prototyping , helps debug the requirements and identify errors earlier in the design process . for cost_effective prototyping , the system designers should have a clear understanding of the intended use of the prototype under 
quiescent_current testing of cmos data converters ii_acknowledgments i dedicate my work to my parents mr . nageswara rao and mrs . vijaya lakshmi , my brother and sister-in law mr . sapta nag and mrs . parimala and my grandparents mr . gandhi and mrs . sambrajam for their constant_support and encouragement throughout my life . i am very grateful to my advisor dr . a . srivastava for his guidance , patience and understanding throughout this work . his suggestions and discussion helped me to get deep insight into the field of vlsi_design and testing . 
high_level crosstalk_defect simulation for system-on-chip interconnects for system-on-chips ( soc ) using deep_submicron ( dsm ) technologies , interconnects are becoming critical determinants for performance and reliability . buses and long interconnects are susceptible to crosstalk_defects and may lead to functional and timing failure . hence , testing for crosstalk errors on interconnects and buses in a soc has become critical . to facilitate development of new crosstalk test_methodologies and to efficiently evaluate crosstalk_defect_coverage for existing tests , there is a need for efficient crosstalk_defect_coverage analysis techniques . in this paper , we present an efficient high_level crosstalk_defect simulation methodology . by using a novel high_level dsm error model for the interconnects , together with hdl models for the cores , our methodology enables fast crosstalk_defect simulation to be conducted at high_level . we validate the high-level interconnect dsm error model by comparing its outputs with hspice simulation_results . the fast_and_accurate high_level crosstalk_defect simulation methodology will enable evaluation and exploration of new crosstalk test techniques , as well as existing tests , leading to the development of low_cost crosstalk test . 
deep_earth seismic structure and earthquake_source processes from long_period waveform modelling deep_earth seismic structure and earthquake_source processes from long_period waveform modelling deep_earth seismic structure and earthquake_source processes from long_period waveform modelling we model long_period seismic waveforms to investigate both the deep_earth velocity structure as well as earthquake_source parameters . we utilize a normal_mode-based perturbation approach to model and invert a global dataset of 3 component long_period seismic waveforms . the approach , which has been used for modelling isotropic velocity structure , is extended for modelling radial anisotropy , which describes an anisotropic medium with a vertical axis of symmetry . a model for shear velocity anisotropy near the core-mantle boundary is developed , and the stability and significance of the fit to the data is analyzed . the model has important implications for relations between flow and observable seismic anisotropy in this important thermal , chemical , and mechanical boundary_layer . this modelling approach is extended to a multiple iteration inversion appropriate for a non-linear problem , and the anisotropic structure of the whole mantle is examined . tests of the stability of the model and the influence of assumptions made in the modelling process are examined . relations between mantle flow and seismic anisotropy are examined for a variety of depth ranges . we also perform inversions for earthquake_source parameters using the same wave-form modelling approach , using the improved velocity model . small , but systematic , relocations of events are observed , as well as small perturbations to the orientation of the mechanisms , and the updated sources result in significant improvement in fit to 2 the data . we also use a finite_difference approach to model regional , rather than tele-seismic , long_period waveforms to determine the significance of observed volumetric components of earthquakes in a volcanic region of eastern california . 
unlocking the nature of the phonological-deep dyslexia continuum : the keys to reading aloud are in phonology and semantics it has been argued that normal reading and acquired dyslexias reflect the role of three underlying primary systems ( phonology , semantics , and vision ) rather than neural mechanisms dedicated to reading . this proposal is potentially consistent with the suggestion that phonological and deep dyslexia represent variants of a single reading disorder rather than two separate entities . the current study explored this possibility , the nature of any continuum between the disorders , and the possible underlying bases of it . a case_series of patients were given an assessment battery to test for the characteristics of phonological and deep dyslexia . the status of their underlying phonological and semantic systems was also investigated . the majority of participants exhibited many of the symptoms associated with deep dyslexia whether or not they made semantic errors . despite wide variation in word and nonword reading accuracy , there was considerable symptom overlap across the cohort and , thus , no sensible dividing line to separate the participants into distinct groups . the patient data indicated that the deep-phonological continuum might best be characterized according to the severity of the individual's reading impairment rather than in terms of a strict symptom succession . assessments of phonological and semantic impairments suggested that the integrity of these primary systems underpinned the patients' reading performance . this proposal was supported by eliciting the symptoms of deep-phonological dyslexia in nonreading tasks . 
content_delivery mechanisms improving content_delivery with padis improving content_delivery with padis cdn limitations network bottlenecks content_delivery costs content_delivery networks ( cdns ) originate a large fraction of internet_traffic; yet , due to how cdns often perform traffic optimization , users aren't always assigned to the best servers for end_user performance . to improve user assignment of cdns , the authors propose and deploy the provider-aided distance information system ( padis ) , which lets isps augment the cdn server by utilizing their unique knowledge about network conditions and user locations . field_tests show that using padis can result in significant improvements in download time . t he internet is now a system in which users generate and share large amounts of content with other users via applications such as online_social_networks , video portals , one-click hosters ( ochs ) , web_services , wikis , blogs , and peer_to_peer ( p2p ) file_sharing applications . multimedia content including photos , music , and videos , as well as software down-loads and updates constitutes most internet_traffic . recent_studies report that most users access this information via ht tp , which accounts for more than 50 percent of internet_traffic http traf-fic's prevalence is due in large part to increased streaming content ( such as that on youtube ) and the popularity of och-offered content 4 ( from sites such as rapidshare . com ) . such popular content is hosted by the internet's " hyper giants , " 1 which include large content providers such as google and yahoo as well as content_delivery networks ( cdns ) such as akamai and limelight . 5 for simplicity's sake , we refer to all these various players simply as cdns . although today's cdns can optimize traffic flows , minimize costs , and bring content closer to end_users , they come with some limitations . to overcome these limitations , we designed a provider-aided distance information system ( padis ) , operated by an isp , that helps isps improve user experiences by utilizing information about network bottlenecks and user locations . padis fills a gap in the content_delivery landscape , in part because it takes_into_account isp constraints and user performance . although padis is deployed at the moment as a platform for isps to improve content_delivery , long_term , it's meant as a generic platform to help both isps and cdns deliver content to users . so , we see padis as an opportunity to think globally about content_delivery by including the network and its users in the picture . to achieve high levels of performance and scalability , cdns rely on distributed infra-structures . some have even deployed servers deep_inside isps in more than 5 , 000 locations throughout the internet . 6 others rely on 
establishing correspondences_between attribute spaces and complex concept spaces using meta-pgn classifier in this paper , we present one approach for extending the learning set of a classification algorithm with additional metadata . it is used as a base for giving appropriate names to found regularities . the analysis of correspondence between connections established in the attribute space and existing links between concepts can be used as a test for creation of an adequate model of the observed world . meta-pgn classifier is suggested as a possible tool for establishing these connections . applying this approach in the field of content_based_image_retrieval of art paintings provides a tool for extracting specific feature combinations , which represent different sides of artists' styles , periods and movements . 1 introduction the problem of resolving the gaps between computer analysis and human understanding has a deep philosophical background even in the problem of understanding between humans . lewis carroll in "through the looking-glass" gives us an example of absurd use of semantics and pragmatics when humpty_dumpty talks with alice : "when i use a word it means just what i choose it to mean neither more nor less" . at the base of semantics lies the definition of concepts as names and corresponding content . our life is full with learning concepts ( their names and contents ) in order to understand each other . there exist many well-suited theories in the area of concept formation based on the attribute models . very close to this understanding is formal_concept_analysis [13] , that uses the philosophical view of a concept as a unit consist
self-healing reconfigurable logic using autonomous group_testing keywords : autonomous systems group_testing reconfigurable architectures evolvable hardware reliable systems organic computing a b s t r a c t a group_testing-based fault resolution is incorporated into sram-based reconfigurable field programma-ble gate arrays ( fpgas ) to provide an evolvable hardware system with self-healing and self-organizing properties . the proposed approach employs adaptive group_testing techniques to autonomously maintain fpga resource viability information as an organic means of transient and permanent fault resolution . reconfigurability of the sram-based fpga is leveraged to locate faulty logic resources which are successively excluded by group_testing using alternate device configurations . this simplifies the system archi-tect's role to definition of functionality using a high-level hardware_description_language ( hdl ) and system-level performance vs . availability operating point . system availability , throughput , and mean time to isolate faults are monitored and maintained using an observer controller model . the proposed group_testing method operates on the output response produced for real-time operational inputs , which eliminates the need for dedicated test_vectors . the proposed system was demonstrated using a data encryption standard ( des ) core on 4-input and 6-input lut-based xilinx fpga models . with a single simulated stuck_at_fault , the system identifies a completely validated replacement configuration within a few test stages . results also include approaches for optimizing group size , resource redundancy , and availability . the approach demonstrates a readily-implemented yet robust organic hardware application that features a high degree of autonomous self-control . implementing a fault-tolerant system has become an essential part in many applications that require high degrees of availability and sustainability [1] . mission critical systems , e . g . those used for deep_space_exploration and communication , in-orbit operations , and unmanned missions deployed in remote terrestrial and marine areas , are constantly exposed to a wide range of environment-related stress , and unknown and unexpected operational conditions not accounted for during design [2 5] . these factors could significantly increase the chance of device misbehaviors [6 8] . in the absence of spare capacity and human maintenance , the need for autonomous fault_tolerant platforms with a self-adaptation property to achieve better availability and sustainability becomes increasingly important [1] . the autonomous , adaptive , and organic systems realized by reconfigurable devices such as fpgas can be leveraged to address the mission sustainability needs of space_exploration projects . nasa has identified autonomous systems as a key area for research [9] , and research is being conducted on building self-managing applications and systems [10 , 11] . auto-nomic computing [12] defines the vision for self-managed organic systems . such systems supplant human monitoring 
adaptation and evaluation of the output-deviations metric to target small_delay_defects in industrial circuits timing_related_defects are a major cause for test escapes and field returns for very-deep-sub-micron ( vdsm ) integrated_circuits ( ics ) . small-delay_variations induced by crosstalk , process_variations , power_supply_noise , and resistive_opens and shorts can cause timing_failures in a design , thereby leading to quality and reliability concerns . we present the industrial application and case study of a previously_proposed test-grading technique that uses the method of output deviations for screening small_delay_defects ( sdds ) . the technique is shown to have significantly lower computational_complexity and test pattern_count , without loss of test_quality , compared to a commercial timing_aware automatic_test_pattern_generation ( atpg ) tool . 
vertical distribution of zooplankton : density dependence and evidence for an ideal free distribution with costs background in lakes with a deep_water algal maximum , herbivorous zooplankton are faced with a trade_off between high_temperature but low food availability in the surface layers and low temperature but sufficient food in deep layers . it has been suggested that zooplankton ( daphnia ) faced with this trade_off distribute vertically according to an "ideal free distribution ( ifd ) with costs" . an experiment has been designed to test the density ( competition ) dependence of the vertical distribution as this is a basic assumption of ifd theory . results experiments were performed in large , indoor mesocosms ( plankton towers ) with a temperature_gradient of 10 degrees c and a deep_water algal maximum established below the thermocline . as expected , daphnia aggregated at the interface between the two different habitats when their density was low . the distribution spread asymmetrically towards the algal maximum when the density increased until 80 % of the population dwelled in the cool , food-rich layers at high densities . small individuals stayed higher in the water_column than large ones , which conformed with the model for unequal competitors . conclusion the daphnia distribution mimics the predictions of an ifd with costs model . this concept is useful for the analysis of zooplankton distributions under a large suite of environmental conditions shaping habitat suitability . fish predation causing diel vertical migrations can be incorporated as additional costs . this is important as the vertical location of grazing zooplankton in a lake affects phytoplankton production and species composition , i . e . ecosystem function . 
implementation of delay_and_power reduction in deep_sub_micron buses using coding implementation of delay_and_power in deep sub-micron buses using coding acknowledgements delay_and_power have become the most important metrics in modern vlsi . applications are becoming more demanding and the need for reducing both delay_and_power is emerging . process scaling is constantly shifting larger portions of delay_and_power to buses and interconnect networks . this work focuses on the design of practical circuit implementations , that address these two problems . a coding scheme that eliminates delay-costly transitions is proposed , thus allowing faster clocking on the bus . an increase of 36% in the total throughput is achieved , while there is a trade_off of increased latency . furthermore , a smart and efficient implementation of charge recycling , which reduces the dynamic power_dissipation when driving long buses , is presented . the design circuit can be used for an arbitrary number of bus lines , and for the test_cases that were examined energy savings up to 32% are reported . 
innovating for emerging_markets : confluence of user , design , business and technology research user_centered_design , new business opportunity development , contextual invention this paper describes a new research methodology , which brings together ethnographic , business , design and technical research in a focused way . the aim is to do this in a principled user-centered way , by using a deep understanding of user and cultural needs to drive design ideas , business modeling and technological investigations . this approach can be seen as an extension of contextual design in which social and cultural factors are considered in the deployment of an existing technology . we call this approach contextual invention , a process of using ethnographic data to generate new technology and business ideas in an interdisciplinary team . to develop and test this new approach , hp_labs india initiated a project towards understanding user needs and deriving concepts , at the same time providing sustainable_business models for the end customer . techniques used in the study include the collection of inspirational materials , the explicit discussion of new product concepts , the feed_forward of new concept ideas , and the circulation of user need and concept sheets . the project was successful in creating business opportunities and design concepts that were at the intersection of user needs , design , business_models and technology feasibility . 
what do exam results really measure ? students are evaluated using examinations , but how do we evaluate whether the examination is correctly measuring the students' knowledge or skill ? this paper presents a methodology which we have used in an experiment : students' exam results were analysed to reveal which different cognitive skills were used in answering different questions . the analysis revealed that students were approaching several questions in ways that the instructor had not anticipated . sometimes questions the instructor considered straightforward actually tested students' conceptual understanding; on other questions which were intended to require problem_solving , many students never identified the concepts involved , so that grades measured primarily the ability to avoid distraction . in other cases , we demonstrated that questions did assess deep understanding of the fundamental concepts , rather than rote_learning or simple pattern_matching . 
on the evolutionary design of heterogeneous bagging models bagging is a popular ensemble algorithm based on the idea of data resampling . in this paper , aiming at increasing the incurred levels of ensemble diversity , we present an evolutionary approach for optimally designing bagging models composed of heterogeneous components . to assess its potentials , experiments with well-known learning_algorithms and classification datasets are discussed whereby the accuracy , generalization and diversity levels achieved with heterogeneous bagging are matched against those delivered by standard bagging with homogeneous components . over the last decades , the strategy of combining multiple_classifiers into ensembles has received increasing interest due to its potential in bringing about significant_improvements in terms of training accuracy and learning generalization [1 , 2] . as the key for the success of any ensemble lies in how its components disagree on their predictions [3] , several approaches for designing diverse components have been conceived , among which those using different subsets of training_data jointly with a single learning method [4 , 5] and those adopting different learning methods associated with different predictors [6 , 7] . a well-known representative of the first group is bagging , which is based on the idea of data resampling [4 , 5 , 8 , 9] . diversity is promoted in bagging by using bootstrapped replicas of the training dataset , each replica being generated by randomly drawing , with replacement , a subset of the training data . typically , each new dataset will have the same number of instances of the original dataset; however , since some instances will appear repeatedly while others will not show up , the effective size will be lower and the datasets will overlap significantly . each derived dataset is used to train a classifier , and then , for any test instance , the outputs of the individual_classifiers are aggregated via the simple majority_vote ( mv ) rule . usually , unstable classifiers are adopted as base models , since this type of classifier can generate sufficiently different decision boundaries even for small perturbations in the training parameters [2 , 4] . in this paper , aiming at further increasing the diversity levels of the ensemble models produced by bagging , we present an evolutionary approach for optimally designing bagging models composed of heterogeneous components . even though the idea of heterogeneous ensembles has been recently advocated [6 , 7] , so far there is no deep investigation on the benefits of adopting different learning_algorithms in the context of bagging . in fact , this idea seems very reasonable since different classes of learning_algorithms are usually associated with different search/represen-tation biases ( and thus hypothesis spaces ) [10] , thereby foment-ing ensemble 
diagram interaction during intelligent_tutoring in geometry : support for knowledge retention and deep_understanding prior research has shown that skilled problem solvers often use features of visual_representations to cue relevant knowledge , but little is known about how to support learners in developing connections between visual and verbal knowledge components . in this research , we investigated two methods to support focus on key visual features during problem_solving in an intelligent tutor : 1 ) student interaction with diagrams during problem_solving , and 2 ) student explanations that connected diagram features to geometry rules at each problem_solving step . research was conducted in 10 th grade classrooms using an experimental version of the geometry cognitive_tutor . interaction with diagrams promoted long_term retention of problem_solving skills and supported deep understanding of geometry rules , as evidenced by items testing transfer and visual-verbal knowledge integration . diagram-rule explanations did not significantly influence learning . findings_suggest that student focus on relevant visual_information should be carefully integrated into problem_solving practice to support robust learning . visual_representations in skilled performance existing research has found that experts use visual_representations in rich and interconnected ways during skilled problem_solving . stylianou ( 2002 ) studied the problem_solving processes of professional mathematicians and noted that mathematicians used diagrams extensively to inform their analysis of the problem , their selection of subgoals , and their eventual solutions . during problem_solving , mathematicians created visual_representations in a step_by_step manner , where visual_information in the representation was analyzed at each step in order to inform reasoning and to cue relevant approaches . mathematicians recognized important features and patterns in their diagrams and revised or annotated their diagrams to reflect the outcome of their analysis at each step . stylianou's ( 2002 ) results complement previous_research in expert problem_solving that has demonstrated close connections between visual_representations and existing knowledge . koedinger and anderson ( 1990 ) found that experts solving geometry problems made inferences that were strongly tied to geometry diagrams , and that features in the problem diagrams cued relevant problem_solving steps . koedinger and anderson found that the problem_solving steps mentioned and skipped by experts could be successfully predicted by a model ( the diagram configuration model ) that parsed diagrams into key geometric configurations and used these configurations to cue relevant schemas . the development of skilled performance in geometry appears to be correlated with attention to key diagram features , as well as successful association of those features with relevant geometry rules . recent eye_tracking research suggests that learner focus on key visual_information predicts successful performance even among non-experts . on insight 
software emulation of programmable optical routers programmable optical networks are a key solution to high_performance dynamic network services in future internet scenario . modular router architecture which meets this concept is here defined and represented in a flexible software context , using click ! tools . evaluation of logical performance and testing of physical_characteristics of information forwarding within this framework is shown as feasible . the approach allows also to easily test functions and interactions of control and data planes to support enhanced future internet_services . sample implementations of programmable router subsystems are given and discussed . i . introduction future internet concepts strongly rely on network virtual-ization and fast , dynamic reconfigurations of network services . emerging and foreseeable network-based applications might have heterogeneous requirements in terms of bandwidth , delay and latency , as well as dynamism and diversity [1] . new services , some of which not envisaged during network design , as well as additional resources can be dynamically added to or removed from the network . smart network control integrates this scenario with fast , rich and flexible signaling procedures [2] . for these reasons , the success and growth of future internet applications require a deep rethink of node and network architectural concepts which have started increasing discussion in the networking research_community [3] . hence , it is vital to understand and redefine the role of networking , in order to implement high_performance network concepts able to support applications with wide range of requirements , by providing different transport services and offering a flexible , scalable and cost_effective solution to service_providers . to respond to a flexible and distributed_environment , flexible architectural and technological solutions should be considered to reflect the characteristics and requirements of dynamic network-enabled applications . the target to design high_performance flexible architectural solutions , both for nodes and networks , should consider state-of-the-art optical and/or electronic technologies combined with modular architectural concepts in the awareness of characteristics and requirements of emerging network-enabled applications as well as service_providers' needs . network and node programmability are key concepts that need to be investigated in this framework [4] . concerning high-capacity programmable optical routers , they are expected to consist of complementary modules , to support control and data plane functions and their interactions inside the node , possibly in a multi-provider context . this last aspect would be important
multiscale brain modelling . a central difficulty of brain modelling is to span the range of spatio-temporal scales from synapses to the whole brain . this paper overviews results from a recent model of the generation of brain electrical activity that incorporates both basic microscopic neurophysiology and large_scale brain anatomy to predict brain electrical activity at scales from a few tenths of a millimetre to the whole brain . this model incorporates synaptic and dendritic dynamics , nonlinearity of the firing response , axonal propagation and corticocortical and corticothalamic pathways . its relatively few parameters measure quantities such as synaptic strengths , corticothalamic delays , synaptic and dendritic time constants , and axonal ranges , and are all constrained by independent physiological measurements . it reproduces quantitative forms of electroencephalograms seen in various states of arousal , evoked response potentials , coherence functions , seizure dynamics and other phenomena . fitting model predictions to experimental_data enables underlying physiological parameters to be inferred , giving a new non-invasive window into brain_function that complements slower , but finer-resolution , techniques such as fmri . because the parameters measure physiological quantities relating to multiple scales , and probe deep_structures such as the thalamus , this will permit the testing of a range of hypotheses about vigilance , cognition , drug action and brain_function . in addition , referencing to a standardized database of subjects adds strength and specificity to characterizations obtained . 
intelligent collaborating agents to support teaching_and_learning this paper presents the intelligent multiagent infrastructure for distributed system in education ( i-minds ) , an innovative application using ai and mul-tiagent systems to help teachers teach better and students_learn better . the i-minds system consists of a group of intelligent_agents that work cooperatively in a distributed_computing environment . a teacher agent monitors the student activities and helps the teacher manage and better adapt to the class . a student agent interacts with the teacher agent and other student agents to support cooperative_learning activities behind the scene for a student . this paper describes two innovations in ( a ) automated ranking of questions and responses , and ( b ) agent-supported " buddy group " formation . the results of the proof_of_concept tests have demonstrated encouraging effectiveness of i-minds in terms of learning gain and deep_understanding . 
on the intrinsic rent_parameter and spectra-based partitioning methodologies the complexity of circuit designs has necessitated a top-down approach to layout synthesis . a large body of work shows that a good partitioning hierarchy , as measured by the associated rent_parameter , will correspond to an area-eecient layout . we deene the intrinsic rent_parameter of a netlist to be a lower_bound on the rent_parameter of any partitioning hierarchy for the netlist . experimental results show that spectra-based ratio cut partitioning methods yield partitioning hierarchies with the lowest observed rent_parameter over all benchmarks and over all algorithms tested . for examples where the intrinsic rent_parameter is known , spectral ratio cut partitioning yields a rent_parameter essentially identical to this theoretical optimum . we provide additional theoretical results supporting the close relationship between spectral partitioning and the intrinsic rent_parameter . these results have deep_implications with respect to the choice of partitioning algorithms and new approaches to layout area estimation . 
race system description other features availability future plans language race [3] is a successor of ham-alc [5 , 2] . based on sound and complete algorithms race currently implements tbox and abox_reasoning for the description_logic alcn h r + [4] that supports number restrictions , role hierarchies , and transitively closed roles . note that this dl implies general concept inclusions as a language feature . race accepts tboxes and aboxes in the krss syntax with appropriate extensions for gcis . race additionally provides a web interface based on cl-http , a common_lisp hypermedia server . implementation race employs standard ( exploitation of told sub-sumers/subsumees by marking and caching operations for computing the subsumption hierarchy , lazy unfolding ) and advanced optimization_techniques ( semantic branching , dependency-directed backtracking , taxo-nomic encoding , gci absorption ) in analogy to [6] as well as deep model merging and extensive model caching . it also exploits new optimization_techniques for abox_reasoning [3] . the programming_language is ansi common_lisp . performance race is one of the fastest systems for testing concept consistency and outperforms any known abox_reasoning system by several orders_of_magnitude . for details on a performance_evaluation regarding abox_reasoning see [3] . the tests presented in the appendix were performed using race version 1 . 0 on a macintosh g3 powerbook ( 266 mhz ) with macintosh common_lisp 4 . 2 in a 80 mb memory partition . if a size attribute is specified for the problem , the runtime is given for the last size that can be solved within the specified timeout limit . we plan to extend race to additionally supporting abox reasoning_with qualified number restrictions and inverse roles . future versions of race will also include more advanced optimization_techniques for the inference algorithms dealing with number restrictions . [3] v . haarslev and r . m ller . an empirical_evaluation of optimization strategies for abox_reasoning in expressive description_logics . 
developing student help desk consultants : a skill-based modular approach student help desk consultants are an essential resource in most college it departments . hiring and training these students presents special challenges : consultants need a set of skills that is both wide and deep , and supervisors may be unsure which skills to hire for and which to train for , how to provide training , and how to measure competency . at duquesne_university we have created a system for developing skilled consultants that is based on : how easy a skill is to teach; which mode of teaching best lends itself to a particular skill; and the need to accommodate different learning_styles in the students . we first grouped the skills into five core_competency areas : technical knowledge , troubleshooting and problem_solving , knowledge of our structure and procedure , customer service skills , and professional behavior . we then identified five modalities for acquiring these skills in our consultants , being : <ul><li>hire people with the skill;</li><li>provide classroom training;</li><li>train through apprenticeship;</li><li>rely on independent learning; or</li><li>provide ongoing training . </li></ul> . different skill sets are emphasized within each modality , but most are addressed across several modalities so as to ensure mastery and accommodate different learning_styles . for example , attitude and interpersonal skills are heavily weighted in the hiring criteria . formal instruction in customer service skills is then provided in a classroom setting and application of these skills is emphasized in the apprenticeship phase of training . finally , customer service skills may be further developed through independent learning and ongoing training . we use this approach to structure all phases of consultant development : candidate selection , training , evaluation , and ongoing support . this poster presentation provides an overview of the analysis and plan , together with a selection of the instruments ( forms , tests , etc . ) that we use in each phase . 
a diagnostic system for photolithography equipment this paper presents a general diagnostic system that can be applied to semiconductor equipment to assist the operator in finding the causes of decreased machine performance . based on conventional probability_theory , the diagnostic system incorporates both shallow and deep level information . from the observed evidence , and from the conditional_probabilities of faults initially supplied by machine experts ( and subsequently updated by the system ) , the fault probabilities and their bounds are calculated , given a specified confidence level . the rate of convergence of the fault probabilities has been derived in detail in the paper , and the procedure for combining the estimates of conditional_probabilities given by the machine experts has also been described in detail . we have implemented a software version of the diagnostic system , and tested it on real photolithography equipment malfunctions and performance drifts . initial experimental_results are encouraging . 
deep start : a hybrid strategy for automated performance problem searches to attack the problem of scalability of performance diagnosis tools with respect to application code size , we have developed the deep start search_strategy a new technique that uses stack sampling to augment an automated search for application performance problems . our hybrid approach locates performance problems more quickly and finds performance problems hidden from a more straightforward search_strategy . the deep start strategy uses stack samples collected as a by-product of normal search instrumentation . using these samples , our strategy selects deep starters functions that are likely to be application bottlenecks and thus are good locations to consider early in the search . with priorities and careful control of the search refinement , our strategy gives preference to experiments on the deep starter functions and their callees . this approach enables the deep start strategy to find application bottlenecks more efficiently and more effectively than a more straightforward search_strategy . we implemented the deep start search_strategy in the performance consultant , paradyn's automated bottleneck detection component . in our tests , the deep start strategy found half of all known bottlenecks between 25% and 63% faster on average than the performance consultant's current search_strategy . the deep start strategy found all bottlenecks in its search between 7% and 61% faster on average than the current strategy . 
lower extremity muscle activity during different types and speeds of underwater movement . to compare the activity of lower extremity muscles during land walking ( lw ) , water walking ( ww ) , and deep_water running ( dwr ) , 9 healthy young subjects were tested at self-selected low , moderate , and high intensities for 8 sec with two repetitions . surface emg electrodes were placed on the tibialis anterior ( ta ) , soleus ( sol ) , medial gastrocnemius ( gas ) , rectus femoris ( rf ) , and biceps femoris ( bf ) . during dwr , the sol and gas activities were lower than lw and ww . the bf activities were higher during dwr than lw and ww . it was considered that the lower activity of sol and gas depended on water_depth , and higher activity of bf occurred by greater flexion of the knee_joint or extension of the hip joint during exercise . 
improvements to cbcm ( charge-based capacitance measurement ) for deep_submicron cmos_technology accurate measurement and analysis of interconnect capacitance is a critical component of nanometer technology verification . the charged-based capacitance measurement ( cbcm ) technique has been widely adopted as a robust technique to measure on-chip capacitance test_structures . in this paper we present two design improvements for cbcm . the first is an area reduction by using bused circuit architecture to reduce probe pad area required for the test structure input and output signals . the second improvement involves techniques to reduce the impact of gate leakage and charge injection currents in 90 and 65nm process technology_nodes . at the 90nm node we demonstrate accuracy improvement of an order_of_magnitude for small test_structures . 
evaluating probabilities under high_dimensional latent_variable models we present a simple new monte_carlo algorithm for evaluating probabilities of observations in complex latent_variable models , such as deep_belief_networks . while the method is based on markov_chains , estimates based on short runs are formally unbiased . in expectation , the log probability of a test set will be underestimated , and this could form the basis of a probabilistic bound . the method is much cheaper than gold_standard annealing-based_methods and only slightly more expensive than the cheapest monte_carlo_methods . we give examples of the new method substantially improving simple variational bounds at modest extra cost . 
test and debug in deep-submicron technologies with the scaling of feature_sizes into deep_submicron ( dsm ) values , the level of integration and performance achievable in vlsi chips increases . a lot of work has been directed to tackle design related issues arising out of scaling , like leakage mitigation etc . however efforts to enhance testability of such designs have not been sufficient . it is not viable to overlook testability issues arising out of these designs because the defect sizes do not scale proportional to the feature_sizes . previously effective fault_models like stuck-at appear archaic and are unable to model faults accurately . this necessitates the need for more detailed models which can more explicitly model the behavior of faulty dsm chips . also there is a significant increase in delay_faults in logical paths of integrated_circuits . delay_faults cause the delay of paths in a chip to be larger than expected resulting in the output of a chip to be deviant from the expected behavior , in spite of the chip being functionally correct . efficient techniques are needed for detecting such defects in first silicon and eliminating them before the final versions of the chips are shipped . this requires efficient debug techniques for performance characterization of large complex integrated_circuits in deep_submicron and nanome-ter technologies . in this paper we present an insight into test_challenges arising out of deep_submicron_technologies and effective approaches to tackle the same . 
obfuscating document stylometry to preserve author anonymity this paper explores techniques for reducing the effectiveness of standard authorship attribution techniques so that an author a can preserve anonymity for a particular document d . we discuss feature_selection and adjustment and show how this information can be fed back to the author to create a new document d' for which the calculated attribution moves away from a . since it can be labor intensive to adjust the document in this fashion , we attempt to quantify the amount of effort required to produce the ano-nymized document and introduce two levels of anonymization : shallow and deep . in our test_set , we show that shallow anonymization can be achieved by making 14 changes per 1000 words to reduce the likelihood of identifying a as the author by an average of more than 83% . for deep anonymization , we adapt the unmasking work of koppel and schler to provide feedback that allows the author to choose the level of ano-nymization . 
the impact of electronic_commerce on the retail brokerage industry electronic_commerce has enjoyed great success in the retail brokerage industry . attracted by commission savings , consumers' use of on-line brokerage firms has grown . however , brokerage customers may have difficulty comparing total trading costs , which consist of both the commission the broker charges and the cost of executing a trade . this paper reports on an experiment to examine whether order handling practices by traditional voice brokers and on-line brokerage firms lead to differences in the quality of trade execution . we test two hypotheses; the first is that execution quality differs among brokers and is positively related to commission rates , and the second is that total trading costs are converging as might be expected in a stable market . in the experiment , we conducted 196 trades , simultaneously purchasing or selling 100 share lots of stock using a voice-based broker , a "brand-name" online broker and a deep discount online broker in each trial . we found 36 percent of our orders received price improvement , a measure of execution quality . the differences among brokers in obtaining price improvements were ( weakly ) statistically_significant for nyse-listed shares only . the brokers do exhibit statistically_significant differences in total trading costs; at a volume of 100 shares commission costs dominate execution quality . we discuss implications for larger lot sizes and speculate on the ability of full-service brokerage firms to maintain high commission charges . the paper concludes that electronic_commerce is having a major impact on the brokerage industry , and has the potential to affect pricing in other industries with bundled products and services . 1 the authors wish to thank professor ingo walter and the salomon_brothers center at the stern school for their support and willingness to underwrite the research reported in this paper . 
new directions in eddy_current sensing enhancing eddy_current_probes nondestructive_testing needs an effective , inexpensive way of detecting deeply buried or small cracks at the edges of metallic parts and structures . one solution comes in the form of solid_state_magnetic sensors based on giant magneto-resistance ( gmr ) and spin-dependent tunneling ( sdt ) effects integrated in eddy_current_probes . eddy_current testing is an effective way of detecting fatigue cracks and corrosion in conductive materials . the cost of using the technology is low , and the devices can monitor subsurface defects and defects under insulating coatings without touching the surface of the specimen . although this technique is applicable for many tasks , the aircraft and nuclear_power industries are the primary users of eddy_current_probes for in-service inspection . because safety_critical systems depend on early detection of fatigue cracks to avoid major failures , there's an increasing need for eddy_current_probes that can reliably detect very small defects . also there are increasing demands for probes that can detect deeply buried defects to avoid disassembling structures . eddy_current testing probes combine an excitation coil that induces eddy currents in a specimen and a detection element that identifies the perturbation of the currents caused by cracks or other defects . the detection elements can be coils , superconducting quantum interference detectors , or solid_state_magnetic sensors ( e . g . , hall_effect , magneto-resistive , and spin-dependent-tunneling sensors ) . the use of low-field , solid_state_magnetic sensors represents a significant advance over more traditional inductive probes in use today . two key attributes will open opportunities for increased use of eddy_current_probes : constant sensitivity over a wide range of frequencies and development of smaller sensors . probes that detect eddy_current fields using inductive coils have less sensitivity at low frequencies . unfortunately , this is where the device would have to operate to detect deep flaws . small sensing coils , which are required to detect small defects , also have low sensitivity . in contrast , small , high-sensitivity thin film sensors can locally measure a magnetic_field over an area comparable to the size of the sensor itself ( tens of micrometers ) . a limitation of conventional eddy_current_probes is the difficulty of detecting small cracks originating at the edges of a specimen . this defect is the most common type encountered in practice . an example is the cracks that appear around the fastener or rivet holes in aircraft multilayered structures . most inductive coil probes are sensitive to both the edge and the cracks initiating from or near the edge . the edge creates a 
a gracefully degrading and energy_efficient modular router architecture for on-chip networks packet-based on-chip_networks are increasingly being adopted in complex system-on-chip ( soc ) designs supporting numerous homogeneous and heterogeneous functional blocks . these network_on_chip ( noc ) architectures are required to not only provide ultra_low latency , but also occupy a small footprint and consume as little energy as possible . further , reliability is rapidly becoming a major challenge in deep sub-micron technologies due to the increased prominence of permanent_faults resulting from accelerated aging effects and manufacturing/testing challenges . towards the goal of designing low-latency , energyefficient and reliable on-chip communication networks , we propose a novel fine_grained modular router architecture . the proposed architecture employs decoupled parallel arbiters and uses smaller crossbars for row and column connections to reduce output port contention probabilities as compared to existing designs . furthermore , the router employs a new switch allocation technique known as "mirroring effect" to reduce arbitration depth and increase concurrency . in addition , the modular_design permits graceful degradation of the network in the event of permanent_faults and also helps to reduce the dynamic power_consumption . our simulation results indicate that in an 8 8 mesh network , the proposed architecture reduces packet latency by 4-40% and power_consumption by 6-20% as compared to two existing router architectures . evaluation using a combined performance , energy and fault-tolerance metric indicates that the proposed architecture provides 35-50% overall improvement compared to the two earlier routers . 
computationally derived points of fragility of a human cascade are consistent with current therapeutic strategies the role that mechanistic mathematical modeling and systems_biology will play in molecular_medicine and clinical development remains uncertain . in this study , mathematical modeling and sensitivity_analysis were used to explore the working hypothesis that mechanistic models of human cascades , despite model uncertainty , can be computationally screened for points of fragility , and that these sensitive mechanisms could serve as therapeutic targets . we tested our working hypothesis by screening a model of the well-studied coagulation cascade , developed and validated from literature . the predicted sensitive mechanisms were then compared with the treatment literature . the model , composed of 92 proteins and 148 protein_protein_interactions , was validated using 21 published datasets generated from two different quiescent in vitro coagulation models . simulated platelet activation and thrombin generation profiles in the presence and absence of natural anticoagulants were consistent with measured values , with a mean correlation of 0 . 87 across all trials . overall state sensitivity coefficients , which measure the robustness or fragility of a given mechanism , were calculated using a monte_carlo strategy . in the absence of anticoagulants , fluid and surface phase factor x/activated factor x ( fx/fxa ) activity and thrombin-mediated platelet activation were found to be fragile , while fix/fixa and fviii/fviiia activation and activity were robust . both anti-fx/fxa and direct thrombin inhibitors are important classes of anticoagulants; for example , anti-fx/fxa inhibitors have fda approval for the prevention of venous thromboembolism following surgical intervention and as an initial treatment for deep venous_thrombosis and pulmonary_embolism . both in vitro and in vivo experimental evidence is reviewed supporting the prediction that fix/fixa activity is robust . when taken together , these results support our working hypothesis that computationally derived points of fragility of human relevant cascades could be used as a rational_basis for target selection despite model uncertainty . 
combined scheme for fast pn code_acquisition i-introduction in direct-sequence wideband ( ds-wb ) systems , long spreading sequences are used to span multiple symbol intervals in order to remove spectral_lines and to mitigate multiple access interference . the usage of long spreading sequences will result in a large search_space during acquisition stage . as a result , the ds-wb systems will need a lot of time to achieve acquisition state . this paper proposes a combined acquisition scheme , known as fast serial_search sequential estimation ( fssse ) , which could be applied to the direct_sequence_spread_spectrum systems with long pn sequences . the proposed scheme is a combination of two acquisition schemes which are the serial_search acquisition scheme and the rapid acquisition by sequential estimation ( rase ) scheme . the proposed scheme gets the advantages of both schemes while overcome their drawbacks . the mean acquisition time of the proposed scheme is reduced 100 times compared to the conventional serial_search scheme for pn sequence with a period of 2 15-1 . search acquisition scheme-fast serial_search sequential estimation acquisition scheme-rapid acquisition by sequential estimation scheme . in direct_sequence_spread_spectrum ( dsss ) systems , the goal of code_acquisition is to achieve a coarse time alignment between the received pn code and the locally generated code to an accuracy of a fraction of one pn sequence chip [1 , 2] . serial_search strategy [3] is the most popular approach to code_acquisition which correlates the received and locally generated code sequences and then tests the synchronization based on either the crossing of threshold or the maximum correlation . a threshold value is determined depending on the signal_to_noise_ratio at the matched filter output [2] and it may be adjusted to the partial correlation [1] . a direct approach [4] for obtaining statistics of the code_acquisition time for serial_search spread_spectrum receivers is presented . it combines algebraic characterization of the search with transform-domain methods . this approach gives a very deep insight into the nature of the acquisition process . this fact permits the author to propose two alternate search strategies that outperform the conventional ones when the code rewinding time is small in comparison to the dwell times . the serial_search scheme has a drawback of its relatively long acquisition time for long periods pn codes . a double-dwell serial code_acquisition system employing an adaptive threshold estimator is described and analyzed and a general expression for the mean acquisition time is derived [5] . 
robust interpretation in dialogue by combining confidence scores with contextual features we present an approach to dialogue management and interpretation that evaluates and selects amongst candidate dialogue moves based on features at multiple levels . multiple interpretation methods can be combined , multiple speech_recognition and parsing hypotheses tested , and multiple candidate dialogue moves considered to choose the highest scoring hypothesis overall . we integrate hypotheses generated from shallow slot-filling methods and from relatively deep parsing , using pragmatic information . we show that this gives more robust performance than using either approach alone , allowing n-best list reordering to correct errors in speech_recognition or parsing . 
deep_learning based semantic video indexing and retrieval we share the implementation_details and testing results for video retrieval_system based exclusively on features_extracted by convolutional_neural_networks . we show that deep learned features might serve as universal signature for semantic content of video useful in many search and retrieval tasks . we further show that graph_based storage structure for video index allows to efficiently retrieving the content with complicated spatial and temporal search queries . 
a fuzzified approach towards global_routing in vlsi layout design in dsm ( deep_sub_micron ) regime , together with the integration density interconnects play a dominant role during layout design of integrated_circuits . it eventually increases the importance of global_routing problem making it more challenging day by day . to cope up with this ever increasing design_complexity , the challenging time faced by researchers provides the important opportunity to explore new ideas to solve it within some reasonable time . heuristic based_approaches are generally used for global_routing . large problem space leads global_routing problem to a np_complete one which is less compatible with modern trends . the proposed multi_objective global_routing technique is formulated using fuzzy_logic to get rid of the limitations of deterministic approaches . after placement and prior to routing phase a set of guiding information is generated from our approach , which will help routing in subsequent steps . during global_routing the decision is taken from a fuzzy_logic expert system . a gui is implemented based on the proposed algorithm which is tested for its feasibility study and experimental validation . success of our proposed approach will open up an avenue for research in global_routing phase . 
a new dvb_rcs satellite_channel_model based on discrete_time markov_chain and quality degree dvb_rcs is an open satellite_communications standard allowing a bi-directional communication via satellite . these characteristics certainly promoted its enormous diffusion in the commercial area and the academic research interest . in this context , it is very important to test dvb_rcs systems using an efficient satellite_channel_model . in the literature many channel models have been carried out , but most of them work at the bit_level or they investigate only some aspects of channel interaction . in this work we provide a high level satellite_channel_model based on discrete_time markov_chain ( dtmc ) modeling , useful in every simulation context . our model , called quality degree-dtmc ( qd-dtmc ) , is based on the concept of quality degree ( qd ) of a given observation windows : the idea is not to analyze a single packet , but fixing an observation window and evaluating the qd of the link , computing the packet error_rate ( per ) associated to the specific window . the effectiveness of the proposed idea has been evaluated through a deep campaign of simulations . 
integrated reliability workshop call for papers the international integrated reliability workshop focuses on ensuring semiconductor reliability through fabrication , design , testing , characterization , and simulation , as well as identification of the defects and physical mechanisms responsible for reliability problems . through tutorials , discussion groups , special interest groups , and the informal format of the technical program , a unique environment is provided for understanding , developing , and sharing reliability technology for present and future semiconductor applications as well as ample opportunity for discussions and interactions with colleagues . hot reliability topics for the workshop include : high- and nitrided sio 2 gate dielectrics , nbti , cu interconnects and low- dielectrics , product reliability and burn-in strategy , impact of transistor degradation on circuit reliability , reliability modeling_and_simulation , sige and strained si , iii-v , soi , optoelectronics , single_event upsets , and reliability assessment of novel devices and future " nano "-technologies . we invite you to submit a presentation proposal that addresses any integrated semiconductor related reliability issue , including the following topics : designing-in reliability ( products , circuits , systems , processes ) : methodologies and concepts , modeling_and_simulation tools , reliability-driven design rules and checkers , use of wlr and/ or reliability test_structures for design rule verification , in-line detection and reliability analysis . customer product reliability requirements / manufacturer reliability tasks : limits to achieving future reliability targets , reliability evaluation methodologies and reporting systems , data bases , chip reliability , product reliability extrapolation to use conditions , wafer and package burn-in , packaging , strategies to eliminate burn-in , correlation between process , yield , and reliability , qualification strategies . root cause defects , physical mechanisms , and simulations nature of defects , physical and electrical characterization of defects , defect generation models , process induced defects and degradation , modeling/simulation of reliability related circuit constraints , accelerated testing and lifetime extrapolation . identification and characterization of reliability effects : failure_mechanisms in new materials and device structures , reliability aspects of : high-k gate stack , cu interconnects and low-k dielectrics , mos and bipolar transistors including finfets and 3d gates , sige and strained si , soi , mems devices , optoelectronics , high voltage devices , unique reliability phenomena and failure_mechanisms in non_volatile memories , new memory technologies , mram , nanotechnology reliability assessment , and limits to accelerated stressing . deep_sub_micron transistor and circuit reliability : single_event_effects and soft_errors , esd , electromigration , mechanical stress related issues , hot carrier effects , nbti , dielectric breakdown , reliability extrapolation , impact of new material systems , modeling_and_simulation , impact of scaling , wafer_level reliability tests , test approaches , and reliability test_structures : fast stress tests and analysis methodologies , reduction in development time , in-line monitors , 
hybrid neural_network_architecture for on-line learning_approaches to machine intelligence based on brain models have stressed the use of neural networks for generalization . here we propose the use of a hybrid neural_network_architecture that uses two kind of neural_networks simultaneously : ( i ) a surface learning agent that quickly adapt to new modes of operation; and , ( ii ) a deep_learning agent that is very accurate within a specific regime of operation . the two networks of the hybrid_architecture perform complementary functions that improve the overall performance . the performance of the hybrid_architecture has been compared with that of back-propagation perceptrons and the cc and fc networks for chaotic time_series prediction , the cats benchmark test , and smooth function_approximation . it has been shown that the hybrid_architecture provides a superior performance based on the rms error criterion . 
taint-based directed whitebox fuzzing we present a new automated white box fuzzing technique and a tool , buzzfuzz , that implements this technique . unlike standard fuzzing techniques , which randomly change parts of the input file with little or no information about the underlying syntactic_structure of the file , buzzfuzz uses dynamic taint tracing to automatically locate regions of original seed input files that influence values used at key program attack points ( points where the program may contain an error ) . buzzfuzz then automatically generates new fuzzed test input files by fuzzing these identified regions of the original seed input files . because these new test files typically preserve the underlying syntactic_structure of the original seed input files , they tend to make it past the initial input parsing components to exercise code deep within the semantic core of the computation . we have used buzzfuzz to automatically find errors in two open_source applications : swfdec ( an adobe flash_player ) and mupdf ( a pdf viewer ) . our results indicate that our new directed fuzzing technique can effectively expose errors located deep within large programs . because the directed fuzzing technique uses taint to automatically discover and exploit information about the input file_format , it is especially appropriate for testing programs that have complex , highly structured input file formats . 
research statement combining ensembles for semi_supervised_learning [webpage] [link to cv] my research has centered around machine_learning , which has exploded in the past two decades . the field has addressed cornerstone problems that recur in myriad application areas , and solved them with general , useful algorithms that are ultimately state-of-the-art across many of these areas given enough data , using statistics and other mathematics . i have been motivated during my phd by this remarkable data-driven combination of generality and practicality , working to devise new algorithms that exploit data_structure in practical learning scenarios . i have focused on two specific problem areas which share the above characteristics , answering the following questions over the course of my phd : 1 . semi_supervised_learning with ensembles of predictors : how can we devise an efficient , practical , and interpretable algorithm that uses large quantities of unlabeled_data to best put together the predictions of a collection of classifiers of varying competence ? 2 . sequential algorithms and stopping : how can we robustly adapt traditional statistical hypothesis tests to report accurate results when the sample_size is unknown ? how can we use as few samples as possible to detect an effect of unknown magnitude ? these areas have interested me because they recur frequently in common practical problems , and yet are still new enough to researchers to present many basic fresh challenges . i now discuss my work in each of them in the first two sections . in the third section , i discuss my future interests , which involve applications of such algorithms to interdisciplinary research . in the fundamental learning problem of ( binary ) classification , each datum has one of two labels; the learner is required to predict those of some unlabeled_data , given a set of labeled_data . many widely-used methods exist for this problem , like linear predictors , decision_trees , bayesian classifiers , and deep_neural_nets . a typical workflow might involve using the labeled_data to train several different such algorithms and estimate their error_rates , and finally choosing the best of the algorithms with which to predict on the unlabeled_data . justifying this empirical risk minimization procedure is part of the bedrock of classical learning theory ( [vapnik , 1982] ) . however , practical cutting-edge usage very frequently involves aspects not addressed in this account , which i have focused on in my work . a ) first and foremost , reliably labeled_data are expensive to obtain in many applications in medicine , nlp , and other areas the phrase "big_data" in classification often solely signifies abundant unlabeled 
mathematical problems who of us would not be glad to lift the veil behind which the future lies hidden; to cast a glance at the next advances of our science and at the secrets of its development during future centuries ? what particular goals will there be toward which the leading mathematical spirits of coming generations will strive ? what new methods and new facts in the wide and rich field of mathematical thought will the new centuries disclose ? history teaches the continuity of the development of science . we know that every age has its own problems , which the following age either solves or casts aside as profitless and replaces by new ones . if we would obtain an idea of the probable development of mathematical knowledge in the immediate future , we must let the unsettled questions pass before our minds and look over the problems which the science of today sets and whose solution we expect from the future . to such a review of problems the present day , lying at the meeting of the centuries , seems to me well adapted . for the close of a great epoch not only invites us to look back into the past but also directs our thoughts to the unknown future . the deep significance of certain problems for the advance of mathematical science in general and the important r le which they play in the work of the individual investigator are not to be denied . as long as a branch of science offers an abundance of problems , so long is it alive; a lack of problems foreshadows extinction or the cessation of independent development . just as every human undertaking pursues certain objects , so also mathematical research requires its problems . it is by the solution of problems that the investigator tests the temper of his steel; he finds new methods and new outlooks , and gains a wider and freer horizon . it is difficult and often impossible to judge the value of a problem correctly in advance; for the final award depends upon the grain which science obtains from the problem . nevertheless we can ask whether there are general criteria which mark a good mathematical problem . an old french mathematician said : " a mathematical theory is not to be considered complete until you have made it so clear that you can explain it to the first man whom you meet on the street . " this clearness and ease of comprehension , here insisted 
cross-domain dependency parsing using a deep_linguistic grammar pure statistical parsing systems achieves high in-domain accuracy but performs poorly out-domain . in this paper , we propose two different approaches to produce syntactic dependency structures using a large-scale hand-crafted hpsg grammar . the dependency backbone of an hpsg analysis is used to provide general linguistic insights which , when combined with state-of-the-art statistical dependency parsing models , achieves performance improvements on out-domain tests . 
geotechnical parameters and distribution characteristics of the cobalt-rich manganese crust for the miner design the importance of deep-ocean deposits for future cobalt resources and geological distribution characteristics have been well recognized . however , no sufficient geotechnical information of the cobalt-rich manganese deposit region is available to aid the design of seafloor mining systems . based on the results of a recent large-diameter gravity coring , a model test of the coring , and analyses of geotechnical properties of seamount sediments , topographic and microtopographic distribution of the deposits , strength characteristics of the crusts and the substrates , and the geotechnical distribution characteristics of the deposits are presented . although the present data represent a small coverage , the data will be useful as preliminary miner design parameters , and the initial design parameters are also discussed . 
extrinsic embryonic sensory stimulation alters multimodal behavior and cellular activation . embryonic vision is generated and maintained by spontaneous neuronal activation_patterns , yet extrinsic stimulation also sculpts sensory development . because the sensory and motor systems are interconnected in embryogenesis , how extrinsic sensory activation guides multimodal differentiation is an important topic . further , it is unknown whether extrinsic stimulation experienced near sensory sensitivity onset contributes to persistent brain changes , ultimately affecting postnatal behavior . to determine the effects of extrinsic stimulation on multimodal development , we delivered auditory stimulation to bobwhite quail groups during early , middle , or late embryogenesis , and then tested postnatal behavioral responsiveness to auditory or visual cues . auditory preference tendencies were more consistently toward the conspecific stimulus for animals stimulated during late embryogenesis . groups stimulated during middle or late embryogenesis showed altered postnatal species-typical visual responsiveness , demonstrating a persistent multimodal effect . we also examined whether auditory-related brain regions are receptive to extrinsic input during middle embryogenesis by measuring postnatal cellular activation . stimulated birds showed a greater number of zenk-immunopositive cells per unit volume of brain tissue in deep optic tectum , a midbrain region strongly implicated in multimodal function . we observed similar results in the medial and caudomedial nidopallia in the telencephalon . there were no zenk differences between groups in inferior_colliculus or in caudolateral nidopallium , avian analog to prefrontal_cortex . to our knowledge , these are the first results linking extrinsic stimulation delivered so early in embryogenesis to changes in postnatal multimodal behavior and cellular activation . the potential role of competitive interactions between the sensory and motor systems is discussed . 
using generation for grammar analysis and error_detection we demonstrate that the bidirectionality of deep grammars , allowing them to generate as well as parse sentences , can be used to automatically and effectively identify errors in the grammars . the system is tested on two implemented hpsg grammars : jacy for japanese , and the erg for english . using this system , we were able to increase generation coverage in jacy by 18% ( 45% to 63% ) with only four weeks of grammar development . 
simultaneous retrieval of sea_surface wind_speed and sea_surface_temperature from a multi-frequency scanning microwave_radiometer derivation of geophysical parameters from satellite measured brightness_temperature ( t b ) is an important aspect of satellite remote_sensing . primarily , this involves development of complex inversion algorithms and empirical relations comprising t b and in situ data for parameter retrieval and algorithm validation . in the present work , an artificial_neural_network model has been attempted to simultaneously obtain sea_surface wind_speed ( ws ) and sea_surface_temperature ( sst ) utilizing t b from 8 channels ( including vertical and horizontal polarizations ) of multi-frequency scanning microwave_radiometer on board indian remote_sensing satellite ( irs-p4 ) and deep_sea ocean buoys in the north_indian ocean region . the ann obtained values are then compared with actual in situ observations as a test for the performance of the model . it is concluded that the ann model is able to provide good estimates of ws and sst within acceptable error limits . the goal of the present work is to pre-establish the suitability of ann approach for geophysical parameter retrieval from satellite measured t b in the indian context particularly keeping in view the forth coming satellite launches like megha tropiques and oceansat-3 . 
novel hierarchical test architecture for soc test_methodology using ieee test standards soc test_methodology in ultra deep_sub_micron ( udsm ) technology with reasonable test time and cost has begun to satisfy high_quality and reliability of the product . a novel hierarchical test architecture using ieee_standard 1149 . 1 , 1149 . 7 and 1500 compliant facilities is proposed for the purpose of supporting flexible test environment to ensure soc test_methodology . each embedded core in a system-on-a-chip ( soc ) is controlled by test access ports ( tap ) and tap controller of ieee_standard 1149 . 1 as well as tested using ieee_standard 1500 . an soc device including taped cores is hierarchically organized by ieee_standard 1149 . 7 in wafer and chip level . as a result , it is possible to select/deselect all cores embedded in an soc flexibly and reduce test_cost dramatically using star scan topology . 
the cognitive representation of computer_supported instructional tools two studies are reported whose aim was to assess whether undergraduates' cognitive representations of the psychological correlates of computer_supported instructional tools ( csit ) vary according to the features of tools themselves . a questionnaire investigating participants' conceptions about motivational and emotional aspects of learning through csit , behavior during the learning_process , required capacities , mental operations , metacognition , preferred style of thinking , cognitive benefits and learning_outcomes was employed in both studies . in study 1 undergraduates were requested to judge to what extent such issues are involved in different kinds ( virtual simulations , web forums , and so on ) of csit , whereas in study 2 the same issues were considered by making reference to different dimensions ( hypertext , multimedia , and so forth ) of csit . results_showed that students have a well-defined and deep-rooted conception about what csit can introduce into a learning_process , by attributing different psychological correlates to different kinds of tools . however , undergraduates failed to recognize that distinct dimensions of csit involve different psychological correlates . implications of these findings for education and tool designing are discussed . from goals to effects : "objective" and "subjective" perspectives computer_supported instructional tools ( csit ) are often presented as instruments that teachers , trainers , and tutors might adopt in order to try to solve problems that they encounter in their job , particularly to lead trainees to overcome the difficulties that they experience in learning . the main question that educators ask when someone proposes to them to employ technological devices is : do this tool actually produce the expected outcomes ? ( giles , 2003 ) . a way to conceptualize this question is the following . there is a need that requires to be satisfied ( for example , rising the level of trainees' motivation ) or a goal to be reached ( for instance , improving students' understanding of a hard concept ) . instructors look for , or devise by themselves , a tool which should help students in satisfying that need or in achieving that goal . then they induce learners to use that tool . the outcomes produced by the tool are detected in order to test the alleged efficacy of the tool . such an approach stresses the importance of the characteristics of the tool to be employed , assuming that the higher is its quality , the better are the learning results . in this perspectives the way of employing a tool derives directly from the affordances of the tool : what was "put" into the tool ( provided that it was phenomenologically salient ) , that will be used properly by students , 
learning from text with diagrams : promoting mental_model development and inference generation learning with text and diagrams two experiments investigated learning_outcomes and comprehension processes when students learned about the heart and circulatory system using ( a ) text only , ( b ) text with simplified diagrams designed to highlight important structural relations , or ( c ) text with more detailed diagrams reflecting a more accurate representation . experiment 1 found that both types of diagrams supported mental_model development , but simplified diagrams best supported factual learning . experiment 2 replicated learning effects from experiment 1 and tested the influence of diagrams on novices' comprehension processes . protocol analyses indicated that both types of diagrams supported inference generation and reduced comprehension errors , but simplified diagrams most strongly supported information integration during learning . visual_representations appear to be most effective when they are designed to support the cognitive_processes necessary for deep comprehension . as multimedia technology becomes increasingly popular in formal and informal educational settings , the importance of research investigating learning with visual and verbal materials takes on added value . understanding the ways in which visual materials influence learning will be essential to developing multimedia tools with consistent and predictable benefits . advancing technology has meant that multimedia often now includes complex forms of interactive and computationally intensive presentations; however , multimedia can be more simply defined as any presentation that includes verbal and visual_information ( mayer , 2001 ) . in practice , basic types of multimedia such as pictures and text still appear to be frequently used . currently , many digital and print materials use pictures , diagrams , and text as their primary communication format . but how does the visual representation of information influence learning ? can changes in comprehension processes account for the impact of diagrams on learning ? the purpose of this research was to investigate potential effects of different diagram representations on students' learning_outcomes and comprehension processes when diagrams were added to a science text . early research on pictures and text consistently demonstrated that students learned more after reading illustrated versus nonil-lustrated text ( for a review , see levie & lentz , 1982 ) . these studies predominantly administered memory measures for the source materials , including multiple_choice and fill-in-the-blank tests . but a long history of cognitive research has distinguished between rote memorization and deeper understanding ( e . for a discussion ) . deeper learning evidenced by measures that assess application and transfer of information was not widely tested in early research on illustrations . however , one early set of studies ( see dwyer , 1967 , 1968 , 1975 ) did test both memory and deep comprehension for an illustrated text . dwyer ( 1967 , 1968 , 1975 ) found benefits 
gamma_ray irradiation effects on cmos image sensors in deep sub-micron technology introduction cmos active pixel sensors ( aps ) excel in domains that include low_power operation and on-chip integration of analog and digital circuitry . since these sensors are utilized for applications involving the detection of signals as low as a few electrons , radiation tolerance of such devices is of primary concern . all possible radiation effects are usually grouped into three basic types : transient effects ( not dealt in this study ) , ionization damage and displacement damages [1] , [2] , [6] . ionization damage has been considered to be the dominant mechanism when energetic photons ( and x-rays ) interact with solid_state matter . the major concerns due to this damage are charge build-up in the gate dielectric and radiation induced interface states . the introduction of discrete energy levels at the si-si0 2 interface leads to increased generation rates and thus higher surface leakage_currents . similarly , displacement of lattice atoms in the bulk leads to modified minority carrier lifetimes and increased bulk-generated leakage_currents [2] , [3] , [4] , [5] . experimental " pinned " cmos photodiodes ( fig . 1 ) utilize a p + pinning layer that shields the photodiode from surface effects that contribute to the leakage mechanism . the doping of the layers are chosen such that the photodiode is depleted completely . one of the most dominant dark current mechanisms in these structures are the defective sidewalls and the edges of shallow trench isolations ( stis ) separating the photodiodes [9] , [10] . to test the effects of radiation , test_structures with and without p-well protected stis ( shallow trench isolation ) were fabricated in philips' 0 . 18- m cmos_technology ( table . 1 ) . the gap between the sti and the photodiode is represented by the parameter nta . the structures were tested by irradiating them with -rays ( 1 . 17 mev , 1 . 33 mev ) ; dose rate of 75 . 9 gray/min . solving the continuity_equation of a usual p + /n photodiode derives an analytic model for the internal spectral response of pinned photodiodes . an equivalent diode reverse voltage v d , is used to represent the depleted diode . the contribution from the p-type epitaxial region is included for the contribution from carriers collected through diffusion . this model is used to estimate the optical degradation of the sensors due to irradiation . standard cmos_process parameters have been used for the simulation . the dispersive transport phenomenon in the sio 2 can be modeled on the concept of small polaron hopping , called as ctrw ( continuous-time random_walk ) . the transport process varies with the fourth power of the oxide 
randomness and non-determinism exponentiation makes the difference between the bit-size of this line and the number ( 2 300 ) of particles in the known universe . the expulsion of exponential time algorithms from computer theory in the 60's broke its umbilical_cord from mathematical_logic . it created a deep gap between deterministic computation and formerly its unremarkable tools randomness and non-determinism . little did we learn in the past decades about the power of either of these two basic " freedoms " of computation , but some vague pattern is emerging in relationships between them . the pattern of similar techniques instrumental for quite different results in this area seems even more interesting . ideas like multilinear and low-degree multivariate polynomials , fourier transformation over low-periodic groups seem very illuminating . the talk surveyed some recent results . one of them , given in a stronger form than previously_published , is described below . |x| will denote the length of string x . let p be the set of fast , i . e . computable in time t f ( x ) = |x| o ( 1 ) , algorithms f ( x ) on binary strings . [blum micali 82 , yao 82] proposed a fast deterministic way to generate " nearly perfect " randomness , using the idea of a hard core or hidden bit . they assume certain length preserving functions f p to be one-way ( owf ) , i . e . infeasible to invert ( a non-deterministically easy task ) . suppose it is hard to compute from f ( x ) not only x but even its one bit b ( x ) { 1} , b p . moreover , assume that even guessing b ( x ) with any noticeable correlation is infeasible . if f is bijective , f ( x ) and b ( x ) are both random and appear to be independent to any feasible test , thus increasing the initial amount |x| of randomness by one bit . then , a short random seed x can be transformed into an arbitrary long string ( 1 ) , ( 2 ) , . . . : ( i ) = b ( f ( i ) ( x ) ) . such passes any feasible randomness test . [goldreich levin 89] showed that every owf f has such a hidden bit with security of f and b polynomially related . it also gives more details on the definitions below . here this result is strengthened to yield the same security for f and b . let p be the set of probabilistic algorithms a ( x , ) using coin-flips {0 , 1} i n and running in average over time e t a ( x , ) = |x| o ( 1 ) . an inverter i p for f 
fabrication and reliability testing of copper-filled through-silicon vias for three-dimensional chip stacking applications all rights reserved ii abstract through-silicon vias ( tsvs ) have been extensively studied because of their ability to achieve chip stacking for enhanced system performance . the fabrication process is becoming somewhat mature . however , reliability issues need to be addressed in order for an eventual transition from laboratory to production . this dissertation discusses the tsv fabrication_process , testing results for tsv reliability investigation of an integration of tsvs and capacitor devices . in our laboratory , vias with tapered sidewalls are formed through a modified bosch process using deep_reactive_ion_etching ( drie ) . cryogenic etching is also considered as a means to etch vias without sidewall scalloping that is observed for the bosch process . vias are lined with silicon_dioxide using plasma enhanced chemical_vapor_deposition ( pecvd ) followed by a sputter deposited titanium barrier and a copper seed layer before filling by a reverse pulse copper electroplating process . following attachment of the process wafer to a carrier wafer , the process wafer is thinned from the backside by a combination of mechanical methods and reactive_ion_etching ( rie ) . fabricated vias are subjected to thermal cycling with temperatures ranging from-25 c to 125 c . tsvs are shown to be stable with small increases in measured resistance for 200 cycles . in addition , small changes in resistance are observed when vias are held at elevated temperatures for extended periods of time . integration of decoupling capacitors with tsvs represents a good alternative to conventional 2-d layouts to achieve miniaturization and increased density . therefore , decoupling capacitors can be brought in close proximity to the active elements , thereby , reducing iii their parasitic inductance and allowing higher clock rates . in this study , capacitors with anodized tantalum as the dielectric are integrated with tsvs without negatively impacting their operation . the performance of these capacitors was evaluated by measuring resonant_frequency , parasitic inductance , and parasitic resistance . iv dedication to my lovely wife , dr . yolande konguep tchouangue . to my late parents marie madeleine and emmanuel tegueu . 
multielectrode microprobes for deep_brain_stimulation fabricated with a customizable 3-d electroplating process although deep_brain_stimulation ( dbs ) can be used to improve some of the severe symptoms of parkinson's_disease ( e . g . , bradykinesia , rigidity , and tremors ) , the mechanisms by which the symptoms are eliminated are not well understood . moreover , dbs does not prevent neurodegeneration that leads to dementia or death . in order to fully investigate dbs and to optimize its use , a comprehensive long_term stimulation study in an animal_model is needed . however , since the brain region that must be stimulated , known as the subthalamic nucleus ( stn ) , is extremely small ( 500 microm x_500 microm x_1 mm ) and deep within the rat brain ( 10 mm ) , the stimulating probe must have geometric and mechanical_properties that allow accurate positioning in the brain , while minimizing tissue damage . we have designed , fabricated , and tested a novel micromachined probe that is able to accurately stimulate the stn . the probe is designed to minimize damage to the surrounding tissue . the probe shank is coated with gold and the electrode interconnects are insulated with silicon_nitride for biocompatibility . the probe has four platinum electrodes to provide a variety of spatially distributed stimuli , and is formed in a novel 3-d plating process that results in a microwire like geometry ( i . e . , smoothly tapering diameter ) with a corresponding mechanically stable shank . 
on a_viterbi_decoder design for low power_dissipation on a_viterbi_decoder design for low power_dissipation ( abstract ) convolutinal coding is a coding scheme often employed in deep_space communications and recently in digital wireless_communications . viterbi decoders are used to decode convolutional codes . viterbi decoders employed in digital wireless_communications are complex and dissipate large power . with the proliferation of battery powered devices such as cellular_phones and laptop computers , power_dissipation , along with speed and area , is a major concern in vlsi_design . in this thesis , we investigated a low_power_design of viterbi decoders for wireless_communications applications . in cmos_technology the major source of power_dissipation is attributed to dynamic power_dissipation , which is due to the switching of signal values . the focus of our research in the low_power_design of viterbi decoders is reduction of dynamic power_dissipation at logic level in the standard_cell design environment . we considered two methods , clock-gating and toggle-filtering , in our design . a_viterbi_decoder consists of five blocks . the clock-gating was applied to the survivor path storage block and the toggle-filtering to the trace-back block of a_viterbi_decoder . we followed the standard_cell design approach to implement the design . the behavior of a_viterbi_decoder was described in vhdl , and then the vhdl description was modified to embed the low_power_design . a gate_level circuit was obtained from the behavioral description through logic_synthesis , and a full scan design was incorporated into the gate level circuit to ease testing . the gate level circuit was placed and routed to generate a layout of the design . our experimental result shows the proposed design reduces the power_dissipation of a_viterbi_decoder by about 42 percent compared with the on without considering the low_power_design . iii acknowledgements special thanks are due to my committee chairman and advisor dr dong s . ha . it was through his patience , understanding and invaluable guidance that this work was accomplished . i would also like to express my appreciation for my committee members dr nathaniel j . davis iv and dr james r . armstrong for serving as my committee members and commenting on this work . i am extremely grateful for the visc computing resources without which none of this work would have been possible . i am greatly indebted to dr . will ebel ( visiting faculty ) , for his expert advice and corrections on this work . i would also like to first thank han bin and then to all other friends for their advise , guidance and help . my parents 
threshold_voltage and power_supply tolerance of cmos logic design families the advent of deep_submicron_technologies brings new challenges to digital_circuit design . a reduced threshold_voltage ( vt ) and power_supply ( vdd ) in addition to process variabilities have a direct impact on circuit_design . in a semiconductor environment it is conventionally thought that parametric yield is high and stable and that the main yield losses are functional . although functional yield remains the main focus of attention , modern and future circuits may not have the presumed high parametric yield . we present a study that compares the tolerance to process variability of various design families for metrics including timing and power_consumption under v , vdd scalability using a nand_gate as a test vehicle . basically , the fundamental limitations to the scaling of the supply_voltage due to the statistical variation of mos v , are investigated and defined . the four logic families under study are : static cmos , differential complementary voltage swing logic ( dcvsl ) , domino and pass logic . 1 . introduction the problem of threshold_voltage variability is considered to be one of the most serious concerns in future gigabit scale vlsi and ulsi designs[1-8] . however , until now there have been limited studies of performance fluctuations of digital_circuits in terms of the statistical fluctuation of the mosfet threshold_voltage ( v t ) [9] . with the non-scalability of v t , due to increase in leakage power_dissipation , circuit_design in the deep sub-micron regime becomes a major challenge . 
3d heterogeneous system integration : application driver for 3d technology_development three dimensional integration complements semiconductor scaling; it enables a higher integration density as well as heterogeneous technology integration . using 3d chip stacking , it is possible to extend the number of functions per 3d chip well beyond the near-term capabilities of traditional scaling . the 3d strata may be realized using advanced cmos_technology nodes but may also exploit a wide variety of device technologies to optimize system performance . 3d of electronic circuits has become a field of great interest in the microelectronics community . a wide variety of approaches are being proposed for an equally wide variety of applications . these approaches can be categorized based on their intersection with the interconnect hierarchy on chip and in the package : from local-to-global interconnect , to bond-pad-level , package and board level . this paper will focus on 3d interconnects using through-si-vias that allow interconnectivity at the on-chip global interconnect level . the physical dimensions of these tsvs are 5&mu;m diameter and are 50&mu;m deep and can be placed at a minimum pitch of 10&mu;m . the study and development of 3d system integration requires a concurrent exploration of technology and design . this is particularly the case when considering heterogeneous systems , exploiting different technologies that cannot be integrated in a single ic process . 3d-technology with high_density tsv's allow for the seamless integration of circuit blocks ( ip_blocks ) of different technologies in the same manner as a traditional soc architecture , resulting in a 3d-soc implementation : concurrently designed ic's in heterogeneous technologies . the 3d integration approach should be cost_effective and consider the potential compound yield risks by adopting the appropriate testing and known-good-die strategies . a path finding design_flow for 3d has been implemented that enables the study the system-level trade_offs at an early phase of the system design . in this path finding design_flow , the physical_characteristics of a 3d stacked circuit implementation are accounted for . this includes detailed electrical models for the tsv and &mu;bump interconnections , compact mechanical models that account for the impact of mechanical stresses inducted buy the tsv on neighboring components , mechanical stresses imposed by the package and compact thermal models that allow for a fast estimation of chip temperatures across the die within a 3d-stack . 
markovian channel modeling for dvb_rcs satellite systems based on observation window concept the utilization of the return channel satellite ( rcs ) in dvb systems allows a bi-directional communication via satellite . with this particular characteristic , dvb_rcs systems promoted its enormous diffusion in the commercial area and the academic research interest and , in this context , it is very important to test dvb_rcs systems using an efficient satellite_channel_model . many channel models have been proposed in the literature , but most of them work at the bit_level or they investigate only some aspects of channel interaction . in this paper we present a high level satellite_channel_model based on markov_chains ( mcs ) modeling , useful in every simulation context : the proposed markovian satellite_channel_model ( mscm ) is based on the concept of windowed observations and the idea is not to analyze a single packet , but fixing an observation window to evaluate the degradation level of the link , computing the per associated to the specific window . the model has been verified through a deep campaign of simulations , which demonstrate how the introduced error is minimized . 
testing network_on_chip communication fabrics network_on_chip ( noc ) communication fabrics will be increasingly used in many large multicore system-on-chip designs in the near future . a relevant challenge that arises from this trend is that the test costs associated with noc infrastruc-tures may account for a significant part of the total test budget . in this paper , we present a novel methodology for testing such noc architectures . the proposed methodology offers a tradeoff between test time and on-chip self-test resources . the fault_models used are specific to deep submicrometer technologies and account for crosstalk_effects due to interwire coupling . the novelty of our approach lies in the progressive reuse of the noc infrastructure to transport test_data to the components under test in a recursive manner . it exploits the inherent parallelism of the data transport mechanism to reduce the test time and , implicitly , the test cost . we also describe a suitable test_scheduling approach . in this manner , the test_methodology developed in this paper is able to reduce the test time significantly as compared to previously_proposed solutions , offering speedup factors ranging from 2 to 34 for the nocs considered for experimental_evaluation . 
number recognition_system using electroencephalogram ( eeg ) signals this paper focuses on number recognition from feature extracted using electroencephalogram ( eeg ) readings . eeg_signals were from 6 volunteer subjects . a random_number_generator graphics user_interface was developed in vb7 . it is used to display numbers from 0 to 9 which worked as visually evoked_potential ( vep ) for the experiment . the database of 6 male right-handed subjects in the age group of ( 20-25 ) was created and used as training data_set . by exposing the same set of subjects to the gui again , new eeg recordings were collected . this new set of eeg readings was considered as testing data_set . the testing data was searched and matched with trained data_set for recognizing pattern of each number . the experiments were conducted by concentrating on beta signal and linear discriminate analysis ( lda ) was implemented to classify the data . the recognition_rate observed was different for different numbers . overall recognition_rate observed was 68 . 33% . it is also seen that there exist a unique pattern for each number . introduction functional brain_imaging techniques that are designed to measure an aspect of brain_function can be employed to obtain tangible information related to brain activity . eeg is one such technique which measures the electric_fields that are produced by the activity in the brain [1 , 2] . eeg_signals arise due to electrical_potential produced by the brain . mostly eeg analysis has been used clinically for pathologies and epilepsies since hans berger's recordings of electrical potentials from the human scalp . more recently new interaction techniques which directly connect a human_brain and a machine are in use . eeg spectrum contain characteristic waveforms which fall in 4 frequency bands viz alpha ( 8-13 hz ) , beta ( 13-30 hz ) , theta ( 4-8 hz ) , delta ( < than 4 hz ) . alpha waves are found in normal awake people , not engaged in intense mental activity , which disappear when a person is asleep . beta waves with higher frequency are seen during intense mental activity and stress . delta waves occur during deep_sleep , during infancy and in serious organic brain diseases . theta waves appear during emotional stress in adults in sleep , particularly during disappoint
challenges and methods in testing the_remote_agent planner the_remote_agent experiment ( rax ) on the deep_space_1 ( ds1 ) mission was the first time that an artificially_intelligent agent controlled a nasa spacecraft . one of the key components of the_remote_agent is an on-board planner . since there was no opportunity for human intervention between plan generation and execution , extensive testing was required to ensure that the planner would not endanger the spacecraft by producing an incorrect plan , or by not producing a plan at all . the testing_process raised many challenging issues , several of which remain open . the planner and domain_model are complex , with billions of possible inputs and outputs . how does one obtain adequate coverage with a reasonable number of test_cases ? how does one even measure coverage for a planner ? how does one determine plan correctness ? other issues arise from developing a planner in the context of a larger operations-oriented project , such as limited workforce and changing domain models , interfaces and requirements . as planning systems are fielded in mission-critical applications , it becomes increasingly important to address these issues . this paper describes the major issues that we encountered while testing the_remote_agent planner , how we addressed them , and what issues remain open . 
discrepant esd-cdm test system and failure yield prediction between esd association and jedec standards cdm test system and verification method of esda and jedec standards have been studied . there are several different items . they can be categorized into 5 major items , which are charging system , discharging system , verification module , waveform verification , and classification level . regarding waveform verifications at each stress level , esda system provides higher peak current whereas lower rise time and lower full width at half maximum , compared to jedec system . it implies that esda standard provides higher inductance in a discharge system and higher discharge energy , which make it more severe system . the current continuously increases with the stress level . the linear relationship of stress_conditions by these standards can be obviously observed . the electrical failure yield of each standard system is then predicted by a stress condition of the other system . introduction along with the development of technology , modern electronic systems become largely integrated , so they have become more and more sensitive to electrostatic_discharge ( esd ) . this phenomenon has turned to be a serious problem for ic products fabricated by deep_submicron cmos_technology although design effort and awareness are significantly improved [1-3] . it impacts the functionality , reliability and lifetime of ics [4] . therefore , eos/esd test is required to qualify products based on customer expectations to minimize failures due to esd from human and mechanical handling . esd events have been divided into 4 models , which are human_body model ( hbm ) , machine model ( mm ) , charge device model ( cdm ) , and socket device model ( sdm ) . regarding the increase of automated component handling systems , cdm becomes a significantly test for microelectronic components . it is performed to classify the susceptibility of a device to determine its esd withstand voltage to such esd events . this cdm test simulates that the device itself becomes charge and is rapidly discharged when it approaches a conductive object . the rapid transfer of an electrical_charge causes most of the esd damages in the electronic manufacturing . to predict and qualify the esd immunity level , there are several organizations that make the esd related primary standards . they are electrostatic_discharge association ( esda ) , automotive electronics council ( aec ) , electronic_industries_alliance/joint electron device engineering_council ( eia/jedec ) and us military standard ( mil-std ) . the commonly used standards are released by esda and jedec . they have made their own definitive stipulations on test_methods and instruments . with the different specification , some unavoidable questions , such as the similarity of the test system verification , occur . in this paper , we present the
two-dimensional capacitive micromachined ultrasonic transducer ( cmut ) arrays for a miniature integrated volumetric ultrasonic imaging system we have designed , fabricated , and characterized two-dimensional 16x16-element capacitive micromachined ultrasonic transducer ( cmut ) arrays . the cmut array elements have a 250- m pitch , and when tested in immersion , have a 5-mhz center frequency and 99% fractional bandwidth . the fabrication process is based on standard silicon micromachining techniques and therefore has the advantages of high yield , low_cost , and ease of integration . the transducers have a si 3 n 4 membrane and are fabricated on a 400- m thick silicon substrate . a low parasitic_capacitance through-wafer via connects each cmut element to a flip_chip bond pad on the back side of the wafer . each through-wafer via is 20 m in diameter and 400 m deep . the interconnects form metal-insulator-semiconductor ( mis ) junctions with the surrounding high-resistivity silicon substrate to establish isolation and to reduce parasitic_capacitance . each through-wafer via has less than 0 . 06 pf of parasitic_capacitance . we have investigated a au-in flip_chip bonding process to connect the 2d cmut array to a custom integrated_circuit ( ic ) with transmit and receive electronics . to develop this process , we fabricated fanout structures on silicon , and flip_chip bonded these test dies to a flat surface coated with gold . the average series resistance per bump is about 3 ohms , and 100% yield is obtained for a total of 30 bumps . 
deep_web collection selection the author hereby grants permission to the queensland_university_of_technology , brisbane to reproduce and distribute publicly paper and electronic copies of this thesis document in whole or in part . abstract the deep web contains a massive number of collections that are mostly invisible to search_engines . these collections often contain high_quality , structured information that cannot be crawled using traditional methods . an important problem is selecting which of these collections to search . automatic collection selection methods try to solve this problem by suggesting the best subset of deep_web collections to search based on a query . a few methods for deep_web collection selection have proposed in collection retrieval inference network system and glossary of servers server system . the drawback in these methods is that they require communication between the search broker and the collections , and need metadata about each collection . this thesis compares three different sampling methods that do not require communication with the broker or metadata about each collection . it also transforms some traditional information_retrieval based_techniques to this area . in addition , the thesis tests these techniques using inex collection for total 18 collections ( including 12232 xml_documents ) and total 36 queries . the experiment shows that the performance of sample-based technique is satisfactory in average . 
learning quadrotor dynamics using neural_network for flight control traditional learning_approaches proposed for controlling quadrotors or helicopters have focused on improving performance for specific trajectories by iteratively improving upon a nominal controller , for example learning from demonstrations , iterative learning , and reinforcement_learning . in these schemes , however , it is not clear how the information gathered from the training trajectories can be used to synthesize controllers for more general trajectories . recently , the efficacy of deep_learning in inferring helicopter dynamics has been shown . motivated by the generalization capability of deep_learning , this paper investigates whether a neural_network_based dynamics model can be employed to synthesize control for trajectories different than those used for training . to test this , we learn a quadrotor dynamics model using only translational and only rotational training trajectories , each of which can be controlled independently , and then use it to simultaneously control the yaw and position of a quadrotor , which is non-trivial because of nonlinear couplings between the two motions . we validate our approach in experiments on a quadrotor testbed . 
polarization lidar for shallow water_depth measurement . a bathymetric , polarization lidar system transmitting at 532 nm and using a single photomultiplier_tube is employed for applications of shallow water_depth measurement . the technique exploits polarization attributes of the probed water body to isolate surface and floor returns , enabling constant fraction detection schemes to determine depth . the minimum resolvable water_depth is no longer dictated by the system's laser or detector pulse width and can achieve better than 1 order_of_magnitude improvement over current water_depth determination techniques . in laboratory tests , an nd : yag microchip laser coupled with polarization optics , a photomultiplier_tube , a constant fraction discriminator , and a time-to-digital converter are used to target various water depths with an ice floor to simulate a glacial meltpond . measurement of 1 cm water depths with an uncertainty of 3 mm are demonstrated using the technique . this novel approach enables new approaches to designing laser bathymetry systems for shallow depth determination from remote platforms while not compromising deep water_depth measurement . 
automation in design_for_test for asynchronous null conventional logic ( ncl ) circuits 1 . 0 introduction the semiconductor era has been thriving since the past four decades but as we continually attain smaller chip sizes comprising of millions of transistors , the problems grow at an unmitigated speed too . testing such huge circuits poses a huge problem unless tackled prudently . the best case scenario given the current circumstances would be to create a favorable test environment on-chip by implementing design_for_test ( dft ) techniques . asynchronous digital_design methodologies are currently gaining popularity since they enjoy the benefits of reduced area , power and low emi . in spite of several advantages , due to the absence of the global clock and the presence of more state holding gates , testing these circuits presents a challenge to the designer . a dft implementation for one such asynchronous class known as null conventional logic ( ncl ) circuits has been proposed in this paper . the methodology discussed , exploits the technique of test points' insertion in order to improve the controllability of difficult feedback paths . enhanced observability of long paths in the circuits is achieved by introducing of balanced tree structures or scan latches . this approach has been automated and works with industry standard tool suites , such as mentor_graphics and synopsis . the implemented tool transforms gate_level ncl descriptions to those understood by the atpg library provided with the dft cad tools . a test_coverage increase of around 75% has been achieved with a less than 5% increase in terms of cost and area . the digital world has been dominated by the growth of synchronous techniques for nearly four decades due only to the ease of design of these circuits . also , cad tools for synchronous_designs have become more advanced and sophisticated allowing total automation of several stages of the design process . however , with clock speeds nearing the gigahertz range and cmos_technology reaching deep_submicron ranges , serious doubts have been cast over the suitability of synchronous_designs for next_generation processors and systems . problems associated with clock synchronization , power_consumption , and noise in synchronous_designs has forced designers to look for alternatives [1] . designers are looking at asynchronous_circuits as a potential solution to these problems as they are modular and do not require clock synchronization . some of the possible benefits of asynchronous techniques include low_power , less emi , less noise , increased robustness and design_reuse [2] . a variety of approaches exist for the design and implementation of asynchronous_circuits . huffman's model and muller's model form 
improving robustness against reverberation for automatic_speech_recognition reverberation is a phenomenon observed in almost all enclosed environments . human listeners rarely experience problems in comprehending speech in reverberant environments , but automatic_speech_recognition ( asr ) systems often suffer increased error_rates under such conditions . in this work , we explore the role of robust acoustic features motivated by human speech_perception studies , for building asr systems robust to reverberation effects . using the dataset distributed for the " automatic_speech_recognition in reverberant environments " ( aspire-2015 ) challenge organized by iarpa , we explore gaussian mixture_models ( gmms ) , deep_neural_nets ( dnns ) and convolutional deep_neural_networks ( cdnn ) as candidate acoustic_models for recognizing continuous_speech in reverberant environments . we demonstrate that dnn-based systems trained with robust features offer significant reduction in word error_rates ( wers ) compared to systems trained with baseline mel-filterbank features . we present a novel time-frequency convolution neural net ( tfcnn ) framework that performs convolution on the feature_space across both the time and frequency scales , which we found to consistently outperform the cdnn systems for all feature sets across all testing conditions . finally , we show that further wer reduction is achievable through system fusion of n-best lists from multiple systems . 
advances in supply_chain_management : potential to improve forecasting accuracy advances in supply_chain_management decision_support_systems : potential to improve forecasting accuracy 3 . statistical_model : var-mgarch working paper series abstract forecasting is a necessity almost in any operation . however , the tools of forecasting are still primitive in view of the great strides made by research and the increasing abundance of data made possible by automatic identification technologies , such as , rfid . the relationship of various parameters that may change and impact decisions are so abundant that any credible attempt to drive meaningful associations are in demand to deliver the value from acquired data . this paper proposes some modifications to adapt an advanced forecasting technique ( garch ) with the aim to develop it as a decision_support tool applicable to a wide variety of operations including supply_chain_management . we have made an attempt to coalesce a few different ideas toward a " solutions " approach aimed to model volatility and in the process , perhaps , better manage risk . it is possible that industry , governments , corporations , businesses , security organizations , consulting firms and academics with deep_knowledge in one or more fields , may spend the next few decades striving to synthesize one or more models of effective modus_operandi to combine these ideas with other emerging concepts , tools , technologies and standards to collectively better understand , analyze and respond to uncertainty . however , the inclination to reject deep rooted ideas based on inconclusive results from pilot projects are a detrimental trend and begs to ask the question whether one can aspire to build an elephant using mouse as a model . forecasting is an ancient activity and has become more sophisticated in recent years . for a long time steady steps in a time_series data_set , such as simple trends or cycles ( such as seasonals ) were observed and extended into the future . however , now a mixture of time_series , econometrics and economic_theory models can be employed to produce several forecasts which can then be interpreted jointly or combined in sensible fashions to give a superior value . the variable being forecast is a random_variable . originally attention was largely directed towards the mean of this variable; later to the variance , and now to the whole marginal_distribution . pre-testing of the data to find its essential features has become important and that has produced modern techniques such as cointegration . the horizon over which the forecast is attempted is also important , and longer-run forecasts are now being considered as well as forecasts of "breaks" in the series . the question of evaluation of forecasts has also been 
reverse_engineering of protocols from network traces communication protocols determine how network components interact with each other . therefore , the ability to derive a specification of a protocol can be useful in various contexts , such as to support deeper black-box testing or effective defense mechanisms . unfortunately , it is often hard to obtain the specification because systems implement closed ( i . e . , undocumented ) protocols , or because a time consuming translation has to be performed , from the textual description of the protocol to a format readable by the tools . to address these issues , we propose a new methodology to automatically infer a specification of a protocol from network traces , which generates automata for the protocol language and state_machine . since our solution only resorts to interaction samples of the protocol , it is well-suited to uncover the message formats and protocol states of closed protocols and also to automate most of the process of specifying open protocols . the approach was implemented in a tool and experimentally evaluated with publicly available ftp traces . our results show that the inferred specification is a good approximation of the reference specification , exhibiting a high level of precision and recall . i . introduction network protocols regulate the communication among entities by defining the syntax and semantics of the messages , and the order in which they need to be exchanged . the ability to obtain a protocol specification can , therefore , play an important role in several contexts . for example , it can help on the implementation of effective defense mechanisms , such as firewalls and intrusion_detection systems , that use the specifications of the protocols to accurately identify malicious traffic by performing deep_packet_inspection [1] . testing tools can take as input a protocol specification to generate test_cases that cover the protocol space for conformance testing [2] or to verify if a server is vulnerable to remote attacks [3] . however , it is typically hard to produce protocol specifications . closed ( or undocumented ) protocols require reverse_engineering the seemingly arbitrary set of bytes that compose each message in order to determine their meaning and structure . open protocols , on the other hand , are well documented and their ( textual ) specification is readily available ( e . g . , ietf protocols ) , but obtaining their specification is also difficult and time consuming because developers have to carefully analyze the textual description of the protocol and translate it into the format supported by the tools . automatic protocol reverse_engineering can address most of these difficulties by deducing an approximate specification of a protocol 
targeting leakage constraints during atpg in previous technology generations iddq_testing used to be a powerful technique to detect physical faults that are not covered by standard fault_models or functional tests . due to shrinking feature_sizes and consequently increasing leakage_currents iddq_testing becomes difficult in the deep-sub-micron area . one of the problems is the vector dependency of leakage_current . even in good devices the leakage_current may vary significantly from one test_vector to the next . in this work we present an atpg framework that allows to generate test_vectors within tight constraints on leakage_currents . the target range for the leakage_current is automatically determined . experiments on the itc99 benchmark suite yield testsets that achieve 100% fault_coverage for the larger circuits , even when the range is narrowed down to 50% of the standard deviation of random vectors . 
deep slab instability , decision_making , and consequences : a case study on march 8 th , 2008 an avalanche occurred on mt . eyak , near cordova , alaska . it resulted in the death of an avalanche forecaster , and the broken femur of another person . through a summary of weather observations and snow profile data taken in the same area for weeks prior , this case_study explores many factors associated with this event , including deep slab instabilities , stability tests , strain softening , triggering mechanisms , human_factors , decision_making , and consequences . 
eeg and bold_contrast_fmri in brain eeg and bold_contrast_fmri in brain acknowledgements cerebrovascular reactivity , suppression of neuronal activity , global_and_local brain_injury cerebrovascular reactivity , suppression of neuronal activity , global_and_local brain_injury academic dissertation to be presented with the assent of abstract the purpose of the present study was to gain more insight into the blood oxygen level-dependent ( bold ) -contrast functional mri ( fmri ) in the brain and its connection to eeg , both in global_and_local scales of their temporal and spatial relations . bold_signal_changes were studied during hyperventilation ( hv ) induced eeg reactivity of intermittent rhythmic delta activity ( irda ) . the bold_signal in gray_matter decreased 30% more in subjects with irda ( n = 4 ) than in controls ( n = 4 ) , during the first two minutes of hv . this difference disappeared during irda in eeg . bold_signal_changes may provide additional information about dynamic hemodynamic changes relative to hv induced eeg reactivity . bold_signal_changes were investigated during sudden deepening of thiopental anesthesia into eeg burst-suppression level in pigs ( n = 5 ) . positive ( 6 8% ) or negative ( -3 -8% ) group average bold_signal_changes correlated to the thiopental bolus injection were seen . positive and negative responses covered 1 . 6% and 2 . 3% of the brain voxels , respectively . bold_signal_changes in brain are associated with sudden deepening of thiopental anesthesia into eeg burst-suppression level , but they are spatially inconsistent and scarce . somatosensory bold response was studied in brain before and after globally induced methotrexate ( mtx ) exposition in pigs ( n = 4 ) . after the mtx exposure , reduced ( from 2 4% to 0 1% ) or negative ( -2% to-3% ) bold responses were detected . somatosensory bold_contrast response shows a slight difference in brain before and after globally induced mtx exposition . an experimental epilepsy model for development of simultaneous eeg and bold_contrast_fmri in the localization of epilepsy was developed and tested . dynamic penicillin induced local epilepsy was applied in deep isoflurane anesthesia in pigs ( n = 6 ) . relatively high ( 10 20% ) and localized bold_signal increase was found . the dynamic penicillin induced focal epilepsy model in deep isoflurane anesthesia with simultaneous eeg and bold_contrast_fmri is feasible for the development of these methods for localization of epileptic focus or foci . in conclusion , with careful experimental design and analysis , bold_contrast_fmri with eeg provides a potential tool for monitoring and localising functional changes in the brain . when you meet a master swordsman , show him your sword . when you meet a man who is not a poet , do not show him your poem . -rinzai
sram delay_fault modeling and test algorithm development with the advent of deep-submicron vlsi technologies , the working speed of sram circuits has grown to a_level that at-speed testing of sram has become an important_issue . in this paper , we present delay_fault models for sram , i . e . , the faults that affect the access time of the sram circuit . we also develop the test algorithm that detects these faults . the proposed sram delay_fault_test algorithm has a complexity of read/write operations , where is the number of words and is the word count in a row . 
biologically inspired kflann place fields for robot_localization this paper presents a hippocampal inspired robot_localization model that provides a means for a simple robotic platform with ultrasonic sensors to localize itself . there have been published neurobiological experiments where rats were found to have hippocampal cell activations that positively correlate with the location of the animal [2 , 3 , 5] . such activations found in the hippocamal region are usually called place fields ( pf ) or place cells ( pc ) . the place field model presented in this paper was designed using a unique k-means fast learning artificial_neural_network ( kflann ) [13 , 14 , 15] and establishes a series of localization minima points that act as references for navigation . while such evidence of place cells are seen in hippocampal ( ca1 ) and deep layers of the entorhinal_cortex ( ec ) [4] , from a literature search , it is uncertain if any applications were ever designed using this biological evidence . the intent of this paper is to focus on experimental_results relevant for a proof-of-concept of robot_localization , rather than illustrating a robustly tested navigation system . as such , basic ultrasonic based experiments will suffice . with some experimental_results , we show that the kflann is suitable for implementing atomic place field vectors ( apfv ) , a data_structure to encapsulate localization information . autonomous unmanned navigational systems have been dependent on localization sensors as a primary means for navigation . in outdoor autonomous systems such as those seen in the darpa_grand_challenge , most vehicles would make use of combinations of gps , radar , lidar and ins [6] . there has also been an increasing amount of research in using only vision to navigate autonomous platforms through spaces such as laboratories and corridors [5] . while these have provided a_level of success , the extent of cognitive robotic navigation is far from being solved as a single failure in localization sensors usually leads to a catastrophic system failure . in the midst of man's quest to create robust autonomous robotic systems that can navigate effortlessly through clutter and uncertain environments , laboratory mice used in experiments are revealing secrets of their navigational capabilities . perhaps one of the differences in mammalian navigation as compared with robotic navigation is the dependence in cognitive capabilities as opposed to the heavy reliance on sensor technology . technological advances need to be complemented with advances in aspects of cognitive processings . we present a method where localization of a robot within a structured space is done using ultrasound information and a compass direction . the 
parkinson's_disease tremor-related metabolic network : characterization , progression , and treatment effects the circuit changes that mediate parkinsonian tremor , while likely differing from those underlying akinesia and rigidity , are not precisely known . in this study , to identify a specific metabolic brain network associated with this disease manifestation , we used fdg pet to scan nine tremor dominant parkinson's_disease ( pd ) patients at baseline and during ventral intermediate ( vim ) thalamic nucleus deep_brain_stimulation ( dbs ) . ordinal trends canonical variates analysis ( ort/cva ) was performed on the within-subject scan data to detect a significant spatial covariance pattern with consistent changes in subject expression during stimulation-mediated tremor suppression . the metabolic pattern was characterized by covarying increases in the activity of the cerebellum/dentate_nucleus and primary_motor_cortex , and , to a less degree , the caudate/putamen . vim stimulation resulted in consistent reductions in pattern expression ( p<0 . 005 , permutation test ) . in the absence of stimulation , pattern expression values ( subject scores ) correlated significantly ( r=0 . 85 , p<0 . 02 ) with concurrent accelerometric measurements of tremor amplitude . to validate this spatial covariance pattern as an objective network biomarker of pd tremor , we prospectively quantified its expression on an individual subject basis in independent pd populations . the resulting subject scores for this pd tremor-related pattern ( pdtp ) were found to exhibit : ( 1 ) excellent test-retest reproducibility ( p<0 . 0001 ) ; ( 2 ) significant_correlation with independent clinical ratings of tremor ( r=0 . 54 , p<0 . 001 ) but not akinesia-rigidity; and ( 3 ) significant elevations ( p<0 . 02 ) in tremor dominant relative to atremulous pd_patients . following validation , we assessed the natural_history of pdtp_expression in early stage patients scanned longitudinally with fdg pet over a 4-year interval . significant increases in pdtp_expression ( p<0 . 01 ) were evident in this cohort over time; rate of progression , however , was slower than for the pd-related akinesia/rigidity pattern ( pdrp ) . we also determined whether pdtp_expression is modulated by interventions specifically directed at parkinsonian tremor . while vim dbs was associated with changes in pdtp ( p<0 . 001 ) but not pdrp expression , subthalamic_nucleus ( stn ) dbs reduced the activity of both networks ( p<0 . 05 ) . pdtp_expression was suppressed more by vim than by stn stimulation ( p<0 . 05 ) . these findings_suggest that parkinsonian tremor is mediated by a distinct metabolic network involving primarily cerebello-thalamo-cortical pathways . indeed , effective treatment of this symptom is associated with significant reduction in pdtp_expression . quantification of treatment-mediated changes in both pdtp and pdrp scores can provide an objective means of evaluating the differential effects of novel antiparkinsonian interventions on the different motor features of the disorder . 
supporting user extensibility in a networked virtual . . . c-visions is a networked multiuser 3d virtual_environment that allows users to interact with each other and learn experientially and collaboratively in simulation and articulation-oriented virtual_worlds . the capabilities of virtual_worlds can be more fully exploited if the freedom to create new worlds is given to the users . this paper describes the development of a graphical user interface ( gui ) that allows users to do this . in the context of our research , the intended users are mainly secondary_school students currently in the process of acquiring new scientific_knowledge . in a virtual_world scenario where the virtual_environment mimics the real_world environment in terms of physics phenomena , users are given a free hand in constructing science experiments in whatever ways they wish . such a learning_environment allows users to freely construct and test their personal scientific hypotheses . by relying on graphical tools that aid the visualization of experiment_results , the learning_process is further facilitated and the learning_environment can help students overcome misconceptions and enhance deep understanding of scientific principles . 
pap smear image_classification using convolutional_neural_network this article presents the result of a comprehensive study on deep_learning based computer_aided diagnostic techniques for classification of cervical dysplasia using pap smear images . all the experiments are performed on a real indigenous image_database containing 1611 images , generated at two diagnostic centres . focus is given on constructing an effective feature vector which can perform multiple level of representation of the features hidden in a pap smear image . for this purpose deep_convolutional_neural_network is used , followed by feature_selection using an unsupervised technique with maximal information compression index as similarity measure . finally performance of two classifiers namely least square support_vector_machine ( lssvm ) and softmax regression are monitored and classifier selection is performed based on five measures along with five fold_cross_validation technique . output classes reflects the established bethesda system of classification for identifying pre-cancerous and cancerous lesion of cervix . the proposed system is also compared with two existing conventional systems and also tested on a publicly available database . experimental_results and comparison shows that proposed system performs efficiently in pap smear classification . 
modeling user goals for notification system interfaces responding to the need within the human_computer_interaction field to address ubiquitous and multitasking systems more scientifically , this research will investigate the usefulness of a new research framework for a particular class of systems . notification systems emerge as interfaces that are typically used in a divided-attention , multitasking situation , attempting to deliver current , valued information through a variety of platforms and modes in an efficient and effective manner . we recognize a lack of unifying framework for understanding , classifying , analyzing , developing , evaluating , and discussing notification systems fundamentally inhibiting scientific growth and knowledge reuse . to this end , we have developed a framework ( referred to here as the irc framework ) for notification systems research . the framework is based on a core taxonomy of critical parameters that describe notification system user goals in terms of expected sources of interaction utility . we conjecture that our framework will allow an improved usability engineering process and increased research cohesion to emerge for notification systems , lending efficiency to several aspects of a system design_cycle . the proposed research will investigate this notion , focusing on three key aspects : 1 . system description , allowing widely understood articulation of design objectives that are focused on critical user requirements . 2 . interface evaluation , enabling comparison of the design and user's models , while supporting generalizability of research and early identification of usability concerns . 3 . design comparison and reuse , saving time and effort in requirements_analysis and early design stages by enabling design_reuse and appreciation of design progress both which are informed by reference task benchmarking . each of these design_cycle improvements can be provided , if supporting analysis_tools and library systems are properly developed and tested . these development efforts are significant and require deep knowledge of the irc framework , notification systems designs , usability engineering , and design_reuse paradigms . contributions resulting from the proposed research include a valuable and living implementation of a notification systems component claims library , a design assessment system for notification system conceptual models , and a demonstration of a research agenda that is integrated with key hci processes . proposed efforts include development and execution of assessment cases , procedures , and instruments to determine the feasibility of these assertions . notification system development efforts emerging from classes , seminars , and independent study activities will contribute to this research as test_cases . successful execution and completion of this research will create many new research opportunities for other students and researchers within the hci community . 
gain control and linearity improvement for low noise amplifiers in 5ghz direct conversion receivers the members of the committee appointed to examine the thesis of mallesh rajashekharaiah find it satisfactory and recommend that it be accepted . ___________________________________ chair _______________________________________________________________________ iii acknowledgment this project was funded by the nsf center for design of analog-digital integrated_circuits ( www . eecs . wsu . edu/~cdadic ) and i would like to express my heartfelt gratitude to cdadic for the same . i would like to acknowledge the support and guidance received from my advisor dr heo , who made all this possible . thanks are due also to my team member and friend , parag upadhyaya for all the learning and continued support which has thoroughly helped me in my program . i would also like to express my deep sense of gratitude to prof . george larue for all the support he has provided in several ways , be it some problem in the testing lab , software tools or the regular dose of laughter the year round ! thanks to all my thesis committee members , prof . also , thanks to jeff dykstra of motorola and matt miller of freescale_semiconductor for their valuable suggestions and guidance during the course of the project . special mention about alaina mccully and joanne buteau for their administrative support . thanks to mark fuller also , for support with the measurements . 
acoustic compaction_layer detection the asae standardized tool to detect the depth and strength of compaction layers in the field is the cone penetrometer . since this method is point-to-point , researchers have experimented with on-the-fly alternatives that can be used as , or in combination with , a standard tillage tool . on-the-fly compaction_layer sensing also enables adaptive tillage , where the soil is only tilled as deep as necessary , which can lead to significant energy savings and erosion reduction . wedged tips , strain gauges mounted on a deflecting tine , air bubbles pushed into the soil , as well as ground_penetrating_radar have been tested for this purpose . in this research , passive acoustics was used to detect the compaction_layer by recording the sound of a cone being drawn through the soil . the premise was that a more compacted layer should cause higher sound levels , which might reveal the depth and strength of the compaction_layer . two experiments were conducted in the soil bins of the usda-ars national soil dynamics laboratory in auburn , alabama . first , constant-depth tests ( 15 and 30 cm ) at three compaction levels ( 0 . 72 , 2 . 8 , and 3 . 6 mpa ) revealed the relationship of sound amplitude with depth and compaction . second , to test the detection capability , the cone was gradually inserted in the soil , passing through an artificial compaction_layer . a windowed , short_time_fourier_transform ( stft ) analysis showed that the compaction_layer is detectable since the sound amplitude was positively related to depth and compaction levels , but only in the highest frequency_range of the spectrum . this led to the conjecture that the soil-cone interface acts as a low_pass filtering mechanism , where the cutoff_frequency becomes higher in the compaction_layer due to a more intimate contact between sensor and soil . oil compaction , caused by either natural causes or human interference , is a major yield-limiting factor . this is because soil_compaction : ( 1 ) reduces soil pore size , ( 2 ) changes pore size distribution , ( 3 ) increases soil strength , ( 4 ) reduces air and water permeability , ( 5 ) increases heat_capacity and bulk_density , and most importantly , ( 6 ) increases root penetration resistance ( al-adawi and reeder , 1996 ) . distinctively high-strength soil layers are commonly termed " hardpans " or " plow soles . " hardpans impede plant roots from uptake of nutrients and soil water reserves in the deeper soil strata . they also decrease water infiltration , which can accelerate loss of nutrients due to erosion and runoff . under wet conditions , roots above the hard-pan layer may suffocate due 
space-time least_squares finite_element_method for shallow_water_equations in this paper , a space-time least_squares finite_element_method for the 2d nonlinear shallow_water_equations ( swe ) is developed . the method can easily handle complex_geometry , bed slope ( source term ) , and radiation boundary_condition without any special treatment . other advantages of the method include : high_order approximations in space and time can easily be employed , no upwind scheme is needed , as well as the resulting system equations is symmetric and positive_definite , therefore , can be solved efficiently with the pre-conditioned conjugate_gradient_method . the model is applied to several benchmark tests , including standing_wave in a flat closed basin , propagation of sinusoidal wave in a flat channel with open boundary , flow past an elliptic hump , and wave-cylinder interactions . simulation of standing_wave in a closed flat basin , with water_depth ranging from shallow_water to deep_water , shows that prediction of swe model is accurate for shallow waters but inaccurate for deep_waters due to the hydrostatic pressure and non-dispersive assumption . computation of propagation of sinusoidal wave in a flat channel shows open boundary_condition is treated satisfactorily . simulation of flow past an elliptical hump shows good conservation property , and complicate and detailed fine wave structures which were not observed using the low order approximations in previous study . computed result of wave-cylinder interactions compare well with other numerical results . 
pre-workshop summary : workshop on iterative , adaptive , and agile processes broadly , the goals of this workshop are to promote a deep and shared understanding of various iterative , adaptive , and agile processes , including crystal , dsdm , extreme_programming ( xp ) , serum , and the unified process ( up ) to identify the factors for their successful adoption to share relevant research results discover lines of research to further this discipline overview we suspect very few methodologists created a process with the explicit intent , "oh boy , this will be heavy and rigid ! " and yet it's a common complaint . why is that ? it influences the motivation for this new generation of process ideas . will the same forces that have led to "unhealthy" application of prior generations of development_processes also inhibit the new ones ? for example , cases are now emerging of organizations just doing occasional unit_testing or avoiding documentation , and calling it "xp . " what are the skillful means and capabilities required to really help these new process ideas be adopted and succeed ? what's working ? what isn't ? and what are the hard-data justifications ? in this first year of the workshop , there are three major goals : to provide a supportive and fun forum for interested stakeholders to share their passion , research , stories , and ideas . compare and contrast ( via a summary of key details , a case study , and discussion ) related processes , ideally including identify cases , best practices , human_factors , and impediments to successful adoption of these process as a whole ( such as xp ) , or specific practices ( such as daily serum or pair_programming ) . in this workshop , we invite practitioners and researchers interested in the application of iterative , adaptive , and agile processes . and we expect to learn from each other . we include those favoring detailed or large_scale process descriptions ( such as the rational up ) to those favoring minimalist descriptions ( such as serum ) , from process evangelists to process agnostics , from expert developers to business managers , from domains where speed is the top priority to domains where quality or verifiability are the top priority . permission to make digital or hard_copies_of all or part of this work for personal or classroom use is granted_without_fee_provided that copies are not made or distributed for profit_or_commercial_advantage_and that copies_bear this notice and the full citation on the first page . to copy otherwise , or republish , to post on servers or to redistribute to fists , requires_prior_specific_permission and/or a fee . 
evacon : a framework for integrating evolutionary and concolic_testing for object_oriented_programs achieving_high structural coverage such as branch_coverage in object_oriented_programs is an important and yet challenging goal due to two main challenges . first , some branches involve complex program logics and generating tests to cover them requires deep knowledge of the program structure and semantics . second , covering some branches requires special method sequences to lead the receiver object or non-primitive arguments to specific desirable states . previous work has developed the concolic_testing technique ( a combination of concrete and symbolic testing techniques ) and the evolutionary_testing technique to address these two challenges , respectively . however , neither technique was designed to address both challenges at the same time . to address the respective weaknesses of these two previous techniques , we propose a novel framework called evacon that integrates evolutionary_testing ( used to search for desirable method sequences ) and concolic_testing ( used to generate desirable method arguments ) . we have implemented our framework and applied it on six classes taken from the java standard_library and basic data_structures . the experimental results show that the tests generated using our framework can achieve higher branch_coverage than evolutionary_testing or concolic_testing alone
architectural design iii : context catalog description 351-5 design iii : context . continuing study of architectural design . projects of increased scope and complexity . continue design_process study ( synthesis ) and appropriate design presentation ( communication ) . working with impingement introduced by external agencies such as social , government , and community , as part of a larger context of planning . study of the impact of site development , for on-site as well as external , contextual issues . prerequisites prerequisite : grade of d or better in arc 252-design ii : order prerequisite to : arc 352-design iv : complexity philosophy do not seek to follow in the footsteps of the masters , seek what they sought . -zen teaching , 8th century the education of an architect must begin with a framework for developing a personal consciousness , an awareness of the ever-changing and growing world around us . this consciousness is not developed by taking what is given and serving it up as a solution to the problem at hand . this consciousness is the result of deep critical_thinking , the development of astute powers of observation , an attunement to poetic systemic thinking , and a desire to become a creative solver of the problems our homes , cities , environment , clients , children , etc . face on a daily basis . a great practitioner is conscious to this world and is ready and willing to embrace this aspect of their lives to the point that it spills unconsciously into daily work and professional life . they don't simply replicate our core disciplinary knowledge , but seek to advance it through their own insight , offering new ways of seeing and inhabiting the world . throughout the curriculum , students must be asked to consider the translation of their consciousness as a human being into an expanded professional context-to vision the depth of their values and beliefs about architecture , test them , and bring them to life through critical work . each student must be encouraged to doubt , question givens , and to generate keen alternatives to what architecture is today . as students transition into practice , many lack the insight to think deeply about their profession and ask significant questions : what is it to be conscious in the profession of architecture ? what are the meaningful contributions i wish to make to the lives of other people , to the environment , and to our collective world ? what enables a practice to transcend the norm , making lasting contributions , point to the future , and wake the light in all us ? great education offers its students the opportunity and challenge of becoming a 
compared deep class-ab and class-b ageing on algan/gan hemt in s-band pulsed-rf operating life algan/gan hemts are on the way to lead the rf-power amplification field according to their outstanding performances . however , due to its relative youth , reliability studies in several types of operating_conditions allow to understand mechanisms peculiar to this technology and responsible for the wearing out of devices . this paper reports the reliability study on two power amplifiers using nitronex algan/gan hemt . based on results of a previous study of 1280 hours in standard operating_conditions wherein no evolution of electrical_parameters have been observed , two ageing tests in deep class-ab ( 432 hours ) and class-b ( 795 hours ) are performed under pulsed-rf operating life at high drain bias voltages and saturated operation . this study shows a drift in rf performances which is linked with the evolution of electrical_parameters ( r ds on , g m and v p ) . similar kinetics and amplitude of degradations are observed revealing quasi similar contribution of thermal effects in both cases . degradations are supposed to be related to trapped charges phenomena induced by high voltage operating_conditions . although , several results attest to this hypothesis , a part of the evolutions seems to be linked with structural changes . 
prediction of diaphragm wall deflection in deep excavations using evolutionary support_vector_machine inference model ( esim ) problems in deep excavations are full of uncertain , vague , and incomplete information . in most instances , successfully solving such problems depends on experts' knowledge and experience . the primary object of this research was to propose an " evolutionary support_vector_machine inference model ( esim ) " to predict wall deformation in deep excavation in taipei basin . esim is developed based on a hybrid approach that fuses support_vector_machines ( svm ) and fast messy genetic_algorithm ( fmga ) . svm is primarily concerned with learning and curve_fitting; and fmga with optimization . fifty-seven wall deformation monitoring database were collected based on monitoring data and compiled from prior projects . fifty-two of 57 were selected for training , leaving 5 valid cases available for testing . results show that esim can successfully predict the deflection and apply to contractors utilizes the knowledge and experience from past projects to predict wall deformation of new projects . therefore the construction and foundation construction contractors can update wall deflection monitoring data of different stages during deep excavation process , in order to predict the wall deformation of the next stage and examine whether the max deflection is within the controlled range . the results are used as guidelines on site safety and risk_management . 
empirical_methods in ai the debate during the workshop can broadly be divided into three categories : ( 1 ) past successes of empirical_methods , ( 2 ) the design of computational experiments , and ( 3 ) the widespread use of random problems . the following summary necessarily offers just a partial description of the topics discussed during the workshop . 1 success stories empirical_methods have been successful in recent years . indeed , as henry kautz reminded the workshop participants , in the last year alone , the new york times has reported two major empirical successes : ( 1 ) deep blue's defeat of kasparov and ( 2 ) the computer generated proof of an open_problem in robbins algebra . pan-durang nayak ( nasa_ames ) described another highly publicized success , the diagnosis system for the deep space one spacecraft , which is based on a highly optimized satisfiability procedure . although deciding satisfiability is intractable in general , this system generates plans in practice in essentially constant time for each step . it comes as quite a surprise to hear about real-time satisfiability testing . henry kautz listed several reasons for the success of empirical_methods . first , empirical studies are often an integral part of ai because systems can be too complex or messy for theory . second , theory is often too crude to provide useful insight . for example , a problem might be exponential in the worst case but tractable in practice . third , some questions are purely empirical . as pedro meseguer ( iiia , csic , spain ) pointed out during one of the panels , two search_algorithms s in the last few years , we have witnessed a major growth in the use of empirical_methods in ai . in part , this growth has arisen from the availability of fast net-worked computers that allow certain problems of a practical size to be tackled for the first time . there is also a growing realization that results_obtained empirically are no less valuable than theoretical results . experiments can , for example , offer solutions to problems that have defeated a theoretical attack and provide insights that are not possible from a purely theoretical analysis . i identify some of the emerging trends in this area by describing a recent workshop that brought together researchers using empirical_methods as far apart as robotics and knowledge_based_systems . t wenty-five researchers gathered together during the fifteenth international joint conference on artificial_intelligence ( ijcai-97 ) in nagoya , japan , for the second workshop on empirical artificial_intelligence . the workshop continued the work of a similar workshop held alongside ecai-96 in 
improved algorithm for gradient vector flow based active_contour model using global_and_local information active_contour models are used to extract object boundary from digital_image , but there is poor convergence for the targets with deep concavities . we proposed an improved approach based on existing gradient vector flow methods . main contributions of this paper are a new algorithm to determine the false part of active_contour with higher_accuracy from the global force of gradient vector flow and a new algorithm to update the external force field together with the local information of magnetostatic force . our method has a semidynamic external force field , which is adjusted only when the false active_contour exists . thus , active_contours have more chances to approximate the complex boundary , while the computational cost is limited effectively . the new algorithm is tested on irregular shapes and then on real images such as mri and ultrasound medical data . experimental_results illustrate the efficiency of our method , and the computational_complexity is also analyzed . 
simple design_tools on 15 april 1999 , people around the world sent thousands of e-mails to our group at stanford_university inquiring about the " secret " performance report of the amd k7 and intel coppermine chips . according to the register web_site in the united_kingdom , there were rumors that some students at stanford_university had tested and compared the two chips using spec's test_suite . at that time , the amd k7 ( now known as athlon ) and the intel coppermine ( now known as pentium_iii ) were not available to the general public , and it is easy to understand the excitement about this news . the real story was simpler . the students in our processor design class did not test the real chips . they estimated the cost and performance of the two chips as part of a case study . this study was based on information available from various public sources . 3 as the chip implementation_details had not been released to the public , the instructors assumed the hardware designs were similar to the simulator default configurations . after comparing the performance and the costs , the students used the simulator tools to design improvements to each chip . designing deep_submicron microprocessors is becoming an increasingly tedious and complicated process . 4 it takes many years and many engineers to design a commercial processor chip . in an introductory computer architecture course , students are usually required to simulate a simplified pipelined microprocessor such as dlx 5 using some hardware_description_language ( typically verilog or vhdl ) or some graphical tool ( such as hase ) . 6 while it is important for a student to understand how a basic processor operates , students may not fully appreciate the complexity and the various trade_offs_involved_in designing a processor in the commercial environment . students cannot afford to write tens of thousands of lines of code to model processor microarchitecture; it is unproductive to ask them to deal with the myriad details at this stage . instead , we focus on high_level issues involving cost ( area ) and performance ( execution time ) . the main_issues are cache size , cycle time , floating_point_unit ( fpu ) area , latencies , branch strategy , and issue width . by emphasizing a few primary high_level issues , students gain a better understanding of the trade_offs_involved_in overall computer architecture design . table 1 lists the architectural design_tools available in our class . students use these tools in conjunction with the class textbook , 7 and they are also available for other 
training deep spiking neural_networks using backpropagation deep spiking neural_networks ( snns ) hold the potential for improving the latency and energy efficiency of deep_neural_networks through data-driven event-based computation . however , training such networks is difficult due to the non-differentiable nature of spike events . in this paper , we introduce a novel technique , which treats the membrane potentials of spiking neurons as differentiable signals , where discontinuities at spike times are considered as noise . this enables an error backpropagation mechanism for deep snns that follows the same principles as in conventional deep_networks , but works directly on spike signals and membrane potentials . compared with previous methods relying on indirect training and conversion , our technique has the potential to capture the statistics of spikes more precisely . we evaluate the proposed framework on artificially generated events from the original mnist handwritten digit benchmark , and also on the n-mnist benchmark recorded with an event-based dynamic vision sensor , in which the proposed method reduces the error_rate by a factor of more than three compared to the best previous snn , and also achieves a higher_accuracy than a conventional convolutional_neural_network ( cnn ) trained and tested on the same data . we demonstrate in the context of the mnist task that thanks to their event_driven operation , deep snns ( both fully_connected and convolutional ) trained with our method achieve accuracy equivalent with conventional neural_networks . in the n-mnist example , equivalent accuracy is achieved with about five times fewer computational operations . 
flexible , high_performance convolutional_neural_networks for image_classification we present a fast , fully parameterizable gpu implementation of convolutional_neural_network variants . our feature extractors are neither carefully designed nor pre-wired , but rather learned in a supervised way . our deep_hierarchical architec-tures achieve the best published_results on benchmarks for object_classification ( norb , cifar10 ) and handwritten digit recognition ( mnist ) , with error_rates of 2 . 53% , 19 . 51% , 0 . 35% , respectively . deep nets trained by simple back-propagation perform better than more shallow ones . learning is surprisingly rapid . norb is completely trained within five epochs . test error_rates on mnist drop to 2 . 42% , 0 . 97% and 0 . 48% after 1 , 3 and 17 epochs , respectively . 
applying convolutional_neural_networks concepts to hybrid nn-hmm model for speech_recognition convolutional_neural_networks ( cnn ) have showed success in achieving translation invariance for many image_processing tasks . the success is largely attributed to the use of local filtering and max_pooling in the cnn architecture . in this paper , we propose to apply cnn to speech_recognition within the framework of hybrid nn-hmm model . we propose to use local filtering and max_pooling in frequency_domain to normalize speaker variance to achieve higher multi-speaker speech_recognition performance . in our method , a pair of local filtering layer and max_pooling layer is added at the lowest end of neural_network ( nn ) to normalize spectral variations of speech signals . in our experiments , the proposed cnn architecture is evaluated in a speaker independent speech_recognition task using the standard timit data_sets . experimental results show that the proposed cnn method can achieve over 10% relative error reduction in the core timit test_sets when comparing with a regular nn using the same number of hidden layers and weights . our results also show that the best result of the proposed cnn model is better than previously_published results on the same timit test_sets that use a pre-trained deep nn model . 
multispectral image_compression based on dsc combined with ccsds-idc remote_sensing multispectral image_compression encoder requires low_complexity , high robust , and high_performance because it usually works on the satellite where the resources , such as power , memory , and processing capacity , are limited . for multispectral images , the compression algorithms based on 3d transform ( like 3d dwt , 3d dct ) are too complex to be implemented in space mission . in this paper , we proposed a compression algorithm based on distributed source coding ( dsc ) combined with image data_compression ( idc ) approach recommended by ccsds for multispectral images , which has low_complexity , high robust , and high_performance . first , each band is sparsely represented by dwt to obtain wavelet coefficients . then , the wavelet coefficients are encoded by bit plane encoder ( bpe ) . finally , the bpe is merged to the dsc strategy of slepian-wolf ( sw ) based on qc-ldpc by deep coupling way to remove the residual redundancy between the adjacent bands . a series of multispectral images is used to test our algorithm . experimental results show that the proposed dsc combined with the ccsds-idc ( dsc-ccsds ) -based algorithm has better compression performance than the traditional compression approaches . 
motion deblurring as optimisation motion_blur is one of the most common causes of image degradation . it is of increasing interest due to the deep penetration of digital_cameras into consumer applications . in this paper , we start with a hypothesis that there is sufficient information within a blurred image and approach the deblurring problem as an optimisation process where the deblurring is to be done by satisfying a set of conditions . these conditions are derived from first principles underlying the degradation process assuming noise-free environments . we propose a novel but effective method for removing motion_blur from a single blurred image via an iterative algorithm . the strength of this method is that it enables deblurring without resorting to estimation of the blur kernel or blur depth . the proposed iterative_method has been tested on several images with different degrees of blur . the obtained results have been compared with state of the art techniques including those that require more than one input image . the results are consistently of high_quality and comparable or superior to the existing_methods which demonstrates the effectiveness of the proposed technique . 
lockin-thermography : principles , nde-applications , and trends a review is given about lockin-thermography , about its photoacoustic and photothermal roots , about the principle and modern applications for nondestructive_testing using different kinds of options . as lockin-thermography is based on thermal waves , a short excursion to this kind of waves and some remarks on the way they were used seems to be appropriate . some indications about promising futural developments are provided as well . when fourier was involved in planning the water_supply tubing for paris , he was concerned with the problem how deep the tubes should be buried in the soil to prevent freezing in winter . so he dealt with the periodical temperature cycles on the surface and how deep they extend into the soil . he found out that the process is described by a linear_differential_equation whose solution is a highly attenuated wave where thermal_diffusivity is the only parameter involved [1] . in order to solve the linear_differential_equation for non-sinusoidal boundary_condition ( daily and annual temperature cycle ) , he decomposed it into a sum of sine functions and superposed the solutions . fourier became famous for developing the mathematical principle of solution which is broadly applicable . his solution is the " thermal wave " that describes the deviation t of temperature from its local average , ) / ( / 0 ) , ( x t i x e e t t x t where the " thermal diffusion length " is defined as 2 c with thermal_conductivity , density , specific_heat c , and angular modulation frequency . this dependence on depth and time displays the strong damping and also the phase shift with increasing depth . hence phase_velocity is given by v = ~ obviously higher frequencies propagate faster which means that thermal waves are dispersive . the consequence is that a temperature pulse starting from the surface changes its shape while it propagates into the material . as only sine waves maintain their shape , in the following we deal only with them while vavilov s review at this conference reports on pulsed excitation . fig . 1 . temporal and spatial behaviour of a one-dimensional thermal wave of 0 . 03 hz propagating in polyvinylchloride ( pvc ) . from thermal_diffusivity = 0 . 0011 cm 2 /s ) results a phase_velocity of v ph = 0 . 2036 mm/s at this frequency [2] . 
high_performance neural networks for visual object_classification high_performance neural networks for visual object_classification we present a fast , fully parameterizable gpu implementation of convolutional_neural_network variants . our feature extractors are neither carefully designed nor pre-wired , but rather learned in a supervised way . our deep_hierarchical architectures achieve the best published_results on benchmarks for object_classification ( norb , cifar10 ) and handwritten digit recognition ( mnist ) , with error_rates of 2 . 53% , 19 . 51% , 0 . 35% , respectively . deep nets trained by simple back-propagation perform better than more shallow ones . learning is surprisingly rapid . norb is completely trained within five epochs . test error_rates on mnist drop to 2 . 42% , 0 . 97% and 0 . 48% after 1 , 3 and 17 epochs , respectively . 
factors influencing employees' deep usage of information_systems the adoption process within organizational settings , which is different from traditional individual adoption , involves both the primary adoption by managers and the secondary adoption by employees . understanding what determinants influence employees to deeply use information_systems ( is ) would help managers improve the process of facilitating system implementation . this study explores the determinants that can affect employees to deeply use information_systems , in particular when there is an organizational mandate to adopt an is . a new framework is developed that combines insights from the individual innovation post-acceptance model as well as from organizational adoption and assimilation frameworks . the research methodology used to test the hypotheses is discussed , and contributions of this study are presented . 
a divide-and-conquer-based algorithm for automatic simulation vector_generation testbenches play one of the most important roles in simulation_based design_verification . given a simulation scenario , a testbench provides specific vectors to simulate the design , then collects responses from the design to monitor whether the simulation has satisfied the scenario . 1 the major_bottleneck in writing testbenches is generating valid simulation_vectors . traditionally , test-bench engineers generate these vectors manually a time-consuming and troublesome task . moreover , manually generated simulation_vectors rarely cover all simulation scenarios . automated generation of simulation_vectors is therefore vital for effective simulation . many current automatic-vector_generation methods focus on exploring a design's state_space . due to memory or runtime limitations , these methods cannot keep up with the rapid growth of design_complexity . we propose a novel algorithm based on the divide-and-conquer paradigm that helps these methods decompose the design's complexity . the algorithm uses a partitioning method that recursively divides a design into smaller , more manageable components . other approaches handle the divided components while maintaining the entire design's proper functioning . experimental_results_demonstrate that vector_generation methods , with the help of our algorithm , improve the coverage of simulation scenarios . researchers have proposed many techniques for automatic simulation vector_generation . these techniques generally fall into three categories : random_simulation , symbolic solvers , and hybrid solvers . random_simulation generates sets of simulation_vectors by randomly assigning the logic values to the design's primary inputs ( pis ) one cycle at a time . random simulation's strengths are that it allows easy acquisition of simulation_vectors , and it offers a deep_state-space search distance . its weakness is that it uses only one trace to explore the state space . because random_simulation is easy to implement and can generate useful simulation_vectors , most vector_generation engines use this technique . unlike random_simulation , which uses only a single trace , symbolic solvers attempt to simultaneously enumerate all possible primary inputs to explore the entire state_space . 2-6 they typically use binary decision diagrams ( bdds ) or satisfiability ( sat ) solvers as their core engine . a symbolic solver's main feature is its exhaustive search ability . furthermore , a symbolic solver obtains simulation_vectors that are much more compact than those obtained by random_simulation . as the state space editor's note : divide-and-conquer is a natural way to cope with the complexity of automatic testbench generation . the key to developing an effective divide-and-conquer approach is to identify the partitioning boundaries where interactions among divided components are minimized . the authors propose a novel design decomposition scheme and 
topic-focus articulation and anaphoric relations : corpus based probe 1 . the objective of the paper is to analyze certain interrelationships between the information_structure , i . e . the topic-focus articulation ( tfa ) of sentences , and anaphoric relations , on the material achieved during the annotation of tfa and of coreference in the prague dependency treebank ( pdt ) . 2 . 1 prague dependency treebank ( pdt ) is conceived of as a collection of 3 , 168 samples of continuous running czech texts ( taken at random from the czech national corpus ) , annotated besides a complex scheme of morphemic tags on two layers of dependency-based sentence structure , the first of which the analytic one is considered to be an intermediate step towards the underlying level of annotation , the so-called tectogrammatical tree structures ( tgtss ) , in which nodes are also reconstructed for items deleted in the surface shape of the sentences . these structures are designed in a way that allows i . a . for an inclusion of information on both intra-and inter-sentential coreference relations . 2 . 2 in addition to the deep_syntactic dependency relations in the tree_structure , individual nodes are assigned one of the three values of contextual boundness : non-contrastive contextually bound " t " , contrastive contextually bound " c " and contextually non-bound " f " . this information at individual nodes of the dependency tree_structure makes it possible to derive the division of the sentence into topic ( in the prototypical case : what the sentence is about ) and focus ( what the sentence says about the topic ) ; the basic algorithm for this procedure was formulated by haji ov and sgall ( see haji ov and sgall 1985 ) and its implementation and testing on pdt is reported in haji ov , havelka and vesel ( 2005 ) . 2 . 3 in a separate path through the corpus annotated on this underlying level , basic coreference relations are being marked independently of the tfa values . in our project , two types of coreference are distinguished : grammatical with verbs ( and also some nouns ) of control , with reflexive pronouns , with verbal complements and with relative pronouns and textual , which may cross sentence boundaries . both endophoric ( anaphora ) and exophoric ( deixis ) relations are taken into account as well as cataphora ( see ku ov and haji ov 2004 ) . for the annotation of grammatical coreference ( which has been given a systematic account in the description , see ku ov et al . 2003 ) a semi_automatic procedure has already been implemented , which is giving rather encouraging results . the manual annotation of textual coreference is carried out with the use of a user-friendly tool in 
mathematical formulation of multilayer networks a network representation is useful for describing the structure of a large variety of complex systems . however , most real and engineered systems have multiple subsystems and layers of connectivity , and the data produced by such systems are very rich . achieving a deep understanding of such systems necessitates generalizing ''traditional'' network_theory , and the newfound deluge of data now makes it possible to test increasingly general frameworks for the study of networks . in particular , although adjacency matrices are useful to describe traditional single_layer networks , such a representation is insufficient for the analysis and description of multiplex and time-dependent networks . one must therefore develop a more general mathematical framework to cope with the challenges posed by multilayer complex systems . in this paper , we introduce a tensorial framework to study multilayer networks , and we discuss the generalization of several important network descriptors and dynamical processes including degree centrality , clustering coefficients , eigenvector centrality , modularity , von_neumann_entropy , and diffusion for this framework . we examine the impact of different choices in constructing these generalizations , and we illustrate how to obtain known results for the special cases of single_layer and multiplex networks . our tensorial approach will be helpful for tackling pressing problems in multilayer complex systems , such as inferring who is influencing whom ( and by which media ) in multichannel social_networks and developing routing techniques for multimodal transportation systems . 
deep bottleneck features for spoken language identification a key problem in spoken language identification ( lid ) is to design effective representations which are specific to language information . for example , in recent years , representations based on both phonotactic and acoustic features have proven their effectiveness for lid . although advances in machine_learning have led to significant_improvements , lid performance is still lacking , especially for short duration speech utterances . with the hypothesis that language information is weak and represented only latently in speech , and is largely dependent on the statistical properties of the speech content , existing representations may be insufficient . furthermore they may be susceptible to the variations caused by different speakers , specific content of the speech segments , and background noise . to address this , we propose using deep bottleneck features ( dbf ) for spoken lid , motivated by the success of deep_neural_networks ( dnn ) in speech_recognition . we show that dbfs can form a low-dimensional compact representation of the original inputs with a powerful descriptive and discriminative capability . to evaluate the effectiveness of this , we design two acoustic_models , termed dbf-tv and parallel dbf-tv ( pdbf-tv ) , using a dbf based i-vector representation for each speech utterance . results on nist language recognition evaluation 2009 ( lre09 ) show significant_improvements over state-of-the-art systems . by fusing the output of phonotactic and acoustic approaches , we achieve an eer of 1 . 08% , 1 . 89% and 7 . 01% for 30 s , 10 s and 3 s test utterances respectively . furthermore , various dbf configurations have been extensively evaluated , and an optimal system proposed . 
comparative_genomics approach to detecting split-coding regions in a low-coverage genome : lessons from the chimaera callorhinchus milii ( holocephali , chondrichthyes ) recent development of deep_sequencing technologies has facilitated de novo genome_sequencing projects , now conducted even by individual laboratories . however , this will yield more and more genome sequences that are not well assembled , and will hinder thorough annotation when no closely_related reference_genome is available . one of the challenging issues is the identification of protein-coding sequences split into multiple unassembled genomic segments , which can confound orthology assignment and various laboratory experiments requiring the identification of individual genes . in this study , using the genome of a cartilaginous fish , callorhinchus milii , as test_case , we performed gene prediction using a model specifically trained for this genome . we implemented an algorithm , designated esprit , to identify possible linkages between multiple protein-coding portions derived from a single genomic locus split into multiple unassembled genomic segments . we developed a validation framework based on an artificially fragmented human_genome , improvements between early and recent mouse genome assemblies , comparison with experimentally validated sequences from genbank , and phylogenetic analyses . our strategy provided insights into practical solutions for efficient annotation of only partially sequenced ( low-coverage ) genomes . to our knowledge , our study is the first formulation of a method to link unassembled genomic segments based on proteomes of relatively distantly related species as references . 
how to test for dual_task-specific effects in brain_imaging studies - an evaluation of potential analysis_methods the study of the concurrent performance of two tasks allows deep insights into the human cognitive system and , accordingly , an increasing number of brain_imaging studies are conducted to identify the neuroanatomical correlates of such dual_task performance . in this overview we present currently used approaches to identify dual_task-specific activations in fmri and pet studies . a comparison is made in order to identify the approaches which have the potential to validly detect dual_task-specific activation_patterns , i . e . activation which cannot be explained by the individual performance of the component tasks alone . we demonstrate that while all approaches suffer from at least some drawbacks , the best ( although potentially over-conservative ) approach is to compare the dual_task with the sum of the single tasks , the second-best is an interaction contrast , and the third-best a conjunction analysis . comparisons of the dual_task with the mean of single-task activity or with only one single task should be avoided except for a few specific situations . we generalize our conclusions to related research areas , such as multisensory_integration or divided attention . 
machine lifelong_learning : challenges and benefits for artificial_general_intelligence we propose that it is appropriate to more seriously consider the nature of systems that are capable of learning over a lifetime . there are three reasons for taking this position . first , there exists a body of related work for this research under names such as constructive induction , continual learning , sequential task learning and most recently learning with deep_architectures . second , the computational and data storage power of modern computers are capable of implementing and testing machine lifelong_learning systems . third , there are significant challenges and benefits to pursuing programs of research in the area to agi and brain sciences . this paper_discusses each of the above in the context of a general framework for machine lifelong_learning . 
jaws : a javascript api for the efficient testing and integration of semantic_web_services semantic_web_services ( sws ) hold a lot of potential to the future of the semantic_web . in this area , a number of tools have been developed to facilitate their definition and deployment . our goal is to support an efficient means of testing and integration within a browser-based solution . for this purpose we propose jaws ( javascript , ajax , web_service ) : a javascript api to facilitate the testing and integration of sws . this software decouples the process of sws integration and development through facilitation of the ajax/rest paradigm . by leveraging meta-programming and deep_integration techniques we support web_2_._0 inspired applications in the context of complete browser-based development . 
tunnel detection using near-surface seismic methods geophysical detection of near-surface voids caused by mining , tunnels , karst features , etc . , is a persistent problem that has not been solved either consistently or across multiple geologic settings . multiple methods have been used with varying degrees of success . we present shallow seismic data_collected at a test site with a 9 . 1-m deep tunnel in unconsolidated sediments similar to geologic settings found along the southwest us border . test results_demonstrate the capability of using p_wave diffraction and surface_wave backscatter techniques to detect a purpose-built subterranean tunnel . data were processed blindly by remote personnel , who were not aware of the target location , and results are in excellent agreement with the ground_truth . these methods show promise for both efficient , production-scale data_acquisition and real-time automated data_processing . 
mechanical coupling error suppression technology for an improved decoupled dual-mass micro_gyroscope this paper presents technology for the suppression of the mechanical coupling errors for an improved decoupled dual-mass micro_gyroscope ( ddmg ) . the improved micro_gyroscope structure decreases the moment arm of the drive decoupled torque , which benefits the suppression of the non-ideal decoupled error . quadrature correction electrodes are added to eliminate the residual quadrature error . the structure principle and the quadrature error suppression means of the ddmg are described in detail . ansys software is used to simulate the micro_gyroscope structure to verify the mechanical coupling error suppression effect . compared with the former structure , simulation results_demonstrate that the rotational displacements of the sense frame in the improved structure are substantially suppressed in the drive mode . the improved ddmg structure chip is fabricated by the deep dry silicon on glass ( ddsog ) process . the feedback_control circuits with quadrature control loops are designed to suppress the residual mechanical coupling error . finally , the system performance of the ddmg prototype is tested . compared with the former ddmg , the quadrature error in the improved dual-mass micro_gyroscope is decreased 9 . 66-fold , and the offset error is decreased 6 . 36-fold . compared with the open loop sense , the feedback_control circuits with quadrature control loop decrease the bias drift by 20 . 59-fold and the scale_factor non-linearity by 2 . 81-fold in the 400 /s range . 
effect of eeg electrode number on epileptic source_localization in pediatric patients . objective to investigate the relationship between eeg source_localization and the number of scalp eeg recording channels . methods 128 eeg channel recordings of 5 pediatric patients with medically intractable partial epilepsy were used to perform source_localization of interictal spikes . the results were compared with surgical resection and intracranial recordings . various electrode configurations were tested and a series of computer simulations based on a realistic head boundary element model were also performed in order to further validate the clinical findings . results the improvement seen in source_localization substantially decreases as the number of electrodes increases . this finding was evaluated using the surgical resection , intracranial recordings and computer simulation . it was also shown in the simulation that increasing the electrode numbers could remedy the localization error of deep sources . a plateauing effect was seen in deep and superficial sources with further increasing the electrode number . conclusion the source_localization is improved when electrode numbers increase , but the absolute improvement in accuracy decreases with increasing electrode number . significance increasing the electrode number helps decrease localization error and thus can more ably assist the physician to better plan for surgical procedures . 
3d object_recognition with deep_belief nets we introduce a new type of top-level model for deep_belief nets and evaluate it on a 3d object_recognition task . the top-level model is a third-order boltzmann machine , trained using a hybrid algorithm that combines both generative and discriminative gradients . performance is evaluated on the norb database ( normalized-uniform version ) , which contains stereo-pair images of objects under different lighting_conditions and viewpoints . our model achieves 6 . 5% error on the test set , which is close to the best published result for norb ( 5 . 9% ) using a convolutional_neural net that has built-in knowledge of translation invariance . it substantially outperforms shallow models such as svms ( 11 . 6% ) . dbns are especially suited for semi_supervised_learning , and to demonstrate this we consider a modified version of the norb recognition task in which additional unlabeled images are created by applying small translations to the images in the database . with the extra unlabeled_data ( and the same amount of labeled_data as before ) , our model achieves 5 . 2% error . 
realizing high test_quality goals with smart test resource usage growing asic design sizes and advanced deep_sub_micron_technologies require new fault_models and more test_vectors to meet high test_quality goals . to realize these goals within given test resources and cost constraints , new dft techniques must be used . this paper reports test_quality metrics and the test cost of industrial designs for different fault_models using three dft techniques : atpg for deterministic patterns , logic_bist for pseudo_random patterns , and edt for compressed deterministic patterns . it is shown how these techniques can be used to achieve the high_quality goals within the test resources currently available for stuck-at tests . 
revealing skype traffic : when randomness plays with you skype is a very popular voip software which has recently attracted the attention of the research_community and network operators . following a closed_source and proprietary design , skype protocols and algorithms are unknown . moreover , strong encryption mechanisms are adopted by skype , making it very difficult to even glimpse its presence from a traffic aggregate . in this paper , we propose a framework based on two complementary techniques to reveal skypetraffic in real time . the first approach , based on pearson'schi-square test and agnostic to voip-related trafficcharacteristics , is used to detect skype's fingerprint from the packet framing structure , exploiting the randomness introduced at the bit_level by the encryption process . conversely , the second approach is based on a stochastic characterization of skype traffic in terms of packet arrival rate and packet length , which are used as features of a decision process based on naive bayesian classifiers . in order to assess the effectiveness of the above techniques , we develop an off_line cross-checking heuristic based on deep_packet_inspection and flow correlation , which is interesting per se . this heuristic allows us to quantify the amount of false negatives and false_positives gathered by means of the two proposed approaches : results obtained from measurements in different networks show that the technique is very effective in identifying skype traffic . while both bayesian classifier and packet_inspection techniques are commonly used , the idea of leveraging on randomness to reveal traffic is novel . we adopt this to identify skype traffic , but the same methodology can be applied to other classification_problems as well . 
a probabilistic model for path_delay_fault_testing testing path delay_faults ( pdfs ) in vlsi_circuits is becoming an important_issue as we enter the deep submicron age . however , it is difficult in general since the number of faults is normally very large and most faults are either hard to sensitize or are untestable . in this paper , we propose a probabilistic pdf model . we investigate probability functions for the wire and path_delay size to model the fault effect in the circuit under test . in our approach , the delay_fault size is assumed to be randomly distributed . an analytical model is proposed to evaluate the pdf coverage . we show that the delay sizes of the untested paths are actually reduced if these paths are conjoined with other tested good paths . therefore , using our approach , path_selection and synthesis of pdf testable circuits can be done more accurately . also , given a test set , more accurate fault_coverage can be predicted by calculating the mean delay of the paths . 
invited paper special section on josephson junctions past 50 years and future recent_developments of high-t c electronic devices with multilayer structures and ramp-edge josephson junctions * seiji adachi summary recent_developments of electronic devices containing josephson junctions ( jj ) with high-t c superconductors ( hts ) are reported . in particular , the fabrication process and the properties of superconduct-ing quantum interference devices ( squids ) with a multilayer structure and ramp-edge-type jjs are described . the jjs were fabricated by re-crystallization of an artificially deposited cu-poor precursory layer . the formation mechanism of the junction barrier is discussed . we have fabricated various types of gradiometers and magnetometers . they have been actually utilized for several application systems , such as a non-destructive evaluation ( nde ) system for deep-lying defects in a metallic plate and a reel-to-reel testing system for striated hts-coated conductors . 
deep_neural_networks for improved , impromptu trajectory_tracking of quadrotors trajectory_tracking control for quadrotors is important for applications ranging from surveying and inspection , to film making . however , designing and tuning classical controllers , such as proportional-integral-derivative ( pid ) controllers , to achieve high tracking precision can be time-consuming and difficult , due to hidden dynamics and other non-idealities . the deep_neural_network ( dnn ) , with its superior capability of approximating abstract , nonlinear functions , proposes a novel approach for enhancing trajectory_tracking control . this paper presents a dnn-based algorithm that improves the tracking performance of a classical feedback controller . given a desired trajectory , the dnns provide a tailored input to the controller based on their gained experience . the input aims to achieve a unity map between the desired and the output trajectory . the motivation for this work is an interactive " fly-as-you-draw " application , in which a user draws a trajectory on a mobile_device , and a quadrotor instantly flies that trajectory with the dnn-enhanced control system . experimental_results_demonstrate that the proposed approach improves the tracking precision for user-drawn trajectories after the dnns are trained on selected periodic trajectories , suggesting the method's potential in real_world_applications . tracking errors are reduced by around 40-50% for both training_and_testing trajectories from users , highlighting the dnns' capability of generalizing knowledge . 
vulnerability of machine_learning models to adversarial examples we propose a genetic algorithm for generating adversarial examples for machine_learning models . such approach is able to find adversarial examples without the access to model's parameters . different models are tested , including both deep_and_shallow neural_networks archi-tectures . we show that rbf networks and svms with gaussian kernels tend to be rather robust and not prone to misclassification of adversarial examples . 
electroluminescence spectroscopy for reliability investigations of 1 . 55 mum bulk semiconductor optical_amplifier this paper demonstrates the complementary relation between functional parameters and electroluminescence spectroscopy for reliability investigations of 1550 nm semiconductor optical amplifiers of 700 m length active region . ageing tests have been set to 270 ma-100c-1500 h and realized on two different wafers showing more impact on wafer 1 than on wafer 2 . our investigations are particularly focused on interpretation of electroluminescence spectra , from reference and aged soas of wafer 1 , leading to an improvement of degradation mechanisms understanding . the shift rate to lower energies of the recombination energy peak at 1550 nm , as reported by electroluminescence spectra between reference and aged soas in relation with the decrease of optical_power measured at 200 ma for the degraded soa and completed by i ( v ) characterizations , suggest occurrence of non radiative deep centers near the buried ridge structure in relation with the cleaning process uniformity of interfaces before epitaxial overgrowth . these defects mainly trap majority injected carriers instead of minority carriers reducing the luminescence in the active zone . by monitoring the most sensitive failure indicator ( pseudo-threshold current ) , lifetime distributions are also calculated to determine failure_rate , between 150 and 200 fits over 15 years for operating_conditions ( 25c-200 ma ) using experimental degradation laws and statistic computations , demonstrating the overall robustness of this technology . 
considering diagnosis functionality during automatic system-level_design of automotive networks today , design_automation approaches for automotive e/e-architectures focus solely on application functionality , neglecting firmware-related functionalities like diagnostic tests that are of utmost importance for quality features such as dependability or maintenance . however , the latter are typically considered dispensable since they do not provide direct service to the user . this paper proposes a novel approach for integrating optional diagnosis functionality into a holistic design space_exploration of automotive e/e-architectures at system-level . opposed to application functionality , hardware-diagnostics dig deep into the hardware-structures and , hence , require specific tailoring for the employed resources . a case study with software_based_self-tests representing advanced diagnosis functionality gives evidence of the viability and efficiency of the proposed approach , highlighting the importance of a holistic consideration of application as well as firmware-related functionality . 
client_side estimation of a remote service execution many use cases , concerning the monitoring and controlling of real physical instruments , demand deep interaction between users and services that virtualize the access to such instruments/devices . in addition , in order to realize high interoperable solutions , soa-based web/grid service technologies must be adopted . when the access to one of these services is performed via internet using a web_service call , the remote invocation time becomes critical in order to understand if an instrument can be controlled properly , or the delays introduced by the wire and the serialization/deserialization process are unacceptable . this paper thus presents methodologies and algorithms , based on a 2 k factorial analysis and a gaussian majorization of previous service execution times , which enables the estimation of a generic remote method execution time . furthermore it suggests three different software_architectures , where the developed algorithms and methodology could be integrated in order to automatically profile the end_to_end service . it is worth noting that our proposals are validated using suitable benchmarks and extensive_tests coming out from a real ( not simulated ) environment . in addition , the outcome of this paper have been used in the realization of a service for remote_control , monitor , and manage of a pool of instruments/devices . 
an architectural approach to the management of applications with qos requirements on grid an architectural approach to the management of applications with qos requirements on grid title : an architectural approach to the management of applications with qos requirements on grid grid_computing is surely one of the most interesting and widely studied distributed paradigms of the last decade . during years of research , development and test , grid has evolved in different directions , far away from its original conception . in particular , grid_computing has been originally designed as an universal platform to share and access worldwide resources in order to support the management and execution of large scientific jobs . despite the first academic target , the potentiality of grid as an architecture for sharing any kind of resources had made the platform suitable for other kinds of tasks . for this , grid evolved from scientific to commercial purposes , extending the number of potential applications . from a system perspective , this corresponds to an increase in the kinds of jobs to manage by the grid and a raise of the qos level that the grid users demand . under this perspective , it is important to find out how the new grid applications can be efficiently managed over a platform that was not designed for their management in its original conception . in particular , the contribution of this thesis is dedicated to the study of jobs with time constraints on execution . currently , these jobs are not efficiently managed by any grid infrastructure . because this topic is general enough , it has been studied from different perspectives , in order to make the research activity consistent and a good basis for further works . for this , the aim is to provide a proper classification of the problem and a contribution both at the theoretical and at the implementation levels . to this regard , this thesis is divided into three main parts . at first , a deep analysis of the state of the art regarding the management of qos constraints is made . the aim of such analysis is double : in primis , it is important for showing the inadequacy of current grid framework solutions for supporting time constraints on grid; moreover , it allows to justify the need of time constraints management on grid through an evaluation of the evolution of the grid under a qos perspective . the second contribution of the thesis is the definition of a proper grid framework architecture , sortgrid , for supporting the management of time constraints on grid . sortgrid framework has been defined and designed starting from the analysis of the state of the art carried out in the first part . in particular , the architectural choices have been made considering the lacks of 
approximating the disambiguation of some german nominalizations by use of weak structural , lexical and corpus information resumen : entre el m todo cl sico y simb lico de desambiguaci n de sentidos ( wsd ) que utiliza representaciones sem nticas profundas de oraciones y textos , y el m todo estad stico que utiliza informaci n relativa a la co-ocurrencia de palabras , existe una tendencia reciente a usar m todos h bridos . de manera similar a la llamada sem ntica lightweight ( marek , 2009 ) , en este art culo se propone hacer uso de escasa informaci n sem ntica . describimos un modelo de aproximaci n sobre la base de flat underspecified discourse representation structures abstract : between classical symbolic word_sense_disambiguation ( wsd ) using explicit deep_semantic representations of sentences and texts and statistical wsd using word co-occurrence information , there is a recent tendency towards mediating methods . similar to so-called lightweight semantics ( marek , 2009 ) we suggest to only make sparse use of semantic_information . we describe an approximation model based upon flat underspecified discourse representation structures ( fudrss , cf . eberle , 2004 ) that weighs knowledge about context structure , lexical semantic restrictions and interpretation preferences . we give a catalogue of guidelines for human annotation of texts by corresponding indicators . using this , the reliability of an analysis tool that implements the model can be tested with respect to annotation precision and disambiguation prediction and how both can be improved by bootstrapping the knowledge of the system using corpus information . for the balanced test corpus considered the recognition_rate of the preferred reading is 80-90% ( depending on the smoothing of parse errors ) . 
research on the effects of mechanical and physical_characteristics on peanut shucking shucking is one of the necessary parts for the deep process of peanut , and has great impact on the quality of peanut kernel . taking the shucking force as the evaluation indicator , the effects of mechanical and physical_characteristics , which include the loading orientation , loading rate , moisture_content , partical size and the number of peanut kernel , on peanut shucking were investigated by means of single factor test . the results show that : 1 ) the loading orientation and moisture_content affect the shucking force significantly , and the needed force is the minimum when the peanut is vertically placed and loaded . with the decrease of moisture_content , the needed force decreases gradually . 2 ) the loading rate , particle size and the number of peanut kernel have no great effect on the shucking force , but the deformation and maximum shucking force vary with the loading rate . additionally , the needed force increases with the number of peanut kernel . the research results can provide some reference for the designment of peanut sheller and selection of the technique parameters . 
robust multiscale stereo_matching from fundus images with radiometric differences a robust multiscale stereo_matching algorithm is proposed to find reliable correspondences_between low contrast and weakly textured retinal image pairs with radiometric differences . existing algorithms designed to deal with piecewise planar surfaces with distinct features and lambertian reflectance do not apply in applications such as 3d reconstruction of medical images including stereo retinal images . in this paper , robust pixel feature vectors are formulated to extract discriminative features in the presence of noise in scale_space , through which the response of low_frequency mechanisms alter and interact with the response of high_frequency mechanisms . the deep_structures of the scene are represented with the evolution of disparity estimates in scale_space , which distributes the matching ambiguity along the scale dimension to obtain globally coherent reconstructions . the performance is verified both qualitatively by face validity and quantitatively on our collection of stereo fundus image sets with ground_truth , which have been made publicly available as an extension of standard test images for performance_evaluation . 
diving decompression models and bubble metrics : modern computer syntheses a quantitative summary of computer models in diving applications is presented , underscoring dual phase dynamics and quantifying metrics in tissue and blood . algorithms covered include the multitissue , diffusion , split phase gradient , linear-exponential , asymmetric tissue , thermodynamic , varying permeability , reduced gradient bubble , tissue bubble diffusion , and linear-exponential phase models . defining relationships are listed , and diver staging regimens are underscored . implementations , diving sectors , and correlations are indicated for models with a history of widespread acceptance , utilization , and safe application across recreational , scientific , military , research , and technical communities . presently , all models are incomplete , but many ( included above ) are useful , having resulted in diving tables , underwater meters , and dive planning software . those herein employ varying degrees of calibration and data tuning . we discuss bubble metrics in tissue and blood as a backdrop against computer models . the past 15 years , or so , have witnessed changes and additions to diving protocols and table procedures , such as shorter nonstop time limits , slower ascent rates , shallow safety stops , ascending repetitive profiles , deep decompression stops , helium based breathing mixtures , permissible reverse profiles , multilevel techniques , both faster and slower controlling repetitive tissue halftimes , smaller critical tensions , longer flying-after-diving surface intervals , and others . stimulated by doppler and imaging technology , table and decompression meter development , theory , statistics , chamber and animal_testing , or safer diving consensus , these modifications affect a gamut of activity , spanning bounce to decompression , single to multiday , and air to mixed_gas diving . as it turns out , there is growing support for many protocols on operational , experimental , and theoretical grounds , with bubble models addressing many concerns on plausible bases , but with further testing or profile data analyses requisite . 
implementation of design_for_test for asynchronous ncl designs in the past two decades , the ic design industry has set what one might refer to as milestones in the golden era of electronics and computers . current statistics reveal that the number of gates on a chip in 2005 is 100k compared to 23k just five years back . with the chip density increasing at this rate , there is an inherent need to allow for some efficient testing mechanism on-chip to avail benefits in terms of quality as well as economy . adding test capabilities to a chip being fabricated increases the initial infrastructure , but the savings that it brings about in terms of cost , time , and maintenance far exceeds the testing cost . in spite of all the innovations , testing asynchronous designs has remained dormant; the reason for this being its inherent complexity . absence of the global clock_signal and presence of more state-holding gates creates a more complex test environment for these designs . the motivation behind this paper stems from the requirement of an efficient testing methodology for a particular class of asynchronous_circuits known as null conventional logic ( ncl ) circuits . the methodology proposed in this paper is easy to use for testing fairly complex designs and is tailored to work with conventional dft tools . the ease of design of synchronous circuits has been the sole reason for widespread development and use of synchronous design techniques . also , cad tools for synchronous_designs have become more advanced and sophisticated allowing total automation of several stages of the design process . however , with clock speeds nearing the gigahertz range and the cmos_technology reaching deep_submicron ranges , serious doubts have been cast over the suitability of synchronous_designs for next_generation processors and systems . problems associated with clock synchronization , power_consumption , and noise in synchronous_designs has forced designers to look for alternatives [7] . 1 . 1 need for asynchronous systems designers are looking at asynchronous_circuits as a potential solution to these problems as they are modular and do not require clock synchronization . some of the possible benefits of asynchronous techniques are listed in table 1 . a variety of approaches exist for the design and implementation of asynchronous_circuits . huffman's model and muller's model form the basis for many of these approaches . asynchronous_circuits fall into two main categories : delay-insensitive and bounded-delay models [13] . table 1 advantages of asynchronous design no global clock low_power average-case performance instead of worst_case less emi no glitch 
book review handbook for language engineers the reader learns from this book's introduction ( written by ali farghaly ) that the objective of the book is to " equip linguists embarking on nlp assignments . " the introduction also explains why language engineers are needed and summarizes the contents of each chapter . " domain analysis and representation " by farghaly and bruce hedin offers a good discussion of the importance of domain analysis in natural_language_processing ( nlp ) . it provides a helpful background on the notion of sublanguages and correctly notes that distinctions between the different domains can be blurred , as domains often overlap . the chapter would have been more convincing if there were further examples of nlp applications that benefit from narrowing down the domain of their operation . the chapter also discusses the analysis of domain into topics . section 2 . 4 . 4 covers statistical approaches to classification , but no references or further reading pointers are given . " the language of the internet " by naomi baron gives an easy-to-read and useful chronological overview of the developments on the internet . it provides information about a number of technologies that are used on the internet , comments on the changing styles of natural_language use , and briefly overviews web markup and programming_languages and the semantic_web . at the same time , i found this chapter too general . it would have been useful if the chapter had covered specific new nlp applications that are relevant to the internet , such as question_answering , and had covered in greater detail low-quality machine_translation for e-mail and chat or text_categorization of web_pages . " grammar writing , testing and evaluation " by miriam butt and tracy holloway king provides a very good and accessible historical and linguistic account of grammars and parsing in nlp . it covers deep_and_shallow parsing and associated techniques as well as testing and evaluation of grammars and parsers . morphological analyzers and part-of-speech taggers are briefly outlined too . a good practical point is the section on documentation of grammar writing . " ontologies " by natalya noy is a concise and plainly written introduction to the topic of ontologies and their development . it clarifies key terms in the ontology development jargon and includes an overview of major ontologies , ontology libraries , and
an approach for designing on-line testable state_machines 1 . introduction synthesis of state_machines have attracted the attention of researchers for more than two decades . several state assignment techniques that result in efficient implementation of the next state logic have been developed [1-3] . however , none of these addresses the testability of an implemented machine . a popular approach for enhancing the testability of a state_machine is to modify its design by using the scan_path technique e . g . lssd [4] . a number of techniques for synthesizing testable state_machine directly from their specifications have also been proposed [5-6] . the goal of these techniques is to make state_machines fully testable for all single stuck_at_faults , and to derive test_sequences for them by using test_generation techniques for combinational circuits . the major problem with these techniques as well as the lssd is that they can only increase the off_line testability i . e . they simplify the detection of permanent_faults . recent_studies show that transient_faults will be the dominant faults in systems designed using deep_submicron_technology [7] . traditional off_line testability approaches cannot guarantee the detection of transient_faults; they have to be detected during normal_operation of a circuit . this in turn requires that circuits be designed so that they have built-in mechanisms for on-line fault_detection . over the years some techniques have been proposed for designing state_machines with on-line fault_detection capability [8-10] . however , these techniques concentrate mainly on the post design modifications rather than designing machines that are on-line testable by design . this paper proposes a new approach for designing state_machines that have built-in capability for enhancing on-line and off_line testability . 
simulating remotely_sensed images of shoaling waves there is substantial current interest in the use of remotely_sensed images to study littoral processes . of particular interest is using the interaction of the ocean waves with the bottom to infer the water_depth in coastal areas . testing the accuracy of processing and analysis_methods requires a known set of bottom depths at many points in the images to compare with depths extracted . such knowledge is not available for most near-shore areas . consequently , we have developed a simulator which produces a time_series of images showing sea_surface elevation for waves moving over a specified piecewise linear bottom of moderate slope . it provides a good approximation to the real sea_surface for a wide range of conditions . hence it can produce controlled datasets for many purposes in shallow_water research . the program uses linear wave theory to model a statistical realization of a deep_water wave field , with a realistic frequency spectrum and directional spreading , and applies the appropriate modifications for shoaling . several examples show that the program produces reasonable results , indicating it should be a useful analytical tool in a range of problems . 
deep_learning using partitioned data vectors deep_learning is a popular field that encompasses a range of multi_layer connectionist techniques . while these techniques have achieved great success on a number of difficult computer_vision problems , the representation biases that allow this success have not been thoroughly explored . in this paper , we examine the hypothesis that one strength of many deep_learning algorithms is their ability to exploit spatially_local statistical information . we present a formal description of how data vectors can be partitioned into sub-vectors that preserve spatially_local information . as a test_case , we then use statistical models to examine how much of such structure exists in the mnist dataset . finally , we present experimental_results from training rbms using partitioned data , and demonstrate the advantages they have over non-partitioned rbms . through these results , we show how the performance advantage is reliant on spatially_local structure , by demonstrating the performance impact of randomly permuting the input data to destroy local structure . overall , our results support the hypothesis that a representation bias reliant upon spatially_local statistical information can improve performance , so long as this bias is a good match for the data . we also suggest statistical tools for determining a priori whether a dataset is a good match for this bias or not . i . introduction in the past few years , the area of exploration known as deep_learning has demonstrated the ability of multilayer con-nectionist networks to achieve good performance on a range of difficult machine_learning_problems , and has been gaining increasing prominence and attention in the machine_learning and neural_network communities . the theoretical basis for understanding why these techniques perform well on particular problems , however , has only begun to be explored . a deeper understanding of how and why deep_learning algorithms work will not only help us to decide what problems are good candidates for their application , but may also suggest ways of improving existing techniques or harnessing their strengths in novel contexts . the hypothesis we seek to examine is the hypothesis that many deep_learning algorithms make use of spatially_local structure in their input data to help them achieve good performance . there has long been an intuition in the deep_learning community that this hypothesis is likely to hold , based partly on everyday experience , and partly on the structure of the human visual_cortex . to our knowledge , however , there has been no previous attempt to formalize and test this hypothesis specifically . 
knowledge matters : importance of prior_information for optimization we explore the effect of introducing prior_information into the intermediate level of deep supervised neural networks for a learning task on which all the black-box state-of-the-art machine_learning_algorithms tested have failed to learn . we motivate our work from the hypothesis that there is an optimization obstacle involved in the nature of such tasks , and that humans learn useful intermediate concepts from other individuals via a form of supervision or guidance using a curriculum . the experiments we have conducted provide positive evidence in favor of this hypothesis . in our experiments , a two-tiered mlp architecture is trained on a dataset for which each image input contains three sprites , and the binary target class is 1 if all three have the same shape . black-box machine_learning_algorithms only got chance on this task . standard deep supervised neural_networks also failed . however , using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts ( the presence of each object ) allows to nail the task . much better than chance but imperfect results are also obtained by exploring architecture and optimization variants , pointing towards a difficult optimization task . we hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks . our findings are also consistent with hypotheses on cultural learning inspired by the observations of effective local_minima ( possibly due to ill-conditioning and the training procedure not being able to escape what appears like a local minimum ) . 
an explicit expression for the newton direction on the complex grassmann manifold we have derived the basic statistical properties for the estimated rotary coefficient . these depend on the true value of the rotary coefficient , and the conjugate coherence 2 3 ; a nuisance parameter . fortunately when the latter is estimated and debiased constructed confidence_intervals maintain appropriate coverage probabilities , so such confidence_intervals have practical utility as illustrated by the labrador_sea current data_analysis . acknowledgment the authors would like to thank j . lilly for making the labrador_sea data available to them . helpful comments and observations by the reviewers were much appreciated . a . benignus , " estimation of the coherence spectrum and its confidence_interval using the fast_fourier_transform , " ieee trans . nonnull distribution of likelihood_ratio criterion for reality of covariance_matrix , " j . multi-a rotary component method for analysing meteorological and oceanographic vector time_series , " deep_sea res . [9] y . hayashi , " space-time spectral analysis of rotary vector series , " j . a variance equality test for two correlated complex gaussian variables with application to spectral power comparison , " ieee trans . a technique for the cross spectrum analysis of pairs of complex_valued time_series , with emphasis on properties of polarized components and rotational invariants , " deep_sea res . the variance of mul-titaper spectrum estimates for real gaussian processes , " ieee trans . the effective band-width of a multitaper spectral estimator , " biometrika , vol . 82 , pp . abstract several important design problems in signal_processing for communications can be cast as optimization_problems in which the objective is a function of the subspaces spanned by tall complex matrix variables with orthonormal columns . such problems can be viewed as optimization_problems on the complex grassmann manifold , and an effective means for performing this optimization is to use a grassmannian version of newton's_method . to facilitate the implementation of that method , we provide an explicit expression for the grassmannian newton direction for an arbitrary twice differentiable_function . we also use an example in which the pairwise chordal frobenius norm between subspaces is to be optimized to outline a systematic procedure for obtaining the hessian_matrix . 
variance_reduction using wafer patterns in i_ddq data the subject of this paper is i ddq testing for deep_sub_micron_cmos technologies . the key concept introduced is the need to reduce the variance of good and faulty i_ddq distributions . other i_ddq based_techniques are reviewed within the context of variance_reduction . using the sematech data and production data , variance_reduction techniques are demonstrated . the main contribution of the paper is the systematic use of the die location and patterns in the i ddq data to reduce variance . variance_reduction is completed before any i_ddq threshold limits are set . challenge the use of i_ddq for deep_sub_micron_technologies is in question because of increased leakage_current [1] . this poses an important challenge to the ic test industry because i_ddq tests have demonstrated good fault_coverage for difficult to detect defects with small sets of test_vectors . the scaling of v t and l eff produces faster parts but also higher leakage . die , wafer and lot differences further complicate the interpretation of i_ddq . t he increases in the normal range of i_ddq currents for healthydevices makes it more difficult to distinguish them from faulty devices . there are different interpretations of the term variance_reduction . when measuring i_ddq values , variance is reduced by repeated measurements of the same test_vector . this is done to reduce the effects of noise in the experimental system . asecond interpretation of variance_reduction relates to the spread of the distribution formed from i_ddq measurements of tens to hundreds of test_vectors . in this paper it is shown that estimates of an i_ddq value can be made solely from the measurements and that this residual has a variance smaller than the original i_ddq distribution . it is the second interpretation of variance_reduction the key contribution of the paper is the systematic use the die location and patterns in the i ddq data to reduce variance . through post_processing of ate i_ddq data new pass/fail criteria are defined . as a result , die with high intrinsic leakage are re-binned as passes and in-line inspection is used to identify real defects on the re-binned die . the paper begins with a reviewofrecent i_ddq research viewed from a perspective of i_ddq variance_reduction of good and faulty die . based on this review , next_generation i_ddq test_methods should concentrate on reducing variance before setting threshold limits . using the 
hierarchical fault response modeling of analog/rf circuits i abstract in this thesis two methodologies have been proposed for evaluating the fault response of analog/rf circuits . these proposed approaches are used to evaluate the response of the faulty circuit in terms of specifications/measurements . faulty response can be used to evaluate important test metrics like fail probability , fault coverage and yield coverage of given measurements under process_variations . once the models for faulty and fault_free circuit are generated , one needs to perform monte_carlo sampling ( as opposed to monte_carlo simulations ) to compute these statistical parameters with high_accuracy . the first method is based on adaptively determining the order of the model based on the error budget in terms of computing the statistical metrics and position of the threshold ( s ) to decide how precisely necessary models need to be extracted . in the second method , using hierarchy in process_variations a hybrid of heuristics and localized linear models have been proposed . experiments on lna and mixer using the adaptive model order selection procedure can reduce the number of necessary simulations by 7 . 54x and 7 . 03x respectively in the computation of fail probability for an error budget of 2% . experiments on lna using the hybrid approach can reduce the number of necessary simulations by 21 . 9x and 17x for four and six output parameters cases for improved accuracy in test statistics estimation . ii_acknowledgments although this thesis book lists only one author , in reality the ideas it molds together were contributed and refined by many extraordinarily insightful colleagues . my first thanks go to the almighty for directing me to prof . sule ozev to conduct this masters research . on most days i leave her office after the meeting wondering if i have really got the benefit of my 18 years of science_education . my interactions with her have proved time and again that learning is a continuous process and this gives me great deal of enthusiasm to delve deep into new and obscure topics . i thank her for all the guidance during this enduring period of research . i would also like to thank christen for promptly agreeing to serve on the defense committee . i am indebted to my parents who have for all the motivation , emotional and financial_support much needed during my educational career . i am thankful to ender yilmaz , afsaneh nassery , osman erol and other members of prof . ozev's research_group for all the fruitful discussions not only restricted to research , but 
text_to_text similarity of sentences text_to_text similarity of sentences introduction computational approaches to language_understanding can be classified into two major categories : true-understanding and text_to_text similarity . in true understanding , the goal is to map language statements onto a deep_semantic representation that relate language constructs to world and domain_knowledge . current_state-of-the-art approaches that fall into this true-understanding category offer adequate solutions only in very limited contexts ( i . e . toy-domains ) lacking scalability and thus having limited use in real_world_applications such as summarization or intelligent_tutoring_systems . abstract assessing the semantic similarity between two texts is a central task in many applications , including sum-marization , intelligent_tutoring_systems , and software_testing . similarity of texts is typically explored at the level of word , sentence , paragraph , and document . the similarity can be defined quantitatively ( e . g . in the form of a normalized value between 0 and 1 ) and qualitatively in the form of semantic_relations such as elaboration , entailment , or paraphrase . in this chapter , we focus first on measuring quantitatively and then on detecting qualitatively sentence-level text_to_text semantic_relations . a generic approach that relies on word-to-word similarity_measures is presented as well as experiments and results_obtained with various instantiations of the approach . in addition , we provide results of a study on the role of weighting in latent_semantic_analysis , a statistical technique to assess similarity of texts . the results were obtained on two data_sets : a standard data_set on sentence-level paraphrase detection and a data_set from an intelligent_tutoring_system . text_to_text similarity approaches ( t2t ) to text semantic_analysis avoid the hard task of true understanding by defining the meaning of a text based on its similarity to other texts , whose meaning is assumed to be known . such methods are called benchmarking methods as they rely on a benchmark text , analyzed by experts , to indentify the meaning of new , unseen texts . we adopt in this chapter a t2t approach to semantic text analysis . in particular , we focus on the task of quantifying how similar two texts are , and based on this , we then decide whether they are similar enough to be considered a paraphrase or not . an example of two texts , a textbase ( t ) and student paraphrase ( sp; reproduced as typed by the student in istart , an intelligent_tutoring_system; mcnamara , levin-stein , & boonthum , 2004 ) , is provided below ( from the user language paraphrase challenge; mccarthy & mcnamara , 2008 ) : t : during vigorous exercise , the heat generated by working muscles can increase 
toward realizing a pram-on-a-chip vision serial computing has become largely irrelevant for growth in computing performance at around 2003 . having already concluded that to maintain past performance growth rates , general_purpose computing must be overhauled to incorporate parallel_computing at all levels of a computer system including the programming model all processor vendors put forward many-core roadmaps . they all expect exponential increase in the number of cores over at least a decade . this welcome development is also a cause for apprehension . the whole world of computing is now facing the challenge of coming up with a truly general_purpose parallel computing_platform the same challenge that eluded high_performance_computing ( hpc ) for so many years-and the clock is ticking . it is becoming common knowledge that if you want your program to run faster you will have to program for parallelism , but the vendors who set up the rules have not yet provided clear and effective means ( e . g . , programming models and languages ) for doing that . how can application_software vendors be expected to make a large investment in new software developments , when they know that in a few years they are likely to have a whole new set of options for getting much better performance ? ! namely , we are already in a problematic transition stage that slows down performance growth , and may cause a recession if it lasts too long . aware of the ills of hpc , some industry leaders are already predicting that the transition period can last a full decade . the pram-on-chip project started at umd in 1997 foreseeing this challenge and opportunity . building on pram a parallel algorithmic approach that has never been seriously challenged on ease of thinking , or wealth of its knowledge_base a comprehensive and coherent platform for on-chip general_purpose parallel_computing has been developed and prototyped . optimizing single-task completion time , the platform accounts for application programming ( vhdl/verilog , opengl , matlab , etc ) , parallel_algorithms , parallel_programming , compiling , architecture and deep_submicron implementation , as well as backward_compatibility on serial code . the approach goes after any type of application parallelism regardless of its amount , regularity , or grain_size . some prototyping highlights include : an explicit multi-threaded ( xmt ) architecture , a new 64-processor , 75mhz xmt ( fpga-based ) computer , 90nm asic tapeout of the key interconnection network component , a basic compiler , class tested programming methodology where ( even high_school ) students are taught only parallel_algorithms and pick the rest on their own , and up to 100x speedups on applications . the talk 
deep - differential evolution entirely parallel method for gene regulatory networks the differential evolution entirely parallel ( deep ) method is applied to the biological_data fitting problem . we introduce a new migration scheme , in which the best member of the branch substitutes the oldest member of the next branch that provides a high_speed of the algorithm convergence . we analyze the performance and efficiency of the developed algorithm on a test problem of finding the regulatory interactions within the network of gap genes that control the development of early drosophila embryo . the parameters of a set of nonlinear differential_equations are determined by minimizing the total error between the model behavior and experimental observations . the age of the individuum is defined by the number of iterations this individuum survived without changes . we used a ring topology for the network of computational nodes . the computer codes are available upon request . 
understanding the inflexibility of process_integration the objective of this study is to establish a base for understanding inflexibility , with different kinds of integration problems being examined and explained . studies of process_integration have focused mostly on the design and management of efficient operation with information_technology but very little on the difficulty in making changes with tightly linked processes . in order to eliminate risks of integration failure , there is a need for deep understanding of the types and causes of inflexibility with process_integration . based on the literature and reported cases of process_integration and enterprise flexibility/inflexibility , this study proposes that inflexibility can be classified as either operational , organizational , or systems inflexibility . the incompleteness of business design and the misalignment of users' and system designers' capability seem to be key sources of inflexibility . though the propositions are to be further tested , the proposed framework with different categories of inflexibility and different sources for inflexibility provides a reference for planning effective management of process_integration . 
eliminating non-determinism during test of high_speed source synchronous differential buses the at-speed functional_testing of deep_sub_micron devices equipped with high_speed i/o ports and the asynchronous nature of such i/o transactions poses significant challenges . in this paper , the problem of non-determinism in the output response of the device-under-test ( dut ) is described . this can arise due to limited automated test_equipment ( ate ) edge placement accuracy ( epa ) in the source synchronous clock of the stimulus stream to the high_speed i/o port from the tester . a simple yet effective solution that uses a trigger signal to initiate a deterministic transfer of test inputs into the core clock domain of the dut from the high_speed i/o port is presented . the solution allows the application of at-speed functional patterns to the dut , while incurring a very small hardware_overhead and trivial increase in test application time . an analysis of the probability of non-determinism as a function of clock_speed and epa is presented . it shows that as the frequency of operation of high_speed i/os continues to rise , non-determinism will become a significant problem that can result in an unacceptable yield_loss . 
a novel mems silicon probe_card we have developed a novel cantilever-type probe which is capable of less than 70 of pitch and 12g of force . this probe is suitable for wafer_level burn-in testing , function testing and circuit_board o/s testing including memory and rf devices . the probe was fabricated with epitaxial polysilicon on silicon substrate . the through via hole interconnection was formed in silicon wafer by nickel electroless plating and cupper electroplating . the electroless plating is easy method and can deposit film uniformly for deep trench and through hole . especially , it can deposit film on any substrates without seed layer and allow it to be electroplated . the aspect ratio of through via hole was larger than 10 : 1 and the contact resistance was less than 1 ohm . 
incremental evolution of animats' behaviors as a multi_objective optimization evolutionary_algorithms have been successfully used to create controllers for many animats . however , intuitive fitness functions like the survival time of the animat , often do not lead to interesting results because of the bootstrap problem , arguably one of the main challenges in evolutionary_robotics : if all the individuals perform equally poorly , the evolutionary process cannot start . to overcome this problem , many authors defined ordered sub-tasks to bootstrap the process , leading to an incremental evolution scheme . published methods require a deep knowledge of the underlying structure of the analyzed task , which is often not available to the experimenter . in this paper , we propose a new incre-mental scheme based on multi_objective evolution . this process is able to automatically switch between each sub-task resolution and does not require to order them . the proposed method has been successfully tested on the evolution of a neuro-controller for a complex-light seeking simulated robot , involving 8 sub-tasks . 
affordable and effective screening of delay_defects in asics using the inline resistance fault_model transition delay_fault ( tdf ) testing has become a necessary test_method in very deep sub micron ( vdsm ) technologies due to the presence of resistive defects that cause subtle timing_failures . the transition delay_fault_model is based on a slow-to-rise and slow-to-fall fault at each node in the circuit . some resistive defects such as resistive vias actually induce both faults and the tdf test_set can contain unnecessary test patterns for proper screening of this type of defect . the inline resistance fault ( irf ) model more accurately represents this defect type and is studied in depth in this paper . atpg experimental results show that irf patterns can be generated 1 . 4 to 1 . 8 times_faster with 45% to 58% fewer patterns than traditional tdf patterns . irf and tdf pattern test_results are presented and show that the more expensive tdf remains a more comprehensive test than irf as expected , but that the quality impact of using only the irf test_set is minimal , especially when combined with effective iddq outlier screening such as statistical_post_processing . additionally , a methodology is presented for the determination of the number of delay_defects that behave according to each model from the test data alone , which is necessary to accurately determine delay defect_coverage from multiple test_coverage metrics . 1 . introduction testing devices in production is meant to screen out defective parts . tests have to be generated that ensure a high probability of uncovering physical_defects like shorts to ground or vdd , bridges to neighboring signal lines , incorrect resistance of lines or vias , which negatively affect the speed of the device , and many more . tests are based on fault_models that abstract physical_defects . fault_models provide a practical approach for tools , which generate or evaluate tests , such as the automated test_pattern_generation ( atpg ) . good fault_models enable the atpg tool to be both , efficient and effective . the efficiency of the tool relies on fault_models that are easy to handle within the tool . the effectiveness of the tool requires fault_models that allow the tool to generate not only the fewest number of patterns , but patterns that are at the same time of high_quality , thus detecting the highest number of physical_defects . traditionally , test_pattern_generation is based on the stuck_at_fault ( saf ) model [1 , 2] . however , geometry features of 0 . 18 m and below , as well as n ew materials and processes , cause new types 
format-transforming encryption : more than meets the dpi nation_states and other organizations are increasingly deploying deep_packet_inspection ( dpi ) technologies to censor internet_traffic based on application_layer content . we introduce a new dpi circumvention approach , format-transforming encryption ( fte ) , that cryptographically transforms the format of arbitrary plaintext data ( e . g . packet contents ) into specified formats that are designed to bypass dpi tests . we show how to build a general_purpose fte system , in which these formats are defined compactly by families of regular_expressions . moreover , we specify and implement a full fte record-layer protocol . we exhibit formats that are guaranteed to avoid known filters , and give a framework for learning formats from non-censored http traffic . these formats are put to use in our fte record layer , to explore trade_offs between performance and steganographic capabilities . as one example , we visit the top 100 alexa webpages through an fte tunnel , incurring an average overhead of roughly 5% . 
learning feature hierarchies with centered deep boltzmann machines deep boltzmann machines are in principle powerful models for extracting the hierarchical structure of data . unfortunately , attempts to train layers jointly ( without greedy layer-wise pretraining ) have been largely unsuccessful . we propose a modification of the learning_algorithm that initially recenters the output of the activation functions to zero . this modification leads to a better conditioned hessian and thus makes learning easier . we test the algorithm on real_data and demonstrate that our suggestion , the centered deep boltzmann machine , learns a hierarchy of increasingly abstract representations and a better generative model of data . 
suppression of deep_brain_stimulation artifacts from the electroencephalogram by frequency_domain hampel filtering . objective currently , electroencephalography ( eeg ) cannot be used to record cortical activity during clinically effective dbs due to the presence of large stimulation artifact with components that overlap the useful spectrum of the eeg . a filtering method is presented that removes these artifacts whilst preserving the spectral and temporal fidelity of the underlying eeg . methods the filter is based on the hampel identifier that treats artifacts as outliers in the frequency_domain and replaces them with interpolated values . performance of the filter was tested with a synthesized dbs signal and actual data recorded during bilateral monopolar dbs . results mean increases in signal_to_noise_ratio of 7 . 8db for single-frequency stimulation and 13 . 8db for dual-frequency stimulation are reported . correlation analysis between eeg with synthesized artifacts and artifact-free eeg reveals that distortion to the underlying eeg in the filtered signal is negligible ( r ( 2 ) >0 . 99 ) . conclusions frequency_domain hampel filtering has been shown to remove monopolar dbs artifacts under a number of common stimulation conditions used for the treatment of parkinson's_disease . significance application of frequency_domain hampel filtering will allow the measurement of eeg in patients during clinically effective dbs and thus may increase our understanding of the mechanisms of action of this important therapeutic intervention . 
interconnect performance estimation models for design planning this paper presents a set of interconnect performance estimation models for design planning with consideration of various effective interconnect layout optimization_techniques , including optimal wire sizing , simultaneous driver and wire sizing , and simultaneous buffer_insertion/sizing and wire sizing . these models are extremely efficient , yet provide high degree of accuracy . they have been tested on a wide range of parameters and shown to have over 90% accuracy on average compared to running best-available interconnect layout optimization algorithms directly . as a result , these fast yet accurate models can be used efficiently during high-level_design space_exploration , interconnect-driven design planning/synthesis , and timing-driven placement to ensure design convergence for deep submicrometer designs . 
parsing german topological fields with probabilistic context_free grammars abstract parsing german topological fields with probabilistic context_free grammars a research paper submitted in conformity with the requirements for the degree of m . sc . 2009 syntactic_analysis is useful for many natural_language_processing applications requiring further semantic_analysis . recent_research in statistical parsing has produced a number of high_performance parsers using probabilistic context_free ( pcfg ) models to parse english text , such these methods to parse sentences in freer-word_order languages . such languages as russian , warlpiri , and german feature syntactic constructions that produce discontinuous constituents , directly violating one of the crucial assumptions of context_free models of syntax . while pcfg technologies may thus be inadequate for full syntactic_analysis of all phrasal structure in these languages , clausal structure can still be fruitfully parsed with these methods . in particular , we examine applying pcfg parsing to parse the topological field structure of german . these topological fields provide a high-level description of the major sections of a clause in relation to the clausal main verb and the subordinating heads and appear in strict linear sequences amenable to pcfg parsing . they are useful for tasks such as deep syntactic_analysis , part_of_speech_tagging and coreference resolution . in this work , we apply an unlexicalized , latent_variable-based parser ( petrov et al . , 2006 ) to topological field parsing , and achieve state-of-the-art parsing results on two german newspaper corpora without any language-or model-dependent adaptation . we perform a qualitative error analysis of the parser output , and identify constructions like ellipses and parentheticals as the chief sources of remaining error . this is confirmed by a 3 further experiment in which parsing performance improves after restricting the training and test_set to those sentences without these constructions . we also explore techniques for further improving parsing results . for example , discrimina-tive reranking of parses made by a generative parser could incorporate linguistic_information such as those derived by our qualitative analysis . self-training is another semi_supervised technique which utilizes additional unannotated data for training . 4 acknowledgements many people have contributed to the making of this document , whom i would like to thank here . first and foremost , i would like to thank my supervisor and mentor , gerald penn . i am continually amazed by the breadth and depth of his knowledge . through this , he has given me a much better perspective of computational_linguistics , yet i know that i have much more to learn from him in the future . i would also like to thank graeme hirst for his role as the second reader of this paper . his comments have 
revealing non-analytic kinematic shifts in smooth goal_directed behaviour how do biological agents plan and organise a smooth accurate path to shift from one smooth mode of behaviour to another as part of graceful movement that is both plastic and controlled ? this paper addresses the question in conducting a novel shape analysis of approach and adjustment phases in rapid voluntary target aiming and 2-d reaching hand actions . a number of mode changing experiments are reported that investigate these actions under a range of goals and conditions . after a typically roughly aimed approach , regular projective adjustment is observed that has height and velocity kinematic profiles that are scaled copies of one another . this empirical property is encapsulated as a novel self-similar shift function . the mathematics shows that the biological shifts consist of continual deviation from their full taylor_series everywhere throughout their interval , which is a deep form of plasticity not described before . the experimental results find the same approach and adjustment strategy to occur with behavioural trajectories over the full and varied range of tested goals and conditions . the trajectory shapes have a large degree of predictability through using the shift function to handle extensive variation in the trajectories' adjustment across individual behaviours and subjects . we provide connections between the behavioural features and results and various neural studies to show how the methodology may be exploited . the conclusion is that a roughly aimed approach followed by a specific highly plastic shift adjustment can provide a regular basis for fast_and_accurate goal_directed motion in a simple and generalisable way . 
toward nature inspired computing 1 formulation and characteristics of nic in this article , we take a look at an emerging computing paradigm called nature inspired computing ( nic ) . we examine the impacts of nic in two aspects . first , nic enables us to explain the underlying mechanism of a real-world complex system by formulating computing models and testing hypotheses through controlled experimentation . the end product of such computing experiments is a deep understanding or a new discovery of the real working mechanism of the modeled system . second , nic enables us to embody autonomous ( e . g . , lifelike ) behavior in solving computing problems . with detailed knowledge of the underlying mechanism , abstracted autonomous behavior can be used as a model for a general_purpose problem_solving strategy or method . generally speaking , the objectives of nic are twofold : ( 1 ) characterizing and understanding complex phenomena or systems behavior , and ( 2 ) designing and developing computing solutions to hard problems . neither of these objectives can be achieved without formulating a model of the factors underlying a complex system . the modeling process can be started with a theoretical analysis from a macroscopic or microscopic view of the system . alternatively , a blackbox or whitebox approach may be adopted . blackbox approaches such as markov models or artificial_neural_networks normally do not tell us much about the working mechanism . on the other hand , whitebox approaches such as agents with bounded_rationality are more useful for explaining behavior [10] . nic formulation . the essence of nic formulation lies in the conception of a computing system that is operated by population ( s ) of autonomous entities . the rest of the system is referred to as the environment . an autonomous entity consists of a detector ( or a set of detectors ) , an effector ( or again , a set of effectors ) , and a repository of local behavior rules ( see figure 1 ) [5][8] . 
silicon_debug of systems-on-chips modern semiconductor process_technologies , advanced design_tools , and the reinvented reuse paradigm enable the design of very complex ics . some call these ics 'system-on-chip' , referring to the fact that their functionality could until recently only be implemented by one or several pcbs filled with ics . while it was always difficult to locate design errors , guaranteeing that a deep_sub_micron 'system-on-chip' is design error free is a real challenge . floating specifications , growing geographically-spread design teams , time-to-market pressure , and the increasing distance of ic designers to actual silicon all make it likely that 'buggy' hardware will become as common as 'buggy' software . of course our industry does whatever is possible within given time and money budgets to prevent design errors before first silicon . hereto techniques as simulation , emulation , and formal_verification are used . however , all these techniques only deal with models of the ic , which do not take into account all effects that might occur on real silicon , and high computational costs often prevent exhaustive error coverage . in order to find design errors before the customer does , debug of actual silicon samples is inevitable . this hot_topic session provides an overview of the state-of-the-art in physical and electrical silicon_debug . the speakers address techniques currently in use , their applications and their limitations , and the research challenges for the future . ( ejm ) silicon_debug can be needed at various stages in the lifecycle of a product ( asic , system ) : during development , qualification , production ramp-up , or in maintenance-like activities in the field life of a product . the root cause of the problem-to-be-debugged can lie in the technology used , as well as in the design implementation ( or the combination of these ) ; the debugging process has to cover both aspects . although it is clear that software tools and design_for_test are gaining in importance with the advent of systems-on-chip , physical de-bug methods have still an important role to play . one clear example is debugging of problems that are only visible by an increased supply_current . the presentation gives an overview of the different categories of physical debug methods , and illustrates their role in the total debug process . physical methods for internal electrical analysis ( e . g . , probing , e-beam test ) , both on active circuitry and on isolated building blocks . physical failure localisation techniques , based on thermal effects , photon emission , or the interaction of the circuitry with electron , ion , or photon beams . physical device modification ( e . g . , using focused_ion_beam ) to verify 
state estimation in power_distribution networks with poorly synchronized measurements we consider the problem of designing a state estimation architecture for the power_distribution grid , capable of collecting raw measurements from low-end phasor measurement units , processing the data in order to minimize the effects of measurement noise , and presenting the state estimate to the control and monitoring applications that need it . the proposed approach is leader-less , scalable , and consists in the distributed solution of a least square problem which takes into explicit consideration the inexact time synchronization of inexpensive pmus . two algorithms are proposed : the first one is a specialization of the alternating directions method of multipliers ( admm ) for which a scalable implementation is proposed; the second one is an extremely lightweight jacobi-like algorithm . both algorithms can be implemented locally by local data aggregators , or by the individual pmus , in a peer_to_peer fashion . the proposed approach is validated via simulations on the ieee 123 test feeder , considering the ieee_standard c37 . 118-2005 for pmus . we also analyze the performance of a distributed control_algorithm that makes use of the grid state estimation and that we adopted as a prototype . we show that , via the proposed state estimation architecture , feedback_control of the distribution grid becomes viable also with poorly synchronized sensors , and possibly also without gps-based synchronization modules . i . introduction the electric grid is currently undergoing a deep renovation process towards the so-called smart_grid , featuring larger hosting capacity , widespread penetration of renewable_energy_sources , better quality of the service , and higher reliability . one of the major aspects of this modernization is the widespread deployment of dispersed measurement , monitoring , and actuation devices . in this paper , we consider one specific thrust , which is the deployment of phasorial measurement units ( pmus ) in the medium and low_voltage power_distribution grid . the presence of pmus is quite uncommon in today's power_distribution networks . however , this scenario has become the subject of recent_research efforts in the power systems community , including a 3-year research_project involving university of california together with the power standards lab and lawrence berkeley national lab [1] . previous investigation on the subject includes the experimental deployment and the extensive measurement campaign in [2] , and the analysis proposed in [3] , [4] , [5] for the possible applications of such measurement devices , also called pmus , in the power_distribution grid . 
sequence based residue_depth prediction using evolutionary information and predicted secondary_structure background residue_depth allows determining how deeply a given residue is buried , in contrast to the solvent accessibility that differentiates between buried and solvent-exposed_residues . when compared with the solvent accessibility , the depth allows studying deep_level structures and functional sites , and formation of the protein_folding nucleus . accurate prediction of residue_depth would provide valuable information for fold recognition , prediction of functional sites , and protein_design . results a new method , rdpred , for the real-value depth prediction from protein sequence is proposed . rdpred combines information extracted from the sequence , psi-blast scoring matrices , and secondary_structure predicted with psipred . three-fold/ten-fold_cross_validation based tests performed on three independent , low-identity datasets show that the distance based depth ( computed using msms ) predicted by rdpred is characterized by 0 . 67/0 . 67 , 0 . 66/0 . 67 , and 0 . 64/0 . 65 correlation with the actual depth , by the mean absolute errors equal 0 . 56/0 . 56 , 0 . 61/0 . 60 , and 0 . 58/0 . 57 , and by the mean relative errors equal 17 . 0%/16 . 9% , 18 . 2%/18 . 1% , and 17 . 7%/17 . 6% , respectively . the mean absolute and the mean relative errors are shown to be statistically significantly better when compared with a method recently proposed by yuan and wang [proteins 2008; 70 : 509-516] . the results show that three-fold_cross_validation underestimates the variability of the prediction quality when compared with the results based on the ten-fold_cross_validation . we also show that the hydrophilic and flexible residues are predicted more accurately than hydrophobic and rigid residues . similarly , the charged residues that include lys , glu , asp , and arg are the most accurately predicted . our analysis reveals that evolutionary information encoded using pssm is characterized by stronger correlation with the depth for hydrophilic amino_acids ( aas ) and aliphatic aas when compared with hydrophobic aas and aromatic aas . finally , we show that the secondary_structure of coils and strands is useful in depth prediction , in contrast to helices that have relatively uniform distribution over the protein depth . application of the predicted residue_depth to prediction of buried/exposed_residues shows consistent improvements in detection rates of both buried and exposed_residues when compared with the competing method . finally , we contrasted the prediction performance among distance based ( msms and dpx ) and volume based ( sadic ) depth definitions . we found that the distance based indices are harder to predict due to the more complex nature of the corresponding depth profiles . conclusion the proposed method , rdpred , provides statistically significantly better predictions of residue_depth when compared with the competing method . the predicted depth can be used to provide improved prediction of both buried and exposed_residues . the prediction of exposed_residues has implications in characterization/prediction of interactions with ligands and other proteins , while the prediction of buried residues could be used in the context of folding predictions and simulations . 
the use of on-line co-training to reduce the training_set size in pattern_recognition methods : application to left ventricle segmentation in ultrasound the use of statistical pattern_recognition models to segment the left ventricle of the heart in ultrasound images has gained substantial attention over the last few years . the main obstacle for the wider exploration of this methodology lies in the need for large annotated training sets , which are used for the estimation of the statistical_model parameters . in this paper , we present a new on-line co-training methodology that reduces the need for large training sets for such parameter_estimation . our approach learns the initial parameters of two different models using a small manually annotated training_set . then , given each frame of a test_sequence , the methodology not only produces the segmen-tation of the current frame , but it also uses the results of both classifiers to retrain each other incrementally . this on-line aspect of our approach has the advantages of producing segmentation results and retraining the classifiers on the fly as frames of a test_sequence are presented , but it introduces a harder learning setting compared to the usual off_line co-training , where the algorithm has access to the whole set of un-annotated training_samples from the beginning . moreover , we introduce the use of the following new types of classifiers in the co-training framework : deep_belief_network and multiple model probabilistic data association . we show that our method leads to a fully automatic left ventricle segmentation system that achieves state-of-the-art accuracy on a public database with training sets containing at least twenty annotated images . 
fluxo : a system for internet_service programming by non-expert developers over the last 10-15 years , our industry has developed and deployed many large_scale internet_services , from e_commerce to social_networking sites , all facing common challenges in latency , reliability , and scalability . over time , a relatively small number of architectural_patterns have emerged to address these challenges , such as tiering , caching , partitioning , and pre- or post_processing compute intensive tasks . unfortunately , <i>following</i> these patterns requires developers to have a deep understanding of the trade_offs_involved_in these patterns as well as an end_to_end understanding of their own system and its expected workloads . the result is that non-expert developers have a hard time applying these patterns in their code , leading to low-performing , highly suboptimal applications . in this paper , we propose fluxo , a system that separates an internet service's logical functionality from the architectural decisions made to support performance , scalability , and reliability . fluxo achieves this separation through the use of a restricted programming_language designed 1 ) to limit a developer's ability to write programs that are incompatible with widely used internet_service architectural_patterns; and 2 ) to simplify the analysis needed to identify how architectural_patterns should be applied to programs . because architectural_patterns are often highly dependent on application performance , workloads and data distributions , our platform captures such data as a runtime profile of the application and makes it available for use when determining how to apply architectural_patterns . this separation makes service development accessible to non-experts by allowing them to focus on application features and leaving complicated architectural optimizations to experts writing application-agnostic , profile-guided optimization tools . to evaluate fluxo , we show how a variety of architectural_patterns can be expressed as transformations applied to fluxo programs . even simple heuristics for automatically applying these optimizations can show reductions in latency ranging from 20-90% without requiring special effort from the application developer . we also demonstrate how a simple shared-nothing tiering and replication pattern is able to scale our test_suite , a web_based im , email , and addressbook application . 
total dose and single_event_effects ( see ) in a 0 . 25 m cmos_technology individual transistors , resistors and shift_registers have been designed using radiation tolerant layout practices in a commercial quarter micron process . a modelling effort has led to a satisfactory formulation for the effective aspect ratio of the enclosed transistors used in these layout practices . all devices have been tested up to a total dose of 30mrad ( sio2 ) . the threshold_voltage shift after irradiation and annealing was about +45mv for nmos and-55mv for pmos transistors , no leakage_current appeared , and the mobility degradation was below 6% . the value of resistors increased by less than 10% . noise measurements made on transistors with w=2mm and l varying between 0 . 36 and 0 . 64 m revealed a corner noise frequency of about 200khz for the nmos and 12khz for the pmos . irradiation up to 30mrad ( sio2 ) did not significantly affect the noise performance . the shift_registers continuously operated at 1 . 25mhz during the irradiation , and no error was detected in the pattern propagation . no functional degradation was observed . an irradiation with a heavy ion_beam was made on the shift_registers to study their sensitivity to single_event_effects ( see ) . no single_event latch-up ( sel ) was observed up to a let of 89 mevcm 2 mg-1 . the register designed using dynamic logic , with a threshold let lower than 3 . 2 mevcm 2 mg-1 , proved to be considerably more sensitive to single_event_upset ( seu ) than its static logic counterpart , which had a threshold let of about 15 mevcm 2 mg-1 . a novel seu-tolerant design was demonstrated to be extremely effective as storage cell . 1 . motivation in agreement with the first measurements on ultra-thin oxides [1] , recent_studies confirmed the total dose hardness of the thin oxide of a commercial 0 . 25 m cmos_technology [2] . the use of radiation tolerant layout practices , based on the systematic use of enclosed nmos transistors and guardrings , was demonstrated to extend the tolerable total dose level well beyond the inherent technology limit [3] . all these results are extremely promising in view of the possible use of deep_submicron_technologies for the readout electronics of lhc . in this respect , some important issues not addressed in references [2] and [3] still needs to be studied . how to estimate the correct effective aspect ratio of enclosed geometry transistors , an important parameter entering in the design of a circuit , is not known . the noise of transistors in deep submicron technologies needs to be characterised . moreover , no investigation of single_event_effects 
power_supply transient_signal_analysis under real process and test hardware models a device testing method_called transient_signal_analysis ( tsa ) is subjected to elements of a real process and testing environment in this paper . simulations experiments are designed to determine the effects of process skew ( obtained from measured parameters of a real process ) on the accuracy of tsa in estimating path_delays from power_supply i ddt and v ddt waveforms . the circuit model is designed to test tsa under deep_submicron process models that incorporate advanced parameters such as transistor v t width dependencies . modeling elements of a testing environment including the probe_card are subsequently introduced as a means of evaluating the effects of tester measurement noise in an actual implementation . a testing method that uses power_supply transient_signals as a means of determining path_delay characteristics of digital devices is attractive for several reasons . first , such a method may be useful in detecting resistive shorting and open defects; defects that traditionally have not been targeted by stuck fault based_methods . second , reliable delay_fault tests are difficult to generate ( because of circuit hazards ) and apply ( because of structural tester timing accuracy ) . the global observability provided by power_supply transient_signals permits the measurement of delay without the need to sensitize paths to observation points such as primary outputs or scan-latches . third , the supply tran-sients potentially provide a higher degree of resolution than logic-based_techniques with respect to the parametric characteristics of the device . this information may be useful as a means of reducing delay_fault_test coverage constraints . preliminary investigations have demonstrated that a method_called transient_signal_analysis ( tsa ) is capable of detecting delay_faults [1] and estimating path_delays in defect_free devices [2] . the focus of this work is to determine the robustness of transient_signal_analysis ( tsa ) to advanced process and tester environment elements . to this end , simulations are conducted on a circuit model derived using the measured circuit_parameters of a real process . for example , the simulation rc-transistor models are based on a sets of specifications sampled from the tsmc's 0 . 25 m process [3] . the bsim transistor models used in this work incorporate important deep_submicron parameters such as transistor v t width dependencies . the tester environment is also simulated using advanced probe_card and tester power_supply models obtained from the literature [4] . for example , the probing environment is modeled using an advanced membrane-style probe_card model . we conduct the analysis on a 
sense making alone doesn't do it : fluency matters too ! its support for robust learning with multiple_representations previous_research demonstrates that multiple_representations of learning content can enhance students' learning , but also that students_learn deeply from multiple_representations only if the learning_environment supports them in making connections between the representations . we hypothesized that connection-making support is most effective if it helps students make both in making sense of the content across representations and in becoming fluent in making connections . we tested this hypothesis in a classroom experiment with 599 4 th-and 5 th-grade students using an its for fractions . the experiment further contrasted two forms of support for sense making : auto-linked representations and the use of worked examples involving one representation to guide work with another . results confirm our main hypothesis : a combination of worked examples and fluency support lead to more robust learning than versions of the its without connection-making support . therefore , combining different types of connection-making support is crucial in promoting students' deep_learning from multiple_representations . 
promoting reflection and its effect on learning in a programming tutor we studied the effect of post-practice reflection on learning , using programming tutors , and multiple_choice format for reflection . we conducted in-vivo controlled studies with introductory programming students from multiple schools over 3 semesters , and used mixed-factor anova to analyze the collected data . we found that reflecting on the concept underlying each problem neither promotes greater learning , measured as pre-post increase in the average score per problem , nor promotes faster learning , measured as the problems solved per concept learned . we conjecture that the benefits of reflecting on the concept underlying each problem may be limited if a tutor already promotes deep understanding of the domain . problets we have been developing software tutors , called problets ( www . problets . org ) to help students_learn c/c++/java/c# programming_language concepts by solving_problems . to date , we have developed , evaluated and deployed problets on expression evaluation ( arithmetic , relational , logical , assignment ) , selection statements , loops ( while and for ) and c++ pointers . the problets present programs to the learner , ask the learner a question about the program , such as predicting its output or identifying bugs in it , grade the student's answer , and provide delayed feedback . figure_1_shows a snapshot of a problet on selection statements , with the program in the left panel and the feedback in the right panel . problets generate problems as instances of parameterized problem templates . each template is associated with a concept in the domain , e . g . , some selection statement concepts include executing a selection statement when the condition is true/false , executing nested if statements , executing if-else statements nested in cascaded/classification style , and executing a program with multiple dependent/independent selection statements . similarly , some loop concepts include nested dependent and independent loops , multiple dependent and independent loops , loops that iterate zero or one time , and loops that update the loop variables multiple times . problets administer the pre-test-practice-post_test protocol : pre-test to evaluate the learner's knowledge; an adaptive practice on only the concepts that the learner has not yet mastered [1] , followed by post_test on only the concepts that the learner has practiced . during the pre-and post-tests , problets do not provide any feedback . during practice , problets provide delayed feedback , which includes a narrative of the step_by_step execution of the program [2] . problets use the concept_map of the domain , enhanced with learning objectives , as the overlay student model [3] . problets use reified interfaces [4] which promote the use of mental models when solving_problems , e . g . , the learner enters the output of the program 
an assessment of package-organisation misalignment : institutional and ontological structures research interests focus on business_process_management issues surrounding initiatives such as business_process redesign , enterprise systems , and web-enabled industry process_integration . christina soh has 15 years of experience in industry and government related research in the areas of it-enabled business strategy , enterprise system implementation , inter-organizational systems , it investment and business value , and national it policy . abstract even with today's 'best practice' software , commercial packages continue to pose significant alignment challenges for many organizations . this paper proposes a conceptual_framework , based on institutional theory and systems ontology , to assess the misalignments between package functionality and organisational requirements . we suggest that these misalignments can arise from incompatibility in the externally imposed or voluntarily adopted structures embedded in the organisation and package , as well as differences in the way the meaning of organisational reality is ontologically represented in the deep or surface structure of packages . the synthesis of the institutional-ontological dimensions leads us to identify four types of misalignments with varying degrees of severity imposed-deep , imposed-surface , voluntary-deep , and voluntary-surface and to predict their likely resolution . we test the predictions using over 400 misalignments from package implementations at three different sites . the findings support the predictions : the majority of imposed-deep misalignments were resolved via package customisation . imposed-surface and voluntary-deep misalignments were more often resolved via organisational adaptation and voluntary-surface misalignments were almost always resolved via organisational adaptation . the extent of project success also appeared to be influenced by the number of misalignments and the proportion of imposed-deep misalignments . we conclude by suggesting strategies that implementing organisations and package vendors may pursue . 
organizing structured web_sources by query_schemas : a clustering approach in the recent_years , the web has been rapidly "deepened" with the prevalence of databases online . on this deep_web , many sources are &#60;i>structured&#60;/i> by providing structured query interfaces and results . organizing such structured sources into a domain hierarchy is one of the critical steps toward the integration of heterogeneous web_sources . we observe that , for structured web_sources , query_schemas &#60;i>ie&#60;/i> , attributes in query interfaces ) are discriminative representatives of the sources and thus can be exploited for source characterization . in particular , by viewing query_schemas as a type of categorical data , we abstract the problem of source organization into the clustering of categorical data . our approach hypothesizes that "homogeneous sources" are characterized by the same hidden generative_models for their schemas . to find clusters governed by such statistical distributions , we propose a new objective_function , &#60;i>model_differentiation&#60;/i> , which employs principled hypothesis_testing to maximize statistical heterogeneity among clusters . our evaluation over hundreds of real sources indicates that ( 1 ) the schema-based clustering accurately organizes sources by object domains &#60;i>eg&#60;/i> , books , movies ) , and ( 2 ) on clustering web query_schemas , the model_differentiation function outperforms existing ones , such as likelihood , entropy , and context linkages , with the hierarchical agglomerative clustering algorithm . 
reduction of crosstalk pessimism using tendency graph approach accurate estimation of worst_case crosstalk_effects is critical for a realistic estimation of the worst-case behavior of deep_sub_micron circuits . crosstalk analysis models usually assume that the worst-case crosstalk occurs with all the aggressors of a victim ( net or path ) simultaneously inducing crosstalk even though this may not be possible at all . this overestimated crosstalk is called false noise . logic correlations have been explored to reduce false noise in [3] , which also used branch_and_bound method to solve the problem . in this paper , we propose a novel approach , named tendency graph approach ( tga ) , which preprocesses the logic constraints of the circuit to drastically speed up the fundamental branch_and_bound algorithm . the new approach has been implemented in c++ and tested on an industrial circuit in a current 90 nm technology , demonstrating that tga considerably accelerates the solution to the false noise problem , and makes in many cases branch_and_bound feasible in the first place . 
abstract shear behavior of reinforced_concrete deep beams strengthened with cfrp laminates shear behavior of reinforced_concrete deep beams strengthened with cfrp laminates shear behavior of reinforced_concrete deep beams strengthened with cfrp laminates the copyright_law of the united_states ( title 17 , united_states_code ) governs the making of photocopies or other reproductions of copyrighted material . under certain conditions specified in the law , libraries and archives are authorized to furnish a photocopy or other reproduction . one of these specified conditions is that the photocopy or reproduction is not to be " used for any purpose other than private study , scholarship , or research . " if a , user makes a request for , or later uses , a photocopy or reproduction for purposes in excess of " fair use " that user may be liable for copyright_infringement , this institution reserves the right to refuse to accept a copying order if , in its judgment , fulfillment of the order would involve violation of copyright_law . please note : the author retains the copyright while the new jersey institute of technology reserves the right to distribute this thesis or dissertation printing note : if you do not wish to print this page , then select " pages from : first page # to : last page # " on the print dialog screen the van houten library has removed some of the personal_information and all signatures from the approval page and biographical sketches of theses and dissertations in order to protect the identity of njit graduates and faculty . considerable research has been performed using cfrp plates bonded to normal reinforced_concrete beams , the objective being to improve shear and flexural strength . however , very few tests have been performed on reinforced_concrete deep beams with cfrp as shear reinforcement . the purpose of this experiment is to investigate the behavior of the deep beam in shear with various arrangements of sika carbodur laminates bonded to the sides . the beams were designed to be shear deficient with stirrups omitted , except at the supports and directly under the load . in total eight beams were tested , four with one-point loading and four with two-point loading . each loading condition contained one control beam without cfrp , and three beams with the cfrp laminates attached at zero , forty-five , and ninety degrees with respect to the neutral axis . the ultimate loading capacity and behavior for each beam bonded with the cfrp laminate was observed and recorded . the mode of failure was shear in all the beams tested , which typically resulted from the delaminating between the concrete and the epoxy . the present test results show that the orientation of the cfrp about the 
reliability evaluation of ndt techniques for cu-welds for risk_assessment of nuclear_waste encapsulation in order to handle the long living radioactive waste ( spent_nuclear_fuel ) skb is planning to build a deep repository that requires no monitoring by future generations . the spent_nuclear_fuel will be encapsulated in copper canisters consisting of a graphite cast_iron insert shielded by an outer 30-50 mm thick copper cylinder for corrosion protection . the most critical part of the encapsulation process is the sealing of the canister , which is done by welding the copper lid to the cylindrical part of the copper shell using radiographic and ultrasonic_testing-must be satisfactorily determined and combined to derive assumptions regarding the frequency of undetected welding defects for the ensemble of canisters as input for the risk_assessment . this is done using the pod method according to the " reliability handbook mil 1823 " and its generalization to more complex defect situations in welds . friction stir welding and electron beam welding . the quality of the welding process and the reliability of the ndt system
built-in self-test and calibration of mixed_signal devices wide adoption of deep_sub_micron and nanoscale technologies in the modern semiconductor_industry is resulted in very large complex mixed_signal devices . it has then become more difficult to estimate and control device parameters , which are now increasingly vulnerable to fabrication process_variations . conventional design_for_test ( dft ) methods have been already well studied for digital circuitry to ensure verification of its functionality and fault_coverage . built-in self-test ( bist ) approaches have been developed for design_automation of digital ics . however , such dft techniques cannot be applied to analog and mixed_signal circuits directly . therefore , new techniques must be employed to detect faults in analog components and to provide certain level of calibration capability to dynamically adjust the parameters of an analog device for better yield of chips . the most important ana-log devices in a mixed_signal system-on-chip ( soc ) are analog_to_digital_converter ( adc ) and digital_to_analog_converter ( dac ) . such converters transfer data between digital_and_analog circuits and convert analog signals to digital bits or vice versa . in this research , novel digital_signal_processor ( dsp ) -based post-fabrication_process-independent bist approaches and variation tolerant design technique for adc and dac are studied . we use a sigma-delta modulation technique for measurement and a polynomial fitting algorithm for device calibration . in the proposed technique , a digital_signal_processor is programmed and used as test pattern_generator ( tpg ) , output response analyzer ( ora ) and test control unit . the polynomial fitting algorithm characterizes the nonlinearity errors and the polynomial is used to generate compensating signals to reduce nonlinearity errors to 0 . 5lsb . this ii technique can be applied to other digitally-controllable mixed_signal devices and a general test-characterization-calibration approach modeled after this work can be developed to detect , measure , and compensate nonlinearity errors caused by device parameter deviations . iii acknowledgments
chinese_room_problem chinese_room_problem chinese_room_problem chinese_room_problem the success of machines over the last few decades in performing tasks that were seeming impossible for humans to perform led to the discussion that can machines be made intelligent . the argument was based on the fact that there was no understanding and the computer merely followed human etc . is not a new one . the question evokes deep programmed rules without any consciousness . the other side countered that an argument like that was arguable , since the results were as if produced by an intelligent being and had meaning , the computer has produced proof of intelligence . in this paper , we would analyze the arguments of both the sides and present a clearer picture of the capabilities of machine . we'll begin by explaining the turing_test , a criteria to test the intelligence of a machine and then move to discuss chinese_room_problem and its implications . i will be highlighting the objections raised against these problems and my own answers to these arguments . 
kalman_filter-based semi-codeless tracking of weak dual-frequency gps signals main research interests are gps software receiver development and gps applications , kalman_filter and estimation , orbit and attitude determination of satellites . mechanical and aerospace_engineering from princeton_university . his research interests are in the areas of estimation and filtering , spacecraft attitude and orbit determination , and gps technology and applications . abstract extended kalman_filter ( ekf ) based_methods have been developed for semi-codeless tracking of the p ( y ) code on weak dual-frequency gps signals . optimal kalman filtering methods are essential to maintain lock with a semi-codeless approach during ionospheric scintillations , when deep power fades can occur . ekf-based semi-codeless techniques use maximum a posteriori estimation techniques to estimate the unknown w code that gets modulo-2 added to the p code in order to provide anti-spoofing protection . these tracking techniques take advantage of the known w-bit chip timing , whose chipping rate is approximately 500 khz . they also use a posteriori cost functions that involve-log[cosh ( ) ] terms , which allow seamless transition between various w-bit decision techniques . the algorithm estimates the p ( y ) carrier amplitudes , accounts for signal dynamics in the optimization process , and optimally couples its code and carrier tracking loops . the algorithm has been successfully tested with real_data under normal signal_strength conditions . 
feasibility of iddq tests for shorts in deep_submicron ics quiescent supply_current ( iddq ) in deep_submicron ics is derived by circuit_simulation and feasibility of iddq tests is examined for short defects in ics fabricated with 0 . 18 m cmos_process . the results show that iddq of each gate depends on input logic values and that shorts can be detected by iddq_testing if some process_variations are small . 
deep_belief net learning in a long_range vision system for autonomous off-road driving we present a learning-based approach for long_range vision that is able to accurately classify complex terrain at distances up to the horizon , thus allowing high_level strategic planning . a deep_belief_network is trained with unsupervised data and a reconstruction criterion to extract features from an input image , and the features are used to train a realtime classifier to predict traversability . the online supervision is given by a stereo module that provides robust labels for nearby areas up to 12 meters distant . the approach was developed and tested on the lagr mobile_robot . 
feature_learning in deep_neural_networks - studies on speech_recognition tasks recent_studies have shown that deep_neural_networks ( dnns ) perform significantly better than shallow networks and gaussian mixture_models ( gmms ) on large vocabulary speech_recognition tasks . in this paper , we argue that the improved accuracy achieved by the dnns is the result of their ability to extract dis-criminative internal representations that are robust to the many sources of variability in speech signals . we show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth , which leads to better speech_recognition performance with deeper networks . we also show that dnns cannot extrapolate to test samples that are substantially different from the training examples . if the training data are sufficiently representative , however , internal features learned by the dnn are relatively stable with respect to speaker differences , bandwidth differences , and environment distortion . this enables dnn-based recognizers to perform as well or better than state-of-the-art systems based on gmms or shallow networks without the need for explicit model adaptation or feature normalization . 
statistical correlations and risk analyses techniques for a diving dual phase bubble model and data bank using massively parallel supercomputers linking model and data , we detail the lanl diving reduced gradient bubble model ( rgbm ) , dynamical principles , and correlation with data in the lanl data bank . table , profile , and meter risks are obtained from likelihood analysis and quoted for air , nitrox , helitrox no-decompression time limits , repetitive dive tables , and selected mixed_gas and repetitive profiles . application analyses include the explorer decompression meter algorithm , naui tables , university of wisconsin seafood diver tables , comparative naui , padi , oceanic ndls and repetitive dives , comparative nitrogen and helium mixed_gas risks , uss perry deep rebreather ( rb ) exploration dive , world record open circuit ( oc ) dive , and woodville karst plain project ( wkpp ) extreme cave exploration profiles . the algorithm has seen extensive and utilitarian application in mixed_gas diving , both in recreational and technical sectors , and forms the bases forreleased tables and decompression meters used by scientific , commercial , and research divers . the lanl data bank is described , and the methods used to deduce risk are detailed . risk functions for dissolved gas and bubbles are summarized . parameters that can be used to estimate profile risk are tallied . to fit data , a modified levenberg-marquardt routine is employed with l2 error norm . appendices sketch the numerical_methods , and list reports from field_testing for ( real ) mixed_gas diving . a monte_carlo-like sampling scheme for fast numerical_analysis of the data is also detailed , as a coupled variance_reduction technique and additional check on the canonical approach to estimating diving risk . the method suggests alternatives to the canonical approach . this work represents a first time correlation effort linking a dynamical bubble model with deep stop data . supercomputing resources are requisite to connect model and data in application . 
knowledge_representation and sense disambiguation for interrogatives in e-hownet in order to train machines to 'understand' natural_language , we propose a meaning representation mechanism called e-hownet to encode lexical senses . in this paper , we take interrogatives as examples to demonstrate the mechanisms of semantic representation and composition of interrogative constructions under the framework of e-hownet . we classify the interrogative words into five classes according to their query types , and represent each type of interrogatives with fine_grained features and operators . the process of semantic composition and the difficulties of representation , such as word_sense_disambiguation , are addressed . finally , machine understanding is tested by showing how machines derive the same deep_semantic structure for synonymous sentences with different surface structures . 
research note nlp revisited : nonverbal communications and signals of trustworthiness a core principle of neurolinguistic programming ( nlp ) is that rapport and trust develop through synchronization of modes of communication between the sender and receiver . nonverbal signals are a particularly important mode of communications in the nlp perspective . this study extends the nlp framework by incorporating findings from neuroscience into research about nonverbal signals and sensory representational systems . three independent but related studies are used to identify nonverbal cues associated with the representational systems , to test if descriptions of these nonverbal signals influence trustworthiness assessments , and , finally , to test if these nonverbal signals trigger buyer's positive assessments of salesperson trust-building characteristics as well as trustworthiness . sales trainers , academics , and sales books consistently advise salespeople to build trust with their clients and customers without providing much practical guidance on how to initiate this relationship . the frequently stated goal of this rapport building is to establish a relationship with deep-rooted trust ( rousseau et al . 1998 ) , which , as a wealth of research demonstrates , leads to positive outcomes for the relationship . however , the actual initiating behavioral correlates of rapport building are without theoretical underpinnings and lack empirical support ( tickle-degnen and rosenthal 1990 ) . advocates of neurolinguistic programming ( nlp ) an approach to human communications that combines cogni-tive theory , split_brain processing , and sensory perception have long suggested that their approach holds the key to understanding rapport building . they propose that understanding rapport and trust begins with the investigation of communications as a process while ignoring any content in the message ( dimmick 1995 ) . this perspective has led , in marketing studies of nlp , to a methodological emphasis upon the signals of the exemplar salesperson as a sender of messages and cues rather than a focus on the buyer as the receiver and interpreter of the signal ( dowlen 1996 ) . unfortunately , investigations based upon nlp have a history of inconclusive and contradictory empirical_evidence ( dowlen 1996; thompson , courtney , and dickson 2002 ) . the inconclusive evidence of previous nlp studies may have resulted from the emphasis upon the sender/salesperson rather than the receiver/ customer . findings from recent_research suggests that buyers today continue to assess the trustworthiness of relationship partners based on their impressions from the initial face-to-face encounter ( chamberlin 2000; mcknight , cummings , and chervany 1998 ) . multiple studies in neuroscience suggest that these impressions are formed by perceptions of nonverbal cues ( puce et al . 2003; winston et al . 2002 ) . using functional_magnetic_resonance_imaging ( fmri ) techniques , these researchers map the structures of the brain that 
characterizing data_structures for volatile forensics volatile_memory forensic tools can extract valuable evidence from latent data_structures present in memory dumps . however , current techniques are generally limited by a lack of understanding of the underlying data without the use of expert knowledge . in this paper , we characterize the nature of such evidence by using deep analysis techniques to better understand the life-cycle and recoverability of latent program data in memory . we have developed cafegrind , a tool that can systematically build an object map and track the use of data_structures as a program is running . statistics collected by our tool can show which data_structures are the most numerous , which structures are the most frequently accessed and provide summary statistics to guide forensic analysts in the evidence gathering process . as programs grow increasingly complex and numerous , the ability to pinpoint specific evidence in memory dumps will be increasingly helpful . cafegrind has been tested on a number of real_world_applications and we have shown that it can successfully map up to 96% of heap accesses . 
the adoption of information_systems in smes : organizational issues and success factors this research approaches the issues of introducing icts ( information and communication technologies ) into small_and_medium_enterprises , with the aim of finding some conditions that make the organizational context able to manage the change process needed to really get the potential benefits of these technologies . the final objective is to propose a theoretical base for a methodology able to improve analytical and diagnostic capabilities of organizational realities before and during planning and evaluation phases of their information_technology investments . the study is planned to go through two phases . the first one is aimed to look for reliable indicators to forecast readiness and adequacy of a specific organizational context towards the positive adoption of i . t . systems through a deep analysis of five significant cases . the second phase will test such indicators through questionnaire research on a larger sample of italian smes . 
through a glass darkly : information_technology design , identity verification , and knowledge contribution in online_communities a variety of information_technology ( it ) artifacts , such as those supporting reputation_management and digital archives of past interactions , are commonly deployed to support online_communities . despite their ubiq-uity , theoretical and empirical_research investigating the impact of such it-based features on online_community communication and interaction is limited . drawing on the social_psychology literature , we describe an identity_based view to understand how the use of it-based features in online_communities is associated with online knowledge contribution . specifically , the use of four categories of it artifacts those supporting virtual co-presence , persistent labeling , self-presentation , and deep profiling is proposed to enhance perceived identity verification , which thereafter promotes satisfaction and knowledge contribution . to test the theoretical model , we surveyed more than 650 members of two online_communities . in addition to the positive effects of community it artifacts on perceived identity verification , we also find that perceived identity verification is strongly linked to member satisfaction and knowledge contribution . this paper offers a new perspective on the mechanisms through which it features facilitate computer-mediated knowledge sharing , and it yields important implications for the design of the supporting it infrastructure . 3 4 months for 2 revisions . 
packet_scheduling for deep_packet_inspection on multi_core architectures multi_core architectures are commonly used for network applications because the workload is highly parallelizable . packet_scheduling is a critical performance component of these applications and significantly impacts how well they scale . deep_packet_inspection ( dpi ) applications are more complex than most network applications . this makes packet_scheduling more difficult , but it can have a larger impact on performance . also , packet latency and ordering requirements differ depending on whether the dpi application is deployed inline . therefore , different packet_scheduling tradeoffs can be made based on the deployment . in this paper , we evaluate three packet_scheduling algorithms with the protocol analysis module ( pam ) as our dpi application using network traces acquired from production networks where intrusion prevention systems ( ips ) are deployed . one of the packet_scheduling algorithms we evaluate is commonly used in production applications; thus , it is useful for comparison . the other two are of our own design . our results show that packet_scheduling based on cache affinity is more important than trying to balance packets . more specifically , for the three network traces we tested , our cache affinity packet scheduler outperformed the other two schedulers increasing throughput by as much as 38% . 
fiber . optic reference frequency_distribution to remote beam waveguide antennas* iii the nasa/jpl deep_space_network ( dsn ) , radio science experiments ( probing outer planet atmospheres , rings , gravitational waves , etc . ) and very long-base interfcrometry ( vl ? h ) require ultra sfa ble , low phase_noise rejerence frequency signals at the user locations . ijpical locations jor radio science/vllll exciters and down-converters are the cone areas o~ the 34 m high efllciency antennas or the 70 m anfennas , located several hundred meters from the reference frequency standards . over the past three years , fiber_optic distribution links have replaced coaxial_cable distribution for reference frequencies to these antenna sites . optical_fibers are the preferred medium for distribution because of their low attenuation , immunity to em1/lwl , and temperature stability . a new network of beam waveguide ( bwg ) antennas p~esently under construction in the ] ) sn requires hydrogen maser stability at tens oj kilometers disfance jrom the jrequency standards central location . 'l'he topic oj this paper is the design and imphm~entation oj an optical jiber distribution link which provides ultra-stable rejerence frequencies to users at a remote iiwg antenna . 7'he temperature profile jrom the earth's surjace to a depth oj six jeet over a time period oj six mon ( hs was used to optimize the piacement oj the jiber optic cables . in-situ evaluation oj the fiber_optic link performance indicates auar deviation on the order ojparts in jo-15 at ]000 and 10 , 000 seconds averaging time; thus , the link sfability degradation due to environmental conditions still preserves hydrogen maser stability at the user locations . l-his paper_reports on the implementation oj optical ~[bers and elecfro-optic devices jor distributing very stable , low phase_noise rejerence signals to remote bwg antenna locations . auan deviation and phase_noise test_results jor a j 6 km fiber_optic distribution link are presented in the paper . 
quantifiers vs . quantification theory the syntax and semantics of quantifiers is of crucial significance in current linguistic theorizing for more than one reason . the last statement of his grammatical theories by the late richard_montague ( 1 973 ) is modestly entitled " the proper treatment of quantification in ordinary english " . in the authoritative statement of his " generative semantics " , george_lakoff ( 1971 , especially pp . 238-267 ) uses as his first and foremost testing-ground the grammar of certain english quantifiers . in particular , they serve to illustrate , and show need of , his use of global constraints governing the derivation of english sentences . evidence from the behavior of quantifiers ( including numerical expressions 1 ) has likewise played a major role in recent discussions of such vital problems as the alleged meaningdpreservation of transformations , 2 co-reference , 3 the role of surface structure in semantical interpretation , and so on . in all these problems , the behavior of natural_language quantifiers is one of the main_issues . quantifiers have nevertheless entered the methodenstreit of contemporary linguistics in another major way , too . ( these two groups of problems are of course interrelated . ) this is the idea that the structures studied iln the so-called quantification theory of symbolic_logic-otherwise know as first_order_logic , ( lower ) predicate_calculus , or elementary logic-can serve and suffice 6 as semantical representations of english sentences . views of this general type have been proposed by mccawley ( 1971 ) * and lakoff ( 1972 ) ' ( among others ) . a related theory of " deep_structure as logical_form " has been put forward and defended by g . harman ( 1972 ) . theories of this general type may be compared with the traditiomal idea that quantification 'theory can be viewed as an abstraction from the behavior
towards anti-model_based_testing software_testing refers to the dynamic verification of a system's behavior based on the observation of a selected set of controlled executions , or test_cases [2] . while in traditional_approaches test_cases were selected aimed at covering the program source_code [5] , nowadays we can apply a variety of testing techniques all along the development process , by basing test selection on different pre_code artifacts , such as requirements , specifications and design models [2] . model_based_testing consists in deriving a suite of test_cases from a model representing software behavior . such a model can be generated from a formal_specification or designed by software engineers through diagrammatic tools . in principle , the derivation of the test_cases can be done automatically , and indeed several approaches have been recently proposed that do this starting from models in different languages , e . g . , [9 , 1 , 6] . by executing the model_based test_cases , the conformance of the implemented system to its specification can be validated . model_based_testing is certainly useful and effective; however , there can be several reasons why such an approach cannot be applied or is too expensive for deployment in a specific context . one generic barrier to a wide adoption of model_based_testing is its inherent complexity , which requires a deep expertise in formal_methods , even where tool support is available as testified by case_studies in the agedis project [1] . another obstacle is the difficulty in forcing the implementation to take a defined path as identified in the model derived test_sequences . the latter are generally expressed at an abstract level , while the ex-ecutable test_cases must be more concrete and more informative ( e . g . , [3] ) . finally , one further counter-motivation to the practice of model_based_testing can be the use of legacy systems or cots , for which behavior models are not available . considering in particular component-based software_development , a system is generally obtained by assembling already existing components , for which we cannot a-priori assume that a specification or the source_code are available . in such cases , model_based_testing is not applicable , or would be too costly . we assume in fact that the system assembler has a high-level specification of the global architecture , but can only pose in practice very basic requirements on the behavior of the acquired components . this is the rationale for an " anti-model_based_testing approach " as the one we outline in this paper . while model_based_testing starts from 
heat_transfer analysis for a winged reentry flight test_bed in this paper we deal with the aero-heating analysis of a reentry flight demonstrator helpful to the research_activities for the design and development of a possible winged reusable launch_vehicle . in fact , to reduce risks in the development of next_generation reusable launch vehicles , as first step it is suitable to gain deep design knowledge by means of extensive numerical computations , in particular for the aero-thermal environment the vehicle has to withstand during reentry . the demonstrator under study is a reentry space glider , to be used both as crew rescue vehicle and crew transfer vehicle for the international_space_station . it is designed to have large atmospheric manoeuvring capability , to test the whole path from the orbit down to subsonic speeds and then to the landing on a conventional runway . several analysis_tools are integrated in the framework of the vehicle aerothermal design . between the others , we used computational analyses to simulate aerothermodynamic flowfield around the spacecraft and heat_flux distributions over the vehicle surfaces for the assessment of the vehicle thermal protection system design . heat_flux distributions , provided for equilibrium conditions of radiation at wall and thermal shield emissivity equal to 0 . 85 , highlight that the vehicle thermal shield has to withstand with about 1500 [kw/m 2 ] and 400 [kw/m 2 ] at nose and wing leading_edge , respectively . therefore , the fast developing new generation of thermal protection materials , such as ultra_high temperature ceramics , are available candidate to built the thermal shield in the most solicited vehicle parts . on the other hand , away from spacecraft leading edges , due to the low angle of attack profile followed by the vehicle during descent , the heat_flux is close to values attainable with conventional heat_shield . also , the paper shows that the flying test_bed is able to validate hypersonic aerothermodynamic design database and passenger experiments , including thermal shield and hot structures , giving confidence that a full-scale development can successfully proceed . n ) q & q r r r re s t t v unit normal heat_flux , w/m 2
parallel gradient_descent for multilayer feedforward_neural_networks we present a parallel approach to classification using neural_networks as the hypothesis class . neural_networks can have millions of parameters and learning the optimum value of all parameters from huge datasets in a serial implementation can be a very time consuming task . in this work , we have implemented parallel gradient_descent to train multilayer feedforward_neural_networks . specifically , we analyze two kinds of par-allelization techniques : ( a ) parallel_processing of multiple training examples across several threads and ( b ) parallelizing matrix operations for a single training example . we have implemented a serial minibatch gradient_descent algorithm , its parallel multithreaded version ( using pthread library in c++ ) , a blas parallelized version and a cuda implementation on a gpu . all implementations have been compared and analyzed for the speedup obtained across various network architectures and increasing problem_sizes . we have performed our tests on the benchmark dataset : mnist , and finally also compared our implementations with the corresponding implementations in the state-of-the-art deep_learning library : theano . 
mace : model-inference-assisted concolic exploration for protocol and vulnerability discovery_program state-space_exploration is central to software security , testing , and verification . in this paper , we propose a novel technique for state_space exploration of software that maintains an ongoing interaction with its environment . our technique uses a combination of symbolic and concrete execution to build an abstract model of the analyzed application , in the form of a finite-state automaton , and uses the model to guide further state-space_exploration . through exploration , mace further refines the abstract model . using the abstract model as a scaffold , our technique wields more control over the search process . in particular : ( 1 ) shifting search to different parts of the search_space becomes easier , resulting in higher code_coverage , and ( 2 ) the search is less likely to get stuck in small local state-subspaces ( e . g . , loops ) irrelevant to the application's interaction with the environment . preliminary experimental results show significant increases in the code_coverage and exploration depth . further , our approach found a number of new deep vulnerabilities . 
threshold measurements and when the locking loop is closed . the closed_loop frequency fluctuations exhibit a root-mean-square ( r . m . s . ) frequency deviation of , 310 khz over a 1-min integration time . the residual noise exhibits a white_noise spectrum with an allan variance of 3 10 211 /t 1/2 for an averaging time of 1 s # t # 25 . 6 s , and is limited by the electronic noise of our detection system ( pd3 in fig . 4a ) . a straightforward improvement would be to use a saturable absorption_line ( which can have a linewidth as narrow as 1 mhz; ref . 13 ) as a reference . the improved signal_to_noise_ratio in hc-pcf also makes overtone absorptions in the visible and near-infrared ( for example , p11 of 12 c 2 h 2 at 790 . 703 nm ) accessible to laser frequency metrology . to test for gas leakage , the pressure-dependent linewidth of the p9 acetylene absorption was monitored daily over two months . the results gave an average value of , 468 mhz with a standard deviation of 30 mhz , that is , the linewidth is consistent with the doppler limited value and was constant within experimental error , indicating no measurable leakage of gas . in conclusion , all-fibre , ultra-compact , high_performance , easy-to-use and unconditionally stable gas-laser devices have been reported . the commercial availability of a wide range of all-fibre components ( for example , lasers , phase modulators , power attenua-tors , isolators , bragg gratings and beam-splitters ) makes complex systems easy to design and construct . it is now possible to imagine miniature laser gas devices , occupying a tiny volume and containing minute amounts of gas , with everyday applications in fields such as the colour conversion of laser light ( perhaps using a built-in diode pump laser ) , and the measurement and stabilization of laser frequency . the unique features of hc-pcfs make these gas laser devices extremely efficient , and their impact in many laser-related fields is likely to be deep and lasting . a methods splicing procedure fusion-splicing was carried out using a commercial filament-based splicer ( vytran ffs-2000-pm ) . in this splicer , the splicing region is continuously purged by argon gas to stop the filament burning . this prevents contamination of the splice by solid deposits and water condensation , and also prevents combustion of flammable gases such as hydrogen and acetylene . for high fill pressures , the splice could be consolidated using heat-curable glue . the energy threshold measurement for the generation of the first stokes line s 00 ( 1 ) was carried out in the following manner . the input power was varied by 
enhancing learning through self_explanation self_explanation is an effective teaching/learning strategy that has been used in several intelligent_tutoring_systems in the domains of mathematics and physics to facilitate deep_learning . since all these domains are well structured , the instructional material to self-explain can be clearly defined . we are interested in investigating whether self_explanation can be used in an open-ended domain . for this purpose , we enhanced kermit , an intelligent_tutoring_system that teaches conceptual database_design . the resulting system , kermit-se , supports self_explanation by engaging students in tutorial dialogues when their solutions are erroneous . we plan to conduct an evaluation in july 2002 , to test the hypothesis that students will learn better with kermit-se than without self_explanation . 
subthreshold behaviour modeling of fgmos transistors using the acm and the bsim3v3 models the modeling behaviour of fgmos devices in the subthreshold region is investigated . experimental and simulated id-vcg , and gmlid vs . log ( 1d ) data for the standard cmos 0 . 6 pm process are compared . the bsim3v3 and the advanced compact model , acm are adopted to model the behaviour of the fgmos transistors in the subthreshold region . two different dc simulation macromodels are used for the dc simulation of the floating gate devices . the performance of the two mosfet models and of the two dc simulation macromodels in modeling the fgmos transistor in the subthreshold region is discussed . i . introduction floating gate mos transistors have been widely used as storage devices in structures such as eeproms and flash memories . they have also been used as nonvolatile information storage devices for analog applications[ 1-31 . the model proposed in [4] considers fgmos transistors as devices which consist of multiple control gates capacitively coupled into a floating gate , and thus the floating gate voltage is a weighted summation of the input voltages where capacitive_coupling coefficients act as the weights . in low_voltage analog circuits the mosfet very often operates in the weak and moderate inversion regions . subthreshold models for the fgmos transistor are presented in [5 , 6] . modeling of the deep submicron mos transistor for low_voltage , low_power analog circuits is a difficult task . it depends on the adopted mosfet_model . a set of tests can be done to evaluate the performance of the mosfet_model [7] . moreover the fgmos transistor has a floating node , so a macromodel has to be developed for dc simulation purposes to avoid dc convergence problems . in this work the subthreshold behaviour modeling of the fgmos transistor is investigated using two compact mosfet models , the berkeley bsim3v3 [si , and the acm model [9] . 
arbitrage tests of israel's currency_options markets arbitrage tests of israel's currency_options markets this paper focuses on the israeli currency_options market , which includes currency_options traded on the tel_aviv stock_exchange and ( non-tradable ) bank of israel currency_options . the three aims of this study are : ( a ) to examine the null_hypothesis that the israeli currency_options market is efficient , an issue that has not yet been thoroughly investigated . ex-post tests of arbitrage and dominance conditions do not permit rejection of the null_hypothesis , except for very-near-maturity , deep-in-the-money ( itm ) options . ( b ) to test the validity of the black and scholes ( b-s ) model as a native option-pricing model for the case of an exchange_rate target zone . we find that although we cannot reject the weakly efficient_market_hypothesis ( except for very-near-maturity deep-itm options ) , we can reject the strongly efficient market and/or the b-s model validity hypotheses . the banking sector could have utilized arbitrage opportunities , notably for out-of-the-money , at-the-money , and far-from-maturity options , especially when employing inter-temporal weighted-average implied standard_deviation . ( c ) to address the issue of the liquidity premium evaluation; this ( rather surprisingly ) is found to be negative for some options . this study extends previous_studies of options by examining the efficiency of currency option markets and the validity of the black and scholes model under a target zone exchange_rate regime . it also compares the performance of currency option trading on the exchange and over-the-counter for the same period . 
progress towards physics-based space weather_forecasting with exascale computing particle in cell simulations applied to space_weather modelling represent an excellent application for code-sign efforts . pic codes are simple and flexible with many variants addressing different physical conditions ( e . g . explicit , implicit , hybrid , gyrokinetic , fluid ) and different architectures ( e . g . vector , parallel , gpu ) . it is relatively easy to consider radical changes and test them in a short time . for this reason , the project deep funded by the european_commission ( www . deep-project . eu ) and the intel exascience lab ( www . exascience . com ) have used pic as one of their target application for a codesign approach aiming at developing pic methods for future exascale comupters . the present work focuses on the efforts within deep . the starting_point is the ipic3d implicit pic approach . here we report on the analysis of code performance , on the use of gpus and the new mics ( intel phi processors ) . we describe how the method can be rethought for hybrid architectures composed of mics and cpus ( as in the new deep supercomputer in juelich , as well as in others ) . the focus is on a codesign approach where computer_science issues motivate modifications of the algorithms used while physics constraints what should be eventually achieved . 
multiple dynamic models for tracking the left ventricle of the heart from ultrasound data using particle filters and deep_learning architectures the problem of automatic tracking and segmentation of the left ventricle ( lv ) of the heart from ultrasound images can be formulated with an algorithm that computes the expected segmentation value in the current time step given all previous and current observations using a filtering distribution . this filtering distribution depends on the observation and transition models , and since it is hard to compute the expected value using the whole parameter space of segmen-tations , one has to resort to monte_carlo sampling techniques to compute the expected segmentation parameters . generally , it is straightforward to compute probability values using the filtering distribution , but it is hard to sample from it , which indicates the need to use a proposal distribution to provide an easier sampling method . in order to be useful , this proposal distribution must be carefully designed to represent a reasonable approximation for the filtering distribution . in this paper , we introduce a new lv tracking and segmentation_algorithm based on the method described above , where our contributions are focused on a new transition and observation models , and a new proposal distribution . our tracking and segmentation_algorithm achieves better overall results on a previously tested dataset used as a benchmark by the current_state-of-the-art tracking algorithms of the left ventricle of the heart from ultrasound images . 
an active_contour-based atlas registration model applied to automatic subthalamic_nucleus targeting on mri : method and validation this paper presents a new non parametric atlas registration framework , derived from the optical_flow model and the active_contour theory , applied to automatic subthalamic_nucleus ( stn ) targeting in deep_brain_stimulation ( dbs ) surgery . in a previous work , we demonstrated that the stn position can be predicted based on the position of surrounding visible structures , namely the lateral and third ventricles . a stn targeting process can thus be obtained by registering these structures of interest between a brain_atlas and the patient image . here we aim to improve the results of the state of the art targeting methods and at the same time to reduce the computational time . our simultaneous segmentation and registration model shows mean stn localization errors statistically similar to the most performing registration_algorithms tested so far and to the targeting expert's variability . moreover , the computational time of our registration method is much lower , which is a worthwhile improvement from a clinical point_of_view . 
deploying a wireless_sensor_network in iceland the glacsweb project a wireless_sensor_network deployment on a glacier in iceland is described . the system uses power_management as well as power harvesting to provide long_term environment sensing . advances in base_station and sensor node design as well as initial results are described . the glacsweb project [1] aimed to study glacier dynamics through the use of wireless_sensor_networks . it replaced wired instruments which had previously been used with radio-linked subglacial probes which contained many sensors . the base of a glacier has a large controlling effect on a glacier's response to climate_change and there is a growing need to study it in order to build better models of their behaviour . several generations of systems were deployed in briksdalsbreen , an outlet of the jostedal icecap in norway . as a multidisciplinary research_project it involved people from many domains : electronics , computer_science , glaciology , electrical_engineering , mechanical_engineering and gis . initial deployments had to solve the mechanical design of the probe cases and the unknown radio_communication issues . the solutions involved craft as much as science and engineering but the key success has been to create data which had not existed before [2 , 3 , 4 , 5] while advancing our knowledge of sensor_network deployments . hot water drills are used in order to produce holes which reach the glacier bed . most probes are placed 10-30cm under the ice while some are placed within the ice . due to the relatively slowly changing environment the probe sense rate is normally set to once every four hours , although an adaptive sampling algorithm has been developed in the lab [6] which would optimise this sampling rate . skalafellsj kull is a part of the large vatnaj_kull icecap in iceland and our site was chosen at ( 64 15'27 . 09"n , 15 50'37 . 68"w ) around 800m altitude near an access road . although there was no local internet connection there was a mobile_phone signal which we used for the main internet link . the glacier is deep enough to test beyond 100m depth in the future ( we used 60-80m ) . due to the nature of the team and time available a few key topics were chosen for the developments for the iceland deployment . one main area was to improve the basestation design through the use of a gumstix processor . this would provide a better development environment and easier package management . the probes would maintain the pic18 microcontroller but would gain an improved power_supply and simplified code . in terms of sensors the 
classification of layered tissue phantoms for detection of changes in epithelial tissue below the surface using a stochastic decomposition model for scattered signal this paper answers the question of whether it is possible to detect changes inside epithelium layered structures using a stochastic decomposition method ( sdm ) [1 , 2] that models the scattered light reflected from the layered structure over an area ( 2-d scan ) illuminated by an optical sensor ( fiber ) emitting light at either one wavelength or with white light . our technique correlates the differential changes in the reflected tissue texture with the morphological and physical changes that occur in the tissue occurring below the surface of the structure . this work has great potential in detecting changes in mucosal structures and may lead to enhanced endoscopy when the disease is developing to the below the surface and hence becoming hidden during colonoscopy or endoscopic examination . tests are performed on layered tissue phantoms and the results_obtained show great effectiveness of the model and method in picking up changes in the morphology of the layered tissue phantoms occurring below the surface ( greater than 0 . 6mm deep ) . 1 . introduction various optical techniques have been developed for early diagnosis of epithelial cancer . detailed reviews of these available optical techniques are presented in the literature with analysis of their advantages_and_disadvantages [3] . the motivation behind using the optical based_techniques is that for the cases of malignancy detected at an earlier stage in colorectal_cancer , a 5-year survival in excess of 97% can occur [4] . in recent years considerable progress has been made to evaluate subsurface structures in biological tissues in vivo . confocal laser endomicroscopy , which is the closest step towards virtual histology has lead to the evaluation of the whole mucosal layer with an infiltration depth up to 250 m [5] . likewise , confocal fluorescence endomicroscopy may have the capacity to reach to 15-100 m depth of penetration [6] . while those techniques use florescent dyes to interrogate subsurface information , other researchers are focusing on extracting information from tissue layers at different depths using reflected light [7 , 8] without the use of dyes . thus , the ultimate goal is to detect changes in the sub-epithelial tissue since early cancer development might occur at this level , hence attention needs to be focused on detection of morphological changes below the surface [9] . in this paper we aim
xsim : an efficient crosstalk simulator for analysis and modelling of signal_integrity faults the paper presents an efficient crosstalk simulator tool " xsim " and it's methodology developed by the signal_integrity fault_modeling and test research_group of item , university of bremen , for analysis and modeling of signal_integrity faults in deep_sub_micron ( dsm ) chip . the tool can be used for analyzing the crosstalk_coupling behavior in both defective and defect_free parallel interconnects . using the xsim tool one can also determine the critical values of various interconnect's parasitics and the critical values of crosstalk coupling_capacitance and resistive_bridging ( i . e . mutual conductance , if any ) beyond which device will most likely suffer from the signal_integrity losses , whereas for lower coupling values device will continue to behave as crosstalk fault_tolerant . the special features of this simulation tool are that it is based on an indigenous methodology implemented in c++ and microsoft foundation classes ( mfc ) and can be run on any standard pc with windows_2000 or windows_xp operating system . the advantage of such implementation is that it provides a user-friendly graphical_user_interface ( gui ) which not only makes the tool very easy-to-use but also flexible and at least 11 times faster than commercial circuit simulator like pspice and provides very accurate simulation_results which are very close to the one obtained from latter . 
arabic qa4mre at_clef 2012 : arabic question_answering for machine reading evaluation this paper presents the work carried out at anlp research_group for the clef-qa4mre 2012 competition . this year , the arabic_language was introduced for the first time on qa4mre lab at_clef whose intention was to ask questions which require a deep knowledge of individual short texts and in which systems were required to choose one answer from multiple answer choices , by analyzing the corresponding test document in conjunction with background collections . in our participation , we have proposed an approach which can answer questions with multiple answer choices from short arabic texts . this approach is constituted essentially of shallow information_retrieval methods . the evaluation results of the running submitted has given the following scores : accuracy calculated overall all questions is 0 . 19 ( i . e . , 31 correct questions answered correctly among 160 ) , while overall c@1 measure is also 0 . 19 . the overall results_obtained are not enough satisfactory comparing to the top works realized last year in qa4mre lab . but as a first step at the roadmap of the evolution of the qa to machine reading ( mr ) systems in arabic_language and with the lack of researches investigated in the mr and deep_knowledge reasoning in arabic_language , it is an encouraging step . our proposed approach with its shallow criterion has succeeded to obtain the goal fixed at the beginning which is : select answers to questions from short texts without required enough external knowledge and complex inference . 
gait assessment in parkinson's_disease : toward an ambulatory system for long_term monitoring an ambulatory gait_analysis method using body-attached gyroscopes to estimate spatio-temporal parameters of gait has been proposed and validated against a reference system for normal and pathologic gait . later , ten parkinson's_disease ( pd ) patients with subthalamic_nucleus deep_brain_stimulation ( stn_dbs ) implantation participated in gait measurements using our device . they walked one to three times on a_20-m walkway . patients did the test twice : once stn_dbs was on and once 180 min after turning it off . a group of ten age-matched normal subjects were also measured as controls . for each gait cycle , spatio-temporal parameters such as stride length ( sl ) , stride velocity ( sv ) , stance ( st ) , double support ( ds ) , and gait cycle time ( gc ) were calculated . we found that pd_patients had significantly different gait parameters comparing to controls . they had 52% less sv , 60% less sl , and 40% longer gc . also they had significantly longer st and ds ( 11% and 59% more , respectively ) than controls . stn_dbs significantly improved gait parameters . during the stim on period , pd_patients had 31% faster sv , 26% longer sl , 6% shorter st , and 26% shorter ds . gc , however , was not significantly different . some of the gait parameters had high correlation with unified parkinson's_disease rating scale ( updrs ) subscores including sl with a significant_correlation ( r = -0 . 90 ) with updrs gait subscore . we concluded that our method provides a simple yet effective way of ambulatory gait_analysis in pd_patients with results confirming those obtained from much more complex and expensive methods used in gait labs . 
xmach-1 : a multi_user benchmark for xml data_management the specification of xmach-1 ( xml data_management benchmark , version 1 ) was developed at the university of leipzig in 2000 and published at the beginning of 2001 [b ra01] . it was the first xml_database benchmark . the benchmark defines a database of xml_documents and a set of operations covering important characteristics of xml processing and querying . key_features of xmach-1 are scalability , multiuser simulation and the evaluation of the entire data_management system . it has been sucessfully implemented for a variety of native xml_database systems and xml-enabled relational and object-relational dbms . the benchmark is based on a web_application in order to model a typical use case of a xml_data management system . the system architecture consists of four parts : the xml_database , application servers , loaders to populate the database and browser clients . the application servers run a web ( http ) server and other middleware components to support processing of the xml_documents and to interact with the backend database . the xml_database contains both document_centric and data-centric xml_documents . the largest part is document_centric consisting of semi_structured documents with larger text portions such as books or essays . these documents are synthetically produced by a parameterizable generator . in order to achieve close-to-reality results when storing and querying text contents , text is generated from the 10 , 000 most frequent english words , using a distribution corresponding to natural_language text . the documents varies in size ( 2-100 kb ) as well as in structure ( flat and deep element hierarchy ) . the second part of the database is a data-centric directory containing the metadata of the other documents such as document url , name , insert-and update time . all data in this document is stored in attributes ( no mixed content ) and the order of element siblings is free . compared to stuctured data in relational_databases it shows some semi_structured properties such as variable path length using recursive elements or optional attributes . the database can be scaled by increasing the number of documents , e . g . from 1000 text-oriented documents to 10 million documents . the metadata document scales proportionally with the number of text documents . a distinctive feature of xmach-1 is that documents may be schema-less or schema-based and that we increase the number schemas with the number of documents . this allows us to test a database system's ability to cope with an increasing number of different element types and to test query execution across multiple schemas . additionally the benchmark supports evaluating schema-less 
genetuc , genia and google : natural_language_understanding in molecular_biology literature with the increasing amount of biomedical literature , there is a need for automatic extraction of information to support biomedical researchers . genetuc has been developed to be able to read biological texts and answer questions about them afterwards . the knowledge_base of the system is constructed by parsing medline abstracts or other online text strings retrieved by the google api . when the system encounters words that are not in the dictionary , the google api can be used to automatically determine the semantic class of the word and add it to the dictionary . the performance of the genetuc parser was tested and compared to the manually tagged genia corpus with evalb , giving bracketing precision and recall scores of 70 , 6% and 53 , 9% respectively . genetuc was able to parse 60 , 2% of the sentences , and the pos-tagging accuracy was 86 . 0% . this is not as high as the best taggers and parsers available , but genetuc is also capable of doing deep reasoning , like anaphora resolution and question_answering , which is not a part of the state-of-the-art parsers . 
salem's secrets : a case study on hypothesis_testing and data_analysis* part i salem's secrets th ere was a chill in the courtroom that day a chill colder than could be explained by the unbearable winter . it was a cold that started at the back of the neck and lodged deep in the spine . something evil was afoot . th e question was : to whom did that evil belong ? " she killed goodwife betty's baby . she killed it with those evil eyes . i saw her staring , as in a trance , at betty's house at sunset one evening last week . th en her cow and her baby died . she also makes poisons in her house . when people won't take her poison , she sends her spirit to force them by choking them until they swallow it . i see her spirit here now . it is over near abby . oh abby , abby ! be careful abby , she has pins and they are red hot ! stop her , she is pricking me ! help me , i am burning help me " th e courtroom hummed with whispers as the spectators watched two young girls , elisabeth , the speaker , and abby , her best friend , tear and swat at their arms and legs as if swarmed by invisible bees . th eir contortions escalated into convulsive fi ts , which were so grotesque and violent that witnesses agreed they could not be manufactured . soon , as if on cue , other girls from elisabeth and abby's circle of friends joined in . th e_girls collapsed in exhaustion . dr . william griggs , the village physician , examined the girls and , fi nding only bruised skin , made a diagnosis; " the evil hand is upon them . th ey are bewitched . " hathorne , the magistrate , directed his attention to sarah good , the latest woman to be accused of witchcraft in salem in , and in a powerful voice demanded , " goodwife , why do you torture these girls so ? " " sir , i do not hurt them . " " who do you employ then to do it ? " " scientifi c research can reduce superstition by encouraging people to think and survey things in terms of cause and eff ect . certain it is that a conviction , akin to religious feeling , of the rationality or intelligibility of the world lies behind all scientifi c work of a higher_order . " albert_einstein
conae microwave_radiometer ( mwr ) counts to brightness_temperature algorithm this dissertation concerns the development of the microwave_radiometer ( mwr ) brightness_temperature ( tb ) algorithm and the associated algorithm validation using on-orbit mwr tb measurements . mission , a joint international science mission , between nasa and the argentine space agency ( comision nacional de actividades espaciales , conae ) . the mwr is a conae developed passive microwave instrument operating at 23 . 8 ghz ( k-band ) h-pol and 36 . 5 ghz ( ka_band ) h-& v-pol designed to complement the aquarius l-band radiometer/scatterometer , which is the prime sensor for measuring sea_surface salinity ( sss ) . mwr measures the earth's brightness_temperature and retrieves simultaneous , spatially collocated , environmental measurements ( surface wind_speed , rain rate , water_vapor , and sea_ice concentration ) to assist in the measurement of sss . this dissertation research addressed several areas including development of : 1 ) a signal_processing procedure for determining and correcting radiometer system non-linearity; 2 ) an empirical method to retrieve switch matrix loss coefficients during thermal-vacuum ( t/v ) radiometric calibration test; and 3 ) an antenna pattern correction ( apc ) algorithm using inter-satellite radiometric cross-calibration of mwr with the windsat satellite radiometer . the validation of the mwr counts-to-tb algorithm was performed using two years of on-orbit data , which included special deep_space calibration measurements and routine clear sky ocean/land measurements . iv acknowledgments
low_power systems on chips ( socs ) low_power issues for socs by christian piguet , csem . for innovative portable products , systems on chips ( socs ) containing several processors , memories and specialised modules are obviously required . performances but also low_power are main_issues in the design of such socs . are these low_power socs only constructed with low_power processors , memories and logic blocks ? if the latter are unavoidable , many other issues are quite important for low_power socs , such as the way to synchronise the communications between processors as well as test procedures , on-line testing , software_design and development_tools . this paper is a general framework for the design of low_power socs , starting from the system level to the architecture level , assuming that the soc is mainly based on the re-use of low_power processors , memories and logic peripherals . . socs with many processors , co-processors , memories and peripherals cannot be synchronised with a single master clock , due to larger and larger wire delays in deep sub-micron technologies . several clocking schemes have been proposed , such as gals ( globally asynchronous locally synchronous ) but also full asynchronous architectures . this paper will present the advantages_and_disadvantages of these soc clocking strategies as well as the impacts on low_power . . for embedded socs containing several processors , one has to write several pieces of software for each processor starting typically from a high-level specification using the c/c++ language . in order to tackle this problem , we propose to first transform the original specification by means of a systematic script of platform-independent source_code transformations . that is illustrated by applying global loop transformation techniques to identify asynchronous partitions exhibiting little communication and high locality of access characteristics . in a second stage , we explore multiple instruction multiple data ( mimd ) mapping onto a given ( partly ) predefined platform using advanced space time analysis techniques to maintain low data transfer rates while achieving_high system throughput . at the soc level , accurate cost feedback including high_level power estimation is required . from this essential information , energy trade_offs between application sub-modules can for example be used to refine the solution further . in the case of mapping onto programmable cores with a shared memory_hierarchy , a final refinement consists in reorganising the data layout for efficient cache utilisation . ward embedded_systems on chip is quite clear . systems on chip ( socs ) consist to integrate several components on the same chip in order to improve performances and reduce the cost . few years ago , a system was a multichip device containing 
usability studies of www sites : heuristic evaluation vs . laboratory testing this paper describes the strengths and weaknesses of two usability assessment methods frequently applied to web_sites . it uses case_histories of www usability studies conducted by the authors to illustrate issues of special interest to designers of web_sites . the discussion not only compares the two methods , but also discusses how an effective usability process can combine them , applying the methods at different times during site development . the two methods discussed in this paper for assessing the usability of web_sites both require the usability specialist to have three vital pieces of background information : the purpose of the web_site; profiles of its intended users; and typical scenarios for users accessing the site . these elements are equally important in evaluating the usability of any product or service , but here is how they apply especially to web_site evaluation . when discussing the purpose of a web_site , it's helpful to consider three categories . web_sites that supply descriptions of companies ( or other organizations ) and their products , services , informational offerings , or events can be described as informational sites . web_sites that provide explicit links to extensive databases are called search sites . web_sites that behave like products , where users perform other tasks in addition to reading or retrieving information , are referred to as transactional sites . multipurpose sites blur these boundaries . unfortunately , the definition of user in our increasingly web-centric environment is becoming more vague , because " anyone can access the site . " however , we must keep in mind which site visitors are the most likely or the most welcome and focus usability efforts on those subgroups . finally , the scenario for accessing a web_site might be a straightforward url to a home_page , or a more roundabout path through a link in search_engine results to a page deep in the bowels of a site . evaluators should keep in mind that any web page might be the user's door to that web_site . although users may perform more complex tasks in transactional sites , the free-form nature of navigation in any type of web_site makes ensuring ( and measuring ) success more complex in the web environment . 
player modeling , search_algorithms and strategies in multi-player games for a long_period of time , two person zero-sum games have been in the focus of researchers of various communities . the efforts were mainly driven by the fascination of special competitions like deep_blue vs . kasparov , and of the beauty of parlor games like checkers , backgammon , othello and go . multi-player games , however , have been inspected by far less , and although literature of game_theory fills books about equilibrium-strategies in such games , practical experiences are rare . korf , sturtevant and a few others started highly interesting research_activities . we have started investigating a four-person chess variant , in order to understand the peculiarities of multi player games without chance components . in this contribution , we present player models and search_algorithms that we tested in the four-player chess world . as a result , we can say that the more successful player models can benefit from more efficient algorithms and speed , because searching deeper leads to better results . moreover , we present a meta-strategy , which beats a paranoid alphabeta player , the best known player in multi player games . 
the characterizing substrate_coupling in deep_submicron_designs substrate_coupling ieee design & test of computers industry trend of integrating higher_levels of circuit functionality in chips designed for compact consumer electronic products and the widespread growth of wireless_communications have triggered the proliferation of mixed analog-digital systems . single_chip designs combining digital_and_analog blocks built over a common substrate feature reduced levels of power dissi-pation , smaller package counts , and smaller package interconnect parasitics . designing such systems , however , is becoming increasingly difficult owing to coupling problems resulting from the combined requirements for high_speed digital and high_precision analog components . noise coupling caused by the common chip substrate's nonideal isolation contributes significantly to the coupling problem in mixed_signal designs . 1 , 2 fast-switching logic components inject current into the substrate , causing voltage fluctuation . because substrate bias strongly affects the transistor threshold_voltage , voltage fluctuations can affect the operation of sensitive analog circuitry through the body effect . figure 1a illustrates this coupling mechanism , in which a switching digital node injects current into the substrate ( currents j 1 and j 2 are drawn to ground , but j 2 affects the analog transistor bulk potential ) , causing the local substrate potential v b to vary at an analog node . figure 1b illustrates this interaction from the circuit viewpoint . other known mechanisms for current injection into the substrate include hot-carrier injection and parasitic bipolar transistors . 2 the effects of substrate_coupling largely depend on the layout specifics . therefore , accurate analysis of these effects is possible only after extraction of the circuit features and the parasitics . as technology and circuit_design advance , substrate noise is beginning to plague even fully digital_circuits . in these circuits , the cumulative effect of thousands or millions of logic_gates changing state across the chip causes current pulses that are injected and absorbed into the substrate . those currents are then transmitted to power and ground buses through direct feed-through and load charge and discharge . such couplings are highly destructive because puls-ing currents , partially injected into the substrate through impact ionization and capacitive_coupling , can be broadcast over great distances and picked up by sensitive circuits through capacitive_coupling and the body effect . the resulting threshold_voltage modulation dynam-4 the accurate_modeling of noise-coupling effects caused by crosstalk through the substrate is an increasingly_important concern for design and verification of analog , digital , and mixed systems . with the technique described here , designers can efficiently extract accurate substrate_coupling parameters from deep_submicron_designs . ically changes gate delays locally , affecting performance unpredictably . switching noise 
cyborg systems as platforms for computer_vision algorithm-development for astrobiology employing the allegorical imagery from the film " the matrix " , we motivate and discuss our 'cyborg astro-biologist' research program . in this research program , we are using a wearable computer and video camcorder in order to test and train a computer_vision system to be a field-geologist and field-astrobiologist . 1 matrix preamble we choose to use the allegorical symbolism from the film , the matrix ( wachowski & wachowski , 1999 ) . ( the speaker emphasized the bold-faced words , and spoke very slowly and with a deep , precise voice , like the matrix character , morpheus , did . ) you also have a choice . you may choose between the blue pill and the red pill ( see figure 1 ) . fig . 1 . you have a choice between the blue pill on the right and the red pill on the left . ( this picture and all the following matrix pictures are from the original matrix film ( wachowski & wachowski , 1999 ) . ) if you should choose the blue pill , the next robots that you send to mars to search for life , will have no or little a . i . ( artificial_intelligence ) in them . all the science that these robots will do will be done by astrobiologists and geologists such as yourselves , here on the earth , after the data has been telemetried by the robots from mars back to earth . if you should choose the blue pill , all of the intelligence will remain here on the earth ( see figure 2 ) . however , if you should choose the red pill , these robots , bound for mars , will have on-board , scientific , astrobiolog-ical artificial_intelligence . this a . i . will allow you and the robots , together , to accomplish much more science , especially given the several minutes of delay for delivery of commands from the earth or for delivery of data from mars ( see figure 2 ) . fig . 2 . when we explore mars , you have a choice between keeping all the geologist intelligence in human form here on the blue_planet , the earth ( on the right ) , and sending significant geologist intelligence in robotic form to the red_planet , mars ( on the left ) . if you should choose the red pill , some of our intelligence will be bound for mars . . . in the mars exploration workshop in madrid , we demonstrated to you some of the early capabilities of our 'cyborg'
automatic test_generation for linear digital systems with bi-level search using matrix transform methods linear state variable digital systems , commonly implemented in bit-serial architecture using silicon compilers , are difficult to test for manufacturing defects due to deep sequentiality , low controllability and observability , and high latency . a novel hierarchical testing approach , based on matrix manipulation and constrained low_level test_generation , is reported here . feast ( functional extractor and sequential test generator ) operates at the high level , where the circuit is described as an interconnection of arithmetic modules . crest ( constrained sequential test generator ) operates at the low_level description of the individual modules , and generates test_sets satisfying constraints imposed by the high-level modules and their interconnection structure . the new approach was found to perform better when compared to automatic test_generation at the gate level using existing algorithms for several large circuits . 
model formulation : multimethod evaluation of information and communication technologies in health in the context of wicked problems and sociotechnical theory objective few research designs look at the deep_structure of complex social systems . we report the design and implementation of a multimethod evaluation model to assess the impact of computerized order entry systems on both the technical and social systems within a health_care organization . design we designed a multimethod evaluation model informed by sociotechnical theory and an appreciation of the nature of wicked problems . we mobilized this model to assess the impact of an electronic medication management system via a three-year program of research at a major academic hospital . measurements model components include measurements relating to three dimensions of system impact : safety and quality , organizational_culture , and work and communication patterns . results application of the evaluation model required the development_and_testing of purpose-built measurement tools such as software to collect multidimensional work measurement data . the model applied established research_methods including medication error audits and social_network_analysis . design features of these tools and techniques are described , along with the practical challenges of their implementation . the distinctiveness of doing research within a unique paradigm of complex systems , explicating the wickedness and the dimensionality of sociotechnical theory , is articulated . conclusion designing an effective evaluation model requires a deep understanding of the nature and complexity of the problems that information_technology interventions in health_care are trying to address . adopting a sociotechnical perspective for model generation improves our ability to develop evaluation models that are adaptive and sensitive to the characteristics of wicked problems and provides a strong theoretical basis from which to analyze and interpret findings . 
bonding to er-yag_laser-treated dentin . er-yag_laser irradiation has been claimed to improve the adhesive properties of dentin . we tested the hypothesis that dentin adhesion is affected by er-yag_laser conditioning . superficial or deep dentin from human molars was : ( a ) acid-etched with 35% h3po4; ( b ) irradiated with an er-yag_laser ( kavo ) at 2 hz and 180 mj , with water_cooling; and ( c ) laser- and acid-etched . single_bond ( 3m espe ) and z100 composite ( 3m espe ) were bonded to the prepared surfaces . after storage , specimens were tested in shear to failure . bonded interfaces were demineralized in edta and processed for transmission_electron_microscopy . two-way anova revealed that conditioning treatment and interaction between treatment and dentin depth significantly influenced shear bond_strength results . acid-etching alone yielded shear bond_strength values that were significantly_higher than those achieved with laser_ablation alone , or in combination with acid-etching . the er-yag_laser created a laser-modified layer that adversely affects adhesion to dentin , so it does not constitute an alternative bonding strategy to conventional acid etching . 
3d motion_planning for steerable needles using path sets introduction bevel-tipped flexible needles can be steered in soft_tissue to clinical targets along curved paths in 3d while avoiding critical structures . duty-cycled rotation [1] during insertion allows for control of the curvature of the needle . these capabilities of 3d steerable needles make it potentially suitable for applications such as deep_brain_stimulation ( dbs ) and drug_delivery to brain tumors [2] . manually guiding a steerable needle is unintuitive due to the high_dimensional nature of the problem . planning_algorithms can help provide a feasible motion plan for steering the needle between the start and target locations while avoiding critical structures in the tissue . imaging feedback is incorporated to control the needle along the pre-planned path . in this paper , we present an algorithm for path_planning of steerable needles based on a* graph search on path sets in 3d . path sets have been utilized extensively , in the past , for motion_planning of mobile_robots with nonholonomic constraints [3] . the formulation using path sets allows the planner to ouput paths that respect the differential or nonholonomic constraints associated with needle steering . compared to more commonly used sampling-based_approaches such as rrt [4 , 5 , 6] , an a* search-based_approach can guarantee optimality of the path with respect to a pre_defined cost function . since safety is of paramount importance in our application , we consider two factors in our path cost that influence safety : a ) path length and b ) curvature . critical structures are avoided by treating them as obstacles . materials and methods figure_1_shows the simulation environment used for evaluating our path_planning algorithm . to test our path_planning algorithm we use the following dbs application scenario in simulation . the needle enters near kocher's point on the cortical surface ( 2 . 5 cm off the midline at the level of the coronal suture ) and must reach the subthalamic nucleus . critical anatomical obstacles that must be avoided include the corticospinal tracts , basal_ganglia , and thalamus . these are approximated by combinations of simple geometrical shapes . algorithm : the algorithm utilizes the needle steering motion model
automatic indexing for content_analysis of whale recordings and xml representation this paper focuses on the robust indexing of sperm_whale hydrophone recordings based on a set of features extracted from a real-time passive underwater acoustic tracking algorithm for multiple whales using four hydrophones . acoustic localization permits the study of whale behavior in deep_water without interfering with the environment . given the position coordinates , we are able to generate different features such as the speed , energy of the clicks , inter-click-interval ( ici ) , and so on . these features allow to construct different markers which allow us to index and structure the audio files . thus , the behavior study is facilitated by choosing and accessing the corresponding index in the audio file . the complete indexing algorithm is processed on real_data from the nuwc ( naval_undersea_warfare_center of the us navy ) and the autec ( atlantic undersea test & evaluation center-bahamas ) . our model is validated by similar results from the us navy ( nuwc ) and soest ( school of ocean and earth_science and technology ) hawaii university labs in a single whale case . finally , as an illustration , we index a single whale sound file using the extracted whale's features provided by the tracking , and we present an example of an xml script structuring it . 
title of dissertation : compatibility testing for component- based systems compatibility testing for component-based systems many component-based systems are deployed in diverse environments , each with different components and with different component versions . to ensure the system builds correctly for all deployable combinations ( or , configurations ) , developers often perform compatibility testing by building their systems on various configurations . however , due to the large number of possible configurations , testing all configurations is often infeasible , and in practice , only a handful of popular configurations are tested; as a result , errors can escape to the field . this problem is compounded when components evolve over time and when test resources are limited . to address these problems , in this dissertation i introduce a process , algorithms and a tool called rachet . first , i describe a formal modeling scheme for capturing the system configuration_space , and a sampling criterion that determines the portion of the space to test . i describe an algorithm to sample configurations satisfying the sampling criterion and methods to test the sampled configurations . second , i present an approach that incrementally tests compatibility between components , so as to accommodate component evolution . i describe methods to compute test obligations , and algorithms to produce configurations that test the obligations , attempting to reuse test artifacts . third , i present an approach that prioritizes and tests configurations based on developers' preferences . configurations are tested , by default starting from the most preferred one as requested by a developer , but cost-related factors are also considered to reduce overall testing time . the testing approaches presented are applied to two large_scale systems in the high_performance_computing domain , and experimental results show that the approaches can ( 1 ) identify compatibility between components effectively and efficiently , ( 2 ) make the process of compatibility testing more practical under constant component evolution , and also ( 3 ) help developers achieve preferred compatibility results early in the overall testing_process when time and resources are limited . 2010 dedication to my wife heejong for her endless love and support . ii acknowledgements it is my honor to know so many good people who helped me to complete this dissertation . first and foremost , i deeply thank my advisor , prof . alan sussman . he guided me to set up my research direction and helped me with his deep insight to realize my rough ideas into concrete artifacts . he has always been generous to me and also encouraged me to be confident . i would also like to thank prof . adam porter and prof . atif memon . throughout discussions with them , i could get many ideas 
seachange : design of online quiz questions to foster deep_learning the design of different types of quiz question will influence the extent to which formative and summative feedback is presented to students . typically , quiz questions are considered limited in their capacity to assess higher_order cognitive skills . this paper extends the notion of online quiz design by presenting examples in a webct learning_environment in order to demonstrate a formative approach to assessment , closely integrated with learning processes . a matrix of questions is presented using bloom's taxonomy showing the type of question , pedagogical underpinnings and cognitive skills required . the implications of this type of question design is that automated quiz type questions do not necessarily imply a narrow focus on recall , but may assess a range of learning processes . limits of traditional assessment educators can be in no doubt of the demands of society for lifelong capable learners who are able to perform cognitive , metacognitive and metacognitive tasks and demonstrate competencies such as problem_solving , critical_thinking , questioning , searching for information , making judgments and evaluating information ( reeves , 2000 ) . assessment processes are now in the limelight , with increasing emphasis placed not on testing discrete skills or on measuring what people know , but on fostering learning and transfer of knowledge . the traditional approach to assessment is largely a form of objective testing which tends to value students' capacity to memorize facts and then recall them during a test situation . magone et al ( 1994 ) calls this the one right answer mentality . a second form of assessment is the measurement of competencies , or what we call 'sequestered problem_solving' ( schwartz et al , 2000 ) . in these contexts students are asked to solve problems in isolation and without the resources that are typically available in the real_world such as texts , web-resources and peers . often these tests of aptitude are single shot , and summative rather than formative . in contrast , assessment that supports learning and knowledge_transfer provides the basis for future learning , and continuing motivation to learn . this approach is sometimes called the alternative assessment movement , as it is concerned with assessing performance ( cumming & maxwell , 1999 ) . both testing and measuring competence as forms of assessment have been critiqued as being controlling , limiting and contrary to student-centered teaching_and_learning . other indicators of the need to rethink online and off_line assessment have come from bull & mckenna ( 2000 ) who argue that "the development and integration of computer_aided assessment has been done in an ad_hoc manner" . 
data_analysis knowledge among preservice elementary education teachers this study investigated whether elementary education majors in the teacher education program at montana_state_university ( msu ) acquire and retain knowledge of statistical data_analysis concepts and skills consistent with expectations specified in the nctm "principles and standards for school mathematics" ( 2000 ) . the following statistical topics were covered : finding , describing and interpreting mean , median and mode; interpreting the spread of a set of data; understanding the meaning of the shape and features of a graph; comparing centers , spreads , and graphical representations of related data_sets; and using scatter plots and lines of best fit . purpose the purpose of this study was to answer the question "to what degree do university students acquire and retain the statistical data_analysis content required for elementary and middle_school as defined by the national council of teachers of mathematics ? " background today's candidate teachers who will become instructors in grades 5-8 often have the same mathematics background as those who will become teachers in grade k-4 , yet they are expected to teach more complex content . the additional challenges inherent in the more ambitious curricular material often require them to be more like mathematics specialists than their original training may have prepared them to be . furthermore , these teachers often have had little exposure to some of the mathematical ideas that ambitious curricula will require them to teach . teaching mathematics and statistics in ways that make it understandable by students requires deep , flexible knowledge on the part of the teacher . " students need to know about data_analysis and related aspects of probability in order to reason statistically skills necessary to becoming informed citizens and intelligent consumers " ( national council of teachers of mathematics , 2000 p . 48 ) . to be effective , teachers must know their subject matter so thoroughly that they can present it in a challenging , clear , and compelling way ( ncate , 1998 ) . the national council of teachers of mathematics ( nctm ) offers detailed recommendations for teaching statistics in grades k-12 in both curriculum and evaluation standards for school mathematics ( 1989 ) and principles and standards for school mathematics ( 2000 ) . data_collection since the msu elementary teacher education program is stable in content and format from year to year , student ( n=232 ) characteristics were sampled simultaneously at four different stages of the teacher education program . students were given a 38-item paper/pencil test that examined their knowledge of elementary statistics applicable to elementary education . the nctm standards for statistics for prek-8 
field experiments in mobility and navigation with a lunar_rover prototype scarab is a prototype rover for lunar missions to survey resources , particularly water_ice , in polar craters . it is designed as a prospector that would use a deep coring drill and apply soil analysis instruments . its chassis can transform to stabilize its drill in contact with the ground and can also adjust posture to ascend and descent steep slopes . scarab has undergone field_testing at lunar analogue sites in washington and hawaii in an effort to quantify and validate its mobility and navigation capabilities . we report on results of experiments in slope ascent and descent and in autonomous kilometer-distance navigation in darkness . 
doa - the deductive object_oriented approach to the development of adaptive natural_language_interfaces ( abstract ) we present the deductive object_oriented approach ( doa ) , a new framework for natural_language interface_design . it uses a deduc-tive object_oriented database ( dood ) for developing the interface as component of the database system . this provides the basis for a consistent and ecient mapping of the user input to the target representation . furthermore , we propose adaptive techniques to deal eciently with the dicult task of dynamic knowledge_engineering . as rst feasibility test we have developed an adaptive interface for japanese , which is applied to the question support facility of a collaborative education system . despite the long tradition of the research eld of natural_language_interfaces , we have to face the situation that natural_language_interfaces are still far away from widespread practicable use [1] . one of the main responsible factors for this is missing integration , which results in insu-cient performance and wrong interpretations . therefore , we dene a new framework for natural_language interface_design in which the interface constitutes a component of the dood rock & roll [2] . especially its inheritance mechanisms make it possible to structure the linguistic_knowledge hierarchically to guarantee a compact representation . another main diculty for natural_language_interfaces is the high amount of necessary manual knowledge_engineering . for each portation to a new application this elaborate process has to be repeated . furthermore , the interfaces are often part of dynamic environments with constant changes concerning topics and user population . therefore , we apply linguistic resources and methods from machine_learning to the automatic acquisition of linguistic_knowledge . the applied interface architecture consists of the two main parts lexical and semantic component . the lexical component possesses three central modules : morpho-lexical_analysis , unknown value list ( uvl ) analysis , and spelling error correction . morpho-lexical_analysis performs the tok-enization of japanese input , i . e . the segmentation into individual input words . by accessing a domain-independent lexicon the module transforms the input into a deep form list ( dfl ) , which indicates for each token its surface form , category , and a set of associated deep forms . during uvl analysis we deal with domain-specic terms in the input and we also support spelling error correction of those terms . besides this , we apply the following three adaptive techniques : ( 1 ) the use of the
multiscale 3-d shape_representation and segmentation using spherical wavelets this paper presents a novel multiscale shape_representation and segmentation_algorithm based on the spherical wavelet_transform . this work is motivated by the need to compactly and accurately encode variations at multiple scales in the shape_representation in order to drive the segmentation and shape analysis of deep_brain_structures , such as the caudate_nucleus or the hippocampus . our proposed shape_representation can be optimized to compactly encode shape variations in a population at the needed scale and spatial locations , enabling the construction of more descriptive , nonglobal , nonuniform shape probability priors to be included in the segmentation and shape analysis framework . in particular , this representation addresses the shortcomings of techniques that learn a global shape prior at a single scale of analysis and cannot represent fine , local variations in a population of shapes in the presence of a limited dataset . specifically , our technique defines a multiscale parametric model of surfaces belonging to the same population using a compact set of spherical wavelets targeted to that population . we further refine the shape_representation by separating into groups wavelet coefficients that describe independent global and/or local biological variations in the population , using spectral graph partitioning . we then learn a prior_probability distribution induced over each group to explicitly encode these variations at different scales and spatial locations . based on this representation , we derive a parametric active surface evolution using the multiscale prior coefficients as parameters for our optimization procedure to naturally include the prior for segmentation . additionally , the optimization method can be applied in a coarse-to-fine manner . we apply our algorithm to two different brain_structures , the caudate_nucleus and the hippocampus , of interest in the study of schizophrenia . we show : 1 ) a reconstruction task of a test set to validate the expressiveness of our multiscale prior and 2 ) a segmentation task . in the reconstruction task , our results show that for a given training_set size , our algorithm significantly improves the approximation of shapes in a testing set over the point distribution model , which tends to oversmooth data . in the segmentation task , our validation shows our algorithm is computationally efficient and outperforms the active_shape_model algorithm , by capturing finer shape details . 
volatility and option pricing acknowledgements it is a pleasure to thank those who made this thesis possible . it is difficult to overstate my gratitude to my ph . avellaneda . his great enthusiasm , deep inspiration , and insightful understanding of financial_mathematics influenced me tremendously ever since the beginning of my phd study at courant . i deeply appreciate his patience , efforts , and creativity in guiding me . this thesis would not have been possible without him . i would like to thank prof . jonathan goodman for his passionate teaching and his time answering my questions . i am also grateful to prof . robert kohn for encouraging and personal guidance . i wish to express my sincere thanks to prof . jim gatheral for reading my thesis and his constructive comments . i thank prof . petter kolm for being in my thesis committee . my warm thanks are due to mike lipkin for his extensive discussions around my work . i wish to extend my thanks to all those who have helped me with my thesis at courant . the financial_support of the maccracken fellowship program is gratefully acknowledged . i am indebted to my many student colleagues for providing a stimulating and fun environment in which to learn and grow . i am lucky to meet these intellectual and caring people , who have helped me through the difficult times . i owe my special loving thanks to stella chen for supporting me spiritually throughout my study . abstract this thesis studies leveraged exchange-traded funds ( etf ) and options written on them . in the first part , we give an exact formula linking the price evolution of a leveraged etf ( letf ) with the price of its underlying etf . we test the formula empirically on historical data for 56 leveraged funds ( 44 double-leveraged , 12 triple-leveraged ) using daily closing prices . the results indicate excellent agreement between the formula and the empirical data . the formula shows that the evolution of the price of an letf is sensitive to the realized volatility of the underlying product . the relationship between an letf and its underlying asset is " path-dependent . " the second part of the study focuses on the relations between options on letfs and options on the underlying etfs . the main result shows that an option on an letf can be replicated by a basket of options on the underlying etf after a suitable choice of strikes and notionals . in particular , we obtain a new , relative-value , model for pricing letf options . the derivation makes strong use 
noise tolerant low_voltage xor_xnor for fast arithmetic with scaling down to deep_submicron and nanometer_technologies , noise immunity is becoming a metric of the same importance as power , speed , and area . smaller feature_sizes , low_voltage , and high_frequency are the characteristics for deep_submicron circuits . this paper proposes a low_voltage noise tolerant xor_xnor gate with 8 transistors . the proposed gate has been implanted in an already existing ( 5-2 ) compressor cell to test its driving capability . the proposed gate is characterized and compared with those published ones for reliability and energy efficiency . the average noise threshold energy ( ante ) and the energy normalized ante metrics are used for quantifying the noise immunity and energy efficiency respectively . results using 0 . 18m cmos_technology and hspice for simulation show that the proposed xor_xnor circuit is more noise-immune and displays better power-delay product characteristics than the compared circuit . also , the circuit proves to be faster in operation and works at all ranges of supply_voltage starting from 0 . 6v to 3 . 3v . 
fast path_selection for testing of small_delay_defects considering path correlations statistical_timing models have been proposed to describe delay_variations in very deep sub-micro process_technologies , which have increasingly significant influence on circuit_performance . under a statistical_timing model , testing of a path can detect potential delay failures caused by different small_delay_defects . due to path correlations , the potential delay failures captured by two different paths overlap between each other more or less . it is difficult to find a given number of paths that can capture most potential delay failures . in this paper , the path_selection problem is converted to a minimal space intersection problem , and a greedy path_selection heuristics is proposed , the key point of which is to calculate the probability that the paths in a specified path set all meet the delay constraint . statistical_timing analysis technologies and heuristics are used in the calculation . experimental results show that the proposed approach is time-efficient and achieves a higher probability of capturing delay failures in comparison with conventional path_selection approaches . 
finding parametric representations of the cortical sulci using an active_contour model the cortical sulci are brain_structures resembling thin convoluted ribbons embedded in three dimensions . the importance of the sulci lies primarily in their relation to the cytoarchitectonic and functional organization of the underlying cortex and in their utilization as features in non_rigid_registration methods . this paper presents a methodology for extracting parametric representations of the cerebral sulcus from magnetic_resonance_images . the proposed methodology is based on deformable_models utilizing characteristics of the cortical shape . specifically , a parametric representation of a sulcus is determined by the motion of an active_contour along the medial surface of the corresponding cortical fold . the active_contour is initialized along the outer boundary of the brain and deforms toward the deep root of a sulcus under the influence of an external force field , restricting it to lie along the medial surface of the particular cortical fold . a parametric representation of the medial surface of the sulcus is obtained as the active_contour traverses the sulcus . based on the first fundamental form of this representation , the location and degree of an interruption of a sulcus can be readily quantified; based on its second fundamental form , shape properties of the sulcus can be determined . this methodology is tested on magnetic_resonance_images and it is applied to three medical_imaging problems : quantitative morphological analysis of the central_sulcus; mapping of functional activation along the primary_motor_cortex and non_rigid_registration of brain_images . 
linguistic feature analysis for protein_interaction extraction background the rapid growth of the amount of publicly available reports on biomedical experimental_results has recently caused a boost of text_mining approaches for protein_interaction extraction . most approaches rely implicitly or explicitly on linguistic , i . e . , lexical and syntactic , data extracted from text . however , only few attempts have been made to evaluate the contribution of the different feature types . in this work , we contribute to this evaluation by studying the relative importance of deep syntactic_features , i . e . , grammatical relations , shallow syntactic_features ( part-of-speech information ) and lexical features . for this purpose , we use a recently proposed approach that uses support_vector_machines with structured kernels . results our results_reveal that the contribution of the different feature types varies for the different data_sets on which the experiments were conducted . the smaller the training corpus compared to the test data , the more important the role of grammatical relations becomes . moreover , deep syntactic_information based classifiers prove to be more robust on heterogeneous texts where no or only limited common vocabulary is shared . conclusion our findings_suggest that grammatical relations play an important role in the interaction extraction task . moreover , the net advantage of adding lexical and shallow syntactic_features is small related to the number of added features . this implies that efficient classifiers can be built by using only a small fraction of the features that are typically being used in recent approaches . 
performance-driven register insertion in placement as the cmos_technology is scaled into the dimension of nanometer , the clock frequencies and die sizes of ics are shown to be increasing steadily [5] . today , global wires that require multiple clock cycles to propagate electrical signal are prevalent in many deep_sub_micron designs . efforts have been made to pipe ine the long wires by introducing registers along these global paths , trying to reduce the impact of wire delay dominance [2 , 8] . the technique of retiming to relocate registers in a circuit without affecting the circuit functionality can be applied in this problem . though the problem of retiming with gate and wire delay has been studied recent y [17 , 1] , the placement of registers after retiming is a new challenge . in this paper , we study the problem of realizing a retiming solution on a global netlist by inserting registers in the placement to achieve the target clock period . in contrast to many previous_works [16 , 11] that performed simple calculations to determine the positions of the registers , our proposed algorithm can preserve the given clock period and utilize as few registers as possible in the realization . what is more , the algorithm is shown to be optimal for nets with 4 or fewer pins and this type of nets constitutes over 90% of the nets in a sequential circuit on average . using the iscas89 benchmark suite , we tested our algorithm with a 0 . 35 &#956;m cmos standard cell_library , and silicon ensemble was used to layout the design with row utilization of 50% . experimenta results_showed that our algorithm can find the best sharing of registers for a net in most of the cases , i . e . , using the minimum number of registers while preserving the target clock period , within a minute running on an intel_pentium iv 1 . 5ghz pc with 512mb ram . 
applying the model-view-controller paradigm to adaptive_test putting it all together yield_learning processes and methods mvc implementation experimental demonstration yield_learning processes and methods h despite the growing complexity of semiconductor devices , adaptive_test deployments for such devices remain elusive . the high cost and difficulties of developing a custom adaptive_test solution are likely causes for this sparsity . indeed , there are many requirements that an adaptive test solution should meet . a good adaptive_test system should dynamically adjust to process_variation statistics that may not be stationary . it should modulate the test_program at high granularities ( die level or even sub-die level ) , have low adaptation latency to achieve real-time adaptivity , and comfortably handle learning from terabyte-scale historical test_data . moreover , it should achieve these goals without violating stringent industrial requirements on permissible test error . finally , many of the specific details of these requirements are likely to differ across product lines , e . g . , consumer products certainly have very different comfort levels for permissible test error than automotive products . however , the advantages of a successful adaptive_test deployment are clear : test time reduction , test_quality improvement , improved data_analysis ability , and acceleration of yield_learning . therefore , substantial industrial interest exists for making adaptive_test a reality in the near term . this interest is driving groups from a broad swath of industry to collaborate on adaptive_test development . the test & test_equipment team within the international_technology_roadmap_for_semiconductors ( itrs ) has created a subgroup specifically targeted at the adaptive_test problem [1] . the requirements that an adaptive test system must meet can be broken down to three discrete sets of challenges . first , there is the problem of data handling : device , wafer , and lot measurements should be linked and traceable throughout the fabrication process , data should be standardized and easy to exchange between adaptive_test programs and throughout the organization , and large data_sets should be readily accessible . second , innovation within the adaptive_test community is rapidly increasing the number of adaptive_test techniques that have been demonstrated in literature . thus , an adaptive test system should simplify integration of new adaptive_test techniques as they become available . third , test engineers should be provided with a convenient means of obtaining deep visibility into the inner workings of an adaptive test system . for editor's note : adaptive testing has been a focus area for ic testing in the last few years . the ''model-view-controller'' ( mvc ) architecture has the potential to improve engineering productivity for analysis and application of adaptive testing . the vision of mvc is that it will be used to 
moderate effect of job_commitment on the relationship between employees' emotional_labor and burnout emotional_labor has important effect on jobs which contact people frequently , such as teachers , salespersons and so on . it is necessary to explore the circumstances of emotional_labor about these special jobs . this study tested the impact of emotional_labor on job burnout with the methods of interview and questionnaire survey , and explored the moderate effect of job_commitment on the relationship between employees' emotional_labor and burnout for teachers and sales employees . results_showed that 1 ) surface acting can positively predict exhaustion and depersonalization , while deep acting negatively predict all three components of burnout; 2 ) employees with higher job_commitment tend to use more deep acting in work than those with lower job_commitment; 3 ) job_commitment can moderate the relationship between surface acting and depersonalization . the relationship will be strong while job_commitment is high , and will be weak while job_commitment is low . at last , compared with teachers , sales employees tend to use more surface acting in work . 
semi_automatic syntactic_and_semantic corpus annotation with a deep_parser we describe a semi_automatic method for linguistically rich corpus annotation using a broad-coverage deep_parser to generate syntactic_structure , semantic representation and discourse information for task-oriented dialogs . the parser-generated analyses are checked by trained annotators . incomplete coverage and incorrect analyses are addressed through lexicon and grammar development , after which the dialogs undergo another cycle of parsing and checking . currently we have 85% correct annotations in our emergency rescue task domain and 70% in our medication scheduling domain . this iterative process of corpus annotation allows us to create domain_specific gold_standard corpora for test_suites and corpus-based experiments as part of general system development . 
ab_initio galaxy_formation the formation and evolution of galaxies can be followed in the context of cosmological structure_formation using the technique of semi-analytic modelling . we give a brief outline of the features incorporated into the semi-analytic model of cole etal ( 1999 ) . we present two examples of model predictions that can be tested using photometric red-shift techniques . the first prediction , of the star_formation history of the universe , has already been shown to be in broad argeement with the observational estimates . the second prediction , of the evolution of galaxy clustering with redshift , will be addressed with some of the forthcoming deep , multi-filter imaging surveys discussed at this meeting . 
fault_tolerant bus architecture for deep_submicron based processors in the deep_submicron_era testing of processors is gaining prominence due to the effect of many analog faults . these faults have a direct impact on the signal_integrity in interconnects . in deep_submicron based processors electromigration poses a major challenge for interconnect reliability . this is of major concern in processor bus architectures . though a lot of research has been focused on fault_tolerance at the module level , no effective research has been carried out to evolve fault_tolerant interconnect structures . faults due to electromigration give rise to weak ones and zeros , thus affecting the signal_integrity . this paper presents a fault tolerant scheme for electromigration which also leads to unification of detection and correction of crosstalk_faults and stuck_at_faults . in the scheme presented , the signal is rerouted through adjacent spare interconnects . the spare interconnect also acts as a shield against crosstalk under fault_free system operation . moreover , the entire testing_process is carried out concurrently with the processor instruction execution . the hardware_overhead is also insignificant compared to the overall device count in a deep_submicron based processor . to the best of our knowledge this is the first time a fault tolerant scheme for bus architectures is being presented . 
a readability checker with supervised_learning using deep indicators checking for readability or simplicity of texts is important for many institutional and individual users . formulas for approximately measuring text readability have a long tradition . usually , they exploit surface-oriented indicators like sentence length , word length , word frequency , etc . however , in many cases , this information is not adequate to realistically approximate the cognitive difficulties a person can have to understand a text . therefore we use deep_syntactic_and_semantic_indicators in addition . the syntactic_information is represented by a dependency_tree , the semantic_information by a semantic_network . both representations are automatically_generated by a deep syntactico-semantic_analysis . a global readability score is determined by applying a nearest neighbor algorithm on 3 , 000 ratings of 300 test persons . the evaluation showed that the deep_syntactic_and_semantic_indicators lead to promising_results comparable to the best surface-based indicators . the combination of deep_and_shallow indicators leads to an improvement over shallow indicators alone . finally , a graphical user interface was developed which highlights difficult passages , depending on the individual indicator values , and displays a global readability score . povzetek : strojno u enje z odvisnostnimi drevesi je uporabljeno za ugotavljanje berljivosti besedil . 
a deep validation process for open document_repositories institutional document_repositories show a systematic growth as well as a sustainable deployment . therefore , they represent the current backbone of a distributed repository infrastructure . many developments for electronic_publishing through digital repositories are heading in the direction of innovative value-added services such as citation_analysis or annotation systems . a rich service-layer based on machine-to-machine communication between value-added services and document_repositories requires a reliable operation and data_management within each repository . aggregating search services such as oaister and base provide good results . but in order to provide good quality they also have to overcome heterogeneity by normalizing many of the data they receive and build specific profiles for sometimes even one individual repository . since much of the normalization is done at the side of the service provider , it often remains unclear maybe sometimes also to the manager of a local repository exactly which data and services are exposed to the public . here , an exploratory validation method for testing specification compliance in repositories is presented . 1 . introduction many developments for electronic_publishing through digital repositories are heading in the direction of innovative value-added services ( e . g . citation_analysis or annotation systems ) and abstract data representation of all types of resources ( e . g . primary data or educational material ) . still , conventional document_repositories show the most systematic growth [1] as well as high_quality , sustainable deployment . therewith , they represent the current backbone of a distributed repository infrastructure . a rich service-layer based on machine-to-machine communication between value-added services and document_repositories requires a reliable operation and data_management within each repository . aggregating search services such as oaister [2] and base [3] provide good results . but in order to provide good quality they also have to normalize many of the data they receive and build specific profiles and processes for even individual repositories . since much of the normalization is done at the side of the service provider , it often remains unclear maybe sometimes also to the manager of a repository exactly which data and services are exposed to the public by a local data provider . existing validation techniques [4 , 5] are important but their current scope is only at the level of testing basic compliance with oai-pmh and simple-dc . as a consequence , quantitative assessments of data_quality , i . e . of what is specifically exposed by a repository and by the whole repository landscape are widely missing . mature infrastructures should , however , provide reliable data resources , robust 
pennsylvania earthquake pennsylvania earthquake digest earthquakes in midplate regions have been considered enigmatic since they cannot be easily associated with major plate_boundary deformations . in north_america , small magnitude midplate seismicity is common in the a moderate-size earthquake occurred in northwestern pennsylvania , near the border of ohio and pennsylvania . rapid analyses of seismic waveforms generated by the m blg 5 . 2 penn-sylvania_ohio border region earthquake suggested an unusual , non_double_couple component to the faulting mechanism . the existence of a substantial non_double_couple component to the faulting mechanism has important implications for the cause of the earthquake ( hydrologically induced shallow faulting or " typical " eastern north_america basement faulting ? ) . preliminary checks of the near real-time solutions suggested the non_double_couple component may have been an artifact caused by the available data . one of the goals of this study was to investigate the size of the non_double_couple faulting component in the pymatuning earthquake the other was to improve mechanism and depth estimates for the event . to investigate the detailed nature of this event i used the observed seismograms ( from the united_states national seismic network , usnsn , and from the canada national seismic network , cnsn ) to constrain faulting parameters including the source depth , fault strike , dip , and slip , and to explore the reason ( s ) why early estimates contained large non_double_couple 1 2 source components . i performed moment tensor inversions with l2 and l1 norms for the closest stations ( epicentral distance less than 1000 km ) , and the results agree with the previous near-real time studies . to test the significance of the non_double_couple component i sought a solution constrained to be a pure double_couple by checking the match to the observations for all values of strike , dip , and rake ( grid search ) for depths between 2 . 5 and 25 km . the final results show that the pymatuning earthquake can be explained with a pure double_couple faulting mechanism , corresponding to a near-vertical , mostly strike-slip fault with planes striking 110 o and 13 o , with dips of 70 o and 71 o , and rakes of 20 o and 159 o . the estimated moment for this inversion is 5 . 610 22 dyne-cm , which corresponds to a moment_magnitude of 4 . 5 . all three inversions ( l2 , l1 moment tensor and l1 grid search ) match the observed seismograms well for a source depth less than 7 . 5 km . the best " formal " fit is for a 2 . 5 km deep source , but the 
an efficient parallelized l7_filter design for multicore servers l7_filter is a significant deep_packet_inspection ( dpi ) extension to netfilter in linux's qos framework . it classifies network_traffic based on information hidden in the packet payload . although the computationally intensive payload classification can be accelerated with multiple processors , the default os scheduler is oblivious to both the software characteristics and the underlying multicore architecture . in this paper , we present a parallelized l7_filter algorithm and an efficient scheduler technique for multicore servers . our multithreaded l7_filter algorithm can process the incoming packets on multiple servers boosting the throughput tremendously . our scheduling_algorithm is based on highest random weight ( hrw ) , which maintains the connection locality for the incoming traffic , but only guarantees load balance at the connection level . we present an adapted highest random weight ( ahrw ) algorithm that enhances hrw by applying packet-level load_balancing with an additional feedback vector corresponding to the queue length at each processor . we further introduce a hierarchical ahrw ( ahrw-tree ) algorithm that considers characteristics of the multicore architecture such as cache and hardware topology by developing a hash tree architecture . the algorithm reduces the scheduling overhead to <i>o</i> ( log <i>n</i> ) instead of <i>o</i> ( <i>n</i> ) and produces a better balance between locality and load_balancing . results show that the ahrw-tree scheduler can improve the l7_filter throughput by about 50% on a sun-niagara- 2-based server compared to a connection locality-based scheduler . although extensively tested for l7_filter traces , our technique is applicable to many other packet processing applications , where connection locality and load_balancing are important while executing on multiple processors . with these speedups and inherent software flexibility , our design and implementation provide a cost_effective alternative to the traffic monitoring and filtering asics . 
radiotelemetry comes of age--perhaps just in time . in actuality , the discipline of functional_genomics deals with the science and art of phenotyping . identifying the genetic bases for complex integrated systems , as well as complex diseases , relies on the validity and accuracy of the " trait measurement . " any trait , real or erroneous , may be mapped , whether in humans or animal models . such errors in quantitative trait locus mapping , or even gene assignment , may be compounded because few replicate studies are conducted . in an effort to avoid such errors , behavioral scientists use three tests of " validity " for a trait ( phenotype ) : " construct , " " face , " and " predictive " ( 11 , 12 ) . however , even these tests may be insufficient if the actual measurement methodology is compromised . there may be many reasons that contribute to inadequate , inappropriate , or erroneous phenotyping , among them the potential interaction between the experimenter ( or environment ) and the test subject . in human studies the cognitive capabilities of the test subject present the challenge . this problem is further exacerbated with animal models , mainly due to the inability to bridge the cognitive gap between animals and humans . one cannot simply ask a rat or mouse how they feel that day or whether the technician's aftershave lotion is disturbing to them ! technology has encouraged major advances over the past decade in monitoring physiological and behavioral traits of human and model organisms . among the factors contributing to these advances are innovations in the fields of computer and information_technology , as well as miniaturization of instru-mentation needed for telemetry or non-hard-wired monitoring . these advances address a major confound , the " biological uncertainty_principle , " which derives from the influence of measurement method and/or experimenter on trait expression or subject performance . the development of small implantable telemetry probes has sparked a revolution in the laboratory not unlike that ignited in molecular_biology and medicine through the discovery of thermostable polymerases and the development of the polymerase_chain_reaction . commercialization of small implantable telemetry probes has made such instrumen-tation available to nearly every investigator . recent advances in radiotelemetry probes provide new opportunities for real-time monitoring of traits such as deep body temperature; motor , neuronal , and cardiovascular activities; and even sleep states . furthermore , radiotelemetry introduces additional opportunities in experimental design by permitting time_series measurements over days , weeks , and months . transference of the methodology to even smaller rodents , such as mus , combined with the assumption of the mus species as an 
neural_network regularization via robust weight factorization regularization is essential when training large neural_networks . as deep_neural_networks can be mathematically interpreted as universal function approximators , they are effective at memorizing sampling noise in the training data . this results in poor generalization to unseen data . therefore , it is no surprise that a new reg-ularization technique , dropout , was partially responsible for the now-ubiquitous winning entry to imagenet 2012 by the university of toronto . currently , dropout ( and related methods such as dropconnect ) are the most effective means of regu-larizing large neural_networks . these amount to efficiently visiting a large number of related models at training time , while aggregating them to a single predictor at test time . the proposed fame model aims to apply a similar strategy , yet learns a factorization of each weight matrix such that the factors are robust to noise . 
spare line borrowing technique for distributed memory_cores in soc in this paper , a new architecture of distributed embedded memory_cores for soc is proposed and an effective memory repair method by using the proposed spare line borrowing ( software-driven reconfiguration ) technique is investigated . it is known that faulty cells in memory core show spatial locality , also known as fault clustering . this physical phenomenon tends to occur more often as deep_submicron_technology advances due to defects that span multiple circuit elements and sophisticated circuit_design . the combination of new architecture & repair method proposed in this paper ensures fault_tolerance enhancement in soc , especially in case of fault clustering . this fault_tolerance enhancement is obtained through optimal redundancy utilization : spare redundancy in a fault-resistant memory core is used to fix the fault in a fault-prone memory core . the effect of spare line borrowing technique on the reliability of distributed memory_cores is analyzed through modeling and extensive parametric simulation . i . introduction system on a chip ( soc ) is driving the vlsi ( very_large_scale_integration ) industry today . chip designers can implement everything from processors , memories , and other logic cores to bus interfaces onto a single chip with the deep_submicron_technology . due to the fact that once system on a chip ( soc ) is fabricated it is impossible to remove , recondition , and replace a faulty core deeply embedded into the chip , test and repair process is one of the challenging issues for the success of soc_design . therefore , more reliable and dependable soc_design with built-in self test/diagnosis/ repair is desirable and more suggested than
cutting recursive autoencoder trees deep_learning models enjoy considerable success in natural_language_processing . while deep_architectures produce useful representations that lead to improvements in various tasks , they are often difficult to interpret . this makes the analysis of learned structures particularly difficult . in this paper , we rely on empirical tests to see whether a particular structure makes sense . we present an analysis of the semi_supervised recursive autoencoder , a well-known model that produces structural representations of text . we show that for certain tasks , the structure of the autoencoder can be significantly reduced without loss of classification_accuracy and we evaluate the produced structures using human judgment . 
automated generation of massive image knowledge collections using microsoft live labs pivot to promote neuroimaging and translational_research background massive datasets comprising high_resolution images , generated in neuro-imaging studies and in clinical imaging research , are increasingly challenging our ability to analyze , share , and filter such images in clinical and basic translational_research . pivot collection exploratory analysis provides each user the ability to fully interact with the massive amounts of visual data to fully facilitate sufficient sorting , flexibility and speed to fluidly access , explore or analyze the massive image_data sets of high_resolution images and their associated meta information , such as neuro-imaging databases from the allen brain_atlas . it is used in clustering , filtering , data sharing and classifying of the visual data into various deep zoom levels and meta information categories to detect the underlying hidden pattern within the data_set that has been used . method we deployed prototype pivot collections using the linux centos running on the apache web_server . we also tested the prototype pivot collections on other operating_systems like windows ( the most common variants ) and unix , etc . it is demonstrated that the approach yields very good results when compared with other approaches used by some researchers for generation , creation , and clustering of massive image collections such as the coronal and horizontal sections of the mouse brain from the allen brain_atlas . results pivot visual_analytics was used to analyze a prototype of dataset dab2 co-expressed genes from the allen brain_atlas . the metadata along with high_resolution images were automatically extracted using the allen brain_atlas api . it is then used to identify the hidden information based on the various categories and conditions applied by using options generated from automated collection . a metadata category like chromosome , as well as data for individual cases like sex , age , and plan attributes of a particular gene , is used to filter , sort and to determine if there exist other genes with a similar characteristics to dab2 . and online access to the mouse brain pivot collection can be viewed using the link http : //edtech-dev . uthsc . edu/ctsi/teedev1/unittest/papa/collection . html ( user name : tviangte and password : demome ) conclusions our proposed algorithm has automated the creation of large image pivot collections; this will enable investigators of clinical_research projects to easily and quickly analyse the image collections through a perspective that is useful for making critical decisions about the image patterns discovered . 
testability of cryptographic_hardware and detection of hardware trojans cryptographic_algorithms are routinely used to perform computationally intense operations over increasingly larger volumes of data , and in order to meet the high_throughput requirements of the applications , are often implemented by vlsi_designs . the high complexity of such implementations raises concern about their reliability . in order to improve upon the testability of sequential_circuits , both at fabrication time and also in the field , design_for_testability ( dft ) techniques are commonly employed . however conventional dft methodologies for digital_circuits have been found to compromise the security of the cryptographic_hardware . in this tutorial we first discuss the challenges and potential attacks on cipher hardware through standard dft techniques , and then potential solutions against them . also , as the electronic design industry has grown globally , economic reasons dictate the widespread participation of external agents in modern design and manufacture of integrated_circuits ( ics ) , which decreases the control that the ic design houses used to traditionally have over their own designs . this issue raises the question of ensuring trust in an integrated_circuit , and whether the ic can be certified to be free of malicious , hard-to-detect circuitry , commonly referred to as hardware trojans . in this tutorial , we would explore the unique challenges and testing solutions to detect/prevent such malicious modifications . i . introduction reliability of devices has become a serious concern , with the increase of complexity of ics and the advent of deep_sub_micron process_technology . the growth of applications of cryptographic_algorithms and their requirement for real time-processing has necessitated the design of crypto-hardware . but along with the design of such devices , testability is a key issue . what makes testability of these circuits more challenging compared to other digital designs , is the fact that popular design_for_testability ( dft ) methodologies , such as " scan_chain insertion " , can be used as a double-edged sword . scan_chain insertion , which is a desirable test technique owing to its high fault coverage and small hardware_overhead , open " side-channels " for cryptanalysis [1] , [2] . scan_chains are used to access intermediate values stored in the flip_flops , thereby ascertaining the secret information , often known as " key " . conventional scan_chains fail to solve the conflicting requirements of effective testing and security [2] . so , one of the solutions that have been suggested is to blow off the scan_chains from the crypto ics , before they are released into the market . but such an approach is unsatisfactory and directly conflicts the paradigm 
research on fault_diagnosis for cnc machine of flexible manufacturing_system in order to find and treat fault or abnormal occurrences in numerically controlled machine_tool of flexible manufacturing_system in real-time , a study for computer numerically controlled ( cnc ) on-line fault diagnostics system has been carried out . the system can look up one or more sticking points of numerically controlled machine_tool fault simply , locate cnc faults rapidly and accurately , but solve the problem between fault_diagnosis and knowledge application . to provide cnc apparatus terminal with reliable trouble shooting suggestion , give a deep analysis between servo control subassembly and background service , make a description of fault database and test procedure design . the research has solved the problem of system maintenance , expansion and upgrade . 
vers un syste me ge ne rique de re e criture de graphes pour l enrichissement de structures syntaxiques r_sum ce travail pr sente une nouvelle approche pour injecter des d pendances profondes ( sujet des verbes contr le , partage du sujet en cas d'ellipses , . . . ) dans un corpus arbor pr sentant un sch ma d'annotation surfacique et projectif . nous nous appuyons sur un syst me de r criture de graphes utilisant des techniques de programmation par contraintes pour produire des r gles g n riques qui s'appliquent aux phrases du corpus . par ailleurs , nous testons la g n ricit des r gles en utilisant des sorties de trois analyseurs syntaxiques diff rents , afin d' valuer la d gradation exacte de l'application des r gles sur des analyses syntaxiques pr dites . abstract towards a generic graph rewriting system to enrich syntactic structures this work aims to present a new approach for injecting deep dependencies ( subject of control verbs , subject sharing in case of ellipsis , . . . ) into a surfacic and projective treebank . we use a graph rewriting system with constraint_programming techniques for producing generic rules which can be easily applied to a treebank . moreover , we are testing the genericity of our rules by using output of three different parsers to evaluate how the rules behave on predicted parse_trees . mots-cl s : r criture de graphes , valuation de sh ma d'annotations , parsing , analyse en syntaxe profonde . 
qualitative_case study guidelines although widely used , the qualitative_case study method is not well understood . due to conflicting epistemological presuppositions and the complexity inherent in qualitative_case-based studies , scientific rigor can be difficult to demonstrate , and any resulting findings can be difficult to justify . for that reason , this paper_discusses methodological problems associated with qualitative_case-based research and offers guidelines for overcoming them . due to its nearly universal acceptance , yin's six-stage case_study process is adopted and elaborated on . moreover , additional principles from the wider methodological literature are integrated and explained . finally , some modifications to the dependencies between the six case_study stages are suggested . it is expected that following the guidelines presented in this paper may facilitate the collection of the most relevant data in the most efficient and effective manner , simplify the subsequent analysis , as well as enhance the validity of the resulting findings . the paper should be of interest to students ( honour , masters , doctoral ) , academics , and practitioners involved with conducting and reviewing qualitative_case-based studies . where quantitative research is mainly concerned with the testing of hypotheses and statistical generalisations ( jackson , 2008 ) , qualitative_research does not usually employ statistical procedures or other means of quantification , focusing instead on understanding the nature of the research problem rather than on the quantity of observed characteristics ( strauss & corbin , 1994 ) . given that qualitative researchers generally assume that social reality is a human creation , they interpret and contextualise meanings from people's beliefs and practices ( denzin & lincoln , 2011 ) . case_study research involves " intensive study of a single unit for the purpose of understanding a larger class of ( similar ) units observed at a single point in time or over some delimited period of time " ( gerring , 2004 , p . 342 ) . as such , case_studies provide an opportunity for the researcher to gain a deep holistic view of the research problem , and may facilitate describing , understanding and explaining a research problem or situation besides being widely used in academia , the method is also popular with practitioners as a tool for evaluation and organisational learning . however , although widely used , the qualitative_case study method is not well understood given the considerable time and resource requirements associated with conducting such studies ( gao , 1990 ) , any misunderstandings regarding the purpose and implementation of the method as well as the validity of the resulting findings can have significant negative consequences . in the context of academic studies , significant misunderstandings identified during the 
preventing single_event latchup with deep p-well on p-substrate we propose a method that prevents single_event latchup ( sel ) using deep p-well on p-substrate . to confirm the effectiveness of the proposed method , sel and single_event_upset ( seu ) are evaluated for three well configurations; double-well , ordinary triple-well and the proposed deep p-well on p-substrate . neutron irradiation test shows that the proposed method achieves sel prevention without seu increase . 
traffic analysis for on-chip networks design of multimedia applications the objective of this paper is to introduce self_similarity as a fundamental property exhibited by the bursty traffic between on-chip modules in typical mpeg_2 video applications . statistical_tests performed on relevant traces extracted from common video clips establish unequivocally the existence of self_similarity in video traffic . using a generic communication architecture , we also discuss the implications of our findings on on-chip buffer space allocation and present quantitative evaluations for typical video streams . we believe that our findings open up new directions of research with deep_implications on some fundamental issues in on-chip network design for multimedia applications . 
signal classification for acoustic neutrino detection this article focuses on signal classification for deep_sea acoustic neutrino detection . in the deep sea , the background of transient_signals is very diverse . approaches like matched filtering are not sufficient to distinguish between neutrino-like signals and other transient_signals with similar signature , which are forming the acoustic background for neutrino detection in the deep-sea environment . a classification system based on machine_learning_algorithms is analysed with the goal to find a robust and effective way to perform this task . for a well-trained model , a testing error on the level of one percent is achieved for strong classifiers like random_forest and boosting trees using the extracted features of the signal as input and utilising dense clusters of sensors instead of single sensors . 
online test macro scheduling and assignment in mpsoc design due to unreliability of the cores in embedded_systems in deep sub-micron technologies , a method for testing cores in the field is needed . in this paper an online method for testing cores of embedded designs is presented . the proposed task scheduling method runs the test routine asap periodically considering the real time constraints . a software test routine based on a proposed_method will be generated and a task scheduling process including the test task ( for each core ) and other existing applications of the embedded system will be presented . a software based checksum is issued for online test result analysis that shortens the memory usage of the test_process . experimental results show that this method improves the test application time ( tat ) and fault_coverage ( in proportion to tat ) as compared with the existing_methods . 
mixed_signal hardware_description languages in the era of system-on-silicon : challenges and opportunities ( abstract of embedded tutorial ) spice-based simulation is recognized as a vital tool in shortening time-to-market , reducing product cost , and improving system reliability . it continues to have a profound impact on today's electronic industry . behavioral modeling_and_simulation with analog and mixed_signal hardware_description languages ( ahdls ) are becoming critical to address the challenge of designing today's increasingly complicated electronic systems . these systems may have millions of transistors , are radically diverse ( e . g . , micro-electro-mechanical ) and governed by tightly coupled physical ( e . g . thermal-electronic and deep_submicron ) effects . the trend towards behavioral modeling_and_simulation has been exemplified by the success of proprietary modeling languages and simulators and by the emergence of several language standards ( vhdl-ams and verilog-ams ) and their application from full system verification to design_reuse , virtual prototyping , testing , and even synthesis . in this tutorial , we will overview some of recent_developments in ahdls and their applications , with particular emphasis on challenges and opportunities . 
challenges of biological realism and validation in simulation_based medical education overview simulation , both physical and computer-based , has a rich history in support of medical education . essentially all these efforts have been aimed at instilling concrete measurable skills , akin to vocational training . they present learners with choices , facilitating a degree of learning by doing . the sets of learner choices are usually limited , with choices clearly classified into "right" and "wrong" . but much of medicine is not much like a multiple_choice test . the realm of choices is broad and not always easily converted to a short list . the "correct" answer is not always known by the experienced physician beforehand , sometimes not even after the die is cast and the future unfolds . computer simulation of human disease and its treatment can in principle be tremendously useful in the education of both basic and clinical scientists . this paper describes some challenges in the construction of simulation_based "liberal_arts" biomedical education . objectives the educator attempting to develop a learning_environment based on simulation of biology faces some special challenges . the challenges addressed in this paper are : face validity and deep validity; finding the right degree of realism; authoring biomedical models efficiently; managing randomness . to illustrate the issues , we trace the history of the oncology thinking cap throughout several versions and expansions of educational objectives , and describe the detection and remediation of shortcomings related to these issues . design dealing effectively with issues of validity and realism can be accomplished if the acquisition of information driving and justifying the model development choices is documented , preferably automatically , during the process . efficiency in authoring is greatly enhanced by judicious modularity to encourage re-use , and by the use of templated statements rather than raw code or exotic graphical components to represent the instructions driving the model . randomness can be used to familiarize learners with the true relative proportions of types of cases , or to enrich the encountered cases with rarer but more instructive cases . when a learner repeats an encounter with a scenario while changing a single option , proper management of randomness is essential to avoid artifacts of random_number_generators . otherwise an outcome change caused by a shift in random number streams may masquerade as an outcome change due to the changed option . conclusion effective use of computer simulation of human disease and its treatment for biomedical education faces daunting obstacles , but these problems can be solved . 
a process_technology for realizing integrated inertial_sensors using deep_reactive_ion_etching ( drie ) and aligned wafer_bonding a process_technology for realizing integrated inertial_sensors using deep_reactive_ion_etching ( drie ) and aligned wafer_bonding the demand for silicon micromachined inertial_sensors is expected to grow tremendously in the next few years . potential benefits such as improved performance , enhanced reliability and lower cost can be gained by integrating these sensors with on-chip electronics . using deep_reactive_ion_etching ( drie ) and aligned wafer_bonding , a process_technology for realizing integrated inertial_sensors is developed . drie allows the formation of high-aspect_ratio structures especially crucial for lateral inertial_sensors . compatibility with standard ic processes is achieved by the sealed-cavity approach as enabled by wafer_bonding . this process also realizes a new interconnection scheme which permits signal crossovers . during process development , drie gap-widening and footing effects are observed . these effects are characterized and ways to minimize them found . the process_technology is successfully demonstrated by the fabrication of functional accelerometers and gyroscopes . the characteristics of the accelerometers are measured by shaker tests and computer microvision . some deviation from the design values is observed , however , its cause is not completely understood . 
integration and evaluation of a video_surveillance system integration and evaluation of a video_surveillance system visual_surveillance systems are getting a lot of attention over the last few years , due to a growing need for surveillance_applications . in this thesis , we present a visual_surveillance system that integrates modules for motion_detection , tracking , and trajectory characterization to achieve robust monitoring of moving_objects in scenes under surveillance . the system operates on video sequences acquired by stationary color and infra_red surveillance cameras . motion_detection is implemented using an algorithm that combines thresh-olding of temporal variance and background modeling . the tracking algorithm combines motion and appearance information into an appearance model and uses a particle_filter framework for object_tracking . the trajectory analysis module builds a model for a given normal activity using a factorization approach , and uses this model for the detection of any abnormal motion pattern . the system was tested on a large ground-truthed data_set containing hundreds of color and flir image_sequences . results of performance_evaluation using these sequences are reported in this thesis . dedication this thesis is dedicated to my beloved parents , sisters , and brother . ii_acknowledgments i would like to thank many people who helped me to bring this work together . first , to my advisor , prof rama chellappa , whose guidance and support over the last two years have been of great help . to dr qinfen zheng , for his valuable support and discussions during the course of this work . i would like to also thank the rest of my committee , prof steve_marcus and prof min wu , for the valuable assistance they gave me either in classes or in my thesis work . i would like to thank my group mates , aswin sankaranarayanan and seong-wook joo for helping me to start this work with there software and for their valuable discussions . i would also like to express me deep gratitude to ahmed sadek and wael abd-almageed for their valuable comments and help in the final stages of this work . 
data_mining tools used in deep_brain_stimulation - analysis results parkinson's_disease is associated with motor_symptoms , including tremor . the dbs ( deep_brain_stimulation ) involves electrode implantation into sub-cortical structures for long_term stimulation at frequencies greater than 100hz . we performed linear and nonlinear analysis of the tremor signals to determine a set of parameters and rules for recognizing the behavior of the investigated patient and to characterize the typical responses for several forms of dbs . we found patterns for homogeneous group for data reduction . we used data_mining and knowledge_discovery techniques to reduce the number of data . to support such predictions , we develop a model of the tremor , to perform tests determining the dbs reducing the tremor or inducing tolerance and lesion if the stimulation is chronic . parkinson's_disease ( pd ) is a serious neurological_disorder with a large spectrum of symptoms ( rest tremor , bradykinesia , muscular rigidity and postural instability ) . the neurons do not produce dopamine anymore or produce very low level of this chemical mediator , necessary on movement coordination . parkinson's_disease seems to occur in about 100-250 cases on 100 000 individuals . in europe were reported about 1 . 2 million parkinson patients [1] . the missing of a good clinical test , combined with the patient's reticence to attend a physician , make the diagnostic to be established very often too late . the accuracy of the diagnosis of pd varies from 73 to 92 percent depending on the clinical criteria used . parkinsonian tremor is a rhythmic , involuntary muscular contraction characterized by oscillation of a part of the body . initial symptoms include resting tremor beginning distally in one arm at a 4 6 hz frequency . deep_brain_stimulation ( dbs ) is an electric therapy approved by fda ( food_and_drug_administration ) for the treatment of parkinson's_disease ( pd ) in 2002 and used now to treat the motor_symptoms like essential_tremor . it consists of a regular high_frequency stimulation of specific subcortical sites involved in the movement-related neural patterns [1] . the exact neurobiological mechanism by which dbs exerts modulator effects on brain tissue are not yet full understood . it is unknown which part of the neuronal
a micromachined silicon multielectrode for multiunit recording . a 16-channel multielectrode was used to record propagating action_potentials from multiple units in the ventral nerve cord of the cricket gryllus bimaculatus . the multielectrode was fabricated using photolithographic and bulk silicon etching techniques . the fabrication differs from standard methods in its use of deep_reactive_ion_etching ( drie ) to form the bulk electrode structure . this technique enables the fabrication of relatively thick ( >100 microm ) , rigid structures whose top surface can have any form of thin film electronics . the multielectrode tested in this paper consists of 16 narrow silicon bridges , 150 microm wide and 350 microm tall , spaced evenly over a centimeter , with passive rectangular gold recording sites on the top surface . the nerve cord was placed perpendicularly across the bridges . in this geometry , the nerve spans a 350 microm deep , 450 microm wide trench between each recording site , permitting adequate isolation of recording sites from each other and a platinum ground_plane . spike templates for eight neurons were formed using principle component analysis and clustering of the concatenated multichannel waveforms . clean templates were generated from a 40 s recording of stimulus evoked activity . conduction velocities ranged from 2 . 59+/-0 . 05 to 4 . 99+/-0 . 12 m/s . two limitations of extracellular electrode arrays are the resolution of overlapping spikes and relation of discriminated units to known anatomy . the high_density , precise positioning , and controlled impedance of recording sites achievable in microfabricated devices such as this one will aid in overcoming these limitations . the rigid devices fabricated using this process offer stable positioning of recording sites over relatively large distances ( several millimeters ) and are suitable for clamping or squeezing of nerve cords . 
learning long_range vision for autonomous off-road driving most vision-based_approaches to mobile_robotics suffer from the limitations imposed by stereo obstacle_detection , which is short-range and prone to failure . we present a self-supervised_learning process for long_range vision that is able to accurately classify complex terrain at distances up to the horizon , thus allowing superior strategic planning . the success of the learning_process is due to the self-supervised training_data that is generated on every frame : robust , visually consistent labels from a stereo module , normalized wide-context input windows , and a discrimina-tive and concise feature representation . a deep_hierarchical network is trained to extract informative and meaningful features from an input image , and the features are used to train a realtime classifier to predict traversability . the trained classifier sees obstacles and paths from 5 to over 100 meters , far beyond the maximum stereo range of 12 meters , and adapts very quickly to new environments . the process was developed and tested on the lagr mobile_robot . results from a ground_truth dataset are given as well as field test_results . 
thermo-mechanical forming of al mg si alloys : modeling and experiments in an ongoing quest to realize lighter vehicles with improved fuel_efficiency , deformation characteristics of the material aa 6016 is investigated . in the first part of this study , material behavior of al mg si sheet alloy is investigated under different process ( temperature and strain_rate ) and loading ( uniaxial and biaxial ) conditions experimentally . later , warm cylindrical cup deep_drawing experiments were performed to study the effect of various parameters on warm forming processes , such as the effect of punch velocity , holding time , temper and temperature on force-displacement response . the plastic anisotropy of the material which can be directly reflected by the earing behavior of the drawn cups has also been studied . finite_element simulations can be a powerful tool for the design of warm forming processes and tooling . their accuracy will depend on the availability of material models that are capable of describing the influence of temperature and strain_rate on the flow stresses . the physically based nes model is used to describe the influence of temperature and strain_rate and the vegter yield criterion is used to describe the plastic anisotropy of the sheet . experimental drawing test_data are used to validate the modeling approaches . 
face_recognition based on deep_neural_network in modern life , we see more techniques of biometric features recognition have been used to our surrounding life , especially the applications in telephones and laptops . these biometric_recognition techniques contain face_recognition , fingerprint_recognition and iris_recognition . our work focuses on the face_recognition problem and uses a deep_learning method , convolutional_neural_network , to solve it . and we use the sobel operator to improve our result accuracy . lfw dataset is used for training_and_testing which gets a considerable result . and we also test our system on other face dataset , which also has a high_accuracy on the recognition . 
bist for deep_submicron asic memories with high_performance application today's asic designs consist of more memory in terms of both area and number of instances . the shrinking of geometries has an even greater effect upon memories due to their tight layouts . these two trends are putting much greater demands upon memory_bist requirements . at-speed testing and custom test algorithms are becoming essential for insuring overall product quality . at-speed testing on memories that now operate in the 10 to 800 mhz range can be a challenge . another demand upon memory_bist is determining the location of defects so that the cause can be diagnosed , or repaired with redundant cells . a tool and methodology that meets these difficult requirements is discussed . 
a comparative evaluation of deep_and_shallow approaches to the automatic detection of common grammatical errors this paper compares a deep and a shallow processing approach to the problem of classifying a sentence as grammatically well-formed or ill-formed . the deep_processing approach uses the xle lfg parser and en-glish grammar : two versions are presented , one which uses the xle directly to perform the classification , and another one which uses a decision_tree trained on features consisting of the xle's output statistics . the shallow processing approach predicts gram-maticality based on n_gram frequency statistics : we present two versions , one which uses frequency thresholds and one which uses a decision_tree trained on the frequencies of the rarest n-grams in the input sentence . we find that the use of a decision_tree improves on the basic approach only for the deep_parser-based_approach . we also show that combining both the shallow and deep decision_tree features is effective . our evaluation is carried out using a large test_set of grammatical and ungrammatical sentences . the ungrammatical test_set is generated automatically by inserting grammatical errors into well-formed bnc sentences . 
pattern_selection for testing of deep_sub_micron timing_defects due to process_variations in deep sub-micron ( dsm ) technologies , the effects of timing_defects are difficult to capture . this paper presents a novel coverage metric for estimating the test_quality with respect to timing_defects under process_variations . based on the proposed metric and a dynamic timing analyzer , we develop a pattern_selection algorithm for selecting the minimal number of patterns that can achieve the maximal test_quality . to shorten the runtime in dynamic timing_analysis , we propose an algorithm to speed up the monte_carlo-based simulation . our experimental results show that , selecting a small percentage of patterns from a multiple-detection transition_fault pattern set is suf . cient to maintain the test_quality given by the entire pattern set . we present run_time and accuracy comparisons to demonstrate the efficiency and effectiveness of our pattern_selection framework . 
a comparison of classical scheduling approaches in power_constrained block_test scheduling classical scheduling approaches are applied here to overcome the problem of unequal-length block-test_scheduling under power_dissipation constraints . list scheduling-like approaches are proposed j r s t as greedy algorithms to tackle the fore mentioned problem . then , distribution-graph_based approaches are described in order to achieve balanced test concurrency and test power_dissipation . an extended tree growing technique is also used in combination with these classical approaches in order to improve the test concurrency having assignedpower dissipation limits . a comparison between the results of the test scheduling experiments highlights the advantages_and_disadvantages of applying different classical scheduling algorithms to the power_constrained test_scheduling problem . partitioned testing with run to completion defined in [2] . an efficient scheme for overlaying the block-tests , called extended tree growing technique is employed to model the fore mentioned problem . thus , traditional high_level synthesis approaches ( e . g . left-edge algorithm , list scheduling and distribution-graph_based scheduling ) can be employed together with this model to search for power_constrained block_test schedule profiles in a polynomial time . the algorithm fully exploits test parallelism under power_dissipation constraints . this is achieved by overlaying the block_test intervals of compatible subcircuits to test as many of them as possible concurrently so that the maximum accumulated power_dissipation does not go over the given limit . a constant additive model is employed for the power_dissipation estimation throughout the algorithms . 1 introduction power_dissipation has become a critical factor in the normal_operation of nowadays' deep_submicron digital systems or under testing conditions . vlsi_circuits running in test_mode may consume more power than when running in normal_mode . it is reported in [i] that one of the major considerations in test_scheduling is the fact that heat dissipated during test_application is typically significantly_higher than the heat dissipated during the circuits' normal_operation ( sometimes 100-200% higher ) . test_scheduling is strongly related to test concurrency . test concurrency is a design property which strongly impacts testability and power dis-sipation . to satisfy high fault_coverage goals with reduced test application time under certain power_dissipation constraints , the testing of all components on the system should be performed in parallel to the greatest extent possible . power_constrained test_scheduling will soon become a very important_issue for the soc designs as well . therefore , this paper focuses on the high-level power_constrained block_test scheduling_problem which lacks of practical solutions . the test scheduling discipline assumed here is the paper 33 . 3 
numerical simulation of two-phase flow in deformable porous media : application to carbon_dioxide storage in the subsurface in this paper , conceptual_modeling as well as numerical simulation of two-phase flow in deep , deformable geological formations induced by co 2 injection are presented . the conceptual approach is based on balance equations for mass , momentum and energy completed by appropriate constitutive relations for the fluid phases as well as the solid matrix . within the context of the primary effects under consideration , the fluid motion will be expressed by the extended darcy's_law for two phase flow . additionally , constraint conditions for the partial saturations and the pressure fractions of carbon_dioxide and brine are defined . to characterize the stress state in the solid matrix , the effective stress principle is applied . furthermore , the interaction of fluid and solid phases is illustrated by constitutive models for capillary pressure , porosity and permeability as functions of saturation . based on this conceptual_model , a coupled system of nonlinear differential_equations for two-phase flow in a deformable porous matrix ( h 2 m model ) is formulated . as the displacement vector acts as primary variable for the solid matrix , multiphase flow is simulated using both pressure/pressure or pressure/saturation formulations . an object_oriented finite_element_method is used to solve the multi-field problem numerically . the capabilities of the model and the numerical tools to treat complex processes during co 2 sequestration are demonstrated on three benchmark examples : ( 1 ) a 1-d case to investigate the influence of variable fluid properties , ( 2 ) 2-d vertical axi-symmetric cross_section to study the interaction between hydraulic and deformation processes , and ( 3 ) 3-d to test the stability and computational costs of the h 2 m model for real applications . 
a comparative_study of probability collectives based multi_agent_systems and genetic_algorithms we compare genetic_algorithms ( ga's ) with probability collectives ( pc ) , a new framework for distributed optimization and control . in contrast to ga's , pc-based_methods do not update populations of solutions . instead they update an explicitly parameterized probability_distribution <i>p</i> over the space of solutions . that updating of <i>p</i> arises as the optimization of a functional of <i>p</i> . the functional is chosen so that any <i>p</i> that optimizes it should be <i>p</i> peaked about good solutions . the pc approach has deep connections with both game_theory and statistical_physics . we review the pc approach using its motivation as the information theoretic formulation of bounded_rationality for multi_agent_systems ( mas ) . it is then compared with ga's on a diverse set of problems . to handle high_dimensional surfaces , in the pc method investigated here <i>p</i> is restricted to a product distribution . each distribution in that product is controlled by a separate agent . the test functions were selected for their difficulty using either traditional gradient_descent or genetic_algorithms . on those functions the pc-based_approach significantly outperforms traditional ga's in both rate of descent , trapping in false minima , and long_term optimization . 
on the relation of speech to language opinion opinion opinion opinion opinion perception of the sounds that convey phonetic structure one finds two very different views of its relation to language . the more conventional holds that speech is merely a vehicle , bearing no organic relationship to the linguistic baggage it carries . on that view , speech is produced and perceived by processes that are not specialized for language but rather serve horizontally the broadest possible variety of behaviors , linguistic and non-linguistic alike . the outcomes of those primary processes are then presumably sent on to language proper , a separate domain where they find the mental machinery capable of the heavy lifting required by phonology , morphology and syntax . preferring a name that reflects the nature of a theory rather than its currency , we will call the conventional view 'horizon-tal' ( as in ref . 1 ) . the other , less conventional view is that the biological roots of language run deep , penetrating even to the level of speech and to the primary motor and perceptual processes that are engaged there . seen from that perspective , speech is a constituent of a vertically organized system , specialized from top to bottom for linguistic communication . such a view may be appropriately called 'vertical' . to evaluate these strongly contrasting views , we propose here to determine which of the two provides the more coherent and plausible account when challenged by several simple and seemingly obvious biological considerations . as we will try to show , those considerations provide decisive tests of any theory of speech , yet they do not normally figure in the calculations of the theorists , nor have they been permitted to ruffle the implicit assumptions that guide ( or misguide ) almost all applied language research , including that which is aimed at determining how to convert fluency in speech to fluency in the use of its alphabetic transcription . indeed , bringing those considerations to notice is one purpose of this paper . but first , we will give a brief description of the theories . the horizontal viewpoint the first assumption of the horizontal view is that the elements of speech are sounds . that is not merely to say the obvious , which is that speech exploits an acoustic medium , but rather to identify sounds as the primitives that are exchanged when linguistic communication occurs . the invariant acoustic patterns that might justify such an assumption have , indeed , been claimed for a variety of phonetic segments , including , for example , stops 2 , nasals 3 , and the voicing contrast of fricatives 
deepbot : a focused crawler for accessing hidden_web content the crawler engines of today cannot reach most of the information contained in the web . a great amount of valuable information is "hidden" behind the query forms of online databases , and/or is dynamically generated by technologies such as javascript . this portion of the web is usually known as the deep web or the hidden_web . we have built deepbot , a prototype of hidden_web focused crawler able to access such content . deepbot receives a set of domain definitions as an input , each one describing a specific data-collecting task and automatically identifies and learns to execute queries on the forms relevant to them . in this paper we describe the techniques employed for building deepbot and report the experimental results obtained when testing it with several real_world data_collection tasks . 
understanding the mind from an evolutionary perspective : an overview of evolutionary_psychology . unlabelled the theory of evolution by natural_selection provides the only scientific explanation for the existence of complex adaptations . the design features of the brain , like any organ , are the result of selection pressures operating over deep time . evolutionary_psychology posits that the human_brain comprises a multitude of evolved psychological mechanisms , adaptations to specific and recurrent problems of survival and reproduction faced over human evolutionary history . although some mistakenly view evolutionary_psychology as promoting genetic_determinism , evolutionary_psychologists appreciate and emphasize the interactions between genes and environments . this approach to psychology has led to a richer understanding of a variety of psychological phenomena , and has provided a powerful foundation for generating novel hypotheses . critics argue that evolutionary_psychologists resort to storytelling , but as with any branch of science , empirical testing is a vital component of the field , with hypotheses standing or falling with the weight of the evidence . evolutionary_psychology is uniquely suited to provide a unifying theoretical framework for the disparate subdisciplines of psychology . an evolutionary perspective has provided insights into several subdisciplines of psychology , while simultaneously demonstrating the arbitrary nature of dividing psychological science into such subdisciplines . evolutionary_psychologists have amassed a substantial empirical and theoretical literature , but as a relatively new approach to psychology , many questions remain , with several promising directions for future_research . for further resources related to this article , please visit the wires website . conflict of interest the authors have declared no conflicts of interest for this article . 
statistical diagnosis for intermittent scan_chain hold-time fault intermittent scan_chain hold-time fault is discussed in this paper and a method to diagnose the faulty site in a scan_chain is proposed as well . unlike the previous scan_chain diagnosis methods that targeted permanent_faults only , the proposed method targets both permanent_faults and intermittent faults . three ideas are presented in this paper . first an enhanced upper bound on the location of candidate faulty scan_cells is obtained . second a new method to determine a lower_bound is proposed . finally a statistical diagnosis algorithm is proposed to calculate the probabilities of the bounded set of candidate faulty scan_cells . the proposed algorithm is shown to be efficient and effective for large industrial designs with multiple faulty scan_chains . 1 introduction the development of improved semiconductor process_technologies and eda tools continuously allow the realization of a more complex system on a single die with higher clock frequency and larger system density . however , several new problems , as described below , appear in designing , manufacturing and testing a system-on-a-chip ( soc ) with multi-million transistors using deep_submicron ( dsm ) technologies . ( 1 ) signal_integrity ( si ) and design integrity ( di ) issues become major challenges with the evolution to dsm . si issues include crosstalk , ir drop , power and ground bounce , etc . di issues include electron migration , hot electrons , wire self-heating , etc [1] . to increase density , interconnects on the chip come closer and are made narrower and thicker [2] . hence the crosstalk is exacerbated by the increased inter-line capacitive and inductive_coupling . switching signals at a faster rate might cause ground bounce and lowering supply_voltages , which lead to decreased noise_margins . all the above-mentioned factors would make the signals on the chip more sensitive to unpredicted disturbances from either internal signal changes or external noises . these problems sometimes lead to incorrect functional behaviors , which are called intermittent faults . unlike the permanent_faults that are deterministically modeled by some known fault_models ( e . g . stuck-at or bridging ) , the intermittent faults are stochastically observed and difficult to be modeled . ( 2 ) at dsm levels of technologies , statistical_timing analysis is the only way to account for wiring delay and process_variations [3] . therefore , instead of thinking that a signal transition happens at a fixed time , we now have to think the transition could happen within a small range of time with an estimated probability_distribution density . for example , when we use traditional static_timing_analysis , we might know that signal a 
dhsnet : deep_hierarchical saliency network for salient object_detection traditional 1 salient object_detection models often use hand-crafted features to formulate contrast and various prior_knowledge , and then combine them artificially . in this work , we propose a novel end_to_end deep_hierarchical saliency network ( dhsnet ) based on convolutional_neural_networks for detecting salient objects . dhsnet first makes a coarse global prediction by automatically learning various global structured saliency cues , including global contrast , objectness , compactness , and their optimal combination . then a novel hierarchical recurrent convolutional_neural_network ( hrcnn ) is adopted to further hierarchically and progressively refine the details of saliency maps step_by_step via integrating local context information . the whole architecture works in a global to local and coarse to fine manner . dhsnet is directly trained using whole images and corresponding ground_truth saliency masks . when testing , saliency maps can be generated by directly and efficiently feedforwarding testing images through the network , without relying on any other techniques . evaluations on four benchmark datasets and comparisons with other 11 state-of-the-art algorithms demonstrate that dhsnet not only shows its significant superiority in terms of performance , but also achieves a real-time speed of 23 fps on modern gpus . 
a fully integrated microneedle-based transdermal drug_delivery system the left picture on the front cover shows an integrated microneedle-based drug_delivery system fabricated and used for experiments by the author . an array of microneedles is located on the other side of the device . the right picture on the cover shows a magnified view of these side-opened hollow microneedles , likewise fabricated by the author . the needles are designed to penetrate skin tissue using a low insertion force . the needles are fabricated by deep_reactive_ion_etching of silicon and the length of the needles is 400 m . abstract iii abstract patch-based transdermal drug_delivery offers a convenient way to administer drugs without the drawbacks of standard hypodermic injections relating to issues such as patient acceptability and injection safety . however , conventional transdermal drug_delivery is limited to therapeutics where the drug can diffuse across the skin barrier . by using miniaturized needles , a pathway into the human_body can be established which allow transport of macromolecular drugs such as insulins or vaccines . these microneedles only penetrate the outermost skin layers , superficial enough not to reach the nerve receptors of the lower skin . thus , microneedle insertions are perceived as painless . the thesis_presents research in the field of microneedle-based drug_delivery with the specific aim of investigating a microneedle-based transdermal_patch concept . to enable controllable drug infusion and still maintain an unobtrusive and easy-to-use , patch-like design , the system includes a small active dispenser mechanism . the dispenser is based on a novel thermal actuator consisting of highly expandable micro-spheres . when actuated , the microspheres expand into a liquid reservoir and , subsequently , dispense stored liquid through outlet holes . the microneedles are fabricated in monocrystalline silicon by deep_reactive_ion_etching . the needles are organized in arrays situated on a chip . to allow active delivery , the microneedles are hollow with the needle bore-opening located on the side of the needle . this way , the needle can have a sharp and well-defined needle tip . a sharp needle is a further requirement to achieve microneedle insertion into skin by hand . the thesis_presents fabrication and evaluation of both the microneedle structure and the transdermal_patch as such . issues such as penetration reliability , liquid delivery into the skin and microneedle packaging are discussed . the microneedle patch was also tested and studied in vivo for insulin delivery . results show that intradermal administration with microneedles give rise to similar insulin concentration as standard subcutaneous delivery with the same dose rate . iv 
breadcrumb navigation : there's more to hansel and gretel than meets the eye in a popular grimm fairytale , hansel and gretel are taken deep into the forest in the hope that they will not find their way out . however , clever hansel has left a trail of breadcrumbs to show their return path . while the story inspires the term " breadcrumb navigation , " it also provides a metaphor of web_usage that is much stronger than many designers realize 1 . an alarming number of sites ( both web and intranet ) are still designed and usability-tested with the home_page as the starting_point . yet there are over three billion searches conducted each month in the u . s . alone , averaging 32 per search_engine user . while some of these searches may link to a homepage , the vast majority lead directly to a page deep within a site . this is the internet equivalent of hansel and gretel's scenario and probably the most important reason for having breadcrumb navigation . figure 1 is an example from a site selling electronic and mechanical_engineering supplies . there are a number of reasons why breadcrumb navigation is better than the alternatives : providing only a link to the home_page is inadequate for all but the simplest sites . users are already in an area they find interesting . with a home_page link you are making them leave the forest so they can find their way back to where they started . site navigation is often difficult to follow backwards a little like trying to follow trail signs in the wrong direction . this is : / 79 i n t e r a c t i o n s / s e p t e m b e r + o c t o b e r 2 0 0 4 hci and the web hci and the web i n t e r a c t i o n s / s e p t e m b e r + o c t o b e r 2 0 0 4 hci and the web especially true when the location of each page being viewed is not reflected in the navigation itself ( with a highlight or other marker of some type ) . site navigation usually does not ( and probably should not ) reflect the whole depth of a hierarchy on each page of a site . in the example above , the product page is at the sixth level in the hierarchy ( or more if there 
an efficient network_on_chip ( noc ) for a parallel , low_power , low-area homogeneous many-core dsp platform an efficient network_on_chip targeted to a parallel , low_power , low-area homogenous many-core dsp platform this thesis_presents an noc architecture that is optimized for a course-grained , deterministic many core dsp platform supporting up to 256 cores . the proposed network supports both local and long-distance communication in the event that large applications or multiple smaller applications are mapped onto the platform by means of a hierarchical cluster topology . the noc is designed to optimize the area-and power-to-performance ratio through implementing the following key characteristics : low hop-count long distance communication , optimized flit buffer size , efficient virtual_channel implementation , and a highly restricted virtual_channel flow control . the noc architecture is implemented in 65 nm cmos_technology with a nominal supply_voltage of 1v . place and route results show that the proposed architecture saves up to 33% in area and up to 87 . 6% in energy-per-flit in comparison to some currently-implemented nocs . through several traffic pattern_tests on a network of 16 cores , the noc attains a throughput of up to 21 . 7gbps . a 256-point fft mapped onto 16 cores executes in 4 . 3 s and dissipates 0 . 649w . this is an improvement of 46% and 81% in latency and power_dissipation over a 256-point xilinx fft ip core implemented on a virtex 6 fpga . dedication it goes without saying that this thesis is dedicated to the wide and talented academic community in hopes to propagate its research endeavors and its cause . however , without two people in particular , i'm not sure i would have made it this far . they are my two grandfathers , thomas " pappy " chandler and charles " pop-pop " vogt . they always took a deep interest in my endeavors , encouraging me to take them further . they pushed me . they provided moral and motivational support . they were there for me . they were always proud of me ( i'm sure i'll figure that out when i have children or grand-children of my own ) . without dragging this on too much longer , i dedicate this thesis to thomas chandler and charles vogt . i miss you both . the journey doesn't stop here . thank you . iii acknowledgments i owe thanks and gratitude to all who have made it possible for my completion of this thesis . first , i want to thank my advisor , dr . tinoosh mohsenin , for her direction throughout the last year and a half . her experience in the field and vast library of publications have most definitely helped me complete this work . thank you for the opportunity to work on this challenging and , more 
instance-based classification by emerging patterns emerging patterns ( eps ) , namely itemsets whose supports change significantly from one class to another , capture discriminating features that sharply contrast instances between the classes . recently , ep-based classifiers have been proposed , which first mine as many eps as possible ( called eager-learning ) from the training data and then aggregate the discriminating power of the mined eps for classifying new instances . we propose here a new , instance-based classifier using eps , called deeps , to achieve much better accuracy and efficiency than the previously_proposed ep-based classifiers . high_accuracy is achieved because the instance-based_approach enables deeps to pinpoint all eps relevant to a test instance , some of which are missed by the eager-learning_approaches . high efficiency is obtained using a series of data reduction and concise data-representation techniques . experiments show that deeps' decision time is linearly scalable over the number of training instances and nearly linearly over the number of attributes . experiments on 40 datasets also show that deeps is superior to other classifiers on accuracy . 
development of hybrid integrated endoscope-holder system for endoscopic microneurosurgery . objective endoscopic techniques in the field of neurosurgery are under development . to perform sophisticated bimanual procedures in the delicate surgical fields of neurosurgery , rigid endoscope fixation devices with accurate locking and a safe releasing system are required . here , we report the development of a new hybrid integrated endoscope-holder system . instrumentation the basic concepts of the holding device were as follows : 1 ) it should combine both video system and holding device; 2 ) it should have easy maneuverability and accurate fixation and be equipped with a safe releasing mechanism; and 3 ) it should be able to be used universally either in a primary or an assisting endoscopic procedure . a negatively actuated air-locking system and a bayonet-shaped endoscope were newly_developed . clinical_trials of 25 patients were performed , and each prototype was tested and modified until the functional_requirements were fulfilled . the final version was tested for accuracy and security in fixation and releasing mechanism . results eight problems were encountered in the clinical_trials and improved . accuracy in fixation of the final version was superior to the most advanced endoscope-holder on the market . no dangerous events were observed during repetitive insertion , fixation , and release in the simulated deep surgical field in the cadaveric skull . no apparent complications were noted in the clinical application . conclusion we have developed a highly reliable , accurately fixable , and easily maneuverable hybrid endoscope-holder system . to achieve a safer , more accurate , and less invasive surgery in the current socioeconomic demands , commercial manufacturers and surgeons in multiple centers need to combine their efforts to create useful techniques . 
editorial : alan_turing and artificial_intelligence his [turing's] point was that we should not be species-chauvinistic , or anthropocentric , about the insides of an intelligent being , for there might be inhuman ways of being intelligent . was one of the most eminent scientists of the 20th century ( figure 1 ) . his research was a central catalyst of the computer revolution . the concept of a turing_machine , which he developed in the 1930s , is still one of the most widely used models of computation in theoretical computer_science , but this monumental contribution was only the first of many . his biographer andrew_hodges author of hodges ( 1992 , 1997 ) , and main-tainer of the " alan_turing home_page " divides turing's publications into five areas : mathematical_logic , mechanical intelligence , pure_mathematics , morphogen-esis , and cryptanalysis . moreover , in sir roger penrose's words , turing was also " a deep and influential philosopher in addition to his having made contributions to mathematics , technology and code-breaking that profoundly contribute to our present-day well being " ( hodges , 1998 ) . in a landmark article published in october 1950 in the philosophy journal mind ( figure 2 ) turing made a famous assertion . he predicted that by the year 2000 it would be feasible to write a program that would , after five minutes of questioning , have at least a 30% chance of fooling an average conversational partner into believing it was a human being ( turing , 1950 ) . as charniak and mcdermott ( 1985 : 10 ) remark " actually , the [mind] paper makes it sound as if turing had in mind the computer pretending to be a woman in the man/woman game , but the point is not completely clear , and most have assumed that he intended the test to be a person/computer one , and not woman/computer . " see saygin et al . ( 1999 ) for an attempt at clarification . 
quantum_gravity with a positive cosmological_constant a quantum_theory of gravity is described in the case of a positive cosmological_constant in 3 + 1 dimensions . both old and new results are described , which support the case that loop_quantum_gravity provides a satisfactory quantum_theory of gravity . these include the existence of a ground_state , discoverd by kodama , which both is an exact solution to the constraints of quantum_gravity and has a semiclassical limit which is desitter space-time . the long wavelength excitations of this state are studied and are shown to reproduce both gravitons and , when matter is included , quantum_field_theory on desitter spacetime . furthermore , one may derive directly from the wheeler-dewitt equation corrections to the energy-momentum relations for matter fields of the form e 2 = p 2 +m 2 + l p l e_3 + . . . where is a computable dimensionless constant . this may lead in the next few years to experimental_tests of the theory . to study the excitations of the kodama state exactly requires the use of the spin network representation , which is quantum deformed due to the cosmological_constant . the theory may be developed within a single horizon , and the boundary states described exactly in terms of a boundary chern-simons theory . the bekenstein bound is recovered and the n bound of banks is given a background independent explanation . the paper is written as an introduction to loop_quantum_gravity , requiring no prior_knowledge of the subject . the deep relationship between quantum_gravity and topological field_theory is stressed throughout . 
critical insights into nhs information_systems deployment abstract this chapter discusses a systems methodology called strategic assumption surfacing and testing ( sast ) that was used to understand the design and deployment of information_systems in the healthcare context . it is based on the experiences of conducting sast with a group of healthcare professionals , working in the national_health_service ( nhs ) in england . this application of sast in the nhs setting highlighted deep politico-cultural concerns in the organizational setting , and it helped towards the conception of a normative inclusive approach for health infor-matics design and deployment . this approach introduces the understanding that the development of information_systems in healthcare is a complex agenda , the success of which demands the active involvement of all stakeholders through all the key stages of the process . critical perspectives on sast have also been considered and the assumptions fostered towards arriving at the conclusions , have been highlighted . 
an efficient speaker_diarization using privacy preserving audio_features based of speech/non speech detection privacy_sensitive_audio_features for speaker diarization in multiparty conversations : i . e . , a set of audio_features having low linguistic_information for speaker diarization in a single and multiple distant microphone scenarios is a challenging research field in now-a-days . existing system used a supervised framework using deep neural architecture for deriving privacy_sensitive_audio_features . in proposed system patterns of speech/nonspeech detection ( snd ) is utilized for privacy_sensitive_audio feature to capture real_world audio . snd and diarization can then be used to analyze social_interactions . in this research privacy preserving audio_features has been investigated instead for recording and storage that can respect privacy by minimizing the amount of linguistic_information , whereas achieving modern performance in conversational speech_processing tasks . certainly , the main contribution of the proposed system is the achievement of state-of-the-art performances in speech/nonspeech detection and speaker_diarization tasks using such features , which we refer to , as privacy_sensitive . in addition a comprehensive analysis of these features has been provided for the two tasks in a variety of conditions , such as indoor ( predominantly ) and outdoor audio . to objectively evaluate the notion of privacy , the proposed system use automatic_speech_recognition tests , with higher_accuracy in either being interpreted as yielding lower privacy . i . introduction speech is acoustic signal which contains information of idea that is formed in speaker's mind . speech is bimodal in nature [1] [2] . speech_processing can be performed at different three levels . signal level processing considers the anatomy of human auditory system and process signal in form of small chunks called frames [3] . in phoneme level processing , speech phonemes are acquired and processed . the major work of speech_recognition has been explored to analyse the social_interactions using multimodal sensors with an emphasis on audio . analysis of conversations can then carried by modeling the speech/speaker activities produced by a speaker_diarization system . the objective of speaker_diarization is to segment an audio recording into speaker-homogeneous regions whereas the output of a diarization system may appear to be restrictive , there are a growing number of applications
putting trust in software code chris wysopal is director of development at symantec_corporation , where he leads research on how to build and test software for security vulnerabilities . the co-father of unix , wrote a paper about the quandary of not being able to trust code that you didn't create yourself . the paper , " reflections on trusting trust , " 1 details a novel approach to attacking a system . thompson inserts a back door into the unix login program when it is compiled and shows how the compiler can do this in a way that can't be detected by auditing the compiler source_code . he writes : " you can't trust code that you did not totally create yourself . no amount of source-level verification or scrutiny will protect you from using untrusted code . in demonstrating the possibility of this kind of attack , i picked on the c compiler . i could have picked on any program-handling program such as an assembler , a loader , or even hardware microcode . " twenty years after thompson' s seminal paper was published , developments in the field of automated binary analysis of executable code are tackling the problem of trusting code you didn't write . binary analysis can take on a range of techniques , from building call trees and looking for external function calls to full decompilation and modeling of a program' s control_flow and data_flow . the latter , which i call deep binary analysis , works by reading the executable machine_code and building a language-neutral representation of the program' s behavior . this model can be traversed by automated scans to find security vulnerabilities caused by coding errors and to find many simple back doors . a source_code emitter can then take the model and generate a human-readable source_code representation of the pro-gram' s behavior . this enables manual code auditing for design-level security_issues and subtle back doors that will typically escape automated scans . the steps of the decompilation process are as follows : 1 . front_end decodes binary to intermediate_language . 2 . data_flow transformer reconstructs variable lifetimes and type information . 3 . control_flow transformer reconstructs loops , condi-tionals , and exceptions . 4 . back end performs language-specific transformation and exports high_level code . to be useful the model must have a query engine that can answer questions for security scanning scripts : 
easily testable and fault_tolerant fft butterfly networks with the advent of deep submicron very_large_scale_integration technology , the integration of a large fast_fourier_transform ( fft ) network into a single chip is becoming possible . however , a practical fft chip is normally very big , so effective testing and fault-tolerance techniques usually are required . in this paper , we first propose a c-testable fft network design . only 20 test_patterns are required to cover all combinational single-cell faults and interconnect stuck-at and break faults for the fft network , regardless of its size . a spare-row based fault_tolerant fft network design is subsequently proposed . compared with previous_works , our approach shows higher reliability and lower hardware_overhead , and only three bit_level cell types are needed for repairing a faulty row in the multiply subtract add module . also , special cell design is not required to implement the reconfiguration scheme . the hardware_overhead for the testable design is low about 4% for 16-bit numbers , regardless of the fft network size . 
the csiro enterprise_search test_collection this article describes a new trec enterprise track search test_collection -- cerc . the collection is designed to represent some real_world search activity within the enterprise , using as a specific example the commonwealth_scientific_and_industrial_research_organisation ( csiro ) . it has a deep crawl of csiro's public-facing information , that is very similar to the crawl of a real-world search service provided by csiro . the search tasks are based on the activities of csiro science communicators , who are csiro employees that deal with public-facing information . topics and judgments are tied to the science communicators in various ways , for example by involving them in the topic development process . the overall approach is to enhance the validity of the test_collection as a model of enterprise_search , by tying it to real_world examples . 
design , development_and_testing of underwater_vehicles : itb experience the last decade has witnessed increasing worldwide interest in the research of underwater robotics with particular focus on the area of autonomous underwater_vehicles ( auvs ) . the underwater robotics technology has enabled human to access the depth of the ocean to conduct environmental surveys , resources mapping as well as scientific and military missions . this capability is especially valuable for countries with major water or oceanic resources . as an archipelagic nation with more than 13 , 000 islands , indonesia has one of the most abundant living and non-organic oceanic resources . the needs for the mapping , exploration , and environmental preservation of the vast marine resources are therefore imperative . the challenge of the deep_water exploration has been the complex issues associated with hazardous and unstructured undersea and sea-bed environments . the paper_reports the design , development_and_testing efforts of underwater_vehicle that have been conducted at institut teknologi bandung . key technology areas have been identified and step_by_step development is presented in conjunction with the need to meet the challenge of underwater_vehicle operation . a number of future_research directions are also highlighted . 
preference constructors for deeply personalized database queries preference constructors for deeply personalized database queries deep personalization of database queries requires a semantically rich , easy to handle and flexible preference model . building on preferences as strict partial orders we provide a variety of intuitive and customizable base preference constructors for numerical and categorical data . for complex constructors we introduce the notion of 'substitutable values' ( sv-semantics ) . preferences with sv-semantics solve major open problems with pareto and prioritized preferences . known laws from preference relational_algebra remain valid under sv-semantics . these powerful modeling capabilities even contribute to improve efficient preference query evaluation . moreover , for the first time we point out a semantic guided way to cope with the infamous flooding effect of query engines . performing a series of test queries on sample data from an e_procurement application , we provide evidence that the flooding problem comes under control for deeply personalized database queries . 
on the intersubject generalization_ability in extracting kinematic information from afferent nervous signals in the recent past , many efforts have been carried out in order to evaluate the feasibility of implementing closed_loop controlled neuroprostheses based on the processing of sensory electroneurographic ( eng ) signals . the success of these techniques mostly relies on the development of processing algorithms capable of extracting the necessary kinematic information from these signals . soft_computing algorithms can be very useful when dealing with the complexity of the neuromuscular system because of their generalization_ability and model-free structure . in this paper , these techniques were used to extract angular position information from the eng signals recorded from muscle afferents in animal_model using cuff electrodes . specifically , a genetic algorithm-based dynamic nonsingleton fuzzy_logic system ( named ga-dnsfls ) was developed and tested on different types of angular trajectories ( characterized by small or large angular excursions ) . in particular , two different takagi_sugeno-kang ( tsk ) -like structures were used in the consequent part of the neuro-fuzzy model in order to verify which one could improve the generalization abilities ( intrasubject and intersubject ) . the results_showed that the ga-dnsfls was able to reconstruct the trajectories giving interesting results in terms of correlation between the actual and the predicted trajectories for small excursion movements during intrasubject and intersubject tests . particularly , one of the tsk models showed better results in terms of intersubject generalization . the simulations conducted with the large excursion movements led in some cases to interesting results but further experiments are necessary in order to analyze this point more in deep . 
project quality of off-shore virtual_teams engaged in software requirements_analysis : an exploratory comparative_study project quality of off-shore virtual_teams engaged in software requirements_analysis abstract the offshore software_development companies in countries such as india use a global delivery model in which initial requirement analysis phase of software projects get executed at client locations to leverage frequent and deep interaction between user and developer teams . subsequent phases such as design , coding and testing are completed at offshore locations . emerging trends indicate an increasing interest in off-shoring even requirements_analysis phase using computer_mediated_communication . we conducted an exploratory research study involving students from management development institute ( mdi ) , india and marquette_university ( mu ) , u_._s_._a . to determine quality of such off-shored requirements_analysis projects . our findings_suggest that project quality of teams engaged in pure offshore mode is comparable to that of teams engaged in collocated mode . however , the effect of controls such as user project monitoring on the quality of off-shored projects needs to be studied further . 
post-placement temperature reduction techniques with technology scaled to deep_submicron_era , temperature and temperature_gradient have emerged as important design criteria . we propose two post-placement techniques to reduce peak temperature by intelligently allocating whitespace in the hotspots . both methods are fully compliant with commercial technologies , and can be easily integrated with state-of-the-art thermal-aware design_flow . experiments in a set of tests on circuits implemented in stm 65nm technologies show that our methods achieve better peak temperature reduction than directly increasing circuit's area . 
detecting spammers and content promoters in online video social_networks a number of online video social_networks , out of which youtube is the most popular , provides features that allow users to post a video as a response to a discussion topic . these features open opportunities for users to introduce polluted content , or simply pollution , into the system . for instance , <i>spammers</i> may post an unrelated video as response to a popular one aiming at increasing the likelihood of the <i>response</i> being viewed by a larger number of users . moreover , opportunistic users--<i>promoters</i>--may try to gain visibility to a specific video by posting a large number of ( potentially unrelated ) responses to boost the rank of the <i>responded video</i> , making it appear in the top lists maintained by the system . content pollution may jeopardize the trust of users on the system , thus compromising its success in promoting social_interactions . in spite of that , the available literature is very limited in providing a deep understanding of this problem . in this paper , we go a step further by addressing the issue of detecting video spammers_and_promoters . towards that end , we manually build a test_collection of real youtube users , classifying them as spammers , promoters , and legitimates . using our test_collection , we provide a characterization of social and content attributes that may help distinguish each user class . we also investigate the feasibility of using a state-of-the-art supervised classification algorithm to detect spammers_and_promoters , and assess its effectiveness in our test_collection . we found that our approach is able to correctly identify the majority of the promoters , misclassifying only a small percentage of legitimate users . in contrast , although we are able to detect a significant fraction of spammers , they showed to be much harder to distinguish from legitimate users . 
teaching motion_planning concepts to undergraduate_students motion_planning is a central problem in robotics . although it is an engaging topic for undergraduate_students , it is difficult to teach , and as a result , the material is often only covered at an abstract level . deep_learning could be achieved by having students implement and test different algorithms . however , there is usually no time within a single class to have students completely implement several motion_planning algorithms as they require the development of many lower-level data_structures . we present an ongoing project to develop a teaching module for robotic motion_planning centered around an integrated software environment . the module can be taught early in the undergraduate curriculum , after students have taken an introductory programming class . 
tele-operated echography and remote guidance for performing tele-echography on geographically isolated patients objective to evaluate the performance of three tele-echography systems for routine use in isolated medical centers . methods three systems were used for deep ( abdomen , pelvis , fetal ) and superficial ( muscle , thyroid , carotid artery ) examinations : ( a ) a robotic_arm ( ra ) holding an echographic probe; ( b ) an echograph with a motorized probe ( mp ) ; and ( c ) remote guidance ( rg ) where the patient site operator performed the examination assisted by an expert via videoconference . all systems were tested in the same medical center located 60 km away from the university hospital . results a total of 340 remote echography examinations were performed ( 41% ra and mp , 59% rg ) . mp and ra allowed full control of the probe orientation by the expert , and provided diagnoses in 97% of cases . the use of rg was sufficient for superficial vessel examinations and provided diagnoses in 98% of cases but was not suited for deep or superficial organs . assessment of superficial organs was best accomplished using the mp . discussion both teleoperated systems provided control of the probe orientation by the expert necessary for obtaining appropriate views of deep organs but the mp was much more ergonomic and easier to use than the ra . rg was appropriate for superficial vessels while the mp was better for superficial volumic organs . 
coaxial needle insertion assistant with enhanced force_feedback many medical procedures involving needle insertion into soft tissues , such as anesthesia , biopsy , brachytherapy , and placement of electrodes , are performed without image guidance . in such procedures , haptic detection of changing tissue properties at different depths during needle insertion is important for needle localization and detection of subsurface structures . however , changes in tissue mechanical_properties deep_inside the tissue are difficult for human operators to sense , because the relatively large friction force between the needle shaft and the surrounding tissue masks the smaller tip forces . a novel robotic coaxial needle insertion assistant , which enhances operator force perception , is presented . this one-degree_of_freedom cable-driven robot provides to the operator a scaled version of the force applied by the needle tip to the tissue , using a novel design and sensors that separate the needle tip force from the shaft friction force . the ability of human operators to use the robot to detect membranes embedded in artificial soft_tissue was tested under the conditions of 1 ) tip force and shaft force_feedback , and 2 ) tip force only feedback . the ratio of successful to unsuccessful membrane detections was significantly_higher ( up to 50% ) when only the needle tip force was provided to the user . 
the limits of speculative trace reuse on deeply pipelined processors trace reuse improves the performance of processors by skipping the execution of sequences of redundant instructions . however , many reusable traces do not have all of their inputs ready by the time the reuse test is done . for these cases , we developed a new technique called reuse through speculation on traces ( rst ) , where trace inputs may be predicted . this paper studies the limits of rst for modern processors with deep pipelines , as well as the effects of constraining resources on performance . we show that our approach reuses more traces than the non-speculative trace reuse technique , with speedups of 43% over a non-speculative trace reuse and 57% when memory_accesses are reused . 
acoustic imaging and visualization of plumes discharging from black smoker vents on the deep seafloor visualization and quantification methods are being developed to analyze our acoustic images of thermal plumes containing metallic mineral particles that discharge from hot_springs on the deep seafloor . the acoustic images record intensity of backscattering from the particulate matter suspended in the plumes . the visualization methods extract , classify , visualize , measure and track reconstructions of the plumes , depicted by isointensity surfaces as 3d volume objects and 2d slices . the parameters measured , including plume volume , cross sectional area , centerline location ( trajec-tory ) , surface area and isosurfaces at percentages of maximum backscatter intensity , are being used to derive elements of plume behavior including expansion with height , dilution , and mechanisms of entrainment of surrounding seawater . our aim is to compare the observational data with predictions of plume theory to test and advance models of the behavior of hydrothermal plumes through the use of multiple_representations . ments; and laboratory tank simulations . of these various approaches , acoustic methods are particularly well suited to synoptically image 3d hydrothermal plumes since they are large ( linear dimensions 10'-103m ) and contain suspended particulate matter . acoustic imaging of black smoker plumes is based on rayleigh backscattering from the metallic mineral particles suspended in the entire volume of the plume . the particles are small ( microns ) relative to the wavelength of the acoustic frequency employed ( wlcm at 330 khz; [3] ) . our initial calculations indicated that this type suspended particulate matter could be detected at ranges of up to hundreds of meters using acoustic frequencies of hundreds of khz [3] . we modified an existing sonar system to calculate specifications ( frequency 330 khz; pulse duration 100 microseconds; transmit power level of 220 db ) . 
using formal tools to study complex circuits behaviour we use a formal tool to extract finite state_machines ( fsm ) based representations ( lists of states and transitions ) of sequential_circuits described by flip_flops and gates . these complete and optimized representations helps the designer to understand the accurate behaviour of the circuit . this deep_understanding is a prerequisite for any verification or test_process . an example is fully presented to illustrate our method . this simple pipelined processor comes from our experience in computer architecture and digital_design education . ( [2] ) 
designing binding_pockets on protein surfaces using the a* algorithm the in silico design of ligands binding to the protein surface instead of deep binding_pockets is still a great challenge . representative examples are small molecules that target protein_protein_interactions [1] . the unbound experimental protein structures often lack appropriate binding_pockets and thus standard virtual screening techniques will fail . we previously presented a pocket detection protocol that provides a starting_point for in silico drug_design for such cases [2] . unfortunately , the underlying molecular_dynamics simulations make this protocol quite time-consuming . however , if the potential binding_site of a ligand is known , conformational sampling focused on this region appears more promising than scanning the whole protein surface for transient pockets . here , we present two new algorithms for designing tailored lig-and binding_pockets on the protein surface that account for backbone and side chain flexibility . at first , a prede-fined region of the protein surface is scanned for potential pocket positions using a program named pocketscanner . this program minimizes the protein energetically in the presence of generic pocket spheres representing the binding_pockets whose positions remain fixed . the side chains of the relaxed protein conformations are then further refined by a second program named pocketbuilder that searches for the best combination of side chain rotamers using the a* algorithm . the approach was tested on the proteins bcl-xl , il_2 , and mdm2 which are involved in protein_protein_interactions and hence challenging drug targets . although the native ligand binding pocket was not or not fully open in the unbound crystal_structure , pocketscanner and pocketbuilder successfully generated conformations with pockets into which the known inhib-itors could be docked in orientations similar to those seen in the inhibitor bound crystal structures . 
a new hyperheuristic algorithm for cross-domain search problems diplom-ingenieur computational_intelligence a new hyperheuristic algorithm for cross-domain search problems diplom-ingenieur computational_intelligence erkl rung zur verfassung der arbeit ( ort , datum ) ( unterschrift verfasser ) i acknowledgements i would like to express my deep gratitude to my advisor , priv . -doz . dr . nysret musliu , who guided me with great patience and encouragement throughout the process of this work . without him , i would not have developed my keen interest in the area of ai problem_solving and without his competent guidance , i could never have finished this work . i am also very grateful for the care and support from my loving parents who made it possible for me to enjoy an academic education . finally , i would like to thank my partner , for she bore with me in difficult times , gave me strength through her love and never lost faith in me . iii abstract hyperheuristics are an emergent area of search methodologies which try to address computationally hard problems at a new level_of_abstraction . instead of having a single algorithm that is optimised to perform well on a certain class of problem_instances , hyperheuristics try to leverage the power of a whole set of problem specific heuristic algorithms . by combining and parametrising these heuristics or heuristic components in different ways , the overall algorithm should be able to perform better over a wider range of problem_instances . ideally , a hyperheuristic does not need to know anything about the problem class it operates on and only very little about the available heuris-tics . because of this , hyperheuristics can often be applied without modifications to whole new problem domains and therefore represent a further step towards a very general problem_solving algorithm . this thesis describes a new hyperheuristic algorithm that was specifically designed to be completely problem domain independent and which works at a clearly defined level_of_abstraction . the algorithm is structured in different search phases that try to balance intensification and diversification of the search process . the available low_level heuristics are evaluated repeatedly in terms of certain properties and ranked according to a quality based metric that was found to work well across all tested low_level heuristics . furthermore , the algorithm incorporates ideas from a number of different areas of metaheuristic research , such as iterated local_search , simulated_annealing , tabu_search and genetic_algorithms . the main goal of this work was to develop a hyperheuristic that achieves good overall performance on a wide variety of unrelated search domains . in order to assess the generality of the algorithm , it was tested on six different , well-studied problem domains from 
boosting attribute and phone estimation accuracies with deep_neural_networks for detection-based speech_recognition generation of high_precision sub-phonetic attribute ( also known as phonological features ) and phone lattices is a key frontend component for detection-based bottom-up speech_recognition . in this paper we employ deep_neural_networks ( dnns ) to improve detection accuracy over conventional shallow mlps ( multi_layer perceptrons ) with one hidden_layer . a range of dnn architectures with five to seven hidden layers and up to 2048 hidden_units per layer have been explored . training on the si84 and testing on the nov92 wsj data , the proposed dnns achieve significant_improvements over the shallow mlps , producing greater than 90% frame-level attribute estimation accuracies for all 21 attributes tested for the full system . on the phone detection task , we also obtain excellent frame-level accuracy of 86 . 6% . with this level of high_precision detection of basic speech units we have opened the door to a new family of flexible speech recognition_system design for both top-down and bottom-up , lattice-based search strategies and knowledge integration . 
low_power floating_point computation sharing multiplier for signal_processing applications design of low_power , higher performance digital_signal_processing elements are the major requirements in ultra deep_sub_micron_technology . i . introduction the finite_impulse_response ( fir ) filters are used in signal_processing applications ranging from video and image_processing to wireless_communications . in some applications , such as video_processing , the fir_filter circuit must be able to operate at high-frequencies , while in other applications , such as cellular telephony , the fir_filter circuit must be a low-power circuit , capable of operating at moderate frequencies . these demands led the designers to focus on the algorithmic as well as numerical strength reduction techniques for the low_complexity design of fir_filter . strength reduction at the algorithmic level can be used to reduce the number of computations ( additions and multiplications ) . numerical strength reduction improves the performance of a computation . in this paper , numerical strength reduction is the area of interest . many previous efforts like common sub-expressions elimination [4] , [5] and differential coefficients method [6] , [7] explore low_complexity design of fir filters by minimizing the number of additions in filtering operations . canonical signed digit ( csd ) [8] is used to reduce the number of the required additions and subtractions for filtering operation by reducing the total number of nonzero bits in coefficients . in [6] the differences between absolute values of filter coefficients were employed to reduce the complexity of computation . all these techniques are limited to the optimization of hardware for a particular fixed coefficient set . a computation sharing multiplier ( cshm ) architecture , which identifies common computations and shares them between different multiplications was suggested in [1] , [2] overcomes this drawback and applicable for applications with programmable filter coefficients . cshm achieves high_performance programmable filtering operation by reusing the optimal precomputations and low-power_consumption , since , redundant computations are removed . in addition to signal_processing applications , the multipliers can also be used to test data_compression/decompression vlsi test applications [16] . the reconfigurable multiplier design based on reordering of partial product [14] and row-bypassing technique [15] are proposed to reduce the switching power . in the literature [2] , [10] and [11] cshm is used only in fixed_point fir_filter implementation . the main contribution of this paper is proposition of a floating_point multiplier based on the cshm technique , and effective implementation of floating_point fir_filter . henceforth , in this paper this multiplier will be referred as fcshm . the floating_point input values are taken in single-precision ieee-754 standard format . in the rounding stage of multiplier 
frame selection key to improve video_compression the huge usage of digital multimedia via communications media , wireless_communications , internet , intranet and cellular mobile leads to incurable growth of data_flow through these media . the researchers go deep in developing efficient techniques in these fields such as data_compression image_compression and video_compression . recently , video_compression techniques and their applications in many areas ( educational , agriculture , medical ) cause this field to be one of the most intersect fields . wavelet_transform is an efficient method that can be used to perform an efficient compression technique . the proposed approach deal with the developing of efficient video_compression approach based on frame selection key that concentrated on the calculation of frame near distance ( or difference between frames ) . the selection of the meaningful frame depends on many factors such as the compression performance , frame details and frame size and near distance between frames . in this paper , many videos are used and tested to insure the efficiency of the proposed technique , in addition a good performance results has been obtained . 
bakar kiasan : flexible contract checking for critical systems using symbolic_execution list of figures list of tables bakar kiasan : flexible contract checking for critical systems using symbolic_execution spark , a subset of ada for engineering safety and security-critical systems , is one of the best commercially available frameworks for formal_methods-supported development of critical software . spark is designed for verification and includes a software contract language for specifying functional properties of procedures . even though spark and its static_analysis components are beneficial and easy to use , its contract language is almost never used due to the burdens the associated tool support imposes on developers . symbolic_execution ( symexe ) techniques have made significant strides in automating reasoning_about deep_semantic properties of source_code . however , most work on symexe has focused on bug-finding and test_case generation as opposed to tasks that are more verification-oriented such as contract checking . in this paper , we present : ( a ) symexe techniques for checking software contracts in embedded critical systems , and ( b ) bakar kiasan , a tool that implements these techniques in an integrated_development_environment for spark . we describe a methodology for using bakar kiasan that provides significant increases in automation , usability , and functionality over existing spark tools , and we present results from experiments on its application to industrial examples . 
tactile_sensing and control of a planar manipulator tactile_sensing and control of a planar manipulator c abstract tactile_sensing and control of a planar manipulator this dissertation explores the shape sensing capabilities of cylindrical tactile_sensing ngers . starting with an elastostatic model for the deformation of rubber ngers , sensor spacing and depth requirements are determined to allow reconstruction of subsurface strain elds with insigniicant aliasing . given this bandlimited version of the strain eld , theoretical limits are found to classiication and scaling of the perceived indentation . these theoretical results lead to the design of a silicone rubber tactile sensor which is characterized and calibrated to the model . the reliability of curvature estimates from the sensor is then determined . finally , use of the sensor during manipulation is demonstrated . a spatial_frequency domain_model for the deformation of an elastic cylinder with a rigid core in plane strain is derived . based on the transfer_function from surface pressure to subsurface strain , several conclusions can be made about bandlimited tactile_sensing . first , we show that shear strain measurements are not useful for shape estimation . secondly we show that a ratio of core radius to outer radius greater than 0 . 85 is required for indenter classiication given sensor noise of 1 . 7% peak_strain . thirdly we show that , for deep sensors , indenter wedge angle may be inferred from an indenter radius estimate . these theoretical results are tested through experiments with a capacitive silicone rubber tactile sensor . the sensor has a noise level of 0 . 5% peak_strain , linearity of 1% peak_strain , and a sensitivity to nearby conductors of 3% peak_strain . identiication of the map from surface pressure fourier coeecients to sensor output is accomplished with a residual error of 1 . 4% peak_strain . nine diierent indenter radii ranging from a radius of 0 . 5 mm to 12 . 7 mm are estimated with a standard deviation of 0 . 6 mm for 200 n/m loads over 40 degrees of the sensor . contact location is estimated with an accuracy of 0 : 19 o 2 ( 0 . 043 mm ) . given the high_accuracy of the position estimation , position feedback is integrated into a grasp controller to allow optimal regrasping and manipulation of disks and rectangles . tactile curvature estimates are displayed to the operator at a 10hz rate during the manipulation . iii contents
k/ka_band channel characterization for mobile satellite systems mobile satellite systems allow truly ubiquitous wireless_communications to users anywhere and anytime , nasa's advanced communications technology satellite ( acts ) provides an ideal space-based platform for the measurement of k/ka_band propagation characteristics in a land mobile satellite application . field_tests for k-band propagation were conducted in three basic environments : clear line-of-sight ( los ) highways , lightly shadowed suburban , and heavily shadowed suburban . preliminary_results of these field_tests indicate very little multipath for rural environments and for clear los links while deep fades were experienced in shadowed areas , especially those where tree canopies covered the road . 
accurate_modeling method for deep_sub_micron cu interconnect this paper newly proposes an accurate_modeling method of the copper interconnect cross_section in which the width and thickness dependence on layout patterns and density are fully incorporated and universally expressed . in addition , we have developed specific test patterns for the model parameters extraction , and an efficient extraction flow . we have extracted the model parameters for 0 . 15 m cmos using this method and confirmed that 10% pd error n ormally observed with conventional lpe ( layout parameters extraction ) was completely dissolved . this is the first time that the practical and accurate_modeling methodology for layout pattern sensitive cu interconnect is ever reported . 
robotics olympiads : a new means to facilitate conceptualization of knowledge acquired in robot projects this paper proposes theoretical robotics competitions , offered in conjunction with robot contests , as the framework to foster deep_learning of concepts which underlie the practical projects and to facilitate the development of engineering aptitude . we present our experiences with integrating theoretical tests in the trinity_college fire-fighting home robot contest and national botball tournament . 
scaffolding for computer_supported writing to learn activities in vocational training dual-t project investigates how ict can support learning activities involving sharing and reflection about professional experience in order to harmonize school learning with practical experience . in this study we tested the effects of low and high scaffolding on collaborative_writing activities on professional procedures . we expected longer , more correct texts to emerge from strongly scaffolded activities than from weakly scaffolded activities . recent_research on initial vocational training education has shown the existence of a gap between field knowledge and knowledge taught in vocational_schools ( filliettaz , 2008 ) . one of the main_issues concerns knowledge and skill transfer between school and workplace ( eraut , 2004 ) . in our project , we are interested in identifying original technological support and pedagogical designs for professional skills learning and transfer in vocational educational training ( vet ) . in this context , we adopt a " writing-to-learn " approach ( hayes and flower , 1980; hayes , 1996 ) . it assumes that writing promotes the acquisition of knowledge , since domain_knowledge should be retrieved , reorganized and incorporated into a linear and understandable form . extending this cognitive view , galbraith ( 1999 ) claims that knowledge transformation leads to knowledge constitution , which makes writing a promising instructional tool . professional procedure learning and transfer is a critical issue in vet . anderson's act_r ( 1993 ) model claims that procedure acquisition is based on learning from declarative traces of initial problem_solving . writing could then be a powerful tool for constructing and refining the declarative representation of procedures . moreover , confrontation between learners' conceptions and experiences should promote reflexive thinking and epistemic monitoring , embodied in the written productions in addition , collaborative_writing activities should support not only individual knowledge_acquisition but also the collaborative dimension of domain knowledge_building . tynj l , mason and lonka ( 2001 ) show that studies of the effects of collaborative_writing on learning are still rare most of the research is done on the improvement of the writing process and writing skills . we consider that a peer collaborative approach to writing-to-learn in a vet context should be valuable in terms of knowledge_building , procedure understanding and acquisition . thus , in this research we are interested in investigating the impact of collaborative_writing activities on the construction of a mutual declarative representation of the procedures . this is the basis for deep understating of procedures thus for acquisition and transfer . computer_supported collaborative_writing to learn activities can be supported by many types of tools . considering our context and the population we are working with , we 
rtl emulation : the next leap in system verification the worldwide electronics market is booming , fueled by the customers' insatiable appetite for low_cost computers , connectivity and appliances packed with high-technology features . while the sheer number of new design starts may not be noteworthy , the development of new chips and systems that are becoming more complex at a phenomenal rate . the effect of increase in design_complexity is dual fold . first , designers are quickly migrating to higher_levels of abstraction . production use of text-based methodology has enabled designers to capture designs of hundreds of thousands of gates using graphic esda tools . second , is the change in manufacturing_process . today , designers are using 0 . 35 micron or even 0 . 25 micron_technology . submicron manufacturing capabilities enable millions of gates on a single chip . sematech predicts that 0 . 25 micron_technology will be ubiquitous by 1998 and the average chip complexity will be 20 million transistors . with most asic fabs running 0 . 5 micron processes reliably , and several delivering or scheduling a move to 0 . 25 micron soon , manufacturing capabilities are quickly outgrowing the capacity of current design_tools . the obstacles in developing new generation of electronics revolve around three key questions : 1 . can we design such complex chips and systems ? 2 . can our factories fabricate these designs ? 3 . can we verify the accuracy of these complex designs ? the advances in high level_design brought about by languages such as verilog and vhdl coupled with logic_synthesis technologies has largely addressed the first question . to answer the second question , semiconductor fabrications can now put millions of logic_gates on a single chip using deep_sub_micron_technology . this rapid increase in the complexity of chips and systems has outstripped traditional verification techniques . this is further complicated when a chip or an asic is plugged into the final system . this is because of the famous 90/50 rule . accordingly 90% of asics will work first time when tested stand-alone . however , only 50 % will work right when brought into the final system . a typical system level sign-off requires billions of clock cycles . when using traditional verification techniques , based upon software simulation , it will be several months , maybe even years before you can obtain a system sign-off in applications . such as real time video , atm , pci protocols and dsp applications . in a typical mpeg video_compression design it is not uncommon to visually verify 5 seconds of real time decoded video ( about 150 frames ) . using rtl simulation tools running at 
multiphase technique to speed-up delay measurement via sub-sampling a multi-phase technique for speeding up the measurement of delays via sub-sampling is presented . measurement of delays using the sub-sampling approach leads to a very simple system implementation , and also provides the opportunity of trading off between bandwidth and accuracy . such a scheme becomes extremely attractive for deep_sub_micron processes due to its highly-digital nature and the ability to offer compact , low_power , mixed_signal implementation alternatives . however , a drawback is the amount of averaging ( measurement time ) that is needed to get accurate results . a multiphase input clock scheme is proposed to address this issue , especially for the measurement of small delays , thereby speeding up the overall measurement . simulation_results from matlab simulink confirm the speedup achieved upto a factor of eight with an eight-phase clock input for sufficiently small fixed test delays and also an improvement in snr upto 11db for slowly varying test delays . 
joint application of ccsds file delivery protocol and erasure coding schemes over space communications the rising demand for multimedia services even in hazardous environments , such as space_missions and military theatres , and the consequent need of proper internetworking technologies have revealed the inapplicability of tcp/ip architectures and highlighted the importance of the communication features provided by the protocol architectures proposed by the consultative committee for space data systems ( ccsds ) . this paper proposes a ccsds file delivery protocol ( cfdp ) extension , based on the implementation of erasure coding schemes , in order to assure high reliability to the data_communication even in presence of very critical conditions , such as hard shadowing , deep fading periods and intermittent links . different encoding techniques are considered and various channel conditions , in terms of bit error ratio and bandwidth values , are tested . i . introduction the integration of satellite and wireless_network segments in the traditional internet has determined an increasing demand for communication technologies able to transport and deliver multimedia services to the final users . on the other hand , it has focused on the limits imposed by tcp/ip protocol architectures , when applied in hazardous environments characterized by long propagation_delays , intermittent , and asymmetric links , frequent transmission errors . taking the satellite technology as reference for its clear benefits concerning the wide area coverage and its inherent capability of broadcast/multicast communications , a protocol architecture alternative to tcp/ip has been designed by the consultative committee for space data systems ( ccsds ) . 
deep_belief_networks for phone recognition hidden_markov_models ( hmms ) have been the state-of-the-art techniques for acoustic_modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states . there are many proposals in the research_community for deeper models that are capable of modeling the many types of variability present in the speech generation process . deep_belief_networks ( dbns ) have recently proved to be very effective for a variety of machine_learning_problems and this paper applies dbns to acoustic_modeling . on the standard timit corpus , dbns consistently outperform other techniques and the best dbn achieves a phone error_rate ( per ) of 23 . 0% on the timit core test_set . 
when timing matters : enabling time accurate & scalable simulation of sensor_network applications the raising complexity of data_processing algorithms in sensor_networks combined with the severely limited computing power of sensor nodes make a deep understanding of the timely behavior of implementations a necessity . however , only cycle accurate emulation and test-beds provide a detailed and accurate insight into the timely behavior of sensor_networks . in this paper we introduce automated instrumentation of simulation models with detailed timing information derived from the sensor node and application binary to provide detailed timing information . the presented approach bridges the gap between scalable but abstracting simulation and cycle accurate emulation for sensor_network evaluation . by mapping device specific code with simulation models , we can derive the timing of each source_code line on a sensor node . as a result of such a mapping it is possible to determine the time and duration a certain code line gets executed . hence , eliminating the need to use expensive instruction level emulators with limited speed , restricted scal-ability and portability . furthermore , the proposed design is not bound to a specific hardware platform , a major advantage compared to existing emulators . our evaluation_shows , that the proposed technique achieves a timing accuracy of 99% compared to emulation while adding only a limited overhead . concluding , it combines essential properties like accuracy , speed and scalability on a single simulation platform . 
test oracle automation for v&v of an autonomous spacecraft's planner challenges and opportunities overall approach context the nasa " deep_space_1 " ( ds-1 ) spacecraft was launched in 1998 to evaluate promising new technologies and instruments . the " remote_agent " , an artificial_intelligence based autonomy architecture , was one of these technologies , and in 1999 this software ran on the spacecraft's flight processor and controlled the spacecraft for several days . we built automation to assist the software_testing efforts associated with the_remote_agent experiment . in particular , our focus was upon introducing test oracles into the testing of the planning and scheduling system component . this summary is intended to provide an overview of the work . the_remote_agent experiment used an on-board planner to generate sequences of high_level commands that would control the spacecraft . from a verification and validation ( v&v ) perspective , the crucial observation is that the_remote_agent was to be a self-sufficient autonomous system operating a spacecraft over an extended period , without human intervention or oversight . hence , v&v of its components , the planner included , was crucial . thorough testing was the primary means by which v&v of the planner was performed . however , the nature of the testing effort differed significantly from that of testing more traditional spacecraft control mechanisms . in particular : the planner's output ( plans ) were detailed and voluminous , ranging from 1 , 000 to 5 , 000 lines long . plans were intended to be read by software , and were not designed for easy perusal by humans . the planner could be called upon to operate ( generate a plan ) in a wide range of circumstances . this variety stems from the many possible initial conditions ( state of the spacecraft ) and the many plausible goals ( objectives the plan is to achieve ) . thorough v&v required testing the planner on thousands of test_cases , yielding a separate plan for each . each plan must satisfy all of the flight rules that characterize correct operation of the spacecraft . ( flight rules may refer to the state of the spacecraft and the activities it performs , and describe temporal conditions required among those states and activities . ) the information pertinent to deciding whether or not a plan passes a flight rule was dispersed throughout the plan . this exacerbated the problems of human perusal . as a consequence , manual inspection of more than a small fragment of plans generated in the course of testing was recognized to be impractical . the nature of the task v&v of a software component of an autonomous system meant that 
advances in optimizing recurrent_networks after a more than decade-long_period of relatively little research activity in the area of recurrent_neural_networks , several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent_networks . these advances have been motivated by and related to the optimization issues surrounding deep_learning . although recurrent_networks are extremely powerful in what they can in principle represent in terms of modeling sequences , their training is plagued by two aspects of the same issue regarding the learning of long_term dependencies . experiments reported here evaluate the use of clipping gradients , spanning longer time ranges with leaky integration , advanced momentum techniques , using more powerful output probability models , and encouraging sparser gradients to help symmetry_breaking and credit assignment . the experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training_and_test error . 
mirplib - a library of maritime inventory routing problem_instances : survey , core model , and benchmark results this paper presents a detailed description of a particular class of deterministic single product maritime inventory routing problems ( mirps ) , which we call deep_sea mirps with inventory tracking at every port . this class involves vessel travel times between ports that are significantly longer than the time spent in port and require inventory levels at all ports to be monitored throughout the planning horizon . after providing a comprehensive literature survey of this class , we introduce a core model for it cast as a mixed-integer linear_program . this formulation is quite general and incorporates assumptions and families of constraints that are most prevalent in practice . we also discuss other modeling features commonly found in the literature and how they can be incorporated into the core model . we then offer a unified discussion of some of the most common advanced techniques used for improving the bounds of these problems . finally , we present a library , called mirplib , of publicly available test problem_instances for mirps with inventory tracking at every port . despite a growing interest in combined routing and inventory management problems in a maritime setting , no data_sets are publicly available , which represents a significant " barrier to entry " for those interested in related research . our main goal for mirplib is to help maritime inventory routing gain maturity as an important and interesting class of planning_problems . as a means to this end , we ( 1 ) make available benchmark instances for this particular class of mirps; ( 2 ) provide the mixed-integer linear_programming community with a set of optimization_problem instances from the maritime transportation domain in lp and mps format; and ( 3 ) provide a template for other researchers when specifying characteristics of mirps arising in other settings . best known computational results are reported for each instance . 
comparing nonlinear vector network analyzer methodologies s cattering parameters ( s-parameters ) were first mentioned in articles and textbooks in the 1950s and 1960s by mat-thews , collins and kurokawa and popularized by the release of hewlett packard's first network analyzers in the 1960s . since then , s-parameters have been used to describe the complex characteristics of a network by quantifying the rf power flowing between its ports . s-parameters are essentially the ratio of the reflected and transmitted signal at a given port to the incident signal at a given port under perfect match conditions ( see figure 1 ) . when referring to a transistor , these parameters can be used to calculate return loss , gain and isolation . 1 it is important to understand that s-parameters are only truly valid for linear networks , where the characteristics of the network are independent of the level of the signal being driven into the network . when considering a semiconductor application such as a transistor , this refers to its small-signal operating condition , in which the input drive power does not affect the s-parameters of the transistor . under small-signal operating_conditions , a transistor will have linear gain at the fundamental drive frequency and not excite additional powers at the harmonic frequencies . linear operation of a transistor and the associated input and output waveforms are shown in figure 2 . the question arises how to thoroughly describe and model a transistor operating under nonlinear large-signal conditions . these conditions occur commonly for high power and high efficiency power amplifiers operating in advanced classes of operation . under these operating_conditions , powers are induced at harmonic frequencies when excited by a sine_wave , and in-band and out-of-band modulation products are created under modulation excita-tion . considering a simple sine_wave excitation , a transistor's characteristics are dependent on the level of the input drive signal , shown in both frequency and time domains in figure 3 . when the transistor is in deep compression and its output is composed of multiple harmonics , the device behavior cannot be described correctly by s-parameters , which are frequency_domain quantities . it is much more natural to analyze the behavior of the device under test ( dut ) in terms of time domain rf voltage and current waveforms . clear evidence of this is provided by the theoretical description of the different modes of operation of power amplifiers , which is completely done in time domain . in this case , a nonlinear vector network ana-lyzer ( nvna ) can be used to measure the incident 
entity-focused sentence simplification for relation extraction relations between entities in text have been widely researched in the natural_language_processing and information_extraction communities . the region connecting a pair of entities ( in a parsed sentence ) is often used to construct kernels or feature vectors that can recognize and extract interesting relations . such regions are useful , but they can also incorporate unnecessary distracting information . in this paper , we propose a rule-based method to remove the information that is unnecessary for relation extraction . protein protein_interaction ( ppi ) is used as an example relation_extraction problem . a dozen simple rules are defined on output from a deep_parser . each rule specifically examines the entities in one target interaction pair . these simple rules were tested using several ppi corpora . the ppi_extraction performance was improved on all the ppi corpora . 
a cross_validation study of deep_brain_stimulation targeting : from experts to atlas-based , segmentation-based and automatic registration_algorithms validation of image_registration algorithms is a difficult task and open-ended problem , usually application-dependent . in this paper , we focus on deep_brain_stimulation ( dbs ) targeting for the treatment of movement disorders like parkinson's_disease and essential_tremor . dbs involves implantation of an electrode deep_inside the brain to electrically stimulate specific areas shutting down the disease's symptoms . the subthalamic nucleus ( stn ) has turned out to be the optimal target for this kind of surgery . unfortunately , the stn is in general not clearly distinguishable in common medical imaging_modalities . usual techniques to infer its location are the use of anatomical atlases and visible surrounding landmarks . surgeons have to adjust the electrode intraoperatively using electrophysiological recordings and macrostimulation tests . we constructed a ground_truth derived from specific patients whose stns are clearly visible on magnetic_resonance ( mr ) t2-weighted images . a patient is chosen as atlas both for the right and left sides . then , by registering each patient with the atlas using different methods , several estimations of the_stn_location are obtained . two studies are driven using our proposed validation scheme . first , a comparison between different atlas-based and nonrigid registration_algorithms with a evaluation of their performance and usability to locate the stn automatically . second , a study of which visible surrounding structures influence the_stn_location . the two studies are cross validated between them and against expert's variability . using this scheme , we evaluated the expert's ability against the estimation error provided by the tested algorithms and we demonstrated that automatic stn targeting is possible and as accurate as the expert-driven techniques currently used . we also show which structures have to be taken into account to accurately estimate the_stn_location . 
image watermarking using psychovisual threshold over the edge currently the digital multimedia data can easily be copied . digital_image watermarking is an alternative approach to authentication and copyright_protection of digital_image content . an alternative embedding watermark based on human_eye properties can be used to effectively hide the watermark image . this paper introduces the embedding watermark scheme along the edge based on the concept of psychovisual threshold . this paper will investigate the sensitivity of minor changes in dct coefficients against jpeg quantization tables . based on the concept of psychovisual threshold , there are still deep holes in jpeg quantization values to embed a watermark . this paper locates and utilizes them to embed a watermark . the proposed scheme has been tested against various non-malicious attacks . the experiment results show the watermark is robust against jpeg image_compression , noise attacks and low_pass filtering . 1 introduction currently , an efficient access internet makes it easy to duplicate digital_image contents . in addition , current mobile_devices view and transfer compressed images heavily [1]-[4] . image watermarking is one of the popular techniques to manage and protect the copyright digital_image content . most of the image watermarking techniques exploits the characteristic of human_visual_system ( hvs ) in effectively embedding a robust watermark [5]-[7] . hvs is less sensitive to noise in highly textured area [8] and significantly changing region of an image . human_visual properties can be utilized in embedding process by insert more bits of watermark image for each block which has complex textures or edges on an image . the watermark with significant coefficients is more robust if it resides near round edges and texture areas of the image [9] . this paper proposes an embedding watermark scheme along the edge of the host image . this scheme enables the watermark to be more robust against non-malicious attacks . the organization of this paper is given as follows . the next section will give a brief description on the concept of pshycovisual threshold for image watermarking . section 3 presents an experimental design of the image watermarking . the
concurrent testing of digital_circuits for non-classical fault_models : resistive_bridging fault_model and n-detect test this work is concerned with the development of generic , non-intrusive and flexible algorithms for the design of digital_circuits with on line testing ( olt ) capability . most of the works presented in the literature on olt have used single stuck_at_fault models . however , in deep_submicron_era single s-a fault_models may not capture more than a fraction of the real defects . to cater to the problem it is now advocated that additional fault_models such as resistive_bridging faults , transition_faults , delay_faults etc . are also used . the proposed technique is one of the first works that enables on-line detection of resistive_bridging faults and provides a high value of n for the n-detect tests . the technique can handle generic digital_circuits with cell count as high as 15 , 000 and having the order of 2 500 states . results for design of on-line detectors for various iscas89 benchmark_circuits are provided . the results illustrate that with marginal increase in area_overhead , if compared to ones with single s-a fault_coverage , the proposed scheme also provides coverage for resistive_bridging faults and high value of n for n-detect coverage . the results have also been verified in silicon using fpgas
current approaches to xml management xml management systems vary widely in their expressive power and query-processing efficiency , and users should choose the xmlms that best meets their needs . t he extensible_markup_language has become the standard for information interchange on the web . developed primarily as a document markup_language more powerful than html yet less complex than sgml , xml does not require content to adhere to structural rules . xml gives a single , human-readable syntax for representing data , including data in relational format . hence xml appeals to both the document and the database communities . early developers of xml content storage tools , who came from the database community , regarded xml as yet another data format for adapting relational and sometimes object-relational data_processing tools . while this use of xml is acceptable , it does not harness xml's full power . xml is inherently semistructured . however , documents subscribing to the data-centric view of xml are highly structured and can be represented equivalently in tables or in xml with document type definitions ( dtds ) or xml_schema specifications ( see the sidebar , " related w3c documents , " for this and other xml specifications ) . as in traditional relational_databases , sibling element order is unimportant in such documents . we refer to documents with implicitly ordered xml content as document_centric . the file's element order ( as siblings in a tree-like representation ) conveys its implicit order , whereas a document attribute or tag expresses an explicit order . although it is easy to express explicit order in relational_databases , capturing the implicit order while converting a document__centric xml document into a rela-tional database is a problem . besides the implicit order , document_centric xml_documents allow little or no structure , deep nesting , and hyperlinked components . tables can represent implicit order , nesting , and hyperlinks but only with costly time and space transformations . this article studies the data-and document__centric uses of xml management systems ( xmlms ) . we want to provide xml_data users with a guideline for choosing the data_management system that best meets their needs . because the systems we test are first-generation
learning factored representations in a deep mixture of experts mixtures of experts combine the outputs of several " expert " networks , each of which specializes in a different part of the input space . this is achieved by training a " gating " network that maps each input to a distribution over the experts . such models show promise for building larger networks that are still cheap to compute at test time , and more parallelizable at training time . in this this work , we extend the mixture of experts to a stacked model , the deep mixture of experts , with multiple sets of gating and experts . this exponentially increases the number of effective experts by associating each input with a combination of experts at each layer , yet maintains a modest model size . on a randomly translated version of the mnist dataset , we find that the deep mixture of experts automatically learns to develop location-dependent ( " where " ) experts at the first layer , and class-specific ( " what " ) experts at the second layer . in addition , we see that the different combinations are in use when the model is applied to a dataset of speech monophones . these demonstrate effective use of all expert combinations . 
is iddq_testing not applicable for deep_submicron_vlsi in year 2011 ? in this work , iddq current for the deep_submicron_vlsi in year 2011 is estimated with a statistical approach according to the international_technology_roadmap_for_semiconductors 1999 edition considering process_variations and different input vectors . the estimated results show that the standard deviation of the iddq current is proportional to the square_root of the circuit size and the iddq currents of the defect_free and the defective devices , which are of the size up to 1x10' gates , are still differentiable under the condition of random_process deviations and input vectors . two new iddq_testing schemes , which detect the defective current based on the two separate lddq distributions , are proposed . from the study , it is concluded that iddq_testing is still applicable for the deep_submicron_vlsi for the next ten years . 
a novel scan segmentation design method for avoiding shift timing failure in scan testing high power_consumption in scan testing can cause undue yield_loss which has increasingly become a serious problem for deep_submicron_vlsi circuits . growing evidence attributes this problem to shift timing_failures , which are primarily caused by excessive switching_activity in the proximities of clock paths that tends to introduce severe clock_skew due to ir-drop-induced delay increase . this paper is the first of its kind to address this critical issue with a novel layout-aware scheme based on scan segmentation design , called lcti-ss ( low-clock-tree-impact scan segmentation ) . an optimal combination of scan segments is identified for simultaneous clocking so that the switching_activity in the proximities of clock trees is reduced while maintaining the average power_reduction effect on conventional scan segmentation . experimental results on benchmark and industrial circuits have demonstrated the advantage of the lcti-ss scheme . 
rl$^2$ : fast reinforcement_learning via slow reinforcement_learning deep reinforcement_learning ( deep rl ) has been successful in learning sophisticated behaviors automatically; however , the learning_process requires a huge number of trials . in contrast , animals can learn new tasks in just a few trials , benefiting from their prior_knowledge about the world . this paper seeks to bridge this gap . rather than designing a " fast " reinforcement_learning algorithm , we propose to represent it as a recurrent neural_network ( rnn ) and learn it from data . in our proposed_method , rl 2 , the algorithm is encoded in the weights of the rnn , which are learned slowly through a general_purpose ( " slow " ) rl algorithm . the rnn receives all information a typical rl algorithm would receive , including observations , actions , rewards , and termination flags; and it retains its state across episodes in a given markov_decision_process ( mdp ) . the activations of the rnn store the state of the " fast " rl algorithm on the current ( previously unseen ) mdp . we evaluate rl 2 experimentally on both small_scale and large_scale problems . on the small_scale side , we train it to solve randomly generated multi-armed bandit problems and finite mdps . after rl 2 is trained , its performance on new mdps is close to human-designed algorithms with optimality guarantees . on the large_scale side , we test rl 2 on a vision-based navigation task and show that it scales up to high_dimensional problems . 
enforcing applicability of real_time_scheduling_theory feasibility_tests with the use of design_patterns this article deals with performance verifications of architecture models of real-time embedded_systems . we focus on models verified with the real_time_scheduling_theory . to perform verifications with the real_time_scheduling_theory , the architecture designers must check that their models are compliant with the assumptions of this theory . unfortunately , this task is difficult since it requires that designers have a deep understanding of the real_time_scheduling_theory . in this article , we investigate how to help designers to check that an architecture model is compliant with this theory . we focus on feasibility_tests . feasibility_tests are analytical methods proposed by the real_time_scheduling_theory . we show how to explicitly model the relationships between an architectural model and feasibility_tests . from these models , we apply a model_based engineering process to generate a decision tool what is able to detect from an architecture model which are the feasibility_tests that the designer can apply . 
cultural ant algorithm for continuous optimization_problems in order to overcome prematurity of ant_colony algorithm , the conception of belief_space originated in cultural algorithm is introduced , and a new cultural ant algorithm is proposed for continuous optimization_problems . firstly , the coding scheme for ant_colony algorithm to solve continuous optimization_problems is discussed . then belief_space is brought in , and designed as the form of two parts : individual belief_space and population belief_space . the former is used to conduct individuals' deep search for better solutions , and the other to help worse individuals drop their current bad solution space for broad search . the update rules of both population space and belief_space are given subsequently . eight common standard functions are used to test the new algorithm , which is compared with four other algorithms at the same time . the results show effectiveness and superiority of the new algorithm . finally the effect of the parameter used in the algorithm is discussed , and so does the both two belief_space . 
tongue_line extraction tongue_line refers to the surface of the tongue covered with fissures or lines in deep or shallow shape and is one type of important features in clinical practice of traditional_chinese tongue diagnosis ( tctd ) . however , it is hard to extract tongue lines completely due to the large variation of the widths of tongue lines and the strong noise caused by the rough surface of tongue and uneven illumination . in this paper , an improved wide line detector ( wld ) is presented for tongue_line extraction . based on the characteristics of tongue lines , the original wld is improved to avoid the undesired separation of a wide line and the influence of uneven lighting_conditions . the proposed method has been tested on a total of 286 tongue_line images and our experimental_results_demonstrate that the improved wld significantly outperforms the original wld for tongue_line extraction by improving the tpr 16 . 5% , fpr 44 . 6% and pm 33 . 4% , respectively . 
extremal combinatorics deals with the problem combinatorics deals with the problem of determining or estimating the maximum or minimum possible value of an invariant of a combinatorial object that satisfies certain requirements . my research is on problems of extremal combinatorics motivated by applications in computer science , engineering , biology , and nanotechnology . the following is a sample of topics that i am currently interested in . coding_theory : how to construct good codes that would protect against errors introduced by communication channels when information is transmitted across them ? of particular interest a r e u n c o n v e n t i o n a l c h a n n e l s s u c h a s insertion/deletion channels , q-ary channels ( q > 2 ) , quantum transmission channels , and dna storage channels . electronic_design_automation : power efficiency and signal_integrity are two important design criteria in deep_submicron_vlsi . how do we design information transmission schemes that are power-efficient , and protects against interference at the deep submicron level ? group_testing : nonadaptive group_testing algorithms are useful when we want to trade resources for time , in identifying objects of a certain trait in large populations . what is the best tradeoff we can obtain ? nanotechnology : nanometer-scale components are prone to manufacturing errors . what and how much redundancy must be introduced in the design of these components so that in the face of manufacturing defects , we can reconfigure via the redundancy introduced to render the component still useful ? extremal combinatorial objects are often difficult to determine . examples of such objects are optimal codes , combinatorial designs , ramsey graphs , and various geometric packings and coverings . i am also interested in the design of practical algorithms for searching for extremal combinatorial objects , and whenever possible to enumerate them . in general , i am interested in multidisciplinary problems , especially those lying in the intersection of discrete_mathematics , computer_science , and engineering . limit on the addressability of fault_tolerant nanowire decoders , " ieee transactions on computers . divisible codes and their application in the construction of optimal constant-composition codes of weight three " , ieee the sizes of optimal q-ary codes of weight three and distance four : a complete solution " , ieee transactions on information on extremal k-graphs without repeated copies of 2-intersecting edges " , siam journal on discrete optimal memoryless encoding for low_power off-chip data buses " , in iccad asymptotically optimal erasure-resilient codes for large disk arrays 
testing the untestable : model_testing of complex software-intensive systems increasingly , we are faced with systems that are <i>untestable</i> , meaning that traditional testing_methods are expensive , time-consuming or infeasible to apply due to factors such as the systems' continuous interactions with the environment and the deep intertwining of software with hardware . in this paper we outline our vision to enable testing of untestable systems . our key idea is to frame testing on <i>models</i> rather than operational systems . we refer to such testing as <i>model_testing</i> . our goal is to raise the level_of_abstraction of testing from operational systems to models of their behaviors and properties . the models that underlie model_testing are executable representations of the relevant aspects of a system and its environment , alongside the risks of system failures . such models necessarily have uncertainties due to complex , dynamic environment behaviors and the unknowns about the system . this makes it crucial for model_testing to be uncertainty-aware . we propose to synergistically combine metaheuristic search , increasingly used in traditional software_testing , with system and risk models to drive the search for faults that entail the most risk . we expect model_testing to bring early and cost_effective automation to the testing of many critical systems that defy existing automation techniques , thus significantly improving the dependability of such systems . 
hybrid simulated-emulated platform for heterogeneous access networks performance investigations the availability of different access technologies enables the creation of heterogeneous_networks supporting users mobility and assuring several different services . meanwhile these networks require complex control techniques to assure quality of service ( qos ) to users . before implementing such networks , a deep performance_analysis , through the use of network simulators or real models , is necessary . in particular the first ones ( e . g . network simulator 3-ns-3 among the others ) are quite simple and easy to manage and configure , while the second ones assure the handling of real traffic flows . the main contribution of this paper is the description of an hybrid simulated and emulated network evaluation platform , developed by the authors . the platform purpose is to execute a performance analysis of different wireless_networks such as long_term evolution ( lte ) and wi_fi , connected to a core_network implementing the differentiated service ( diffserv ) protocol . the paper contains also the results of preliminary validation tests . 
semantic_role labeling using lexical statistical information our system for semantic_role labeling is multi-stage in nature , being based on tree pruning techniques , statistical_methods for lexicalised feature encoding , and a c4 . 5 decision_tree classifier . we use both shallow and deep syntactic_information from automatically_generated chunks and parse_trees , and develop a model for learning the semantic arguments of predicates as a multi_class decision_problem . we evaluate the performance on a set of relatively 'cheap' features and report an f 1 score of 68 . 13% on the overall test_set . 
where am i ? excerpt from brainstorms : philosophical essays on mind and psychology now that i've won my suit under the freedom of information act , i am at liberty to reveal for the first time a curious episode in my life that may be of interest not only to those engaged in research in the philosophy of mind , artificial_intelligence , and neuroscience but also to the general public . several years_ago i was approached by pentagon officials who asked me to volunteer for a highly dangerous and secret mission . in collaboration with nasa and howard_hughes , the department of defense was spending billions to develop a supersonic tunneling underground device , or stud . it was supposed to tunnel through the earth's core at great speed and deliver a specially designed atomic warhead "right up the red's missile silos , " as one of the pentagon brass put it . the problem was that in an early test they had succeeded in lodging a warhead about a mile deep under tulsa , oklahoma , and they wanted me to retrieve it for them . "why me ? " i asked . well , the mission involved some pioneering applications of current brain research , and they had heard of my interest in brains and of course my faustian curiosity and great courage and so forth . . . . well , how could i refuse ? the difficulty that brought the pentagon to my door was that the device i'd been asked to recover was fiercely radioactive , in a new way . according to monitoring instruments , something about the nature of the device and its complex interactions with pockets of material deep in the earth had produced radiation that could cause severe abnormalities in certain tissues of the brain . no way had been found to shield the brain from these deadly rays , which were apparently harmless to other tissues and organs of the body . so it had been decided that the person sent to recover the device should leave his brain behind . it would be kept in a sale place as there it could execute its normal control functions by elaborate radio links . would i submit to a surgical_procedure that would completely remove my brain , which would then be placed in a life_support system at the manned_spacecraft_center in houston ? each input and output pathway , as it was severed , would be restored by a pair of microminiaturized radio transceivers , one attached precisely to the brain , the other to the nerve stumps in the empty cranium . no information would be lost , all 
lacunar strokes : does shape matter ? u p to one in every four ischemic strokes can be lacunar_infarcts . traditionally , deep_brain infarctions with a maximum diameter of 15-20 mm have been attributed to occlusion of a single penetrating artery by lipohyalinosis or microatheromatosis , but it has been suggested that other etiologies such as embolism may be responsible for up to one third of the cases . lacunes can be detected by neuroimaging in asympto-matic individuals and are associated with greater risk of cognitive decline 1 , 2 . lacunar_infarcts can have a diameter greater than 15 mm on axial sections and greater than 20 mm on coronal or sagittal mri sections 3 , 4 . it has been argued that shapes of lacunar_infarcts may be linked to pathogenesis : lesions with an irregular shape may result from occlu-sion of largest perforating arteries , or from confluence of lesions , or even from secondary tissue degeneration . lacunes with ovoid or spheroid , regular shapes may reflect involvement of smallest arteries 5 . in this issue of arquivos de neuro-psiquiatria , feng et al . 6 evaluated clinical and imaging features as well as prognosis of 204 chinese patients admitted with acute lacunar_infarcts , defined as hypointense focal lesions in t1_weighted images and hyperintense focal le-sions in t2-weighted , flair , and dwi images with a diameter ranging from 3 mm to 20 mm . radiologists ( unaware of clinical characteristics ) classified infarcts as regular or irregular by visual inspection of mri images . the authors hypothesized that pathogenesis , clinical symptoms , and prognosis would differ between patients with acute regular or irregular infarcts . sizes of the infarcts and extent of leukoaraiosis were also evaluated . blood_pressure , blood_glucose , hemoglobin a1c and lipids , as well as blood_pressure varia bility ( bpv ) were checked within the first 24 hours after stroke . the authors did not mention criteria used to define risk factors for vascular_disease such as arterial hyperten-sion , diabetes_mellitus , dyslipidemia ( history ? medical records ? measurements/tests performed only before stroke , or either before or after stroke ? ) . logistic regressions were used to test for associations between risk factors for vascular_disease , variation in systolic blood , shape and size of lacunar lesions , extent of leukoaraiosis , neurological deterioration ( increase in nih stroke scale scores at two weeks after stroke ) , and prognosis ( modified rankin scale at 3 months ) . the only variable independently associated with shapes of lacunar_infarcts was bpv within the first 24 hours . the authors concluded that " bpv is an independent risk_factor for irregularly shaped 
deterministically testing sparse polynomial identities of unbounded degree we present two deterministic algorithms for the arithmetic circuit identity testing problem . the running time of our algorithms is polynomially bounded in s and m , where s is the size of the circuit and m is an upper bound on the number monomials with non-zero coefficients in its standard representation . the running time of our algorithms also has a logarithmic dependence on the degree of the polynomial but , since a circuit of size s can only compute polynomials of degree at most 2 s , the running time does not depend on its degree . before this work , all such deterministic algorithms had a polynomial dependence on the degree and therefore an exponential dependence on s . our first algorithm works over the integers and it requires only black-box access to the given circuit . though this algorithm is quite simple , the analysis of why it works relies on linnik's theorem , a deep result from number_theory about the size of the smallest prime in an arithmetic_progression . our second algorithm , unlike the first , uses elementary arguments and works over any integral domains , but it uses the circuit in a less restricted manner . in both cases the running time has a logarithmic dependence on the largest coefficient of the polynomial . 
an evaluation of penny : a system for fine grain implicit parallelism the penny system is an implementation of akl , a concurrent constraint language with deep guards , on shared_memory multiprocessors . it automatically extracts paral-lelism in arbitrary akl programs . no user annotations are required nor there is any compiler support to extract par-allelism . we give an overview of the system and present empirical_evaluation results from a set of benchmarks with varying characteristics . the evaluation_shows that it is possible to build a system that automatically exploits ne-grain parallelism for a wide range of programs . 1 introduction the penny system is an implementation of akl , a concurrent constraint language with deep guards . the system has been implemented on a high_performance shared_memory multiprocessor and is able to outperform c implementations of algorithms with complex dependencies without any user annotations . in this paper we describe a performance_evaluation of the system . extensive measurements were done using both smaller benchmarks as well as real_life programs . the evaluation uses detailed instruction-level simulation , including cache-performance , to explain the behavior of the system . section 2 of the evaluation_shows the performance of the system for diierent classes of benchmarks . the tests include simple recursive , stream parallel and non_deterministic benchmarks . the next two section show the limitations of the system when the granularity of work decreases . in section 3
institute of physics publishing journal of micromechanics and microengineering fabrication of keyhole-free ultra-deep high-aspect_ratio isolation trench and its applications an ultra-deep ( 40 120 m ) keyhole-free electrical isolation trench with an aspect ratio of more than 20 : 1 has been fabricated . the process combines drie ( deep_reactive_ion etch ) , lpcvd insulating materials refilling and tmah or koh backside etching technologies . employing multi-step drie with optimized etching conditions and a sacrificial polysilicon layer , the keyholes in trenches are prevented; as a result the mechanical strength and reliability of isolation trenches are improved . electrical tests show that such an isolation trench can electrically isolate the mems structures effectively from each other and from on-chip detection circuits . the average resistance in the range of 0 100 v is more than 10 12 , and the breakdown_voltage is above 205 v . this technology has been successfully employed in the fabrication of the monolithic integrated bulk micromachining mems gyroscope . 
power crisis in soc_design : strategies for constructing low_power , high_performance soc designs this special panel session brings together several leading technologists to discuss the challenges and solutions in constructing soc designs that achieve their performance goals within a very tight power budget . these challenges are addressed from the often conflicting perspectives of semiconductor design teams and commercial solutions providers of eda construction tools , eda analysis_tools and semiconductor ip ( sip ) . with the addition of high_performance features to battery-operated devices and packages hitting thermal limits in desktop and server devices , automated soc low_power design_methodology is at a crisis . in order for designers to develop algorithms and partition their systems for optimized power , they must depend on a set of power-efficient and accurately power-characterized sip components ( standard cells , memories , i/o , and mixed_signal ) . designers then construct their socs using a long chain of eda design_tools from design planning , simulation , and synthesis , to placement and routing . they must analyze these designs using a chain of extraction and verification tools for timing , power , test , design rule compliance and signal_integrity . traditionally , synthesis tools have focussed on achieving timing closure , routers with area minimization and floor planners arbitrating between those two often conflicting goals . dynamic and static power minimization have been relegated to analysis_tools that point out problems designed-in by the construction tools . today's deep_submicron_cmos transistors are incredibly dense , amazingly fast , but ravenously hungry in dynamic and static power often making the task of power_optimization the major design challenge . the results are often missed power budgets , larger batteries and heavy heat sinks . soc_design is a team_sport requiring the close coordination of many ic design disciplines with the commercial solutions providers of eda tools and soc ip . specifically : what are the most significant power issues ? who should drive the sip power characterization standards shared by the construction and analysis_tools ? where do designers need the most low_power design_methodology leadership to be successful and from where is it going to come ? 
situated interaction on spatial topics acknowledgements several people and institutions were involved in process of creating this work , and i would like to express my gratefulness towards them here . professor wahlster provided me with the chance to work on my phd at his chair , and i would like to thank him for finding the time to advise me in spite of the tremendous amount of his other duties . professor freaks agreed to become my second advisor , for which i am very grateful . the klaus tschira foundation ( kts ) supported me either directly or indirectly throughout nearly the entire time i was working on my phd . i would like to express my gratitude for providing me with two project grants ( maptalk and sisto ) , not only towards the kts but also towards klaus tschira personally . the work has also been partially supported by the deutsche_forschungsgemeinschaft ( dfg ) within the collaborative research_center 378 . a thesis such as this one is never realized in the void . i would therefore like to thank my colleagues at the european media_lab as well as those at other institutes , who worked hard with me to realize deep map , for the good cooperation . the same applies to my coworkers at the german research_center for artificial_intelligence and at the chair of professor wahlster . i also want to thank the coauthors of the publications that i was involved in for inspiring conversations and for jointly working out many ideas . i am especially thankful for the constructive criticism from the readers of the many draft versions of this thesis : short abstract in this thesis , we present a model and an implementation to handle situational interactions on spatial topics as well as several adaptation strategies to cope with common problems in real_world_applications . the model is designed to incorporate situational factors in spatial reasoning processes at the basic level and to facilitate its use in a wide range of applications . the implementation realizing the model corresponds very closely to the structure of the model , and was put to test in a scenario of a mobile tourist guide . the adaptation strategies address the lack of information , resource restrictions as well as the problem of varying availability and quality of positional information . abstract the interaction on spatial topics is highly important no only in the context of mobile and situated systems but also in other fields such as natural_language access to maps or 
the effects of bilateral subthalamic_nucleus deep_brain_stimulation ( stn_dbs ) on cognition in parkinson_disease . the effects of subthalamic_nucleus ( stn ) stimulation on cognition and mood have not been well established . the authors estimated cognitive and mood effects of bilateral subthalamic_nucleus deep_brain_stimulation ( stn_dbs ) in patients with parkinson's disease ( pd ) at 6 months and 1 year postoperatively . forty-six patients were recruited from the movement_disorder center at seoul national_university_hospital . neuropsychologic tests were performed three times , before , 6 months after , and 1 year after surgery . mean patient age was 58 and mean education duration 8 years . eighteen of the 46 patients were men . the instruments used for assessing cognitive functions were; the mini-mental_status_examination ( mmse ) , the trail making test ( tmt ) , the korean boston naming test ( k-bnt ) , the rey-kim memory battery , the grooved pegboard test , the stroop test , a fluency test , the wisconsin card sorting test ( wcst ) , and the beck_depression_inventory ( bdi ) . of these tests , the verbal memory test , the stroop test , and the fluency test showed statistically_significant changes . the verbal memory test using the rey-kim memory battery showed a decline in delayed recall and recognition at 6 months and 1 year postoperatively , whereas nonverbal memory showed no meaningful change . in terms of frontal_lobe function tests , stroop test and fluency test findings were found to be aggravated at 6 months and this continued at 1 year postoperatively . previous_studies have consistently reported a reduction in verbal fluency and improvements in self-reported symptoms of depression after stn_dbs . however , in the present study , beck_depression_inventory ( b . d . i . ) was not significantly changed . other tests , namely , mmse , tmt , k-bnt , grooved pegboard test , and the wcst also failed to show significant changes . of the baseline characteristics , age at onset , number of years in full-time education , and l_dopa equivalent dosage were found to be correlated with a postoperative decline in neuropsychological_test results . the correlation of motor improvement and cognitive deterioration was not significant , which suggests that the stimulation effect is rather confined to the motor-related part in the stn . in conclusion , bilateral stn_dbs in parkinson's_disease did not lead to a significant global deterioration in cognitive_function . however , our findings_suggest that it has minor detrimental long_term impacts on memory and frontal_lobe function . 
supporting children's learning with body-based metaphors in a mixed_reality environment we describe an approach to designing immersive learning experiences for children using <i>body-based metaphors</i> . previous_research shows benefits for learning through physical interactions in virtual spaces ( e . g . , [1 , 16] ) ---here we look specifically at using mixed_reality to embed children as elements within the systems they are attempting to learn . using gross body-movements the children are able to test predictions and have their intuitions challenged , laying the foundation for deeper conceptual understanding . we present data from a study we conducted comparing the mixed_reality experience with a desktop version of the same simulation . results_suggest that children's interactions with designs supporting body-based metaphors can lead them to better grasp the "deep_structure" of the learning domain . 
extracting relations with integrated information using kernel_methods entity relation detection is a form of information_extraction that finds predefined relations between pairs of entities in text . this paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel_methods . information from three different levels of processing is considered : tokenization , sentence parsing and deep dependency analysis . each source of information is represented by kernel functions . then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels . we present an evaluation of these methods on the 2004 ace relation detection task , using support_vector_machines , and show that each level of syntactic processing contributes useful information for this task . when evaluated on the official test_data , our approach produced very competitive ace value scores . we also compare the svm with knn on different kernels . 
deep_sub_micron iddq_testing : issues and solutions the effectiveness of i/sub ddq/_testing in deep sub-micron is threatened by the increased transistor sub-threshold leakage_current . in this article , we survey possible solutions and propose a deep_sub_micron i/sub ddq/ test_mode . the methodology provides means for unambiguous measurements of i/sub ddq/ components and defect diagnosis . the effectiveness of the test_mode is demonstrated with a real_life example . 
adaptive acquisition and tracking for deep_space array feed antennas the use of radial_basis_function ( rbf ) networks and least_squares algorithms for acquisition and fine tracking of nasa's 70-m-deep_space_network antennas is described and evaluated . we demonstrate that such a network , trained using the computationally efficient orthogonal least_squares algorithm and working in conjunction with an array feed compensation system , can point a 70-m-deep_space antenna with root mean square ( rms ) errors of 0 . 1-0 . 5 millidegrees ( mdeg ) under a wide range of signal_to_noise ratios and antenna elevations . this pointing accuracy is significantly better than the 0 . 8 mdeg benchmark for communications at ka_band frequencies ( 32 ghz ) . continuous adaptation strategies for the rbf network were also implemented to compensate for antenna aging , thermal gradients , and other factors leading to time-varying changes in the antenna structure , resulting in dramatic improvements in system performance . the systems described here are currently in testing phases at nasa's goldstone deep_space_network ( dsn ) and were evaluated using ka_band telemetry from the cassini spacecraft . 
mathesis : an intelligent web_based algebra tutoring school this article describes an intelligent , integrated , web_based school for tutoring expansion and factoring of algebraic expressions . it provides full support for the management of the usual teaching tasks in a traditional school : student and teacher registration , creation and management of classes and test papers , individualized assignment of exercises , intelligent step_by_step guidance in solving exercises , student interaction recording , skill mastery statistics and student assessment . the intelligence of the system lies in its algebra tutor , a model-tracing tutor developed within the mathesis project , that teaches a breadth of 16 top-level math skills ( algebraic operations ) : monomial multiplication , division and power , monomial-polynomial and polynomial-polynomial multiplication , parentheses elimination , collect like terms , identities ( square of sum and difference , product of sum by difference , cube of sum and difference ) , factoring ( common factor , term grouping , identities , quadratic_form ) . these skills are further decomposed in simpler ones giving a deep domain expertise model of 104 primitive skills . the tutor has two novel features : a ) it exhibits intelligent task recognition by identifying all skills present in any expression through intelligent parsing , and b ) for each identified skill , the tutor traces all the sub-skills , a feature we call deep model tracing . furthermore , based on these features , the tutor achieves broad knowledge monitoring by recording student performance for all skills present in any expression . forty teachers who evaluated the system in a 3-hours workshop appreciated the fine_grained step_by_step guidance of the student , the equally fine_grained student model created by the tutor and its ability to tutor any exercise that contains the aforementioned math skills . the system was also used in a real junior high_school classroom with 20 students for three months . evaluation of the students' performance in the domain of factoring gave positive learning results . 
dynamic data_mining : exploring large rule spaces by sampling a great challenge for data mining_techniques is the huge space of potential rules which can be generated . if there are tens of thousands of items , then potential rules involving three items number in the trillions . traditional data_mining techniques rely on downward-closed measures such as support to prune the space of rules . however , in many applications , such pruning techniques either do not suuciently reduce the space of rules , or they are overly restrictive . we propose a new solution to this problem , called dynamic data_mining ddm . ddm foregoes the completeness ooered by traditional techniques based on downward-closed measures in favor of the ability to drill deep into the space of rules and provide the user with a better view of the structure present in a data_set . instead of a single determinstic run , ddm runs continuously , exploring more and more of the rule space . instead of using a downward-closed measure such as support to guide its exploration , ddm uses a user-deened measure called weight , which is not restricted to be downward closed . the exploration is guided by a heuristic called the heavy edge property . the system incorporates user feedback b y allowing weight to be redeened dynamically . we test the system on a particularly diicult data_set the word usage in a large subset of the world_wide_web . we nd that dynamic data_mining is an eeective tool for mining such diicult data_sets . 
a comprehensive benchmark of kernel_methods to extract protein protein_interactions from literature the most important way of conveying new findings in biomedical research is scientific publication . extraction of protein_protein_interactions ( ppis ) reported in scientific publications is one of the core topics of text_mining in the life_sciences . recently , a new class of such methods has been proposed - convolution kernels that identify ppis using deep parses of sentences . however , comparing published_results of different ppi_extraction methods is impossible due to the use of different evaluation corpora , different evaluation metrics , different tuning procedures , etc . in this paper , we study whether the reported performance metrics are robust across different corpora and learning settings and whether the use of deep parsing actually leads to an increase in extraction quality . our ultimate goal is to identify the one method that performs best in real_life scenarios , where information_extraction is performed on unseen text and not on specifically prepared evaluation data . we performed a comprehensive benchmarking of nine different methods for ppi_extraction that use convolution kernels on rich linguistic_information . methods were evaluated on five different public corpora using cross_validation , cross-learning , and cross-corpus evaluation . our study confirms that kernels using dependency trees generally outperform kernels based on syntax trees . however , our study also shows that only the best kernel_methods can compete with a simple rule_based approach when the evaluation prevents information_leakage between training_and_test corpora . our results further reveal that the f-score of many approaches drops significantly if no corpus-specific parameter optimization is applied and that methods reaching a good auc score often perform much worse in terms of f-score . we conclude that for most kernels no sensible estimation of ppi_extraction performance on new text is possible , given the current heterogeneity in evaluation data . nevertheless , our study shows that three kernels are clearly superior to the other methods . 
radar_imaging and sounding of polar ice sheets we developed a synthetic apertur radar ( sar ) for imaging the ice_bed_interface , and a wideband radar for measuring ice thickness and fine-resolution mapping of internal layers . we designed the synthetic_aperture_radar ( sar ) to operate in bistatic or monostatic modes for generating two-dimensional re-flectivity maps of the bed , which can be used to determine basal conditions . the sar operates at 80 , 150 and 350 mhz . we also developed a compact , wide-band , dual-mode radar for measuring ice thickness and mapping internal layers in both shallow and deep ice . for ice thickness measurements and mapping layers at depth , it operates over the frequency_range from 50 to 200 mhz , and for fine-resolution mapping of near surface layers it operates over 500 to 2000 mhz [1 , 2] . during the 2004 field season , at summit camp on the greenland ice_sheet , we collected radar data over 3-km lines at 80 , 150 , and 350 mhz with hh polarization . we aquired data along parallel paths offset by 2-10 m to test the feasibility of an interferomet-ric sar to generate basal topography . the preliminary_results demontrate that the ice_bed_interface can be imaged with the sar operating in monostatic mode at incidence angles between 5 and 15 degrees . figure_1_shows sample images collected along two offset passes . we believe that these images are the first and only successful demonstration of imaging the ice_bed_interface through 3-km thick ice . figure 1 : sar images of ice_bed_interface at summit camp . based on the results , we are developing a system that operates over the frequency_range from 100 to 300 mhz to image the ice_bed_interface with 1-10 m resolution . we will be using this sytem to collect data over a_20-km swath between the gisp and grip cores during july 05 . the wide frequency_range and fine resolution will be useful for unambiguous determination of basal conditions . such a system will be useful for identifying frozen or liquid water on mars , and sub-surface characterization of other planets . we also collected data over a_10 km x 10 km grid with the dual-mode radar . the results_demonstrate that we can sound 3-km thick ice and map deep internal layers with about 2 m resolution , and can map near-surface internal-layer echoes to depths of about 150 m with about 15 cm resolution , as shown in figure 2 [3] . figure 2 : radar echogram of near-surface internal layers at sumimit . in this 
diversity in global virtual_teams : a partnership development perspective this dissertation is an attempt to develop and test a comprehensive model for global_virtual team effectiveness based on development of partnership among diverse team members and the moderating role of collaborative_technology and task interdependence . the model will be tested using filed survey methodology . the model is based on traditional i-p-o framework for understanding team effectiveness . team diversity in terms of , surface level , deep_level and functional , and diversity perceptions are treated as the central tenant of team inputs . collaborative partnership quality is at the process level , moderated by task interdependence and use of collaborative_technology as characterized by parallelism , transparency , and sociality . at the outcome level this dissertation is more interested in global_virtual team effectiveness as measured by team performance and individual member satisfaction and the effect of partnership development towards relational conflict . 
an empirical_evaluation of four algorithms for multi_class classification : mart , abc-mart , robust logitboost , and abc_logitboost this empirical study is mainly devoted to comparing four tree_based boosting algorithms : mart , abc-mart , robust logitboost , and abc_logitboost , for multi_class classification on a variety of publicly available datasets . some of those datasets have been thoroughly tested in prior studies using a broad range of classification algorithms including svm , neural_nets , and deep_learning . in terms of the empirical classification errors , our experiment results_demonstrate : 1 . abc-mart considerably improves mart . 2 . abc_logitboost considerably improves ( robust ) logitboost . 3 . ( robust ) logitboost considerably improves mart on most datasets . 4 . abc_logitboost considerably improves abc-mart on most datasets . 5 . these four boosting algorithms ( especially abc_logitboost ) outperform svm on many datasets . 6 . compared to the best deep_learning methods , these four boosting algorithms ( especially abc_logitboost ) are competitive . 
in search of the optimum test_set - adaptive_test methods for maximum defect_coverage and lowest test_cost maintaining product quality at reasonable test_cost in very deep sub-micron process has become a major challenge especially due to multiple manufacturing locations with varying defect and parametric distributions . increasing vector counts and binary_search routines are now necessary for subtle defect screening . in addition , parametric tests and at-spec testing is still often necessary to ensure customer quality . systematic defects are becoming more common and threaten to dominate the yield pareto . adaptive_test methods are introduced in this paper that demonstrate the capability of increasing or decreasing the test_coverage based on the predicted or measured defect and parametric behavior of the silicon being tested . results promise an increase in product quality at the same time a reduction in test costs . 1 . introduction abraham lincoln said " you may deceive all the people part of the time , and part of the people all the time , but not all the people all the time . " in the quest for identifying the most optimum test_set for defect and performance based testing an appropriate adaptation of that famous quote could be : " you can test for all of the defects part of the time , part of the defects all of the time but you cannot test for all the defects all of the time ! " this sums up the test cost vs . test_quality trade_off problem . the cost of applying all the " necessary " test_vectors , temperatures , voltages , fmax and minvdd ( with binary searches ) , iddq , functional and at-speed patterns , path_delay tests , i/o tests and diagnostic patterns is excessive and certainly not in line with our profitability targets . however , the behavior of defects in very deep sub-micron ( vdsm ) processes is systematic and often design dependant in nature [1] . this is increasing the need to adapt new test_methods to supplement the traditional stuck-at tests . also , consider the fact that if an ic is defect_free and meets all of the performance requirements , it does not need to be tested at all ! the same issue applies for burn-in where often 100% of the ics are burned in even though a very small percentage of the ics have latent defects . the burn-in of defect_free die actually reduces the wear out lifetime of the ic , increasing danger of damage due to handling , thermal_runaway or overstress . the obvious problem is that we do not know , in advance , which ics are defect_free and which ones meet 
simple design_tools first-year graduate design students_learn to quickly evaluate complicated cost-performance processor trade_offs using simple the world sent thousands of e-mails to our group at stanford_university inquiring about the " secret " performance report of the amd k7 and intel coppermine chips . according to the register web_site in the united_kingdom , there were rumors that some students at stanford_university had tested and compared the two chips using spec's test_suite . at that time , the amd k7 ( now known as athlon ) and the intel coppermine ( now known as pentium_iii ) were not available to the general public , and it is easy to understand the excitement about this news . the real story was simpler . the students in our processor design class did not test the real chips . they estimated the cost and performance of the two chips as part of a case study . this study was based on information available from various public sources . 3 as the chip implementation_details had not been released to the public , the instructors assumed the hardware designs were similar to the simulator default configurations . after comparing the performance and the costs , the students used the simulator tools to design improvements to each chip . designing deep_submicron microprocessors is becoming an increasingly tedious and complicated process . 4 it takes many years and many engineers to design a commercial processor chip . in an introductory computer architecture course , students are usually required to simulate a simplified pipelined microprocessor such as dlx 5 using some hardware_description_language ( typically verilog or vhdl ) or some graphical tool ( such as hase ) . 6 while it is important for a student to understand how a basic processor operates , students may not fully appreciate the complexity and the various trade_offs_involved_in designing a processor in the commercial environment . students cannot afford to write tens of thousands of lines of code to model processor microarchitecture; it is unproductive to ask them to deal with the myriad details at this stage . instead , we focus on high_level issues involving cost ( area ) and performance ( execution time ) . the main_issues are cache size , cycle time , floating_point_unit ( fpu ) area , latencies , branch strategy , and issue width . by emphasizing a few primary high_level issues , students gain a better understanding of the trade_offs_involved_in overall computer architecture design . table 1 lists the architectural design_tools available in our class . students use these tools in conjunction with the class textbook , 7 and they are also available for other researchers and
a common neonatal image_phenotype predicts adverse neurodevelopmental outcome in children born preterm diffuse_white_matter_injury is common in preterm_infants and is a candidate substrate for later cognitive impairment . this injury pattern is associated with morphological changes in deep grey nuclei , the localization of which is uncertain . we test the hypotheses that diffuse_white_matter_injury is associated with discrete focal tissue loss , and that this image_phenotype is associated with impairment at 2years . we acquired magnetic_resonance_images from 80 preterm_infants_at term_equivalent ( mean gestational_age 29 ( +6 ) weeks ) and 20 control infants ( mean ga 39 ( +2 ) weeks ) . diffuse_white_matter_injury was defined by abnormal apparent diffusion coefficient values in one or more white_matter region ( frontal , central or posterior white_matter at the level of the centrum semiovale ) , and morphological difference between groups was calculated from 3d images using deformation based morphometry . neurodevelopmental assessments were obtained from preterm_infants_at a mean chronological age of 27 . 5months , and from controls at a mean age of 31 . 1months . we identified a common image_phenotype in 66 of 80 preterm_infants_at term_equivalent comprising : diffuse_white_matter_injury; and tissue volume reduction in the dorsomedial nucleus of the thalamus , the globus pallidus , periventricular white_matter , the corona radiata and within the central region of the centrum semiovale ( t=4 . 42 p<0 . 001 false_discovery_rate corrected ) . the abnormal image_phenotype is associated with reduced median developmental quotient ( dq ) at 2years ( dq=92 ) compared with control infants ( dq=112 ) , p<0 . 001 . these findings indicate that specific neural systems are susceptible to maldevelopment after preterm_birth , and suggest that neonatal image_phenotype may serve as a useful biomarker for studying mechanisms of injury and the effect of putative therapeutic interventions . 
using svg and xslt for graphic representation biography andres baravalle is a full-time ph . d . student at turin university , department of informatics . his actual research topics are artificial_intelligence , human_computer_interaction , usability . he took his degree summa cum laude in communications at turin university . his degree thesis discusses about different technologies for information storage ( databases , xml etc . ) and about languages and techniques for multimodal web interaction with desktop and mobile users . economical aspects concerning implementation are also covered . among his key qualifications he lists a deep knowledge of the main languages and technologies related to web_development . he worked at several projects as consultant . among them he worked to develop multimodal interaction systems based on xml and server side technologies for the web_sites costameno . it and loescher . it . publications "remote web usability_testing : a proxy approach" , to be published at hci2003 , creete . "remote web usability_testing" , measuring behaviour 2002 , amsterdam . "a usable web for long-stay hospitalised children" , hci2002 , london . biography vitaveska lanfranchi is a full-time ph . d . student at turin university , department of informatics . her actual research topics are artificial_intelligence , human_computer_interaction , usability . she took his degree summa cum laude in communications at turin university . her degree thesis discusses about different languages and 08/05/2004 techniques for multimodal web interaction with desktop and mobile users . among her key qualifications she lists a deep knowledge of the main languages and technologies related to semantic_web . she worked at several projects as consultant . among them she worked to develop multimodal interaction systems based on xml and server side technologies for the web_sites costameno . it and loescher . it . selected publications : "remote web usability_testing : a proxy approach" , to be published at hci2003 , creete; "remote web usability_testing" , measuring behaviour 2002 , amsterdam : "a usable web for long-stay hospitalised children" , hci2002 , london . 
stratified prototype selection based on a steady_state memetic algorithm : a study of scalability prototype selection ( ps ) is a suitable data reduction process for refining the training_set of a data_mining algorithm . performing ps processes over existing datasets can sometimes be an inefficient task , especially as the size of the problem increases . however , in recent years some techniques have been developed to avoid the drawbacks that appeared due to the lack of scalability of the classical ps approaches . one of these techniques is known as stratification . in this study , we test the combination of stratification with a previously_published steady_state memetic algorithm for ps in various problems , ranging from 50 , 000 to more than 1 million instances . we perform a comparison with some well-known ps methods , and make a deep study of the effects of strati-fication in the behavior of the selected method , focused on its time complexity , accuracy and convergence capabilities . furthermore , the trade_off between accuracy and efficiency of the proposed combination is analyzed , concluding that it is a very suitable option to perform ps tasks when the size of the problem exceeds the capabilities of the classical ps methods . 
multimodality imaging of the peripheral venous system the purpose of this article is to review the spectrum of image_based diagnostic tools used in the investigation of suspected deep_vein_thrombosis ( dvt ) . summary of the experience gained by the author as well as relevant publications , regarding vein imaging_modalities taken from a computerized database , was reviewed . the imaging_modalities reviewed include phlebography , color doppler duplex ultrasonography ( cddus ) , computerized tomography angiography ( cta ) and venography ( ctv ) , magnetic_resonance venography ( mrv ) , and radionuclide venography ( rnv ) . cddus is recommended as the modality of choice for the diagnosis of dvt . a strategy combining clinical score and d_dimer test refines the selection of patients . phlebography is reserved for discrepant noninvasive studies . 
rsw-seq : algorithm for detection of copy_number alterations in deep_sequencing data background recent advances in sequencing technologies have enabled generation of large_scale genome_sequencing data . these data can be used to characterize a variety of genomic features , including the dna copy_number profile of a cancer genome . a robust and reliable method for screening chromosomal alterations would allow a detailed characterization of the cancer genome with unprecedented accuracy . results we develop a method for identification of copy_number alterations in a tumor genome compared to its matched control , based on application of smith-waterman algorithm to single-end sequencing data . in a performance test with simulated data , our algorithm shows >90% sensitivity and >90% precision in detecting a single copy_number change that contains approximately 500 reads for the normal sample . with 100-bp reads , this corresponds to a ~50 kb region for 1x genome coverage of the human_genome . we further refine the algorithm to develop rsw-seq , ( recursive smith-waterman-seq ) to identify alterations in a complex configuration , which are commonly observed in the human cancer genome . to validate our approach , we compare our algorithm with an existing algorithm using simulated and publicly available datasets . we also compare the sequencing-based profiles to microarray-based results . conclusion we propose rsw-seq as an efficient method for detecting copy_number changes in the tumor genome . 
a linux-based real-time operating system this work describes the design , implementation , and possible applications of real-time linux | a hard real-time version of the linux operating system . in this system , a standard time_sharing os and a real-time executive run on the same computer . interrupt controller emulation is used to guarantee a low maximum interrupt latency independently of the base system . the use of a one-shot timer makes it possible to achieve a low task release jitter without compromising throughput . lock-free fifo buuers are employed for communication between real-time tasks and linux processes . user-deened schedulers are allowed as are run_time changes in the scheduling policy . the system is in active use for real-time data_acquisition , control , and communications . acknowledgements i would like to express my deep appreciation to my advisor , victor yodaiken , for his support , encouragement , and friendship . his original ideas , comments_and_suggestions were invaluable . his help was essential in bringing this thesis to completion . i also wish to thank victor for his patience in answering my numerous questions . i thank other members of my committee : dr . lassez and dr . mazumdar , for their time and eeort in reviewing my thesis . i found dr . lassez's comments on my presentation style to be immensely helpful . i am grateful to dr . mazumdar for encouraging me to perform more substantial testing of the system . thanks to yuri g . karpov , my academic advisor in russia , for giving me an opportunity to study in the us and helping me deal with many important matters . last , but not least , i would like to thank olga tomina for her patience and understanding , and my parents , alexander and irina , for everything good they have done for me . 
mixed plb and interconnect bist for fpgas without fault_free assumptions we tackle the problem of fault_free assumptions in current plb and interconnect built-in-self-test ( bist ) techniques for fpgas . these assumptions were made in order to develop strong bist methods for one class of components ( plbs or interconnects ) while assuming that the other class is fault_free . this results in a cyclical conundrum that renders current plb and interconnect bist_techniques impractical , since current deep_submicron fpgas as well as those of emerging single-digit nanometer_technologies are expected to have a profusion of hard ( permanent ) plb as well as interconnect faults . we address this issue here and develop a novel method m-bist that uses a combination of ( i ) iterative bootstrapping that without any knowledge of the state of any plb or interconnect determines a minimum contingent of fault_free test circuit components with high probability , and ( ii ) mixed testing of plbs and interconnects in an interleaved manner that identifies fault_free components that are used in subsequent testing phases until the entire fpga is tested . this approach is overlaid on current_state-of-the-art plb and interconnect bist_techniques . simulation_results obtained for faults present in both plbs and interconnects show significant improvements in both fault coverage and false_positives yielded by m-bist compared to the plb-only and interconnect-only bist_techniques used within the m-bist wrapper that make fault_free assumptions about the other component type . 
glaucoma surgery : taking the sub-conjunctival route we are currently in the midst of a surge in interest in glaucoma surgery . novel pathways for reducing intraocular_pressure ( iop ) have been tried with various levels of success over the last few years . while the trabecular bypass and suprachoroidal approaches have captured much of the attention , filtering aqueous into the sub-conjunctival space remains the gold_standard for lowering iop . this review attempts to focus on current research in surgical methods to enhance filtration by potentially improving on tried and tested methods like the trabeculectomy , deep sclerectomy , and tube surgeries . 
overview of the nasa/jpl lasercom program the nasa-funded opticai communications program being conducted at jpl is described , the spacecraft transceiver terminal developments , a test infrastructure for assessing the performance of future space terminals , an atmospheric visibility monitoring program , some recent systems-level demonstrations , and a communications technology roadmap for planetary missions over the next 25 years are presented . 1 . introduction communications demands for spacecraft are ever-increasing and the technology required to satisfy those link demands often dominates the archkecture of the spacecraft structures . this has been true for some time on military and nasa missions , and more recently has become a driver for many proposed commercial satellite networks , accordingly , nasa has been developing optical_communications technology so that titure missions can satisfy those demands with much less impact on the space platforms , or the launch vehicles required to lift them off the earth's surface . studies , technology_development , systems_design and deployment planning for this technology have been underway at nasa's jet_propulsion_laboratory for the past 18 years [1 , 2] . in this paper the optical_communications space terminai technology being developed to address these applications will be described . next , the development of a test stand to evaluate this such spacecraft terminals is described . following this , , a program to gather detailed statistics on the cloud-cover outages for space-to-ground links will be described , including the data distributions that have been produced from those data . finally , a roadmap for how thk technology can augment or enable deep-space_missions of the future will be described . 2 , optical spacecraft terminal development the centerpiece of the nasa spacecraft technology_development is the opticai communications demonstrator ( ocd ) program [3] . this program is developing an engineering model of a flight terminal capable of returning kbps to mbps from the planets , // or mbps-bps from high-earth_orbit to the ground . the system uses a " minimum-complexity " architecture that uses only one detector array and one fine steering mirror to accomplish beacon signal acquisition , tracking , transmit beam pointing , and transmit/receive co-alignment ( with point-ahead to accommodate cross velocity ) . tracking of the beacon signal is accomplished by using a windowed sub-frame readout from the detector array . initial development of the concept involved setting up a tracking system breadboard in the laboratory . this was followed by a contract with 20/20 systems inc . to package the ccd/camera readout electronics , and the tracking processor assembly ( tpa ) , which determines , filters and conditions the tracking error signals for actuation of the steering mirror ( and 
voxel-based analysis derived from fractional anisotropy images of white_matter volume changes with aging although age-related effects on brain volume have been extensively investigated post mortem and in vivo using magnetic_resonance_imaging ( mri ) , regional and temporal patterns of white_matter ( wm ) volume changes with aging are not defined yet . the aim of this study was to assess the topographical distribution of age-related wm volume changes using a recently_developed voxel-based_method to obtain estimates of wm fiber_bundle volumes using diffusion tensor ( dt ) mri . brain conventional and dt mri were obtained from 84 healthy subjects ( mean age=44 years , range=13-70 ) . linear and non-linear relationships between age and wm fiber_bundle volume changes were tested . a negative linear correlation was found between age and wm volume decline in the corona radiata , anterior cingulum , body and crus of the fornix and left superior cerebellar peduncle . a positive linear correlation was found between age and volume increase of the right deep temporal association fibers . the non-linear_regression analysis also showed age-related changes of the genu of the corpus_callosum and fitted better the volume changes of the right deep temporal association fibers . wm volume decline with age is unevenly distributed across brain regions . our approach holds promise to gain additional_information on the pathological changes associated to neurological_disorders of the elderly . 
performance analysis of adsl this study will present a deep research regarding the characteristics of adsl in term of its advantages_and_disadvantages . the performance_analysis concentrates on the impact of the services . tests are being carried out to prove that the performance is affected by loop length and also noise issue as well as their impairment issues especially crosstalk . crosstalk modeling is performed to discuss the performance of adsl with the supports of a proposed statistical methodology . at the end of the study , a comparison of adsl with other copper using fixed line broadband communication system , cable_modem is discussed . 
injecting various faults for the dependability validation of commercial microcontrollers faults will be a great challenge in modern vlsi_circuits . different faults are injected into the vhdl model of a commercial microcontroller and their effects have been compared . bridging fault and stuck_at_faults are injected . the methodology used is vhdl based fault_injection technique , which allows an exhaustive analysis of the influence of different fault_models and system parameters . fault_models are used at logic and rtl levels . from the simulation result , the occurrences of failures and have been found out . fault coverage and test_coverage are found out using dft compiler and tetramax tool . i . introduction faults will have a great impact in deep submicron technologies . fault_models used are stuck_at_fault , bridging fault , delay_fault . a stuck_at_fault is a particular fault_model used by fault simulators and automatic_test_pattern_generation ( atpg ) tools to mimic a manufacturing defect within an integrated_circuit . individual signals and pins are assumed to be stuck at logical '1' , '0' and 'x . the fault can be at an input or output of a gate . the stuck_at_fault model assumes that only one input on one gate will be faulty at a time , assuming that if more are faulty , a test that can detect any single fault , should easily find multiple faults . to use this fault_model , each input pin on each gate in turn , is assumed to be grounded , and a test_vector is developed to indicate the circuit is faulty . two signals are connected together when they should not be . depending on the logic circuitry employed , this may result in a wired-or or wired-and logic function . in order to study the impact of various faults , a methodology based on fault_injection is used . fault_injection technique allows a controlled introduction of faults in the system , not being necessary to wait for a long time to log the apparition of real faults . vhdl based fault_injection can be a very suitable option due to its flexibility as well as the high observability and controllability of all the model components . ii . fault_injection experiments . fault_injection experiments were carried out on the vhdl model of 8051 microcontroller . stuck_at_fault , , bridging_faults are injected into the ram and psw register of 8051 microcontroller . a stuck_at_fault is a particular fault_model used by fault simulators and automatic_test_pattern_generation ( atpg ) tools to mimic a manufacturing defect within an integrated_circuit . individual signals and 
atpg for crosstalk using hybrid structural sat as technology evolves into the deep sub-micron era , signal_integrity problems are growing into a major challenge . an important source of signal_integrity problems is the crosstalk_noise generated by coupling capacitances between wires . test_vectors that activate and propagate crosstalk_noise effects are becoming an essential part of design_verification and manufacturing_test . however , deriving such vectors is a complex task . in this paper , we propose hyac , a fast yet accurate hybrid atpg method targeting multiple-aggressor induced crosstalk errors . given a victim and a set of aggressors , the proposed atpg method searches for test_vectors to activate and propagate a crosstalk error for the victim . due to logic constraints , it may not be possible to trigger all aggressors simultaneously . therefore , firstly we use an implication graph ( ig ) that consists of logic variables and structural information to check for logic conflicts . if the current set of aggressors is not feasible , our algorithm automatically searches for the next-best subset of aggressors ( resulting in the largest noise ) . after a set of feasible aggressors is identified , we use a modified podem [21] algorithm to search for test_vectors . this hybrid structural sat-based atpg method inherits advantages from both boolean satisfiabilitity based_methods and structural-based_methods to achieve flexibility and efficiency . we demonstrate the accuracy , high_quality , and run_time efficiency of hyac through experiments conducted on several benchmark_circuits as well as a circuit from a commercial processor . 
syntactic discriminative language_model rerankers for statistical_machine_translation this article describes a method that successfully exploits syntactic_features for n-best translation candidate reranking using perceptrons . we motivate the utility of syntax by demonstrating the superior performance of parsers over n_gram language models in differentiating between statistical_machine_translation output and human translations . our approach uses discriminative language modelling to rerank the n-best translations generated by a statistical_machine_translation system . the performance is evaluated for arabic-to-english translation using nist's mt-eval benchmarks . while deep features extracted from parse_trees do not consistently help , we show how features extracted from a shallow part-of-speech annotation layer outper-form a competitive baseline and a state-of-the-art comparative reranking approach , leading to significant bleu improvements on three different test_sets . 
stacked extreme_learning machines extreme_learning machine ( elm ) has recently attracted many researchers' interest due to its very fast learning speed , good generalization_ability , and ease of implementation . it provides a unified solution that can be used directly to solve regression , binary , and multiclass classification_problems . in this paper , we propose a stacked elms ( s-elms ) that is specially designed for solving large and complex data problems . the s-elms divides a single large elm network into multiple stacked small elms which are serially connected . the s-elms can approximate a very large elm network with small memory requirement . to further improve the testing accuracy on big_data problems , the elm autoencoder can be implemented during each iteration of the s-elms algorithm . the simulation results show that the s-elms even with random hidden nodes can achieve similar testing accuracy to support_vector_machine ( svm ) while having low memory requirements . with the help of elm autoencoder , the s-elms can achieve much better testing accuracy than svm and slightly better accuracy than deep_belief_network ( dbn ) with much faster training speed . 
studying the characteristics of a "good" gui test_suite the widespread deployment of graphical_user_interfaces ( guis ) has increased the overall complexity of testing . a gui_test designer needs to perform the daunting task of adequately testing the gui , which typically has very large input interaction spaces , while considering tradeoffs between gui test_suite characteristics such as the number of test_cases ( each modeled as a sequence of events ) , their lengths , and the event composition of each test_case . there are no published empirical studies on gui testing that a gui_test designer may reference to make decisions about these characteristics . consequently , in practice , very few gui testers know how to design their test_suites . this paper takes the first step towards assisting in gui_test design by presenting an empirical study that evaluates the effect of these characteristics on testing cost and fault_detection effectiveness . the results show that two factors significantly effect the fault_detection effectiveness of a test suite : ( 1 ) the diversity of states in which an event executes and ( 2 ) the event coverage of the suite . test designers need to improve the diversity of states in which each event executes by developing a large number of short test_cases to detect the majority of " shallow " faults , which are artifacts of modern gui design . additional resources should be used to develop a small number of long test_cases to detect a small number of " deep " faults . 
the uot system : improve string-to-tree translation using head_driven_phrase_structure_grammar and predicate_argument structures we present the uot machine_translation system that was used in the iwslt-09 evaluation campaign . this year , we participated in the btec track for chinese-to-english translation . our system is based on a string-to-tree framework . to integrate deep syntactic_information , we propose the use of parse_trees and semantic dependencies on english sentences described respectively by head_driven_phrase_structure_grammar and predicate_argument structures . we report the results of our system on both the development and test_sets . 
compensation of deep_drawing tools for springback and tool-deformation manual tool reworking is one of the most time-consuming stages in the preparation of a deep_drawing process . finite elements ( fe ) analyses are now widely applied to test the feasibility of the forming process , and with the increasing accuracy of the results , even the springback of a blank can be predicted . in this paper , the results of an fe analysis are used to carry out tool compensation for both springback and tool/press deformations . especially when high-strength steels are used , or when large body panels are produced , tool compensation in the digital_domain helps to reduce work and save time in the press workshop . a successful compensation depends on accurate and efficient fe-prediction , as well as a flexible and process-oriented compensation algorithm . this paper is divided in two sections . the first section deals with efficient modeling of tool/press deformations , but does not discuss compensation . the second section is focused on springback , but here the focus is on the compensation algorithm instead of the springback phenomenon itself . 
systematicity as a selection constraint in analogical mapping analogy is often viewed as a partial similarity match between domains . but not all partial similarities qualify as analogy : there must be some selection of which commonalities count . three experiments tested a particular selection constraint i n analogical mapping , namely , systemoticity . that is , we tested whether a given predicate is more likely to figure i n the interpretation of and prediction from an analogy if the predicate participates i n a common system of relations . in experiment 1 , subjects iudged two matches to be i ncl uded i n an analogy : an isolated match , and a match embedded i n a larger matching system . subjects preferred the embedded match . in experiments 2 and 3 , subjects made analogical predictions about a target domain . subjects predicted information that followed from a causal system that matched the base domain , rather than information that was equally plausible , but that created an isolated match with the base . results support gentner's ( 1983 , 1989 ) structure-mapping theory i n that analogical mapping concerns systems and not i ndi vi dual predicates , and that attention to shared systematic structure constrains the selection of information to i ncl ude i n on analogy . in an analogy , a familiar domain is used to understand a novel domain in order to highlight important similarities between the domains , or to predict new features of the novel domain . for example , we use our knowledge about water flow to elucidate properties of electric circuitry . such an analogy can lead to useful inferences , and reveal deep structural features about a domain . in this research we ask how an analogical mapping is constructed . in particular , we ask whether systematic relational structure acts as a psychological selection constraint in interpreting an analogy . an analogy can be seen as a partial similarity match between situations . but not all partial similarity matches qualify as analogy : there must be some selection of which commonalities count . the selection problem exists
built-in self-test for signal_integrity unacceptable loss of signal_integrity may harm the functionality of socs permanently or intermittently . we propose a systematic approach to model and test signal_integrity in deep_submicron high_speed interconnects . various signal_integrity problems occurring on such interconnects ( e . g . crosstalk , overshoot , noise , skew , etc . ) are considered in a unified model . we also present a test_methodology that uses a noise detection circuitry to detect low integrity signals and an inexpensive test architecture to measure and read the statistics for final observation and analysis . 
understanding deep_learning requires rethinking generalization despite their massive size , successful deep artificial_neural_networks can exhibit a remarkably small difference between training_and_test performance . conventional_wisdom attributes small generalization error either to properties of the model family , or to the regularization techniques used during training . through extensive systematic experiments , we show how these traditional_approaches fail to explain why large neural_networks generalize well in practice . specifically , our experiments establish that state-of-the-art convolutional networks for image_classification trained with stochastic gradient methods easily fit a random labeling of the training data . this phenomenon is qualitatively unaffected by explicit regularization , and occurs even if we replace the true images by completely unstructured random noise . we corroborate these experimental findings with a theoretical construction showing that simple depth two neural_networks already have perfect finite_sample expressivity as soon as the number of parameters exceeds the number of data_points as it usually does in practice . we interpret our experimental findings by comparison with traditional models . 
verifying distributed protocols using msc-assertions , run_time monitoring , and automatic test_generation 1 the research reported in this article was funded in part by a grant from the u . s . missile_defense_agency . the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements , either expressed or implied , of the u . s . government . the u . s . government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright annotations thereon . abstract this paper addresses the need for formal_specification and runtime verification of system-level requirements of distributed reactive systems . it describes a formalism for specifying global system behaviors in terms of message sequence chart assertions and a technique for the evaluation of the likelihood of success of a distributed protocol under non-trivial communication conditions via discrete_event_simulation and runtime execution monitoring . we constructed a proof-of-concept prototype for the leader-election algorithm within a 4-node ring network . the prototype consists of the following components : ( i ) an omnet++ model of the network using non-trivial communication conditions , ( ii ) c++ code for the network agents , ( iii ) a system-level assertion stipulating the formal requirement for a correct , time-bound , leader election , ( iv ) simulation of the formal assertion , ( v ) automatic scenario generation , and ( vi ) run_time monitoring of the formal assertion and stochastic-based estimation of the likelihood of success of this assertion under the specified communication conditions . 1 introduction the design and implementation of reliable applications on top of asynchronous distributed_systems that are prone to processor and network crashes is a difficult and complex task . a distributed system is made up of several components , executing concurrently and interacting with each other under the control of specialized procedures called protocols . individual components usually do not have real-time knowledge of the global state of the system , and it may not even have the notion of a global clock . moreover , whenever the application departs from its correct " state " due to processor crashes , the live processors must execute some algorithms ( i . e . protocols ) to restore the application back to the correct state . runtime execution monitoring ( rem ) is a class of methods for tracking the temporal behavior of an underlying application . rem methods range from simple print statement logging methods to run_time tracking of complete formal requirements for verification purposes . nasa used rem to verify the flight code for its deep impact project [5] . a recent paper by the authors describes run_time verification of the cara infusion pump using uml-statechart 
advanced deepwater monitoring system this study investigates new methods to improve deepwater monitoring and addresses installation of advanced sensors on " already deployed " risers , flowlines , trees , and other deepwater devices . a major shortcoming of post installed monitoring systems in subsea is poor coupling between the sensor and structure . this study provided methods to overcome this problem . both field_testing in subsea environments and laboratory testing were performed . test articles included actual flowline pipe and steel catenary risers up to twenty-four inches in diameter . a monitoring device resulting from this study can be installed in-situ on underwater structures and could enhance productivity and improve safety of offshore operations . this paper details the test_results to determine coupling methods for attaching fiber_optic sensor systems to deepwater structures that have already been deployed . subsea attachment methods were evaluated in a forty foot deep pool by divers . afterword , structural testing was conducted on the systems at the nasa johnson_space_center . additionally a 7 , 000 foot deep sensor station was attached to a flowline with the aid of a remote operated vehicle . various sensor to pipe coupling methods were tested to measure tensile load , shear_strength and coupling capability . several adhesive bonding methods in a subsea environment were investigated and subsea testing yielded exceptionally good results . tensile and shear properties of subsea application were approximately 80 percent of those values obtained in dry conditions . additionally , a carbide alloy coating was found to increase the shear_strength of metal to metal clamping interface by up to 46 percent . this study provides valuable data for assessing the feasibility of developing the next generation fiber_optic sensor system that could be retrofitted onto existing subsea pipeline structures . 
stochastic analysis of interconnect performance in the presence of process_variations deformations in interconnect due to process_variations can lead to significant performance degradation in deep sub-micron circuits . timing analyzers attempt to capture the effects of variation on delay with simplified models . the timing verification of rc or rlc networks requires the substitution of such simplified models with spatial stochastic_processes that capture the random nature of process_variations . the present work proposes a new and viable method to compute the stochastic response of interconnects . the technique models the stochastic response in an infinite dimensional hilbert_space in terms of orthogonal polynomial expansions . a finite representation is obtained by using the galerkin approach of minimizing the hilbert_space norm of the residual error . the key advance of the proposed method is that it provides a functional representation of the response of the system in terms of the random_variables that represent the process_variations . the proposed algorithm has been implemented in a procedure called opera . results from opera simulations on commercial design test_cases match well with those from the classical monte_carlo spice simulations and from perturbation methods . additionally opera shows good computational efficiency : speedup factor of 60 has been observed over monte_carlo spice simulations . introduction the performance of integrated_circuits ( ics ) is increasingly less predictable as device dimensions shrink below the sub-100 nanometer scale . the modeling accuracy problem stems from poor control of the physical device and interconnect characteristics during the manufacturing_process . uncertainties due to variations in the manufacturing_process are reflected in variations in the circuit_parameters . examples of manufacturing variations are the variations in materials , variations in geometry ( t ox l eff , w ) and doping profiles of mosfets , material and geometric_variations of the interconnects etc . the many sources of variations in the ic fabrication_process lead to a hierarchy of random and systematic effects on circuit_performance [16] . a common way of accounting for process_variations is to use a linear_model to represent a circuit parameter . for instance , a parameter p would be expressed as p = p + 1 , p + 2 , p , where p is a ( nominal ) mean value , 1 , p and 2 , p are random_variables with mean zero and variances 1 , p and 2 , p . these represent the inter-die and intra_die variations , respectively . designers interested in performance_analysis and optimization typically use only a single value of p . the performance of devices and interconnects depends on several parameters 
space_communication channel emulation using digital_and_analog signal_processing title : space_communication channel emulation using digital_and_analog signal_processing space_communication channel emulation using digital_and_analog signal_processing new communication protocols intended for large distances , including low orbit and deep_space , can be inherently difficult to evaluate since trial implementations are often impractical . in order to accurately measure the performance of a new protocol , it is important to evaluate it in an environment that most closely matches that in which it will be used . this thesis demonstrates the ability to emulate a space communications channel through digitizing a transmission centered at an intermediate_frequency of 70 mhz with a bandwidth of 24 mhz , digitally introducing the characteristics of a transmission through space , and reconstructing the digital data to its analog counterpart . delay , doppler_shift , gaussian_noise , and fading are among the most prevalent characteristics of such a channel , and thus were the focus of this thesis . special care was given to the design of each digital_and_analog component to maintain the integrity of the original signal by minimizing all undesired noise introduced . the final design can accurately produce a given dynamic transmission signature or continually output a static set of channel characteristic parameters to test new communication protocols . 
natural_language for human_robot_interaction 2 . embodied construction_grammar 3 . system architecture natural_language_understanding ( nlu ) was one of the main original goals of artificial_intelligence and cognitive_science . this has proven to be extremely challenging and was nearly abandoned for decades . we describe an implemented system that supports full nlu for tasks of moderate complexity . the natural_language interface is based on embodied construction_grammar and simulation semantics . the system described here supports human dialog with an agent controlling a simulated robot , but is flexible with respect to both input language and output task . interfaces natural_language . natural_language_interfaces have long been a topic of hri research . winograd's 1971 shrdlu was a landmark program that allowed a user to command a simulated arm and to ask about the state of the block world ( winograd , 1971 ) . there is currently intense interest in both the promise and potential dangers of much more capable robots . 1 ) much more computation 2 ) nlp technology 3 ) construction_grammar : form-meaning pairs 4 ) 10 ) general nlu front_end : modest effort to link to a new action side as shown in table 1 , we believe that there have been sufficient scientific and technical advances to now make nlu of moderate scale an achievable goal . the first two points are obvious and general . all of the others except for point 8 are discussed in this paper . the cprm mechanisms were not needed in the current system , but are essential for more complex actions and simulation ( barrett 2010 ) . this work is based on the embodied construction_grammar ( ecg ) , and builds on decades of work on the neural theory of language ( ntl ) project . the meaning side of an ecg construction is a schema based on embodied cognitive_linguistics . ( feldman , dodge , and bryant 2009 ) . ecg is designed to support the following functions : 1 ) a formalism for capturing the shared grammar and beliefs of a language community . 2 ) a precise notation for technical linguistic work 3 ) an implemented specification for grammar testing 4 ) a front_end for applications involving deep semantics 5 ) a high level description for neural and behavioral experiments . 6 ) a basis for theories and models of language learning . in this work , we focus on point 4; we are using ecg for the natural_language interface to a robot simulator . we suggest that nlu can now be the foundation for hri with the current generation of robots of limited complexity . any foreseeable robot will have limited capabilities and will not be able 
metanet : deep_semantic automatic metaphor analysis this paper describes a system that makes use of a repository of formalized frames and metaphors to automatically detect , categorize , and analyze expressions of metaphor in corpora . the output of this system can be used as a basis for making further refinements to the system , as well as supporting deep_semantic analysis of metaphor expressions in corpora . this in turn provides a way to ground and test empirical conceptual_metaphor theory , as well as serving as a means to gain insights into the ways conceptual metaphors are expressed in language . 
a tunneling model for gate oxide failure in deep sub-micron technology parametric failures in cmos ic nanoelectronics , leads to strong detection problem . in order to develop new defect oriented test_methods , it is of prime importance to study the behavior of the transistor affected by those kind of failures . in this paper , we present a new electricaltransistor model , which allows to study the impact of gate oxide thickness drop . it is shown that electrical behavior of the proposed model matches in a satisfactory way the defective transistor behavior . 
double-tree scan : a novel low_power scan_path architecture states at circuit nodes may erroneously change . further , bist schemes with random test_patterns may need an excessive amount of energy because of longer test length . abstract i n a scan_based system with a large number of flip_flops , a major component of power is consumed during scan-shift and clocking operation in test_mode . in this paper , a novel scan_path architecture called double-tree scan ( dts ) is proposed that drastically reduces the scan-shifi and clock activity during testing . the inherent combinatorial properties of double-tree_structure are employed to design the scan architecture , clock gating logic , and a simple shift controller . the design is independent of the structure of the circuit-under-test ( cut ) or its test_set . it provides a significant_reduction both in instantaneous and average power needed for clocking and scan-shifting . the architecture fits well to built-in self-test ( bist ) scheme under random_testing , as well as to deterministic test environment . 1 . introduction with the emergence of mobile_devices , design of low_power vlsi systems has become a major concern in circuit synthesis . a significant component of the power consumed in cmos_circuits is caused by the switching_activity ( sa ) at various circuit nodes during operation . the dynamic power consumed at a circuit node is proportional to the total number of 0 + 1 and 1-+ 0 transitions that the logic signal undergoes at that node multiplied by its capacitance and the frequency of operation . powertenergy minimization during testing has become important in the context of deep_sub_micron_technology because of higher device densities and clock rates . in a scan_based system , a significant amount of power is consumed during the scan operations , as the activity in the scan_path , clock tree , and in the cut becomes very high [ 11 . the average-power_optimization extends the battery life in mobile applications . maximum sustained power over a specified limit , may cause excessive heating of the device , whereas , the instantaneous power may cause excessive ( inductive ) voltage_drop in the power and ground lines because of current swing . thus , the logic 2 . background existing powertenergy minimization techniques include test_scheduling [ 2 ] , toggle suppression and blocking useless patterns [13] , designing low_power tpg for bist applications [8 , 121 , use of golomb coding for scan testing [7] , and power-aware_atpg [14 , 151 . for deterministic testing , power_reduction can be achieved by reordering scan_chains and test_vectors [4] . compaction of test_vectors for low_power in a scan_based system 
deep_brain_stimulation induces bold activation in motor and non-motor networks : an fmri comparison study of stn and en/gpi dbs in large animals the combination of deep_brain_stimulation ( dbs ) and functional mri ( fmri ) is a powerful means of tracing brain circuitry and testing the modulatory effects of electrical stimulation on a neuronal network in vivo . the goal of this study was to trace dbs-induced global neuronal network activation in a large animal_model by monitoring the blood oxygenation level-dependent ( bold ) response on fmri . we conducted dbs in normal anesthetized pigs , targeting the subthalamic nucleus ( stn ) ( n=7 ) and the entopeduncular nucleus ( en ) , the non-primate analog of the primate globus pallidus interna ( n=4 ) . using a normalized functional activation map for group analysis and the application of general linear modeling across subjects , we found that both stn and en/gpi dbs significantly increased bold activation in the ipsilateral sensorimotor network ( fdr<0 . 001 ) . in addition , we found differential , target-specific , non-motor network effects . in each group the activated brain areas showed a distinctive correlation pattern forming a group of network connections . results_suggest that the scope of dbs extends beyond an ablation-like effect and that it may have modulatory effects not only on circuits that facilitate motor function but also on those involved in higher cognitive and emotional processing . taken together , our results show that the swine model for dbs fmri , which conforms to human implanted dbs electrode configurations and human neuroanatomy , may be a useful platform for translational studies investigating the global neuromodulatory effects of dbs . 
active stereovision using invariant visual servoing the objective of this paper is to propose an innovative visual servoing method in order to improve the 3d reconstruction of objects for quantitative measurements . the method uses a stereo_vision system that allows to obtain various shots of an object , at regular intervals according to a predefined trajectory . in our case , the stereo rig is equipped with two different cameras , the first one is fixed while the other one is mounted on pan and tilt . so , the intrinsic parameters are not the same for the two cameras . to validate our approach , first experiments have been conducted by simulation and under laboratory conditions . i . introduction the aim of our research is to develop , implement and test an original robotic method dedicated to compute a metric 3d reconstruction in order to describe and quantify biodiversity in deep_sea fragmented habitats [17] . the objective consists in applying techniques of computer_vision to create tools adapted to the exploitation of underwater images . today , some deep_sea vehicles are equipped with measuring devices and a manipulator arm . vision systems are often used to complete information provided by acoustic sensors [12] . taking advantage of possibilities allowed by underwater_vehicles , our goal is to develop a methodology based on a vision system , to obtain quantitative measurements through a 3d reconstruction of underwater scenes . the images used for the reconstruction are acquired when the vehicle is deployed on the sea_floor at a fixed and stable attitude . the images are subject to several constraints linked to the underwater environment . first of all , the observed scenes are unknown , and the objects to be reconstructed in these scenes are made up of random textures and shapes . we only know that the objects are rigid and have a vertical overall shape . moreover , the refraction , the presence of particles , the absorption and the problems of lighting in an underwater environment considerably alter the image quality . noisy images and an unknown model of the object have a knock-on effect on the 3d reconstruction accuracy . the idea is thus to define a trajectory adapted to the type of object in order to improve the reconstruction accuracy . the originality of the method lies in the use of an active stereo rig ( i . e . with variable geometry ) mounted on a 6 dof manipulator arm effector , which equips the underwater_vehicle . the stereo rig is equipped with two different underwater cameras , the first one is fixed while the 
paper special section on low-leakage , low_voltage , low_power and high_speed technologies for system lsis in deep_submicron_era generalized stochastic collocation method for variation-aware capacitance extraction of interconnects considering arbitrary random probability summary for variation-aware capacitance extraction , stochastic col-location method ( scm ) based on homogeneous chaos expansion has the exponential convergence rate for gaussian geometric_variations , and is considered as the optimal solution using a quadratic model to model the parasitic capacitances . however , when geometric_variations are measured from the real test_chip , they are not necessarily gaussian , which will significantly compromise the exponential convergence property of scm . in order to pursue the exponential convergence , in this paper , a generalized stochas-tic collocation method ( gscm ) based on generalized polynomial chaos ( gpc ) expansion and generalized sparse grid quadrature is proposed for variation-aware capacitance extraction that further considers the arbitrary random probability of real geometric_variations . additionally , a recycling technique based on minimum_spanning_tree ( mst ) structure is proposed to reduce the computation cost at each collocation point , for not only " recycling " the initial value , but also " recycling " the preconditioning matrix . the exponential convergence of the proposed gscm is clearly shown in the numerical results for the geometric_variations with arbitrary random probability . 
neighbor current_ratio ( ncr ) : a new metric for iddq data_analysis i ddq test loses its effectiveness for deep_sub_micron chips since it cannot distinguish between faulty and fault_free currents . the concept of current_ratios , in which the ratio of maximum to minimum i_ddq is used to screen faulty chips , has been previously_proposed . at the wafer_level neighboring chips have similar fault_free properties and are correlated . in this paper , use of spatial_correlation in combination with current_ratios is investigated . by differentiating chips based on their nonconformance to local i_ddq variation , outliers are identified . the analysis of sematech data is presented . 
robust tts duration modelling using dnns accurate modelling and prediction of speech-sound durations is an important component in generating more natural synthetic speech . deep_neural_networks ( dnns ) offer a powerful modelling paradigm , and large , found corpora of natural and expressive speech are easy to acquire for training them . unfortunately , found datasets are seldom subject to the quality_control that traditional synthesis methods expect . common issues likely to affect duration modelling include transcription errors , reductions , filled pauses , and forced-alignment inaccuracies . to combat this , we propose to improve modelling and prediction of speech durations using methods from robust_statistics , which are able to disregard ill-fitting points in the training material . we describe a robust fitting criterion based on the density power divergence ( the-divergence ) and a robust generation heuristic using mixture density networks ( mdns ) . perceptual tests indicate that subjects prefer synthetic speech generated using robust models of duration over the baselines . 
the ties that bind : social_network principles in online_communities in a web_2_._0 environment , the online_community is fundamental to the business_model , and participants in the online_community are often motivated and rewarded by abstract concepts of social_capital . how networks of relationships in online_communities are structured has important implications for how social_capital may be generated , which is critical to both attract and govern the necessary user base to sustain the site . we examine a popular website , slashdot , which uses a system by which users can declare relationships with other users , and also has an embedded reputation system to rank users called 'karma' . we test the relationship between user's karma level and the social_network structure , measured by structural holes , to evaluate the brokerage and closure theories of social_capital development . we find that slashdot users develop deep_networks at lower levels of participation indicating value from closure and that participation intensity helps increase the returns . we conclude with some comments on mechanism_design which would exploit these findings to optimize the social_networks and potentially increase the opportunities for monetization . since the opening of the internet to commercial organizations , there have been many experiments with business_models to capitalize on the features and capabilities of the world_wide_web . some have become mainstream , like online storefronts and auctions , and some seem to be fading , like subscription based newspapers . several of the newest sets of online business opportunities involve " web_2_._0 " concepts , especially those centered on virtual_communities . we use the term web_2_._0 to refer to websites which provide content that gets richer as more people use the website , harnessing " the power of user contribution , collective_intelligence , and network effects " [24 , 25] . the most prominent of these sites include youtube , myspace , wikipedia , facebook and orkut , which together encompass a sizable share of the world's most popular sites ( #3 , 6 , 7 , 8 and 11 respectively by ranking of global traffic ( alexa . com july 2008 ) . clearly , this form of online organization is creating a large impact in the business community . however , the business principles behind websites based around social_networks are not well-understood . this paper seeks to illuminate a part of this area of study . online organizations face unique challenges managing their organizational interests given their customer base is physically distant , psychologically unknown and literally faceless . this becomes more critical as businesses move from a model of simple transactional online functionality to one in 
impedance profile of a commercial power_grid and test system an impedance profile of a commercial power_grid and a tester power_distribution system is developed in this paper . the profile is used to identify the measurable frequency_range of the power supply transient_signals generated by a chip . several resistance-capacitance ( rc ) models of the power_grid are analyzed to determine the impact of each capacitance type . the impedance profile of a c4-based production testing environment is then developed . the impedance profile of the combined probe_card and the power_grid rc models illustrates the range of frequencies that are measurable at the supply ports of the chip-under-test ( cut ) . the results suggest that it is possible to measure the important frequency components of a chip's power_supply transients in a production test environment for use in fault detection and localization procedures . conventional testing_methods are challenged by changing circuit sensitivities and emerging defect mechanisms resulting from the use of new fabrication materials in very deep submicron processes [1] . for example , the change from a subtractive aluminum process to damascene cu may lead to more particle-related blocked-etch resistive_opens . technology scaling also increases the probability of resis-tive vias caused by incomplete etch . the additional delays introduced by these types of resistive defects in combination with increased circuit sensitivity due to shorter clock cycles , reduced timing slack , crosstalk and pwr/gnd bounce increase the likelihood of random defects causing delay fails . similarly , hardware_based fault localization is challenged by increases in chip complexity as well as additional interconnection levels and the limitations on the spatial resolution of imaging technology . the increase in difficulty and cost of performing hardware physical failure_analysis is likely to move it into a sampling/verification role . these trends continue to increase the importance of developing alternative software-based fault localization procedures . we believe that power_supply testing_methods are well aligned with these needs and others as described in the international_technology_roadmap_for_semiconductors . in our previous work , a testing method is presented for fault_detection that uses correlation analysis of multiple simultaneously measured power_supply transient_signals [2] . the transients at each of the supply ports of a chip-under-test ( cut ) are cross-correlated to reduce the adverse effects of process_variations on fault_detection resolution . the multiple supply port measurements are analyzed for the regional signal anomalies introduced by defects . the regression_analysis technique that we propose in [3] is able to detect anomalies in the ratios of the waveform 
differential expression analysis of digital gene_expression_data : rna-tag filtering , comparison of t-type tests and their genome_wide co-expression based adjustments deep_sequencing techniques have shown a promising impact on biomedical studies . based on a recently published two-sample digital gene_expression ( dge ) data_set , we compared three widely used t-type tests for the differential expression analysis . both the 'soft' and 'hard' filtering strategies were considered . for the 'hard' filtering strategy , we also considered a genome_wide co-expression based adjustment for each t-type test . our results_suggest that excluding rna-tags at an appropriate level of data variability can improve the control of false_positives . furthermore , the genome_wide co-expression based adjustments consistently provide comparably low levels of false_positive control for different exclusion criteria . 
neonatal seizure detection using blind adaptive fusion seizure is the result of excessive electrical discharges of neurons , which usually develops synchronously and happens suddenly in the central nervous system . clinically , it is difficult for physician to identify neonatal seizures visually , while eeg seizures can be recognized by the trained experts . usually , in nicus , eeg monitoring systems are used instead of the expensive on-site supervision . however , it is time-consuming to review an overnight recording , which motivates the researchers to develop automated seizure detection algorithms . although , there are few detection algorithms existed in the literature , it is difficult to evaluate these mathematical_model based algorithms since their performances vary significantly on different data_sets . by extending our previous results on mul-tichannel information fusion , we propose a distributed detection system consisting of the existing detectors and a fusion center to detect the seizure activities in the newborn eeg . the advantage of our technique is that it does not require any prior_knowledge of the hypotheses or the detector performances , which are often unknown in real applications . therefore , this proposed technique has the potential to improve the performances of the existing neonatal seizure detectors . in this thesis , we first review two newborn eeg models , one of which is used to generate neonatal eeg_signals . the synthetic data is used later for testing purpose . iii we also review three existing algorithms and implement them to work as the local detectors . then , we introduce the fusion algorithms applied in the fusion center for two different scenarios : large sample_size and small sample_size . we finally provide some numerical results to show the applicability , effectiveness , and the adaptability of the blind algorithms in the seizure detection problem . we also provide the testing results_obtained using the synthetic to show the improvement of the detection system . iv acknow ledgements i wish to express my deep and sincere gratitude to my superviosr , dr . aleksandar jeremic , for his encouragement and guidance from the initial to the final stage of my graduated study and for his support and detailed comments on the completion of this thesis . i wish to express my thanks to my parents , for their continuous support on my study from the day i went aboard from home . without their financial_support , it is impossible to complete my undergraduate study at mcmaster_university . without their support on taking care of my daughter , it is impossible to complete the research and write this thesis . i also would 
objectively structured performance_evaluation a learning tool the teaching_and_learning of medical students has always been a complicated process . even the best of teachers at time may struggle in communicating knowledge and assessing its uptake . simulated clinical and practical tools have recently gained popularity across the globe . they provide information regarding all the three aspects of assessment namely knowledge , skills and attitude . osce was first introduced by harden in 1975 . it encourages deep_learning by testing higher cognitive functions . university of health sciences lahore ( uhs ) modified the osce and introduced objectively structured performance_evaluation ( ospe ) in 2008 . in pakistan , it is a relatively new assessment method . the aim of ospe is to make practical examinations fair , objective and standardized in line with best evidence medical education ( beme ) and the local needs . assessment techniques appear to have an impact on students' study strategies' and influence their performance , that is , " assessment drives learning . " therefore in order to cope with assessments , students adapt different learning_styles , viz . , deep approach ( da ) , surface apathetic approach ( saa ) and strategic approach ( sa ) . ospe assesses students' knowledge , different skills and attitude at the same time , therefore , leads the students to read the subject widely and to practice clinical skills extensively . it helps students not to just remember theory but also helps them to critically reflect on their learning course and its outcomes , therefore covering not only the cognitive but also effective domains . the aim of this paper is to review the impact of ospe on students learning i . e . ospe as a learning tool . literature has been reviewed extensively using pubmed medline , paknet , mediscape and goo-gle socratic . review of literature has shown ospe is a valuable learning tool . introduction the teaching_and_learning of medical students has always been a complicated process , at times even the best of teachers may struggle in communicating knowledge and assessing its uptake . assessment of gained knowledge is probably more difficult than delivering it . assessment of clinical skills is far more important and complex as it directly link with patients care . the aim of this paper is to review the impact of ospe on students learning i . e . ospe as a learning tool . literature has been reviewed extensively using pubmed medline , paknet , mediscape and google socratic . 
a low-overhead self-healing embedded system for ensuring high yield and long_term sustainability of 60ghz 4gb/s radio-on-a-chip the available ism_band from 57-65ghz has become attractive for high_speed wireless applications including mass data transfer , streaming high_definition_video and even biomedical applications . while silicon based data transceivers at mm_wave frequencies have become increasingly mature in recent years [1 , 2 , 3] , the primary focus of the circuit community remains on the design of mm_wave front-ends to achieve higher data rates through higher_order modulation and beamforming techniques . however , the sustainability of such mm_wave systems when integrated in a soc has not been addressed in the context of die performance yield and device aging . this problem is especially challenging for the implementation of mm_wave soc's in deep sub-micron technology due to its process & operating_temperature variations and limited ft / fmax with respect to the operation frequency . to address the issue of sustainability in integrated mm_wave transceivers , this paper presents a low overhead self-healing system that can be embedded in an mm_wave transceiver to continually monitor and optimize its performance throughout the lifetime of the radio . the proposed system monitors key trans-ceiver parameters including transmitter image rejection , transmitter p1db , oim3 , and the receiver noise_figure and oim3 . the mm_wave front_end is designed to be extremely tunable allowing for adjustments to be made automatically as the device ages . figure 18 . 5 . 1 shows the architecture of the self-healing mm_wave transceiver that implements a dual-controller with cautious tracking for the robust_control of multiple circuit knobs within the mm-transceivers to meet a target performance specification shown_in_fig . 18 . 5 . 6 . using the numerically controlled oscillators ( ncos ) , the probe generator in the self-healing controller ( shc ) produces test tones with programmable frequencies and amplitudes to probe the mm_wave transceiver . with known test tones , transceiver impairments , such as image , noise , and intermodulation distortion , can be measured by sensors embedded throughout the transceiver . these sensors measure envelop variations , power level , and temperature . the measured parameters are digitized by the 10b , 5msps instrument adc and processed by the parameter estimator ( pe ) , which contains a 128-point fft processor used for spectral analysis and a statistical processor used to produce reliability measures employed by the dual controller for cautious tracking . to meet aggressive performance metrics such as better than-40dbc of image level and oim3 in the presence of background circuit noise , the controller through cautious tracking can dynamically adjust the rate of control of various tuning knobs in the transceiver according to the reliability measures from the pe . the self-healing transmitter is shown in 
current testing procedure for deep_submicron devices this paper presents a test technique that employs two different supply_voltages for the same iddq pattern . the results of the two measurements are subtracted in order to eliminate the inherent subthreshold leakage . summary of the experiment carried out on " system on a chip " ( soc ) device build in 0 . 35 technology is also shown . these experiments proved that the method is effective in detecting failures not detectable with the single limit iddq . 
designed -in-diagnostics : a new optical method 2 . a new optical sampling scheme an in-circuit diagnostic_test structure triggered by a light pulse captures logic states on-chip with picosecond timing accuracy , and the results read out via a scan_chain thus providing precise logic transition time information from deep_inside the chip , greatly aiding failure_analysis . the method could also make time measurement of switching events inside an ic when it is mounted in a printed_circuit_board environment , enabling correlation of these events to board level logic timing , i . e . system validation . . 1 . introduction today only two optical techniques for the internal timing_analysis of today's most advanced ic designs are viable . laser voltage probe instruments provide waveforms from any transistor on the device under test ( dut ) but require the locking of the dut test_pattern to a mode locked laser providing the probing pulses [1] . dynamic emission detection instruments are passive , detecting the weak photon emission produced by device switching action . dynamic emission is the simpler method to use but the timing data obtained by it shows only the falling edge positions of n-channel devices clearly . the rising edges of n-channel devices and any p-channel device switching are poorly detected [2] . with both instruments the signal_to_noise_ratio is poor , so that signal averaging is required to give accurate waveforms . signal_to_noise ratios will fall further as device geometry shrinks and as core voltages decrease , so that acquisition times will become extremely long . averaging times of tens of minutes to hours will be required for devices running test_patterns of 100's of microseconds . another serious problem for these instruments is that optical separation of nearby transistors is limited even with the best lenses to around 0 . 25 m . transistor spacing of 0 . 25 m is expected to be reached at the 45 nm node so that waveforms from separate transistors will be confusingly merged together . device testing is moving more towards structural on-chip testing . the proposed optically-driven logic-sampling scheme would provide rapid on-chip waveform acquisition capabilities for design debug and will be viable for present and future ic technologies . a test chip has been produced to demonstrate the capabilities of the method . tests of the basic structure have begun and the results are presented here for the first time . the proposed new optical method , while requiring on-chip circuits , would allow critical logic timing waveforms to be obtained quickly from internal nodes . the on-chip circuits provide means for generating a logic sample pulse , for capturing the logic 
research on scene infrared image simulation vega has been widely used in the virtual_reality field . its infrared ( ir ) module can implement ir_simulation , but vega ir imaging simulation's general approach does not apply to the complex scene . this article deeps into the scene's ir_simulation method based on vega . we design and realize a real time scene ir image simulation system in this article . we quantitatively define the scene as a simple and complex scene according to the scene range and whether it includes digital_elevation_model ( dem ) / digital surface model ( dsm ) data . for the simple scene , we directly process ir image simulation according to the vega general ir_simulation process . while for the complex scene , we propose an ir image simulation_method based on image_classification and automatic texture material mapping technique . at the aspect of image_classification , we develop a coarse to fine k_means_clustering method based on the consistency of image color for color image_classification and an additional support_vector_machine ( svm ) classification method based on texture_features for gray level image_classification . the method was tested on different scene's ir_simulation . experimental results show that the proposed approach can achieve better applicability and greater efficiency than the popular vega ir_simulation method . 
a simulation game for teaching secure data communications protocols with the widespread commercial use of the internet , secure data communications over the internet has become an important aspect of business operations . thus , it is an important study for information_technology and management students . the security protocol game is an interactive group activity for exploring secure data_communication protocols . using pen and paper , envelopes and game tokens , students simulate security_protocols and possible attacks against them . the game provides simple and intuitive representations for cryptographic methods , including both public_key and secret key techniques . using these representations , students can simulate internet application protocols such as pretty good privacy ( used to secure email ) and transport_layer_security ( used for secure web transactions ) . they can explore well-known protocols for authentication , key_exchange and blind_signatures . students can also develop and test their own protocols using public_key certificates , encrypted key transmission , tunnelling and other well-known techniques . through this learning activity , students gain a deep understanding of how security_protocols operate and are designed . the game has been used in tertiary units of study for managers and information_technology students . 
software test_program : a software residency experience the software test_program ( stp ) is a cooperation between motorola and the center for informatics of the federal_university_of_pernambuco . it has been conceived with inspiration on the medical residency , adjusted to the software_development practice . a software residency includes the formal teaching of the relevant concepts and deep practice , with specialization on some specific subject; here the focus is on software_testing . the stp has been of great benefit to all parties involved . 
testing the reliability of genetic methods of species identification via simulation . although genetic methods of species identification , especially dna_barcoding , are strongly debated , tests of these methods have been restricted to a few empirical cases for pragmatic reasons . here we use simulation to test the performance of methods based on sequence comparison ( blast and genetic_distance ) and tree topology over a wide range of evolutionary scenarios . sequences were simulated on a range of gene trees spanning almost three orders_of_magnitude in tree depth and in coalescent depth; that is , deep or shallow trees with deep or shallow coalescences . when the query's conspecific sequences were included in the reference alignment , the rate of positive identification was related to the degree to which different species were genetically differentiated . the blast , distance , and liberal tree_based methods returned higher rates of correct identification than did the strict tree_based requirement that the query was within , but not sister to , a single-species clade . under this more conservative approach , ambiguous outcomes occurred in inverse proportion to the number of reference sequences per species . when the query's conspecific sequences were not in the reference alignment , only the strict tree_based approach was relatively immune to making false_positive identifications . thresholds affected the rates at which false_positive identifications were made when the query's species was unrepresented in the reference alignment but did not otherwise influence outcomes . a conservative approach using the strict tree_based method should be used initially in large_scale identification systems , with effort made to maximize sequence sampling within species . once the genetic_variation within a taxonomic group is well characterized and the taxonomy resolved , then the choice of method used should be dictated by considerations of computational efficiency . the requirement for extensive genetic sampling may render these techniques inappropriate in some circumstances . 
emotion technology , wearables , and surprises could we help people have healthier lives and better experiences if computers could measure and help communicate our emotion ? years_ago , my students at mit and i began to design , build , and test both wearable and other sensors for recognizing emotion . we designed studies , gathered data , and developed signal_processing and machine_learning techniques to see what could be reliably extracted . in this talk i will highlight several of the most surprising findings during this adventure . these include new insights about the "true smile of happiness , " discovering that regular cameras ( and your smartphone , even in your handbag ) can compute some of your biosignals , finding electrical signals on the wrist that give insight into deep_brain activity , and learning surprising implications of wearable sensing for autism , anxiety , depression , sleep-memory_consolidation , epilepsy , and more . 
active traffic monitoring for heterogeneous_environments traffic management of ip networks comprises increasing challenges due to the occurrence of sudden and deep traffic variations that can be mainly attributed to the combined effects of several factors , like the great diversity of supported applications and services , different user's behaviors and different mechanisms of traffic generation and control . in this context , active traffic monitoring is particularly important as it enables characterizing essential aspects in network operation , like for example , quality of service as measured in terms of packet delays and losses . the main goal of this work is to carry out active measurements in a real operational network consisting in a heterogeneous environment that includes both wired and wireless lans . in order to perform this task , a measurement methodology , and its corresponding measurement platform , will be proposed . the measurement methodology is based on the one-way active measurement protocol ( owamp ) , a recent proposal from the internet2 and ietf ippm groups for active measurements of delays and losses in a single direction . the measurement platform was implemented , tested and conveniently validated . this paper begins by a brief presentation of the measurements that we intend to perform , then it describes the owamp protocol and the developed measurement system , including its implementation , test and validation through its application to different network scenarios . 
pattern_mining with natural_language_processing : an exploratory approach pattern_mining derives from the need of discovering hidden knowledge in very large amounts of data , regardless of the form in which it is presented . when it comes to natural_language_processing ( nlp ) , it arose along the humans' necessity of being understood by computers . in this paper we present an exploratory approach that aims at bringing together the best of both worlds . our goal is to discover patterns in linguistically processed texts , through the usage of nlp state-of-the-art tools and traditional pattern_mining algorithms . articles from a portuguese newspaper are the input of a series of tests described in this paper . first , they are processed by an nlp chain , which performs a deep_linguistic analysis of text; afterwards , pattern_mining algorithms apriori and genprefixspan are used . results_showed the applicability of pattern_mining techniques in textual structured_data , and also provided several evidences about the structure of the language . 
a lateral contribution learning algorithm for multi mlp architecture gradient is large and , for close to is close to . on the other hand and if lc is sufficiently sharp this has no far range influence , this " cooperation principle " is only local . and the snowball effect spreads this influence from one nn to its neighbors . of course , we can use existing optimization of the backpropagation learning_algorithm [13] to increase the quickness and find the edge of a deep well of l in one . when this edge is found we can use an algorithm , such as bfgs [9][3] , to go rapidly to this local_minima and then go back to the previous optimized lcl algorithm to search an other edge of a deep well of l . we can also improve it by an optimization of lc ( . ) kernel , by defining one kernel per connection . the heuristic of this idea is that each feature extracted by one weight in could be the same for an other . so , each component of this new kernel ( matrix indexed by ij ) can be rewriten as : where is the lateral contribution rate for the connection ij with regard to and . this rate can be on-line adapted with regard to the comparison of and the direction for each pair at each step , or off_line adapted with regard to the symmetries of the function f ( . ) . this off_line adaptation can be included in the term of distortion , allowing the vq algorithm to move two classes closer than if the term of distortion should only take the input space topology into account . the main improvements of lcl algorithm are not only the quickness of convergence of learning processing but also a new insight of the synaptic weight , showing that the behavior of a neural network in context is directly linked to the behavior of every nns in close to . this link is due to the strong constraint that must be continuous . it is now natural to try to merge these nns into one with synaptic weights estimated through the behavior of context . accordingly , we have developed an architecture for weight estimation named owe ( orthogonal weight estimator ) and tested it on control problems[5] . the ongoing studies correspond to testing the lcl algorithm for more and more complex input space , and defining generalized models to embed lcla and the on-line vector_quantization [6] . w t ( ) 0 w t 0 ( ) 0 nn 
efficient interconnect test patterns for crosstalk and static faults in this paper , we present efficient test patterns for the crosstalk induced faults on system-on-a-chip and board level interconnects considering actual effective aggressors to minimize the pattern size . all static faults also can be detected . the proposed method achieved the significant_reduction of the number of test_patterns than prior works , while preserving 100% fault_coverage . we are in the process of extending the proposed technique to built-in-self test logics . as deep_submicron techniques are increasingly developed , system-on-a-chips ( socs ) include more reusable cores such as processors , memories , and peripheral interfaces . today's boards become tomorrow's ic's , whereby today's ic's become tomorrow's cores . it becomes highly important to capture critical timing_defects as well as conventional static faults on the interconnect lines among socs on a board and cores on an soc . the ieee 1149 . 1 boundary scan and ieee 1500 are standards for testing boards and socs , and the interconnect testing is performed by serially scanning test_patterns through the boundary wrapper cells following the standards [1] , [2] . when boundary scan design techniques are adopted , interconnect test_generation and the application of test_patterns are greatly simplified . various test_generation algorithms have been developed to address the static interconnect faults [3]-[6] . crosstalk_faults generated through high_speed signal transmissions become significant challenges for soc interconnect testing [7]-[13] . a fast_and_accurate technique to estimate the crosstalk fault_coverage of any general test_set was developed [7] . although linear feedback shift_registers ( lfsr ) were extensively adopted to generate a few random patterns , the fault coverage was not satisfied . several deterministic and pseudorandom test_pattern generators for embedding necessary patterns of crosstalk_faults have been proposed [9] , [10] , and 4n+1 patterns were presented against 6n patterns considering ineffective aggressors [13] . all of the test patterns for interconnect faults introduced in the literatures target a single victim line . however , if an effective distance among interconnects can be extracted from the physical layout information , more than one victim can be targeted by a single test_pattern . in this paper , we introduce highly compact interconnect test patterns for crosstalk and static faults , which is independent of the number of total interconnects . this paper is organized as follows . section describes definitions and fault_models , related works are introduced in section , and the proposed method is investigated in section , which is followed by concluding remarks . . definitions and fault_models crosstalk_faults can be classified into positive glitch , 
testing a cmos operational_amplifier circuit using a combination of oscillation and i_ddq test_methods and my brother kiran , for their constant prayers and encouragement throughout my life . i am very grateful to my advisor dr . a . srivastava for his guidance , patience and understanding throughout this work . his suggestions , discussions and constant encouragement have helped me to get a deep insight in the field of vlsi_design . departments , for supporting me financially during my stay at lsu . i am very thankful to my friends anand , satish and syam for their extensive help throughout my graduate studies at lsu . i take this opportunity to thank my friends maruthi , sudheer , anoop and siva for their help and encouragement at times i needed them . i would also like to thank all my friends here who made my stay at lsu an enjoyable and a memorable one . last of all i thank god for keeping me in good health and spirits throughout my stay at lsu . 
ieee intelligent_systems knowledge_representation_and_reasoning ai's greatest trends and controversies except perhaps for the ai naysayers , ai practitioners are creators of software artifacts , their underlying algorithms , and their underlying theories . we begin our feature with herbert simon , one of our three contributors ( together with john mccarthy and oliver selfridge ) who are our links to the landmark dartmouth conference in 1956 , where modern ai is often said to have begun . simon paints a broad picture of ai as a discipline constantly pursuing computational creations that challenge the uniqueness of biologically grounded intelligence . ai has been thought controversial because it challenged the uniqueness of human thought , as darwin challenged the uniqueness of human origins . the boundaries of ai continue to expand rapidly , settling the controversy for those who know the evidence . ai first demonstrated that important intellectual tasks could be accomplished by selective heuristic search , often in a thoroughly human way . gps is one product of that line of research . then ai explored the role of large bodies of knowledge in expert thinking . dendral was an early important success , as was the extensive research on human chess expertise , modeled with such programs as chrest ( not deep_blue , which is only partly humanoid ) . a third successful line has been the research on learning for , example , siklossy's zbie program , which learned natural_language by comparing sentences with pictures . finally , there has been the great recent advance in robotics , based on progress in simulating sensory and motor functions . the basic strategy of ai has always been to seek out progressively more complex human tasks and show how computers can do them , in humanoid ways or by brute force . with a half-century of steady progress , we have assembled a solid_body of tested theory on the processes of human thinking and the ways to simulate and supplement them . in what media do ai practitioners create ? the answer to this question is a depiction of ai itself , so it is not too surprising that most of our contributions address this question . wolfgang bibel , a proponent of the formalist agenda in ai , argues for the need for sophisticated logic formalisms and inferential methods for ai . alan bundy adds to these arguments , discussing further the advances achieved by those taking the formal_logic approach to ai , especially in light of the critiques raised by those on the other side of the ai fence . among the controversies in ai , none is as persisting as the one about logic's role in ai . it 
a bayesian approach to relevance in game playing 1 . the point of game_tree search is to insulate oneself from errors in the evaluation function . the standard approach is to grow a full width tree as deep as time allows , and then value the tree as if the leaf evaluations were exact . this has been eeective in many games because of the computational eeciency of the alpha_beta algorithm . our approach is to form a bayesian model of our uncertainty . we adopt an evaluation function that returns a probability_distribution estimating the probability of various errors in valuing each position . these estimates are obtained by training from data . we thus use additional_information at each leaf not available to the standard approach . we utilize this information in three ways : to evaluate which move is best after we are done expanding , to allocate additional thinking time to moves where additional time is most relevant to game outcome , and , perhaps most importantly , to expand the tree along the most relevant lines . our measure of the relevance of expanding a given leaf provably approximates a measure of the impact of expanding the leaf on expected payoo , including the impact of the outcome of the leaf expansion on later expansion decisions . our algorithms run ( under reasonable assumptions ) in time linear in the size of the nal tree and hence except for a small constant factor , are as time eecient as alpha_beta . in a given amount of time our algorithm can thus in principle grow a tree several times as deep as alpha_beta along the relevant lines . we have tested our approach on a variety of games , including othello , kalah , warri , and others . our probability independence approximations are seen to be signiicantly violated , but nonetheless our tree valuation scheme was found to play signiicantly better than minimax or the probability product_rule when both competitors search the same tree . our full search_algorithm was found to outplay a highly ranked , directly comparable alpha_beta othello program even when the alpha_beta program was given sizeable time odds , and also performed well against the three top othello programs on the internet othello server . 
iddx-based test_methods : a survey supply_current measurement-based test is a valuable defect-based test method for semiconductor chips . both static leakage_current ( i<sub>ddq</sub> ) and transient current ( i<sub>ddt</sub> ) based tests have the capability of detecting unique defects that improve the fault_detection capacity of a test suite . collectively these test_methods are known as i<sub>ddx</sub> tests . however , due to advances in the semiconductor_manufacturing process , the future of these test_methods is uncertain . this paper presents a survey of the research reported in the literature to extend the use of i<sub>ddx</sub> tests to deep_sub_micron ( dsm ) technologies . 
a new method of implementing hierarchical opc for emerging deep-subwavelength lithography technologies ( 90 nm and following ) the data volume and the complexity of optical proximity correction ( opc ) have increased dramatically . this has added to the total cost of ic manufacturing and become an increasingly critical issue in optical lithography . in this paper , we present a new method of implementing hierarchical opc to explore its merits in runtime saving . the interactions and propagating corrections between neighboring cells during opc have been discussed and appropriate solutions have been proposed . segment-moving map ( smm ) and dynamic correction are brought forward for the first time to identify the interacting regions in hierarchical opc and automatically adjust the corrections in these regions . furthermore , total edge placement error ( epe ) is calculated in controlled experiments to test the accuracy of this method . results have shown that approximately 5x speedup has been achieved with similar accuracy when compared with the conventional opc method . 
lithology identification of aquifers from geophysical well logs and fuzzy_logic analysis : shui-lin area , taiwan the purpose of this study is to construct a fuzzy_lithology system from well logs to identify formation lithology of a groundwater aquifer system in order to better apply conventional well logging interpretation in hydro-geologic studies because well log responses of aquifers are sometimes different from those of conventional oil and gas reservoirs . the input variables for this system are the gamma_ray log reading , the separation between the spherically focused resistivity and the deep very-enhanced resistivity curves , and the borehole compensated sonic log reading . the output variable is groundwater formation lithology . all linguistic variables are based on five linguistic terms with a trapezoidal membership function . in this study , 50 data_sets are clustered into 40 training sets and 10 testing sets for constructing the fuzzy_lithology system and validating the ability of system prediction , respectively . the rule_based database containing 12 fuzzy_lithology rules is developed from the training data sets , and the rule strength is weighted . a madani inference system and the bisector of area defuzzification method are used for fuzzy_inference and defuzzification . the success of training performance and the prediction ability were both 90% , with the calculated correlation of training_and_testing equal to 0 . 925 and 0 . 928 , respectively . well logs and core data from a clastic aquifer ( depths 100 198 m ) in the shui-lin area of west-central taiwan are used for testing the system's construction . comparison of results from core analysis , well logging and the fuzzy_lithology system indicates that even though the well logging method can easily define a permeable sand formation , distinguishing between silts and sands and determining grain_size variation in sands is more subjective . these shortcomings can be improved by a fuzzy_lithology system that is able to yield more objective decisions than some conventional methods of log interpretation . 
component repair using laser direct metal deposition recent_studies have indicated that laser direct metal deposition can be used for repairing deep or internal cracks and defects in metallic components . in order to implement the method , it is necessary to machine a groove or slot to the depth of the defect and refill it . this work investigates advantages and potential problems with the technique and compares the results from using two different slot geometries : one rectangular and one triangular in cross_section . h13 hot-work tool_steel components are used and h13 powder is deposited using a 1 . 5 kw diode laser and lateral nozzle . different combinations of deposition parameters are tested and each sample is analysed in terms of mass deposition rate , deposition microstructure , evidence of porosity , size of the heat-affected zone , and microhardness . results are evaluated using statistical_techniques and the important parameters that control each variable are identified . the work provides evidence that the method can produce high_quality repairs , but poros-ity at the boundaries between the original part and the added material is a problem . 
yield enhancement methodology for cmos standard cells in order to maximize the yield of random logic in today's advanced deep_sub_micron_cmos technologies we have developed a complete yield enhancement methodology for cmos standard cells . this methodology based on a test vehicle approach covers design , industrial test , data_collection and volume analysis , design debug , failure location and analysis . it has proven to be successful on three consecutive technology_nodes down to 65nm . this paper will explain the methodology and demonstrate the results and benefits of this work through illustrated examples . 
memory_bist with the advent of deep-submicron vlsi technology , core-based system-on-chip ( soc ) design is attracting an increasing attention . on an soc , popular reusable cores include memories ( such as rom , sram , dram and flash_memory ) , processors ( such as cpu , dsp and microcontroller ) , input/output circuits , etc . memory_cores are obviously among the most universal ones-almost all system chips contain some type of embedded_memory [5] . however , to provide a low cost-cost test solution for the on-chip memory cores is not a trivial task . this paper presents a study on memory_bist , algorithms of different test_patterns , survey of memory_bist implementations , and discussion of some novel design issues . this paper shall serve as a knowledge_base for future design in memory_bist . 
methodology on extracting compact layout rules for latchup prevention in deep_submicron bulk cmos_technology an experimental methodology to find area-efficient compact layout rules to prevent latchup in bulk complimentary metal oxide semiconductor ( cmos ) integrated_circuits ( ics ) is proposed . the layout rules are extracted from the test_patterns with different layout spacings or distances . a new latchup prevention design by adding the additional internal double guard rings between input/output cells and internal circuits is first reported in the literature , and its effectiveness has been successfully proven in three different bulk cmos_processes . through detailed experimental verification including temperature effect , the proposed methodology to extract compact layout rules has been established to save silicon_area of cmos_ics but still to have high enough latchup immunity . this proposed methodology has been successfully verified in a 0 . 5-m nonsilicided , a 0 . 35-m silicided , and a 0 . 25-m silicided shallow-trench-isolation bulk cmos_processes . 
implant survival , adverse events , and bone_remodeling of osseointegrated percutaneous implants for transhumeral_amputees background osseointegrated percutaneous implants provide direct anchorage of the limb prosthesis to the residual limb . these implants have been used for the rehabilitation of transhumeral_amputees in sweden since 1995 using a two-stage surgical approach with a 6-month interval between the stages , but results on implant survival , adverse events , and radiologic signs of osseointegration and adaptive bone_remodeling in transhumeral_amputees treated with this method are still lacking . questions/purposes this study reports on 2- and 5-year implant survival , adverse events , and radiologic signs of osseointegration and bone_remodeling in transhumeral_amputees treated with osseointegrated prostheses . methods between 1995 and 2010 , we performed 18 primary osseointegrated percutaneous implants and two implant revisions in 18 transhumeral_amputees; of those , 16 patients were available for followup at a minimum of 2 years ( median , 8 years; range , 2-19 years ) . these include all transhumeral_amputees who have received osseointegrated prostheses and represented approximately 20% of the all transhumeral_amputees we evaluated for potential osseointegration during that time; general indications for this approach included transhumeral amputation resulting from trauma or tumor , inability to wear or severe problems wearing a conventional socket prosthesis , eg , very short residual limb , and compliant patients . medical charts and plain radiographs were retrospectively evaluated . results the 2- and 5-year implant survival rates were 83% and 80% , respectively . two primary and one revised implant failed and were removed because of early loosening . a fourth implant was partially removed because of ipsilateral shoulder osteoarthritis and subsequent arthrodesis . the most common adverse_event was superficial infection of the skin penetration site ( 15 infections in five patients ) followed by skin reactions of the skin penetration site ( eight ) , incomplete fracture at the first surgery ( eight ) , defective bony canal at the second surgery ( three ) , avascular skin flap necrosis ( three ) , and one deep implant infection . the most common radiologic finding was proximal trabecular buttressing ( 10 of 20 implants ) followed by endosteal bone_resorption and cancellization ( seven of 20 ) , cortical thinning ( five of 20 ) , and distal bone_resorption ( three of 20 ) . conclusions the implant system presented a survivorship of 83% at 5 years and a 38% 5-year incidence of infectious complications related to the skin penetration site that were easily managed with nonoperative treatment , which make it a potentially attractive alternative to conventional socket arm prostheses . osseointegrated arm prostheses have so far only been used in transhumeral amputations resulting from either trauma or tumor . their use has not been tested and is therefore not recommended in transhumeral amputations resulting from vascular_disease . this method could theoretically be superior to socket prostheses , especially in transhumeral_amputees with very short residual humerus in which the suspension of a conventional prosthesis is difficult . comparative studies are needed to support its potential superiority . moreover , the radiological findings in this study need to be followed over time because some of them are of uncertain long_term clinical relevance . 
optimizing and contrasting recurrent neural_network architectures recurrent_neural_networks ( rnns ) have long been recognized for their potential to model complex time_series . however , it remains to be determined what optimization_techniques and recurrent architectures can be used to best realize this potential . the experiments presented take a deep look into hessian free optimization , a powerful second order optimization method that has shown promising_results , but still does not enjoy widespread use . this algorithm was used to train to a number of rnn architectures including standard rnns , long short_term_memory , multiplicative rnns , and stacked rnns on the task of character prediction . the insights from these experiments led to the creation of a new multiplicative lstm hybrid_architecture that outperformed both lstm and multiplicative rnns . when tested on a larger scale , multiplicative lstm achieved character level modelling results competitive with the state of the art for rnns using very different methodology . 
stress_migration and electromigration improvement for copper dual_damascene interconnection stress_migration sm and electromigration em were widely used to study the performance of interconnection process of metal/via formation in copper dual_damascene of wafers . necking and voids at the via bottom were important in causing failures in tests of stress_migration and electromigration . in this report , the contamination of the bottom of via , which results in poor step coverage , the adhesion of seed layers , and poor copper grain formation are identified to be the underlying causes of the necking and void formation after the first em and sm tests are performed . the contamination of the via formation processes included via etching , trench etching , and barrier/seed layer depositions . a well-shaped via profile can be optimized using three methods , the first involves cu/sin interface stress , the second involves cu grain growth , and the third involves post via etching clean study . eliminating the contamination of the via bottom and optimizing step coverage and adhesion of the barrier seed layers improve the em and sm performance from time-to-fail 13 to 59 s , in the copper-related processes for fabricating 300 mm wafers using technology that is beyond 0 . 13 m technology . copper cu has been adopted in deep submicrometer ultralarge scale integration ulsi metallization and interconnection because it has low resistivity and better reliability than other metals . 1-4 a dual_damascene process has been used for fabricating copper inter-connections . via/trenching etch , tantalum nitride tan deposition , copper seed layer deposition , cu electrochemical plating ecp , and copper chemical mechanical polishing cmp are the major processes in the fabrication of dual_damascene . stress_migration sm and electromigration em tests are usually used to qualify the performance of the interconnection process associated with metal/via formation in copper dual_damascene of wafers . ogawa et al . 5 explained electromigration in terms of mass transport . fischer et al . 6 established a strong correlation of electromigration failure with local defects from processes such as liner deposition , preclean , or trench etches . tokei et al . 7 claimed that electromigration was influenced by argon preclean . suzuki et al . 8 showed that the failure_rate of stress_migration depends on both the line width and via diameter . ishikawa et al . 9 revealed that stress_migration of cu interconnects depends strongly on the adhesive strength of the barrier metal/cu interface and the step coverage . alers et al . 10 explained copper contamination of via sidewalls and interlevel dielectric ild damage during sputter preclean affects stress_migration . ogawa et 
using symbolic learning to improve knowledge_based neural_networks the previously-reported kbann system integrates existing knowledge into neural_networks by deening the network_topology and setting initial link weights . standard neural learning_techniques can then be used to train such networks , thereby reening the information upon which the network is based . however , standard neural learning_techniques are reputed to have dii-culty training networks with multiple layers of hidden_units; kbann commonly creates such networks . in addition , standard neural learning_techniques ignore some of the information contained in the networks created by kbann . this paper describes a symbolic inductive learning algorithm for training such networks that uses this previously-ignored information and which helps to address the problems of training \deep" networks . empirical_evidence shows that this method improves not only learning speed , but also the ability of networks to generalize correctly to testing examples . 
magnetization transfer ratio in neuro-beh_et_disease . the aim of this study was to determine the contribution of magnetization transfer ratios ( mtrs ) in detecting disease in normal-appearing brain regions of patients with neuro-beh et ( nb ) disease . thirty-two patients with nb disease were assessed . fifteen healthy volunteers were examined as the control group . magnetic_resonance ( mr ) imaging of the head was performed without and with magnetization transfer ( mt ) contrast . signal intensity measurements were obtained from ten anatomical regions ( centrum semiovale , corona radiata , internal_capsule , forceps major , forceps minor , thalamus , substantia_nigra pars compacta , substantia_nigra pars grisea , inferior pons and middle cerebellar peduncle ) in both groups . also measured in the nb group were parenchymal lesions in the brain_stem , basal_ganglia and cerebral deep white_matter . mtr was calculated for each measurement . statistical_analysis was performed with mann-whitney u and independent t-tests with computer-based spss 11 . 0 for windows software . a p_value below 0 . 05 was considered statistically_significant . the mean mtr of the parenchymal lesions in the nb group was lower than the mean mtr of the normal-appearing parenchyma in both the nb patients and the normal group . for the normal-appearing parenchyma the mean mtr in the nb group was higher than that for the controls for all regions except the corona radiata; however , the difference was statistically_significant only for the thalamus . the mri-visible parenchymal involvement of beh_et's_disease causes a decrease in mtr . for the normal-appearing brain , although lacking statistical_significance for the most regions studied , the tendency for higher mtr in nb patients compared with controls may offer an insight into the pathophysiology of beh_et's_disease . 
electromigration-aware physical_design of integrated_circuits high_speed interconnect technology : on-chip and off-chip p . 7 testing nanometer digital integration circuits : myths , reality and the road ahead p . 8 soc design_methodology : a practical approach p . 10 test_methodologies in the deep_submicron_era-analog , mixed_signal , and rf p . 12 recent advances in verification , equivalence checking and sat-solvers p . 14 compact mosfet models for low_power analog cmos design p . 15 physics and technology : towards low_power dsm design p . 16 architectural , system level and protocol level techniques for power_optimization for networked embedded_systems p . 18 the high walls have crumpled p . 21 65nm omnibudsman p . 25 esl-the next leadership opportunity for india ? p . 26 vlsi_design challenges for gigascale integration p . 27 moore's_law is unconstitutional p . 31 configurable processor the building block for soc ( system-on-a-chip ) p . 35 modeling usable and reusable transactors in system verilog p . 36 optimizing soc manufacturability p . 37 tuple detection for path delay_faults : a method for improving test_set quality p . 41 a delay test to differentiate resistive interconnect faults from weak transistor defects p . 47 efficient space/time compression to reduce test_data_volume and testing time for ip_cores p . 53 on efficient x-handling using a selective compaction scheme to achieve high test response compaction ratios p . 59 heterogeneous and multi-level compression techniques for test volume reduction in systems-on-chip p . 65 cellular_automata based test_structures with logic folding p . 71 electromigration-aware physical_design of integrated_circuits p . 77 variance_reduction in monte_carlo capacitance extraction p . 85 a fast buffered routing tree construction algorithm under accurate delay model p . 91 improved layout-driven area-constrained timing optimization by net buffering p . 97 battery model for embedded_systems p . 105 rapid embedded hardware/software system generation p . 111 a unified architecture for adaptive compression of data and code on embedded_systems p . 117 a heuristic for peak power_constrained design of network_on_chip ( noc ) based multimode systems p . 124 a low-power current-mode clock distribution scheme for multi-ghz noc-based socs p . 130 implementing ldpc decoding on network_on_chip p . 134 a risc hardware platform for low_power java p . 138 a low power reprogrammable parallel_processing vlsi architecture for computation of b_spline based medical image_processing system for fast characterization of tiny objects suspended in cellular fluid p . 147
prediction of financial time_series with hidden_markov_models prediction of financial time_series with hidden_markov_models in this thesis , we develop an extension of the hidden_markov_model ( hmm ) that addresses two of the most important challenges of financial time_series modeling : non-stationary and non-linearity . specifically , we extend the hmm to include a novel exponentially weighted expectation-maximization ( em ) algorithm to handle these two challenges . we show that this extension allows the hmm algorithm to model not only sequence data but also dynamic financial time_series . we show the update rules for the hmm parameters can be written in a form of exponential moving averages of the model variables so that we can take the advantage of existing technical_analysis techniques . we further propose a double weighted em_algorithm that is able to adjust training sensitivity automatically . convergence results for the proposed algorithms are proved using techniques from the em theorem . experimental results show that our models consistently beat the s&p_500_index over five 400-day testing periods from 1994 to 2002 , including both bull and bear markets . our models also consistently outperform the top 5 s&p_500 mutual_funds in terms of the sharpe_ratio . iii to my mom iv acknowledgments i would like to express my deep gratitude to my senior supervisor , dr . anoop sarkar for his great patience , detailed instructions and insightful comments at every stage of this thesis . what i have learned from him goes beyond the knowledge he taught me; his attitude toward research , carefulness and commitment has and will have a profound impact on my future academic activities . i am also indebted to my co-senior supervisor , dr . andrey pavlov , for trusting me before i realize my results . i still remember the day when i first presented to him the " super five day pattern " program . he is the source of my knowledge in finance that i used in this thesis . without the countless time he spent with me , this thesis surely would not have been impossible . i cannot thank enough my supervisor , dr . oliver schulte . part of this thesis derives from a course project i did for his machine_learning course in which he introduced me the world of machine learning and interdisciplinary research approaches . many of my fellow graduate students offered great help during my study and research at the computing_science school . i thank wei luo for helping me with many issues regarding the l a t e xand unix; cheng lu and chris demwell for sharing their 
movemine : mining moving_object_data for discovery of animal movement patterns with the maturity and wide availability of gps , wireless , telecommunication , and web technologies , massive amounts of object movement data have been collected from various moving_object targets , such as animals , mobile_devices , vehicles , and climate radars . analyzing such data has deep_implications in many applications , such as , ecological study , traffic control , mobile communication management , and climatological forecast . in this article , we focus our study on animal movement data_analysis and examine advanced data_mining methods for discovery of various animal movement patterns . in particular , we introduce a moving_object data_mining system , movemine , which integrates multiple data_mining functions , including sophisticated pattern_mining and trajectory analysis . in this system , two interesting moving_object pattern_mining functions are newly_developed : ( 1 ) <i>periodic_behavior mining</i> and ( 2 ) <i>swarm pattern_mining</i> . for mining periodic behaviors , a reference location-based_method is developed , which first detects the reference locations , discovers the periods in complex movements , and then finds periodic patterns by hierarchical_clustering . for mining swarm patterns , an efficient method is developed to uncover flexible moving_object clusters by relaxing the popularly-enforced collective movement constraints . in the movemine system , a set of commonly used moving_object mining functions are built and a user-friendly interface is provided to facilitate interactive exploration of moving_object data_mining and flexible tuning of the mining constraints and parameters . movemine has been tested on multiple kinds of real datasets , especially for movebank applications and other moving_object_data analysis . the system will benefit scientists and other users to carry out versatile analysis tasks to analyze object movement regularities and anomalies . moreover , it will benefit researchers to realize the importance and limitations of current techniques and promote future studies on moving_object data_mining . as expected , a mastery of animal movement patterns and trends will improve our understanding of the interactions between and the changes of the animal world and the ecosystem and therefore help ensure the sustainability of our ecosystem . 
bayesian generic priors for causal_learning structure and strength in causal models causal graphs lend themselves to the development of rational models based on bayesian we present a bayesian model of causal_learning that incorporates generic priors on distributions of weights representing potential powers to either produce or prevent an effect . these generic priors favor necessary and sufficient causes . the ns power model couples these priors with a causal generating_function derived from the power pc theory ( cheng , 1997 ) . we test this and other alternative bayesian models using the strategy of computational cognitive psychophysics , fitting multiple data_sets in which several parameters are varied parametrically across multiple types of judgments . the ns power model accounts for a wide range of data concerning judgments of both causal strength ( the power of a cause to produce or prevent an effect ) and causal structure ( whether or not a causal link exists ) . for both types of causal judgments , a generic prior favoring a cause that is jointly necessary and sufficient explains interactions involving causal direction ( generative versus preventive causes ) . for structure judgments , an additional prior that a new candidate cause will be deterministic ( i . e . , sufficient or else ineffective ) explains why people's causal structure judgments are based primarily on causal power and the base rate of the effect , rather than sample_size . alternative bayesian formulations that lack either causal power assumptions or generic priors for necessity and sufficiency proved inadequate . broader implications of the bayesian_framework for human learning are discussed . intelligent behavior in a complex and potentially hostile environment depends on acquiring and exploiting knowledge of " what causes what . " although causal induction can certainly be constrained in a top-down fashion by prior causal knowledge , new causal knowledge ultimately depends on an inference_engine that takes non-causal knowledge ( most notably , information about temporal order and covariation ) as input and yields causal knowledge as its output ( cheng , 1993 ) . it is likely that the cognitive mechanisms for causal_learning have deep evolutionary roots , a conjecture supported by many parallels between phenomena in animal conditioning and human causal_learning ( see shanks , 2004 ) . it is therefore a plausible hypothesis that the system for general causal_learning is well-adapted to the characteristics of causes that operate in the natural_environment , and hence will be characterized by bounded_rationality ( simon , 1955 ) . a useful formalism for representing causal hypotheses is provided by directed causal graphs , simple examples of which are shown in figure 1 . within a causal graph , each directed arrow connects a node representing a cause to one of its effects , where it is 
video_compression algorithm based on frame difference approaches the huge usage of digital multimedia via communications , wireless_communications , internet , intranet and cellular mobile leads to incurable growth of data_flow through these media . the researchers go deep in developing efficient techniques in these fields such as compression of data , image and video . recently , video_compression techniques and their applications in many areas ( educational , agriculture , medical ) cause this field to be one of the most interested fields . wavelet_transform is an efficient method that can be used to perform an efficient compression technique . this work deals with the developing of an efficient video_compression approach based on frames difference approaches that concentrated on the calculation of frame near distance ( difference between frames ) . the selection of the meaningful frame depends on many factors such as compression performance , frame details , frame size and near distance between frames . three different approaches are applied for removing the lowest frame difference . in this paper , many videos are tested to insure the efficiency of this technique , in addition a good performance results has been obtained . 
electronic_commerce and the strategic_management of deep_sea container shipping companies : an exploratory survey analysis despite the increased adoption of electronic_commerce ( ec ) among container shipping companies , the role of ec practice on their strategic_management is poorly_understood . this study takes an interdisciplinary approach to examine the uses , motivations , barriers and strategic relevance of ec in the container shipping industry in 1992 and 2002 . to test five hypotheses , a 41-question survey was sent to 297 shipping companies , yielding an 11 . 1% response rate . the research found that the role of ec became more strategic and relevant for the identification of business goals . this change , however , could not be explained solely by the desire of companies to exploit ec externally with clients , but also by their ability to master ec internally . the role of ec was more strategic in companies where service is a key element of the company's competitive strategy and ec is regarded as a competitive necessity . 
rt_level fast fault simulator in this paper a new fast fault simulation technique is presented for calculation of fault propagation through hlps ( high_level primitives ) . rotdds ( reduced ordered ternary decision diagrams ) are used to describe hlp modules . the technique is implemented in the htdd rt_level fault simulator . the simulator is evaluated with some itc99 benchmarks . a hypothesis is proved that a test set coverage of physical failures can be anticipated with high_accuracy when rtl fault_model takes_into_account optimization strategies that are used in cae system applied . 1 . introduction recent_developments in the area of deep_submicron_technology have challenged integrated_circuit ( ic ) test_methods as never before [1] . the increasing complexity of systems being designed makes test development more time-consuming . moreover , nanometer technology has introduced new problems such as new types of defects or higher data rates . to reduce manufacturing cost and time-to-market , efficient fault detection and location should be used . one of the most essential tasks in fault_diagnosis is fault simulation [2] . current computer_aided_engineering ( cae ) tools must address the needs for new generation of ics e . g . , systems-on-a-chip ( soc ) . recent works in this area have increased emphasis on new design techniques such as high_level synthesis , behavioral synthesis , design_reuse and ip-based design . for this reason , new atpg tools that reflect new design flows should be developed , especially tools working at higher level_of_abstraction than gate_level . several approaches for high_level automatic_test_pattern_generation ( hlatpg ) have already been proposed . in artist [3 , 4] a quality of generated test_sequence is measured in terms of the number of blocks of statements in source description of a system activated during its true-value rt_level simulation . artist accepts synthesizable functional register_transfer_level ( rtl ) 
a scalable model of the substrate network in deep n-well rf mosfets with multiple fingers a novel scalable model of substrate components for deep n-well ( dnw ) rf mosfets with different number of fingers is presented for the first time . the test structure developed in [1] is employed to directly access the characteristics of the substrate to extract the different substrate components . a methodology is developed to directly extract the parameters for the substrate network from the measured data . by using the measured two-port data of a set of nmosfets with different number of fingers , with the dnw in grounded and float configuration , respectively , the parameters of the scalable substrate model are obtained . the method and the substrate model are further verified and validated by matching the measured and simulated output admit-tances . excellent agreement up to 40 ghz for configurations in common-source has been achieved . 
clipper package ( version 1 . 14 . 0 ) 1 along signal paths : an empirical gene_set approach exploiting pathway topology 1 . 1 clipper approach different experimental conditions are usually compared in terms of their gene_expression mean differences . in the univariate case , if a gene_set changes significantly its multivariate mean expression in one condition with respect to the other , it is said to be differentially expressed . however , the difference in mean expression levels does not necessarily result in a change of the interaction strength among genes . in this case , we will have pathways with significant altered mean expression levels but unaltered biological interactions . on the contrary , if transcripts abundances ratios are altered , we expect a significant alteration not only of their mean , but also of the strength of their connections , resulting in pathways with completely corrupted functionality . therefore , to look for pathways strongly involved in a biological process , we should look at pathways with both mean and variance significantly altered . clipper is based on a two-step approach : 1 ) it selects pathways with covariance matrices and/or means significantly different between experimental conditions and 2 ) on such pathways , it identifies the sub-paths mostly associated to the phenotype . this is a very peculiar feature in pathway_analysis . to our knowledge this is the first approach able to systematically inspect a pathway deep in its different portions . currently there are some pathway_analysis methods implemented in bioconductor ( probably the most famous is gsea ) , but very few of them try to exploit pathway topology . example of the latter category are spia and degraph . in martini et al . 2012 is provided a detailed comparison of the performance of non-topological analysis ( gsea ) and topological analysis ( spia and clipper ) using both real and simulated data . in the next few words , we are going to highlight the defferences of these apperoaches . gsea uses pathway as a list of genes without taking into account the structure of the pathway while spia takes_into_account pathway topological information , gene fold-changes and pathway enrichment scores . then spia takes as input only the list of differentially expressed genes . so , from a practical point_of_view clipper and spia test different null hypotheses . more importantly , clipper is able to highlight the portions ( sub-path ) of the pathway that are mostly involved in the phenotype under study using graph decomposition theory . for more ditails please refer to martini et al . 2012 . 
scaffolding problem_solving with annotated , worked-out examples to promote deep_learning this study seeks to compare the relative utility for learning college-level physics of intelligent_tutoring_systems that have procedural based hints and worked-out examples . in order to test which produced better gains , a modified version of andes was used in which participants either received hints or annotated , worked-out examples in response to their help requests . we found that providing annotated , worked-out examples instead of hint sequences was more efficient in the number of problems it took to obtain basic mastery . 
better mixing via deep representations it has been hypothesized , and supported with experimental evidence , that deeper representations , when well trained , tend to do a better job at disentangling the underlying factors of variation . we study the following related conjecture : better representations , in the sense of better disentangling , can be exploited to produce markov_chains that mix faster between modes . consequently , mixing between modes would be more efficient at higher_levels of representation . to better understand this , we propose a secondary conjecture : the higher_level samples fill more uniformly the space they occupy and the high_density manifolds tend to unfold when represented at higher_levels . the paper_discusses these hypotheses and tests them experimentally through visualization and measurements of mixing between modes and interpolating between samples . 
a sparse and locally shift invariant feature extractor applied to document images we describe an unsupervised_learning algorithm for extracting sparse and locally shift-invariant_features . we also devise a principled procedure for learning hierarchies of invariant_features . each feature detector is composed of a set of trainable convolutional filters followed by a max_pooling layer over non-overlapping windows , and a point-wise sig-moid non-linearity . a second stage of more invariant_features is fed with patches provided by the first stage feature extractor , and is trained in the same way . the method is used to pre-train the first four layers of a deep convolutional network which achieves state-of-the-art performance on the mnist dataset of handwritten digits . the final testing error_rate is equal to 0 . 42% . preliminary experiments on compression of bitonal document images show very promising results in terms of compression_ratio and reconstruction error . 
deep_learning of appearance models for online object_tracking this paper introduces a novel deep_learning based approach for vision based single target tracking . we address this problem by proposing a network_architecture which takes the input video frames and directly computes the tracking score for any candidate target location by estimating the probability_distributions of the positive and negative examples . this is achieved by combining a deep_convolutional_neural_network with a bayesian loss layer in a unified framework . in order to deal with the limited number of positive training examples , the network is pre_trained offline for a generic image feature representation and then is fine-tuned in multiple steps . an online fine-tuning step is carried out at every frame to learn the appearance of the target . we adopt a two-stage iterative algorithm to adaptively update the network parameters and maintain a probability_density for target/non-target regions . the tracker has been tested on the standard tracking benchmark and the results indicate that the proposed solution achieves state-of-the-art tracking results . 
enabling large_scale pervasive_logic verification through multi-algorithmic formal reasoning pervasive_logic is a broad term applied to the variety of logic present in hardware designs , yet not a part of their primary functionality . examples of pervasive_logic include initialization and self-test logic . because pervasive_logic is intertwined with the func-tionality of chips , the verification of such logic tends to require very deep sequential_analysis of very large slices of the design . for this reason , pervasive_logic verification has hitherto been a task for which formal algorithms were not considered applicable . in this paper , we discuss several pervasive_logic verification tasks for which we have found the proper combination of algorithms to enable formal_analysis . we describe the nature of these verification tasks , and the testbenches used in the verification process . we furthermore discuss the types of algorithms needed to solve these verification tasks , and the type of tuning we performed on these algorithms to enable this analysis . 
hybrid-sbst methodology for efficient testing of processor cores &aggressive semiconductor fabrication processes have resulted in state-of-the-art processor designs and socs built around processor cores that contain more than a billion transistors and operate at gigahertz frequencies . deep_submicron geometries and complex speed-enhancing mechanisms produce excellent performance , but serious testability challenges arise . new types of defects require the deployment of at-speed tests to achieve high test_quality . traditional processor at-speed manufacturing tests based on functional ate cannot be considered an economically viable scheme . several software_based_self-test ( sbst ) methodolo-gies have been proposed as an effective alternative or supplement for the manufacturing_test of microprocessors and embedded processors in socs ( see the ''related work'' sidebar ) . sbst is a nonintrusive approach that embeds a ''software tester'' in the form of a self-test_program in a processor's on-chip memory . the ate loads the program into the processor's on-chip memory . during test_application , the processor executes this self-test program at its normal operational frequency , thereby achieving at-speed testing . test responses collected from the processor are stored in the on-chip data memory . finally , the ate unloads the test responses from the on-chip memory . modern microprocessors integrate large caches on the same die , which enables self-test_program execution from the on-chip cache , provided there is a cache loader mechanism to load the test_program and unload the test response . thus , by changing the external ate's role from actual test_application to that of an interface with the on-chip memory before and after the test , sbst achieves the goal of at-speed testing using low-speed , low_cost external ate . sbst is a scalable , portable , and reusable methodology for high_quality testing that incurs virtually zero performance , power , and circuit area_overhead . in addition , because the vehicles for applying sbst programs are existing processor instructions , at-speed testing is feasible without the risk of thermal damage due to excessive signal activity in special test modes . furthermore , by using the processor's instruction_set_architecture ( isa ) and complying with all the restrictions enforced by both the isa and the designers' decisions , sbst avoids overtesting for faults that don't appear during normal circuit operation and saves valuable yield . despite the significant advantages , however , most present forms of sbst represent a semi_automated approach that requires test engineer expertise for self-test_program development . modern commercial processors are characterized by a high level of complexity , and their architectural features introduce test_challenges that no single sbst methodology can effectively address . additionally , sbst methodologies have their individual advantages and 
digital_design with kp-lab kp-lab is an eu integrated project envisioning a learning system that facilitates innovative practices of sharing , creating and working with knowledge in education and workplaces . the project exploits a novel pedagogical view , the knowledge-creation metaphor of learning . according to such " trialogical " approach , cognition arises through collaborative_work in systematically developing shared " knowledge artefacts " , such as concepts , plans , material products , or social practices . the paper presents the plan of a pilot course to test the kp-lab methodologies and tools in the field of digital_design . 1 engineering education today in spite of the recent hype on new educational technologies and their potential for a novel approach on learning and acquiring knowledge , university education has remained the same that it has been since the first universities have been established , many centuries ago . the transmission model is still dominant and now , as in the middle age , students of higher_education take many years in acquisition-oriented and teacher-centered studies . technical education at a university level has a different history , since the first engineering schools in europe were established only during the nineteen century . for example , the origins of the university of genoa go back to the thirteen century , while its first technical school , the " royal_naval_school " has been established only in 1870 . before the diffusion of formal engineering studies , the practitioners got their training outside a classroom , usually by joining a community of people already engaged in the exercise of the profession , acquiring and sharing knowledge , information , processes and practices . a natural and deep_learning resulted from the deep interaction among people themselves and among people and the objects of their practice . the accelerated rate of technological innovation brought about by the industrial_revolution showed the limitations of this ageless mode of learning and prompted the birth of formal technical education . an engineering school is , in fact , more effective in transmitting knowledge at a fast pace and in providing the stronger theoretical background necessary for the new technologies . an unwanted casualty of this process has been , unfortunately , the active_learning mode typical of the former mode . engineering curricula are able to adjust their contents to support the new technologies , but are still based largely on frontal lectures , assembled into courses that provide fragmented pieces of knowledge and skills . there is hardly any explicit continuation from one course to another . in conclusion , today's engineering curricula embody in 
logical validation , answer merging and witness selection - a study in multi-stream question_answering the paper presents an approach to multi-stream question_answering ( qa ) using deep_semantic parsing and logical validation for filtering answer candidates . a robust entailment check is accomplished by embedding the prover in a relaxation loop . fallback strategies ensure a graceful degradation of performance in the case of parsing problems . the logical validity score is complemented by false_positive tests and heuristic quality indicators which also affect the selection of the most trusted answers . separate criteria are used for choosing a suitable 'witness' , i . e . a text passage which substantiates the answer . we present two experiments in which the method is applied for merging the results of various state-of-the-art qa systems . the evaluation demonstrates that the approach is applicable to heterogeneous qa streams in particular it improves results for a combination of precision-oriented and recall-oriented answer streams . the method automatically adapts to these characteristics of a qa system by learning parameters from a training sample . 
an information theoretic analysis of decision in computer_chess the basis of the method proposed in this article is the idea that information is one of the most important factors in strategic decisions , including decisions in computer_chess and other strategy games . the model proposed in this article and the algorithm described are based on the idea of a information theoretic basis of decision in strategy games . the model generalizes and provides a mathematical justification for one of the most popular search_algorithms used in leading computer_chess programs , the fractional ply scheme . however , despite its success in leading computer_chess applications , until now few has been published about this method . the article creates a fundamental basis for this method in the axioms of information_theory , then derives the principles used in programming the search and describes mathematically the form of the coefficients . one of the most important parameters of the fractional ply search is derived from fundamental principles . until now this coefficient has been usually handcrafted or determined from intuitive elements or data_mining . there is a deep , information theoretical justification for such a parameter . in one way the method proposed is a generalization of previous methods . more important , it shows why the fractional depth ply scheme is so powerful . it is because the algorithm navigates along the lines where the highest information gain is possible . a working and original implementation has been written and tested for this algorithm and is provided in the appendix . the article is essentially self-contained and gives proper background knowledge and references . the assumptions are intuitive and in the direction expected and described intuitively by great champions of chess . 
high_fidelity image_based modeling figure 1 : shaded and texture-mapped images of the reconstructions of a polynesian statue , a homo_heidelbergensis skull , resting on a circular cork stand , and an action_figure depicting a roman soldier . abstract this article presents a novel method for acquiring high_fidelity solid models of complex 3d shapes from multiple calibrated photographs . the proposed approach enforces both the photometric and geometric constraints associated with available image_data using a simple iterative deformation process . concretely , a wide-baseline stereo_matching technique based on mikolajczyk's and schmid's affine regions is first used to reconstruct a dense set of patches on the surface of the object of interest . next , the boundary of the object's visual hull is deformed to pass through the centers of the reconstructed patches and recover the surface's main structural features and concavities . fine surface details are finally reconstructed using a local refinement process that enforces smoothness , photometric , and geometric constraints at every vertex of the surface . the proposed approach has been implemented , and tested on 10 real datasets including objects with fine details , high-curvature areas , and deep concavities , and an object with little texture . qualitative and quantitative comparisons with models obtained by state-of-the-art image_based modeling algorithms and laser_range scanners are also presented . 
temperature_dependence of neutron-induced soft errors in srams available online xxxx a b s t r a c t we irradiated commercial srams with wide-spectrum neutrons at different temperatures . we observed that , depending on the vendor , the soft_error_rate either increases or slightly decreases with temperature , even in devices belonging to the same technology node . spice simulations were used to investigate the temperature_dependence of the cell feedback time and restoring current . the shape and magnitude of the particle-induced transient current is discussed as a function of temperature . the variability in the response is attributed to the balance of contrasting factors , such as cell speed reduction and increased diffusion with increasing temperature . soft_errors are a serious concern not only in space but also at sea_level , due to neutrons originating from the interactions of cosmic_rays with the atmosphere , and alpha_particles , coming from radioactive contaminants in the chip package and solder materials [1] . a large amount of work has been carried out in the field of soft_errors ( also known as single_event upsets , seu ) . experimental , simulation , and modeling efforts have led to a deep understanding of these phenomena , especially in sram cells , which , nowadays , are the most radiation-sensitive type of solid_state storage [1] . funneling phenomena have been discovered and modeled [2] . the role of the struck junction load on the induced transient has been elucidated [3] . neutron nuclear reactions leading to charged byproducts have been analyzed in the context of terrestrial soft_error_rate ( ser ) [4] . yet , despite the huge amount of work , some areas have received relatively little attention from the research_community . it is well known that electronic chips must operate at temperatures significantly_higher than room_temperature , especially for high_performance , space , or automotive applications . contrary to single_event latchup ( sel ) tests that are performed at the highest operating_temperature [5] , seu measurements are usually carried out at room_temperature [6] , and little is known about the seu temperature_dependence . only few works in the literature analyzed temperature effects on ser [7 9] . some conclude that temperature plays a marginal role in determining the critical charge in srams and , in general , the ser [7] . sexton et al . observed an increased seu sensitivity at high_temperature in hardened memories and attributed this behavior to variations in the conductivity of the feedback and load resistors [8] . truyen et al . performed tcad simulations of heavy-ion strikes in 180-nm 6-t cells as a function of temperature [9] . 
retrieval term prediction using deep_belief_networks this paper presents a method to predict retrieval terms from relevant/surrounding words or descriptive texts in japanese by using deep_belief_networks ( dbn ) , one of two typical types of deep_learning . to determine the effectiveness of using dbn for this task , we tested it along with baseline methods using example-based_approaches and conventional machine_learning methods , i . e . , multi_layer perceptron ( mlp ) and support_vector_machines ( svm ) , for comparison . the data for training and testing were obtained from the web in manual and automatic manners . automatically created pseudo data was also used . a grid search was adopted for obtaining the optimal hyper-parameters of these machine_learning methods by performing cross_validation on training_data . experimental_results showed that ( 1 ) using dbn has far higher prediction precisions than using baseline methods and higher prediction precisions than using either mlp or svm; ( 2 ) adding automatically gathered data and pseudo data to the manually gathered data as training_data is an effective measure for further improving the prediction precisions; and ( 3 ) dbn is able to deal with noisier training_data than mlp , i . e . , the prediction precision of dbn can be improved by adding noisy training_data , but that of mlp cannot be . 
genome_wide generalized additive models 13 chromatin immunoprecipitation followed by deep_sequencing ( chip_seq ) is a widely used 14 approach to study protein-dna interactions . to analyze chip_seq data , practitioners are 15 required to combine tools based on different statistical assumptions and dedicated to spe-16 cific applications such as calling protein occupancy peaks or testing for differential occu-17 pancies . here , we present genogam ( genome_wide generalized additive model ) , which 18 brings the well-established and flexible generalized additive models framework to genomic 19 applications using a data parallelism strategy . we model chip_seq read count frequencies 20 as products of smooth functions along chromosomes . smoothing parameters are estimated 21 from the data eliminating ad_hoc binning and windowing needed by current approaches . 
fault simulation of digital_circuits at register_transfer_level as the complexity of very_large_scale_integration ( vlsi ) is growing , testing becomes tedious and tougher . as of now fault_models are used to test digital_circuits at the gate level or below that level . by using fault_models at the lower levels , testing becomes cumbersome and will lead to delays in the design_cycle . in addition , developments in deep_submicron_technology provide an opening to new defects . we must develop efficient fault detection and location methods in order to reduce manufacturing costs and time to market . thus there is a need to look for a new approach of testing the circuits at higher_levels to speed up the design_cycle . this paper proposes on register_transfer_level ( rtl ) modeling for digital_circuits and computing the fault coverage . the result obtained through this work establishes that the fault coverage with the rtl fault_model is comparable to the gate level fault_coverage . 
screening vdsm outliers using nominal and subthreshold supply_voltage iddq very deep sub-micron ( vdsm ) defects are resolved as statistical_post_processing ( spp ) outliers of a new iddq screen . the screen applies an iddq pattern once to the device under test ( dut ) and takes two quiescent_current measurements . the quiescent_current measurements are taken at nominal and at subthreshold supply_voltages . the screen is demonstrated with 0 . 18 m and 0 . 13 m volume data . the screen's effectiveness is compared to stuck-at and other iddq screens . 1 . introduction this paper describes a new , two-measurement iddq_test scheme called stiddq . the screen applies an iddq pattern once to the device under test ( dut ) but takes two quiescent_current measurements . the first iddq measurement is taken at or above the nominal supply_voltage and the second at a supply_voltage less than 2vt . similar to statistical_post_processing methods , the ate collects the raw-data and pass/fail is determined off-tester . the paper shows that the two-measurement iddq yields an outlier screen that is suitable for very-deep_sub_micron_technologies . the paper presents results for a 0 . 18 m asic and a 0 . 13 m large die , low yield asic . 
yafima : yet another frequent itemset mining algorithm efficient discovery of frequent_patterns from large databases is an active research area in data_mining with broad applications in industry and deep_implications in many areas of data_mining . although many efficient frequent-pattern_mining techniques have been developed in the last decade , most of them assume relatively small databases , leaving extremely large but realistic datasets out of reach . a practical and appealing direction is to mine for closed or maximal itemsets . these are subsets of all frequent_patterns but good representatives since they eliminate what is known as redundant patterns . the practicality of discovering closed or maximal itemsets comes from the relatively inexpensive process to mine them in comparison to finding all patterns . in this paper we introduce a new approach for traversing the search_space to discover all frequent_patterns , the closed or the maximal patterns efficiently in extremely large datasets . we present experimental_results for finding all three types of patterns with very large database sizes never reported before . our implementation tested on real and synthetic data shows that our approach outperforms similar state-of-the-art algorithms by at least one order_of_magnitude in terms of both execution time and memory usage , in particular when dealing with very large databases . 
automated testing of planning models automated planning systems ( aps ) are maturing to the point that they have been used in experimental mode on both the nasa deep_space_1 spacecraft and the nasa earth orbiter 1 satellite . one challenge is to improve the test_coverage of aps to ensure that no unsafe plans can be generated . unsafe plans can cause wasted resources or damage to hardware . model_checkers can be used to increase test_coverage for large complex distributed_systems and to prove the absence of certain types of errors . in this work we have built a generalized tool to convert the input models of an aps to promela , the modeling_language of the spin model_checker . we demonstrate on a mission sized aps input model , that we with spin can explore a large part of the space of possible plans and verify with high probability the absence of unsafe plans . 
northern gulf_of_mexico stock 98 96 94 92 90 88 86 84 82 80 98 96 94 92 90 88 86 84 82 80 23 25 27 29 31 33 23 25 27 29 31 33 tx la ms al fl ga tx la ms al fl ga figure 1 . distribution of atlantic_spotted dolphin sightings from sefsc spring and fall vessel surveys during 1996-2001 . all the on-effort sightings are shown , though not all were used to estimate abundance . solid lines indicate the 100 m and 1000 m isobaths and the dotted line shows the offshore extent of the u . stock definition and geographic range the atlantic_spotted dolphin is endemic to the atlantic_ocean in temperate to tropical waters ( perrin et al . 1987 , 1994 ) . in the gulf_of_mexico , atlantic_spotted dolphins occur primarily from continental_shelf waters 10-200 m deep to slope waters <500 m deep ( fulling et al . 2003; mullin and fulling , in review ) . this species has also been reported around oceanic islands and far offshore in other areas ( perrin et al . 1994 ) . atlantic_spotted dolphins were seen in all seasons during gulfcet aerial surveys of the northern gulf_of_mexico from 1992 to 1998 ( hansen et al . 1996; mullin and hoggard 2003 ) . it has been suggested that this species may move inshore seasonally during spring , but data supporting this hypothesis are limited ( caldwell and caldwell 1966; fritts et al . 1983 ) . in a recent study , bero ( 2001 ) presented strong genetic support for differentiation between gulf_of_mexico and western north_atlantic management stocks using both mitochondrial and nuclear markers . however , this study did not test for further population subdivision with the gulf_of_mexico . perrin et al . ( 1994 ) suggested that island and offshore form of the atlantic_spotted dolphin may be a different stock from those occurring on the continental_shelf . however , the atlantic_spotted dolphin has not been sighted in the deep_waters of the northern gulf_of_mexico ( mullin and fulling , in review ) . population size estimates of abundance were derived through the application of distance sampling analysis ( buckland et al . 2001 ) and the computer program distance ( thomas et al . 1998 ) to sighting data . from 1991 through 1994 , line-transect vessel surveys were conducted during spring in the northern gulf_of_mexico from the 200 m isobath to the seaward extent of the u . s . exclusive_economic_zone ( eez ) ( hansen et al . 1995 ) . survey effort-weighted estimated average abundance of atlantic_spotted dolphins 
waveform analysis of uwb gpr antennas ground_penetrating_radar ( gpr ) systems fall into the category of ultra_wideband ( uwb ) devices . most gpr equipment covers a frequency_range between an octave and a decade by using short-time pulses . each signal recorded by a gpr gathers a temporal log of attenuated and distorted versions of these pulses ( due to the effect of the propagation medium ) plus possible electromagnetic interferences and noise . in order to make a good interpretation of this data and extract the most possible information during processing , a deep knowledge of the wavelet emitted by the antennas is essential . moreover , some advanced processing techniques require specific knowledge of this signal to obtain satisfactory results . in this work , we carried out a series of tests in order to determine the source wavelet emitted by a ground-coupled antenna with a 500 mhz central frequency . 
precision dsn radiometer systems : impact on microwave calibrations precision dsn radiometer systems : impact on microwave calibrations the nasa_deep_space_network ( dsn ) has a long history of providing large parabolic dish antennas with precision surfac%s , low-loss feeds and ultra_low noise amplifiers for deep_space telecommunications . to realize the benefits of high sensitivity , it is important that receiving systems are accurately calibrated and monitored to maintain peak performance . a method is described to measure system performance and to calibrate the receiving system using procedures , software and commercial instruments that are easy to implement and efficient to use , the utillity of the measurement procedures and the precision of the receiver calibration technque were demonstrated by performing tests at ka_band ( 32 and 33 . 68 ghz ) frequencies at goldstone on a 34-m beam-waveguide antenna observations of multiple calibration radio sources are used to measure the dependence of antenna_gain and system noise_temperature on source elovat " km and derive the peak value . receiving system non-linearities are frequently overlooked as an error source in the calibration of microwave radiometers . 1 he experimental resutts described in this paper illustrate some of the ways that receiving system non-linearity can negatively impact system performance . a simple radiometer calibration technque and analysis provide quantitative information that enables the system engineer to adjust and linearize the receiving system . when that is not practical , tha experimenter or the operator can apply correction coefficients to the measured values of system noise_temperature and thereby compensate for the receiving system non-linearity . the hgh performance antennas and the sensitive receiving systems of the dsn are valuablo resources for scientific research in addition to the primary telecommun'k , ation tasks that support spaoe missions . the antenna_gain and system noise_temperature measurements and the radiometer calibration method described in this paper are also useful to perform precision research experiments . 
enhanced launch-off-capture transition_fault testing with today's design size in millions of gates and working frequency in gigahertz range , timing_related_defects are high proportion of the total chip defects and at-speed test is crucial . the transition_fault model is widely used for detecting delay-induced defects . there are two transition_fault pattern_generation methods; i . e . launch-off-shift ( los ) and launch-off-capture ( loc ) . in los , the transition is launched during the last shift cycle from the scan_path ( non-functional ) . the scan_enable ( sen ) is high during the last shift and must go low to enable response capture during the capture cycle . the time period for sen to make this transition corresponds to the functional frequency . this is not applicable for very low_cost ate , which have a limitation of one at-speed signal port . in loc method , the at-speed constraint on the sen signal is relaxed and the transition is launched from the functional path . the controlla-bility of launching a transition at the target gate is less as it depends on the functional response of the circuit under test to the initialization vector . a novel scan_based at-speed test is proposed in which a transition can be launched either from the scan_path or the functional path . the technique improves the controllability of transition_fault testing and it does not require the scan_enable to change at-speed . the scan_enable control information is encapsulated in the test data and transferred during the scan operation to generate the local scan_enable signals during the launch and capture cycle . a new scan cell , referred to as local scan_enable generator ( lseg ) , is inserted in the scan_chains to generate the local scan_enable signals . the proposed technique is robust , practice-oriented and suitable for designs targeted for very low_cost ates . i . introduction the semiconductor industry is adopting new fabrication processes to meet the area , power and performance requirements . as a result , modern ics are growing more complex in terms of gate_count and operating_frequency [1] . the deep-submicron ( dsm ) effects are becoming more prominent with shrinking technology , thereby increasing the probability of timing_related_defects [2] [3] . for dsm designs , the stuck_at_fault test alone cannot ensure high_quality level of chips . in the past , functional patterns were used for at-speed test . however , functional_testing is not a viable solution because of the difficulty and time to generate these tests for complex designs with very high gate density . moreover , the number of required patterns 
leveraging infrastructure_ip for soc yield in addition to the functional ip_cores , today's soc necessitates embedding a special family of ip_blocks , called infrastructure_ip blocks . these are meant to ensure the manufacturability of the soc and to achieve adequate levels of yield_and_reliability . the infrastructure_ip leverages the manufacturing knowledge and feeds back the information into the design phase . this keynote address analyzes the key trends and challenges resulting in manufacturing susceptibility and field reliability that necessitate the use of such infrastructure_ip . then , it concentrates on certain examples of such embedded ips for detection , analysis and correction . 1 . introduction this every new semiconductor_technology node provides further miniaturization and higher performance , thus increasing the functions that electronic products could offer . although adding such new functions do benefit the end_user , but they also necessitate finer and denser semiconductor fabrication processes , which make chips more susceptible to defects . today's very deep-submicron semiconductor technologies of 130 nanometers and below are reaching defect susceptibility levels that result in lowering the manufacturing_yield and reliability , and hence lengthening the production ramp-up period , and therefore the time to volume ( ttv ) . the very deep submicron impact on yield , reliability and ttv is very critical for the semiconductor industry . it puts the conventional ic realization flow at an impasse . in fact , every single phase in the ic realization flow impacts yield_and_reliability . this includes the design phase , prototyping or production ramp up , volume fabrication , test , assembly , packaging , and even the post_production life cycle of the chip . in order to optimize yield and reach acceptable ttv levels , the semiconductor industry needs to adopt advanced yield optimization solutions . these solutions need to be implemented at different phases of the chip realization flow . the conventional semiconductor_manufacturing infrastructure , i . e . the external equipment and processes , alone are insufficient to handle such advanced yield optimization solutions; supplemental on-chip infrastructure is needed . to optimize yield_and_reliability , the industry has recently introduced a range of embedded intellectual_property ( ip ) blocks . hence , in addition to the functional ip_cores , today's soc necessitates embedding a special family of ip_blocks , called infrastructure_ip blocks . these are meant to ensure the manufacturability of the soc and to achieve adequate levels of yield_and_reliability . the infrastructure leverages the manufacturing knowledge and feeds back the information into the design phase . this embedded tutorial analyzes the key trends and challenges resulting in manufacturing susceptibility and field reliability that necessitate the use 
a comparison of usability methods for testing interactive health technologies : methodological aspects and empirical_evidence objective usability_evaluation is now widely recognized as critical to the success of interactive health_care applications . however , the broad range of usability inspection and testing_methods available may make it difficult to decide on a usability assessment plan . to guide novices in the human_computer_interaction field , we provide an overview of the methodological and empirical_research available on the three usability inspection and testing_methods most often used . methods we describe two 'expert-based' and one 'user-based' usability method : ( 1 ) the heuristic evaluation , ( 2 ) the cognitive walkthrough , and ( 3 ) the think aloud . results all three usability_evaluation methods are applied in laboratory settings . heuristic evaluation is a relatively efficient usability_evaluation method with a high benefit-cost ratio , but requires high skills and usability experience of the evaluators to produce reliable results . the cognitive walkthrough is a more structured approach than the heuristic evaluation with a stronger focus on the learnability of a computer application . major drawbacks of the cognitive walkthrough are the required level of detail of task and user background descriptions for an adequate application of the latest version of the technique . the think aloud is a very direct method to gain deep insight in the problems end_users encounter in interaction with a system but data analyses is extensive and requires a high level of expertise both in the cognitive ergonomics and in computer system application domain . discussion and conclusions each of the three usability_evaluation methods has shown its usefulness , has its own advantages_and_disadvantages; no single method has revealed any significant results indicating that it is singularly effective in all circumstances . a combination of different techniques that compliment one another should preferably be used as their collective application will be more powerful than applied in isolation . innovative mobile and automated solutions to support end_user testing have emerged making combined approaches of laboratory , field and remote usability evaluations of new health_care applications more feasible . 
recent_developments in large vocabulary continuous_speech_recognition this paper overviews a series of recent approaches to front_end processing , acoustic_modeling , language modeling , and back-end search and system combination which have made contributions for large vocabulary continuous_speech_recognition ( lvcsr ) systems . these approaches include the feature transformations , speaker-adaptive features , and discriminative features in front_end processing , the feature_space and model-space discriminative_training , deep_neural_networks , and speaker adaptation in acoustic_modeling , the backoff smoothing , large-span modeling , and model regularization in language modeling , and the system combination , cross-adaptation , and boosting in search and system combination . some future directions for lvcsr research are also addressed . i . introduction over the past decade , several advances have been made to the design of modern lvcsr systems to the point where their application has broadened from early speaker-dependent dictation systems to speaker-independent automatic broadcast news transcription and indexing , lectures and meetings transcription , conversational telephone speech transcription , open-domain voice search , medical and legal speech_recognition and call center applications to name a few . the commercial success of these systems is an impressive testimony to how far research in lvcsr has come and the aim of this paper is to describe some of the technological underpinnings of modern systems . it must be said however that , despite the commercial success and widespread adoption , the problem of large vocabulary speech_recognition is far from being solved : background noise , channel distortions , foreign accents , casual and disfluent speech or unexpected topic change can cause automated systems to make egregious recognition errors . this is because current lvcsr systems are not robust to mismatched training_and_test conditions and cannot handle context as well as human listeners despite being trained on thousands of hours of speech and billions of words of text . technological improvements have been made in four components of an lvcsr system : front_end processing , acoustic_modeling , language modeling , hypothesis search and system combination . a comprehensive survey of early lvcsr systems was presented in [35] . the state of the art in lvcsr has shifted considerably since then through the advent of powerful speaker adaptation , discriminative_training and language modeling techniques . this paper reports some advanced developments which are a substantial step toward making a number of high-utility applications possible [28] . 
decoding the value of computer_science in the social_network , a computer-programming prodigy goes to harvard and creates a technology company in his sophomore dorm . six year later , the company is worth billions and touches one out of every 14 people on earth . facebook is a familiar american success story , with its founder , mark_zuckerberg , following a path blazed by bill gates and others like him . but it may also become increasingly rare . far fewer students are studying computer_science in college than once did . this is a problem in more ways than one . the signs are everywhere . this year , for the first time in decades , the college_board failed to offer high_school students the advanced_placement ab computer_science exam . the number of high_schools teaching computer_science is shrinking , and last year only about 5 , 000 students sat for the ab test . two decades ago , i was one of them . i have never held an information_technology job . yet the more time passes , the more i understand how important that education was . something is lost when students no longer study the working of things . my childhood interest in programming was a product of nature and nurture . my father worked as a computer scientist , first in a university and then as a researcher for general_electric . as a kid , i tagged along to his lab on weekends , watching him connect single-board dec computers into ring networks and two-dimensional arrays , feeling the ozone hum of closet-sized machines . by my adolescence , in the mid-1980s , we had moved to a well-off suburb whose high_school could afford its own mainframe . that plus social awkwardness meant many a night plugged into a 300-baud modem , battling other 15-year-olds in rudimentary deep_space combat and accumulating treasure in ascii-rendered dungeons without end . before long i wanted to understand where those games came from and how , exactly , they worked . so i took to programming , first in basic and then pascal . coding taught me the shape of logic , the value of brevity , and the attention to detail that debugging requires . i learned that a beautiful program with a single misplaced semicolon is like a sports_car with one piston out of line . both are dead machines , functionally indistinguishable from junk . i learned that you are good enough to build things that do what they are supposed to do , or you are not . i left for college intending to major in computer science . that lasted until about 
exploiting scene_context for image captioning this paper presents a framework for image captioning by exploiting the scene context . to date , most of the captioning models have been relying on the combination of convolutional_neural_networks ( cnn ) and the long-short_term_memory ( lstm ) model , trained in an end_to_end fashion . recently , there has been extensive research towards improving the language_model and the cnn architecture , utilizing attention mechanisms , and improving the learning_techniques in such systems . a less studied area is the contribution of the scene context in the captioning . in this work , we study the role of the scene context , consisting of the scene type and objects . to this end , we augment the cnn features with scene_context features , including scene detectors , objects and their localization , and their combinations . we use the scene context features as an initialization feature at the zeroth time step in a lstm model with deep residual connections . in subsequent time steps , the model , however , uses the original cnn features . the proposed language_model , contrary to more conventional ones , thus has access to visual features through the whole process of sentence generation . we demonstrate that the scene context features affect the language formation and improve the captioning results in the proposed framework . we also report results from the microsoft coco benchmark , where our model achieves the state-of-the-art performance on the test set . 
capture_power-aware test data_compression using selective encoding keywords : test_compression low_power testing scan_based testing a b s t r a c t ever-increasing test_data_volume and excessive test power are two of the main concerns of vlsi testing . the ''don't-care'' bits ( also known as x-bits ) in given test cube can be exploited for test data_compression and/or test power_reduction , and these techniques may contradict to each other because the very same x-bits are likely to be used for different optimization objectives . this paper proposes a capture_power-aware test_compression scheme that is able to keep capture_power under a safe limit with low test compression_ratio loss . experimental results on benchmark_circuits validate the effectiveness of the proposed solution . the test_data_volume for today's very large_scale integrated ( vlsi ) circuits has been exploding with the ever-increasing integration capability of semiconductor_technology [1] . in addition , besides the test_vectors targeting traditional stuck_at_faults , test_patterns targeting delay_faults and many other subtle faults are becoming essential to improve test_quality for deep submi-cron designs . large test_data_volume not only raises memory depth requirements for the automatic_test_equipment ( ate ) , but also prolongs ics' testing time , thus significantly increasing test_cost . to address this issue , various test_compression techniques [2 24] have been proposed in the literature [25] , and most of them exploited the ''don't-care'' bits ( also known as x-bits ) in given test cubes for effective test_compression . 1 generally speaking , the more x-bits in test cubes , the higher the test compression_ratio can be achieved . at the same time , power_dissipation during scan_based testing of vlsi_circuits can be significantly_higher than that during normal_operation [26] . elevated average test power , dominated by scan shift-power may cause structural damage to the circuit under test ( cut ) ; while excessive peak test power in the capture phase is likely to cause good circuit to fail test , thus leading to unnecessary yield_loss [27] . there is a rich literature on reducing test power in shift mode , in which design_for_testability ( dft ) based_methods such as scan_chain partitioning technique [28 33] are very effective ( when compared to x-filling techniques such as [34] ) . compared to shift-power , yield_loss caused by excessive capture_power has become a more serious concern with technology scaling . there are , however , no such effective dft-based_techniques for capture_power reduction , and we mainly resort to x-filling techniques ( e . g . , [35 39] ) to reduce the excessive capture_power in scan_based testing . there is usually 
risk_assessment of software-system specifications summary & conclusions this paper presents a methodology and an example of risk_assessment of functional-requirement specifications for complex real-time software systems . a heuristic risk_assessment technique based on cpn ( colored petri_net ) models is presented . this technique is used to classify software functional-requirement specification components according to their relative importance in terms of such factors as severity and complexity . a dynamic complexity measure , based on concurrence in the functional_requirements , is introduced . this technique is applied on the earth operation commanding center ( eoc commanding ) , a large component of the nasa earth observing system ( eos ) project . two specification models of the system are considered . results of applying this technique to both models are presented . the risk_assessment methodology in this paper suggests the following conclusions : risk_assessment at the functional-requirement specification phase can be used to classify functional_requirements in terms of their complexity & severity . the methodology identifies high-risk functional specification components that require appreciable development & verification resources during design , implementation , and testing . dynamic complexity metrics and the concurrence metric ( introduced in this paper ) can important in assessing the risk factors based on the complexity of functional specifications . the concurrence complexity metric ( introduced in this paper ) is an important aspect of dynamic complexity . cpn models can be used to build an executable specification of the system , which helps the analyst not only to acquire deep understanding of the system but also to study the dynamic behavior of the system by simulating the model . future_research in early risk_assessment and complexity analysis could focus on : 1 ) software_architectures based on object technology : the technique in this paper , with some modifications on complexity analysis and severity analysis , applies to the design_methods and software_architectures based on object technology . further research is required to establish the complexity metrics for object-based systems . 2 ) sre ( software reliability_engineering ) : one of the main tasks in sre is designing the operational profiles . operational profiles are built according to the user_profile and the understanding of the system analyst/designer . these profiles can be used for estimating the system reliability at the early phases of development . results obtained from this analysis can be incorporated into sre for conducting reliability analysis at the analysis/design phases , based on dynamic simulation . more research is needed to establish a method for incorporating the risk_assessment method within the sre process . acronyms 1 aoi accept operator input commands bsrc 
batch mode sparse active_learning sparse representation , due to its clear and powerful insight deep into the structure of data , has seen a recent surge of interest in the classification community . based on this , a family of reliable classification methods have been proposed . on the other hand , obtaining sufficiently labeled training_data has long been a challenging problem , thus considerable research has been done regarding active selection of instances to be labeled . in our work , we will present a novel unified framework , i . e . bmsal ( batch mode sparse active_learning ) . based on the existing sparse family of classifiers , we define rigorously the corresponding bmsal family and explore their shared properties , most importantly ( approximate ) submodu-larity . we focus on the feasibility and reliability of the bmsal family : the first one inspires us to optimize the algorithms and conduct experiments comparing with state-of-the-art methods; for reliability , we give error-bounded algorithms , as well as detailed logical deductions and empirical tests for applying sparse in non-linear data_sets . 
review of "exploiting software : how to break code by greg hoglund and gary mcgraw" a book with a dark-gray hat on its cover and the subtitle " how to break code " makes a strong statement . it does not disappoint . it covers many form of exploiting software that you never dared to explore . the book approaches its problem from many security disciplines . it takes on the reverse_engineering angle to break copyright_protection systems or to find software defects . it takes the pentest ( penetration_test ) view when it explores how to attack server-side software , with local and remote attack options . it describes the botnet ( robot network ) master's options when it targets client software problems . it shows how to hide malware ( malicious_software ) via the rootkit approach , diving deep even into flash_memory and evading forensic analysis . the authors also present more conceptual views , such as the root cause of software security problems , 49 attack_patterns , how to craft malicious input , and buffer overflows in all variations . each topic includes a tutorial , sample systems or code , and known exploits using these techniques . if the topic is unfamiliar , the tutorial may be insufficient , but links to further information are provided . the sample code is clear enough to allow smarter scripters to elaborate on it . there are not many details on the known exploits , but a simple web_search on any of the key terms will provide all that are necessary . the book is about 450 pages , and contains eight chapters . the three longest chapters are on reverse_engineering , buffer_overflow , and rootkits . the others are on software , attack_patterns , exploiting server software , and exploiting client software who should read this book ? the authors start by defending why anyone would write such a book . they show that anything they describe has been exploited already . they spell out how it was done , loud and clear . this takes away ignorance . so , if your job is to build secure software systems , to implement license or copyright_protection systems , to pentest systems , or to do forensic analysis , you will benefit from reading the book . ed felten , princeton_university professor of computer science , is quoted on the cover : " it's hard to protect yourself if you don't know what you're up against . " but having that knowledge , after reading this book , may not improve your peace of mind . a . mari n passin , a principal systems_engineer specializing in data_modeling , web_databases , and xml projects . thus , it is not unexpected to find that he covers 
extrapolation of pile capacity from non-failed load tests static pile load test to failure is the ultimate procedure available to examine the capacity and integrity of deep foundations . being expensive and time consuming , the procedure is often substituted for the application of a load to a certain factor ( most often two ) times the contemplated design load . in fact , only a proof test is carried out while the ultimate_capacity and actual factor of safety remain unknown . this procedure results in uneconomic foundation solution , unknown capacity when modifications are required and inability of the engineer to gain insight of the controlling mechanism for improved design . the described state of practice calls for the ability to reliably estimate the ultimate pile capacity for non-failed load tests . a practical analytical method is proposed , capable of extrapolating the measured load-settlement relations beyond the maximum tested load . the proposed procedure along with two other possible methods are evaluated . the procedures are examined through a data base of 63 piles load tested to failure . loading is assumed to be known for only 25% , 33% , 50% , 75% and 100% of the actual ultimate_capacity . the " known " data is then extrapolated using the different methods and the obtained ultimate_capacity is compared to the actual measurements . for consistency , only one failure criterion ( davidson ) is applied . the obtained results are analyzed statistically to evaluate the accuracy and reliability of the proposed methods of analysis . it is shown that the accuracy of the proposed method is 0 . 99 0 . 21 ( 1s . d . ) , 0 . 96 0 . 27 , 0 . 87 0 . 3 . and 0 . 78 0 . 33 when assuming 75% , 50% , 33% and 25% of the ultimate_capacity to be known , respectively . the obtained results for the 63 data base cases suggest that even when the predicted ultimate_capacity is four times the maximum actual tested load , the associated risk is zero for exceeding the design load , when using the extrapolated value with a factor of safety of 2 . 0 . case history analyses of six load-tested piles at two sites are presented . the analyzed cases indicate possible substantial savings when the ultimate_capacity well exceeds the maximum applied load . moreover , the method already demonstrates its enormous importance from both aspects , engineering and financial . 
closer to deep underwater science with odyssey iv class hovering autonomous_underwater_vehicle ( hauv ) closer to deep underwater science with odyssey w class hovering autonomous_underwater_vehicle ( hauv ) the autonomous_underwater_vehicle laboratory ( auvlab ) at the massachusetts_institute_of_technology ( mit ) is currently building and testing a new , general_purpose and inexpensive 6000 meter capable hovering autonomous_underwater_vehicle ( hauv ) , the 'odyssey iv class' . the vehicle is intended for rapid deployments , potentially with minimal navigation , thus supporting episodic dives for exploratory missions . for that , the vehicle is capable of fast dive times , short survey on bottom and simple navigation . 
fdm : a graph_based statistical method to detect differential_transcription using rna_seq data motivation in eukaryotic_cells , alternative_splicing expands the diversity of rna transcripts and plays an important role in tissue-specific differentiation , and can be misregulated in disease . to understand these processes , there is a great need for methods to detect differential_transcription between samples . our focus is on samples observed using short-read rna sequencing ( rna_seq ) . methods we characterize differential_transcription between two samples as the difference in the relative abundance of the transcript isoforms present in the samples . the magnitude of differential_transcription of a gene between two samples can be measured by the square_root of the jensen shannon divergence ( jsd* ) between the gene's transcript abundance vectors in each sample . we define a weighted splice-graph representation of rna_seq data , summarizing in compact form the alignment of rna_seq reads to a reference_genome . the flow difference metric ( fdm ) identifies regions of differential rna transcript expression between pairs of splice graphs , without need for an underlying gene model or catalog of transcripts . we present a novel non-parametric statistical test between splice graphs to assess the significance of differential_transcription , and extend it to group-wise comparison incorporating sample replicates . results using simulated rna_seq data consisting of four technical replicates of two samples with varying transcription between genes , we show that ( i ) the fdm is highly correlated with jsd* ( r=0 . 82 ) when average rna_seq coverage of the transcripts is sufficiently deep; and ( ii ) the fdm is able to identify 90% of genes with differential_transcription when jsd* >0 . 28 and coverage >7 . this represents higher sensitivity than cufflinks ( without annotations ) and rdiff ( mmd ) , which respectively identified 69 and 49% of the genes in this region as differential transcribed . using annotations identifying the transcripts , cufflinks was able to identify 86% of the genes in this region as differentially transcribed . using experimental_data consisting of four replicates each for two cancer cell lines ( mcf7 and sum102 ) , fdm identified 1425 genes as significantly different in transcription . subsequent study of the samples using quantitative real time polymerase_chain_reaction ( qrt-pcr ) of several differential_transcription sites identified by fdm , confirmed significant differences at these sites . availability http : //csbio-linux001 . cs . unc . edu/nextgen/software/fdm contact : darshan@email . unc . edu supplementary information supplementary data are available at bioinformatics online . 
high_frequency power transformer measurement and modeling transformer design example figure_1_shows a simple 1 : 1 transformer . the transformer uses an ungapped epc-25 core from tdk , made from pc-44 material . this transformer was designed for use in a 60 w forward converter with 36-60 v input and 12 v output . figure 2 shows the winding layout , with just a single_layer of 18 turns for the primary winding , a layer of thin insulation tape , and a single_layer of 18 turns for the secondary winding . this is a very straightforward , easy-to-manufacture design of a two-winding transformer . however , as you will see below , the resulting circuit element created is anything but simple . transformer model figure 3 shows a commonly-used model for a two-winding transformer . on the primary side , the winding resistance is represented by r p , the leakage inductance by l lk , magnetizing inductance by l m , core loss by r c , and self-capacitance by c p . the secondary winding resistance is r s , the secondary self-capacitance is c s , and the primary to secondary capaci-tance is c m . the elements of this transformer model are used for several purposes-characterizing components , identifying problem design areas , and circuit_simulation . however , this apparently simple model is complicated by the fact that all of the resistors and inductors of the model are nonlinear functions of either frequency , or excitation level , or both . the capacitors can also exhibit minor nonlinearities , but are further complicated by the fact that the lumped elements shown_in_fig . 3 are a very crude approximation to the true multiple interwinding capacitance effects that really exist in the component . transformer frequency_response measurements it is very useful to make frequency_response measurements on a high_frequency power transformer , using a wide range of frequencies . for a two-winding transformer such as the example above , the most common measurements are impedance measurements from the primary side , with the secondary both open-circuited , and short-circuited . fig . 4 shows a typical test setup for impedance measurements , using the ap200 frequency_response analyzer . more detail of this test can be found in [1] . while many manufacturers only despite efforts from some magnetics vendors to provide off-the-shelf components to power_supply designers , almost all high_performance magnetics are custom . there are many deep and complex issues involved in the design of magnetics . i will try to cast some light on just a few of these issues . this article points out some 
revisiting ucs : description , fitness_sharing , and comparison with xcs this paper provides a deep insight into the learning mechanisms of ucs , a learning_classifier_system ( lcs ) derived from xcs that works under a supervised_learning scheme . a complete description of the system is given with the aim of being useful as an implementation guide . besides , we review the fitness computation , based on the individual accuracy of each rule , and introduce a fitness_sharing scheme to ucs . we analyze the dynamics of ucs both with fitness_sharing and without fitness_sharing over five binary-input problems widely used in the lcss framework . also xcs is included in the comparison to analyze the differences in behavior between both systems . results show the benefits of fitness_sharing in all the tested problems , specially those with class imbalances . comparison with xcs highlights the dynamics differences between both systems . 
center for reliable computing elf35 experiment -chip and experiment design elf35 experiment -chip and experiment design a test chip has been designed and manufactured to evaluate the effectiveness of different test techniques for deep_submicron_technologies . the test_chip uses lsi logic g10p 0 . 35 m cell-based technology . it has approximately 265k lsi logic equivalent gates . there are six types of circuits-under-test ( cuts ) . two cuts are arithmetic processors , which perform the same function but were implemented in different ways . these two cuts are full-scan sequential_circuits . the other four are combinational circuits . three of these four combinational cuts are datapath circuits . the other one is a translator that maps a pseudo_random sequence into a binary sequence . the tests include stuck_at_fault , delay_fault , transition_fault , design_verification , pseudo_random , weighted-random , and i_ddq tests . the i ddq test_sets were generated based on various fault_models , such as pseudo-stuck-at and bridging_faults . a built-in-self-test ( bist ) circuitry was implemented for one datapath cut . emulated bist test_sets for some cuts will also be used . this report describes the chip design and the test applied . future reports will describe the experimental results and data_analysis . abstract a test chip has been designed and manufactured to evaluate the effectiveness of different test techniques for deep_submicron_technologies . the test_chip uses lsi logic g10p 0 . 35 m cell-based technology . it has approximately 265k lsi logic equivalent gates . there are six types of circuits-under-test ( cuts ) . two cuts are arithmetic processors , which perform the same function but were implemented in different ways . these two cuts are full-scan sequential_circuits . the other four are combinational circuits . three of these four combinational cuts are datapath circuits . the other one is a translator that maps a pseudo_random sequence into a binary sequence . the tests include stuck_at_fault , delay_fault , transition_fault , design_verification , pseudo_random , weighted-random , and i_ddq tests . a built-in-self-test ( bist ) circuitry was implemented for one datapath cut . emulated bist patterns for some cuts will also be used . this report describes the chip design and the test applied . future reports will describe the experimental results and data_analysis . 
building deep dependency structures using a wide-coverage ccg parser this paper describes a wide-coverage statistical parser that uses combinatory categorial grammar ( ccg ) to derive dependency structures . the parser differs from most existing wide-coverage tree-bank parsers in capturing the long_range dependencies inherent in constructions such as coordination , extraction , raising and control , as well as the standard local predicate_argument dependencies . a set of dependency structures used for training_and_testing the parser is obtained from a treebank of ccg normal-form derivations , which have been derived ( semi- ) automatically from the penn treebank . the parser correctly recovers over 80% of labelled dependencies , and around 90% of unlabelled dependencies . 
current-based testing for deep_submicron vlsis - design & test of computers , ieee recently published articles have questioned the ability to carry out effective current-based testing ( i ddx ) for deep-micron vlsis . 1-5 yet , current-based test_methods for such devices are more relevant than ever . the probability of a defect occurring increases exponentially as its size decreases . as the technology scales , even smaller defects may become potential threats to yield . furthermore , ensuring gate oxide quality and reliability for a multimillion-transistor device under test ( dut ) solely through voltage may become unrealistic . other techniques , such as burn-in , although particularly successful for memories , might not be economically viable for most commercial digital products . several recent_studies have raised concerns about new failure_mechanisms in scaled geometries that may be more difficult to detect with conventional means . nigh et al . reported the existence of many timing-only failures . these failures did not influence the circuit logic functionality; hence , slow-speed sa-based ( stuck_at_fault ) or functional tests did not detect them . 6 similarly , for intel's manufacturing processes , needham et al . reported an increasing shift toward " soft defects " as technology migrated from 0 . 35 to 0 . 25 microns . 7 these defects do not always cause failures at all conditions of temperature and voltage . according to needham et al . , defects correlate with long_term device reliability . these defects may be due to resistive vias , highly resistive_bridging defects , and so on . i ddq testing can detect some defects , provided background leakages are under control and circuits are designed to make them i_ddq testable . traditionally , voltage testing and i ddq testing have had complementary objectives . in logic testing , the stress is on dut logic correctness , performance_evaluation , and detection of catastrophic faults such as stuck_at_faults . in i ddq testing , on the other hand , the focus is on detecting subtle manufacturing_process defects and reliability failures . as the technology scales , the roles for these two types of testing will diverge further . therefore , effective deep-micron current-based testing can play an important role not only in ensuring vlsi quality and reliability but also in arresting the already escalated costs of vlsi testing . the simplified mos theory assumes a zero drain current for v gs < v t . in fact , drain current ( i ds ) does not drop abruptly , but decreases exponentially , similar to a bipolar transistor's operation . the leakage_current stems from minority carriers and diffusion currents in the noninverted mos transistor . in the subthresh-old region , the inverse rate of decrease of i ds in volts per decade , 
behaviour & information_technology information_structure and practice as facilitators of deaf_users' navigation in textual websites information_structure and practice as facilitators of deaf_users' navigation in textual websites taylor & francis makes every effort to ensure the accuracy of all the information ( the " content " ) contained in the publications on our platform . however , taylor & francis , our agents , and our licensors make no representations or warranties whatsoever as to the accuracy , completeness , or suitability for any purpose of the content . any opinions and views expressed in this publication are the opinions and views of the authors , and are not the views of or endorsed by taylor & francis . the accuracy of the content should not be relied upon and should be independently verified with primary sources of information . taylor and francis shall not be liable for any losses , actions , claims , proceedings , demands , costs , expenses , damages , and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with , in relation to or arising out of the use of the content . deaf_users might find it difficult to navigate through websites with textual content which , for many of them , constitutes the written representation of a non-native oral language . with the aim of testing how the information_structure could compensate for this difficulty , 27 prelingual deaf_users of sign_language were asked to search a set of headlines in a web newspaper where information_structure and practice were manipulated . while practice did not affect deep_structures ( web_content distributed through four layers of nodes ) , wide structures ( web_content concentrated in two layers ) did facilitate users' performance in the last trial block and compromised it in the first trial block . it is argued that wide structures generate a textual information_overload for deaf_users , which decreases with practice . thus , wide structures seem preferable for websites requiring frequent use , rather than for those intended for occasional interaction . 1 . introduction international organisations such as the international_organization_for_standardization ( iso 9241 171 ) and the web_accessibility initiative ( wai 1999 ) offer web_accessibility guidelines aimed at improving the use of the internet by disabled people . however , accessibility guidelines often lack empirical validation ( ivory and hearst 2001 ) and tend to focus on the physical and sensorial features of their deficiencies ( e . g . seeman 2002 ) , not considering other cognitive , linguistic and cultural aspects . this situation is particularly true for the guidelines developed to improve web use in prelingual deaf_users , generally treated as hearing users who cannot hear which can lead to simplistic and often erroneous web_design solutions like providing text or graphic equivalents to 
investigation of speech separation as a front_end for noise robust speech_recognition recently , supervised classification has been shown to work well for the task of speech separation . we perform an in-depth evaluation of such techniques as a front_end for noise-robust automatic_speech_recognition ( asr ) . the proposed separation front_end consists of two stages . the first stage removes additive noise via time-frequency masking . the second stage addresses channel mismatch and the distortions introduced by the first stage; a non-linear_function is learned that maps the masked spectral features to their clean counterpart . results show that the proposed front_end substantially improves asr performance when the acoustic_models are trained in clean conditions . we also propose a diagonal feature discriminant linear_regression ( dfdlr ) adaptation that can be performed on a per-utterance basis for asr systems employing deep_neural_networks and hmm . results show that dfdlr consistently improves performance in all test conditions . surprisingly , the best average results are obtained when dfdlr is applied to models trained using noisy log-mel spectral features from the multi-condition training_set . with no channel mismatch , the best results are obtained when the proposed speech separation front_end is used along with multi-condition training using log-mel features followed by dfdlr adaptation . both these results are among the best on the aurora-4 dataset . 
probabilistic analysis of activation volumes generated during deep_brain_stimulation deep_brain_stimulation ( dbs ) is an established therapy for the treatment of parkinson's_disease ( pd ) and shows great_promise for the treatment of several other disorders . however , while the clinical analysis of dbs has received great attention , a relative paucity of quantitative techniques exists to define the optimal surgical target and most effective stimulation protocol for a given disorder . in this study we describe a methodology that represents an evolutionary addition to the concept of a probabilistic brain_atlas , which we call a probabilistic stimulation atlas ( psa ) . we outline steps to combine quantitative clinical outcome measures with advanced computational models of dbs to identify regions where stimulation-induced activation could provide the best therapeutic improvement on a per-symptom basis . while this methodology is relevant to any form of dbs , we present example results from subthalamic_nucleus ( stn ) dbs for pd . we constructed patient-specific computer models of the volume of tissue activated ( vta ) for 163 different stimulation parameter settings which were tested in six patients . we then assigned clinical outcome scores to each vta and compiled all of the vtas into a psa to identify stimulation-induced activation targets that maximized therapeutic response with minimal side effects . the results suggest that selection of both electrode placement and clinical stimulation parameter settings could be tailored to the patient's primary symptoms using patient-specific models and psas . 
fivatech : page-level web_data extraction from template pages web data_extraction has been an important part for many web data_analysis applications . in this paper , we formulate the data_extraction problem as the decoding process of page generation based on structured_data and tree templates . we propose an unsupervised , page-level data_extraction approach to deduce the schema and templates for each individual deep website , which contains either singleton or multiple data_records in one webpage . fivatech applies tree matching , tree alignment , and mining_techniques to achieve the challenging task . in experiments , fivatech has much higher precision than exalg and is comparable with other record-level extraction systems like viper and mse . the experiments show an encouraging result for the test pages used in many state-of-the-art web data_extraction works . 
speech-to-speech translation : the project verbmobil 1 summary the research_project verbmobil combines the research branches of continuous_speech_recognition and machine_translation . a consortium of more than 20 universities and 8 companies in germany as well as two partner institutes in usa and japan is enganged in the development of a system for translation of spontaneous and continuous_speech input to speech output . in the rst phase the language pair german and japanese has been chosen with english as a presentation language ( to check the translation quality ) . the domain of discourse is appointment scheduling . the nal application may be an appointment negotiation among two industrial managers from japan and germany . in the second project phase the domain will be extended to include travel planning discussions . there exists a prototype implementation integrating the modules of the partners . additional background research has been carried out to investigate prablems e . g . of dialogue and discourse modelling , stochastic parsing and innovative software_architectures . this paper gives an overview of the project , its goals and results . it describes in more details research done on integrative architectures . 2 aims of the project verbmobil combines research_activities in continuous_speech_recognition and in machine_translation . it tries to develop a speech-to-speech translation prototype as a feasibility study . the methods to achieve this goal vary among the partners and result in rather heterogeneous modules . especially in syntax , rule based_approaches ( mostly \deep" analysis ) vs . stochastic approaches ( mostly \shal-low" understanding components ) are tested and reened in parallel . this fact reeects not only the current situation in computational_linguistics but may also stem from diierent cognitive theories about holistic understanding . to handle such heterogeneous components a lot of research has been done on innovative architectures the technical aim at the end of the rst project phase is : 70 % approximately correct translation ( end_to_end ) on unknown examples within the domain of appointment scheduling and within the lexical set of the given 2285 words . 
a low-power , process-and- temperature- compensated ring_oscillator with addition-based current_source the design of a 1 . 8 ghz 3-stage current-starved ring_oscillator with a process-and temperature-compensated current_source is presented . without post-fabrication calibration or off-chip components , the proposed low variation circuit is able to achieve a 65 . 1% reduction in the normalized standard deviation of its center frequency at room_temperature and 85 ppm/ c temperature stability with no penalty in the oscillation frequency , the phase_noise or the start-up time . analysis on the impact of transistor scaling indicates that the same circuit topology can be applied to improve variability as feature size scales beyond the current deep_submicron_technology . measurements taken on 167 test chips from two different lots fabricated in a standard 90 nm cmos_process show a 3x improvement in frequency variation compared to the baseline case of a conventional current-starved ring_oscillator . the power and area for the proposed circuitry is 87 w and 0 . 013 mm 2 compared to 54 w and 0 . 01 mm 2 in the baseline case . 
discrimination ability of individual measures used in sleep_stages classification objective the paper goes through the basic knowledge about classification of sleep_stages from polysomnographic recordings . the next goal was to review and compare a large number of measures to find the suitable candidates for the study of sleep onset and sleep evolution . methods and material a huge number of characteristics , including relevant simple measures in time domain , characteristics of distribution , linear spectral measures , measures of complexity and interdependency measures were computed for polysomnographic recordings of 20 healthy subjects . summarily , all-night evolutions of 818 measures ( 73 characteristics for various channels and channel combinations ) were analysed and compared with visual scorings of experts ( hypnograms ) . our tests involved classification of the data into five classes ( waking and four sleep_stages ) and 10 classification tasks to distinguish between two specific sleep_stages . to discover measures of the best decision_making ability , discriminant_analysis was done by fisher quadratic classifier for one-dimensional case . results and conclusions the most difficult decision_problem , between s1 and rem_sleep , were best managed by measures computed from electromyogram led by fractal exponent ( classification error 23% ) . in the simplest task , distinction between wake and deep_sleep , the power ratio between delta and beta band of electroencephalogram was the most successful measure ( classification error 1% ) . delta/beta ratio with mean classification error 42 . 6% was the best single-performing measure also in discrimination between all five stages . however , the error level shows impossibility to satisfactorily separate the five sleep_stages by a single measure . use of a few additional characteristics is necessary . some novel measures , especially fractal exponent and fractal_dimension turned up equally successful or even superior to the conventional scoring methods in discrimination between particular states of sleep . they seem to provide a very promising basis for automatic sleep analysis particularly in conjunction with some of the successful spectral standards . 
computational_intelligence and ai in games : a new ieee transactions games have long been seen as an ideal test_bed for the study of ai . until recently , most of the academic work in the area focused on traditional board_games and card games , the challenge being to beat expert human players . following the release of pong in the early 1970s , the last few decades have seen a phenomenal increase in the quality , diversity and pervasiveness of video_games . the value of the worldwide computer and video_games market is estimated to be $usd25bn annually 1 , and is predicted to grow rapidly over the next decade . this presents academic researchers [1] and game developers with the challenge of developing next_generation game ai . the future will see games with intelligent empathetic characters who get to know you , and work hard to optimise your experience while playing the game . new titles such as left_4_dead_2 have already made important steps in this direction . superior game ai will lead to more entertaining and immersive games and also add value to the burgeoning serious games market . traditional games are constrained by the ability of the players to manipulate the game objects such as pieces on a chessboard or cards from a deck . the rules of a game specify how the game objects interact and fundamental combinatorics quickly produce enormous game trees; this ensures that complex decision_making processes are required in order to play such games to a high standard . implementing computer programs that are able to defeat their human opponents has been the subject of a great deal of ai research , epitomised by outstandingly successful programs such as deep_blue for chess [2] and chinook for checkers [3] , [4] . these programs expend most of their effort on game_tree search , and for this reason are often called brute force approaches . this term although widely used [5] is somewhat inaccurate : there is a great deal of subtlety in the way these algorithms are implemented and fine-tuned . the levels of play achieved demonstrate extraordinary prowess in computer science and engineering for the authors of those systems . on a broader note , they force us to reconsider the nature of intelligence . game_tree search has limited applicability when the game has a large branching factor or the state-space is continuous . hence the techniques applied to chess and checkers do not work well for go , and are even less applicable to video_games where the state-space is for practical purposes 
using graphical representation of user_interfaces as visual references many user_interfaces use indirect references to identify specific objects and devices . my thesis investigates using graphical representations of user_interfaces ( i . e . screenshots ) as direct visual references to support various kinds of applications . sikuli script enables users to programmatically control guis without the support from the underlying applications . sikuli test lets gui developers and testers create test scripts without coding . deep shot introduces a framework and interaction techniques to migrate work states across heterogeneous devices in one action , taking a picture . in addition to these pure pixel-based systems , pax associates the pixel representation with the internal structures and metadata of the user_interface . based on these building blocks , we propose to develop a visual history system that enables users to search and browse what they have seen on their computer screens . we outline some interesting use cases and discuss the challenges in this ongoing work . 
hybrid concolic_testing we present hybrid concolic_testing , an algorithm that interleaves random_testing with concolic execution to obtain both a deep and a wide exploration of program state_space . our algorithm generates test inputs automatically by interleaving random_testing until saturation with bounded exhaustive symbolic exploration of program points . it thus combines the ability of random search to reach deep program states quickly together with the ability of concolic_testing to explore states in a neighborhood exhaustively . we have implemented our algorithm on top of cute and applied it to obtain better branch_coverage for an editor implementation ( vim 5 . 7 , 150k lines of code ) as well as a data_structure implementation in c . our experiments suggest that hybrid concolic_testing can handle large programs and provide , for the same testing budget , almost 4 the branch_coverage than random_testing and almost 2 that of concolic_testing . 
regularizing recurrent_networks - on injected noise and norm-based_methods advancements in parallel_processing have lead to a surge in multilayer_perceptrons' ( mlp ) applications and deep_learning in the past decades . recurrent_neural_networks ( rnns ) give additional representa-tional power to feedforward mlps by providing a way to treat sequential data . however , rnns are hard to train using conventional error backpropagation methods because of the difficulty in relating inputs over many time-steps . regularization approaches from mlp sphere , like dropout and noisy weight_training , have been insufficiently applied and tested on simple rnns . moreover , solutions have been proposed to improve convergence in rnns but not enough to improve the long_term dependency remembering capabilities thereof . in this study , we aim to empirically evaluate the remembering and generalization_ability of rnns on polyphonic musical datasets . the models are trained with injected noise , random dropout , norm-based regularizers and their respective performances compared to well-initialized plain rnns and advanced regularization methods like fast-dropout . we conclude with evidence that training with noise does not improve performance as conjectured by a few works in rnn optimization before ours . 
site-wide wrapper_induction for life science deep web_databases we present a novel approach to automatic information_extraction from deep_web life science databases using wrapper_induction . traditional wrapper_induction techniques focus on learning wrappers based on examples from one class of web_pages , i . e . from web_pages that are all similar in structure and content . thereby , traditional wrapper_induction targets the understanding of web_pages generated from a database using the same generation template as observed in the example set . however , life science web_sites typically contain structurally diverse web_pages from multiple classes making the problem more challenging . furthermore , we observed that such life science web_sites do not just provide mere data , but they also tend to provide schema information in terms of data labels giving further cues for solving the web_site wrapping task . our solution to this novel challenge of site-wide wrapper_induction consists of a sequence of steps : 1 . classification of similar web_pages into classes , 2 . discovery of these classes and 3 . wrapper_induction for each class . our approach thus allows us to perform unsupervised information_retrieval from across an entire web_site . we test our algorithm against three real_world biochemical deep web_sources and report our preliminary_results , which are very promising . 
relation information_extraction using deep syntactic_analysis there has been an increasing need for natural_language_processing technology to information_extraction ( ie ) , such as relations between entities , which are more informative than mere documents searched by key words . this dissertation proposes a novel method to construct and utilize extraction_patterns for relation extraction based on deep_syntactic relations obtained by full parsing . the process which requires the most amount of manual work in construction of ie systems is construction of extraction_patterns which extract target information from source texts , because the same information can be represented through many kinds of syntactic variations . to reduce this amount of manual work , our approach has two phases : first , we raise representation ability of extraction_patterns and reduce number of necessary patterns by normalizing syntactic variations into predicate_argument structures ( pass ) using a full parser based on head_driven_phrase_structure_grammar ( hpsg ) . then , pass which connect entities to extract in a small training corpus are considered as extraction_patterns , and we divide them into components and utilize combinations of the components for generalization . as a real world application , we have constructed an ie system for protein_protein_interactions , which are important knowledge in biomedical research . we evaluated the ie system on a small test_case corpus and a large real_world corpus , and show its effectiveness . this dissertation also describes aspects that should be considered to ensure effectiveness of full parsers on domain_specific ie . the first aspect is the ability of deep_syntactic relations obtained by parsing to capture syntactic_information , which is necessary for constructing extraction_patterns . to show enough accuracy of full parsing on a biomed-ical text , we evaluated precision of primitive pass obtained from a biomedical text by an hpsg parser . and to compare performance of pas patterns to patterns of part-of-speeches , we also evaluated performance of verb-argument relations obtained from a biomedical text by pas patterns and by patterns of part-of-speeches . the second aspect is difficulties to apply general_purpose parsers to domain_specific domains . to measure domain_specific coverage of a general_purpose hpsg , we investigated deficiencies of the grammar on parsing a biomedical text . we also show preliminary investigation on performance of general_purpose parsers that suggested parsing accuracy on general corpus does not ensure parsing accuracy or ie accuracy on a domain_specific text . through all results on this dissertation , we show that full parsing is effective for ie . to obtain more performance of a domain_specific ie with full parsing , we should use shallow 
modeling the game of arimaa with linguistic geometry a computer defeated a chess world champion for the first time in 1997 . this event inspired omar syed to develop the game of arimaa . he intended to make it difficult to solve under present search approaches . linguistic geometry is a technique that offers a formal method based on the expertise of human chess masters , to make the development of complex heuristics easier . this article introduces a linguistic geometry based model for the game of arimaa . it gives implementation for each of the essential components of linguistic geometry : trajectories , zones , translations and searches . a test_case is given and it is used as input for a software implementation of the proposed model . the results given by the software are compared against the analysis made by a human player . i . introduction to study games , they can be represented through large trees . these trees include every possible evolution of the game . searching through the game's tree for a solution of the game is the current approach in computer sciences . to reduce the execution time of game search_algorithms , heuristics are used during the analysis of the game's tree . a heuristic is an estimation mechanism or a rule of thumb . heuristics were first proposed by claude_shannon [7] . the most popular search_algorithms through general trees are depth-first , breath first and best first , while for search in games trees are minimax and alpha_beta [4] . games are an interesting area of study because of their complexity , it is believed that techniques used for solving some games can also be used to solve other kind of problems . in particular , the study of chess has excelled . a new game called arimaa was created by omar and aamir syed [10] . they were inspired by the defeat of garry_kasparov against the supercomputer deep_blue . the game of arimaa is a two player complete information , zero_sum_game with no random factors . arimaa was released in 2002 and it was designed to be complex to play well under traditional game search_algorithms . this was done with the purpose of fomenting the development of new , ground breaking techniques . a challenge was published along the rules of the game , it consists of a prize of $10 , 000 usd for anyone who creates a computer program capable of defeating a human expert in a competition of six games . many computer programs have participated in the ari-maa challenge . the vast majority of them are 
explaining across contrasting cases for deep_understanding in science : an example using interactive simulations undergraduate_students used a simulation to learn about electromagnetic flux . they were provided with three simulated cases that illustrate how changes in flux induce current in a coil . in the poe condition , students predicted , observed , and explained the outcome of each case , treating each case separately . in the ge condition , students were asked to produce a general explanation that would work for all three cases . a second factor crossed whether students had access to a numerical measurement tool . effects of the measurement tool were less conclusive , but there was a strong effect of instructional method . compared to poe students , ge students were better able to induce an underlying principle of electromagnetic flux during instruction and were better able to apply this principle to novel problems at post_test . moreover , prior achievement predicted learning in the poe group , while students of all academic levels benefited equally from the ge condition . science_education has learning goals that range from basic lab skills to beliefs about the sources of scientific_knowledge . one enduring goal is for students to develop a deep understanding of phenomena so they can engage in the structure of scientific explanation . one way to characterize deep_understanding is the capability and disposition to perceive and explain natural phenomena in terms of general principles . in this study , we show that deep_understanding can depend critically on the way in which multiple instances of phenomena are presented to students and how students are instructed to explain those instances . the research is done in the context of undergraduate physics students learning about magnetic_flux with a computer simulation . it is common in science instruction to ask students to solve or conceptually explain a series of problems . one version of this approach is the predict-observe-explain ( poe ) cycle ( white & gunstone , 1992 ) . students receive the setup of an experiment and predict what will happen . they then observe the outcome and develop an explanation for why their prediction did or did not match the expected outcome . for poe and other sequenced formats , a series of questions or examples is carefully selected to help students instantiate a given core principle in multiple contexts , so that they develop a deeper , more abstract sense of the principle and learn the kinds of situations to which it applies . formats such as poe are considered to be effective in part because they foster deep and often extended engagement with each new question 
evidence for a motor and a non-motor domain in the human dentate_nucleus - an fmri study dum and strick ( j . neurophysiol . 2003; 89 , 634-639 ) proposed a division of the cerebellar dentate_nucleus into a "motor" and "non-motor" area based on anatomical data in the monkey . we asked the question whether motor and non-motor domains of the dentate can be found in humans using functional_magnetic_resonance_imaging ( fmri ) . therefore dentate activation was compared in motor and cognitive tasks . young , healthy participants were tested in a 1 . 5 t mri scanner . data from 13 participants were included in the final_analysis . a block_design was used for the experimental conditions . finger tapping of different complexities served as motor tasks , while cognitive testing included a verbal working_memory and a visuospatial task . to further confirm motor-related dentate activation , a simple finger movement task was tested in a supplementary experiment using ultra-highfield ( 7 t ) fmri in 23 participants . for image_processing , a recently_developed region of interest ( roi ) driven normalization method of the deep cerebellar nuclei was used . dorso-rostral dentate_nucleus activation was associated with motor function , whereas cognitive tasks led to prominent activation of the caudal nucleus . the visuospatial task evoked activity bilaterally in the caudal dentate_nucleus , whereas verbal working_memory led to activation predominantly in the right caudal dentate . these findings are consistent with dum and strick's anatomical findings in the monkey . 
multimode scan : test per clock bist for ip_cores built-in self-test ( bist ) is an attractive design_for_test methodology for core-based soc_design because of the minimal need for test access when tests are generated and evaluated within the core itself . however , the scan_based logic_bist approach being widely considered for this application suffers from two significant weaknesses : slow test-per-scan execution , and a limited capability for detecting realistic timing and delay_faults , critical in deep submicron technologies . the new multimode scan_based approach presented here supports test-per-clock bist , which runs orders_of_magnitude faster , and also provides significantly better delay fault_coverage . 
gates foots a malaria bill genuine search for the public view , but as a way of getting greater acceptance , " he wrote . " now they are hoist with their own petard . the people who registered their views were more hostile than the public at_large . " the figures were drawn from over 600 public meetings throughout the country which , as the independent pointed out , were not designed to achieve a cross-sample of the population at_large . " indeed , there is a case for saying that there is a 'self-selecting' effect at work in such a consultation exercise , because those most likely to turn up to such a meeting might be expected to be those most passionately opposed to gm products . " the times insisted that the results had particular force because they did not come solely from meetings packed with opponents . " the findings were confirmed by 77 people who were selected randomly as representative of the general public , a grouping named 'narrow-but-deep' . the panel was less dogmatic in its opposition to gm but wished the government to delay a decision until there were more tests . " a third surprise was the response of the guardian , which in 1999 highlighted arpad pusztai's claims about alleged dangers of gm crops , and whose editor co-authored the anti-gm television drama fields of gold last year . though it carried two articles on the survey results , the guardian devoted as much space to a piece warning that " public antipathy toward gm crops is driving britain's leading plant scientists to seek greener pastures abroad " . prominent among researchers quoted was richard flavell , formerly of the john_innes_centre in norwich and now with ceres in california . " the situation is more disturbing in the uk than anywhere else in the world , " flavell was quoted as saying . " the untruths , lies and lack of orchestrated information make it impossible for the average person to make an informed decision . " so " gm nation ? " ( who on earth decided to call it that ? ) did not spawn a uniform , hysterical chorus from the media . true , most journalists and editors decided to amplify its negative verdicts . but a significant minority urged caution . of these , the times struck arguably the most appropriate note . citing both " scaremongering about health effects " and " genuine scientific uncertainty about environmental effects " , it concluded that the british people " do not want to close 
comparative_study of gene_set enrichment methods background the analysis of high_throughput gene_expression_data with respect to sets of genes rather than individual genes has many advantages . a variety of methods have been developed for assessing the enrichment of sets of genes with respect to differential expression . in this paper we provide a comparative_study of four of these methods : fisher's_exact_test , gene_set enrichment analysis ( gsea ) , random-sets ( rs ) , and gene list analysis with prediction accuracy ( glapa ) . the first three methods use associative statistics , while the fourth uses predictive statistics . we first compare all four methods on simulated data_sets to verify that fisher's_exact_test is markedly worse than the other three approaches . we then validate the other three methods on seven real data_sets with known genetic perturbations and then compare the methods on two cancer data_sets where our a priori knowledge is limited . results the simulation study highlights that none of the three method outperforms all others consistently . gsea and rs are able to detect weak signals of deregulation and they perform differently when genes in a gene_set are both differentially up and down regulated . glapa is more conservative and large differences between the two phenotypes are required to allow the method to detect differential deregulation in gene sets . this is due to the fact that the enrichment statistic in glapa is prediction error which is a stronger criteria than classical two sample statistic as used in rs and gsea . this was reflected in the analysis on real data_sets as gsea and rs were seen to be significant for particular gene sets while glapa was not , suggesting a small effect_size . we find that the rank of gene_set enrichment induced by glapa is more similar to rs than gsea . more importantly , the rankings of the three methods share significant overlap . conclusion the three methods considered in our study recover relevant gene sets known to be deregulated in the experimental conditions and pathologies analyzed . there are differences between the three methods and gsea seems to be more consistent in finding enriched gene sets , although no method uniformly dominates over all data_sets . our analysis highlights the deep difference existing between associative and predictive methods for detecting enrichment and the use of both to better interpret results of pathway_analysis . we close with suggestions for users of gene_set methods . 
temporal redundancy based encoding technique for peak_power and delay reduction of on-chip buses power_consumption and delay are two of the most important constraints in current-day on-chip bus design . the two major sources of dynamic power_dissipation on a bus are the self capacitance and the coupling_capacitance . as technology scales , the interconnect resistance increases due to shrinking wire-width . at the same time , spacing between the interconnects decreases resulting in an increase in the coupling_capacitance . this , in turn , leads to stronger crosstalk_effects between the interconnects . in deep sub-micron technology the coupling_capacitance exceeds the self capacitance , which , in turn , cause more power_consumption and delay on the bus . recently , the interest has also shifted to minimizing peak power_dissipation . the reason being that higher peak_power leads to an undesired increase in switching noise , metal electromigration problems and operation-induced variations due to non-uniform temperature on the die . thus , minimizing power_consumption and delay are the most important design objectives for on-chip buses . several bus encoding schemes have been proposed in the literature for reducing crosstalk . most of these encoding techniques use spatial redundancy that requires additional transmission wires on the bus . in this paper , a new temporal encoding scheme is proposed , which uses self-shielding memory-less codes to completely eliminate worst_case crosstalk_effects and hence significantly minimizes power_consumption and delay of the bus . a major advantage of the proposed temporal redundancy based encoding scheme is the reduction in the number of wires of the on-chip bus . this reduction facilitates extra spacing between the bus wires , when compared with the normal bus , for a given area . this , in turn , leads to reduced crosstalk_effects between the wires . the proposed encoding scheme is tested with the spec2000 cint benchmarks . the experimental results , when compared to the transmission over a normal bus , show that on an average the proposed technique leads to a reduction in the peak-power_consumption by 51% ( 28% ) , 51% ( 29% ) and 52% ( 30% ) in the data ( address ) bus for 90nm , 65nm and 45nm technologies , respectively . for a bus length of 10mm the proposed technique also achieves 17% , 31% and 37% reduction in the bus delay for 90nm , 65nm and 45nm technologies , respectively , when compared to what is incurred by the data_transmission on a normal bus . 
evaluation of effectiveness of median of absolute deviations outlier_rejection-based iddq_testing for burn-in reduction cmos chips having high leakage are observed to have high burn-in fallout rate . i ddq testing has been considered as an alternative to burn-in . however , increased sub-threshold leakage_current in deep sub-micron technologies limits the use of i ddq testing in its present form . in this work , a statistical outlier_rejection technique known as the median of absolute deviations ( mad ) is evaluated as a means to screen early failures using i_ddq data . mad is compared with delta_i_ddq and current signature methods . the results of the analysis of the sematech data are presented . 
benchmarking user performance by using virtual_reality for task-based training conveyor belts have a high accident and fatality rate associated with them because of the dangerous environment their constantly moving parts create . because of this high fatality rate , different methods are being considered to improve current safety training methods . by looking at the principles of cognitive learning and what makes a computer-based training_program successful , a safety training_program using virtual_reality ( vr ) is being proposed . the training_program structure includes four steps in creating a comprehensive two phase training_program that will train personnel on the required operational processes in an instructional-based phase and then test their abilities through an interactive task-based session that tracks the user's progress and choices , tallies points based on corrective actions taken , and gives immediate feedback and consequences to the user's actions . this paper focus on the task-based phase of the prototype development and what steps are taken in creating an individual exercise . one example exercise is described in detail from choosing the material that is tested , implementing the animations and coding using right hemisphere's deep creator and lisp coding , implementing the tracking methods , and the output file that has been designed to keep track of the user's performance . 
microwave radiometric measurements of soil_moisture in italy microwave radiometric measurements of soil_moisture in italy within the framework of the map and raphael projects , airborne experimental campaigns were carried out by the ifac group in 1999 and 2000 , using a multifrequency microwave_radiometer at l , c and x bands ( 1 . 4 , 6 . 8 and 10 ghz ) . the aim of the experiments was to collect soil_moisture and vegetation biomass information on agricultural areas to give reliable inputs to the hydrological models . it is well known that microwave emission from soil , mainly at l-band ( 1 . 4 ghz ) , is very well correlated to its moisture_content . two experimental areas in italy were selected for this project : one was the toce valley , domodossola , in 1999 , and the other , the agricultural area of cerbaia , close to florence , where flights were performed in 2000 . measurements were carried out on bare soils , corn and wheat fields at different growth stages and on meadows . ground data of soil_moisture ( smc ) were collected by other research teams involved in the experiments . from the analysis of the data_sets , it has been confirmed that l-band is well related to the smc of a rather deep soil layer , whereas c-band is sensitive to the surface smc and is more affected by the presence of surface_roughness and vegetation , especially at high incidence angles . an algorithm for the retrieval of soil_moisture , based on the sensitivity to moisture of the brightness_temperature at c-band , has been tested using the collected data_set . the results of the algorithm , which is able to correct for the effect of vegetation by means of the polarisation index at x-band , have been compared with soil_moisture measurements in the ground . finally , the sensitivity of emission at different frequencies to the soil_moisture profile was investigated . experimental data_sets were interpreted by using the integral_equation model ( iem ) and the outputs of the model were used to train an artificial_neural_network to reproduce the soil_moisture content at different depths . 
improvement of lagrangian relaxation convergence for production_scheduling it is widely accepted that new production_scheduling tools are playing a key role in flexible manufacturing_systems to improve their performance by avoiding idleness machines while minimizing setup times penalties , reducing penalties for do not delivering orders on time , etc . since manufacturing scheduling_problems are np-hard , there is a need of improving scheduling methodologies to get good solutions within low cpu time . la-grangian relaxation ( lr ) is known for handling large_scale separable problems , however , the convergence to the optimal solution can be slow . lr needs customized parametrization , depending on the scheduling_problem , usually made by an expert user . it would be interesting the use of lr without being and expertise , i . e . , without difficult parameters tuning . this paper presents innovative approaches on the lr method to be able to develop a tool capable of solve scheduling_problems applying the lr method without requiring a deep expertise on it . first approach is the improvement of an already existing method which use constraint_programming ( cp ) to obtain better primal cost convergence . second approach is called extended subgradient information ( esgi ) and it speed up the dual cost convergence . finally , a set of step size rules for the subgradient ( sg ) method are compared to choose the most appropriate rule depending on the scheduling_problem . test results_demonstrate that the application of cp and esgi approaches , together with lr and the selected step size rule depending on the problem , generates better solutions than the lr method by itself . note to practitioners-production_scheduling tools are one of the keys in flexible mannfacturing systems to improve its performance . these tools are usually based on optimization methods , as could be the lagrangian relaxation . the problems of using optimization methods are the need of time to get the solution , and the need of a high-specialized user to tune them . therefore , optimization methods must be improved to use less time to obtain solutions and to do not need high-specialized users . this paper was motivated by these needs : reducing the cpu time when scheduling operations in production_planning to permit quick replies to real-time perturbations into production processes; and making easier the use of production_scheduling tools . this paper suggests new approaches for the lagrangian relaxation ( lr ) method applying constraint logic_programming ( clp ) and improving the multipliers calculation ( inside the subgradient method ) during the iterations to speed up the convergence of the lr method and make it easily tuned . thus , the cpu 
finite state_machines : composition , verification , minimization : a case study a deep understanding of circuit behaviour is a prerequisite for any validation process ( simulation , formal_verification , test_generation ) . we propose to use a tool which gives complete and optimized representations of sequential_circuits allowing the designer to understand the accurate behaviour of the circuit . a detailed example is introduced to help reader's understanding . for obvious reasons , we choose a small size circuit . the example comes from our experience ( [2] ) in computer architecture and digital_design education . 
parameter sensitivity_analysis of crop_growth_models based on the extended fourier amplitude sensitivity test_method keywords : extended fast wofost crop_growth_models sample_size parameter variation range time-dependent properties multivariable output a b s t r a c t sensitivity_analysis ( sa ) has become a basic tool for the understanding , application and development of models . however , in the past , little attention has been paid to the effects of the parameter sample_size and parameter variation range on the parameter sa and its temporal properties . in this paper , the corn crop planted in 2008 in the yingke oasis of northwest_china is simulated based on meteorological observation data for the inputs and statistical data for the parameters . furthermore , using the extended fourier amplitude sensitivity ( efast ) algorithm , sa is performed on the 47 crop parameters of the world food studies ( wofost ) crop_growth_models . a deep analysis is conducted , including the effects of the parameter sample_size and variation range on the parameter sa , the temporal properties and the multivariable output issues of sa . the results show that sample_size highly affects the convergence of the sensitivity indices . two types of parameter variation ranges are used for the analysis , and the results show that the sensitive parameters of the two parameter spaces are distinctly different . in addition , taking the storage organ biomasses at the different growth stages as the objective output , the time-dependent characteristics of the parameter sensitivity are discussed . the results show that several sensitive parameters exist in the grain biomass throughout the entire development stage . in addition , analyzing the twelve sensitive parameters has proven that although certain parameters have no effect on the final yield , they play key roles in certain growth stages , and the importance of these parameters gradually increases . finally , the sensitivity analyses of different state variable outputs are performed , including the biomass , yield , leaf area index , and transpiration coefficient . the results suggest that the sensitive parameters of various variable processes differ . this study highlights the importance of considering multiple characteristics of the model parameters and the responses of the models in specific phenological stages . crop_growth_models are a valuable tool for the quantitative analysis of the growth and production of crops and play an important role in crop monitoring , crop_yield prediction , field management recommendations , agricultural production potential evaluation , and climate_change impact evaluation ( batchelor et al . , crop_growth_models primarily simulate the growth and development of crops , and they encompass the primary biophysical and biochemical processes in the soilecropeatmosphere system , 
rendergan : generating realistic labeled_data deep convolutional neuronal networks ( dcnn ) are showing remarkable performance on many computer_vision tasks . due to their large parameter space , they require many labeled samples when trained in a supervised setting . the costs of annotating data manually can render the usage of dcnns infeasible . we present a novel framework called rendergan that can generate large amounts of realistic , labeled images by combining a 3d model and the generative adversarial network framework . in our approach , image augmentations ( e . g . lighting , background , and detail ) are learned from unlabeled_data such that the generated images are strikingly realistic while preserving the labels known from the 3d model . we apply the rendergan framework to generate images of barcode-like markers that are attached to honeybees . a dcnn is trained on this data only . it performs better on a test set of real_data than an equal dcnn trained on the limited amounts of real_data available . 
automated determination of axonal orientation in the deep white_matter of the human_brain the wide-spread utilization of diffusion-weighted imaging in the clinical neurosciences to assess white_matter ( wm ) integrity and architecture calls for robust validation strategies applied to the data that are acquired with noninvasive imaging . however , the pathology and detailed fiber architecture of wm tissue can only be observed postmortem . with these considerations in mind , we designed an automated method for the determination of axonal orientation in high_resolution microscope images . the algorithm was tested on tissue that was stained using a silver impregnation technique that was optimized to resolve axonal fibers against very low levels of background . the orientation of individual nerve fibers was detected using spatial filtering and a template-matching_algorithm , and the results are displayed as color-coded overlays . quantitative models of wm fiber architecture at the microscopic level can lead to improved interpretation of low_resolution neuroimaging data and to more accurate mapping of fiber pathways in the human_brain . 
'disappearing sensor'-textile based sensor for monitoring breathing textile based sensors were developed and used for remote monitoring of breathing . the breathing is simulated by using a new cyclic tester device . in the simulated a cyclic force is applied along the length of the textile sensor . however due to the morphology of human_body , in real situation the sensor is not only under stretching but also under a certain degree of bending . a prototype garment with the sensor situated on the chest area was made . the prototype was worn by 10 persons , and breathing was recorded as the persons were sitting still , walking and jogging . deep_breathing in the supine_position and breathing with a method_called athletic breathing were used to evaluate the sensor . a testing circuit and a labview program were made for preliminary test . the sensor is wearable , washable and comfortable . sensor construction is totally 'disappearing' and visualize as printed pattern onto the surface of garment . 
is quality a design constraint for sub 100nm designs ? description deep_sub_micron design ( below 100nm ) present a number of new design challenges . these include very high masking costs , new interconnect materials and parasitic phenomenon , significant re-engineering at the device level due to changes in basic device performance , very high gate_count and pin count designs , complexity in high pin count packaging & test , and finally reduced product life in the marketplace do to the rapid rollout of new technologies . one of the trade offs that is taking place in the industry to address these issues is the decision toward " design existence " , which is the selection of the " first functional implementation " of a design , over " design quality " which is the selection of the " optimal implementation " of a design . this panel will discuss the trends in the issue with respect to the re-targeting of the design quality issues from the soc level to the flow and device levels and the impact on this " shift " on the manufacturability of the resulting designs . issues discussed will include the use of pre-tested ip as a quality metric , the coverage and quality of the eda design and validation tools , the correlation of these metrics to the actual manufacturing_process and the impact of post fabrication_process steps ( packaging , test , etc ) on the yield of the resulting design . 
test_challenges for deep_sub_micron_technologies the use of deep_submicron process_technologies presents several new challenges in the area of manufacturing_test . while a significant body of work has been devoted to identifying and investigating design challenges in nanometer_technologies , the impact on test strategies and methodologies is still not well understood . this paper highlights the challenges to current test_methodologies arising from technology driven trends , and will present an overview of emerging techniques that address deep_submicron test_challenges . 
intelligence : what's in a name ? times piece "nobody's smart about intelligence" ( march 1 , 1998 ) , he offers this lament : "iqs are up . s . a . t . s are down . americans flunk math and prosper . somebody with brains should figure this out . " the best anyone can offer , johnson claims , is a conjecture that the complexity of everyday life ( programming small electronic devices or calculating the latest projection of your net worth when you retire ) has stretched and exercised our brains into faster and more agile computing engines . this might explain our increasing iqs while mtv and video_game overload might explain our increasing ignorance and declining capabilities at the logical plodding deductive thought of traditional intelligence . so what type of intelligence is ai trying to create ? astro teller ( new ~rk 7qmes , op_ed , march 21 , 1998 ) , suggests that no matter the type , building intelligences will make our world better as we learn more about our minds and who we are . but what will we understand ? how better to exploit our neighbors or sell them goods and services at ever increasing profits ? will we understand the difference between gandhi and saddam ? mozart and madonna ? or just what is it that everyone finds funny about seinfield ? the recent pinnacle of ai achievement has not come from our half century long quest to pass the turing_test , but from our fascination at a machine beating a human at the complex task of playing chess . deep_blue , a parallel supercomputing creation from ibm for processing hundreds of millions of chess moves per second , is the hardware and software that realized this e . , o oe . eeeoeleqo e*e eolee oo t e#= the recent pinnacle of ai achievement has come from deep_blue , a machine that beat a human at playing chess . but what kind of intelligence is this ? impressive accomplishment . an ancient game , long attacked by ai'ers and now empirically conquered . everyone can honestly admit that deep_blue doesn't have a clue about what it is doing , so self_awareness is not an issue . it just "knows" the next best move from an intensive search . so what kind of intelligence is this ? it is dearly '~ai intelligence , " a smart machine; honored byai associations and foundations with a small pile of cash ( compared to ibm expenses ! ) and a newell research medal . new york times piece on deep_blue , asked whether deep_blue is indeed intelligent . he offered that although human chess grandmasters don't do exactly 
compression-based classification of biological sequences and structures via the universal similarity metric : experimental assessment background similarity of sequences is a key mathematical notion for classification and phylogenetic studies in biology . it is currently primarily handled using alignments . however , the alignment methods seem inadequate for post-genomic studies since they do not scale well with data set_size and they seem to be confined only to genomic and proteomic sequences . therefore , alignment-free similarity_measures are actively pursued . among those , usm ( universal similarity metric ) has gained prominence . it is based on the deep theory of kolmogorov_complexity and universality is its most novel striking feature . since it can only be approximated via data_compression , usm is a methodology rather than a formula quantifying the similarity of two strings . three approximations of usm are available , namely ucd ( universal compression dissimilarity ) , ncd ( normalized compression dissimilarity ) and cd ( compression dissimilarity ) . their applicability and robustness is tested on various data_sets yielding a first massive quantitative estimate that the usm methodology and its approximations are of value . despite the rich theory developed around usm , its experimental assessment has limitations : only a few data compressors have been tested in conjunction with usm and mostly at a qualitative level , no comparison among ucd , ncd and cd is available and no comparison of usm with existing_methods , both based on alignments and not , seems to be available . results we experimentally test the usm methodology by using 25 compressors , all three of its known approximations and six data_sets of relevance to molecular_biology . this offers the first systematic and quantitative experimental assessment of this methodology , that naturally complements the many theoretical and the preliminary experimental_results available . moreover , we compare the usm methodology both with methods based on alignments and not . we may group our experiments into two sets . the first one , performed via roc ( receiver operating curve ) analysis , aims at assessing the intrinsic ability of the methodology to discriminate and classify biological sequences and structures . a second set of experiments aims at assessing how well two commonly available classification algorithms , upgma ( unweighted pair group method with arithmetic mean ) and nj ( neighbor joining ) , can use the methodology to perform their task , their performance being evaluated against gold standards and with the use of well known statistical indexes , i . e . , the f-measure and the partition distance . based on the experiments , several conclusions can be drawn and , from them , novel valuable guidelines for the use of usm on biological_data . the main ones are reported next . conclusion ucd and ncd are indistinguishable , i . e . , they yield nearly the same values of the statistical indexes we have used , accross experiments and data_sets , while cd is almost always worse than both . upgma seems to yield better classification results with respect to nj , i . e . , better values of the statistical indexes ( 10% difference or above ) , on a substantial fraction of experiments , compressors and usm approximation choices . the compression program ppmd , based on ppm ( prediction by partial matching ) , for generic data and gencompress for dna , are the best performers among the compression algorithms we have used , although the difference in performance , as measured by statistical indexes , between them and the other algorithms depends critically on the data_set and may not be as large as expected . ppmd used with ucd or ncd and upgma , on sequence data is very close , although worse , in performance with the alignment methods ( less than 2% difference on the f-measure ) . yet , it scales well with data set_size and it can work on data other than sequences . in summary , our quantitative analysis naturally complements the rich theory behind usm and supports the conclusion that the methodology is worth using because of its robustness , flexibility , scalability , and competitiveness with existing techniques . in particular , the methodology applies to all biological_data in textual format . the software and data_sets are available under the gnu gpl at the supplementary material web page . 
ladd laser assisted deep_drawing in the current work , cup drawing experiments with laser assistance are presented where only selected areas of the work piece have been heated up . since the strongest deformations occur at the outer circumference of the blank , only that area has been heated up by a defocused laser_beam . selective laser heating of the work pieces was performed by diode as well as nd : yag laser radiation . the forming load for the experiments was established by a small hydraulic_press with a maximum drawing force of 630kn and a hydraulic die cushion . during the experiments , the drawing path and all relevant forces have been recorded . experimental_results clearly demonstrate that this combined laser forming process is enlarging the possibilities of conventional deep_drawing . 1 introduction in production engineering 3-d parts can be produced by different forming methods . for example , deep_drawing is a well established process in automotive_industry . fig . 1 shows schematically the setup of a simple cupping test . this process transforms a mainly flat sheet blank into a hollow 3-d part by pressing it into a die . high true strain results from strong plastic deformations which can only be achieved by strong forces applied to the work piece . in some cases limitations arise from the mechanical_properties of the used materials since the material cannot withstand the needed forces . flow curves ( e . g . fig . 2 ) of different materials show that plastic deformations can be facilitated by higher temperatures . 
distinctive neuronal firing patterns in subterritories of the subthalamic nucleus . objective deep_brain_stimulation of the subthalamic nucleus ( stn_dbs ) is an established treatment for parkinson's_disease ( pd ) . anatomical connectivity analyses and task-related physiological studies have divided the stn into different functional domains : sensorimotor , limbic , and associative - located in its dorsolateral ( dstn ) , anteroventral ( vstn ) and medial territories , respectively . targeting sensorimotor stn is essential for stimulation efficacy and is supported by intraoperative micro-electrode recordings . a different neuronal signature in microelectrode recordings across stn subterritories was explored in this study . methods stable recordings from 30 pd_patients were assigned to dstn or vstn by means of an anatomical method ( based on fused computed_tomography/magnetic_resonance_images ) and through a priori tri-segmented partition of the recording itself . we computed the inter-spike interval ( isi ) and isi-characteristics , mean firing rate ( mfr ) , discharge patterns and mean burst rate ( mbr ) of each detected single unit activity . results we showed a different mbr between dstn and vstn ( 1 . 51 0 . 18 vs . 1 . 76 0 . 22events/minute , wilcoxon rank sum test , p<0 . 05 ) and a trend in the difference between their mfr ( 12 . 78 vs . 15 . 05hz , wilcoxon rank sum test , p=0 . 053 ) only with the anatomically based_method . conclusion burst firing differs across stn subterritories . significance different functions of subthalamic domains might be reflected by distinctive burst signalling of its subterritories . 
a curved-beam bistable mechanism this paper presents a monolithic mechanically -bistable mechanism that does not rely on residual_stress for its bistability . the typical implementation of this mechanism is two curved centrally-clamped parallel beams , hereafter referred to as " double curved beams " . modal analysis and finite_element_analysis ( fea ) simulation of the curved beam are used to predict , explain , and design its bistable behavior . microscale double curved beams are fabricated by deep_reactive_ion_etching ( drie ) and their test_results agree well with the analytic predictions . approaches to tailor the bistable behavior of the curved beams are also presented . [1079]
embedded memory_bist for systems-on-a-chip embedded memory_bist for systems-on-a-chip title : embedded memory_bist for systems-on-a-chip embedded_memories consume an increasing portion of the die area in deep_submicron systems-on-a-chip ( socs ) . manufacturing_test of embedded_memories is an essential step in the soc production that screens out the defective chips and accelerates the transition from the yield_learning phase to the volume production phase of a new manufacturing technology . built-in self-test ( bist ) is establishing itself as an enabling technology that can effectively tackle the soc test problem . however , unless consciously implemented , its main limitations lie in elevated power_dissipation and area_overhead , and potential performance penalty and increased testing time , all of which directly influence the cost and quality of manufacturing_test . this thesis introduces two new embedded memory_bist architectures , whose objective is to reduce the cost of test and increase the test_quality to improve product reliability and yield . a distributed memory_bist approach with a serial interconnect scheme is first developed . this solution can concurrently support multiple memory test algorithms for heterogeneous memories with low power_dissipation during test and with relatively low gate and routing area_overhead , in addition to facilitating self-diagnosis . the distributed bist approach is then extended to a hardware/software co-testing memory_bist architecture for complex socs . by reusing the existing on-chip resources ( e . g . , processor cores and busses ) , further savings in area_overhead can be achieved and performance penalty for bus-connected memories can be eliminated . this is accomplished using a design space_exploration framework based on a new test_scheduling_algorithm that balances the usage of the existing on-chip resources and dedicated design_for_test ( dft ) hardware such that the functional power constraints are not exceeded during test , while trading-off the testing time against the dft area . iii acknowledgments i will begin by thanking my supervisor , nicola , for his valuable assistance and energetic support during this project . i also wish to thank my colleagues in the computer_aided_design and test who were of great help when ideas and questions needed to be discussed . in particular , i would like to express my appreciation to qiang xu for his help in debugging the test_scheduling_algorithm . i wish to acknowledge canadian microelectronics corporation ( cmc ) for their manufacturing grants , as well as the technical support and training they have provided . i am also grateful to the graduate students , faculty , administrative and technical members in de-for their continuous help during my study and research . members of my family and many more than i can 
the evaluation of an effective out-of-core run_time system in the context of parallel mesh generation we present an out-of-core run_time system that supports effective parallel computation of large irregular and adaptive problems , in particular unstructured mesh generation ( pumg ) . pumg is a highly challenging application due to intensive memory_accesses , unpredictable communication patterns , and variable and irregular data dependencies reflecting the unstructured spatial connectivity of mesh elements . our runtime system allows to transform the footprint of parallel applications from wide and shallow into narrow and deep by extending the memory utilization to the out-of-core level . it simplifies and streamlines the development of otherwise highly time consuming out-of-core applications as well as the converting of existing applications . it utilizes disk , network and memory_hierarchy to achieve high utilization of computing resources without sacrificing performance with pumg . the runtime system combines different programming paradigms : multi-threading within the nodes using industrial strength software_framework , one-sided active messages among the nodes , and an out-of-core subsystem for managing large datasets . we performed an evaluation on traditional parallel platforms to stress test all layers of the run_time system using three different pumg methods with significantly varying communication and synchronization patterns . we demonstrated high overlap in computation , communication , and disk i/o which results in good performance when computing large out-of-core problems . the runtime system adds very small overhead ( up to 18% on most configurations ) when computing in-core which means performance is not compromised . 
event-related rtms at encoding affects differently deep_and_shallow memory traces the "level of processing" effect is a classical finding of the experimental_psychology of memory . actually , the depth of information_processing at encoding predicts the accuracy of the subsequent episodic_memory performance . when the incoming stimuli are analyzed in terms of their meaning ( semantic , or deep , encoding ) , the memory_performance is superior with respect to the case in which the same stimuli are analyzed in terms of their perceptual features ( shallow encoding ) . as suggested by previous neuroimaging studies and by some preliminary findings with transcranial_magnetic_stimulation ( tms ) , the left prefrontal_cortex may play a role in semantic processing requiring the allocation of working memory resources . however , it still remains unclear whether deep_and_shallow encoding share or not the same cortical networks , as well as how these networks contribute to the "level of processing" effect . to investigate the brain areas casually involved in this phenomenon , we applied event-related repetitive tms ( rtms ) during deep ( semantic ) and shallow ( perceptual ) encoding of words . retrieval was subsequently tested without rtms interference . rtms applied to the left dorsolateral_prefrontal_cortex ( dlpfc ) abolished the beneficial effect of deep encoding on memory_performance , both in terms of accuracy ( decrease ) and reaction times ( increase ) . neither accuracy nor reaction times were instead affected by rtms to the right dlpfc or to an additional control site excluded by the memory process ( vertex ) . the fact that online measures of semantic processing at encoding were unaffected suggests that the detrimental effect on memory_performance for semantically encoded items took place in the subsequent consolidation phase . these results highlight the specific causal role of the left dlpfc among the wide left-lateralized cortical network engaged by long_term_memory , suggesting that it probably represents a crucial node responsible for the improved memory_performance induced by semantic processing . 
critical_path_selection for delay_fault_testing based upon a statistical_timing model critical_path_selection is an indispensable step for testing of small-size delay_defects . historically , this step relies on the construction of a set of worst_case paths , where the timing lengths of the paths are calculated based upon discrete-valued timing_models . the assumption of discrete-valued timing_models may become invalid for modeling delay effects in the deep sub-micron domain , where the effects of timing_defects and process_variations are often statistical in nature . this paper studies the problem of critical_path_selection for testing small-size delay_defects , assuming that circuit delays are statistical . we provide theoretical analysis to demonstrate that the new path_selection problem consists of two computationally intractable subproblems . then , we discuss practical heuristics and their performance with respect to each subproblem . using a statistical defect injection and timing-simulation framework , we present experimental_results to support our theoretical analysis . 
the university of texas at austin school of information : deep in the heart of the information_age uated within the thriving yet bohemian silicon hills and urban beauty of south central_texas . the school of information has a rich tradition of training information professionals of all ilk : archivists , record managers , librarians , intelligence analysts , and conservators . it is within this context that the human_computer_interaction ( hci ) /information_architecture ( ia ) /usability program has evolved , addressing the profusion of digital information and its impacts on users and communities . editorial board member of journals such as ijhcs and interacting with computers , and is known for his research in human information_processing ( e . g . , [2] and [3] ) . he brought an immediate focus on hci and ia , and has striven to grow that area of expertise , while anchoring it in the context of the strong , historical education of librarians and attention to information studies . the philosophy of our hci/ia/usability curriculum in the school of information is that this is a professional discipline , steeped in science and applied via well-established methods [1] . we have six assistant , associate , and full professors ( among our 23 ) with degrees in cognitive_psychology , information_science , or computer_science , all with real_world experience providing usability support ( big u-connoting attention to the full lifespan of user_interfaces , from user-requirements gathering , through design and prototype testing , to field_testing and maintenance ) . we offer a flexible curriculum for someone who wishes to become a professional in usability/ia/hci design . such a student would take core courses in research_methods and statistics , and in understanding and serving users . same student would likely take a six-course series of intro and advanced information_architecture , intro and advanced digital_media design , and intro and advanced usability . he or she would likely take another class in human information_processing , maybe one in information_science and knowledge systems : concepts in information_retrieval . he or she would complete the master's_degree with a thesis , or maybe a capstone experience where he or she carried out some industrial strength piece of work for a company or nonprofit or government entity . 
numerical_stability in linear_programming and semidefinite_programming i hereby declare that i am the sole author of this thesis . this is a true copy of the thesis , including any required final revisions , as accepted by my examiners . i understand that my thesis may be made electronically available to the public . ii abstract we study numerical_stability for interior-point methods applied to linear_programming , lp , and semidefinite_programming , sdp . we analyze the difficulties inherent in current methods and present robust algorithms . we start with the error bound analysis of the search directions for the_normal_equation approach for lp . our error analysis explains the surprising fact that the ill-conditioning is not a significant problem for the_normal_equation system . we also explain why most of the popular lp solvers have a default stop tolerance of only 10 8 when the machine precision on a 32-bit computer is approximately 10 16 . we then propose a simple alternative approach for the_normal_equation based interior_point_method . this approach has better numerical_stability than the_normal_equation based_method . although , our approach is not competitive in terms of cpu time for the netlib problem set , we do obtain higher_accuracy . in addition , we obtain significantly smaller cpu times compared to the_normal_equation based direct solver , when we solve well-conditioned , huge , and sparse problems by using our iterative based linear solver . additional techniques discussed are : crossover; purification step; and no backtracking . finally , we present an algorithm to construct sdp problem_instances with prescribed strict_complementarity_gaps . we then introduce two measures of strict_complementarity_gaps . we empirically show that : ( i ) these measures can be evaluated accurately; ( ii ) the size of the strict_complementarity_gaps correlate well with the number of iteration for the sdpt3 solver , as well as with the local asymptotic convergence rate; and ( iii ) large strict_complementarity_gaps , coupled with the failure of slater's condition , correlate well with loss of accuracy in the solutions . in addition , the numerical tests show that there is no correlation between the strict_complementarity_gaps and the geometrical measure used in [31] , or with renegar's condition_number . iii acknowledgments i would like to express my deep thanks to my supervisor , professor henry wolkowicz . without his continues guidance and support , i could not finish this thesis . i would also like to thank the committee members , professor miguel anjos , professor chek beng chua , professor levent tun el , and professor yin zhang , for their detailed comments and careful 
modeling and testing of interference faults in the nano nand flash_memory advance of the fabrication technology has enhanced the size and density for the nand flash_memory but also brought new types of defects which need to be tested for the quality consideration . this work analyzes three types of physical_defects for the deep nano-meter nand flash_memory based on the circuit level simulation and proposes new categories of interference faults ( ifs ) . testing algorithm is also proposed to test the faults under the worst case condition . the algorithm , in addition to test ifs , can also detect the conventional address faults , disturbance faults and other ram-like faults for the nand flash . 
perspectives on chinese question_answering_systems question_answering ( qa ) is becoming an increasingly_important research area in natural_language_processing . since 1999 , many international question_answering contests have been held at conferences and workshops , such as trec , clef , and ntcir . thus far , eleven languages spanish have been tested on monolingual or cross-lingual question_answering tasks . although chinese is the second most popular language in the world , ntcir only conducted the first qa contest in chinese this year . the results_reveal that there seems to be a performance gap between chinese question_answering_systems and some systems of other languages . in this paper , we review previous_works on chinese_question_answering , including our two systems on frequently asked questions and factoid questions . comparing chinese with other languages , word_segmentation is a key problem in chinese_question_answering . we review studies on word_segmentation and discuss important issues , such as part_of_speech_tagging , named_entity_recognition , deep_and_shallow parsing , semantic_role and relation labeling etc . , which are helpful for building qa systems . machine_learning approaches currently represent the main stream on many qa research issues , we believe , by efficiently utilizing the above resources , the performance of machine_learning approaches can be improved further in chinese_question_answering . 
blondie24 , playing at the edge of ai ( book review ) 1 computer_science has failed abysmally at producing machines which display intelligence . according to fogel , the last 50 years of effort in artificial_intelligence have been on the wrong track , leaving us no closer to the goal than when we started . the wrong track has been the attempt to make the computer imitate our behavior . scientists striving to build an artificial_intelligence load the computer with knowledge on a chosen topic , along with an algorithm to do the associated task . the hope is that the computer will equal or surpass our intelligence for that subject . this approach dates back to the turing_test , which fogel points out has been misquoted and misinterpreted almost from day one . misquoted or not , the turing_test has a computer pretend to be human , and this paradigm became a signpost saying that the road to artificial_intelligence is through mimicry of human behavior . according to fogel , however , that path leads only to an illusion of intelligence for example the kind of wooden intelligence exhibited by deep_blue . if imitation of human_intelligence has not led to machine intelligence , then what will ? what is in-telligence ? fogel is critical in general of researchers in this field for skirting this last question . he believes that if the field of artificial_intelligence had started with a proper definition of intelligence then there would have been a better chance of creating it . that is common sense knowing what you are trying to build is crucial . fogel therefore gives the following definition of intelligence . intelligence is the capacity of a decision_making system to adapt its behavior to meet goals in a range of environments . fogel looks to nature for an example of intelligence . he describes the intricate and seemingly clever behavior of a certain species of wasp . however , he points out that an individual wasp is fixed in its behavior . an experiment by jean henri fabr described in the notes section of the book seems to make this clear . the wasp , fogel says , is like the 'proverbial robot' , an automaton with no adaptive behavior . therefore the individual wasp is not intelligent . however , fogel considers the species of wasp to be intelligent as a group or system , since it evolves to meet a changing environment . the species is what has learned the intricate behavior . the point is made that in general , intelligence requires a reservoir of knowledge and 

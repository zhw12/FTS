{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n__author__ : Hanwen Zha\\n__description__: Function for attach a node to the general taxonomy\\n__latest_update__: 2/27/2018\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "__author__ : Hanwen Zha\n",
    "__description__: Function for attach a node to the general taxonomy\n",
    "__latest_update__: 2/27/2018\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import ES\n",
    "from util import rmTag_concat\n",
    "from util import wrapTag\n",
    "from util import rmUnderscore\n",
    "from util import cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = '/home/hanwen/Desktop/demo/NSCTA-Demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load word embedding\n",
    "file_word2vec = '{}/concept_based_retrieval/output/SEMANTIC_SCHOLAR120w/wordvec'.format(root_dir)\n",
    "model = word2vec.Word2Vec.load(file_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_word2vec = '{}/embedding/word2vec_tuned_pretrained.bin'.format(root_dir)\n",
    "model2 = word2vec.Word2Vec.load(file_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load taxonomy\n",
    "with open('{}/webUI/static/tree.json_bak'.format(root_dir)) as f_in:\n",
    "    taxonomy_data = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import spacy\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# import ipdb\n",
    "from spacy.tokens import Doc\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "try:\n",
    "  unicode;\n",
    "except NameError as e:\n",
    "  unicode = str\n",
    "\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser', 'tagger'])\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class taxonomy_tree():\n",
    "    def __init__(self):\n",
    "        self.tree_list = []\n",
    "    def parse_tree(self, node, level):\n",
    "        if level == 0:\n",
    "            self.tree_list.append({'name':node['name'], 'keywords':node['keywords'], 'level':level})\n",
    "        if 'children' not in node:\n",
    "            self.tree_list.append({'name':node['name'], 'keywords':node['keywords'], 'level':level})\n",
    "            return\n",
    "        if 'children' in node:\n",
    "            self.tree_list.append({'name':node['name'], 'keywords':node['keywords'], 'level':level})\n",
    "            for child in node['children']:\n",
    "                n = self.parse_tree(child, level+1)\n",
    "    def output_tree_by_level(self):\n",
    "        output_tree_list = []\n",
    "        for lvl in range(4):\n",
    "            for node in self.tree_list:\n",
    "                if node['level'] == lvl:\n",
    "                    output_tree_list.append(node)\n",
    "        return output_tree_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(matched_phrase_docs):\n",
    "    flat_matched_phrase_docs = []\n",
    "    for matched_phrase_doc in matched_phrase_docs:\n",
    "        for doc in matched_phrase_doc:\n",
    "            flat_matched_phrase_docs.append(doc)\n",
    "    return flat_matched_phrase_docs\n",
    "\n",
    "def flatten_once(matched_phrase_docs):\n",
    "    flat_matched_phrase_docs = []\n",
    "    for matched_phrase_doc in matched_phrase_docs:\n",
    "        for doc in set(matched_phrase_doc):\n",
    "            flat_matched_phrase_docs.append(doc)\n",
    "    return flat_matched_phrase_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = taxonomy_tree()\n",
    "tree.parse_tree(taxonomy_data, 0)\n",
    "tree_list = tree.output_tree_by_level()\n",
    "# tree_list = tree.tree_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "general_taxonomy_phrases = []\n",
    "general_taxonomy_keywords = []\n",
    "for node in tree_list:\n",
    "    general_taxonomy_phrases.append(node['name'])\n",
    "    general_taxonomy_keywords.extend(node['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "general_taxonomy_phrases.remove('quantum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "general_taxonomy_phrases = [rmUnderscore(node) for node in general_taxonomy_phrases]\n",
    "general_taxonomy_keywords = [rmUnderscore(node) for node in general_taxonomy_keywords] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INDEX_NAME = \"taxongen1\"\n",
    "TYPE_NAME = \"taxongen_docs\"\n",
    "es = ES(index_name=INDEX_NAME, type_name=TYPE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: low dimensional, Total hits = 167\n",
      "Phrase: low dimensional, Total hits = 6498\n",
      "Phrase: key agreement protocol, Total hits = 209\n",
      "Phrase: key agreement protocol, Total hits = 652\n",
      "Phrase: xml, Total hits = 4372\n",
      "Phrase: xml, Total hits = 10930\n",
      "Phrase: bioinformatics, Total hits = 1109\n",
      "Phrase: bioinformatics, Total hits = 5549\n",
      "Phrase: face detection and recognition, Total hits = 20\n",
      "Phrase: face detection and recognition, Total hits = 1409\n",
      "Phrase: web pages, Total hits = 917\n",
      "Phrase: web pages, Total hits = 10410\n",
      "Phrase: google, Total hits = 445\n",
      "Phrase: google, Total hits = 3963\n",
      "Phrase: ddh assumption, Total hits = 4\n",
      "Phrase: ddh assumption, Total hits = 803\n",
      "Phrase: keys, Total hits = 5619\n",
      "Phrase: keys, Total hits = 79329\n",
      "Phrase: feed forward neural networks, Total hits = 95\n",
      "Phrase: feed forward neural networks, Total hits = 1632\n",
      "Phrase: image processing applications, Total hits = 60\n",
      "Phrase: image processing applications, Total hits = 11889\n",
      "Phrase: kohonen, Total hits = 173\n",
      "Phrase: kohonen, Total hits = 831\n",
      "Phrase: emergency, Total hits = 3990\n",
      "Phrase: emergency, Total hits = 47123\n",
      "Phrase: relational databases, Total hits = 986\n",
      "Phrase: relational databases, Total hits = 14502\n",
      "Phrase: affine invariant transform, Total hits = 0\n",
      "Phrase: affine invariant transform, Total hits = 398\n",
      "Phrase: discrete logarithm, Total hits = 205\n",
      "Phrase: discrete logarithm, Total hits = 1376\n",
      "Phrase: based on wavelet, Total hits = 233\n",
      "Phrase: based on wavelet, Total hits = 7468\n",
      "Phrase: dss, Total hits = 158\n",
      "Phrase: dss, Total hits = 975\n",
      "Phrase: association rule mining, Total hits = 365\n",
      "Phrase: association rule mining, Total hits = 2615\n",
      "Phrase: partially observable, Total hits = 314\n",
      "Phrase: partially observable, Total hits = 4226\n",
      "Phrase: secure multi party computation, Total hits = 47\n",
      "Phrase: secure multi party computation, Total hits = 1240\n",
      "Phrase: online community, Total hits = 385\n",
      "Phrase: online community, Total hits = 7734\n",
      "Phrase: path planner, Total hits = 46\n",
      "Phrase: path planner, Total hits = 1183\n",
      "Phrase: neural networks, Total hits = 13955\n",
      "Phrase: neural networks, Total hits = 30504\n",
      "Phrase: virtual characters, Total hits = 117\n",
      "Phrase: virtual characters, Total hits = 1438\n",
      "Phrase: rdf, Total hits = 753\n",
      "Phrase: rdf, Total hits = 2908\n",
      "Phrase: feature selection method, Total hits = 132\n",
      "Phrase: feature selection method, Total hits = 8862\n",
      "Phrase: biological networks, Total hits = 237\n",
      "Phrase: biological networks, Total hits = 5951\n",
      "Phrase: multiclass, Total hits = 512\n",
      "Phrase: multiclass, Total hits = 1158\n",
      "Phrase: agent systems, Total hits = 2280\n",
      "Phrase: agent systems, Total hits = 23236\n",
      "Phrase: large graphs, Total hits = 162\n",
      "Phrase: large graphs, Total hits = 9290\n",
      "Phrase: mobile robot localization, Total hits = 119\n",
      "Phrase: mobile robot localization, Total hits = 2031\n",
      "Phrase: multiple classifiers, Total hits = 152\n",
      "Phrase: multiple classifiers, Total hits = 4843\n",
      "Phrase: judgments, Total hits = 464\n",
      "Phrase: judgments, Total hits = 4237\n",
      "Phrase: belief desire intention bdi, Total hits = 1\n",
      "Phrase: belief desire intention bdi, Total hits = 838\n",
      "Phrase: escrow, Total hits = 67\n",
      "Phrase: escrow, Total hits = 283\n",
      "Phrase: key exchange, Total hits = 420\n",
      "Phrase: key exchange, Total hits = 3220\n",
      "Phrase: artificial neural network, Total hits = 1912\n",
      "Phrase: artificial neural network, Total hits = 7102\n",
      "Phrase: segmentation, Total hits = 12467\n",
      "Phrase: segmentation, Total hits = 38839\n",
      "Phrase: discrete time, Total hits = 1574\n",
      "Phrase: discrete time, Total hits = 15798\n",
      "Phrase: belief functions, Total hits = 196\n",
      "Phrase: belief functions, Total hits = 1887\n",
      "Phrase: structure from motion, Total hits = 207\n",
      "Phrase: structure from motion, Total hits = 3360\n",
      "Phrase: planning system, Total hits = 193\n",
      "Phrase: planning system, Total hits = 20677\n",
      "Phrase: subgraph, Total hits = 871\n",
      "Phrase: subgraph, Total hits = 4001\n",
      "Phrase: information theoretically secure, Total hits = 38\n",
      "Phrase: information theoretically secure, Total hits = 1216\n",
      "Phrase: philosophy, Total hits = 278\n",
      "Phrase: philosophy, Total hits = 4300\n",
      "Phrase: secure two party computation, Total hits = 43\n",
      "Phrase: secure two party computation, Total hits = 1465\n",
      "Phrase: human pose, Total hits = 145\n",
      "Phrase: human pose, Total hits = 2968\n",
      "Phrase: evolution strategies, Total hits = 237\n",
      "Phrase: evolution strategies, Total hits = 3113\n",
      "Phrase: visual surveillance, Total hits = 86\n",
      "Phrase: visual surveillance, Total hits = 1064\n",
      "Phrase: rl, Total hits = 190\n",
      "Phrase: rl, Total hits = 1699\n",
      "Phrase: packets, Total hits = 3984\n",
      "Phrase: packets, Total hits = 25314\n",
      "Phrase: d reconstruction, Total hits = 94\n",
      "Phrase: d reconstruction, Total hits = 1942\n",
      "Phrase: wavelet analysis, Total hits = 125\n",
      "Phrase: wavelet analysis, Total hits = 3292\n",
      "Phrase: ensembles, Total hits = 2340\n",
      "Phrase: ensembles, Total hits = 7356\n",
      "Phrase: container terminal, Total hits = 106\n",
      "Phrase: container terminal, Total hits = 2172\n",
      "Phrase: discriminative training, Total hits = 165\n",
      "Phrase: discriminative training, Total hits = 3840\n",
      "Phrase: electronic marketplaces, Total hits = 104\n",
      "Phrase: electronic marketplaces, Total hits = 824\n",
      "Phrase: graded, Total hits = 726\n",
      "Phrase: graded, Total hits = 5173\n",
      "Phrase: research activities, Total hits = 43\n",
      "Phrase: research activities, Total hits = 25155\n",
      "Phrase: disaster recovery, Total hits = 66\n",
      "Phrase: disaster recovery, Total hits = 1027\n",
      "Phrase: pervasive, Total hits = 1673\n",
      "Phrase: pervasive, Total hits = 6498\n",
      "Phrase: evolution strategy, Total hits = 237\n",
      "Phrase: evolution strategy, Total hits = 3113\n",
      "Phrase: sensitive information, Total hits = 47\n",
      "Phrase: sensitive information, Total hits = 9240\n",
      "Phrase: imbalanced, Total hits = 305\n",
      "Phrase: imbalanced, Total hits = 760\n",
      "Phrase: confidential data, Total hits = 16\n",
      "Phrase: confidential data, Total hits = 2538\n",
      "Phrase: logic programming, Total hits = 2354\n",
      "Phrase: logic programming, Total hits = 12748\n",
      "Phrase: bio text mining, Total hits = 1\n",
      "Phrase: bio text mining, Total hits = 882\n",
      "Phrase: face identification, Total hits = 57\n",
      "Phrase: face identification, Total hits = 1694\n",
      "Phrase: image processing, Total hits = 1545\n",
      "Phrase: image processing, Total hits = 36121\n",
      "Phrase: subspace, Total hits = 1913\n",
      "Phrase: subspace, Total hits = 7083\n",
      "Phrase: knowledge representation, Total hits = 638\n",
      "Phrase: knowledge representation, Total hits = 12542\n",
      "Phrase: neurons, Total hits = 2650\n",
      "Phrase: neurons, Total hits = 11035\n",
      "Phrase: unauthorised, Total hits = 3\n",
      "Phrase: unauthorised, Total hits = 139\n",
      "Phrase: image thresholding, Total hits = 58\n",
      "Phrase: image thresholding, Total hits = 4052\n",
      "Phrase: epistemic, Total hits = 431\n",
      "Phrase: epistemic, Total hits = 1441\n",
      "Phrase: local search, Total hits = 895\n",
      "Phrase: local search, Total hits = 10438\n",
      "Phrase: evolutionary algorithms, Total hits = 1641\n",
      "Phrase: evolutionary algorithms, Total hits = 9092\n",
      "Phrase: user privacy, Total hits = 80\n",
      "Phrase: user privacy, Total hits = 5273\n",
      "Phrase: image matching, Total hits = 220\n",
      "Phrase: image matching, Total hits = 11470\n",
      "Phrase: computer vision, Total hits = 766\n",
      "Phrase: computer vision, Total hits = 13121\n",
      "Phrase: point correspondences, Total hits = 54\n",
      "Phrase: point correspondences, Total hits = 9817\n",
      "Phrase: agents, Total hits = 17119\n",
      "Phrase: agents, Total hits = 44115\n",
      "Phrase: fuzzy inference, Total hits = 377\n",
      "Phrase: fuzzy inference, Total hits = 2141\n",
      "Phrase: community structure, Total hits = 204\n",
      "Phrase: community structure, Total hits = 18470\n",
      "Phrase: implicit feedback, Total hits = 56\n",
      "Phrase: implicit feedback, Total hits = 525\n",
      "Phrase: gossip, Total hits = 410\n",
      "Phrase: gossip, Total hits = 925\n",
      "Phrase: itemsets, Total hits = 488\n",
      "Phrase: itemsets, Total hits = 1199\n",
      "Phrase: bagging, Total hits = 603\n",
      "Phrase: bagging, Total hits = 3274\n",
      "Phrase: knowledge based, Total hits = 2647\n",
      "Phrase: knowledge based, Total hits = 52935\n",
      "Phrase: transportation, Total hits = 3210\n",
      "Phrase: transportation, Total hits = 17541\n",
      "Phrase: mining, Total hits = 11346\n",
      "Phrase: mining, Total hits = 27178\n",
      "Phrase: motor control, Total hits = 141\n",
      "Phrase: motor control, Total hits = 4487\n",
      "Phrase: mobile nodes, Total hits = 112\n",
      "Phrase: mobile nodes, Total hits = 9079\n",
      "Phrase: semistructured data, Total hits = 114\n",
      "Phrase: semistructured data, Total hits = 545\n",
      "Phrase: encryption keys, Total hits = 32\n",
      "Phrase: encryption keys, Total hits = 4578\n",
      "Phrase: hardness of factoring, Total hits = 1\n",
      "Phrase: hardness of factoring, Total hits = 2539\n",
      "Phrase: fisher, Total hits = 397\n",
      "Phrase: fisher, Total hits = 2184\n",
      "Phrase: information extraction, Total hits = 866\n",
      "Phrase: information extraction, Total hits = 27418\n",
      "Phrase: self organisation, Total hits = 297\n",
      "Phrase: self organisation, Total hits = 1131\n",
      "Phrase: aes, Total hits = 468\n",
      "Phrase: aes, Total hits = 1928\n",
      "Phrase: correspondence, Total hits = 1243\n",
      "Phrase: correspondence, Total hits = 61153\n",
      "Phrase: base classifiers, Total hits = 207\n",
      "Phrase: base classifiers, Total hits = 25078\n",
      "Phrase: trajectory generation, Total hits = 144\n",
      "Phrase: trajectory generation, Total hits = 4252\n",
      "Phrase: structured documents, Total hits = 174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: structured documents, Total hits = 10277\n",
      "Phrase: development managment, Total hits = 14\n",
      "Phrase: development managment, Total hits = 40715\n",
      "Phrase: multilayer perceptrons, Total hits = 261\n",
      "Phrase: multilayer perceptrons, Total hits = 2313\n",
      "Phrase: thai, Total hits = 198\n",
      "Phrase: thai, Total hits = 409\n",
      "Phrase: dec pomdps, Total hits = 27\n",
      "Phrase: dec pomdps, Total hits = 1264\n",
      "Phrase: bisimulation, Total hits = 424\n",
      "Phrase: bisimulation, Total hits = 1348\n",
      "Phrase: spatial data, Total hits = 535\n",
      "Phrase: spatial data, Total hits = 17403\n",
      "Phrase: marketplace, Total hits = 415\n",
      "Phrase: marketplace, Total hits = 2271\n",
      "Phrase: spiking, Total hits = 1576\n",
      "Phrase: spiking, Total hits = 4071\n",
      "Phrase: artificial intelligence, Total hits = 940\n",
      "Phrase: artificial intelligence, Total hits = 8326\n",
      "Phrase: manifold, Total hits = 1478\n",
      "Phrase: manifold, Total hits = 5544\n",
      "Phrase: belief change, Total hits = 86\n",
      "Phrase: belief change, Total hits = 1373\n",
      "Phrase: minds, Total hits = 615\n",
      "Phrase: minds, Total hits = 7219\n",
      "Phrase: mobile ad hoc networks, Total hits"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 12203 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# es2 = ES(index_name=INDEX_NAME, type_name=TYPE_NAME, size=1)\n",
    "# total_dict = Counter()\n",
    "# total_dict_loose = Counter()\n",
    "\n",
    "# for phrase in tqdm_notebook(set(general_taxonomy_phrases + general_taxonomy_keywords)):\n",
    "# #     _, total = es2.search_articles_by_phrase(phrase.replace('_', ' '))\n",
    "#     _, total_loose = es2.search_articles_by_common_term(phrase.replace('_', ' '))\n",
    "#     total_dict[phrase.replace(' ', '_')] = total\n",
    "#     total_dict_loose[phrase.replace(' ', '_')] = total_loose\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('total_dict.json', 'w') as f:\n",
    "#     json.dump(total_dict, f)\n",
    "\n",
    "# with open('total_dict_loose.json', 'w') as f:\n",
    "#     json.dump(total_dict_loose, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('total_dict.json') as fin:\n",
    "    total_dict = json.load(fin)\n",
    "with open('total_dict_loose.json') as fin:\n",
    "    total_dict_loose = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_generator = (i for i in general_taxonomy_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: quantum learning, Total hits = 1\n",
      "Phrase: quantum learning, Total hits = 274\n",
      "Finish saving generating 1 documents\r",
      "Finish saving generating 274 documents\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "isprint = False\n",
    "# query = next(query_generator)\n",
    "query = 'quantum_learning'\n",
    "\n",
    "ids, total = es.search_articles_by_phrase(query.replace('_', ' '))\n",
    "\n",
    "ids_loose, total_loose = es.search_articles_by_common_term(query.replace('_', ' '))\n",
    "\n",
    "texts = es.search_articles_by_ids(ids)\n",
    "texts = [cleanText(line) for line in texts]\n",
    "\n",
    "texts_loose = es.search_articles_by_ids(ids_loose)\n",
    "texts_loose = [cleanText(line) for line in texts_loose]\n",
    "\n",
    "sim_list = []\n",
    "for node in tree_list:\n",
    "    if node['level'] >= 4:\n",
    "        continue\n",
    "    try:\n",
    "        sim = model.wv.similarity(wrapTag(node['name']), wrapTag(query))\n",
    "        sim_list.append([node['name'], sim])\n",
    "    except KeyError as e:\n",
    "        sim_list.append([node['name'], 0])\n",
    "\n",
    "if isprint:\n",
    "    print('\\n')\n",
    "    print('sim_list -->')\n",
    "    pprint(sim_list[:10])\n",
    "\n",
    "sorted_sim_list = sorted(sim_list, key=lambda x:-x[1])\n",
    "if isprint:\n",
    "    print('\\n')\n",
    "    print('sorted_sim_list -->')\n",
    "    pprint(sorted_sim_list[:10])\n",
    "\n",
    "# these phrases are limited\n",
    "phrases = general_taxonomy_phrases\n",
    "\n",
    "# build phrase matcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "for v in phrases:\n",
    "    matcher.add(v, None, nlp(unicode(v))) # python3 is unicode\n",
    "\n",
    "# matcher.add(v, None, nlp(unicode(query.replace('_', ' '))))\n",
    "\n",
    "phrase_docs = []\n",
    "for text in tqdm_notebook(texts):\n",
    "    doc = nlp(text.lower())\n",
    "    matches = matcher(doc)\n",
    "    nps = []\n",
    "    prev_st = 0\n",
    "    prev_ed = 0\n",
    "    for m in matches:\n",
    "        tokens = text.lower().split(' ')\n",
    "        st = m[1]\n",
    "        ed = m[2]\n",
    "        if ed > prev_ed:\n",
    "            np = {'st':st, 'ed':ed, 'text': '_'.join(tokens[st:ed])}\n",
    "            nps.append(np)\n",
    "            prev_ed = ed\n",
    "    phrase_docs.append([np['text'] for np in nps])\n",
    "\n",
    "phrase_docs_loose = []\n",
    "for text in tqdm_notebook(texts_loose):\n",
    "    doc = nlp(text.lower())\n",
    "    matches = matcher(doc)\n",
    "    nps = []\n",
    "    prev_st = 0\n",
    "    prev_ed = 0\n",
    "    for m in matches:\n",
    "        tokens = text.lower().split(' ')\n",
    "        st = m[1]\n",
    "        ed = m[2]\n",
    "        if ed > prev_ed:\n",
    "            np = {'st':st, 'ed':ed, 'text': '_'.join(tokens[st:ed])}\n",
    "            nps.append(np)\n",
    "            prev_ed = ed\n",
    "    phrase_docs_loose.append([np['text'] for np in nps])\n",
    "\n",
    "flat_matched_phrase_docs = flatten(phrase_docs)\n",
    "flat_matched_phrase_docs_loose = flatten(phrase_docs_loose)\n",
    "\n",
    "coocur_cnter = Counter(flat_matched_phrase_docs)\n",
    "coocur_cnter_loose = Counter(flat_matched_phrase_docs_loose)\n",
    "\n",
    "# -------------------\n",
    "flat_matched_phrase_docs = flatten_once(phrase_docs)\n",
    "flat_matched_phrase_docs_loose = flatten_once(phrase_docs_loose)\n",
    "coocur_cnter_once = Counter(flat_matched_phrase_docs)\n",
    "coocur_cnter_loose_once = Counter(flat_matched_phrase_docs_loose)\n",
    "\n",
    "if isprint:\n",
    "    print('\\n')\n",
    "    print('coocur_cnter -->')\n",
    "    pprint(coocur_cnter.most_common(20))\n",
    "\n",
    "    print('\\n')\n",
    "    print('coocur_cnter_loose -->')\n",
    "    \n",
    "    pprint(coocur_cnter_loose.most_common(20))\n",
    "\n",
    "\n",
    "    \n",
    "    print('\\n')\n",
    "    print('coocur_cnter -->')\n",
    "    pprint(coocur_cnter_once.most_common(20))\n",
    "\n",
    "    print('\\n')\n",
    "    print('coocur_cnter_loose -->')\n",
    "    \n",
    "    pprint(coocur_cnter_loose_once.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_learning 40 274 21728 0.45 0.29\n",
      "neural_network 34 274 30504 0.39 0.25\n",
      "cryptography 23 274 5003 0.41 0.17\n",
      "oblivious_transfer 9 274 665 0.49 0.07\n",
      "reasoning 9 274 64993 0.45 0.07\n",
      "information_retrieval 5 274 23896 0.33 0.04\n",
      "geometry 5 274 17466 0.39 0.04\n"
     ]
    }
   ],
   "source": [
    "if total < 500:\n",
    "    use_dict = total_dict_loose\n",
    "    use_total = total_loose\n",
    "    use_coour_counter_once = coocur_cnter_loose_once\n",
    "else:\n",
    "    use_dict = total_dict\n",
    "    use_total = total\n",
    "    use_coour_counter_once = coocur_cnter_once\n",
    "    \n",
    "norm = sum([p[1] for p in use_coour_counter_once.most_common(10)])\n",
    "    \n",
    "for phrase, freq in use_coour_counter_once.most_common(10):\n",
    "#     flag = 0\n",
    "#     freq/min(use_dict.get(phrase, 100000), use_total, 10000) >= ratio_threshold and\n",
    "    if freq < hard_threshold:  \n",
    "        continue\n",
    "    try:\n",
    "        embeding_sim = model.similarity(wrapTag(phrase), wrapTag(query))\n",
    "    except:\n",
    "        embeding_sim = None\n",
    "    if embeding_sim and embeding_sim < 0.3:\n",
    "        continue\n",
    "        \n",
    "    print(phrase, freq, use_total, use_dict[phrase], end=' ')\n",
    "    print(round(embeding_sim, 2), end = ' ')\n",
    "    print(round(freq/norm, 2))\n",
    "#     flag = 'P'\n",
    "#     if embeding_sim:\n",
    "#         if embeding_sim >= 0.6:\n",
    "#             flag = 'S'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratio_threshold = 0.05\n",
    "hard_threshold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # coocur by search\n",
    "# with open('../../data/query_docs/{}.txt'.format(query)) as fin:\n",
    "#     texts_loose = []\n",
    "#     for line in fin:\n",
    "#         line = line.replace('_', ' ')\n",
    "#         line = cleanText(line)\n",
    "#         if line:\n",
    "#             texts_loose.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# texts_keep_phrases = []\n",
    "# for line in texts:\n",
    "#     words = line.strip().split()\n",
    "#     words = [w for w in words if '_' in w]\n",
    "#     texts_keep_phrases.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "code_folding": [
     0.0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get phrase list\n",
    "# only fit for quantum learning and quantum cryptography\n",
    "# text file need to be prepared first\n",
    "\n",
    "# phrases = []\n",
    "# with open('{}/local-embedding/data/{}/input/keywords.txt'.format(root_dir, query)) as fin:\n",
    "#     for line in fin:\n",
    "#         phrase = line.lower().strip().split('_')\n",
    "#         phrase = ' '.join(phrase)\n",
    "#         phrases.append(phrase)\n",
    "\n",
    "# phrases = list(set(phrases + general_taxonomy_phrases))\n",
    "# phrases = [unicode(p) for p in phrases]\n",
    "\n",
    "# get query relevant corpus\n",
    "# only fit for quantum learning and quantum cryptography\n",
    "# text file need to be prepared first\n",
    "\n",
    "# texts = []\n",
    "# with open('{}/local-embedding/data/{}/input/papers.txt'.format(root_dir, query)) as fin:\n",
    "#     for line in fin:\n",
    "#         line = line.replace('_', ' ')\n",
    "#         line = cleanText(line)\n",
    "#         if line:\n",
    "#             texts.append(line)\n",
    "\n",
    "# with open('{}/data/query_docs/arxiv/{}_with_fulltext.json'.format(root_dir, '_AND_'.join(query.split('_')))) as fin:\n",
    "#     for line in fin:\n",
    "#         line = json.loads(line)\n",
    "#         abstract = line['abstract']\n",
    "#         abstract = cleanText(abstract)\n",
    "#         if abstract:\n",
    "#             texts.append(abstract)\n",
    "\n",
    "# # coocur by search (common term)\n",
    "# with open('../../data/query_docs/{}.txt'.format(query)) as fin:\n",
    "#     texts_loose = []\n",
    "#     for line in fin:\n",
    "#         line = cleanText(line)\n",
    "#         if line:\n",
    "#             texts_loose.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matched_phrase_docs = []\n",
    "for phrase_d in phrase_docs:\n",
    "    if query in phrase_d:\n",
    "        matched_phrase_docs.append([d.lower() for d in phrase_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(matched_phrase_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat_matched_phrase_docs = flatten(matched_phrase_docs)\n",
    "flat_matched_phrase_docs_loose = flatten(phrase_docs_loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coocur_cnter = Counter(flat_matched_phrase_docs)\n",
    "\n",
    "coocur_cnter_loose = Counter(flat_matched_phrase_docs_loose)\n",
    "coocur_cnter_loose.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bdi 1.0 674\n",
      "intelligent_agents 0.615422694555 23\n",
      "reasoning 0.527531676139 113\n",
      "virtual_agents 0.521432768301 3\n",
      "cognition 0.506350408816 1\n",
      "logic_programming 0.479229724513 2\n",
      "reinforcement_learning 0.441630086817 3\n",
      "pomdps 0.440560558399 5\n",
      "ontologies 0.434279850902 1\n",
      "markov_decision_processes 0.41674360131 2\n",
      "planner 0.40754569657 6\n",
      "semantic_web 0.407159572861 10\n",
      "natural_language 0.359362183561 1\n",
      "requirements_engineering 0.338489398075 3\n",
      "emergency_response 0.3382106412 2\n",
      "intrusion_detection 0.272758775944 2\n",
      "graphical_models 0.251794794745 1\n",
      "information_retrieval 0.24297572823 2\n",
      "neural_network 0.185705947814 5\n",
      "web_content 0.165298131255 3\n",
      "manufacturing_system 0 3\n"
     ]
    }
   ],
   "source": [
    "for phrase, score in sorted_sim_list:\n",
    "    if coocur_cnter[phrase]:\n",
    "        print(phrase, score, coocur_cnter[phrase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bdi 1.0 1203\n",
      "intelligent_agents 0.615422694555 69\n",
      "multi_agent_systems 0.553742866717 4\n",
      "reasoning 0.527531676139 391\n",
      "virtual_agents 0.521432768301 14\n",
      "cognition 0.506350408816 13\n",
      "logic_programming 0.479229724513 20\n",
      "reinforcement_learning 0.441630086817 10\n",
      "pomdps 0.440560558399 8\n",
      "ontologies 0.434279850902 10\n",
      "intelligent_decision_support_systems 0.432681444238 1\n",
      "semantic_web_services 0.430769278309 1\n",
      "markov_decision_processes 0.41674360131 3\n",
      "planner 0.40754569657 8\n",
      "semantic_web 0.407159572861 26\n",
      "social_interaction 0.404729673139 1\n",
      "swarm_intelligence 0.369946202315 1\n",
      "natural_language 0.359362183561 5\n",
      "pervasive_computing 0.346570987073 1\n",
      "path_planning 0.340350956931 1\n",
      "access_control 0.340031544903 3\n",
      "requirements_engineering 0.338489398075 3\n",
      "emergency_response 0.3382106412 2\n",
      "business_intelligence 0.311680722299 1\n",
      "computer_science 0.310400427224 9\n",
      "machine_learning 0.281612373504 2\n",
      "intrusion_detection 0.272758775944 2\n",
      "graphical_models 0.251794794745 1\n",
      "intelligent_transportation 0.244349306154 2\n",
      "information_retrieval 0.24297572823 3\n",
      "search_engine 0.218528998701 1\n",
      "neural_network 0.185705947814 7\n",
      "association_rules 0.178390950513 2\n",
      "face_recognition 0.166810872819 1\n",
      "web_content 0.165298131255 3\n",
      "planning_system 0 4\n",
      "manufacturing_system 0 4\n"
     ]
    }
   ],
   "source": [
    "for phrase, score in sorted_sim_list:\n",
    "    if coocur_cnter_loose[phrase]:\n",
    "        print(phrase, score, coocur_cnter_loose[phrase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.wv.most_similar(wrapTag('deep learning'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.wv.similar_by_vector(model.wv['quantum']+model.wv['wells'], topn=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

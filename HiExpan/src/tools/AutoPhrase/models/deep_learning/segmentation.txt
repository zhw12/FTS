Supporting Self-explanation in a <phrase>Data</phrase> Normalization <phrase>Tutor</phrase> Self-explanation is one of the most effective <phrase>learning strategies</phrase>, resulting in deep <phrase>knowledge</phrase>. In this <phrase>paper</phrase>, we discuss how self-explanation is scaffolded in NORMIT, a <phrase>data</phrase> normalization <phrase>tutor</phrase>. <phrase>Data</phrase> normalization is the process of refining a <phrase>relational database</phrase> schema in <phrase>order</phrase> to ensure that all relations are of <phrase>high</phrase> quality. We present the system first, and then discuss how it supports self-explanation. We hypothesized the self-explanation support in NORMIT would improve students' <phrase>problem solving</phrase> skills, and also result in better conceptual <phrase>knowledge</phrase>. A preliminary evaluation study of the system was performed in October 2002, the <phrase>results</phrase> of which show that both <phrase>problem-solving</phrase> performance and the understanding of the domain of students who self-explained increased. We also discuss our plans for <phrase>future research</phrase>.
<phrase>Ontology</phrase>-Based <phrase>Protein</phrase>-<phrase>Protein</phrase> Interactions Extraction from <phrase>Literature</phrase> Using the Hidden <phrase>Vector</phrase> <phrase>State</phrase> <phrase>Model</phrase> This <phrase>paper</phrase> proposes a novel framework of incorporating <phrase>protein</phrase>-<phrase>protein</phrase> interactions (PPI) <phrase>ontology</phrase> <phrase>knowledge</phrase> into PPI extraction from biomedical <phrase>literature</phrase> in <phrase>order</phrase> to address the emerging challenges of deep <phrase>natural language understanding</phrase>. It is built upon the existing work on <phrase>relation extraction</phrase> using the Hidden <phrase>Vector</phrase> <phrase>State</phrase> (HVS) <phrase>model</phrase>. The HVS <phrase>model</phrase> belongs to the category of statistical learning methods. It can be trained directly from un-annotated <phrase>data</phrase> in a constrained way whilst at the same time being able to capture the underlying <phrase>named entity</phrase> relationships. However , it is difficult to incorporate <phrase>background knowledge</phrase> or non-local <phrase>information</phrase> into the HVS <phrase>model</phrase>. This <phrase>paper</phrase> proposes to represent the HVS <phrase>model</phrase> as a conditionally trained undirected <phrase>graphical model</phrase> in which non-local features derived from PPI <phrase>ontology</phrase> through inference would be easily incorporated. The seamless <phrase>fusion</phrase> of <phrase>ontology</phrase> inference with statistical learning produces a new <phrase>paradigm</phrase> to <phrase>information extraction</phrase>.
7 the Self-explanation Principle in <phrase>Multimedia</phrase> Learning <phrase>Multimedia</phrase> <phrase>learning environments</phrase> combine multiple sources of <phrase>information</phrase> (e.g., text, diagrams, and simulations) to help students <phrase>master</phrase> cognitively challenging domains. However, in <phrase>order</phrase> to benefi t from these environments , students need to make connections among the sources of <phrase>information</phrase>. One strategy for encouraging students to think deeply about and cognitively engage with the learning material is prompted self-explanation. Self-explanation is a constructive or generative learning activity that facilitates deep and robust learning by encouraging students to make inferences using the learning materials, identify previously held misconceptions, and repair mental models. In this chapter, we present a framework for categorizing the many forms of prompted self-explanation and highlight ways that self-explanation has been successfully incorporated into <phrase>multimedia</phrase> <phrase>learning environments</phrase> to improve <phrase>student</phrase> learning. In addition, we discuss specifi c forms of self-explanation that may be particularly well suited for <phrase>multimedia</phrase> <phrase>learning environments</phrase>. We end with a discussion of implications for cogni-tive theory and <phrase>instructional design</phrase> and ideas for future work .
Evolving <phrase>Culture</phrase> vs <phrase>Local Minima</phrase> We propose a theory that relates difficulty of learning in <phrase>deep architectures</phrase> to <phrase>culture</phrase> and <phrase>language</phrase>. It is articulated around the following hypotheses: (1) learning in an individual <phrase>human brain</phrase> is hampered by the presence of effective <phrase>local minima</phrase>; (2) this optimization difficulty is particularly important when it comes to learning <phrase>higher-level</phrase> abstractions, i.e., concepts that <phrase>cover</phrase> a vast and highly-nonlinear span of sensory configurations; (3) such <phrase>high</phrase>-level abstractions are best represented in brains by the composition of many levels of representation, i.e., by <phrase>deep architectures</phrase>; (4) a <phrase>human brain</phrase> can learn such <phrase>high</phrase>-level abstractions if guided by the signals <phrase>produced</phrase> by other humans, which <phrase>act</phrase> as hints or indirect supervision for these <phrase>high</phrase>-level abstractions; and (5), <phrase>language</phrase> and the <phrase>recombination</phrase> and optimization of mental concepts provide an efficient <phrase>evolutionary</phrase> <phrase>recombination</phrase> operator, and this gives rise to rapid search in the space of communicable ideas that help humans build up better <phrase>high</phrase>-level <phrase>internal representations</phrase> of their world. These hypotheses put together imply that <phrase>human</phrase> <phrase>culture</phrase> and the <phrase>evolution</phrase> of ideas have been crucial to counter an optimization difficulty: this optimization difficulty would otherwise make it very difficult for <phrase>human</phrase> brains to capture <phrase>high</phrase>-level <phrase>knowledge</phrase> of the world. The theory is grounded in <phrase>experimental</phrase> observations of the difficulties of training deep <phrase>artificial neural networks</phrase>. Plausible consequences of this theory for the efficiency of <phrase>cultural</phrase> <phrase>evolution</phrase> are sketched.
<phrase>Time Series</phrase> Analysis Using Deep <phrase>Feed Forward</phrase> <phrase>Neural Networks</phrase> <phrase>Time Series</phrase> Analysis Using Deep <phrase>Feed Forward</phrase> <phrase>Neural Networks</phrase> <phrase>Deep neural networks</phrase> can be used for abstraction and as a preprocessing step for other <phrase>machine learning</phrase> classifiers. Our goal was to develop methods for a more accurate automated seizure detection. <phrase>Deep architectures</phrase> have been used for classification of events, and shown in this <phrase>research</phrase> to be an effective way of classifying multichannel <phrase>high</phrase> resolution <phrase>medical</phrase> <phrase>data</phrase>. The <phrase>medical</phrase> <phrase>data</phrase> used in this <phrase>thesis</phrase> was gathered from an electroencephalograph (<phrase>EEG</phrase>) used in a <phrase>hospital</phrase> setting on seizure patients. To demonstrate the ability of <phrase>deep architectures</phrase> to learn and abstract from <phrase>input data</phrase>, the signals from the <phrase>EEG</phrase> that contained both seizure and non seizure <phrase>data</phrase> were given both as featurized <phrase>data</phrase> and raw <phrase>data</phrase> to the <phrase>deep architecture</phrase>. In addition to the multiple types of <phrase>data</phrase> preparation, a patients <phrase>EEG</phrase> <phrase>data</phrase> was tested not only against their own <phrase>EEG</phrase> signal <phrase>training data</phrase> but other patients as well. This study supports the effectiveness of deep <phrase>feed forward</phrase> <phrase>neural networks</phrase> for usage in the seizure classification scenario, as well as highlights some of the difficulties associated with training <phrase>deep neural networks</phrase>, as shown through <phrase>experimental</phrase> <phrase>results</phrase>. The sooner I'm done with this thing the sooner we can get a kitten. I <phrase>love</phrase> you. <phrase>ii</phrase> Acknowledgments I would like to thank all of the people who have helped me, while specifically ensuring that I do so in no particular <phrase>order</phrase>, as to not offend someone by accident. To help facilitate this, I generated the list components, and modified their <phrase>order</phrase> with a cryptographically secure random number generator multiple times, while simultaneously poking the <phrase>cat</phrase>. When the <phrase>cat</phrase> moved, I used that configuration of acknowledgements. I would like to thank: Firstly Andrea. She didn't get <phrase>randomized</phrase>, because she's the reason I want to get this <phrase>thesis</phrase> done ASAP or sooner. You have heard me complain and stress so much about <phrase>deep learning</phrase> you probably never want to hear it again. Thank you for everything you mean to me, and all the help you gave me in this <phrase>thesis</phrase>! I <phrase>love</phrase> you. Secondly, I would like to thank my <phrase>family</phrase>. You've supported me through the years, even when there wasn't much of me left to support. If I could beat any of you at <phrase>fantasy</phrase> <phrase>hockey</phrase>, it would be the perfect <phrase>family</phrase>. Lab members of <phrase>Adam</phrase> and Tinoosh that we collaborated with getting published at AAAI Spring Symposia and FLAIRS too! We all 
<phrase>AIGA</phrase> National <phrase>Design</phrase> Conference: an <phrase>interaction design</phrase> perspective of a new experience (not to mention a <phrase>tax</phrase>-deductible trip to lovely <phrase>Vancouver</phrase>, <phrase>British Columbia</phrase>), we found ourselves at the <phrase>American Institute of Graphic Arts</phrase> (<phrase>AIGA</phrase>) National <phrase>Design</phrase> Conference. Every two years, the <phrase>AIGA</phrase> holds a conference to discuss issues relevant to designers and designing. The 2003 theme was The Power of <phrase>Design</phrase>, focusing on the role of <phrase>design</phrase> in <phrase>culture</phrase>, <phrase>economy</phrase>, and the environment. There we were, surrounded by hundreds of other designers, talking about our approach to <phrase>design</phrase>, when we realized something very important. They had no idea what we were talking about. Our attempts to describe our work were met with blank stares and a sort of bemused tolerance. The fact is, despite our similar titles, interaction and <phrase>graphic</phrase> designers do not share a <phrase>culture</phrase>. While they knew a tremendous amount about innovative typogra-phy and classic <phrase>letterpress</phrase> print, the <phrase>graphic</phrase> designers we met seemed to know very little about the <phrase>basic</phrase> tenets of <phrase>user-centered design</phrase>. Judging by the conference speakers , this trend appears to be chang-Drentell, partners in Winterhouse <phrase>design</phrase> studio, expressed concern about the emphasis of style over substance in the <phrase>curriculum</phrase> of top <phrase>graphic design</phrase> schools. They talked about the need to <phrase>research</phrase> and understand the <phrase>culture</phrase>, processes, and needs of the user and the user's <phrase>community</phrase>. Brenda Laurel, Chair of the Graduate <phrase>Media</phrase> <phrase>Design</phrase> Program at <phrase>Art Center College of Design</phrase>, presented <phrase>research</phrase> and analysis supporting the use of <phrase>user-centered design</phrase>. Interestingly, this <phrase>basic</phrase> tenet of <phrase>interaction design</phrase>-a focus on beginning a <phrase>design</phrase> project with deep <phrase>information</phrase>-gathering and continuing throughout with a <phrase>user-centered design</phrase> process-is only now making its way into the <phrase>graphic design</phrase> world. Ideas we learned 15 <phrase>years ago</phrase> are beginning to migrate from the realm of pure <phrase>interaction design</phrase> into the <phrase>graphic design</phrase> world. Conference speakers discussed what we had learned <phrase>years ago</phrase>: A product has to be functional and meet the unique needs of its audience, as well as be pretty. Not all of the talks were so closely related to <phrase>interaction design</phrase> principles. One of the most interesting themes of the conference : / 62 i n t <phrase>e</phrase> r a c t i o n s / j a n u a r y + f <phrase>e</phrase> b r u a r y 2 0 0 5
(2008). Promoting Learning by Observing Deep-level <phrase>Reasoning Questions</phrase> on Quantitative <phrase>Physics</phrase> <phrase>Problem Solving</phrase> with <phrase>Andes</phrase>. Promoting Learning by Observing Deep-level <phrase>Reasoning Questions</phrase> on Quantitative <phrase>Physics</phrase> <phrase>Problem Solving</phrase> with <phrase>Andes</phrase> As <phrase>e</phrase>-<phrase>learning environments</phrase> become a standard form of instruction more learners are forced to view <phrase>video</phrase> examples as a <phrase>substitute</phrase> for interactive classroom learning. These non-interactive <phrase>multimedia</phrase> environment that use auditory narration to improve learning have been referred to as a <phrase>vicarious learning</phrase> environment (See Gholson & Craig, 2006 for review). More specifically, <phrase>vicarious learning</phrase> environments are <phrase>multimedia</phrase> environments in which the learners see and/or hear content for which they are not the addressees and have no way of physically interacting with the source of the content they are attempting A primary mechanism required for learning within these environments is that the learner stays cognitively active during the learning process (Chi, Hausman, & Roy, in press). In that, the learners' process incoming <phrase>information</phrase> so that coherent mental representations are formed and integrated into existing mental representations or schemas (Craig, et al., 2006). The deep-level <phrase>reasoning questions</phrase> effect The deep-level <phrase>reasoning questions</phrase> effect is one example of the impact that these narratives can have on learning from <phrase>multimedia</phrase>. It has <phrase>long</phrase> been known that question generation is one of the processing components that The Adjunct question <phrase>literature</phrase> also shows that forcing students to answer questions during <phrase>reading</phrase> will improve learning from text (Hamaker, 1986). However, in a series of two studies, Craig, et al. (2006) showed that inserting these deeper-level questions into the auditory <phrase>narrative</phrase> of a <phrase>multimedia</phrase> environment improves learning over both a <phrase>narrative</phrase> description without the deep-level questions and even above an interactive tutoring system, AutoTutor. What is a deep-level question A deep-level reasoning question is a question integrated into a <phrase>narrative</phrase> that scaffolds links between mechanisms, components, or processes. These questions would be analogous to the higher levels of Bloom's <phrase>taxonomy</phrase> and the <phrase>long</phrase> answer question categories from the Graesser & Person <phrase>taxonomy</phrase> (1994). Some examples of these types of questions are given in Table 1. Table 1. Examples of three categories of deep-level <phrase>reasoning questions</phrase>. For more examples see Craig et al., 2000 or Gholson & Craig, 2006. Causal consequence What happens to <phrase>RAM</phrase> when the computer is turned off? Goal orientation Why was the application loaded into <phrase>RAM</phrase>? <phrase>Instrumental</phrase>/procedural How does the operating system open an application after you click on an <phrase>icon</phrase>? Deep-level <phrase>reasoning questions</phrase> serve as two functions during learning. First, as discussed earlier, the deep-level questions provide scaffolded learning that help to <phrase>free</phrase> up mental resources so learners can process new <phrase>information</phrase> and 
Understanding how <phrase>Deep Belief</phrase> Networks perform <phrase>acoustic</phrase> modelling <phrase>Deep Belief</phrase> Networks (DBNs) are a very competitive <phrase>alternative</phrase> to <phrase>Gaussian mixture</phrase> models for relating states of a <phrase>hidden Markov model</phrase> to frames of coefficients derived from the <phrase>acoustic</phrase> input. They are competitive for three reasons: DBNs can be <phrase>fine-tuned</phrase> as <phrase>neural networks</phrase>; DBNs have many non-linear <phrase>hidden layers</phrase>; and DBNs are generatively <phrase>pre-trained</phrase>. This <phrase>paper</phrase> illustrates how each of these three aspects contributes to the DBN's good <phrase>recognition performance</phrase> using both <phrase>phone recognition</phrase> performance on the TIMIT corpus and a dimensionally reduced visualization of the relationships between the feature vectors learned by the DBNs that preserves the similarity structure of the feature vectors at multiple scales. The same two methods are also used to investigate the most suitable type of input representation for a DBN.
Maximizing learning from collaborative activities Utilizing a Preparation for Future Learning <phrase>paradigm</phrase> and the Interactive-Constructive-Active-Passive framework, this study examined how two different kinds of cognitively engaging activities prepared students to learn from collaborating. Findings show that preparing prior to collaborating improved learning, but a difference was not detected in the type of preparation. In addition, differences in <phrase>learning outcomes</phrase> were only present in measures of deep <phrase>knowledge</phrase>. Analyses used a multilevel method targeted to dyadic <phrase>data</phrase>. Discussion addresses designing collaborative classroom activities that are effective and efficient for <phrase>deep learning</phrase>, as well as the importance of aligning assessments to depth of learning.
Classroom Uses for Besocratic Emerging <phrase>Technology</phrase> <phrase>Research</phrase> 1. Abstract This <phrase>paper</phrase> describes how BeSocratic can be used to improve learning and class interaction. BeSocratic is a novel <phrase>intelligent tutoring</phrase> system that aims to fill the gap between simple <phrase>multiple-choice</phrase> systems and <phrase>free</phrase>-response systems. The system includes a set of interactive modules that provide instructors with powerful tools to assess <phrase>student</phrase> performance. Beyond text boxes and <phrase>multiple-choice</phrase> questions, BeSocratic contains several <phrase>feedback</phrase> driven modules that can capture <phrase>free</phrase>-form <phrase>student</phrase> drawings. These drawings can be automatically recognized and evaluated as complex structures including <phrase>Euclidean</phrase> <phrase>graphs</phrase>, <phrase>chemistry</phrase> <phrase>molecules</phrase>, <phrase>computer science</phrase> <phrase>graphs</phrase>, or simple drawings for use within <phrase>science</phrase>, <phrase>technology</phrase>, <phrase>engineering</phrase>, and <phrase>mathematics</phrase> courses. This <phrase>paper</phrase> describes three use-cases for BeSocratic and how each scenario can improve learning and class interaction throughout the <phrase>curriculum</phrase>. These scenarios are: (1) formative assessments and tutorials, (2) <phrase>free</phrase>-response exercises, and (3) in-class real-time activities. 2. Problem Statement and Context Since the early days of <phrase>personal computing</phrase>, <phrase>software</phrase> has been developed for educational purposes. The number of such applications continues to increase, and the sophistication of the systems is constantly evolving. There exists a wide <phrase>spectrum</phrase> of <phrase>educational software</phrase> meeting needs from Pre-K to <phrase>industry</phrase>. Today, most <phrase>higher education</phrase> institutions use broad <phrase>learning management systems</phrase> such as <phrase>Blackboard</phrase>, <phrase>Moodle</phrase>, or Instructure <phrase>Canvas</phrase> to aid in assessment. Additionally, specialized systems (such as the <phrase>Mastering</phrase> <phrase>software</phrase> series, <phrase>OWL</phrase>, etc.) exist for individual disciplines and courses. A <phrase>subset</phrase> of these systems includes <phrase>intelligent tutoring</phrase> <phrase>software</phrase> such as MathTutor and CogTutor, which provide students with step-by-step guidance during <phrase>problem solving</phrase>. While these systems have been shown to enhance <phrase>student</phrase> learning in a <phrase>range</phrase> of domains [4, 5, 6], they tend to be difficult to <phrase>author</phrase>, and the majority of questions they can ask fall into one of two categories: <phrase>free</phrase>-response <phrase>text-based</phrase> questions or <phrase>multiple-choice</phrase>/matching questions. <phrase>Free</phrase>-response systems allow teachers to ask meaningful questions that require students to have a <phrase>deep understanding</phrase> of the subject in <phrase>order</phrase> to answer correctly; unfortunately, they are difficult to assess quickly and without bias. <phrase>Multiple choice</phrase> and matching questions are more restrictive by <phrase>nature</phrase>, and <phrase>research</phrase> has suggested that these questions cannot be used to properly assess deep <phrase>knowledge</phrase> on a subject since the exercises often only involve memorization [2, 3, 7, 8].
Context-aware Query Suggestion List of Publications Acknowledgment I would like to sincerely thank the many people who made this <phrase>research</phrase> possible. I wish to <phrase>express my deep</phrase> gratitude to my supervisor, Dr. Ziv Bar-Yossef, for the guidance, partnership, support and encouragement. Thanks for the many things I've learned while working on this <phrase>research</phrase>. I highly appreciate the ability to balance between guiding me, and giving me the freedom to explore. I would like to thank my <phrase>friends</phrase> at the Technion, for warm words and beneficial discussions. My deep thanks to my parents, Sara and Melech Westreich, for their continuous support and encouragement. Thanks for teaching me to strive for <phrase>knowledge</phrase> and wisdom. I wish to thank my parents in <phrase>law</phrase>, Leah and Jehoshua Kraus, for the kind support. I am grateful for the assistance, which enabled me to invest time and resources in my <phrase>research</phrase>. To my beloved children, Mattan, Noa and Yuval, who gave me the <phrase>light</phrase> and happiness in a way that only children can. I am deeply grateful to my husband, Shraga, for accompanying me along the way, and for believing me even more than I believed in myself. Thanks also for the technical assistance in various aspects of this <phrase>research</phrase>.
Offline/realtime <phrase>Network Traffic</phrase> Classification Using <phrase>Semi-supervised</phrase> Learning The undersigned certify that they have read, and recommend to the Faculty of Graduate Studies for acceptance, a <phrase>thesis</phrase> entitled " Offline/Realtime <phrase>Network Traffic</phrase> Classification Using <phrase>Semi-Supervised</phrase> Learning " submitted by Jeffrey Erman in partial fulfillment of the requirements for the <phrase>degree</phrase> of <phrase>MASTER</phrase> OF <phrase>SCIENCE</phrase>. Date <phrase>ii</phrase> Abstract Identifying and categorizing <phrase>network traffic</phrase> by application type is challenging because of the continued <phrase>evolution</phrase> of applications, especially of those with a desire to be undetectable. The diminished effectiveness of <phrase>port</phrase>-based identification and the overheads of <phrase>deep packet inspection</phrase> approaches motivate us to classify traffic by exploiting distinctive flow characteristics of applications when they communicate on a network. This <phrase>thesis</phrase> proposes a new <phrase>machine learning</phrase> approach for the classification of network flows using only flow <phrase>statistics</phrase>. Specifically, a <phrase>semi-supervised</phrase> classification method that allows classifiers to be designed from <phrase>training data</phrase> consisting of only a few labelled and many unlabelled flows. This <phrase>thesis</phrase> considers pragmatic classification issues such as <phrase>longevity</phrase> of classifiers and the need for retraining of classifiers. At the network core, only unidirectional flow records are available due to routing asymmetries. This <phrase>thesis</phrase> develops and validates an <phrase>algorithm</phrase> that can estimate the missing <phrase>statistics</phrase> from a unidirectional packet trace. The offline and realtime clas-sifiers developed can achieve <phrase>high</phrase> flow and <phrase>byte</phrase> <phrase>classification accuracy</phrase> (i.e., greater than 90%). iii Acknowledgments In this section, I would like to thank the people who have helped me along the way in completing this M.Sc. <phrase>thesis</phrase>. Firstly, I would like to thank my supervisor <phrase>Professor</phrase> Anirban Mahanti for his guidance, insights, <phrase>patience</phrase>, and encouragement which made this <phrase>thesis</phrase> possible. From him I have learned a great deal and most importantly he has taught me how to <phrase>research</phrase>. His enthusiasm and strives for excellence has made working with him a joy. I also would like to thank Martin Arlitt. His help and expertise were very <phrase>instrumental</phrase> to my <phrase>research</phrase>. He collected the <phrase>Internet</phrase> traces from the <phrase>University</phrase> of <phrase>Calgary</phrase> that I used to properly evaluate my classifiers. He also assisted me with using Bro. His attention to detail and valuable comments have greatly improved this <phrase>thesis</phrase> and the papers we have written for publication. I also would like to thank <phrase>Ira Cohen</phrase> and <phrase>Professor</phrase> Carey Williamson for their <phrase>feedback</phrase> and assistance in my <phrase>research</phrase>. I thank my wife: Sherri. She is my best friend and has helped keep me sane at times. She has always given 
Deep Adaptive Networks for Visual <phrase>Data</phrase> Classification This <phrase>paper</phrase> proposes a classifier called deep adap-tive networks (DAN) based on <phrase>deep belief</phrase> networks (DBN) for visual <phrase>data</phrase> classification. First, we construct a <phrase>directed</phrase> <phrase>deep belief</phrase> nets by using a set of <phrase>Restricted Boltzmann Machines</phrase> (RBM) and a Gaussian RBM via greedy and <phrase>layer-wise</phrase> <phrase>unsupervised learning</phrase>. Then, we refine the <phrase>parameter space</phrase> of the <phrase>deep architecture</phrase> to adapt the classification requirement by using global <phrase>gradient-descent</phrase> based <phrase>supervised learning</phrase>. An exponential <phrase>loss function</phrase> is utilized to maximize the separability of different classes. Moreover, we apply DAN to visual <phrase>data</phrase> classification task and observe an important fact that the learning ability of <phrase>deep architecture</phrase> is seriously underrated in <phrase>real-world</phrase> applications, especially when there are not enough <phrase>labeled data</phrase>. Experiments conducted on standard datasets of different types and different scales demonstrate that the proposed classifier outperforms the representative classification techniques and <phrase>deep learning</phrase> methods.
<phrase>Design</phrase> and Performance Verification of UHPC Piles for Deep Foundations versity is to develop and implement innovative methods, materials, and technologies for improv ing transportation efficiency, <phrase>safety</phrase>, and reliability while improving the <phrase>learning environment</phrase> of students, faculty, and staff in transportation-related fi elds. The contents of this <phrase>report</phrase> reflect the views of the authors, who are responsible for the facts and the accuracy of the <phrase>information</phrase> presented herein. The opinions, findings and conclusions expressed in this publication are those of the authors and not necessarily those of the sponsors. The sponsors assume no liability for the contents or use of the <phrase>information</phrase> contained in this document. This <phrase>report</phrase> does not constitute a standard, specification, or regulation. The sponsors do not endorse <phrase>products</phrase> or manufacturers. <phrase>Trademarks</phrase> or manufacturers' names appear in this <phrase>report</phrase> only because they are considered essential to the objective of the document. 16. Abstract The strategic plan for <phrase>bridge</phrase> <phrase>engineering</phrase> issued by <phrase>AASHTO</phrase> in 2005 identified extending the service <phrase>life</phrase> and optimizing structural systems of bridges in the <phrase>United States</phrase> as two <phrase>grand challenges</phrase> in <phrase>bridge</phrase> <phrase>engineering</phrase>, with the objective of producing safer bridges that have a minimum service <phrase>life</phrase> of 75 years and reduced maintenance cost. Material deterioration was identified as one of the primary challenges to achieving the objective of extended <phrase>life</phrase>. In substructural applications (e.g., deep foundations), <phrase>construction</phrase> materials such as <phrase>timber</phrase>, <phrase>steel</phrase>, and <phrase>concrete</phrase> are subjected to deterioration due to environmental impacts. Using innovative and new materials for foundation applications makes the <phrase>AASHTO</phrase> objective of 75 years service <phrase>life</phrase> achievable. Ultra <phrase>High</phrase> Performance <phrase>Concrete</phrase> (UHPC) with <phrase>compressive strength</phrase> of 180 MPa (26,000 psi) and excellent durability has been used in <phrase>superstructure</phrase> applications but not in <phrase>geotechnical</phrase> and foundation applications. This study explores the use of <phrase>precast</phrase>, prestressed UHPC piles in future foundations of bridges and other structures. An H-shaped UHPC section, which is 10-in. (250-mm) deep with weight similar to that of an HP1057 <phrase>steel</phrase> pile, was designed to improve constructability and reduce cost. In this project, instrumented UHPC piles were cast and <phrase>laboratory</phrase> and field <phrase>tests</phrase> were conducted. <phrase>Laboratory</phrase> <phrase>tests</phrase> were used to verify the moment-<phrase>curvature</phrase> response of UHPC pile section. In the field, two UHPC piles have been successfully driven in <phrase>glacial till</phrase> <phrase>clay</phrase> <phrase>soil</phrase> and load tested under vertical and lateral loads. This <phrase>report</phrase> provides a complete set of <phrase>results</phrase> for the field investigation conducted on UHPC H-shaped piles. <phrase>Test</phrase> <phrase>results</phrase>, durability, drivability, and other material advantages over normal <phrase>concrete</phrase> and 
Audio Chord Recognition with a <phrase>Hybrid</phrase> <phrase>Recurrent Neural Network</phrase> In this <phrase>paper</phrase>, we present a novel <phrase>architecture</phrase> for audio chord estimation using a <phrase>hybrid</phrase> <phrase>recurrent neural network</phrase>. The <phrase>architecture</phrase> replaces <phrase>hidden Markov models</phrase> (HMMs) with <phrase>recurrent neural network</phrase> (RNN) based <phrase>language</phrase> models for modelling temporal dependencies between chords. We demonstrate the ability of <phrase>feed forward</phrase> <phrase>deep neural networks</phrase> (DNNs) to learn <phrase>discriminative features</phrase> directly from a time-<phrase>frequency</phrase> representation of the <phrase>acoustic</phrase> signal , eliminating the need for a complex <phrase>feature extraction</phrase> stage. For the <phrase>hybrid</phrase> RNN <phrase>architecture</phrase>, inference over the output variables of interest is performed using <phrase>beam</phrase> search. In addition to the <phrase>hybrid</phrase> <phrase>model</phrase>, we propose a modification to <phrase>beam</phrase> search using a <phrase>hash table</phrase> which yields improved <phrase>results</phrase> while reducing <phrase>memory</phrase> requirements by an <phrase>order</phrase> of <phrase>magnitude</phrase>, thus making the proposed <phrase>model</phrase> suitable for real-time applications. We evaluate our model's performance on a dataset with publicly available annotations and demonstrate that the performance is comparable to existing <phrase>state</phrase> of the <phrase>art</phrase> approaches for chord recognition .
Neural Abstractive Text Summarization ive text summarization is a complex task whose goal is to generate a concise version of a text without necessarily reusing the sentences from the original source, but still preserving the meaning and the key contents. We address this issue by modeling the problem as a <phrase>sequence</phrase> to <phrase>sequence</phrase> learning and exploiting <phrase>Recurrent Neural Networks</phrase> (RNNs). This work is a discussion about our ongoing <phrase>research</phrase> on abstractive text summarization, where our aim is to investigate methods to infuse <phrase>prior knowledge</phrase> into <phrase>deep neural networks</phrase>. We believe that these approaches can obtain better performance than the <phrase>state</phrase>-of-the-<phrase>art</phrase> models for generating well-formed and meaningful summaries.
Active Deep Networks for <phrase>Semi-Supervised</phrase> Sentiment Classification This <phrase>paper</phrase> presents a novel <phrase>semi-supervised learning</phrase> <phrase>algorithm</phrase> called Active Deep Networks (ADN), to address the <phrase>semi-supervised</phrase> sentiment <phrase>classification problem</phrase> with <phrase>active learning</phrase>. First, we propose the <phrase>semi-supervised</phrase> learning method of ADN. ADN is constructed by <phrase>Restricted Boltzmann Machines</phrase> (RBM) with <phrase>unsupervised learning</phrase> using <phrase>labeled data</phrase> and abundant of <phrase>unlabeled data</phrase>. Then the constructed structure is <phrase>fine-tuned</phrase> by <phrase>gradient-descent</phrase> based <phrase>supervised learning</phrase> with an exponential <phrase>loss function</phrase>. Second, we apply <phrase>active learning</phrase> in the <phrase>semi-supervised</phrase> learning framework to identify reviews that should be labeled as <phrase>training data</phrase>. Then ADN <phrase>architecture</phrase> is trained by the selected <phrase>labeled data</phrase> and all <phrase>unlabeled data</phrase>. Experiments on five sentiment classification datasets show that ADN outper-forms the <phrase>semi-supervised learning</phrase> <phrase>algorithm</phrase> and <phrase>deep learning</phrase> techniques applied for sentiment classification.
The neural-SIFT feature descriptor for visual <phrase>vocabulary</phrase> <phrase>object recognition</phrase> In <phrase>computer vision</phrase>, one <phrase>area</phrase> of <phrase>research</phrase> which receives a lot of attention is recognizing the <phrase>semantic</phrase> content of an image. It's a challenging problem where varying pose, occlusion, scale and differing <phrase>light</phrase> conditions affect the ease of recognition. A common approach is to extract local feature descriptors from images and attach object class <phrase>labels</phrase> to them, but choosing the best type of feature to use is still an <phrase>open problem</phrase>. Some use <phrase>deep learning</phrase> methods to learn to create features during training. Others apply local image descriptors to extract features from an image. In most cases these <phrase>algorithms</phrase> show good performance, however, the downside of these type of <phrase>algorithms</phrase> is that they are not trainable by <phrase>design</phrase>. After training there is no <phrase>feedback loop</phrase> to update the type of features to extract, while there possibly could be room for improvement. In this <phrase>thesis</phrase>, a continuous <phrase>deep neural network</phrase> <phrase>feedback</phrase> system is proposed , which consists of an adaptive <phrase>neural network</phrase> feature descriptor, the bag of visual words approach, and a neural classifier. Two initialization methods for the <phrase>neural network</phrase> feature descriptor were compared, one where it was trained on the popular <phrase>Scale Invariant Feature Transform</phrase> (SIFT) descriptor output, and one where it was randomly initialized. After initial training, the system propagates the classification error from the <phrase>neural network</phrase> classifier through the entire pipeline, updating not only the classifier itself, but also the type of features to extract. The feature descriptor, before and after additional training, was also applied using a <phrase>support vector machine</phrase> (<phrase>SVM</phrase>) classifier to <phrase>test</phrase> for generalizability. <phrase>Results</phrase> show that for both initialization methods the <phrase>feedback</phrase> system increased accuracy substantially when regular training was not able to increase it any further. The proposed neural-SIFT feature descriptor performs better than the SIFT descriptor itself even with limited number of training instances. Initializing on an existing feature descriptor is beneficial when not a lot of <phrase>training samples</phrase> are available. However, when there are a lot of <phrase>training samples</phrase> available the system is able to construct a well-performing feature descriptor when starting in a random <phrase>state</phrase>, solely based on classifier <phrase>feedback</phrase>. The improved feature descriptor did not only show improved performance in the setting in which it was trained, but also while using an <phrase>SVM</phrase> classifier. However, the improvements were small and were only demonstrated with one other classifier. Therefore, more experiments are needed to get a better grip on 
<phrase>Boolean</phrase> hierarchies - on collapse properties and query <phrase>order</phrase> To my <phrase>family</phrase> Acknowledgements Words can not <phrase>express my deep</phrase> gratitude to my advisor <phrase>Professor</phrase> Gerd Wechsung. Generously he ooered support, guidance, and encouragement throughout the past four years. Learning from him and working with him was and still is a pleasure and privilege I much appreciate. Through all the ups and downs of my <phrase>research</phrase> his optimism and humane warmth have made the downs less frustrating and the ups more encouraging. I want to <phrase>express my deep</phrase> gratitude to <phrase>Professor</phrase> Lane Hemaspaandra and <phrase>Professor</phrase> Edith Hemaspaandra. Allowing me to becomepart of so many joint projects has <phrase>beena</phrase> wonderful learning experience and I much beneeted from their scientiic expertise. Their generous help and advice helped me to gain insights into how <phrase>research</phrase> is done and made this <phrase>thesis</phrase> possible. For serving as referees for this <phrase>thesis</phrase> I am grateful to <phrase>Professor</phrase> Edith Hemaspaandra and <phrase>Professor</phrase> Klaus <phrase>Wagner</phrase>. I w <phrase>ant</phrase> to thank all my colleagues at <phrase>Jena</phrase>, especially, Haiko M uller, Dieter Kratsch, JJ org Rothe, Johannes Waldmann, and Maren Hinrichs for generously ooering help and support regarding the many little things that scientiic work seems to require these days (L A T <phrase>E</phrase> X, <phrase>unix</phrase>, <phrase>email</phrase>, etc). Working in the theory group of <phrase>Professor</phrase> Gerd Wechsung has made the past years a wonderful time. I w <phrase>ant</phrase> to thank my p a r <phrase>e</phrase> n ts who brought about my i n terest into <phrase>mathematics</phrase>. For their encouragement and support I am grateful to my <phrase>family</phrase>, m y mother-in-<phrase>law</phrase>, and most of all, my l o ving wife Ines. Chapter 3 and Section 4.2 are based on joint w ork with Edith Hemaspaandra and Lane Hemaspaandra HHH98d] and part of it appeared in <phrase>journal</phrase> as HHH98e]. The <phrase>research</phrase> underlying Section 4.3 is joint work with Edith Hemaspaandra and Lane Hemaspaandra HHH98a]. The <phrase>results</phrase> of Section 5.2 were discovered jointly with Lane Hemaspaandra and Gerd Wechsung and will appear in <phrase>journal</phrase> as HHW99]. Section 5.4 is based on joint w ork with Edith Hemaspaandra and Lane Hemaspaandra and has (partially) appeared in <phrase>journal</phrase> as HHH97a].
<phrase>Ontology</phrase> creation using formal concepts approach Usually different types of <phrase>knowledge</phrase> represented in hierarchical <phrase>ontology</phrase> are used for designing of <phrase>information</phrase> systems. One of methods to represent <phrase>ontology</phrase> is <phrase>formal concept analysis</phrase>. In this <phrase>paper</phrase> we propose the method how to create <phrase>ontology</phrase> and don't use deep specific domain analysis. In this <phrase>paper</phrase> we propose the method to extend <phrase>ontology</phrase> using <phrase>action</phrase> rules. The <phrase>action</phrase> rules will be created using learned <phrase>neural network</phrase>. That allows more quick and simplifying creation of <phrase>ontology</phrase> without deep specific domain analysis.
Second Generation <phrase>Knowledge</phrase> Acquisition Methods and Their Application to <phrase>Medicine</phrase> First generation <phrase>expert systems</phrase> rely on the use of surface <phrase>knowledge</phrase>, such as associational or <phrase>heuristic</phrase>. This <phrase>knowledge</phrase> is typically acquired from <phrase>domain experts</phrase> through exhaustive <phrase>knowledge engineering</phrase> sessions. On the other hand, second generation <phrase>knowledge</phrase> acquisition <phrase>technology</phrase> is characterized by two main features: the use of deep <phrase>knowledge</phrase> and <phrase>machine learning</phrase>. In the <phrase>paper</phrase> we review three second generation methods that partially automate the <phrase>knowledge</phrase> acquisition process: inductive learning of rules from examples, <phrase>model</phrase>-based rule learning, and qualitative <phrase>model</phrase> acquisition. <phrase>Results</phrase> of their application to some <phrase>medical</phrase> domains are presented. Finally, we outline diierent stages of expert system development. An extended expert system shell schema is presented which includes a <phrase>knowledge</phrase> acquisition and a <phrase>knowledge</phrase> explanation module.
Deep Representations and Codes for Image Auto-Annotation The task of image auto-annotation, namely assigning a set of relevant tags to an image, is challenging due to the size and variability of tag vocabularies. Consequently , most existing <phrase>algorithms</phrase> focus on tag assignment and fix an often large number of <phrase>hand-crafted</phrase> features to describe image characteristics. In this <phrase>paper</phrase> we introduce a hierarchical <phrase>model</phrase> for learning representations of standard sized color images from the <phrase>pixel</phrase> level, removing the need for engineered <phrase>feature representations</phrase> and subsequent <phrase>feature selection</phrase> for annotation. We benchmark our <phrase>model</phrase> on the <phrase>STL</phrase>-10 recognition dataset, achieving <phrase>state</phrase>-of-the-<phrase>art</phrase> performance. When our features are combined with TagProp (Guillaumin et al.), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. Furthermore, using 256-<phrase>bit</phrase> codes and <phrase>Hamming distance</phrase> for training TagProp, we exchange only a small reduction in performance for efficient storage and fast comparisons. Self-taught learning is used in all of our experiments and deeper architectures always outperform shallow ones.
From Requirements to Code: Issues and Learning in IS Students' Systems Development Projects Executive Summary The <phrase>Computing</phrase> Curricula (2005) place <phrase>Information</phrase> Systems (IS) at the intersection of exact sciences (e.g. <phrase>General</phrase> Systems T heory), <phrase>technology</phrase> (e.g. <phrase>Computer Science</phrase>), and behavioral sciences (e.g. <phrase>Sociology</phrase>). This presents particular challenges for <phrase>teaching and learning</phrase>, as future IS professionals need to be equipped with a wide <phrase>range</phrase> of analytical and <phrase>critical thinking</phrase> skills that will enable them to solve <phrase>business</phrase> problems. In addition, they require technical, strong interper-sonal <phrase>communication</phrase>, and team skills to contribute to the successful delivery of <phrase>software</phrase> <phrase>products</phrase>. At the <phrase>University</phrase> of Cape T own (<phrase>UCT</phrase>) the capstone course of the IS <phrase>undergraduate</phrase> <phrase>curriculum</phrase> is structured around three main areas: <phrase>Project Management</phrase>; People <phrase>Management</phrase>; and Implementation. T he theoretical parts of this course introduce the <phrase>student</phrase> to important aspects of managing projects and people in the <phrase>Information</phrase> <phrase>Communication</phrase> and T echnology (ICT) Project environment. The practical part comprises a group systems development project, which forms a core part of the course and requires students to apply theoretical skills in a <phrase>real-world</phrase> context. Although the impact of the issues relating to <phrase>soft skills</phrase> on <phrase>student</phrase> learning is neither underestimated nor ignored in the course, this <phrase>paper</phrase> mainly focuses on the technical issues that are experienced during the <phrase>life</phrase> of the projects. Students generally experience difficulty in the areas of <phrase>problem-solving</phrase>, coding and testing, all of which are required for successful systems development. IS students are often less technically oriented than their counterparts in the other <phrase>computing</phrase> disciplines and their courses involve less technical content. As a result, they may be inadequately prepared for the technical demands of the project. IS professionals must be able to interact with <phrase>business</phrase> experts and apply <phrase>problem-solving</phrase> skills in developing possible solutions. It is thus reasonable to argue that the completion of a full <phrase>life</phrase> cycle of a project provide IS students with invaluable experience in testing the effectiveness of their proposed <phrase>solution</phrase>. A reflective approach has been applied to the course <phrase>design</phrase>, resulting in the development of a framework to sufficiently address the issues of <phrase>problem-solving</phrase>, coding, and testing through an <phrase>action</phrase> learning cycle. This approach has proved to <phrase>lead</phrase> to improved solutions and to encourage <phrase>deep learning</phrase>. It also shows how teaching practices are shaped by looking back reflexively of <phrase>student</phrase> learning and the facilitating environments. This <phrase>paper</phrase> describes how the course has evolved through four phases, culminating in an approach that guides students Material published as part 
Why Size Matters: Feature Coding as Nystrom Sampling Recently, the <phrase>computer vision</phrase> and <phrase>machine learning</phrase> <phrase>community</phrase> has been in favor of <phrase>feature extraction</phrase> pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this <phrase>paper</phrase> we propose a novel view of this pipeline based on <phrase>kernel methods</phrase> and Nystrom sampling. In particular, we focus on the coding of a <phrase>data</phrase> point with a local representation based on a <phrase>dictionary</phrase> with fewer elements than the number of <phrase>data</phrase> points, and view it as an approximation to the actual <phrase>function</phrase> that would compute pair-wise similarity to all <phrase>data</phrase> points (often too many to compute in practice), followed by a Nystrom sampling step to select a <phrase>subset</phrase> of all <phrase>data</phrase> points. Furthermore, since bounds are known on the approximation power of Nystrom sampling as a <phrase>function</phrase> of how many samples (i.e. <phrase>dictionary</phrase> size) we consider, we can derive bounds on the approximation of the exact (but expensive to compute) kernel matrix, and use it as a proxy to predict accuracy as a <phrase>function</phrase> of the <phrase>dictionary</phrase> size, which has been observed to increase but also to saturate as we increase its size. This <phrase>model</phrase> may help explaining the positive effect of the codebook size and justifying the need to stack more layers (often referred to as <phrase>deep learning</phrase>), as flat models empirically saturate as we add more complexity.
<phrase>Brain</phrase>-Machine Interfaces beyond <phrase>Neuroprosthetics</phrase> The field of <phrase>invasive</phrase> <phrase>brain</phrase>-machine interfaces (BMIs) is typically associated with neuroprosthetic applications aiming to recover loss of motor <phrase>function</phrase>. However, BMIs also represent a powerful tool to address fundamental questions in <phrase>neuroscience</phrase>. The observed subjects of <phrase>BMI</phrase> experiments can also be considered as indirect observers of their own neurophysiological activity, and the relationship between observed <phrase>neurons</phrase> and (<phrase>artificial</phrase>) behavior can be genuinely causal rather than indirectly correlative. These two characteristics defy the <phrase>classical</phrase> object-<phrase>observer</phrase> duality, making BMIs particularly appealing for investigating how <phrase>information</phrase> is encoded and decoded by neural circuits in real time, how this coding changes with <phrase>physiological</phrase> learning and plasticity, and how it is altered in pathological conditions. Within neuroengineering, <phrase>BMI</phrase> is like a <phrase>tree</phrase> that opens its branches into many traditional <phrase>engineering</phrase> fields, but also extends deep roots into <phrase>basic</phrase> <phrase>neuroscience</phrase> beyond <phrase>neuroprosthetics</phrase>.
<phrase>Wire</phrase>-length prediction using statistical techniques We address the classic <phrase>wire</phrase>-length estimation problem and propose a new statistical <phrase>wire</phrase>-length estimation approach that captures the <phrase>probability</phrase> <phrase>distribution function</phrase> of net lengths after placement and before routing. The <phrase>wire</phrase>-length prediction <phrase>model</phrase> was developed using a combination of paramet-ric and non-parametric statistical techniques. The <phrase>model</phrase> predicts not only the length of the net using input parameters extracted from the floorplan of a <phrase>design</phrase>, but also <phrase>probability distributions</phrase> that a net with given characteristics obtained after placement will have a particular length. The <phrase>model</phrase> is validated using both learn-and-<phrase>test</phrase> and resubsti-tution techniques. The <phrase>model</phrase> can be used for a <phrase>variety</phrase> of purposes, including the generation of a large number of statistically <phrase>sound</phrase> and therefore realistic instances of designs. We applied the net models to the probabilistic buffer insertion problem and obtained substantial improvement in net delay after routing. INTRODUCTION <phrase>Wire</phrase>-length has become one of the most critical metrics in physical <phrase>design</phrase> primarily due to the rise of the deep submi-<phrase>cron</phrase> <phrase>era</phrase>. There is a large number of different parameters and constraints, such as the bounding box of the net, number of routing grids and the grid capacity, total number of nets routed in the vicinity of the pertinent net, that are all potentially relevant, but are typically very hard to capture into consistent <phrase>wire</phrase>-length <phrase>model</phrase>. Hence, estimating an exact value for the <phrase>wire</phrase>-length is a very hard problem. We have developed a new wirelength <phrase>model</phrase> that uses <phrase>data</phrase> that can be extracted once the placement of the designs is completed. In <phrase>order</phrase> to build the <phrase>model</phrase> we used a combination of parametric and non-parametric techniques [2, 3]. <phrase>Statistical models</phrase> and prediction methodology can be used in many ways. For example, one can use the prediction <phrase>information</phrase> to evaluate the suitability of a particular floorplan for obtaining final routing where nets satisfy a particular user specified condition. For instance, the goal can be to determine which among a number of competing floorplans is most likely to result in a final <phrase>design</phrase> with few <phrase>long</phrase> nets or overall small sum of wirelengths. They are also a natural component of the overall probabilistic <phrase>design</phrase> <phrase>automation</phrase> methodology. One such probabilistic <phrase>algorithm</phrase> is [5] which performs buffer insertion assuming <phrase>wire</phrase>-lengths which are estimated as <phrase>distributions</phrase>. We used our models in the proba-bilistic buffer insertion approach of [5] and obtained massive improvements in net delay (40%) after routing when compared with traditional bounding box strategies [1].
Deep neural <phrase>heart rate variability</phrase> analysis Despite of the <phrase>pain</phrase> and limited accuracy of <phrase>blood</phrase> <phrase>tests</phrase> for early recognition of <phrase>cardiovascular disease</phrase>, they dominate <phrase>risk</phrase> screening and <phrase>triage</phrase>. On the other hand, <phrase>heart rate variability</phrase> is non-<phrase>invasive</phrase> and cheap, but not considered accurate enough for clinical practice. Here, we tackle <phrase>heart</phrase> beat interval based classification with <phrase>deep learning</phrase>. We introduce an <phrase>end to end</phrase> <phrase>differentiable</phrase> <phrase>hybrid</phrase> <phrase>architecture</phrase>, consisting of a layer of biological <phrase>neuron</phrase> models of <phrase>cardiac</phrase> dynamics (modified FitzHugh Nagumo <phrase>neurons</phrase>) and several layers of a standard <phrase>feed-forward</phrase> <phrase>neural network</phrase>. The proposed <phrase>model</phrase> is evaluated on ECGs from 474 stable at-<phrase>risk</phrase> (<phrase>coronary artery disease</phrase>) patients, and 1172 <phrase>chest pain</phrase> patients of an <phrase>emergency department</phrase>. We show that it can significantly outperform models based on traditional <phrase>heart rate variability</phrase> predictors, as well as approaching or in some cases outperforming clinical <phrase>blood</phrase> <phrase>tests</phrase>, based only on 60 seconds of inter-beat intervals.
<phrase>Context-dependent</phrase> <phrase>deep neural networks</phrase> for commercial <phrase>Mandarin</phrase> <phrase>speech recognition</phrase> applications Recently, <phrase>context-dependent</phrase> <phrase>deep neural network</phrase> <phrase>hidden Markov models</phrase> (<phrase>CD</phrase>-DNN-HMMs) have been successfully used in some commercial large-<phrase>vocabulary</phrase> <phrase>English</phrase> <phrase>speech recognition</phrase> systems. It has been proved that <phrase>CD</phrase>-DNN-HMMs significantly outperform the conventional <phrase>context-dependent</phrase> <phrase>Gaussian mixture</phrase> <phrase>model</phrase> (GMM)-HMMs (<phrase>CD</phrase>-GMM-HMMs). In this <phrase>paper</phrase>, we <phrase>report</phrase> our latest progress on <phrase>CD</phrase>-DNN-HMMs for commercial <phrase>Mandarin</phrase> <phrase>speech recognition</phrase> applications in <phrase>Baidu</phrase>. Experiments demonstrate that <phrase>CD</phrase>-DNN-HMMs can get relative 26% word error reduction and relative 16% sentence error reduction in Baidu's <phrase>short</phrase> message (<phrase>SMS</phrase>) voice input and voice search applications , respectively, compared with <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>CD</phrase>-GMM-HMMs trained using fMPE. To the best of our <phrase>knowledge</phrase>, this is the first time the performances of <phrase>CD</phrase>-DNN-HMMs are reported for commercial <phrase>Mandarin</phrase> <phrase>speech recognition</phrase> applications. We also propose a <phrase>GPU</phrase> on-chip speed-up training approach which can achieve a speed-up ratio of nearly two for DNN training. I. INTRODUCTION The mainstream of traditional <phrase>automatic speech recognition</phrase> (ASR) system typically uses <phrase>hidden Markov models</phrase> (HMMs) to <phrase>model</phrase> the evolvement of speech units (e.g., <phrase>phonemes</phrase>) and uses <phrase>Gaussian mixture</phrase> models (GMMs) to represent the relationship between <phrase>acoustic</phrase> inputs and speech units. Speech co-articulation is modeled by <phrase>context-dependent</phrase> (<phrase>CD</phrase>) units, such as triphones. This is the well-known generative <phrase>CD</phrase>-GMM-HMM <phrase>architecture</phrase> in the <phrase>literature</phrase>. Expectation-maximization (EM) <phrase>algorithm</phrase> is usually used for HMM training, while further <phrase>recognition accuracy</phrase> improvement can be achieved using discriminative training <phrase>algorithms</phrase> [1-3] such as MMI, MCE and MPE, etc. About two decades ago, <phrase>artificial neural networks</phrase> (ANNs), as a kind of discriminative <phrase>model</phrase>, were also investigated in <phrase>speech recognition</phrase> [4-6] with some limited success. In a typical ANN approach, instead of using GMMs, ANNs with a <phrase>single</phrase> layer of nonlinear <phrase>hidden units</phrase> are used to predict HMM states from <phrase>acoustic</phrase> observations. However, due to the limitations of the computation power and the <phrase>learning algorithms</phrase>, such a <phrase>single</phrase>-<phrase>hidden-layer</phrase> ANN approach was not sufficiently powerful to seriously challenge GMMs. As a result, the main practical contribution of ANNs was to provide useful features, namely <phrase>tandem</phrase> or bottleneck features [7], in which the <phrase>posterior probability</phrase> of each phone was estimated using ANN.
On the Expressive Power of <phrase>Deep Architectures</phrase> 1 Learning <phrase>Artificial Intelligence</phrase> <phrase>Deep architectures</phrase> are families of functions corresponding to deep circuits. <phrase>Deep Learning</phrase> <phrase>algorithms</phrase> are based on parametrizing such circuits and tuning their parameters so as to approximately optimize some training objective. Whereas it was thought too difficult to <phrase>train</phrase> <phrase>deep architectures</phrase>, several successful <phrase>algorithms</phrase> have been proposed in recent years. We review some of the theoretical motivations for <phrase>deep architectures</phrase>, as well as some of their practical successes, and propose directions of investigations to address some of the remaining challenges. An <phrase>intelligent agent</phrase> takes good decisions. In <phrase>order</phrase> to do so it needs some form of <phrase>knowledge</phrase>. <phrase>Knowledge</phrase> can be embodied into a <phrase>function</phrase> that maps inputs and states to states and actions. If we saw an agent that always took what one would consider as the good decisions, we would qualify the agent as intelligent. <phrase>Knowledge</phrase> can be explicit, as in the form of symbolically expressed rules and facts of <phrase>expert systems</phrase>, or in the form of <phrase>linguistic</phrase> statements in an <phrase>encyclopedia</phrase>. However, <phrase>knowledge</phrase> can also be implicit, as in the complicated wiring and <phrase>synaptic</phrase> strengths of <phrase>animal</phrase> brains, or even in the mechanical properties of an animal's body. Whereas <phrase>Artificial Intelligence</phrase> (<phrase>AI</phrase>) <phrase>research</phrase> initially focused on providing <phrase>computers</phrase> with <phrase>knowledge</phrase> in explicit form, it turned out that much of our <phrase>knowledge</phrase> was not easy to express formally. What is a chair? We might write a definition that can help another <phrase>human</phrase> understand the concept (if he did not know about it), but it is difficult to make it sufficiently complete for a computer to translate into the same level of competence (e.g. in recognizing chairs in images). Much so-called common-sense <phrase>knowledge</phrase> has this <phrase>property</phrase>. If we cannot endowe <phrase>computers</phrase> with all the required <phrase>knowledge</phrase>, an <phrase>alternative</phrase> is to let them learn it from examples. <phrase>Machine learning</phrase> <phrase>algorithms</phrase> aim to extract <phrase>knowledge</phrase> from examples (i.e., <phrase>data</phrase>), so as to be able to properly generalize to new examples. Our own implicit <phrase>knowledge</phrase> arises either out of our <phrase>life</phrase> experiences (<phrase>lifetime</phrase> learning) or from the longer scale form of learning that <phrase>evolution</phrase> really represents, where the result of adaptation is encoded in the <phrase>genes</phrase>. <phrase>Science</phrase> itself is a process of learning from observations and experiments in <phrase>order</phrase> to produce actionable <phrase>knowledge</phrase>. Understanding the principles by which agents can capture <phrase>knowledge</phrase> through examples, i.e., learn, is therefore a central scientific question with implications not only for <phrase>AI</phrase> and <phrase>technology</phrase>, but also to 
Dancing Hamsters and <phrase>Marble</phrase> Statues: Characterizing <phrase>Student</phrase> Visualizations of <phrase>Algorithms</phrase> <phrase>Algorithm</phrase> visualization <phrase>research</phrase> for <phrase>computer science</phrase> <phrase>education</phrase> has primarily focused on expert-created visualizations. However, constructionist and situated theories of learning suggest that students should develop and share their own diverse understandings of a concept for <phrase>deep learning</phrase>. This <phrase>paper</phrase> presents a novel approach to <phrase>algorithm</phrase> learning by visualization <phrase>construction</phrase>, sharing, and evaluation. Three <phrase>empirical studies</phrase> in which students engaged in these activities are discussed. The resulting learning benefits are quantified, and <phrase>student</phrase> visualizations are characterized in multiple ways. Then another study that investigated how specific characteristics of such visualizations influence learning is described. This work demonstrates the effectiveness of having students create <phrase>algorithm</phrase> visualizations, identifies characteristics of <phrase>student</phrase>-created <phrase>algorithm</phrase> visualizations and illuminates the learning benefits derived from these characteristics.
Learning <phrase>deep architectures</phrase> using kernel modules (thanks collaborations/discussions with many people)
Heterogeneous Network Embedding via <phrase>Deep Architectures</phrase> <phrase>Data</phrase> embedding is used in many <phrase>machine learning</phrase> applications to create <phrase>low-dimensional</phrase> <phrase>feature representations</phrase>, which preserves the structure of <phrase>data</phrase> points in their original space. In this <phrase>paper</phrase>, we examine the scenario of a heterogeneous network with nodes and content of various types. Such networks are notoriously difficult to mine because of the bewildering combination of heterogeneous contents and structures. The creation of a multidimensional embedding of such <phrase>data</phrase> opens the door to the use of a wide <phrase>variety</phrase> of off-the-shelf <phrase>mining</phrase> techniques for multidimensional <phrase>data</phrase>. Despite the importance of this problem, limited efforts have been made on embedding a network of scalable, dynamic and heterogeneous <phrase>data</phrase>. In such cases, both the content and linkage structure provide important cues for creating a unified <phrase>feature representation</phrase> of the underlying network. In this <phrase>paper</phrase>, we <phrase>design</phrase> a deep embedding <phrase>algorithm</phrase> for networked <phrase>data</phrase>. A highly nonlinear multi-layered embedding <phrase>function</phrase> is used to capture the complex interactions between the heterogeneous <phrase>data</phrase> in a network. Our goal is to create a multi-resolution deep embedding <phrase>function</phrase>, that reflects both the local and global network structures, and makes the resulting embedding useful for a <phrase>variety</phrase> of <phrase>data mining</phrase> tasks. In particular, we demonstrate that the rich content and linkage <phrase>information</phrase> in a heterogeneous network can be captured by such an approach, so that similarities among cross-modal <phrase>data</phrase> can be measured directly in a common embedding space. Once this goal has been achieved, a wide <phrase>variety</phrase> of <phrase>data mining</phrase> problems can be solved by applying off-the-shelf <phrase>algorithms</phrase> designed for handling <phrase>vector</phrase> representations. Our experiments on <phrase>real-world</phrase> network datasets show the effectiveness and <phrase>scalability</phrase> of the <phrase>proposed algorithm</phrase> as compared to the <phrase>state</phrase>-of-the-<phrase>art</phrase> embedding methods.
<phrase>Deep Learning</phrase>, Dark <phrase>Knowledge</phrase>, and <phrase>Dark Matter</phrase> <phrase>Particle</phrase> colliders are the primary <phrase>experimental</phrase> instruments of <phrase>high-energy physics</phrase>. By creating conditions that have not occurred naturally since the <phrase>Big Bang</phrase>, <phrase>collider</phrase> experiments aim to probe the most fundamental properties of <phrase>matter</phrase> and the <phrase>universe</phrase>. These costly experiments generate very large amounts of noisy <phrase>data</phrase>, creating important challenges and opportunities for <phrase>machine learning</phrase>. In this work we use <phrase>deep learning</phrase> to greatly improve the <phrase>statistical power</phrase> on three benchmark problems involving: (1) <phrase>Higgs</phrase> <phrase>bosons</phrase>; (2) su-persymmetric particles; and (3) <phrase>Higgs boson</phrase> decay modes. This approach increases the expected discovery significance over traditional shallow methods, by 50%, 2%, and 11% respectively. In addition, we explore the use of <phrase>model</phrase> compression to transfer <phrase>information</phrase> (dark <phrase>knowledge</phrase>) from deep networks to shallow networks.
Learning Features from <phrase>Music</phrase> Audio with <phrase>Deep Belief</phrase> Networks <phrase>Feature extraction</phrase> is a crucial part of many <phrase>MIR</phrase> tasks. In this work, we present a system that can automatically extract relevant features from audio for a given task. The <phrase>feature extraction</phrase> system consists of a <phrase>Deep Belief</phrase> Network (DBN) on Discrete <phrase>Fourier</phrase> Transforms (DFTs) of the audio. We then use the activations of the trained network as inputs for a non-linear <phrase>Support Vector Machine</phrase> (<phrase>SVM</phrase>) classifier. In particular, we learned the features to solve the task of <phrase>genre</phrase> recognition. The <phrase>learned features</phrase> perform significantly better than MFCCs. Moreover, we obtain a <phrase>classification accuracy</phrase> of 84.3% on the Tzanetakis dataset, which compares favorably against <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>genre</phrase> classifiers using frame-based features. We also applied these same features to the task of auto-tagging. The autotaggers trained with our features performed better than those that were trained with <phrase>timbral</phrase> and temporal features.
The Impact of Diversity on Online Ensemble Learning in the Presence of Concept Drift On-line <phrase>learning algorithms</phrase> often have to operate in the presence of concept drift (i.e., the concepts to be learnt can change with time). This <phrase>paper</phrase> presents a new categorization for concept drift, separating drifts according to different criteria into mutually exclusive and non-heterogeneous categories. Moreover, although ensembles of learning machines have been used to learn in the presence of concept drift, there has been no deep study of why they can be helpful for that and which of their features can contribute or not for that. As diversity is one of these features, we present a diversity analysis in the presence of different types of drift. We show that, before the drift, ensembles with less diversity obtain <phrase>lower</phrase> <phrase>test</phrase> errors. On the other hand, it is a good strategy to maintain highly diverse ensembles to obtain <phrase>lower</phrase> <phrase>test</phrase> errors shortly after the drift <phrase>independent</phrase> on the type of drift, even though <phrase>high</phrase> diversity is more important for more severe drifts. Longer after the drift, <phrase>high</phrase> diversity becomes less important. Diversity by itself can help to reduce the initial increase in error caused by a drift, but does not provide a faster recovery from drifts in <phrase>long</phrase> term.
<phrase>Learning Styles</phrase> Can Become <phrase>Learning Strategies</phrase> I n the last 30 or 40 years, a number of educators have proposed that teaching would be more effective if <phrase>faculty members</phrase> took account of differences in students' <phrase>learning styles</phrase>. A number of different conceptions of <phrase>learning styles</phrase> have been proposed, each with some plausibility. Probably the most widely accepted and best validated is Marton and Slj's (1976a,b) " deep processors " vs. " surface processors " based upon the levels of processing theory developed by Craik and Lockhart (1972). Deep processors think about the author's purpose and relate a <phrase>reading</phrase> assignment to <phrase>prior knowledge</phrase>; surface processors read with little thought. Another well validated style is " field dependent " vs. " field <phrase>independent</phrase> " (Witkin and Goodenough, 1981). In addition to these, there are also ten or twelve less well validated attempts to describe differing styles of learning. Probably the most over-generalized and misused has been " right-<phrase>brain</phrase> dominant " vs. " <phrase>left-brain</phrase> dominant. " Regardless of their validity, any of these methods may have <phrase>heuristic</phrase> value for faculty development by <phrase>drawing</phrase> attention to the fact that learners differ and that we need to take account of these differences in teaching. Too many teachers think of students as a featureless <phrase>mass</phrase>; too many rarely vary their teaching methods, thinking that the method by which they were taught is best for everyone. A method appropriate for most students may be ineffective for other students who could learn more easily with a different approach. Methods of teaching (e.g., <phrase>graphic</phrase> or verbal), ways of representing <phrase>information</phrase>, personality characteristics of teachers all affect learning and affect different learners differently. Thinking about <phrase>learning styles</phrase> can <phrase>lead</phrase> a <phrase>teacher</phrase> to think about different ways of teaching, and that is good. An effective <phrase>teacher</phrase> needs to vary techniques and to have an armamentarium of teaching methods and learning activities that can be drawn upon from moment to moment or from week to week to facilitate maximum learning for as many students as possible. Nonetheless, as in most things, there are potential undesirable side effects from the use of learning style concepts. Probably the most serious is that styles are often taken to be fixed, inherited characteristics that limit students' ability to learn in ways that do not fit their styles. Thus, some teachers draw the implication that they must match their teaching to the student's particular style, and some students
Joint Training <phrase>Deep Boltzmann Machines</phrase> for Classification We introduce a new method for training <phrase>deep Boltzmann machines</phrase> jointly. Prior methods of training <phrase>DBMs</phrase> require an initial learning <phrase>pass</phrase> that trains the <phrase>model</phrase> greedily, one layer at a time, or do not perform well on <phrase>classification tasks</phrase>. In our approach, we <phrase>train</phrase> all layers of the <phrase>DBM</phrase> simultaneously, using a novel training procedure called multi-prediction training. The resulting <phrase>model</phrase> can either be interpreted as a <phrase>single</phrase> <phrase>generative model</phrase> trained to maximize a variational approximation to the generalized pseudolikelihood, or as a <phrase>family</phrase> of recurrent networks that share parameters and may be approximately averaged together using a novel technique we call the multi-inference trick. We show that our approach performs competitively for classification and outperforms previous methods in terms of accuracy of approximate inference and classification with missing inputs. A deep <phrase>Boltzmann</phrase> machine (Salakhutdinov and Hinton, 2009) is a <phrase>probabilistic model</phrase> consisting of many layers of <phrase>random variables</phrase>, most of which are latent. Typically, a <phrase>DBM</phrase> contains a set of D input features v that are called the visible units because they are always observed during both training and evaluation. The <phrase>DBM</phrase> is usually applied to classification problems and thus often represents the class <phrase>label</phrase> with a one-of-k code in the form of a discrete-valued <phrase>label</phrase> unit y. y is observed (on examples for which it is available) during training. The <phrase>DBM</phrase> also contains several <phrase>latent variables</phrase> that are never observed. These <phrase>hidden units</phrase> are usually organized into L layers h (i) of size N i , i = 1,. .. , L, with each unit in a layer conditionally <phrase>independent</phrase> of the other units in the layer given the neighboring layers. These conditional <phrase>independence</phrase> properties allow fast <phrase>Gibbs sampling</phrase> because an entire layer of units can be sampled at a time. Likewise, mean field inference with <phrase>fixed point</phrase> equations is fast because each <phrase>fixed point</phrase> equation gives a <phrase>solution</phrase> to roughly half of the variational parameters. Inference proceeds by alternating between updating all of the even numbered layers and updating all of the odd numbered layers. A <phrase>DBM</phrase> defines a <phrase>probability distribution</phrase> by exponentiating and normalizing an <phrase>energy</phrase> <phrase>function</phrase> P (v, h, y) = 1 Z exp (<phrase>E</phrase>(v, h, y)) where Z = v ,h ,y exp (<phrase>E</phrase>(v , h , y)). Z, the <phrase>partition</phrase> <phrase>function</phrase>, is intractable, due to the summation over all possible states. <phrase>Maximum likelihood</phrase> learning requires <phrase>computing</phrase> the <phrase>gradient</phrase> of log Z. Fortunately, the 
<phrase>Data Visualization</phrase> on Shared Usage Multi-Screen Environment The modern <phrase>multimedia</phrase> technologies based on the whole palette of hardware and <phrase>software</phrase> facilities of real-time <phrase>high</phrase>-speed <phrase>information processing</phrase>, in a combination with effective facilities of the remote access to <phrase>information</phrase> resources, allow us to visualize diverse types of <phrase>information</phrase>. <phrase>Data visualization</phrase> facilities is the face of the Automated Control System on whom often <phrase>judge</phrase> about their efficiency. They take a special place, providing visualization of the diverse <phrase>information</phrase> necessary for <phrase>decision-making</phrase> by a final control link-the person allocated by certain powers. At creation of Shared Usage Multi-Screen Environment (SUME) one of the <phrase>basic</phrase> technical problem-<phrase>data</phrase> visualizing <phrase>process management</phrase>. However there are cases, when document loading process or displaying service <phrase>information</phrase> (for example, <phrase>windows</phrase> headers, service panels, etc.) is extremely undesirable. Decision of this problem-development special client-server applications which allows to operate with videoserver through LAN or <phrase>communication</phrase> <phrase>port</phrase>. If SUME installed in the conference hall which used in commercial objectives for carrying out various <phrase>business</phrase> presentations, it is possible to use videoserver under the control of any <phrase>Windows</phrase> platform. But, if SUME used as Crisis <phrase>Center</phrase> at the territory with restricted access, where there are special requirements on <phrase>software</phrase> certification and <phrase>information security</phrase>,-as videoserver operational system it is necessary to use only certificated Operational Systems. In that case most rational and <phrase>universal</phrase> decision-using <phrase>Java</phrase> <phrase>technology</phrase>. Unfortunately, here again there are lacks main of which-impossibility to get direct access to DCOM objects, so also to any files created with <phrase>Microsoft Office</phrase> and the majority of other applications. To not go deep into the problem of <phrase>data</phrase> storage formats studying it is necessary to use special bridges. Bridges it's pure <phrase>Java</phrase> implementation of DCOM enables <phrase>programming</phrase> with COM objects from any platform which supports the <phrase>Java</phrase> Developer Kit (JDK). <phrase>Visual Studio</phrase> developers also benefit from the ability to access any <phrase>Java</phrase> object as if it were implemented as a COM object. For today it is known such three projects: Intrinsys J-<phrase>Integra</phrase> [1], EZ-JCom [2], <phrase>IBM</phrase> Interface Tool for <phrase>Java</phrase> [3]. Bridges allows developers to work in their respective environments, while having the benefit of accessing COM components from <phrase>Java</phrase>, and <phrase>Java</phrase> components from COM (fig.1). The ultimate benefit is that developers can save time by leveraging existing <phrase>software</phrase> code, delivering best-in-class <phrase>software</phrase> solutions, and avoiding the need to learn new <phrase>programming</phrase> environments.
Towards <phrase>Information</phrase>-Seeking Agents We develop a <phrase>general</phrase> problem setting for training and testing the ability of agents to gather <phrase>information</phrase> efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of <phrase>information</phrase> which can be pieced together to accomplish various goals. We combine <phrase>deep architectures</phrase> with techniques from <phrase>reinforcement learning</phrase> to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new <phrase>information</phrase> to reduce their uncertainty, and to exploit <phrase>information</phrase> they have already acquired.
Towards Next Generation Web <phrase>Information Retrieval</phrase> Today <phrase>search engines</phrase> have become one of the most critical applications on the Web, driving many important online businesses that connect people to <phrase>information</phrase>. As the Web continues to grow its size with a <phrase>variety</phrase> of new <phrase>data</phrase> and penetrate into every <phrase>aspect</phrase> of people's <phrase>life</phrase>, the need for developing a more intelligent <phrase>search engine</phrase> is increasing. In this <phrase>talk</phrase>, we will briefly review the current status of <phrase>search engines</phrase>, and then present some of our recent works on building next generation <phrase>web search</phrase> technologies. Specifically, we will <phrase>talk</phrase> about how to extract <phrase>data records</phrase> from <phrase>web pages</phrase> using vision-<phrase>based approach</phrase>, and introduce new <phrase>research</phrase> opportunities in exploring the complementary properties between the surface Web and the <phrase>deep Web</phrase> to mutually facilitate the processes of web <phrase>information extraction</phrase> and <phrase>deep web</phrase> crawling. We will also present a search <phrase>prototype</phrase> that <phrase>data</phrase>-mines <phrase>deep web</phrase> structure to enable one-stop search of multiple online web <phrase>databases</phrase>. In contrast with current <phrase>web search</phrase> that is essentially document-level ranking and retrieval, an old <phrase>paradigm</phrase> in IR for more than 25 years, we will introduce our works in building a new <phrase>paradigm</phrase> called object-level <phrase>web search</phrase> that aims to automatically discover sub-topics (or <phrase>taxonomy</phrase>) for any given query and put retrieved web documents into a meaningful <phrase>organization</phrase>. We are developing techniques to provide object-level ranking, trend analysis, and <phrase>business intelligence</phrase> when the search is intended to find web objects such as people, papers, conferences, and interest groups. We will also <phrase>talk</phrase> about <phrase>vertical search</phrase> opportunities in some emerging new areas such as <phrase>mobile</phrase> search and <phrase>media</phrase> search. In addition to providing <phrase>information</phrase> adaptation on <phrase>mobile</phrase> devices, we believe location-based and context-aware search is going to be important for <phrase>mobile</phrase> search. We also think that by bridging physical world search to <phrase>digital</phrase> world search, many new user scenarios that do not yet exist on <phrase>desktop search</phrase> can potentially make a huge impact on the <phrase>mobile</phrase> <phrase>Internet</phrase>. For <phrase>media</phrase> search, we will present those new opportunities in analyzing the multi-typed interrelationship between <phrase>media</phrase> objects and other content such as text, <phrase>hyperlinks</phrase>, <phrase>deep web</phrase> structure, and user interactions for better <phrase>semantic</phrase> understanding and indexing of <phrase>media</phrase> objects. We will also discuss our goal of continually advancing <phrase>web search</phrase> to next level by applying <phrase>data mining</phrase>, <phrase>machine learning</phrase>, and <phrase>knowledge</phrase> discovery techniques into the process of <phrase>information</phrase> analysis, <phrase>organization</phrase>, retrieval, and visualization.
Multi-object Deformable Templates Dedicated to the Segmentation of <phrase>Brain</phrase> Deep Structures We propose a new way of embedding shape <phrase>distributions</phrase> in a <phrase>topological</phrase> deformable template. These <phrase>distributions</phrase> rely on global shape descriptors corresponding to the 3D moment invariants. In opposition to usual <phrase>Fourier</phrase>-like descriptors, they can be updated during deformations at a relatively <phrase>low cost</phrase>. The moment-based <phrase>distributions</phrase> are included in a framework allowing the <phrase>management</phrase> of several simultaneously deforming objects. This framework is dedicated to the segmentation of <phrase>brain</phrase> deep nuclei in 3D MR images. The <phrase>paper</phrase> focuses on the learning of the shape <phrase>distributions</phrase>, on the initialization of the <phrase>topological</phrase> <phrase>model</phrase> and on the multi-resolution <phrase>energy</phrase> minimization process. <phrase>Results</phrase> are presented showing the segmentation of twelve <phrase>brain</phrase> deep structures.
<phrase>Economic Model</phrase> for Vehicle Ownership Quota Policy Analysis <phrase>Economic Model</phrase> for Vehicle Ownership Quota Policy Analysis Over the years, <phrase>traffic congestion</phrase> has become an ever more serious issue in many mega-regions worldwide, and has caused huge economic and environmental loss. Having witnessed big increase in the sales of passenger cars, some authorities have turned to vehicle ownership quota policy, which is a direct tool to control the number of vehicles on the <phrase>road</phrase>. Two of the most typical ownership quota policies, are the vehicle plate <phrase>lottery</phrase> system and vehicle plate <phrase>auction</phrase> system. This <phrase>thesis</phrase> developed an analytical framework that utilized the joint vehicle ownership and usage decision <phrase>model</phrase> to quantitatively measure the impacts of the two quota policies, in terms of compensating variation. It is shown that implementation of vehicle plate <phrase>lottery</phrase> system will invite more households who have <phrase>lower</phrase> preference of owning a vehicle, and will result in a decrease in net social impact. The <phrase>thesis</phrase> then proposed an <phrase>alternative</phrase> policy that restricts the <phrase>lottery</phrase> to only previous <phrase>car</phrase> owners, and shows that it will give a higher net social impact. A numerical example is then conducted to determine the optimal quota ratio for each of the policies, and to compare their social benefits. Various policy implications and <phrase>future research</phrase> directions are also discussed. Dedication This <phrase>thesis</phrase> is dedicated to my parents Daogeng Du and Daomei Lin, for their <phrase>love</phrase> and support. <phrase>ii</phrase> Acknowledgments I would like to show my gratitude to many people that made this <phrase>thesis</phrase> possible. I am heartily thankful to my advisor, Dr. Lei Zhang, for offering me this precious opportunity to work on this very interesting problem. He has always been inspirational and supportive. His wide <phrase>knowledge</phrase> and kind guidance is invaluable to me. It's been a true pleasure to work with and learn from such an extraordinary advisor. I would also like to thank my <phrase>thesis</phrase> committee members, Dr. <phrase>Gang</phrase>-Len Chang and Dr. <phrase>Ali</phrase> Haghani for sparing their precious time reviewing the <phrase>manuscript</phrase>. great guidance and suggestions on this topic. Shanjiang and I have collaborated on different <phrase>research</phrase> projects and papers, and his <phrase>deep understanding</phrase> on various problems have been extremely helpful. I wish him all the best in his future career. <phrase>Lei Feng</phrase> and Yang (Karl) Lu for all the fun we had during graduate studies. During my 2 years in <phrase>UMD</phrase>, I have the privilege to be <phrase>apartment</phrase>-mate with Xinbo Yang, Rayhoo Zhang and Xin Wang. Together we share many <phrase>sweet memories</phrase> in that <phrase>apartment</phrase> of <phrase>University</phrase> 
<phrase>Convolutional Neural Networks</phrase> Analyzed via Convolutional <phrase>Sparse Coding</phrase> In recent years, <phrase>deep learning</phrase> and in particular <phrase>convolutional neural networks</phrase> (<phrase>CNN</phrase>) have <phrase>led</phrase> to some remarkable <phrase>results</phrase> in various fields. In this scheme, an input signal is convolved with learned filters and a non-linear point wise <phrase>function</phrase> is then applied on the response map. Of-tentimes, an additional non-linear step, termed pooling, is applied on the outcome. The obtained result is then fed to another layer that operates similarly, thereby creating a multi-layered convolutional <phrase>architecture</phrase>. Despite its marvelous empirical success, a clear and profound theoretical understanding of this scheme, termed <phrase>forward pass</phrase>, is still lacking. Another popular <phrase>paradigm</phrase> in recent years is the sparse representation <phrase>model</phrase>, where one assumes that a signal can be described as the <phrase>multiplication</phrase> of a <phrase>dictionary</phrase> by a sparse <phrase>vector</phrase>. A special case of this <phrase>model</phrase> that has drawn interest in recent years is the convolutional <phrase>sparse coding</phrase> (CSC) <phrase>model</phrase>, in which the <phrase>dictionary</phrase> assumes a specific structure a <phrase>union</phrase> of banded and Circulant matrices. Unlike <phrase>CNN</phrase>, sparsity inspired models are accompanied by a thorough theoretical analysis. Indeed, such a study of the CSC <phrase>model</phrase> and its properties has been performed in [1,2], establishing this <phrase>model</phrase> as a reliable and stable <phrase>alternative</phrase> to the commonly practiced patch-based processing. In this work we leverage the recent study of the CSC <phrase>model</phrase>, in <phrase>order</phrase> to bring a fresh view to <phrase>CNN</phrase> with a deeper theoretical understanding. Our analysis relies on the observation that akin to the original signal, the sparse representation itself can also be modeled as a sparse composition of yet another set of <phrase>atoms</phrase> from a convolutional <phrase>dictionary</phrase>. This <phrase>construction</phrase> , which can clearly be extended to more than two layers, <phrase>results</phrase> in the definition of the multi-layered convolutional sparse <phrase>model</phrase>, which is tightly related to deconvolutional networks. Several questions arise from the above <phrase>model</phrase>: * The authors contributed equally to this work. 1. What is the relation between <phrase>CNN</phrase> and the multi-layered convolu-tional sparse <phrase>model</phrase>? 2. In particular, can we interpret the <phrase>forward pass</phrase> of <phrase>CNN</phrase> as a simple pursuit <phrase>algorithm</phrase>? 3. If so, can we leverage this connection to provide a <phrase>sound</phrase> theoretical foundation for the <phrase>forward pass</phrase> of <phrase>CNN</phrase>? Specifically, is this <phrase>algorithm</phrase> guaranteed to succeed assuming certain conditions are met? Is it stable to slight perturbations in its input? 4. Lastly, can we leverage the answers to the above questions, and propose alternatives to CNN's <phrase>forward pass</phrase>? The answers to 
An <phrase>Artificial Neural Networks</phrase> based <phrase>Temperature</phrase> Prediction Framework for Network-on-Chip based Multicore Platform <phrase>ii</phrase> To my beloved parents Mr. Aswath Narayana and Mrs. Shubha, and my precious brother <phrase>Aryan</phrase> Rajeev. iii Acknowledgements I take this opportunity to express my profound gratitude and deep regards to my primary advisor Dr. Amlan Ganguly for his exemplary guidance, monitoring and constant encouragement throughout this <phrase>thesis</phrase>. Dr. Amlan dedicated his valuable time to review my work constantly and provide valuable suggestions which helped in overcoming many obstacles and keeping the work on the right <phrase>track</phrase>. I would like to express my deepest gratitude to Dr. Ray Ptucha and Dr. Mehran Mozaffari Kermani for sharing their thoughts and suggesting valuable ideas which have had significant impact on this <phrase>thesis</phrase>. I am grateful for their valuable time and cooperation during the course of this <phrase>thesis</phrase>. I also take this opportunity to thank my <phrase>research</phrase> group members for all the constant support and help provided by them. Abstract Continuous improvement in <phrase>silicon</phrase> process technologies has made possible the integration of hundreds of cores on a <phrase>single</phrase> chip. However, power and <phrase>heat</phrase> have become dominant constraints in designing these massive multicore chips causing issues with reliability, timing variations and reduced <phrase>lifetime</phrase> of the chips. Dynamic Thermal <phrase>Management</phrase> (<phrase>DTM</phrase>) is a <phrase>solution</phrase> to avoid <phrase>high</phrase> temperatures on the die. Typical <phrase>DTM</phrase> schemes only address core level thermal issues. However, the Network-on-chip (<phrase>NoC</phrase>) <phrase>paradigm</phrase>, which has emerged as an enabling methodology for integrating hundreds to thousands of cores on the same die can contribute significantly to the thermal issues. Moreover, the typical <phrase>DTM</phrase> is triggered reactively based on <phrase>temperature</phrase> measurements from on-chip thermal <phrase>sensor</phrase> requiring <phrase>long</phrase> <phrase>reaction</phrase> times whereas predictive <phrase>DTM</phrase> method estimates future <phrase>temperature</phrase> in advance, eliminating the chance of <phrase>temperature</phrase> overshoot. <phrase>Artificial Neural Networks</phrase> (ANNs) have been used in various domains for modeling and prediction with <phrase>high</phrase> accuracy due to its ability to learn and adapt. This <phrase>thesis</phrase> concentrates on designing an ANN prediction <phrase>engine</phrase> to predict the thermal profile of the cores and Network-on-Chip elements of the chip. This thermal profile of the chip is then used by the predictive <phrase>DTM</phrase> that combines both core level and network level <phrase>DTM</phrase> techniques. On-chip <phrase>wireless</phrase> interconnect which is recently envisioned to enable <phrase>energy</phrase>-efficient <phrase>data</phrase> exchange between cores in a multicore environment, will be used to provide a <phrase>broadcast</phrase>-capable medium to efficiently distribute thermal control messages to trigger and manage the <phrase>DTM</phrase> schemes.
<phrase>Computational intelligence</phrase> techniques in <phrase>bioinformatics</phrase> <phrase>Computational intelligence</phrase> (CI) is a well-established <phrase>paradigm</phrase> with current systems having many of the characteristics of biological <phrase>computers</phrase> and capable of performing a <phrase>variety</phrase> of tasks that are difficult to do using conventional techniques. It is a methodology involving adaptive mechanisms and/or an ability to learn that facilitate intelligent behavior in complex and changing environments, such that the system is perceived to possess one or more attributes of reason, such as generalization, discovery, association and abstraction. The objective of this article is to present to the CI and <phrase>bioinformatics</phrase> <phrase>research</phrase> communities some of the <phrase>state</phrase>-of-the-<phrase>art</phrase> in CI applications to <phrase>bioinformatics</phrase> and motivate <phrase>research</phrase> in new trend-setting directions. In this article, we present an overview of the CI techniques in <phrase>bioinformatics</phrase>. We will show how CI techniques including <phrase>neural networks</phrase>, <phrase>restricted Boltzmann machine</phrase>, <phrase>deep belief</phrase> network, <phrase>fuzzy logic</phrase>, rough sets, <phrase>evolutionary algorithms</phrase> (<phrase>EA</phrase>), <phrase>genetic algorithms</phrase> (GA), <phrase>swarm intelligence</phrase>, <phrase>artificial</phrase> <phrase>immune systems</phrase> and <phrase>support vector machines</phrase>, could be successfully employed to tackle various problems such as <phrase>gene expression</phrase> clustering and classification, <phrase>protein</phrase> <phrase>sequence</phrase> classification, <phrase>gene</phrase> selection, <phrase>DNA</phrase> fragment <phrase>assembly</phrase>, <phrase>multiple sequence alignment</phrase>, and <phrase>protein</phrase> <phrase>function</phrase> prediction and its structure. We discuss some representative methods to provide inspiring examples to illustrate how CI can be utilized to address these problems and how <phrase>bioinformatics</phrase> <phrase>data</phrase> can be characterized by CI. Challenges to be addressed and future directions of <phrase>research</phrase> are also presented and an extensive bibliography is included.
<phrase>Mining</phrase> for Complex Models Comprising <phrase>Feature Selection</phrase> and Classification Different <phrase>classification tasks</phrase> require different learning schemes to be satisfactorily solved. Most <phrase>real-world</phrase> datasets can be modeled only by complex structures resulting from deep <phrase>data</phrase> exploration with a number of different classification and <phrase>data transformation</phrase> methods. The search through the space of complex structures must be augmented with reliable validation strategies. All these techniques were necessary to build accurate models for the five <phrase>high</phrase>-dimensional datasets of the NIPS 2003 <phrase>Feature Selection</phrase> Challenge. Several <phrase>feature selection</phrase> <phrase>algorithms</phrase> (e.g. based on <phrase>variance</phrase>, <phrase>correlation coefficient</phrase>, <phrase>decision trees</phrase>) and several classification schemes (e.g. nearest neighbors, Normalized RBF, <phrase>Support Vector Machines</phrase>) were used to build complex models which transform the <phrase>data</phrase> and then classify. Committees of <phrase>feature selection</phrase> models and ensemble classifiers were also very helpful to construct models of <phrase>high</phrase> generalization abilities.
<phrase>Machine Learning</phrase> for <phrase>Medical</phrase> Applications <phrase>Machine learning</phrase> (ML) has been well recognized as an effective tool for researchers to handle the problems in signal and <phrase>image processing</phrase>. <phrase>Machine learning</phrase> is capable of offering automatic learning techniques to excerpt common patterns from empirical <phrase>data</phrase> and then make sophisticated decisions, based on the learned behaviors. <phrase>Medicine</phrase> has a large dimen-sionality of <phrase>data</phrase> and the <phrase>medical</phrase> application problems frequently make the <phrase>human</phrase>-generated, <phrase>rule-based</phrase> heuristics intractable. In this <phrase>special issue</phrase>, we provide a forum to present the cutting-edge <phrase>machine learning</phrase> techniques in <phrase>medical</phrase> applications, including the learning of similarities across different image modalities, <phrase>organ</phrase> localization, learning of anatomical changes, tissue classification, and <phrase>computer-aided</phrase> diagnosis. The topics of the accepted papers in this <phrase>Special Issue</phrase> spread from <phrase>electroencephalography</phrase> (<phrase>EEG</phrase>) <phrase>signal processing</phrase> to <phrase>image segmentation</phrase>. Z. Yang et al. in " Adaptive neuro-fuzzy inference system for classification of background <phrase>EEG</phrase> signals from ESES patients and controls " introduced an adaptive neurofuzzy inference system for classification of background <phrase>EEG</phrase> signals from the patients of <phrase>slow-wave sleep</phrase> syndrome and control subjects. Their study showed that the <phrase>entropy</phrase> measures of <phrase>EEG</phrase> were significantly different between the patients and normal subjects. Therefore, a classification framework based on <phrase>entropy</phrase> measures was proposed. S. Jiray-ucharoensak et al. in " <phrase>EEG</phrase>-based <phrase>emotion</phrase> recognition using <phrase>deep learning</phrase> network with <phrase>principal component</phrase> based covari-ate shift adaptation " proposed the utilization of a <phrase>deep learning</phrase> network (DLN) to discover unknown feature correlation between input signals. The DLN was implemented with a stacked autoencoder (SAE) using hierarchical <phrase>feature learning</phrase> approach. D. Al-Jumeily et al. in " A novel method of early diagnosis of <phrase>Alzheimer's disease</phrase> based on <phrase>EEG</phrase> signals " introduced three neural synchrony measurement techniques: phase synchrony, <phrase>magnitude</phrase> squared coherence, and <phrase>cross correlation</phrase> for classification of mild <phrase>Alzheimer's disease</phrase> patients and healthy subjects. K. Zhang et al. in " Adaptive <phrase>bacteria</phrase> colony picking in unstructured environments using intensity <phrase>histogram</phrase> and unascertained LS-<phrase>SVM</phrase> classifier " presented a novel approach for adaptive colony segmentation in unstructured environments by treating the detected <phrase>peaks</phrase> of intensity histograms as a morphological feature of images. In <phrase>order</phrase> to avoid disturbing <phrase>peaks</phrase>, an <phrase>entropy</phrase> based mean shift filter was introduced to smooth images as a preprocessing step. The relevance and importance of these features can be determined in an improved <phrase>support vector machine</phrase> classifier using unascertained least square estimation. M. Cabrerizo et al. in " Induced effects of <phrase>transcranial magnetic stimulation</phrase> on the <phrase>autonomic</phrase> <phrase>nervous</phrase> system and the 
Deep Distance Metric Learning with <phrase>Data</phrase> Summarization We present Deep <phrase>Stochastic</phrase> Neighbor Compression (DSNC), a framework to compress <phrase>training data</phrase> for instance-based methods (such as k-nearest neighbors). We accomplish this by inferring a smaller set of pseudo-inputs in a new <phrase>feature space</phrase> learned by a <phrase>deep neural network</phrase>. Our framework can equivalently be seen as jointly learning a nonlinear distance metric (induced by the deep <phrase>feature space</phrase>) and learning a compressed version of the <phrase>training data</phrase>. In particular , compressing the <phrase>data</phrase> in a deep <phrase>feature space</phrase> makes DSNC robust against <phrase>label</phrase> noise and issues such as within-class multi-modal <phrase>distributions</phrase>. This leads to DSNC yielding better accuracies and faster predictions at <phrase>test</phrase> time, as compared to other competing methods. We conduct <phrase>comprehensive</phrase> empirical evaluations, on both quantitative and qualitative tasks, and on several <phrase>benchmark datasets</phrase>, to show its effectiveness as compared to several baselines.
The role of cue <phrase>information</phrase> in the outcome-<phrase>density</phrase> effect: evidence from <phrase>neural network</phrase> simulations and a causal learning experiment Although normatively irrelevant to the relationship between a cue and an outcome, outcome <phrase>density</phrase> (i.e. its base-rate <phrase>probability</phrase>) affects people's estimation of <phrase>causality</phrase>. By what process <phrase>causality</phrase> is incorrectly estimated is of importance to an integrative theory of causal learning. A potential explanation may be that this happens because outcome <phrase>density</phrase> induces a judgement bias. An <phrase>alternative</phrase> explanation is explored here, following which the incorrect estimation of <phrase>causality</phrase> is grounded in the processing of cueoutcome <phrase>information</phrase> during learning. A first <phrase>neural network</phrase> <phrase>simulation</phrase> shows that, in the absence of a deep processing of cue <phrase>information</phrase>, cueoutcome relationships are acquired but <phrase>causality</phrase> is correctly estimated. The second <phrase>simulation</phrase> shows how an incorrect estimation of <phrase>causality</phrase> may emerge from the active processing of both cue and outcome <phrase>information</phrase>. In an experiment inspired by the simulations, the role of a deep processing of cue <phrase>information</phrase> was put to <phrase>test</phrase>. In addition to an outcome <phrase>density</phrase> manipulation, a shallow cue manipulation was introduced: cue <phrase>information</phrase> was either still displayed (concurrent) or no longer displayed (delayed) when outcome <phrase>information</phrase> was given. Behavioural and <phrase>simulation</phrase> <phrase>results</phrase> agree: the outcome-<phrase>density</phrase> effect was maximal in the concurrent condition. The <phrase>results</phrase> are discussed with respect to the extant explanations of the outcome-<phrase>density</phrase> effect within the causal learning framework.
<phrase>Cognitive</phrase> and Motivational Consequences of Adapting an Agent <phrase>Metaphor</phrase> in <phrase>Multimedia</phrase> Learning: Do the Benefits Outweigh the <phrase>Cognitive</phrase> and Motivational Consequences of Adapting an Agent <phrase>Metaphor</phrase> in <phrase>Multimedia</phrase> Learning: Do the Benefits Outweigh the Costs? This <phrase>paper</phrase> reviews a set of studies designed to <phrase>test</phrase> the <phrase>hypothesis</phrase> that the presence of <phrase>animated</phrase> pedagogical agents in <phrase>multimedia</phrase> environments can <phrase>promote deep learning</phrase>. This was done by first comparing the learning and motivational outcomes of students who learned in the context of social-agency to students who learned in a more traditional text and graphics context. Second, the particular features of the social-agency environment were manipulated to examine which of its attributes (e.g., visual and auditory presence, students' interaction, and agents' learning style) are most important in the promotion of meaningful learning. The theoretical and practical implications of the findings are'discussed. (Contains 23 references.) (MES) Reproductions supplied by EDRS are the best that can be made from the original document. EDUCATIONAL RESOURCES <phrase>INFORMATION</phrase> <phrase>CENTER</phrase> (ERIC) This document has been reproduced as received from the person or <phrase>organization</phrase> originating it. Minor changes have been made to improve reproduction quality. Points of view or opinions stated in this document do not necessarily represent official OERI position or policy. Abstract: What are the <phrase>cognitive</phrase> and motivational consequences of adapting an agent <phrase>metaphor</phrase> in <phrase>multimedia</phrase> learning? The present <phrase>paper</phrase> reviews a set of studies designed to <phrase>test</phrase> the <phrase>hypothesis</phrase> that the presence of <phrase>animated</phrase> pedagogical agents in <phrase>multimedia</phrase> environments can <phrase>promote deep learning</phrase>. This was done by first, comparing the learning and motivational outcomes of students who learned in the context of social-agency was to those of students who learned in a more traditional text and graphics context. Second, the particular features of the social agency environment were manipulated to examine which of its attributes are most important in the promotion of meaningful learning. The theoretical and practical implications of the findings are discussed.
A <phrase>machine learning</phrase> approach for real-time <phrase>reachability</phrase> analysis Assessing <phrase>reachability</phrase> for a dynamical system, that is deciding whether a certain <phrase>state</phrase> is reachable from a given initial <phrase>state</phrase> within a given cost threshold, is a central concept in controls, <phrase>robotics</phrase>, and optimization. Direct approaches to assess <phrase>reachability</phrase> involve the <phrase>solution</phrase> to a two-point boundary value problem (2PBVP) between a pair of states. <phrase>Alternative</phrase>, indirect approaches involve the characterization of reachable sets as level sets of the value <phrase>function</phrase> of an appropriate <phrase>optimal control</phrase> problem. Both methods solve the problem accurately, but are computationally intensive and do no appear amenable to real-time implementation for all but the simplest cases. In this work, we leverage <phrase>machine learning</phrase> techniques to devise query-based <phrase>algorithms</phrase> for the approximate, yet real-time <phrase>solution</phrase> of the <phrase>reachability</phrase> problem. Specifically, we show that with a <phrase>training set</phrase> of pre-solved 2PBVP problems, one can accurately classify the cost-reachable sets of a differentially-constrained system using either (1) locally-weighted <phrase>linear regression</phrase> or (2) <phrase>support vector machines</phrase>. This novel, query-<phrase>based approach</phrase> is demonstrated on two systems: the Dubins <phrase>car</phrase> and a deep-space <phrase>spacecraft</phrase>. Classification errors on the <phrase>order</phrase> of 10% (and often significantly less) are achieved with <phrase>average</phrase> execution times on the <phrase>order</phrase> of milliseconds, representing 4 <phrase>orders-of-magnitude</phrase> improvement over exact methods. The proposed <phrase>algorithms</phrase> could find application in a <phrase>variety</phrase> of time-critical robotic applications, where the driving factor is computation time rather than optimality.
Adaptive <phrase>Neural Network</phrase> Representations for Parallel and Scalable <phrase>Bayesian</phrase> Optimization <phrase>Bayesian</phrase> optimization has emerged as a powerful , new technique for <phrase>interpolating</phrase> and optimizing a wide <phrase>range</phrase> of functions which are expensive to compute. The primary tool of <phrase>Bayesian</phrase> optimization is the <phrase>Gaussian process</phrase> , which permits one to define a prior belief , which is then transformed into a posterior through sequential sampling of points. Unfortunately, <phrase>Gaussian process</phrase> <phrase>interpolation</phrase> suffers from a <phrase>major</phrase> computational bottleneck that makes it prohibitively expensive to use in <phrase>large-scale</phrase> optimization routines. In this work, we investigate deep <phrase>neu</phrase>-ral networks for learning a representation of the space for which a much less computation-ally expensive <phrase>interpolation</phrase> <phrase>algorithm</phrase> may be used. Our implementation adaptively updates the representation of the <phrase>neural network</phrase> as more <phrase>data</phrase> becomes available over time. Since this <phrase>neural network</phrase>-<phrase>based approach</phrase> is parallelizable, we are able to exploit simultaneous <phrase>function</phrase> evaluation and network parameter learning. We show that this <phrase>neural network</phrase>-<phrase>based approach</phrase> leads to many of the same appealing properties of Gaussian processes, and which scales only linearly in the number of observations <phrase>ceteris paribus</phrase>. We additionally seek to provide a theoretical explanation regarding why an optimization <phrase>algorithm</phrase> of this form functions at all. For this, we develop a probabilistic theory of martingales and apply it to a performance metric for the <phrase>neural network</phrase>-<phrase>based approach</phrase> to <phrase>Bayesian</phrase> optimization. We find that the performance of the <phrase>deep neural network</phrase> is consistent with the predictions of our prob-abilistic theory. We therefore speculate on the kind of representation that is learned by the network in its final <phrase>hidden layer</phrase>. We additionally show that our approach is easily extended to a distributed environment, which <phrase>results</phrase> in faster optimization. Numerical experiments demonstrate the efficacy of using deep networks for learning representations amenable to computationally inexpensive <phrase>interpolation</phrase> methods.
Teaching <phrase>Mathematics</phrase> Concepts Using a Multicultural Approach Today's <phrase>student</phrase> <phrase>population</phrase> continues to change rapidly and steadily reflects the myriad of cultures it represents. <phrase>Culture</phrase>, in this particular instance, includes but not limited to ethnicity, <phrase>socioeconomic status</phrase>, <phrase>language</phrase>, geographic origin, learning manner and abilities, <phrase>gender</phrase>, etc. Given this situation, it is sensible to reexamine our teaching approaches and to ponder the role of multicultural approach in the <phrase>teaching and learning</phrase> of our students, particularly in the subject of <phrase>mathematics</phrase>. Why is multicultural approach needed in teaching <phrase>mathematics</phrase>? A multicultural approach in teaching <phrase>mathematics</phrase> is important because it: Humanizes <phrase>mathematics</phrase> lessons and topics People all over the world have developed <phrase>mathematics</phrase> practices consistent with their needs and interests, specifically for practical, <phrase>aesthetic</phrase> and recreational purposes. Many cultures have developed counting practices consistent with their needs. Others also have made use of <phrase>arts</phrase> and designs that were rich in <phrase>symmetry</phrase>, transformations, proportions, etc. Finally, many cultures developed <phrase>games</phrase> and other fun activities that employ <phrase>mathematics</phrase> concepts such as networks, strategies, and patterns. Includes all students and boosts the confidence levels of students Doing <phrase>mathematics</phrase> is a <phrase>universal</phrase> activity; hence, everyone does some form of <phrase>mathematics</phrase>. Therefore, students see that <phrase>mathematics</phrase> is indeed for everyone and not for a few. Gives a holistic learning and connects to other disciplines (interdisciplinary approach), and determines the usage of <phrase>mathematics</phrase> in <phrase>society</phrase> and in other groups Students see the application and connection of <phrase>mathematics</phrase> not only in other disciplines but also in the <phrase>real world</phrase>. When students get exposed to the use and importance of <phrase>mathematics</phrase> in the <phrase>real world</phrase>, questions like " Why are we learning this? " and " When are we ever going to use this? " are answered automatically and clearly. Also, different professions and jobs use <phrase>mathematics</phrase> differently and accordingly. Corrects inaccuracies within <phrase>mathematics</phrase>, increases the universality of <phrase>mathematics</phrase>, and recognizes and acknowledges the existence of " other " <phrase>mathematics</phrase> In using a multicultural approach to the teaching of <phrase>mathematics</phrase>, the teachers are helping to overcome the existing deep-rooted <phrase>Euro</phrase>-centric bias relating to the origins and practices of <phrase>mathematics</phrase>. For example, is it still accurate and correct to call it Horner's Method when the <phrase>Chinese</phrase> have used it about four hundred years earlier? (Li and Du, 1987) Also, multicultural teaching gives a clearer picture of how <phrase>mathematics</phrase> is done and practiced formally in an <phrase>academic</phrase> setting and informally outside the classroom. For example, <phrase>drawing</phrase> a 
TargetSpy: a supervised <phrase>machine learning</phrase> approach for <phrase>microRNA</phrase> <phrase>target</phrase> prediction BACKGROUND Virtually all currently available <phrase>microRNA</phrase> <phrase>target</phrase> site prediction <phrase>algorithms</phrase> require the presence of a (conserved) <phrase>seed</phrase> match to the 5' end of the <phrase>microRNA</phrase>. Recently however, it has been shown that this requirement might be too stringent, leading to a substantial number of missed <phrase>target</phrase> sites. <phrase>RESULTS</phrase> We developed TargetSpy, a novel computational approach for predicting <phrase>target</phrase> sites regardless of the presence of a <phrase>seed</phrase> match. It is based on <phrase>machine learning</phrase> and automatic <phrase>feature selection</phrase> using a wide <phrase>spectrum</phrase> of compositional, structural, and <phrase>base pairing</phrase> features covering current biological <phrase>knowledge</phrase>. Our <phrase>model</phrase> does not rely on <phrase>evolutionary</phrase> conservation, which allows the detection of <phrase>species</phrase>-specific interactions and makes TargetSpy suitable for analyzing unconserved <phrase>genomic</phrase> sequences.In <phrase>order</phrase> to allow for an unbiased comparison of TargetSpy to other methods, we classified all <phrase>algorithms</phrase> into three groups: I) no <phrase>seed</phrase> match requirement, <phrase>II</phrase>) <phrase>seed</phrase> match requirement, and III) conserved <phrase>seed</phrase> match requirement. TargetSpy predictions for classes <phrase>II</phrase> and III are generated by appropriate postfiltering. On a <phrase>human</phrase> dataset revealing fold-change in <phrase>protein</phrase> <phrase>production</phrase> for five selected microRNAs our method shows <phrase>superior</phrase> performance in all classes. In <phrase>Drosophila melanogaster</phrase> not only our class <phrase>II</phrase> and III predictions are on par with other <phrase>algorithms</phrase>, but notably the class I (no-<phrase>seed</phrase>) predictions are just marginally less accurate. We estimate that TargetSpy predicts between 26 and 112 functional <phrase>target</phrase> sites without a <phrase>seed</phrase> match per <phrase>microRNA</phrase> that are missed by all other currently available <phrase>algorithms</phrase>. CONCLUSION Only a few <phrase>algorithms</phrase> can predict <phrase>target</phrase> sites without demanding a <phrase>seed</phrase> match and TargetSpy demonstrates a substantial improvement in prediction accuracy in that class. Furthermore, when conservation and the presence of a <phrase>seed</phrase> match are required, the performance is comparable with <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>algorithms</phrase>. TargetSpy was trained on <phrase>mouse</phrase> and performs well in <phrase>human</phrase> and <phrase>drosophila</phrase>, suggesting that it may be applicable to a broad <phrase>range</phrase> of <phrase>species</phrase>. Moreover, we have demonstrated that the application of <phrase>machine learning</phrase> techniques in combination with upcoming deep sequencing <phrase>data</phrase> <phrase>results</phrase> in a powerful <phrase>microRNA</phrase> <phrase>target</phrase> site prediction tool http://www.targetspy.org.
" Blind Multiuser <phrase>Receivers</phrase> for <phrase>Ds</phrase>-<phrase>cdma</phrase> in <phrase>Frequency</phrase>-selective Fading Channels " Blind Multiuser <phrase>Receivers</phrase> for <phrase>Ds</phrase>-<phrase>cdma</phrase> in <phrase>Frequency</phrase>-selective Fading Channels Dedication Dedicated with <phrase>love</phrase> to my <phrase>family</phrase>. <phrase>ii</phrase> Acknowledgements Back to the early morning on Aug. 3rd three <phrase>years ago</phrase> in 2000, I had just packed my luggage in to my 93's Camry and started warming up the <phrase>engine</phrase> in preparation for a <phrase>long</phrase> trip from east to <phrase>west coast</phrase>. I took a deep breath of the <phrase>fresh air</phrase> of <phrase>Texas</phrase> morning and looked around the small <phrase>town</phrase> where I had stayed for the first year of my graduate study. I wondered if I was moving on the right <phrase>track</phrase> towards an unpredictable future. I had been vacillating for the whole summer between continuing at the <phrase>Texas A&M University</phrase>, a place that I had just gotten familiar with, or transferring to the <phrase>University of Southern California</phrase> (<phrase>USC</phrase>), a whole new restart of my graduate study. Finally, I made a phone call to Prof. J.H. Chou to seek for a second opinion. Prof. Chou was the graduate advisor of my <phrase>master</phrase> <phrase>thesis</phrase>. Whenever I am perplexed or irresolute, I can always find some help from him, even thought he usually does not make definite suggestions to me. I felt much more confident after talking with him and made another phone call to Prof. C.-C. Jay Kuo at <phrase>USC</phrase> to see if he still wanted iii to take me as his <phrase>student</phrase>. He gave me a simple yet solid answer: " call me when you arrive in <phrase>Los Angeles</phrase> ". I could not have thanked Prof. Kuo more. He gave me a chance to pursue my Ph.D <phrase>degree</phrase> at <phrase>USC</phrase> and took me into the breadth of <phrase>signal processing</phrase> <phrase>area</phrase>. He maintains a large <phrase>research</phrase> group that provides a fertile bed where thoughts and ideas can sprout and grow. He takes care of more than thirty Ph.D students, and still can memorize each student's characteristic and explore their potentials. I could barely learn a little piece of his abundant <phrase>knowledge</phrase>, endless <phrase>energy</phrase> for doing <phrase>research</phrase> and his solid attitude. He finds every chance to help his students. Even at the last stage of my doctoral study, he still helps me on my career development and job <phrase>hunting</phrase>. My <phrase>research</phrase> at <phrase>USC</phrase> started with a class offered by Prof. Urbashi Mitra. Since then she have <phrase>led</phrase> me to explore many interesting fields in <phrase>wireless communications</phrase>. I could not have gone this far without her guidance. She showed me how to stimulate <phrase>creativity</phrase>, draw 
Boosted <phrase>Backpropagation</phrase> Learning for Training Deep Modular Networks Divide-and-conquer is key to building sophisticated learning machines: hard problems are solved by <phrase>composing</phrase> a network of modules that solve simpler problems (LeCun et al. such existing systems rely on <phrase>learning algorithms</phrase> which are based on simple paramet-ric <phrase>gradient descent</phrase> where the parametriza-tion must be predetermined, or more specialized per-application <phrase>algorithms</phrase> which are usually <phrase>ad-hoc</phrase> and complicated. We present a novel approach for training generic modular networks that uses two existing techniques: the error propagation strategy of <phrase>backpropagation</phrase> and more recent <phrase>research</phrase> on descent in spaces of functions (Mason et al., 1999; Scholkopf & Smola, 2001). Combining these two methods of optimization gives a simple <phrase>algorithm</phrase> for training heterogeneous networks of functional modules using simple <phrase>gradient</phrase> propagation <phrase>mechanics</phrase> and established <phrase>learning algorithms</phrase>. The resulting separation of concerns between learning individual modules and error propagation <phrase>mechanics</phrase> eases implementation, enables a larger class of modular <phrase>learning strategies</phrase>, and allows per-module control of complex-ity/regularization. We derive and demonstrate this functional <phrase>backpropagation</phrase> and contrast it with traditional <phrase>gradient descent</phrase> in <phrase>parameter space</phrase>, observing that in our example domain the method is significantly more robust to local <phrase>optima</phrase>.
<phrase>Large-Margin</phrase> kNN Classification Using a Deep Encoder Network KNN is one of the most popular classification methods, but it often fails to work well with inappropriate choice of distance metric or due to the presence of numerous class-irrelevant features. Linear feature transformation methods have been widely applied to extract class-relevant <phrase>information</phrase> to improve kNN classification , which is very limited in many applications. Kernels have been used to learn powerful non-linear feature transformations, but these methods fail to scale to <phrase>large datasets</phrase>. In this <phrase>paper</phrase>, we present a scalable non-linear feature mapping method based on a <phrase>deep neural network</phrase> pretrained with <phrase>restricted boltzmann machines</phrase> for improving kNN classification in a <phrase>large-margin</phrase> framework, which we call DNet-kNN. DNet-kNN can be used for both classification and for supervised <phrase>dimensionality reduction</phrase>. The <phrase>experimental</phrase> <phrase>results</phrase> on two benchmark <phrase>handwritten digit</phrase> datasets show that DNet-kNN has much better performance than <phrase>large-margin</phrase> kNN using a linear mapping and kNN based on a deep autoencoder <phrase>pre-trained</phrase> with retricted <phrase>boltzmann</phrase> machines.
Analyzing sedentary behavior in <phrase>life</phrase>-<phrase>logging</phrase> images We describe a study that aims to understand physical activity and sedentary behavior in <phrase>free</phrase>-living settings. We employed a wearable <phrase>camera</phrase> to record 3 to 5 days of imaging <phrase>data</phrase> with 40 participants, resulting in over 360,000 images. These images were then fully annotated by experienced staff with a rigorous coding protocol. We designed a <phrase>deep learning</phrase> based classifier in which we adapted a <phrase>model</phrase> that was originally trained for ImageNet [1]. We then added a <phrase>spatio-temporal</phrase> <phrase>pyramid</phrase> to our <phrase>deep learning</phrase> based classifier. Our <phrase>results</phrase> show our <phrase>proposed method</phrase> performs better than the <phrase>state</phrase>-of-the-<phrase>art</phrase> visual classification methods on our dataset. For most of the <phrase>labels</phrase> our system achieves more than 90% <phrase>average</phrase> accuracy across different individuals for frequent <phrase>labels</phrase> and more than 80% <phrase>average</phrase> accuracy for rare <phrase>labels</phrase>.
<phrase>Object Recognition</phrase> with and without Objects While recent <phrase>deep neural network</phrase> models have given promising performance on <phrase>object recognition</phrase>, they rely implicitly on the visual contents of the whole image. In this <phrase>paper</phrase>, we <phrase>train</phrase> <phrase>deep neural networks</phrase> on the foreground (object) and background (context) regions of images respectively. Considering <phrase>human</phrase> recognition in the same situations , <phrase>networks trained</phrase> on pure background without objects achieves highly reasonable <phrase>recognition performance</phrase> that beats humans to a <phrase>large margin</phrase> if only given context. However, humans still outperform networks with pure object available, which indicates networks and <phrase>human</phrase> beings have different mechanisms in understanding an image. Furthermore , we straightforwardly combine multiple trained networks to explore the different visual clues learned by different networks. Experiments show that useful visual hints can be learned separately and then combined to achieve higher performance, which confirms the advantages of the <phrase>proposed framework</phrase>.
Joint Training of <phrase>Deep Boltzmann Machines</phrase> We introduce a new method for training <phrase>deep Boltzmann machines</phrase> jointly. Prior methods require an initial learning <phrase>pass</phrase> that trains the deep <phrase>Boltzmann</phrase> machine greedily, one layer at a time, or do not perform well on <phrase>classification tasks</phrase>. A deep <phrase>Boltzmann</phrase> machine (Salakhutdinov and Hinton, 2009) is a <phrase>probabilistic model</phrase> consisting of many layers of <phrase>random variables</phrase>, most of which are latent. Typically, a <phrase>DBM</phrase> contains a set of D input features v that are called the visible units because they are always observed during both training and evaluation. The <phrase>DBM</phrase> is usually applied to classification problems and thus often represents the class <phrase>label</phrase> with a one-of-k code in the form of a discrete-valued <phrase>label</phrase> unit y. y is observed (on examples for which it is available) during training. The <phrase>DBM</phrase> also contains several <phrase>hidden units</phrase>, which are usually organized into L layers h (i) of size N i , i = 1,. .. , L,with each unit in a layer conditionally <phrase>independent</phrase> of the other units in the layer given the neighboring layers. These conditional <phrase>independence</phrase> properties allow fast <phrase>Gibbs sampling</phrase> because an entire layer of units can be sampled at a time. Likewise, mean field inference with <phrase>fixed point</phrase> equations is fast because each <phrase>fixed point</phrase> equation gives a <phrase>solution</phrase> to an entire layer of variational parameters. A <phrase>DBM</phrase> defines a <phrase>probability distribution</phrase> by exponen-tiating and normalizing an <phrase>energy</phrase> <phrase>function</phrase> Z, the <phrase>partition</phrase> <phrase>function</phrase>, is intractable, due to the summation over all possible states. <phrase>Maximum likelihood</phrase> learning requires <phrase>computing</phrase> the <phrase>gradient</phrase> of log Z. Fortunately, the <phrase>gradient</phrase> can be estimated using an MCMC procedure (Younes, 1999; Tieleman, 2008). Block <phrase>Gibbs sampling</phrase> of the layers makes this procedure efficient. The structure of the interactions in h determines whether further approximations are necessary. In the pathological case where every element of h is conditionally <phrase>independent</phrase> of the others given the visible units, the <phrase>DBM</phrase> is simply an RBM and logZ is the only intractable term of the <phrase>log likelihood</phrase>. In the <phrase>general</phrase> case, interactions between different elements of h render the posterior P (h | v, y) intractable. Salakhutdinov and Hinton (2009) overcome this by maximizing the <phrase>lower</phrase> bound on the <phrase>log likelihood</phrase> given by the mean field approximation to the posterior rather than maximizing the <phrase>log likelihood</phrase> itself. Again, block mean field inference over the layers makes this procedure efficient. An interesting <phrase>property</phrase> of the <phrase>DBM</phrase> is that the 
Factored 3-Way <phrase>Restricted Boltzmann Machines</phrase> For Modeling <phrase>Natural Images</phrase> <phrase>Deep belief</phrase> nets have been successful in mod-eling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the <phrase>restricted Boltzmann machine</phrase> (RBM) which is used as a module for learning <phrase>deep belief</phrase> nets one layer at a time. The Gaussian-<phrase>Binary</phrase> RBMs that have been used to <phrase>model</phrase> <phrase>real-valued</phrase> <phrase>data</phrase> are not a good way to <phrase>model</phrase> the <phrase>covariance</phrase> structure of <phrase>natural images</phrase>. We propose a factored 3-way RBM that uses the states of its <phrase>hidden units</phrase> to represent abnormalities in the local <phrase>covariance</phrase> structure of an image. This provides a probabilistic framework for the widely used simple/complex <phrase>cell</phrase> <phrase>architecture</phrase>. Our <phrase>model</phrase> learns <phrase>binary</phrase> features that work very well for <phrase>object recognition</phrase> on the " tiny images " <phrase>data set</phrase>. Even better features are obtained by then using standard <phrase>binary</phrase> RBM's to learn a deeper <phrase>model</phrase>.
Interview with <phrase>Alan Kay</phrase> The legendary <phrase>Alan Kay</phrase> and Roy <phrase>E</phrase>. <phrase>Disney</phrase> have graciously appeared on <phrase>camera</phrase> for interviews and joined the <phrase>ACM</phrase> <phrase>Computers</phrase> in <phrase>Entertainment</phrase> magazine's editorial board. Alan and Roy are two of the nicest people to <phrase>talk</phrase> to and work with. Alan talked about soft fun versus hard fun, and his <phrase>research</phrase> on <phrase>Squeak</phrase> for enhancing and amplifying learning in children's <phrase>education</phrase>. Roy told us about educators versus entertainers, and his views on traditional and <phrase>CGI</phrase> animations. The <phrase>video</phrase> clips of the interviews are available at http://www.acm.org/<phrase>pubs</phrase>/cie/oct2003/index.htmlAlan Kay, <phrase>HP</phrase> <phrase>Fellow</phrase> and <phrase>President</phrase> of Viewpoints <phrase>Research</phrase> Institute, is best known for the idea of <phrase>personal computing</phrase> the conception of the intimate <phrase>laptop</phrase> computer, and the inventions of the now ubiquitous overlapping-window interface and modern <phrase>object-oriented programming</phrase>. His deep interests in children and <phrase>education</phrase> were the <phrase>catalyst</phrase> for these ideas, and they continue to be a <phrase>major</phrase> source of inspiration. In the past, Alan has been a <phrase>Xerox</phrase> <phrase>Fellow</phrase>, Chief <phrase>Scientist</phrase> of <phrase>Atari</phrase>, <phrase>Apple</phrase> <phrase>Fellow</phrase>, and <phrase>Disney</phrase> <phrase>Fellow</phrase>. More <phrase>information</phrase> is available at http://www.squeakland.org/<phrase>community</phrase>/biography/alanbio.html and http://www.viewpointsresearch.org/alan.html
Toward an Affect-Sensitive AutoTutor This <phrase>paper</phrase> investigates the reliability of detecting a learner's affective states in an attempt to augment an <phrase>Intelligent Tutoring</phrase> System (AutoTutor) with the ability to incorporate such states into its pedagogical strategies to improve learning. We describe two studies that used observational and emote-aloud protocols in <phrase>order</phrase> to identify the affective states that learners experience while interacting with AutoTutor. In a third study, training and validation <phrase>data</phrase> were collected from three sensors in a learning session with AutoTutor, after which the affective states of the learner were identified by the learner, a peer, and two trained judges. The third study assessed the reliability of automatic detection of boredom, confusion, delight, flow, and frustration (versus the neutral baseline) from sensors that monitored the manner in which learners communicate affect through conversational cues, gross <phrase>body language</phrase>, and facial expressions. Although the primary focus of this article is on the classification of learner affect, we also explore how an affect-sensitive AutoTutor can adapt its instructional strategies to promote learning. 1. Introduction Emotions (affective states) are inextricably bound to the learning process in addition to the well-documented impact of <phrase>cognition</phrase>, <phrase>motivation</phrase>, discourse, <phrase>action</phrase>, and the environment. Attempts to <phrase>master</phrase> difficult technical material, such as conceptual <phrase>physics</phrase> or <phrase>mathematics</phrase>, inevitably require learners to confront contradictions, anomalous events, obstacles to goals, salient contrasts, and other stimuli or experiences that fail to match expectations. In response to these discrepant events, the <phrase>autonomic</phrase> <phrase>nervous</phrase> system increases its <phrase>arousal</phrase> and the learner experiences emotions such as confusion, frustration, irritation, anger, rage, or even despair. <phrase>Cognitive</phrase> equilibrium is subsequently restored when discrepancies are resolved, misconceptions are discarded, and confusion is alleviated. At that point the learner resumes with hope, determination, renewed curiosity, and maybe even enthusiasm. Given this link between affect and <phrase>cognition</phrase>, an agile <phrase>learning environment</phrase> that is sensitive to a learner's affective states will presumably enrich learning, particularly when <phrase>deep learning</phrase> is accompanied by confusion, frustration, boredom, interest, excitement, and insight. In this article we consider the possibility of endowing an existing <phrase>Intelligent Tutoring</phrase> System (ITS), AutoTutor, with the ability to process the learners' affective states in addition to their <phrase>cognitive</phrase> states. AutoTutor would ideally identify the learners' emotions and adjust its pedagogical strategies during the learning of complex material. AutoTutor is a fully automated computer <phrase>tutor</phrase> that simulates <phrase>human</phrase> tutors and holds conversations with students in <phrase>natural language</phrase>
A Hierarchical <phrase>Generative Model</phrase> of Recurrent Object-Based Attention in the <phrase>Visual Cortex</phrase> In line with recent work exploring <phrase>Deep Boltzmann Machines</phrase> (<phrase>DBMs</phrase>) as models of <phrase>cortical</phrase> processing, we demonstrate the potential of <phrase>DBMs</phrase> as models of object-based attention, combining generative principles with attentional ones. We show: (1) How inference in <phrase>DBMs</phrase> can be related qualitatively to theories of attentional recurrent processing in the <phrase>visual cortex</phrase>; (2) that deepness and topographic <phrase>receptive fields</phrase> are important for realizing the attentional <phrase>state</phrase>; (3) how more explicit atten-tional suppressive mechanisms can be implemented, depending crucially on sparse representations being formed during learning.
Automatic <phrase>software architecture</phrase> recovery: A <phrase>machine learning</phrase> approach It all started with a vision, inspired by a <phrase>video</phrase> on <phrase>YouTube</phrase> of a <phrase>physics</phrase> <phrase>professor</phrase> <phrase>drawing</phrase> on an <phrase>electronic</phrase> <phrase>whiteboard</phrase> and being able to instantly simulate the behavior of a <phrase>car</phrase> barreling down a hill, jumping off, and <phrase>hitting</phrase> and setting in motion a <phrase>variety</phrase> of obstacles. What if, <phrase>Professor</phrase> Andr van der Hoek and his graduate <phrase>student</phrase> Alex Baker pondered, <phrase>software engineering</phrase> diagrams could behave in much the same way? What if one could quickly <phrase>sketch</phrase> a <phrase>software architecture</phrase>, and then ask the diagram to tell us how it " felt, " for instance by moving incompatible interfaces further apart, coloring servers <phrase>red</phrase> if their expected load is too <phrase>high</phrase>, or blinking those components that it deems insecure? On board came Nicolas Mangano, an <phrase>undergraduate</phrase> <phrase>student</phrase> at the time, to explore this question. It is now five years later and Alex Baker graduated with his Ph.D. in 2010, Nicolas Mangano is close to finishing his Ph.D., and a large group of M.S. students, undergraduates, and visitors have contributed to the project. What happened is an intriguing <phrase>tale</phrase> of <phrase>academic</phrase> <phrase>research</phrase>, corporate partnership, a close look at how <phrase>software</phrase> designers actually work, a novel <phrase>software design</phrase> sketching tool, and <phrase>technology</phrase> transition into the classroom and <phrase>industry</phrase>. Van der Hoek and Baker realized early on that they needed to develop a <phrase>deep understanding</phrase> of how <phrase>software</phrase> designers actually work at the <phrase>whiteboard</phrase>. How do they navigate a <phrase>design</phrase> problem? What kinds of diagrams do they draw? How do they coordinate their mutual ideas? What do they do when they get stuck? What kinds of conversations take place? To answer these questions, they approached <phrase>professional</phrase>, highly experienced <phrase>software</phrase> designers and <phrase>architects</phrase> and asked if they could <phrase>videotape</phrase> them while at work. " The response was amazing, " says van der Hoek. " We received positive responses all around, including from designers of some of the most well-known <phrase>software</phrase> <phrase>products</phrase> to date. " The resulting <phrase>video</phrase> catalogue formed the basis not only for Alex Baker's eventual dissertation, but also for an <phrase>NSF</phrase>-sponsored workshop that took place at <phrase>UC Irvine</phrase> in 2010 (http:// www.ics.uci.edu/<phrase>design</phrase>-workshop). Baker took the idea of <phrase>video</phrase> analysis even further, compiling additional videos so as to compare the <phrase>design</phrase> approaches of the experienced professionals with those of novice designers. The <phrase>results</phrase> are intriguing. Experienced designers exhibited a pattern of rotating between pairs of topics in their discussions. Unplanned, and indeed 
Playfully teaching <phrase>artificial intelligence</phrase> by implementing <phrase>games</phrase> to undergraduates Using term-work, practical and project <phrase>based approach</phrase> is a common practice for teaching <phrase>AI</phrase> course. Teaching <phrase>Artificial Intelligence</phrase> is always a great task for teachers of technical institutes. It is common practice to use computer <phrase>games</phrase> to be used as a tool to help introduce <phrase>basic</phrase> <phrase>computer science</phrase> concepts to the students. <phrase>Digital</phrase> <phrase>games</phrase> could have a much bigger role in learning than just as a motivational tool. In this <phrase>paper</phrase>, it is observed the efforts of teaching <phrase>Artificial Intelligence</phrase> (<phrase>AI</phrase>) concepts to <phrase>undergraduate</phrase> students of computer <phrase>engineering</phrase> and <phrase>information technology</phrase> with <phrase>games</phrase>, because <phrase>game</phrase> motivate students, which increase retention and helps to become better computer engineers. <phrase>Paper</phrase> describes a <phrase>case study</phrase> of all types of <phrase>games</phrase> and/or puzzles inculcate in teaching <phrase>AI</phrase> concepts and other searching <phrase>algorithms</phrase> to the students. Teaching informed searching techniques like generate-and-<phrase>test</phrase>, <phrase>hill climbing</phrase>, A-<phrase>star</phrase>, <phrase>AO</phrase>-<phrase>star</phrase>, <phrase>constraint satisfaction</phrase> problems and means-end analysis is easy, if <phrase>real-life</phrase>-problems or puzzles are used for explanation. Using <phrase>water</phrase>-jug problem, eight-<phrase>puzzle</phrase> (sliding tiles), <phrase>money</phrase>-<phrase>banana</phrase> problem or block-world puzzles helps in understanding these informed searching techniques. Students really put more efforts in inventing good heuristics functions for above <phrase>games</phrase>. <phrase>Minimax</phrase> <phrase>algorithm</phrase> for two player <phrase>game</phrase> is complex <phrase>algorithm</phrase> which is used in IBM's <phrase>Deep-blue</phrase>, who defeated <phrase>world chess champion</phrase> Gary <phrase>Kasparov</phrase> in 1997 is also implemented in lab. Our approach in writing and implementing those <phrase>games</phrase> or <phrase>puzzle</phrase> in understanding concepts better.
Estimation Based on RBM from <phrase>Label</phrase> Proportions in Large Group Case Consider a similar learning formulation that is more informative than mutilple-instance learning. To be specific, observations are given in the setting that divided into some subsets without <phrase>labels</phrase>(knowing features), but the <phrase>label</phrase> proportions for each <phrase>subset</phrase> are provided. Is it possible to learn a mapping from every instance to its class like a <phrase>supervised learning</phrase> scenario? Some methods have been presented for solving this problem. But when the set size of each <phrase>subset</phrase> is large, the empirical result will converge to random guess quickly. Here we show theoretical demonstration on the necessity of assumptions for solving this problem, and then propose a Deep Networks for Proportion Learning (DNPL) <phrase>algorithm</phrase> for solving the large <phrase>subset</phrase> size case. This is also empirically demonstrated in practice compared with other well-performed methods.
Learning Machines Implemented on Non-Deterministic Hardware This <phrase>paper</phrase> highlights new opportunities for designing <phrase>large-scale</phrase> <phrase>machine learning</phrase> systems as a consequence of blurring traditional boundaries that have allowed <phrase>algorithm</phrase> designers and application-level practitioners to stay for the most part oblivious to the details of the underlying hardware-level implementations. The hardware/<phrase>software</phrase> co-<phrase>design</phrase> methodology advocated here hinges on the deployment of compute-intensive <phrase>machine learning</phrase> kernels onto compute platforms that <phrase>trade</phrase>-off <phrase>determinism</phrase> in the computation for improvement in speed and/or <phrase>energy</phrase> efficiency. To achieve this, we revisit <phrase>digital</phrase> <phrase>stochastic</phrase> circuits for approximating matrix computations that are ubiquitous in <phrase>machine learning</phrase> <phrase>algorithms</phrase>. Theoretical and empirical evaluation is undertaken to assess the impact of the hardware-induced computational noise on <phrase>algorithm</phrase> performance. As a <phrase>proof-of-concept</phrase>, a <phrase>stochastic</phrase> hardware simulator is employed for training <phrase>deep neural networks</phrase> for <phrase>image recognition</phrase> problems.
Towards Score Following In <phrase>Sheet Music</phrase> Images This <phrase>paper</phrase> addresses the matching of <phrase>short</phrase> <phrase>music</phrase> audio snippets to the corresponding <phrase>pixel</phrase> location in images of <phrase>sheet music</phrase>. A system is presented that simultaneously learns to read notes, listens to <phrase>music</phrase> and matches the currently played <phrase>music</phrase> to its corresponding notes in the sheet. It consists of an <phrase>end-to-end</phrase> multi-modal convolu-tional <phrase>neural network</phrase> that takes as input images of <phrase>sheet music</phrase> and spectrograms of the respective audio snippets. It learns to predict, for a given unseen audio snippet (covering approximately one bar of <phrase>music</phrase>), the corresponding position in the respective score line. Our <phrase>results</phrase> suggest that with the use of (deep) <phrase>neural networks</phrase> which have <phrase>proven</phrase> to be powerful <phrase>image processing</phrase> models working with <phrase>sheet music</phrase> becomes feasible and a promising <phrase>future research</phrase> direction.
Generalization, Similarity, and <phrase>Bayesian Inference</phrase> <phrase>Long</phrase> Abstract <phrase>Internalization</phrase>: A <phrase>metaphor</phrase> we can <phrase>live</phrase> without [<phrase>PDF</phrase> version] Behavioral and <phrase>Brain</phrase> Sciences 24 (2): XXX-XXX. Is <phrase>kinematic</phrase> <phrase>geometry</phrase> an internalized regularity? Behavioral and <phrase>Brain</phrase> Sciences 24 (2): XXX-XXX. This is the unedited draft of a <phrase>BBS</phrase> <phrase>target</phrase> article that has been accepted for publication (<phrase>Copyright</phrase> 1999: <phrase>Cambridge University Press</phrase> U.K./ / <phrase>U.S</phrase>.-publication date provisional) and is currently being circulated for Open Peer Commentary. This <phrase>preprint</phrase> is for inspection only, to help prospective commentators decide whether or not they wish to prepare a formal commentary. Please do not prepare a commentary unless you have received the <phrase>hard copy</phrase>, invitation, instructions and deadline <phrase>information</phrase>. For <phrase>information</phrase> on becoming a commentator on this or other <phrase>BBS</phrase> <phrase>target</phrase> articles, write to: <phrase>bbs</phrase>@soton.ac.uk For <phrase>information</phrase> about subscribing or purchasing offprints of the published version, with commentaries and author's response, write to: journals_subscriptions@cup.org (<phrase>North America</phrase>) or journals_marketing@cup.cam.ac.uk (All other countries). Joshua B. Tenenbaum is Assistant <phrase>Professor</phrase> of <phrase>Psychology</phrase> at <phrase>Stanford University</phrase>. In 1999, he received a Ph.D. in <phrase>Brain</phrase> and <phrase>Cognitive Sciences</phrase> from <phrase>MIT</phrase>. His <phrase>research</phrase> focuses on learning and inference in humans and machines, with specific interests in concept learning and generalization, similarity, reasoning, causal induction, and learning <phrase>perceptual</phrase> representations. Thomas L. Griffiths is a doctoral <phrase>student</phrase> at <phrase>Stanford University</phrase>. His <phrase>research</phrase> interests concern the application of <phrase>mathematical</phrase> and <phrase>statistical models</phrase> to <phrase>human</phrase> <phrase>cognition</phrase>. <phrase>Short</phrase> abstract Shepard's theoretical analysis of generalization, originally formulated only for the ideal case of encountering a <phrase>single</phrase> consequential stimulus that can be represented as a point in a continuous <phrase>metric space</phrase>, is here recast in a more <phrase>general</phrase> <phrase>Bayesian</phrase> framework. This formulation naturally extends to the more realistic situation of generalizing from multiple consequential stimuli with arbitrary representational structure. Our framework also subsumes a version of Tversky's <phrase>set-theoretic</phrase> models of similarity, which are conventionally thought of as the primary <phrase>alternative</phrase> to Shepard's approach. This unification not only allows us to draw deep parallels between the <phrase>set-theoretic</phrase> and spatial approaches, but also to significantly advance the explanatory power of <phrase>set-theoretic</phrase> models. Shepard has argued that a <phrase>universal</phrase> <phrase>law</phrase> should govern generalization across different domains of <phrase>perception</phrase> and <phrase>cognition</phrase>, as well as across organisms from different <phrase>species</phrase> or even different <phrase>planets</phrase>. Starting with some <phrase>basic</phrase> assumptions about natural kinds, he derived an <phrase>exponential decay</phrase> <phrase>function</phrase> as the form of the <phrase>universal</phrase> generalization <phrase>gradient</phrase>, which accords strikingly well with a wide <phrase>range</phrase> of empirical <phrase>data</phrase>. However, his original formulation applied only to 
Using host profiling to refine statistical application identification The identification of <phrase>Internet traffic</phrase> applications is very important for <phrase>ISPs</phrase> and network administrators to protect their resources from unwanted traffic and prioritize some <phrase>major</phrase> applications. <phrase>Statistical methods</phrase> are preferred to <phrase>port</phrase>-based ones and <phrase>deep packet inspection</phrase> since they don't rely on the <phrase>port</phrase> number and they also work for encrypted traffic. These methods combine the <phrase>statistical analysis</phrase> of the application packet flow parameters, such as packet size and inter-packet time, with <phrase>machine learning</phrase> techniques. Other approaches rely on the way the hosts communicate and their traffic patterns to identify applications. In this <phrase>paper</phrase>, we propose a new online method for <phrase>traffic classification</phrase> that combines the statistical and host-based approaches in <phrase>order</phrase> to construct a robust and precise method for early <phrase>Internet traffic</phrase> identification. We use the packet size as the main feature for the classification and we benefit from the traffic profile of the host (i.e. which application and how much) to decide in favor of this or that application. This latter profile is updated online based on the result of the classification of previous flows originated by or addressed to the same host. We evaluate our method on real traces using several applications. The <phrase>results</phrase> show that leveraging the traffic pattern of the host ameliorates the performance of <phrase>statistical methods</phrase>. They also prove the capacity of our <phrase>solution</phrase> to derive profiles for the traffic of <phrase>Internet</phrase> hosts and to identify the services they provide.
Experiment of <phrase>metaverse</phrase> learning method using anatomical 3D object We created anatomical 3D objects regarding eye mechanism in Second <phrase>life</phrase> as learning materials. We conducted several substantiative experiments to prove how much understanding of participants of the experiments could improve their <phrase>knowledge</phrase> by lecturing with these objects. The 3D objects which we created provide an <phrase>artificial</phrase> experience which we can never have in the actual world. For instance, we can see a huge eye ball and we can enter inside of it. Enjoyment and excitement through such an <phrase>artificial</phrase> experience becomes a trigger of arousing our <phrase>motivation</phrase> for learning and bring us <phrase>deep understanding</phrase>.
Multimodal DBN for Predicting <phrase>High</phrase>-Quality Answers in cQA portals In this <phrase>paper</phrase>, we address the problem for predicting cQA answer quality as a classification task. We propose a multimodal <phrase>deep belief</phrase> nets <phrase>based approach</phrase> that operates in two stages: First, the joint representation is learned by taking both tex-tual and non-textual features into a <phrase>deep learning</phrase> network. Then, the joint representation learned by the network is used as input features for a linear classifier. Extensive <phrase>experimental</phrase> <phrase>results</phrase> conducted on two cQA datasets demonstrate the effectiveness of our <phrase>proposed approach</phrase>.
Latent learning - What your net also learned A <phrase>neural net</phrase> can learn to discriminate among a set of classes without explicitly training to do so. It does not even need exposure to any instances of those classes. The learning occurs while the net is being trained to discriminate among a set of related classes. This form of <phrase>transfer learning</phrase> is referred to as 'Latent Learning' by <phrase>psychologists</phrase>, because until specifically elicited, the <phrase>knowledge</phrase> remains latent. Evidence that latent learning has occurred lies in the existence of consistent, unique responses to the unseen classes. Standard <phrase>supervised learning</phrase> can improve the accuracy of those responses with exceedingly small sets of labeled images. In this <phrase>paper</phrase>, we use a convolutional <phrase>neural net</phrase> (<phrase>CNN</phrase>) to demonstrate not only a method of determining a net's latent responses, but also simple ways to optimize latent learning. Additionally, we take advantage of the fact that CNN's are deep nets in <phrase>order</phrase> to show how the latently learned accuracy of the <phrase>CNN</phrase> may be greatly improved by allowing only its output layer to <phrase>train</phrase>. We compare our <phrase>results</phrase> both to those obtained with standard <phrase>backpropagation</phrase> training of the <phrase>CNN</phrase> on small datasets without any <phrase>transfer learning</phrase> and to a related set of current <phrase>published results</phrase>. I. INTRODUCTION We have observed that one way for a <phrase>neural net</phrase> to employ <phrase>transfer learning</phrase> is through reuse of its learned <phrase>internal representations</phrase>. <phrase>Internal representations</phrase> that facilitate differentiation among one set of mutually exclusive classes may also contain latent responses for similar classes to which the net has not been exposed. These latent responses can be used to distinguish among the not yet seen classes. Because the responses are learned without any specific reinforcement and are not demonstrated until necessary, <phrase>psychologists</phrase> refer to the manner in which they are learned as 'latent learning'. <phrase>Psychologists</phrase> had observed latent learning in rats as early as 1930 [1]. We determine the latent response of a net to a new input class by observing its <phrase>average</phrase> response to a small number of <phrase>labeled samples</phrase> from that class. This <phrase>average</phrase> response determines what we will consider the net's latent response to be for that particular new class. Determination of the latent responses is not training. Training is an iterative optimization process. Observation of a net's <phrase>average</phrase> responses to a small number of <phrase>labeled samples</phrase> from a new input class does not involve optimization and is not iterative (i.e. the <phrase>average</phrase> response is measured 
Impact of <phrase>modulation</phrase> instability on distributed <phrase>optical fiber</phrase> sensors I hide my tears when I say your name, But the <phrase>pain</phrase> in my <phrase>heart</phrase> is still the same. In loving <phrase>memory</phrase> of Mom and Aunty.. . Acknowledgements Throughout these years as a doctoral <phrase>student</phrase> in <phrase>Switzerland</phrase> I had the great privilege of learning new things, sharing <phrase>knowledge</phrase> and experiencing delightful moments with wonderful people to whom my sincere gratitude goes forever. First and foremost of all, I would like to express my warmest gratitude to my supervisor, Prof. Luc Thvenaz, for sharing his <phrase>knowledge</phrase> and experience with me and for providing a warm and stimulating environment in which I could learn and progress. I thank my former and current colleagues in the Group for Fibre <phrase>Optics</phrase> at <phrase>EPFL</phrase> for sharing their time and thoughts with me discussing about <phrase>life</phrase> and <phrase>science</phrase>. In particular, I would like to thank our postdoctoral fellows Marcelo Soto and Kenny Hey Tow and our doctoral assistants, and our <phrase>master</phrase> students, Simon Zaslawski and Konrad Rolle. I am also grateful to the former and current administrative assistants of our group and the <phrase>EPFL</phrase> doctoral program in <phrase>photonics</phrase>, the <phrase>president</phrase> of the <phrase>thesis</phrase> <phrase>jury</phrase>. I am extremely grateful to my lovely <phrase>friends</phrase> who made my stay in <phrase>Switzerland</phrase> pleasant and unforgettable: special thanks go to Kamran for all the moments we have shared in our endless and thoughtful discussions. I would also like to thank my <phrase>music</phrase> <phrase>teacher</phrase> Sogol Mirzaei for being patient with me and giving positive encouragement. Finally, I am deeply indebted to my <phrase>family</phrase>, especially my parents, Bahman and late Fatemeh, for the unconditional <phrase>love</phrase> and support they have given to me throughout my <phrase>life</phrase>. I am also most grateful to my dear brothers, Behrouz, Abbas, and <phrase>Amin</phrase> and my lovely sisters-in-<phrase>law</phrase> Elham and Negar for having always been supportive in difficult times. Many thanks also go to my brotherly cousins, <phrase>Ali</phrase> and Erfan, that in spite of a <phrase>long</phrase> distance have always been with me. I finish with expressing my deep affection for my sweet nephew, Sepehr, whose cuteness reminds me of good memories of the past. Abstract <phrase>Modulation</phrase> instability as the main limit to the sensing distance of distributed fiber sensors is throughly investigated in this <phrase>thesis</phrase> in <phrase>order</phrase> to obtain a <phrase>model</phrase> for predicting its characteristics and alleviating its effects. Starting from <phrase>Maxwell's equations</phrase> in <phrase>optical fibers</phrase>, the nonlinear Schrdinger equation describing the propagation of <phrase>wave</phrase> envelope in nonlinear dispersive 
<phrase>Deep Learning</phrase> <phrase>Architecture</phrase> for <phrase>Univariate</phrase> <phrase>Time Series</phrase> Forecasting This <phrase>paper</phrase> studies the problem of applying <phrase>machine learning</phrase> with <phrase>deep architecture</phrase> to <phrase>time series</phrase> forecasting. While these techniques have shown promise for modeling static <phrase>data</phrase>, applying them to sequential <phrase>data</phrase> is gaining increasing attention. This <phrase>paper</phrase> overviews the particular challenges present in applying Conditional <phrase>Restricted Boltzmann Machines</phrase> (CRBM) to <phrase>univariate</phrase> <phrase>time-series</phrase> forecasting and provides a comparison to common <phrase>algorithms</phrase> used for <phrase>time-series</phrase> prediction.
<phrase>Debugging</phrase> <phrase>Machine Learning</phrase> Extended Abstract Paste the appropriate <phrase>copyright</phrase> statement here. <phrase>ACM</phrase> now supports three different <phrase>copyright</phrase> statements: <phrase>ACM</phrase> <phrase>copyright</phrase>: <phrase>ACM</phrase> holds the <phrase>copyright</phrase> on the work. This is the historical approach. License: The <phrase>author</phrase>(s) retain <phrase>copyright</phrase>, but <phrase>ACM</phrase> receives an exclusive publication license. <phrase>Open Access</phrase>: The <phrase>author</phrase>(s) wish to pay for the work to be <phrase>open access</phrase>. The additional fee must be paid to <phrase>ACM</phrase>. This text field is large enough to hold the appropriate release statement assuming it is <phrase>single</phrase> spaced in a <phrase>sans-serif</phrase> 7 point <phrase>font</phrase>. Every submission will be assigned their own unique DOI string to be included here. Abstract Creating a <phrase>machine learning</phrase> <phrase>solution</phrase> for a <phrase>real world</phrase> problem often requires multiple iterations of investigation and improvement until it reaches satisfactory performance. Even after deployment, it is common to discover limitations of the <phrase>model</phrase> or changes in the <phrase>target</phrase> concept that necessitate modifications to the <phrase>training data</phrase> and parameters. However, as of today, there is no common wisdom about what these iterations consist of, nor what <phrase>debugging</phrase> tools are needed to aid the investigative process. In this work we present a novel technique to help <phrase>model</phrase> developers find the <phrase>root</phrase> causes of prediction error on <phrase>test</phrase> items (henceforth '<phrase>bugs</phrase>') and so help the developer to fix them. Given an observed bug our method aims to identify the training items most responsible for biasing the <phrase>model</phrase> towards giving the wrong prediction on the specific <phrase>test</phrase> item. This set of training items can aid in discovery of common errors like faulty training <phrase>labels</phrase> or poor <phrase>training data</phrase> coverage. Our method is applicable over many different learners, including deep <phrase>neural nets</phrase> with large and complex <phrase>model</phrase> representations, as well as many different <phrase>data</phrase> types.
SILA: a spatial instance learning approach for deep webpages <phrase>Deep Web</phrase> pages convey very relevant <phrase>information</phrase> for different application domains like <phrase>e-government</phrase>, <phrase>e-commerce</phrase>, <phrase>social networking</phrase>. For this reason there is a constant <phrase>high</phrase> interest in efficiently, effectively and automatically extracting <phrase>data</phrase> from <phrase>Deep Web</phrase> <phrase>data</phrase> sources. In this <phrase>paper</phrase> we present SILA, a novel Spatial Instance Learning Approach, that allows for extracting <phrase>data records</phrase> from <phrase>Deep Web</phrase> pages by exploiting both the spatial <phrase>arrangement</phrase> and the presentation features of <phrase>data</phrase> items/fields <phrase>produced</phrase> by layout engines of <phrase>Web browsers</phrase> in visualizing <phrase>Deep Web</phrase> pages on the screen. SILA is <phrase>independent</phrase> from the internal <phrase>HTML</phrase> encodings of <phrase>Web pages</phrase>, and allows for recognizing <phrase>data records</phrase> in pages having multiple <phrase>data</phrase> regions in which <phrase>data</phrase> items are arranged by many different presentation layouts. <phrase>Experimental</phrase> <phrase>results</phrase> show that SILA has very <phrase>high</phrase> precision and recall and that it works much better than MDR and ViNTs approaches.
<phrase>Multipath</phrase> <phrase>Sparse Coding</phrase> Using Hierarchical Matching Pursuit Complex <phrase>real-world</phrase> signals, such as images, contain dis-criminative structures that differ in many aspects including scale, invariance, and <phrase>data</phrase> <phrase>channel</phrase>. While progress in <phrase>deep learning</phrase> shows the importance of learning features through <phrase>multiple layers</phrase>, it is equally important to learn features through multiple paths. We propose <phrase>Multipath</phrase> Hierarchical Matching Pursuit (M-HMP), a novel <phrase>feature learning</phrase> <phrase>architecture</phrase> that combines a collection of hierarchical sparse features for <phrase>image classification</phrase> to capture multiple aspects of discriminative structures. Our <phrase>building blocks</phrase> are MI-KSVD, a codebook <phrase>learning algorithm</phrase> that balances the <phrase>reconstruction</phrase> error and the mutual incoherence of the codebook, and batch orthogonal matching pursuit (OMP); we apply them recursively at varying layers and scales. The result is a highly discriminative image representation that leads to large improvements to the <phrase>state</phrase>-of-the-<phrase>art</phrase> on many
Modular <phrase>deep belief</phrase> networks that do not forget <phrase>Deep belief</phrase> networks (DBNs) are popular for learning compact representations of <phrase>high</phrase>-dimensional <phrase>data</phrase>. However, most approaches so far rely on having a <phrase>single</phrase>, complete <phrase>training set</phrase>. If the distribution of relevant features changes during subsequent training stages, the features learned in earlier stages are gradually forgotten. Often it is desirable for <phrase>learning algorithms</phrase> to retain what they have previously learned, even if the input distribution temporarily changes. This <phrase>paper</phrase> introduces the M-DBN, an unsupervised modular DBN that addresses the forgetting problem. M-DBNs are composed of a number of modules that are trained only on samples they best reconstruct. While modularization by itself does not prevent forgetting, the M-DBN additionally uses a learning method that adjusts each module's learning rate proportionally to the fraction of best reconstructed samples. On the MNIST <phrase>handwritten digit</phrase> dataset module specialization largely corresponds to the digits discerned by humans. Furthermore, in several learning tasks with changing MNIST digits, M-DBNs retain <phrase>learned features</phrase> even after those features are removed from the <phrase>training data</phrase>, while monolithic DBNs of comparable size forget feature mappings learned before.
Evaluating the Robustness of EmotiBlog for <phrase>Sentiment Analysis</phrase> and Opinion <phrase>Mining</phrase> Preliminary <phrase>research</phrase> demonstrated the <phrase>Emo</phrase>-tiBlog annotated corpus relevance as a <phrase>Machine Learning</phrase> resource to detect subjective <phrase>data</phrase>. In this <phrase>paper</phrase> we compare EmotiBlog with the JRC Quotes corpus in <phrase>order</phrase> to check the robustness of its annotation. We concentrate on its <phrase>coarse-grained</phrase> <phrase>labels</phrase> and carry out a deep <phrase>Machine Learning</phrase> experimentation also with the inclusion of lexical resources. The <phrase>results</phrase> obtained show a similarity with the ones obtained with the JRC Quotes corpus demonstrating the EmotiBlog validity as a resource for the SA task.
Solving <phrase>Physics</phrase> Problems with <phrase>Multiple Representations</phrase> Solving <phrase>Physics</phrase> Problems with <phrase>Multiple Representations</phrase> We present a teaching strategy to encourage flexible, non algorithmic <phrase>problem solving</phrase>. Students create several problem representations to answer questions about a <phrase>single</phrase> problem situation. Through reflection <phrase>students learn</phrase> the value of non <phrase>algebraic</phrase> representations for analyzing and solving <phrase>physics</phrase> problems. <phrase>Problem solving</phrase> in introductory <phrase>math</phrase> and <phrase>science</phrase> courses is often algorithmic or procedural. Students are assigned lots of problems, which they solve by mimicking the way similar problems are solved by the <phrase>teacher</phrase> and the <phrase>textbook</phrase>. Most of the assigned problems are strictly quantitative and have solutions that can be obtained by selecting and manipulating one or two appropriate equations. Unfortunately, this type of <phrase>problem solving</phrase> does not result in a <phrase>deep understanding</phrase> of concepts nor in robust <phrase>problem solving</phrase> skills. There are several reasons. Students usually select an equation for a problem without checking its appropriateness and then apply the equation without seeking to understand its content. Rarely will a <phrase>student</phrase> reason conceptually about a problem or seek a
Fast <phrase>Algorithms</phrase> for Learning <phrase>Deep Neural Networks</phrase> With the increase in computation power and <phrase>data</phrase> availability in recent times, <phrase>machine learning</phrase> and <phrase>statistics</phrase> have seen an enormous development and widespread application in areas such as <phrase>computer vision</phrase>, <phrase>computational biology</phrase> and others. A focus of <phrase>current research</phrase> are deep <phrase>neural nets</phrase>: nested functions consisting of a hierarchy of layers of thousands of weights and nonlinear, <phrase>hidden units</phrase>. These <phrase>hidden units</phrase> encode hierarchical, distributed features that are useful for automatic discovery of patterns in complex, <phrase>high</phrase>-dimensional <phrase>data</phrase> such as images or speech, for applications such as classification, <phrase>regression</phrase> or <phrase>dimensionality reduction</phrase>. However, as was already observed in the early 1990s, deep nets are very difficult to <phrase>train</phrase> because of the ill-conditioned <phrase>nature</phrase> of their <phrase>objective function</phrase>, in turn caused by its deep level of nesting and the use of squashing nonlinearities. This <phrase>results</phrase> in methods such as <phrase>stochastic gradient descent</phrase> and nonlinear conjugate gradients taking tiny steps towards a minimum. The optimization proceeds so slowly that in many cases the researcher stops training after a predetermined number of iterations, so that the weights obtained may be far from a minimum. Second-<phrase>order</phrase> methods still move slowly and have limited applicability because of the large size of the <phrase>Hessian</phrase>. The relative merit of different methods is unclear at the moment. Finally, all these methods require several user parameters that are difficult to set and whose effect on the convergence is crucial: learning rate and its decay, <phrase>momentum</phrase> coefficient, minibatch size, etc. [1]. In summary, although there has been some recent progress in optimizing deep nets, this has been limited to (1) the use of parallel <phrase>computers</phrase> and <phrase>GPUs</phrase>, (2) heuristics to obtain a good initialization such as pretraining [2], and (3) <phrase>hand-crafted</phrase> implementations of standard optimization methods. At present, training <phrase>neural nets</phrase> remains an <phrase>art</phrase>, and for <phrase>large datasets</phrase> it requires hours or days even with parallel <phrase>computers</phrase>. This makes it hard to select the best <phrase>architecture</phrase> for a given task, and the amount of hand-tuning involved means the <phrase>results</phrase> are difficult to replicate by others. We propose a framework for deep net optimization, that we call method of <phrase>auxiliary</phrase> coordinates (MAC), that directly address the ill-conditioning problem, based on an idea of introducing <phrase>auxiliary</phrase> variables. This replaces the original problem involving a deeply <phrase>nested function</phrase> with an equivalent , constrained problem involving a different <phrase>function</phrase> in an augmented space, but much better conditioned and without nesting. Thus, it directly addresses 
<phrase>Multi-Objective</phrase> <phrase>Deep Reinforcement Learning</phrase> We propose Deep Optimistic Linear Support Learning (DOL) to solve <phrase>high</phrase>-dimensional <phrase>multi-objective</phrase> decision problems where the relative importances of the objectives are not known a priori. Using features from the <phrase>high</phrase>-dimensional inputs, DOL computes the convex coverage set containing all potential optimal solutions of the convex combinations of the objectives. To our <phrase>knowledge</phrase>, this is the first time that <phrase>deep reinforcement learning</phrase> has succeeded in learning <phrase>multi-objective</phrase> policies. In addition, we provide a <phrase>testbed</phrase> with two experiments to be used as a benchmark for deep <phrase>multi-objective</phrase> <phrase>reinforcement learning</phrase>.
Implement Web Learning System Based on <phrase>Genetic Algorithm</phrase> and Pervasive Agent <phrase>Ontology</phrase> For a <phrase>web-based</phrase> dynamic <phrase>learning environment</phrase>, personalized support for learners becomes more important. In <phrase>order</phrase> to achieve optimal efficiency in a learning process, individual learner's <phrase>cognitive</phrase> learning style should be taken into account. It is necessary to provide learners with an individualized learning support system. In this <phrase>paper</phrase>, a framework of web learning system based on <phrase>genetic algorithm</phrase> and Pervasive Agent <phrase>Ontology</phrase> is presented. The <phrase>proposed framework</phrase> utilizes <phrase>genetic algorithm</phrase> for representing and extracting a dynamic learning process and learning pattern to support students' <phrase>deep learning</phrase> in <phrase>web-based</phrase> <phrase>learning environment</phrase>. Aiming at the problems in current Web environment , we put <phrase>forward</phrase> the <phrase>information</phrase> integration method of <phrase>Semantic Web</phrase> based on Pervasive Agent <phrase>Ontology</phrase> (SWPAO method), which will integrate , analyze and process enormous web <phrase>information</phrase> and extract answers for students on the basis of <phrase>semantics</phrase>. And experiments do prove that it is feasible to use the method to develop an individual <phrase>Web-based</phrase> learning system, which is valuable for further study in more depth.
Improving <phrase>Neural Network</phrase> Generalization by Combining Parallel Circuits with Dropout In an attempt to solve the lengthy training times of <phrase>neural networks</phrase>, we proposed Parallel Circuits (PCs), a biologically inspired <phrase>architecture</phrase>. Previous work has shown that this approach fails to maintain generalization performance in spite of achieving sharp speed gains. To address this issue, and motivated by the way Dropout prevents node co-adaption, in this <phrase>paper</phrase>, we suggest an improvement by extending Dropout to the <phrase>PC</phrase> <phrase>architecture</phrase>. The <phrase>paper</phrase> provides multiple insights into this combination, including a <phrase>variety</phrase> of <phrase>fusion</phrase> approaches. Experiments show <phrase>promising results</phrase> in which improved error rates are achieved in most cases, whilst maintaining the speed advantage of the <phrase>PC</phrase> approach. 1 Introduction <phrase>Deep learning</phrase> has repeatedly made <phrase>significant improvements</phrase> to the generalization capability of <phrase>neural networks</phrase> through several core strategies including: (i) increasing <phrase>model</phrase> size, (<phrase>ii</phrase>) undergoing longer training periods as well as (iii) adopting larger datasets. Unfortunately, this approach creates a tremendous overhead in terms of both time and computational resources. <phrase>Deep learning</phrase> is increasingly becoming more feasible with the support of specialized hardware systems (e.g. multicore CPUs, graphics processing units (<phrase>GPUs</phrase>), and <phrase>high-performance computing</phrase> (HPC)). However, these solutions tend to be costly and require careful tailoring to derive maximal benefits. We attempt to apply <phrase>deep learning</phrase> to a <phrase>remote sensing</phrase> problem, within the constraints of an online platform with limited computational power. In <phrase>order</phrase> to address the feasibility concerns mentioned above, we decided to tackle the <phrase>computational problem</phrase> at the algorithmic level, as a pure hardware approach would be too expensive to solve the problem alone [1]. Within the scope of our previous <phrase>paper</phrase> [2], we have proposed Parallel Circuits (PCs), a <phrase>biology</phrase>-inspired <phrase>Artificial Neural Network</phrase> (ANN) <phrase>architecture</phrase>, as an attempt to reduce heavy computational loads without harming performance. Our preliminary experiments showed that <phrase>PC</phrase> architectures could decrease training time up to 40% under some constraints. On the contrary, the impact on <phrase>classification accuracy</phrase> was still debatable, as the proposed network exhibited unstable performance across configurations, especially when the size of the original ANN was small. Dropout is a recent technique for regularization that has been boosting <phrase>neural network</phrase> accuracy in many applications. By randomly dropping nodes in the network,
<phrase>Model</phrase>-Based <phrase>Programming</phrase> of Fault-Aware Systems A wide <phrase>range</phrase> of <phrase>sensor</phrase>-rich, networked <phrase>embedded systems</phrase> are being created that must operate robustly for years in the face of novel failures by managing complex <phrase>autonomic</phrase> processes. These systems are being composed, for example, into vast networks of space, air, ground, and underwater vehicles. Our objective is to revolutionize the way in which we control these new artifacts by creating reactive <phrase>model</phrase>-based <phrase>programming languages</phrase> that enable everyday systems to reason intelligently and enable machines to explore other worlds. A <phrase>model</phrase>-based program is <phrase>state</phrase> and fault aware; it elevates the <phrase>programming</phrase> task to specifying intended <phrase>state</phrase> evolutions of a system. The pro-gram's executive automatically coordinates system interactions to achieve these states, entertaining known and potential failures, using models of its constituents and environment. At the executive's core is a method, called CONFLICT-<phrase>DIRECTED</phrase> A*, which quickly prunes promising but infeasible solutions , using a form of one-<phrase>shot</phrase> learning. This approach has been demonstrated on a <phrase>range</phrase> of systems , including the National <phrase>Aeronautics</phrase> and Space Administration's Deep Space One probe. <phrase>Model</phrase>-based <phrase>programming</phrase> is being generalized to <phrase>hybrid</phrase> discrete-continuous systems and the coordination of networks of robotic vehicles. T he demands we place on robotic explorers and everyday <phrase>embedded systems</phrase> have gone through a <phrase>major</phrase> transformation over the last decade. For example, the challenge of <phrase>ro</phrase>-botic <phrase>space exploration</phrase> has dramatically shifted from simple planetary flybys to microrovers that can alight on several <phrase>asteroids</phrase>, collect the most interesting geologic samples, and return with their findings. This challenge will not be answered through billion-dollar missions with 100-<phrase>member</phrase> ground teams but through <phrase>innovation</phrase>. Future <phrase>space exploration</phrase> will be enabled in significant part by inexpensive, " <phrase>fire-and-forget</phrase> " space explorers that are self-reliant and capable of handling unexpected situations; they must balance curiosity with caution. Self-reliance of this sort can only be achieved through an explicit understanding of mission goals and the ability to reason from a <phrase>model</phrase> of how the explorer and its environment can support or circumvent these goals. This <phrase>knowledge</phrase> is used to carefully coordinate the <phrase>complex network</phrase> of sensors and actuators within the explorer. Given the complexity of current and future <phrase>spacecraft</phrase>, such <phrase>fine-tuned</phrase> coordination seems to be a nearly impossible task using traditional <phrase>software engineering</phrase> approaches. Our demand for this level of fault resilience is no longer isolated to the realm of exotic space explorers. It has shifted to systems that are part of our everyday activities, such as our houses 
A <phrase>Hybrid</phrase> <phrase>Input-Output</phrase> Approach to <phrase>Model</phrase> <phrase>Metabolic</phrase> Systems: An Application to <phrase>Intracellular</phrase> <phrase>Thiamine</phrase> <phrase>Kinetics</phrase> Models of the dynamics of complex <phrase>metabolic</phrase> systems offer potential benefits to the deep comprehension of the system under study as well as for the performance of certain tasks. Unfortunately, dynamic modeling of a great deal of <phrase>metabolic</phrase> systems may be problematic due to the incompleteness of the available <phrase>knowledge</phrase> about the underlying mechanisms and to the lack of an adequate observational <phrase>data set</phrase>. In theory, a valid <phrase>alternative</phrase> to <phrase>classical</phrase> structural modeling through <phrase>ordinary differential equations</phrase> could be represented by <phrase>input-output</phrase> approaches. But, in practice, such methods, which learn the nonlinear dynamics of the system from <phrase>input-output</phrase> <phrase>data</phrase>, fail when the <phrase>experimental</phrase> <phrase>data set</phrase> is poor either in size or in quality. Such a situation is not rare in the case of <phrase>metabolic</phrase> systems. This <phrase>paper</phrase> deals with a <phrase>hybrid</phrase> approach which aims at overcoming the problems addressed above. More specifically, it allows us to solve the identification problems of the <phrase>intracellular</phrase> <phrase>thiamine</phrase> <phrase>kinetics</phrase> in the intestine tissue. The method, which is half way between the structural and <phrase>input-output</phrase> approach, uses the outcomes of the <phrase>simulation</phrase> of a qualitative structural <phrase>model</phrase> to build a good initialization of a fuzzy system identifier. Such an initialization allows us to efficiently cope with both the incompleteness of <phrase>knowledge</phrase> and the inadequacy of the available <phrase>data set</phrase>, and to derive an <phrase>input-output</phrase> <phrase>model</phrase> of the <phrase>intracellular</phrase> <phrase>thiamine</phrase> <phrase>kinetics</phrase> in the intestine tissue. The comparison of the predictions of the <phrase>intracellular</phrase> <phrase>thiamine</phrase> <phrase>kinetics</phrase> obtained by the application of such a <phrase>model</phrase> with those obtained by <phrase>traditional approaches</phrase>, namely compartmental models, <phrase>neural networks</phrase>, and fuzzy systems, highlighted a better performance of our <phrase>model</phrase>. As the structural assumptions are relaxed, we obtained a <phrase>model</phrase> slightly less informative than a purely structural one but robust enough to be used as a simulator. The <phrase>paper</phrase> also discusses the interpretative potential offered by such a <phrase>model</phrase>, as tested on <phrase>diabetic</phrase> subjects.
Multimodal <phrase>Emotion</phrase> Recognition Using Multimodal <phrase>Deep Learning</phrase> To enhance the performance of affective models and reduce the cost of acquiring <phrase>physiological</phrase> signals for <phrase>real-world</phrase> applications, we adopt mul-timodal <phrase>deep learning</phrase> approach to construct af-fective models from multiple <phrase>physiological</phrase> signals. For unimodal enhancement task, we indicate that the best <phrase>recognition accuracy</phrase> of 82.11% on <phrase>SEED</phrase> dataset is achieved with shared representations generated by Deep AutoEncoder (DAE) <phrase>model</phrase>. For multimodal facilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE) achieves the mean accuracies of 91.01% and 83.25% on <phrase>SEED</phrase> and DEAP datasets, respectively , which are much <phrase>superior</phrase> to the <phrase>state</phrase>-of-the-<phrase>art</phrase> approaches. For cross-modal learning task, our <phrase>experimental</phrase> <phrase>results</phrase> demonstrate that the mean accuracy of 66.34% is achieved on <phrase>SEED</phrase> dataset through shared representations generated by <phrase>EEG</phrase>-based DAE as <phrase>training samples</phrase> and shared representations generated by eye-based DAE as testing sample, and vice versa.
Parzen <phrase>Discriminant</phrase> Analysis In this <phrase>paper</phrase>, we propose a non-parametric <phrase>Discriminant</phrase> Analysis method (no assumption on the <phrase>distributions</phrase> of classes), called Parzen <phrase>Discriminant</phrase> Analysis (<phrase>PDA</phrase>). Through a deep investigation on the non-parametric <phrase>density estimation</phrase>, we find that minimizing/maximizing the distances between each <phrase>data</phrase> sample and its nearby similar/dissimilar samples is equivalent to minimizing an <phrase>upper</phrase> bound of the <phrase>Bayesian</phrase> <phrase>error rate</phrase>. Based on this theoretical analysis, we define our criterion as maximizing the <phrase>average</phrase> local dissimilarity scatter with respect to a fixed <phrase>average</phrase> local similarity scatter. All local scatters are calculated in fixed size local regions, resembling the idea of Parzen estimation. Experiments in <phrase>UCI</phrase> <phrase>machine learning</phrase> <phrase>database</phrase> show that our method impressively outperforms other related neighbor based non-parametric methods.
<phrase>Knowledge</phrase> acquisition and opinion formation at <phrase>science</phrase> <phrase>museums</phrase>: the potential of a discussion terminal for collaborative elaboration on controversial issues This <phrase>PhD</phrase> project examines the potential of a discussion terminal to support deep elaboration of controversial <phrase>information</phrase> and formation of well-founded opinions at <phrase>science</phrase> <phrase>museums</phrase>. It is assumed that the salience of controversial <phrase>information</phrase>, the opportunity to express one's own opinion, and availability of social comparison <phrase>information</phrase> are crucial factors for both learning and opinion formation. A first <phrase>data</phrase> collection concerned the impact of active opinion expression and salience of arguments on elaboration processes and <phrase>knowledge</phrase> acquisition in a 2x2-<phrase>design</phrase>. <phrase>Results</phrase> are still outstanding. The second <phrase>data</phrase> collection phase will also consider the influence of social comparison <phrase>information</phrase> and asynchronous discussion at the discussion terminal. <phrase>Science</phrase> <phrase>Museums</phrase> and <phrase>Public</phrase> Understanding of <phrase>Science</phrase> <phrase>Oppenheimer</phrase> has already stated 1968 (p. 206) that there is an " increasing need to develop <phrase>public</phrase> understanding of <phrase>science</phrase> and <phrase>technology</phrase> " and today, due to rapid growth of new technologies, this need is even bigger than ever before. <phrase>Informal learning</phrase> in <phrase>science</phrase> <phrase>museums</phrase> can be a <phrase>major</phrase> contributor in promoting <phrase>public</phrase> understanding of <phrase>science</phrase> as <phrase>museums</phrase> are one central medium in communicating central scientific ideas and presenting relevant objects (Durant, 1992). To promote <phrase>public</phrase> understanding of <phrase>science</phrase>, multiple viewpoints from different perspectives are needed to be presented (Bayrhuber, 2001): Boyd (1998, p. 214) considers the modern <phrase>science museum</phrase> as a " marketplace of multiple points of view, a forum where controversy can be aired ". In addressing current socio-scientific issues today, <phrase>science</phrase> <phrase>museums</phrase> are challenged to present the ambiguity and controversy of these topics and to support visitors in developing reflective and <phrase>critical thinking</phrase> (Halpern, 1989). Thus, new installations are needed which emphasize involvement and activity of the <phrase>museum</phrase> visitor and put the exhibition content in socially and personally relevant context (McLean, 2006). Pedretti (2006, p. 30) states that " spaces for dialogue [] enhance the <phrase>spirit</phrase> of inquiry, allow for a <phrase>free</phrase> exchange of ideas, and encourage the formulation and articulation of carefully thought out, defensible opinions. " To create this space, in this project, a computer-mediated discussion terminal was designed to mediate and encourage elaboration on and opinion exchange about the topic <phrase>nanotechnology</phrase> as one the most explosive <phrase>science</phrase> topics nowadays. Discussion involves the <phrase>museum</phrase> visitor in the <phrase>public</phrase> debate about <phrase>science</phrase>, turns <phrase>public</phrase> debate into a personal, " <phrase>private</phrase> " one, and should therefore foster reconsideration and reflection of <phrase>information</phrase> (Schellens, & Valcke, 2004). A Discussion Terminal as Scaffold for <phrase>Critical Thinking</phrase> and 
Modeling <phrase>image patches</phrase> with a <phrase>directed</phrase> hierarchy of <phrase>Markov</phrase> <phrase>random fields</phrase> We describe an efficient learning procedure for multilayer <phrase>generative models</phrase> that combine the best aspects of <phrase>Markov</phrase> <phrase>random fields</phrase> and deep, <phrase>directed</phrase> <phrase>belief nets</phrase>. The <phrase>generative models</phrase> can be learned one layer at a time and when learning is complete they have a very fast inference procedure for <phrase>computing</phrase> a good approximation to the posterior distribution in all of the <phrase>hidden layers</phrase>. Each <phrase>hidden layer</phrase> has its own MRF whose <phrase>energy</phrase> <phrase>function</phrase> is modulated by the top-down <phrase>directed</phrase> connections from the layer above. To generate from the <phrase>model</phrase>, each layer in turn must settle to equilibrium given its top-down input. We show that this type of <phrase>model</phrase> is good at capturing the <phrase>statistics</phrase> of patches of <phrase>natural images</phrase>.
Issues in the <phrase>Design</phrase> of <phrase>AI</phrase>-Based Schedulers - Workshop <phrase>Report</phrase> Based on the experience in <phrase>manufacturing</phrase> <phrase>production</phrase> scheduling problems which the <phrase>AI</phrase> <phrase>community</phrase> has amassed over the last ten years, a workshop was held to provide a forum for discussion of the issues encountered in the <phrase>design</phrase> of <phrase>AI</phrase>-based scheduling systems. Several topics were addressed including : the relative virtues of expert system , deep method, and interactive approaches , the balance between predictive and reac-tive components in a scheduling system, the maintenance of convenient scheduling descriptions, the application of the ideas of <phrase>chaos theory</phrase> to scheduling, the <phrase>state</phrase> of the <phrase>art</phrase> in schedulers which learn, and the practi-cality and desirability of a set of benchmark scheduling problems. This article expands on these issues, abstracts the papers which were presented, and summarizes the lengthy discussions that took place.
<phrase>Model</phrase>-based development: applications by H.S. Lahman I picked up this <phrase>book</phrase> because I am always interested in learning about new <phrase>software design</phrase> methodologies. I learned Structured <phrase>design</phrase> by analyzing PL/1 code and <phrase>Object-oriented design</phrase> by creating <phrase>Smalltalk</phrase> and <phrase>Java</phrase> applications. This <phrase>book</phrase> was my first introduction to <phrase>Model</phrase>-Based development. Fundamentally, <phrase>Model</phrase>-based development (MBD) is not much different than an <phrase>object-oriented</phrase> (OO) approach. In MBD the focus is on <phrase>large-scale</phrase> re-use instead of object re-use. In MBD a <phrase>deep understanding</phrase> of the customer's domain is essential in producing a <phrase>design</phrase> which will stand the <phrase>test</phrase> of time. The developer should look for modeling invariants, groups that are stable in the customer's problem space. The <phrase>software</phrase> structure should <phrase>model</phrase> the customer's <phrase>infrastructure</phrase>. Instead of finding a generalized structure which works across many different domains the goal is to find structures relative to the customer's space being analyzed. <phrase>Design</phrase> for re-use during the <phrase>evolution</phrase> of an <phrase>organization</phrase> rather than re-use among different organizations. Another difference between <phrase>Object-oriented design</phrase> and <phrase>Model</phrase>-Based development is how they handle implementation hiding. MBD is more restrictive in the accessibility of subsystems than OO applications. I like how MBD approaches this better than OO <phrase>design</phrase>. I agree with the <phrase>author</phrase> that subsystems should be hidden from one another and only interact through well-defined interfaces. When I develop <phrase>software</phrase> I don't like passing around handlers all over my application. This is one part of the <phrase>book</phrase> where I think the <phrase>author</phrase> really <phrase>hits</phrase> it on the nose. Code should reduce dependencies instead of adding them through implementation hiding. What I enjoyed about this <phrase>book</phrase> is that it is not trying to sell the reader on only <phrase>Model</phrase>-Based development. Instead the <phrase>book</phrase> discusses the thought processes behind <phrase>software design</phrase>. There is a lot of theory and not a lot of code. For me this was the only negative <phrase>aspect</phrase> of the <phrase>book</phrase>. I like to see code accompany the discussion about <phrase>design</phrase>. It allows me to apply a new <phrase>design</phrase> methodology faster when I can see how it is used in a <phrase>programming language</phrase>. Although, the vast <phrase>UML</phrase> examples allow the reader to quickly take any <phrase>Object-oriented language</phrase> and apply the <phrase>design</phrase> principles described in the <phrase>book</phrase>. The <phrase>book</phrase> is divided into three parts. The first part discusses the <phrase>history</phrase> of <phrase>object-oriented design</phrase> principles. There is a Pet Care <phrase>center</phrase> example which really highlights the use of MBD and how to identify subsystems. This example helps build 
<phrase>Neural Network</phrase> Assisted <phrase>Tile</phrase> Size Selection <phrase>Data</phrase> <phrase>locality</phrase> optimization plays a significant role in reducing the execution time of many loop-intensive kernels. Loop tiling at various levels is often used to effectively exploit <phrase>data</phrase> <phrase>locality</phrase> in deep <phrase>memory</phrase> hierarchies. The recent development of frameworks for parametric loop tiling of user code has <phrase>lead</phrase> to a widening of the <phrase>range</phrase> of applications that could benefit from auto-tuning of <phrase>tile</phrase> sizes. Current <phrase>model</phrase>-driven approaches suffer from limitations, such as the inability to accurately <phrase>model</phrase> the complex interplay between multiple hardware components that affect performance. Auto-tuning <phrase>libraries</phrase> such as ATLAS rely on extensive empirical search for <phrase>tile</phrase> size optimization, which has been shown to be very effective. However, the effectiveness of such approaches for arbitrary parametrically tiled user code has not been demonstrated. We consider the problem of selecting the best <phrase>tile</phrase> sizes for arbitrary user-defined programs, by sampling in the full space of <phrase>tile</phrase> sizes. We have developed a technique to build a performance predictor associated with a specific program. Our approach uses <phrase>statistical machine learning</phrase> to <phrase>train</phrase> an <phrase>artificial neural network</phrase> (ANN) to predict the performance distribution of execution time for scientific kernels. We show how this search strategy significantly improves over the variability of random search. Our observations and <phrase>results</phrase> on various kernels also show promise for the use of ANNs in predicting the runtime behavior for variations of tiling configurations .
<phrase>Deep Learning</phrase>, Ict and <phrase>21 St Century</phrase> Skills Leading for <phrase>Education</phrase> Quality <phrase>Deep Learning</phrase>, Ict and <phrase>21 St Century</phrase> Skills Leading for <phrase>Education</phrase> Quality One of the key challenges confronting <phrase>Catholic</phrase> <phrase>Education</phrase> is improving educational quality. A key indicator of improved educational quality is improving <phrase>student</phrase>-<phrase>learning outcomes</phrase> so that students acquire the skills needed for <phrase>21 st century</phrase> learning. These skills include <phrase>creativity</phrase>, <phrase>communication</phrase>, collaboration and <phrase>critical thinking</phrase> as well as being productive users of <phrase>technology</phrase>. The acquisition of <phrase>21 st century</phrase> skills requires the development of personalised learning that is broader than simply an acquisition of the basics. Leaders have a crucial role to <phrase>play</phrase> in understanding <phrase>student</phrase> learning and in facilitating a <phrase>teaching and learning</phrase> program which moves from surface to <phrase>deep learning</phrase>. The use of <phrase>information and communications technology</phrase> (ICT) is fundamental for the kind of personalised learning needed for the development of <phrase>deep learning</phrase> but, as the SAMR <phrase>model</phrase> makes clear, simply using <phrase>technology</phrase> does not guarantee that <phrase>deep learning</phrase> will occur. What is crucial is the transformative use of ICT for learning. This <phrase>paper</phrase> is designed to link understandings about <phrase>deep learning</phrase> with the appropriate use of ICT to enhance learning. Abstract One of the key challenges confronting <phrase>Catholic</phrase> <phrase>Education</phrase> is improving educational quality. A key indicator of improved educational quality is improving <phrase>student</phrase>-<phrase>learning outcomes</phrase> so that students acquire the skills needed for <phrase>21 st century</phrase> learning. These skills include <phrase>creativity</phrase>, <phrase>communication</phrase>, collaboration and <phrase>critical thinking</phrase> as well as being productive users of <phrase>technology</phrase>. The acquisition of <phrase>21 st century</phrase> skills requires the development of personalised learning that is broader than simply an acquisition of the basics. Leaders have a crucial role to <phrase>play</phrase> in understanding <phrase>student</phrase> learning and in facilitating a <phrase>teaching and learning</phrase> program which moves from surface to <phrase>deep learning</phrase>. The use of <phrase>information and communications technology</phrase> (ICT) is fundamental for the kind of personalised learning needed for the development of <phrase>deep learning</phrase> but, as the SAMR <phrase>model</phrase> makes clear, simply using <phrase>technology</phrase> does not guarantee that <phrase>deep learning</phrase> will occur. What is crucial is the transformative use of ICT for learning. This <phrase>paper</phrase> is designed to link understandings about <phrase>deep learning</phrase> with the appropriate use of ICT to enhance learning.
Interactional achievement of shared <phrase>mathematical</phrase> understanding in a virtual <phrase>math</phrase> team Learning <phrase>mathematics</phrase> involves specific forms of social practice. In this <phrase>paper</phrase>, we describe socially situated, interactional processes involved with <phrase>collaborative learning</phrase> of <phrase>mathematics</phrase> in a special online <phrase>collaborative learning</phrase> environment. Our analysis highlights the methodic ways group members enact the affordances of their situation (a) to visually explore a <phrase>mathematical</phrase> pattern, (b) to co-construct shared <phrase>mathematical</phrase> artifacts, (c) to make visible the meaning of the <phrase>construction</phrase>, (d) to translate between <phrase>graphical</phrase>, <phrase>narrative</phrase> and symbolic representations and (<phrase>e</phrase>) to coordinate their actions across multiple interaction spaces, while they are working on <phrase>open-ended</phrase> <phrase>math</phrase> problems. In particular, we identify key roles of referential and representational practices in the co-<phrase>construction</phrase> of deep <phrase>mathematical</phrase> group understanding. The <phrase>case study</phrase> illustrates how <phrase>mathematical</phrase> understanding is built and shared through the online interaction.
<phrase>Syntactic</phrase> Features for <phrase>Protein</phrase>-<phrase>Protein</phrase> Interaction Extraction Background: Extracting <phrase>Protein</phrase>-<phrase>Protein</phrase> Interactions (PPI) from <phrase>research</phrase> papers is a way of translating <phrase>information</phrase> from <phrase>English</phrase> to the <phrase>language</phrase> used by the <phrase>databases</phrase> that store this <phrase>information</phrase>. With <phrase>recent advances</phrase> in automatic PPI detection, it is now possible to speed up this process considerably. <phrase>Syntactic</phrase> features from different parsers for biomedical <phrase>English</phrase> text are readily available, and can be used to improve the performance of such PPI extraction systems. <phrase>Results</phrase>: A complete PPI system was built. It uses a <phrase>deep syntactic</phrase> parser to capture the <phrase>semantic</phrase> meaning of the sentences, and a shallow dependency parser to improve the performance further. <phrase>Machine learning</phrase> is used to automatically make rules to extract pairs of interacting <phrase>proteins</phrase> from the <phrase>semantics</phrase> of the sentences. The <phrase>results</phrase> have been evaluated using the AImed corpus, and they are better than earlier <phrase>published results</phrase>. The F-score of the current system is 69.5% for <phrase>cross-validation</phrase> between pairs that may come from the same abstract, and 52.0% when complete abstracts are hidden until final testing. Automatic 10-fold <phrase>cross-validation</phrase> on the entire AImed corpus can be done in less than 45 minutes on a <phrase>single</phrase> server. We also present some previously unpublished <phrase>statistics</phrase> about the AImed corpus, and a <phrase>short</phrase> analysis of the AImed representation <phrase>language</phrase>. Conclusions: We present a PPI extraction system, using different <phrase>syntactic</phrase> parsers to extract features for <phrase>SVM</phrase> with <phrase>Tree</phrase> Kernels, in <phrase>order</phrase> to automatically create rules to discover <phrase>protein</phrase> interactions described in the <phrase>molecular biology</phrase> <phrase>literature</phrase>. The system performance is better than other published systems, and the implementation is freely available to anyone who is interested in using the system for <phrase>academic</phrase> purposes. The system can help researchers quickly discover reported PPIs, and thereby increasing the speed at which <phrase>databases</phrase> can be populated and novel <phrase>signaling pathways</phrase> can be constructed.
Advanced <phrase>probabilistic models</phrase> for clustering and projection Probabilistic modeling for <phrase>data mining</phrase> and <phrase>machine learning</phrase> problems is a fundamental <phrase>research</phrase> <phrase>area</phrase>. The <phrase>general</phrase> approach is to assume a <phrase>generative model</phrase> underlying the observed <phrase>data</phrase>, and estimate <phrase>model</phrase> parameters via likelihood maximization. It has the deep <phrase>probability theory</phrase> as the <phrase>mathematical</phrase> background, and enjoys a large amount of methods from statistical learning, sampling theory and <phrase>Bayesian statistics</phrase>. In this <phrase>thesis</phrase> we study several advanced <phrase>probabilistic models</phrase> for <phrase>data</phrase> clustering and feature projection, which are the two important <phrase>unsupervised learning</phrase> problems. The goal of clustering is to group similar <phrase>data</phrase> points together to uncover the <phrase>data</phrase> clusters. While numerous methods exist for various clustering tasks, one important question still remains, i.e., how to automatically determine the number of clusters. The first part of the <phrase>thesis</phrase> answers this question from a mixture modeling perspective. A finite <phrase>mixture model</phrase> is first introduced for clustering, in which each mixture component is assumed to be an <phrase>exponential family</phrase> distribution for generality. The <phrase>model</phrase> is then extended to an infinite <phrase>mixture model</phrase>, and its strong connection to <phrase>Dirichlet</phrase> process (DP) is uncovered which is a non-parametric <phrase>Bayesian</phrase> framework. A variational <phrase>Bayesian</phrase> <phrase>algorithm</phrase> called VBDMA is derived from this new insight to learn the number of clusters automatically, and <phrase>empirical studies</phrase> on some 2D <phrase>data</phrase> sets and an image <phrase>data set</phrase> verify the effectiveness of this <phrase>algorithm</phrase>. In feature projection, we are interested in <phrase>dimensionality reduction</phrase> and aim to find a <phrase>low-dimensional</phrase> <phrase>feature representation</phrase> for the <phrase>data</phrase>. We first review the well-known <phrase>principal component analysis</phrase> (<phrase>PCA</phrase>) and its probabilistic interpretation (PPCA), and then generalize PPCA to a novel <phrase>probabilistic model</phrase> which is able to handle non-linear projection known as kernel <phrase>PCA</phrase>. An expectation-maximization (EM) <phrase>algorithm</phrase> is derived for kernel <phrase>PCA</phrase> such that it is fast and applicable to large <phrase>data</phrase> sets. Then we propose a novel supervised projection method called MORP, which can take the output <phrase>information</phrase> into account in a <phrase>supervised learning</phrase> context. <phrase>Empirical studies</phrase> on various <phrase>data</phrase> sets show much better <phrase>results</phrase> compared to unsupervised projection and other supervised projection methods. At the end we generalize MORP probabilistically to propose SPPCA for supervised projection, and we can also naturally extend the <phrase>model</phrase> to S 2 PPCA which is a <phrase>semi-supervised</phrase> projection method. This allows us to incorporate both the <phrase>label</phrase> <phrase>information</phrase> and the <phrase>unlabeled data</phrase> into the projection process. In the third part of the <phrase>thesis</phrase>, we introduce a unified <phrase>probabilistic model</phrase> which 
Co-<phrase>evolution</phrase> of Learning, New <phrase>Media</phrase>, and New Learning Organizations Our <phrase>research</phrase> at the <phrase>Center</phrase> for <phrase>Lifelong Learning</phrase> & <phrase>Design</phrase> (L3D) over the past two decades has been focused on creating a new understanding of learning, new <phrase>media</phrase>, and new learning organizations. Our co-<phrase>evolutionary</phrase> perspective explores the <phrase>dialectical</phrase> relationship between: how a <phrase>deep understanding</phrase> of learning (specifically: <phrase>lifelong learning</phrase>, self-<phrase>directed</phrase> learning, and <phrase>informal learning</phrase>) creates innovative demands and <phrase>design</phrase> criteria for future generations of social-technical environments; how the unique potential of computational <phrase>media</phrase> (specifically: meta-<phrase>design</phrase>, <phrase>distributed cognition</phrase>, context-aware environments) impacts and transforms learning by transcending "gift-wrapping" and " <phrase>technology</phrase>-centered " approaches; and how new learning organizations (specifically: cultures of participation, learning webs, niche communities in <phrase>Long</phrase>-Tail environments) contribute to reinvent learning and <phrase>education</phrase> in the <phrase>21st century</phrase>. The <phrase>conceptual framework</phrase> is illustrated by specific developments of social-technical environments that we have designed and evaluated including: courses-as-seeds; collaborative, domain-oriented <phrase>design</phrase> environments, context-aware systems, courses with massive enrollments, and course environments for large number of topics.
BrainsComputersMachines: Neural <phrase>Engineering</phrase> in <phrase>Science</phrase> Classrooms With its roots in <phrase>neuroscience</phrase>, <phrase>robotics</phrase>, and <phrase>engineering</phrase>, the field of neural <phrase>engineering</phrase> is of <phrase>high</phrase> interest to many students. Neural <phrase>engineering</phrase> brings together brains, <phrase>computers</phrase>, and machines with impressive emerging technologies and has an undeniable " wow " factor. Moreover, many students may already be engaged with gaming, <phrase>programming</phrase>, and <phrase>robotics</phrase> through <phrase>school</phrase> classes and clubs, out-of-<phrase>school</phrase> time programs, and personal hobbies. Therefore, by bringing neural <phrase>engineering</phrase> topics and related <phrase>design</phrase> challenges into the classroom, teachers have the ability to leverage students' interests and to connect their identities inside and outside of the <phrase>school</phrase> day. By integrating a study of <phrase>neuroethics</phrase> with neural <phrase>engineering</phrase> , students can engage in deep discussions about the <phrase>ethical</phrase> implications of these new technologies and develop an understanding of the influences of <phrase>science</phrase>, <phrase>engineering</phrase>, and <phrase>technology</phrase> on individuals and <phrase>society</phrase> (NGSS <phrase>Lead</phrase> States, 2013). The field of neural <phrase>engineering</phrase> aims to <phrase>design</phrase> solutions for people with disabilities, disorders, and injuries that affect the <phrase>nervous</phrase> system. <phrase>Neurological disorders</phrase> likely touch the lives of every <phrase>student</phrase>, directly or indirectly, demonstrating the top-ic's personal relevance and society's need for these new technologies. The <phrase>World Health Organization</phrase> (2006) estimates that more than one billion people worldwide are affected by <phrase>neu</phrase>-rological disorders. In the <phrase>United States</phrase>, almost 800,000 people experience a <phrase>stroke</phrase> (one <phrase>stroke</phrase> every 40 s); <phrase>stroke</phrase> is also responsible for one of every 20 deaths (one <phrase>death</phrase> every 4 min; Mozaffarian, 2015). <phrase>Traumatic brain injury</phrase> affects 1.7 million people each year and causes 52,000 deaths (30.5% of all injury related deaths in the <phrase>United States</phrase>) and 275,000 hospitalizations (Faul et al., 2010). Particularly relevant to precollege students is the estimated 300,000 <phrase>sports</phrase>-related <phrase>traumatic brain injuries</phrase> that occur annually, making <phrase>sports</phrase> the second most common cause of <phrase>concussions</phrase> in <phrase>Americans</phrase> 1524 yr old (Marar et al., 2012). Most students will likely know someone with damage or <phrase>disease</phrase> of the <phrase>nervous</phrase> system because of the <phrase>high</phrase> prevalence of <phrase>stroke</phrase> and <phrase>traumatic brain injury</phrase> and other disorders such as <phrase>spinal cord injury</phrase>, <phrase>Parkinson's disease</phrase>, <phrase>Alzheimer's disease</phrase>, and <phrase>amyotrophic lateral sclerosis</phrase>. Although significant progress has been made in treating many of these disorders, effective therapies are still lacking for conditions such as <phrase>traumatic brain injury</phrase>, <phrase>stroke</phrase>, and <phrase>Secondary</phrase> <phrase>science</phrase> teachers are challenged to align their cur-ricula to <phrase>state</phrase> and national standards while also engaging students' interests and leveraging their everyday experiences through equitable learning opportunities. As the Next Generation <phrase>Science</phrase> Standards 
Spatial and Temporal Models for Texture-based <phrase>Video</phrase> Coding <phrase>ii</phrase> ACKNOWLEDGMENTS Upon the completion of this dissertation, I realized that there will never be the perfect words to <phrase>express my gratitude</phrase> towards those who have inspired, encouraged and motivated me through my years at <phrase>Purdue</phrase>. Yet, these names and faces will always remain deep in my <phrase>heart</phrase>. First of all, I would like to thank my <phrase>major</phrase> advisor, <phrase>Professor</phrase> Edward J. Delp. I thank him for taking me under his guidance. He has provided me with the opportunity to expand my learning horizon by becoming a <phrase>member</phrase> of the VIPER lab. I am grateful for his constant encouragement of <phrase>independent</phrase> thinking and learning through struggles. Moreover, I have enjoyed many non-<phrase>academic</phrase> related conversations with him, from which I have grew further respect for his broad and diverse <phrase>knowledge</phrase> of various topics. I would like to thank my other committee members, <phrase>Professor</phrase> Charles A. Bouman and <phrase>Professor</phrase> M. J. T. Smith, for their advice, guidance and criticism. I have enjoyed my learning experience in <phrase>Professor</phrase> Bouman's <phrase>Image Processing</phrase> classes and am grateful for his <phrase>patience</phrase> and insightful suggestions. I would like to thank the <phrase>Nokia</phrase> <phrase>Research</phrase> <phrase>Center</phrase> for their sponsorship on the <phrase>research</phrase> in this dissertation. Their generous support has made this dissertation possible. For the past year and a half I joined the VIPER lab, I have been lucky to be surrounded by my colleagues who are bright individuals capable of <phrase>critical thinking</phrase>. I appreciate the support and friendship of my <phrase>fellow</phrase> colleagues in the VIPER lab: I would like to especially thank Golnaz and Ka Ki for their great work on the temporal motion analysis of the texture-based <phrase>video</phrase> coding; to Limin, for her <phrase>consul</phrase>-iii tation on H.264 related topics; to Aravind, for his extraordinary <phrase>patience</phrase> in helping me with <phrase>software</phrase> <phrase>programming</phrase> and other computer related issues.
Extracting Temporal <phrase>Information</phrase> from <phrase>Electronic</phrase> Patient Records A method for automatic extraction of clinical temporal <phrase>information</phrase> would be of significant practical importance for deep <phrase>medical</phrase> <phrase>language</phrase> understanding, and a key to creating many successful applications, such as <phrase>medical</phrase> <phrase>decision making</phrase>, <phrase>medical</phrase> question and answering, etc. This <phrase>paper</phrase> proposes a rich <phrase>statistical model</phrase> for extracting temporal <phrase>information</phrase> from an extremely noisy clinical corpus. Besides the common <phrase>linguistic</phrase>, contextual and <phrase>semantic</phrase> features, the highly restricted training sample expansion and the structure distance between the temporal expression & related event expressions are also integrated into a supervised <phrase>machine-learning</phrase> approach. The learning method produces almost 80% F- score in the extraction of five temporal classes, and nearly 75% F-score in identifying temporally related events. This process has been integrated into the document-processing component of an implemented clinical <phrase>question answering</phrase> system that focuses on answering patient-specific questions (See demonstration at http://hitrl.cs.usyd.edu.au/ICNS/).
<phrase>Chinese</phrase>/<phrase>English</phrase> mixed Character Segmentation as <phrase>Semantic</phrase> Segmentation <phrase>OCR</phrase> character segmentation for multilingual printed documents is difficult due to the diversity of different <phrase>linguistic</phrase> characters. Previous approaches mainly focus on monolingual texts and are not suitable for multilingual-lingual cases. In this work, we particularly tackle the Chi-nese/<phrase>English</phrase> mixed case by reframing it as a <phrase>semantic</phrase> seg-mentation problem. We take advantage of the successful <phrase>architecture</phrase> called fully <phrase>convolutional networks</phrase> (FCN) in the field of <phrase>semantic</phrase> segmentation. Given a wide enough <phrase>receptive field</phrase>, FCN can utilize the necessary context around a horizontal position to determinate whether this is a splitting point or not. As a deep neural <phrase>architecture</phrase>, FCN can automatically learn useful features from raw text line images. Although trained on synthesized samples with simulated random disturbance, our FCN <phrase>model</phrase> generalizes well to <phrase>real-world</phrase> samples. The <phrase>experimental</phrase> <phrase>results</phrase> show that our <phrase>model</phrase> <phrase>significantly outperforms</phrase> the previous methods.
<phrase>Unsupervised feature learning</phrase> for bootleg detection using <phrase>deep learning</phrase> architectures The widespread <phrase>diffusion</phrase> of portable devices capable of capturing <phrase>high</phrase>-quality <phrase>multimedia</phrase> <phrase>data</phrase>, together with the rapid proliferation of <phrase>media</phrase> sharing platforms, has determined an incredible growth of <phrase>user-generated content</phrase> available online. Since it is hard to strictly regulate this trend, illegal <phrase>diffusion</phrase> of <phrase>copyrighted</phrase> material is often likely to occur. This is the case of audio bootlegs, i.e., concerts illegally recorded and redistributed by fans. In this <phrase>paper</phrase>, we propose a bootleg detector, with the aim of disambiguating between: i) bootlegs unofficially recorded; <phrase>ii</phrase>) <phrase>live</phrase> concerts officially published; iii) studio recordings from officially released albums. The <phrase>proposed method</phrase> is based on audio feature analysis and <phrase>machine learning</phrase> techniques. We exploit a <phrase>deep learning</phrase> <phrase>paradigm</phrase> to extract highly characterizing features from audio excerpts, and a supervised classifier for detection. The method is validated against a dataset of nearly 500 songs, and <phrase>results</phrase> are compared to a <phrase>state</phrase>-of-the-<phrase>art</phrase> detector. The conducted experiments confirm the capability of <phrase>deep learning</phrase> techniques to outperform classic <phrase>feature extraction</phrase> approaches.
<phrase>Digital</phrase> <phrase>Storytelling</phrase> <phrase>Research</phrase> <phrase>Design</phrase> If <phrase>Digital</phrase> <phrase>Storytelling</phrase> is to become accepted in today's schools, it will be important to collect <phrase>data</phrase> to be able to draw conclusions about the impact that the process has on <phrase>student</phrase> learning, <phrase>motivation</phrase> and engagement and how teaching practices and strategies change with <phrase>technology</phrase> integration through <phrase>digital</phrase> <phrase>storytelling</phrase>. This document outlines a potential study of the issues related to learning and reflection through <phrase>digital</phrase> <phrase>storytelling</phrase>. The <phrase>data</phrase> collected will provide <phrase>research</phrase>-based evidence on the effect <phrase>digital</phrase> <phrase>storytelling</phrase> has on <phrase>student</phrase> learning, <phrase>motivation</phrase>, and engagement. To that end we will seek to identify what conditions facilitate and encourage students to care about their work and be proud of it. Can we identify the conditions necessary to motivate students to reflect on their learning as a record of their growth over time and as a story of their learning? Some of the key <phrase>research</phrase> questions that will guide the study include: How do <phrase>digital</phrase> stories provide evidence of <phrase>deep learning</phrase>? Under what conditions can <phrase>digital</phrase> stories be successfully used to support assessment for learning? Under what conditions do students take ownership of their <phrase>digital</phrase> stories? What are the benefits of developing <phrase>digital</phrase> stories as perceived by students, teachers, administrators, and/or parents? What are perceived obstacles to implementing <phrase>digital</phrase> <phrase>storytelling</phrase> with <phrase>P-12</phrase> students and how can they be overcome? How does the quality of <phrase>paper</phrase>-based reflection differ from <phrase>digital</phrase> stories? <phrase>Data</phrase> could be generated through surveys, on-site observations, online discussions, and <phrase>journals</phrase>, as well as the aggregation of <phrase>student</phrase> performance-based assessment <phrase>data</phrase>. The <phrase>research</phrase> should draw upon the established <phrase>literature</phrase> and theoretical constructs with validated <phrase>research</phrase> instruments and <phrase>data</phrase> collection protocols. A more <phrase>comprehensive</phrase> <phrase>literature review</phrase> is included at the end of this document. These resources could include:
<phrase>Decision making</phrase> based on cohort scores for <phrase>speaker</phrase> verification <phrase>Decision making</phrase> is an important component in a <phrase>speaker</phrase> verification system. For the conventional GMM-UBM <phrase>architecture</phrase>, the decision is usually conducted based on the <phrase>log likelihood</phrase> ratio of the <phrase>test</phrase> utterance against the GMM of the claimed <phrase>speaker</phrase> and the UBM. This <phrase>single</phrase>-score decision is simple but tends to be sensitive to the complex variations in speech signals (e.g. text content, <phrase>channel</phrase>, speaking style, etc.). In this <phrase>paper</phrase>, we propose a <phrase>decision making</phrase> approach based on multiple scores derived from a set of cohort GMMs (cohort scores). Importantly, these cohort scores are not simply averaged as in conventional cohort methods; instead, we employ a powerful discriminative <phrase>model</phrase> as the decision maker. <phrase>Experimental</phrase> <phrase>results</phrase> show that the <phrase>proposed method</phrase> delivers substantial <phrase>performance improvement</phrase> over the baseline system, especially when a <phrase>deep neural network</phrase> (DNN) is used as the decision maker, and the DNN input involves some statistical features derived from the cohort scores. I. INTRODUCTION <phrase>Speaker</phrase> verification aims to verify claimed identities of speakers, and has gained great popularity in a wide <phrase>range</phrase> of applications including <phrase>access control</phrase>, <phrase>forensic</phrase> evidence provision and user <phrase>authentication</phrase>. After decades of <phrase>research</phrase>, lots of popular <phrase>speaker</phrase> verification approaches have been proposed , such as <phrase>Gaussian mixture</phrase> <phrase>model</phrase>-<phrase>universal</phrase> background <phrase>model</phrase> (GMM-UBM) [1], joint <phrase>factor analysis</phrase> (JFA) [2] and its 'simplified' version, the i-<phrase>vector</phrase> <phrase>model</phrase> [3]. Accompanied with these models, various back-end techniques have also been proposed to promote the discriminative capability for speakers, such as within-class <phrase>covariance</phrase> normalization (WCCN) [4], nuisance attribute projection (NAP) [5] and probabilistic LDA (PLDA) [6], etc. These methods have been demonstrated to be highly successful. Recently, <phrase>deep learning</phrase> has been applied to <phrase>speaker</phrase> verification and gained much interest [7], [8]. Within a <phrase>speaker</phrase> verification system, <phrase>decision making</phrase> is an important component [9]. To make a decision, the verification system first determines a score for the <phrase>test</phrase> utterance that reflects the confidence that the utterance is from the claimed <phrase>speaker</phrase>, and then compares the score with a predefined threshold. In a typical GMM-UBM system, the score is often computed as the <phrase>log likelihood</phrase> ratio that the <phrase>test</phrase> utterance being generated from the GMM of the claimed <phrase>speaker</phrase> and
Deep Image Homography Estimation We present a <phrase>deep convolutional</phrase> <phrase>neural network</phrase> for estimating the relative homography between a pair of images. Our <phrase>feed-forward</phrase> network has 10 layers, takes two stacked <phrase>grayscale</phrase> images as input, and produces an 8 <phrase>degree</phrase> of freedom homography which can be used to map the <phrase>pixels</phrase> from the first image to the second. We present two <phrase>convolutional neural network</phrase> architectures for HomographyNet: a <phrase>regression</phrase> network which directly estimates the <phrase>real-valued</phrase> homography parameters, and a classification network which produces a distribution over quantized homographies. We use <phrase>a 4</phrase>-point homography param-eterization which maps the four corners from one image into the second image. Our networks are trained in an <phrase>end-to-end</phrase> <phrase>fashion</phrase> using warped MS-COCO images. Our approach works without the need for separate local feature detection and transformation estimation stages. Our deep models are compared to a traditional homography <phrase>estimator</phrase> based on ORB features and we highlight the scenarios where HomographyNet outperforms the traditional technique. We also describe a <phrase>variety</phrase> of applications powered by deep homography estimation, thus showcasing the flexibility of a <phrase>deep learning</phrase> approach.
Modularity Based <phrase>Community</phrase> Detection with <phrase>Deep Learning</phrase> Identification of module or <phrase>community</phrase> structures is important for characterizing and understanding <phrase>complex systems</phrase>. While designed with different objectives , i.e., <phrase>stochastic</phrase> models for regeneration and modularity maximization models for <phrase>discrimination</phrase>, both these two types of <phrase>model</phrase> look for low-rank embedding to best represent and reconstruct <phrase>network topology</phrase>. However, the mapping through such embedding is linear, whereas real networks have various nonlinear features, making these models less effective in practice. Inspired by the strong representation power of <phrase>deep neural networks</phrase>, we propose a novel nonlinear <phrase>reconstruction</phrase> method by adopting <phrase>deep neural networks</phrase> for representation. We then extend the method to a <phrase>semi-supervised</phrase> <phrase>community</phrase> detection <phrase>algorithm</phrase> by incorporating pairwise constraints among <phrase>graph</phrase> nodes. Extensive <phrase>experimental</phrase> <phrase>results</phrase> on synthetic and real networks show that the new methods are effective, outperforming most <phrase>state</phrase>-of-the-<phrase>art</phrase> methods for <phrase>community</phrase> detection.
Teaching Secure <phrase>Data</phrase> Communications Using a <phrase>Game</phrase> Representation The <phrase>Security</phrase> Protocol <phrase>Game</phrase> is a highly visual and interactive <phrase>game</phrase> for teaching secure <phrase>data</phrase> <phrase>communication</phrase> protocols. Students use the <phrase>game</phrase> to simulate protocols and explore possible attacks against them. The power of the <phrase>game</phrase> lies in the representation of secret and <phrase>public key cryptography</phrase>. Specifically, the <phrase>game</phrase> provides representations for <phrase>plain text</phrase> and encrypted messages, message digests, <phrase>digital signatures</phrase> and <phrase>cryptographic</phrase> keys. Using these representations, students can construct <phrase>public</phrase> key certificates and perform multiple <phrase>encryption</phrase>, tunnelling and encrypted key transmission. They can simulate a wide <phrase>range</phrase> of protocols including <phrase>authentication</phrase>, key exchange and blind signature protocols. Application protocols such as <phrase>Transport Layer Security</phrase> and Pretty Good <phrase>Privacy</phrase> can be simulated in detail. The <phrase>game</phrase> clearly reveals the key issues of confidentiality, integrity, <phrase>authentication</phrase> and <phrase>non-repudiation</phrase> in secure <phrase>data</phrase> communications. Used as a small group learning activity, students gain a <phrase>deep understanding</phrase> of protocol <phrase>design</phrase> and operation issues. The <phrase>game</phrase> is suitable for use in <phrase>tertiary</phrase> and <phrase>professional</phrase> <phrase>education</phrase> courses for managers and <phrase>information technology</phrase> students at all levels.
<phrase>Semi-Supervised</phrase> Learning with <phrase>Deep Generative Models</phrase> The ever-increasing size of modern <phrase>data</phrase> sets combined with the difficulty of obtaining <phrase>label</phrase> <phrase>information</phrase> has made <phrase>semi-supervised</phrase> learning one of the problems of significant practical importance in modern <phrase>data analysis</phrase>. We revisit the approach to <phrase>semi-supervised</phrase> learning with <phrase>generative models</phrase> and develop new models that allow for effective generalisation from small labelled <phrase>data</phrase> sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that <phrase>deep generative models</phrase> and approximate <phrase>Bayesian inference</phrase> exploiting <phrase>recent advances</phrase> in variational methods can be used to provide <phrase>significant improvements</phrase>, making generative approaches highly competitive for <phrase>semi-supervised</phrase> learning.
Dense <phrase>Associative Memory</phrase> for <phrase>Pattern Recognition</phrase> A <phrase>model</phrase> of <phrase>associative memory</phrase> is studied, which stores and reliably retrieves many more patterns than the number of <phrase>neurons</phrase> in the network. We propose a simple duality between this dense <phrase>associative memory</phrase> and <phrase>neural networks</phrase> commonly used in <phrase>deep learning</phrase>. On the <phrase>associative memory</phrase> side of this duality, a <phrase>family</phrase> of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of <phrase>pattern recognition</phrase>, and the other one as the <phrase>prototype</phrase> regime. On the <phrase>deep learning</phrase> side of the duality, this <phrase>family</phrase> corresponds to <phrase>feedforward neural</phrase> networks with one <phrase>hidden layer</phrase> and various <phrase>activation functions</phrase>, which transmit the activities of the visible <phrase>neurons</phrase> to the <phrase>hidden layer</phrase>. This <phrase>family</phrase> of <phrase>activation functions</phrase> includes <phrase>logistics</phrase>, rectified linear units, and rectified <phrase>polynomials</phrase> of higher degrees. The proposed duality makes it possible to apply <phrase>energy</phrase>-based intuition from <phrase>associative memory</phrase> to analyze computational properties of <phrase>neural networks</phrase> with unusual <phrase>activation functions</phrase> the higher rectified <phrase>polynomials</phrase> which until now have not been used in <phrase>deep learning</phrase>. The utility of the dense memories is illustrated for two <phrase>test</phrase> cases: the logical gate XOR and the recognition of <phrase>handwritten digits</phrase> from the MNIST <phrase>data set</phrase>.
Learning Search Control <phrase>Knowledge</phrase> for <phrase>Deep Space Network</phrase> Scheduling While the <phrase>general</phrase> class of most scheduling problems is NPhard in worstcase complexity, in practice, for specific <phrase>distributions</phrase> of problems and constraints, domainspecific solutions have been shown to perform in much better than exponential time. Unfortunately, constructing such techniques is a knowledgeintensive and timeconsuming process that requires a <phrase>deep understanding</phrase> of the domain and the scheduler. The goal of our work is to develop techniques to allow for automated learning of an effective domainspecific search strategy given a <phrase>general</phrase> problem solver with a flexible control <phrase>architecture</phrase>. In this approach, a learning system searches a space of possible control strategies, using <phrase>statistics</phrase> to evaluate performance over the expected problem distribution. We discuss an application of the approach to scheduling <phrase>satellite communications</phrase>. Using problem <phrase>distributions</phrase> based on actual mission requirements, our approach identified strategies that both decrease the amount of <phrase>CPU</phrase> time required to produce schedules, and increase the percentage of problems that are solvable within computational resource limitations.
T Building Watson: an Overview of the Deepqa Project the Categories he goals of <phrase>IBM Research</phrase> are to advance <phrase>computer science</phrase> by exploring new ways for computer <phrase>technology</phrase> to affect <phrase>science</phrase>, <phrase>business</phrase>, and <phrase>society</phrase>. Roughly three <phrase>years ago</phrase>, <phrase>IBM Research</phrase> was looking for a <phrase>major</phrase> <phrase>research</phrase> challenge to rival the scientific and popular interest of <phrase>Deep Blue</phrase>, the computer <phrase>chess</phrase>-playing champion (Hsu 2002), that also would have clear relevance to <phrase>IBM</phrase> <phrase>business</phrase> interests. With a wealth of enterprise-critical <phrase>information</phrase> being captured in <phrase>natural language</phrase> documentation of all forms, the problems with perusing only the top 10 or 20 most popular documents containing the user's two or three key words are becoming increasingly apparent. This is especially the case in the enterprise where popularity is not as important an indicator of relevance and where recall can be as critical as precision. There is growing interest to have enterprise computer systems deeply analyze the breadth of relevant content to more precisely answer and justify answers to user's <phrase>natural language</phrase> questions. We believe advances in <phrase>question-answering</phrase> (QA) <phrase>technology</phrase> can help support professionals in critical and timely <phrase>decision making</phrase> in areas like compliance, <phrase>health care</phrase>, <phrase>business</phrase> integrity, <phrase>business intelligence</phrase>, <phrase>knowledge</phrase> discovery, enterprise <phrase>knowledge management</phrase>, <phrase>security</phrase>, and <phrase>customer support</phrase>. For I <phrase>IBM Research</phrase> undertook a challenge to build a computer system that could compete at the <phrase>human</phrase> champion level in real time on the <phrase>American</phrase> <phrase>TV</phrase> quiz show, <phrase>Jeopardy</phrase>. The extent of the challenge includes <phrase>fielding</phrase> a real-time automatic contestant on the show, not merely a <phrase>laboratory</phrase> exercise. The <phrase>Jeopardy</phrase> Challenge helped us address requirements that <phrase>led</phrase> to the <phrase>design</phrase> of the DeepQA <phrase>architecture</phrase> and the implementation of Watson. After three years of intense <phrase>research</phrase> and development by a core team of about 20 researchers, Watson is performing at <phrase>human</phrase> expert levels in terms of precision , confidence, and speed at the <phrase>Jeopardy</phrase> quiz show. Our <phrase>results</phrase> strongly suggest that DeepQA is an effective and extensible <phrase>architecture</phrase> that can be used as a foundation for combining , deploying, evaluating, and advancing a wide <phrase>range</phrase> of algorithmic techniques to rapidly advance the field of <phrase>question answering</phrase> (QA). researchers, the open-domain QA problem is attractive as it is one of the most challenging in the realm of <phrase>computer science</phrase> and <phrase>artificial intelligence</phrase> , requiring a synthesis of <phrase>information retrieval</phrase>, <phrase>natural language processing</phrase>, <phrase>knowledge representation and reasoning</phrase>, <phrase>machine learning</phrase>, and computer-<phrase>human</phrase> interfaces. It has had a <phrase>long</phrase> <phrase>history</phrase> (Simmons 1970) and saw rapid advancement spurred by system building, experimentation 
Learning <phrase>emotion</phrase>-based <phrase>acoustic</phrase> features with <phrase>deep belief</phrase> networks The medium of <phrase>music</phrase> has evolved specifically for the expression of emotions, and it is natural for us to organize <phrase>music</phrase> in terms of its emotional associations. But while such <phrase>organization</phrase> is a natural process for humans, quantifying it empirically proves to be a very difficult task, and as such no dominant <phrase>feature representation</phrase> for <phrase>music</phrase> <phrase>emotion</phrase> recognition has yet emerged. Much of the difficulty in developing <phrase>emotion</phrase>-based features is the ambiguity of the <phrase>ground-truth</phrase>. Even using the smallest time window, opinions on the <phrase>emotion</phrase> are bound to vary and reflect some disagreement between listeners. In previous work, we have modeled <phrase>human</phrase> response <phrase>labels</phrase> to <phrase>music</phrase> in the <phrase>arousal</phrase>-valence (A-V) representation of affect as a time-varying, <phrase>stochastic</phrase> distribution. Current methods for automatic detection of <phrase>emotion</phrase> in <phrase>music</phrase> seek performance increases by combining several feature domains (e.g. <phrase>loudness</phrase>, <phrase>timbre</phrase>, <phrase>harmony</phrase>, <phrase>rhythm</phrase>). Such work has focused largely in <phrase>dimensionality reduction</phrase> for minor classification performance gains, but has provided little insight into the relationship between audio and emotional associations. In this new work we seek to employ <phrase>regression</phrase>-based <phrase>deep belief</phrase> networks to learn features directly from <phrase>magnitude</phrase> spectra. While the system is applied to the specific problem of <phrase>music</phrase> <phrase>emotion</phrase> recognition, it could be easily applied to any <phrase>regression</phrase>-based audio <phrase>feature learning</phrase> problem.
Designing and Training <phrase>Feedforward Neural</phrase> Networks: A Smooth Optimisation Perspective Despite the recent <phrase>great success</phrase> of <phrase>deep neural networks</phrase> in various applications , designing and training a <phrase>deep neural network</phrase> is still among the greatest challenges in the field. In this work, we present a smooth optimisation perspective on designing and training multilayer <phrase>Feedforward Neural</phrase> Networks (FNNs) in the <phrase>supervised learning</phrase> setting. By characterising the critical point conditions of an FNN based optimisation problem, we identify the conditions to eliminate local <phrase>optima</phrase> of the corresponding cost <phrase>function</phrase>. Moreover, by studying the <phrase>Hessian</phrase> structure of the cost <phrase>function</phrase> at the global minima, we develop an approximate <phrase>Newton</phrase> FNN <phrase>algorithm</phrase>, which is capable of alleviating the vanishing <phrase>gradient</phrase> problem. Finally, our <phrase>results</phrase> are numerically verified on two classic benchmarks, i.e., the XOR problem and the four <phrase>region</phrase> <phrase>classification problem</phrase>.
LCNN: <phrase>Low-level</phrase> Feature Embedded <phrase>CNN</phrase> for Salient <phrase>Object Detection</phrase> In this <phrase>paper</phrase>, we propose a novel <phrase>deep neural network</phrase> framework embedded with <phrase>low-level</phrase> features (LCNN) for salient <phrase>object detection</phrase> in complex images. We utilise the advantage of <phrase>convolutional neural networks</phrase> to automatically learn the <phrase>high-level</phrase> features that capture the structured <phrase>information</phrase> and <phrase>semantic</phrase> context in the image. In <phrase>order</phrase> to better adapt a <phrase>CNN</phrase> <phrase>model</phrase> into the saliency task, we redesign the <phrase>network architecture</phrase> based on the <phrase>small-scale</phrase> datasets. Several <phrase>low-level</phrase> features are extracted, which can effectively capture contrast and spatial <phrase>information</phrase> in the salient regions, and incorporated to compensate with the learned <phrase>high-level</phrase> features at the output of the last <phrase>fully connected</phrase> layer. The concatenated feature <phrase>vector</phrase> is further fed into a hinge-loss <phrase>SVM</phrase> detector in a joint discriminative learning manner and the final saliency score of each <phrase>region</phrase> within the bounding box is obtained by the <phrase>linear combination</phrase> of the detector's weights. Experiments on three challenging benchmarks (MSRA-5000, PASCAL-S, ECCSD) demonstrate our <phrase>algorithm</phrase> to be effective and <phrase>superior</phrase> than most <phrase>low-level</phrase> oriented <phrase>state</phrase>-of-the-<phrase>arts</phrase> in terms of P-R curves, F-measure and mean absolute errors.
Transactional <phrase>Information</phrase> Systems: Theory, <phrase>Algorithms</phrase>, and the Practice of <phrase>Concurrency Control</phrase> and Recovery of the <phrase>book</phrase> The <phrase>book</phrase> gives both a <phrase>comprehensive</phrase> overview and an in-depth presentation of the field of transactional <phrase>data processing</phrase> covering the latest findings of the <phrase>research</phrase> <phrase>community</phrase> as well as practical experiences with <phrase>state</phrase>-of-the <phrase>art</phrase> <phrase>algorithms</phrase> and efficient implementation techniques. It reflects the advances of the field, particularly over the last decade. In the formal foundations, the authors emphasize a novel approach that unifies the discussion of <phrase>concurrency control</phrase> and recovery aspects. This is a significant improvement over traditional text books that <phrase>cover</phrase> the latter more on the level of imple-menation details rather than giving a solid formal background , integrated with the body of <phrase>concurrency control</phrase> theory. Also, their approach extends nicely from a standard page-oriented <phrase>model</phrase> to a multi-level, object <phrase>model</phrase> of <phrase>transaction processing</phrase>. The <phrase>book</phrase> develops the necessary formal background and does not require a deep <phrase>database</phrase> background. The authors argue that the scope of transaction <phrase>technology</phrase> is wider than <phrase>classical</phrase> <phrase>database systems</phrase>, aiming at new fields, such as today's <phrase>e-commerce</phrase> or <phrase>communication</phrase> systems. From time to time, the authors dive deeper into formal arguments, but the text always allows to skip details. The reader is further guided through the material by pairs of " Goal and Overview " " <phrase>Lessons Learned</phrase> " sections as well as illustrative examples, extraordinarily <phrase>comprehensive</phrase> bibliographic notes, and a suite of exercises of varying complexity in each chapter. The two constituents of transactional <phrase>technology</phrase>, con-<phrase>currency</phrase> control and recovery, are presented in the two core Parts <phrase>II</phrase> and III of the <phrase>book</phrase>. Part I starts out with a broad discussion of background and <phrase>motivation</phrase>, ending in the presentation of the two computational models for transactional processing, the page and the object <phrase>model</phrase>, and the definitions of the <phrase>basic</phrase> notions. Also, this part provides the necessary understanding of <phrase>database</phrase> system concepts for those readers not familiar with that material. Part IV is devoted to the coordination of distributed transactions and the last, Part V, gives an outlook on topics not <phrase>covered</phrase> in the <phrase>book</phrase>. The notions and techniques of <phrase>concurrency control</phrase> in Part <phrase>II</phrase> are presented in eight chapters (310). Chapter 3 introduces potential problems, the notions of schedules and histories, defines the several correctness criteria in terms of final <phrase>state</phrase>, view, conflict, and commit <phrase>serializability</phrase>, and elaborates on the (inclusion) relationships between the corresponding classes of histories. Next, <phrase>algorithms</phrase> for guaranteeing correct execution of concurrent transactions are given in 
Ensemble <phrase>Deep Learning</phrase> for Biomedical <phrase>Time Series</phrase> Classification Ensemble learning has been proved to improve the generalization ability effectively in both theory and practice. In this <phrase>paper</phrase>, we briefly outline the current status of <phrase>research</phrase> on it first. Then, a new <phrase>deep neural network</phrase>-based ensemble method that integrates filtering views, local views, distorted views, explicit training, implicit training, subview prediction, and Simple <phrase>Average</phrase> is proposed for biomedical <phrase>time series</phrase> classification. Finally, we validate its effectiveness on the <phrase>Chinese</phrase> <phrase>Cardiovascular Disease</phrase> <phrase>Database</phrase> containing a large number of <phrase>electrocardiogram</phrase> recordings. The <phrase>experimental</phrase> <phrase>results</phrase> show that the <phrase>proposed method</phrase> has certain advantages compared to some well-known ensemble methods, such as Bagging and AdaBoost.
3D Deeply Supervised Network for Automatic <phrase>Liver</phrase> Segmentation from CT Volumes Automatic <phrase>liver</phrase> segmentation from CT volumes is a crucial prerequisite yet <phrase>challenging task</phrase> for <phrase>computer-aided</phrase> <phrase>hepatic</phrase> <phrase>disease</phrase> diagnosis and treatment. In this <phrase>paper</phrase>, we present a novel 3D deeply supervised network (3D DSN) to address this <phrase>challenging task</phrase>. The proposed 3D DSN takes advantage of a fully convolutional <phrase>architecture</phrase> which performs efficient <phrase>end-to-end</phrase> learning and inference. More importantly, we introduce a deep supervision mechanism during the learning process to combat potential optimization difficulties, and thus the <phrase>model</phrase> can acquire a much faster convergence rate and more powerful <phrase>discrimination</phrase> capability. On top of the <phrase>high</phrase>-quality score map <phrase>produced</phrase> by the 3D DSN, a <phrase>conditional random field</phrase> <phrase>model</phrase> is further employed to obtain refined segmentation <phrase>results</phrase>. We evaluated our framework on the <phrase>public</phrase> MICCAI-SLiver07 dataset. <phrase>Extensive experiments</phrase> demonstrated that our <phrase>method achieves</phrase> competitive segmentation <phrase>results</phrase> to <phrase>state</phrase>-of-the-<phrase>art</phrase> approaches with a much faster processing speed.
Using <phrase>Deep Belief</phrase> Nets to Learn <phrase>Covariance</phrase> Kernels for Gaussian Processes We show how to use <phrase>unlabeled data</phrase> and a <phrase>deep belief</phrase> net (DBN) to learn a good <phrase>covariance</phrase> kernel for a <phrase>Gaussian process</phrase>. We first learn a deep <phrase>generative model</phrase> of the <phrase>unlabeled data</phrase> using the fast, <phrase>greedy algorithm</phrase> introduced by [7]. If the <phrase>data</phrase> is <phrase>high</phrase>-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both <phrase>regression</phrase> and classification can then be further improved by using <phrase>backpropagation</phrase> through the DBN to discriminatively fine-tune the <phrase>covariance</phrase> kernel.
Gearing up for <phrase>Robotics</phrase> An investigation into the acquisition of the concepts associated with gears by teachers in a constructionist <phrase>robotics</phrase> environment Abstract. This naturalistic, <phrase>qualitative research</phrase> investigated the extent and depth to which the perceived embedded educational skills and concepts associated with gears in a <phrase>robotics</phrase> environment were realized. The <phrase>research</phrase> <phrase>literature</phrase> revealed a paucity and confusion of description and definition of gears, and a lack of articulation of the embedded skills associated with gears. Based on some seminal <phrase>research</phrase> papers, a novel skills grid was developed as a <phrase>measuring instrument</phrase>, against which deep <phrase>mining</phrase> of the <phrase>knowledge</phrase> progress of the understanding of gears of four teachers was observed. The analysis of <phrase>results</phrase> pertaining to considered gear integration in the <phrase>construction</phrase> of a <phrase>robot</phrase> revealed that learning does not occur serendipitously and, unless taught overtly, the opportunities for the learning of gear concepts are often missed or deliberately bypassed. This lack of desire to learn about, and consider gear integration in constructions, could be attributed to confusion of interpretation of a non-contextualised ratio, or more deeply, the confusion of ratio representation in <phrase>rational number</phrase> and colon formats. <phrase>Robots</phrase> are rapidly becoming an <phrase>integral</phrase> component of everyday <phrase>life</phrase>. In <phrase>order</phrase> to facilitate an understanding of this future environment, the study of <phrase>robots</phrase>, known as <phrase>robotics</phrase>, has become a growing field of interest and learning. Historically situated in <phrase>information technology</phrase> courses, <phrase>robotics</phrase> has developed from an element of investigation in control <phrase>technology</phrase>, to a compulsory, selected, or elective subject in schools at all levels. Over the last two decades the inherent complexity in the
Mixed-<phrase>project-based learning</phrase> methodology in computer <phrase>electronic engineering</phrase> In Computer and <phrase>Electronic Engineering</phrase> teaching, to introduce the <phrase>student</phrase> in deep practical skills for quality while maintaining a good output ratio has become one of the most difficult tasks of the <phrase>professor</phrase>. This <phrase>paper</phrase> resumes the scheduled objectives and the <phrase>results</phrase> obtained using the Mixed-<phrase>Project-Based Learning</phrase> methodology in a third year <phrase>Digital</phrase> <phrase>Systems Design</phrase> course in <phrase>Computer Science</phrase> Electrical <phrase>Bachelor</phrase> <phrase>Engineering</phrase> studies. The methodology combines the <phrase>Project-Based Learning</phrase> with the traditional learning methodologies in <phrase>order</phrase> to enhance the <phrase>results</phrase> in a <phrase>short</phrase>-time extensive-subject course.
Temporal Autoencoding <phrase>Restricted Boltzmann Machine</phrase> Much work has been done refining and characterizing the <phrase>receptive fields</phrase> learned by <phrase>deep learning</phrase> <phrase>algorithms</phrase>. A lot of this work has focused on the development of Gabor-like filters learned when enforcing sparsity constraints on a natural image dataset. Little work however has investigated how these filters might expand to the temporal domain, namely through training on natural movies. Here we investigate exactly this problem in established temporal <phrase>deep learning</phrase> <phrase>algorithms</phrase> as well as a new learning <phrase>paradigm</phrase> suggested here, the Temporal Autoencoding <phrase>Restricted Boltzmann Machine</phrase> (TARBM).
Training <phrase>restricted Boltzmann machines</phrase>: An introduction <phrase>Restricted Boltzmann machines</phrase> (RBMs) are probabilistic <phrase>graphical</phrase> models that can be interpreted as <phrase>stochastic</phrase> <phrase>neural networks</phrase>. They have attracted much attention as <phrase>building blocks</phrase> for the <phrase>multi-layer</phrase> learning systems called <phrase>deep belief</phrase> networks, and variants and extensions of RBMs have found application in a wide <phrase>range</phrase> of <phrase>pattern recognition</phrase> tasks. This tutorial introduces RBMs from the viewpoint of <phrase>Markov</phrase> <phrase>random fields</phrase>, starting with the required concepts of undirected <phrase>graphical</phrase> models. Different <phrase>learning algorithms</phrase> for RBMs, including contrastive divergence learning and parallel tempering, are discussed. As sampling from RBMs, and therefore also most of their <phrase>learning algorithms</phrase>, are based on <phrase>Markov chain Monte Carlo</phrase> (MCMC) methods, an introduction to <phrase>Markov</phrase> chains and MCMC techniques is provided. Experiments demonstrate relevant aspects of RBM training.
Culturalhistorical <phrase>Activity Theory</phrase> the Theoretical Framework A <phrase>long</phrase>-standing challenge in educational <phrase>research</phrase> is to describe and explain the <phrase>complex dynamics</phrase> of learning and development that occur in educational settings. This article summarizes ways in which qualitative methods are essential to this enterprise from the perspective of scholars who approach the issues using the theoretical <phrase>lens</phrase> of culturalhistorical <phrase>activity theory</phrase> (CHAT). After summarizing <phrase>basic</phrase> principles of this theoretical approach we provide four examples involving different levels of analysis and methodologies. (Methodology is used to refer to the ensemble of methods that mediate between theoretical statements and <phrase>data</phrase> used to evaluate them.) To some researchers who employ qualitative methods , the very fact that we enter into this topic guided by a theoretical framework disqualifies our claim to be qualitative researchers. Smith argues that ''qualitative approaches in <phrase>psychology</phrase> are generally engaged with exploring, describing and interpreting the personal and social experiences of participants. An attempt is usually made to understand a small number of participants' own frames of reference or view of the world rather than trying to <phrase>test</phrase> a preconceived <phrase>hypothesis</phrase> on a large sample'' (Smith, 2003: 2). Our approach involves small samples, and we are interested in participants' own understandings; however, we do operate from a preconceived theoretical base and in that sense we have preconceived hypotheses. Moreover, the approach we espouse does not preclude quantifica-tion. However, such quantification is more likely to be used for purposes of comparative analysis of qualitatively different activities (Cole et al., 1978) or summary evaluations of <phrase>products</phrase> than for a deep analysis of the process of change (cf., Hayes, 1997). CHAT refers to an interdisciplinary approach to studying <phrase>human</phrase> learning and development associated with the names of the <phrase>Soviet Russian</phrase> <phrase>psychologists</phrase>, L.There has been a lively debate in recent years about the extent to which these three thinkers represent a <phrase>single</phrase> theoretical perspective. According to one line of interpretation, those who follow <phrase>Vygotsky</phrase> have focused attention on processes of mediation , adopting mediated <phrase>action</phrase> in context as a <phrase>basic</phrase> unit of analysis (Wertsch et al., 1995). This line of work is often referred to as sociocultural <phrase>research</phrase>. By contrast, followers of Leontiev are said to choose activity as a <phrase>basic</phrase> unit of analysis (Kaptelinin, 1996). For our present purposes, these distinctions are not central and we will treat the differing formulations as expressions of a <phrase>single</phrase> <phrase>family</phrase> of theoretical commitments.) The following are some theoretical principles of this approach: 1. 
Beyond the Limits of Planning Theory: Response to My Critics 'Little Things' that Re-enchant the World I want to thank the editors for giving me this opportunity to respond to the comments on <phrase>Rationality</phrase> and Power made above what follows I shall give my response to their criticisms. I shall focus less on the many positive things they also say about the <phrase>book</phrase>. First let me mention, however, that I was particularly happy to learn that the critics are favourable to the depth of detail in the book's <phrase>case-study</phrase> of planning in <phrase>Aalborg</phrase>. This is especially important to me, because during the years when I was working in the archives, doing interviews, making observations, talking with my informants, etc. a nagging question kept resurfacing in my mind. This is a question bound to haunt many carrying out what Peattie calls 'dense <phrase>data</phrase> <phrase>case-studies</phrase>': 'Who will want to learn about a case like this, and in this kind of detail?' I wanted the <phrase>case-study</phrase> to be particularly dense because I wished to <phrase>test</phrase> the <phrase>thesis</phrase> that the most interesting phenomena in planning and policy making, and those of most <phrase>general</phrase> import, would be found in the most minute and most <phrase>concrete</phrase> of details. Or to put the <phrase>matter</phrase> differently, I wanted to see whether the dualisms generalspecic and abstractconcrete would <phrase>metamorphose</phrase> or vanish if I went into sufciently deep detail. Following <phrase>Dewey</phrase>, Rorty has perceptively observed that the way to re-enchant the world is to stick to the <phrase>concrete</phrase>. <phrase>Nietzsche</phrase> similarly advocates a focus on 'little things' if we are to understand the problems of <phrase>politics</phrase> and social <phrase>organization</phrase>, which, needless to say, include problems of planning. Both Rorty and <phrase>Nietzsche</phrase> seem right to me. I saw the <phrase>Aalborg</phrase> case as being made up of the type of <phrase>concrete</phrase>, little things they <phrase>talk</phrase> about. Indeed, I saw the case itself as such a thing, what <phrase>Nietzsche</phrase> calls a discreet and apparently insignicant truth, which, when closely examined, would reveal itself to be pregnant with paradigms, <phrase>metaphors</phrase> and <phrase>general</phrase> signicance. That was my <phrase>thesis</phrase>, but theses may be wrong and the study could have fallen at on its face. This has not happened, and it is especially satisfying to me to see that this particular aspectthe focus on 'little things'is emphasized by many reviewers, including the present ones, as a strength of the <phrase>Aalborg</phrase> study.
<phrase>Bilinear</phrase> <phrase>deep learning</phrase> for <phrase>image classification</phrase> <phrase>Image classification</phrase> is a well-known <phrase>classical</phrase> problem in <phrase>multimedia</phrase> <phrase>content analysis</phrase>. This <phrase>paper</phrase> proposes a novel <phrase>deep learning</phrase> <phrase>model</phrase> called <phrase>bilinear</phrase> <phrase>deep belief</phrase> network (BDBN) for <phrase>image classification</phrase>. Unlike previous <phrase>image classification</phrase> models, BDBN aims to provide <phrase>human</phrase>-like judgment by referencing the <phrase>architecture</phrase> of the <phrase>human</phrase> visual system and the procedure of intelligent <phrase>perception</phrase>. Therefore, the <phrase>multi-layer</phrase> structure of the <phrase>cortex</phrase> and the propagation of <phrase>information</phrase> in the visual areas of the <phrase>brain</phrase> are realized faithfully. Unlike most existing deep models, BDBN utilizes a <phrase>bilinear</phrase> <phrase>discriminant</phrase> strategy to simulate the "initial guess" in <phrase>human</phrase> <phrase>object recognition</phrase>, and at the same time to avoid falling into a bad local optimum. To preserve the natural <phrase>tensor</phrase> structure of the image <phrase>data</phrase>, a novel <phrase>deep architecture</phrase> with <phrase>greedy layer-wise</phrase> <phrase>reconstruction</phrase> and global <phrase>fine-tuning</phrase> is proposed. To adapt <phrase>real-world</phrase> <phrase>image classification</phrase> tasks, we develop BDBN under a <phrase>semi-supervised</phrase> learning framework, which makes the deep <phrase>model</phrase> work well when labeled images are insufficient. Comparative experiments on three standard datasets show that the <phrase>proposed algorithm</phrase> outperforms both representative classification models and existing <phrase>deep learning</phrase> techniques. More interestingly, our demonstrations show that the proposed BDBN works consistently with the <phrase>visual perception</phrase> of humans.
Learning Complex Similarity Measures 1.1 Classification and <phrase>Case-based Reasoning</phrase> 1.2 from a Case Base to a Similarity Measure <phrase>Case-based reasoning</phrase> is a <phrase>knowledge</phrase> processing concept that has shown success in various problem classes. One key challenge in CBR is the <phrase>construction</phrase> of a measure that adequately models the similarity between two cases. Typically, a similarity measure consists of a set of feature-specific distance functions coupled with an underlying feature weighting (importance) scheme. While the definition of the distance functions is often straightforward, the estimation of the weighting scheme requires a <phrase>deep understanding</phrase> of the domain and the underlying connections. The <phrase>paper</phrase> in hand addresses this problem. It shows how <phrase>discrimination</phrase> <phrase>knowledge</phrase>, which is coded within an already solved <phrase>classification problem</phrase>, can be transformed towards a similarity measure. Moreover, it demonstrates our approach at the problem of diagnosing <phrase>heart</phrase> diseases. 1 Background and Related Theory <phrase>Discrimination</phrase> <phrase>knowledge</phrase> that is coded within an already solved <phrase>classification problem</phrase> can be transformed towards a similarity measure of a <phrase>case-based reasoning</phrase> (CBR) system. This chapter first points out relationships between classification and similarity assessment in a <phrase>case-based reasoning</phrase> system. It then motivates and defines a generic transformation procedure from a case base to a similarity measure; the last two sections of this chapter discuss related realizational aspects. Chapter 2 presents an application, the diagnosis of <phrase>heart</phrase> diseases, to demonstrate the development of a similarity measure at a <phrase>real-world</phrase> problem. Let x denote a problem or some description of a situation. Then a common task is to find another problem y amongst a set S of problems, such that y is more similar to x than it is to any other z S. Using the terminology of <phrase>case-based reasoning</phrase>, we are given a pair <phrase>CB</phrase>, sim, where <phrase>CB</phrase>, the case base, denotes a set of cases, and sim denotes a similarity measure, sim : <phrase>CB</phrase> <phrase>CB</phrase> [0, 1]. With x, y, and z <phrase>CB</phrase> the <phrase>semantics</phrase> of sim is as follows. sim(x, y) > sim(x, z) " x is more similar to y than it is to z. " 255 A case x <phrase>CB</phrase> usually embodies both a problem description and a related <phrase>solution</phrase>. A <phrase>basic</phrase> idea of CBR is to exploit previously solved cases when solving a new problem. I. <phrase>e</phrase>., the collection of cases, <phrase>CB</phrase>, is browsed for the most similar case, whose <phrase>solution</phrase> then is adapted to solve the new problem. However, within a classification task one is not interested in case adaptation: A case's 
<phrase>Machine Learning</phrase> of <phrase>Hybrid</phrase> Classification Models for <phrase>Decision Support</phrase> <phrase>Machine learning</phrase> methods used for <phrase>decision support</phrase> must achieve (a) <phrase>high</phrase> accuracy of decisions they recommend, and (b) <phrase>deep understanding</phrase> of decisions, so decision makers could trust them. Methods for learning implicit, non-symbolic <phrase>knowledge</phrase> provide better predictive accuracy. Methods for learning explicit, symbolic <phrase>knowledge</phrase> produce more comprehensible models. <phrase>Hybrid</phrase> <phrase>machine learning</phrase> models combine strengths of both <phrase>knowledge representation</phrase> <phrase>model</phrase> types. In this <phrase>paper</phrase> we compare predictive accuracy and comprehensibility of explicit, implicit, and <phrase>hybrid</phrase> <phrase>machine learning</phrase> models for several standard <phrase>medical</phrase> diagnostics, <phrase>electronic commerce</phrase>, <phrase>e</phrase>-<phrase>marketing</phrase> and financial <phrase>decision making</phrase> problems. Their applicability in different environments-desktop, <phrase>mobile</phrase> and <phrase>cloud computing</phrase> is briefly analyzed. <phrase>Machine learning</phrase> methods from <phrase>Weka</phrase> and R/<phrase>Revolution</phrase> environments are used.
A Synchronous <phrase>High</phrase>-Speed, <phrase>High</phrase>-Accuracy, Loser-Take-All Circui I. Abstract This <phrase>paper</phrase> presents an accurate current mode synchronous Loser-Take-All circuit based on a simple regenerative pair. The regenerative pair and the related resetting <phrase>transistors</phrase> <phrase>lead</phrase> to a circuit requiring only four <phrase>transistors</phrase>, seven for deep <phrase>binary tree</phrase> structures. It achieves its <phrase>high</phrase> speed through regenerative <phrase>feedback</phrase>. The <phrase>basic</phrase> circuit requires no current mirrors and has only parasitic losses, resulting in <phrase>high</phrase> accuracy. This <phrase>paper</phrase> also includes <phrase>simulation</phrase> <phrase>results</phrase> for a <phrase>single</phrase> <phrase>LTA</phrase> circuit with two inputs and for a sixty-four input, six-layer, <phrase>binary tree</phrase> circuit. The MAX or Winner-Take-All (WTA) and MIN or Loser-Take-All (<phrase>LTA</phrase>) circuits are important parts of many neuro-fuzzy systems. MIN and MAX circuits are the prime components of <phrase>fuzzy logic</phrase>. WTA circuits are used in many neural layer architectures such as Learning <phrase>Vector Quantization</phrase> (LVQ), Adaptive <phrase>Resonance</phrase> Theory (<phrase>ART</phrase>), Kohonen feature maps, and many others. Gnay and Snchez-Sinencio [1] provide a detailed overview and comparison of several <phrase>CMOS</phrase> WTA circuits. However, this <phrase>paper</phrase> limits its consideration to asynchronous circuits nor does it mention several newer concepts [2], [3], [4]. Asynchronous WTA circuits can find the winner as the signals change due to their ability to process and compare signals in continuous mode. On the other hand, these circuits tend to have rather limited accuracy for multiple inputs due to signal interaction and the difficulty in matching <phrase>transistors</phrase> which are not near to each other. Usually, one must limit the number of inputs to ten or fewer to obtain acceptable resolution. Some applications, such as LVQ for graphics compression, require selecting the winner out of hundreds of signals. Demosthenous, et al., [5] developed a <phrase>binary tree</phrase> approach that solves this problem with a circuit using three current mirrors and one latch per synchronous WTA stage. This leads to a complex circuit with sixteen <phrase>transistors</phrase> per <phrase>cell</phrase>, twelve when eliminating one current <phrase>mirror</phrase> by using complimentary <phrase>design</phrase> techniques, and slower speeds. This <phrase>paper</phrase> proposes an <phrase>alternative</phrase> approach. Take the DeMorgan transform [2] of the inputs, then compare them using synchronous <phrase>LTA</phrase> circuits. The <phrase>basic</phrase> circuit presented here uses a regenerative pair as the <phrase>LTA</phrase> network, thus it requires only four <phrase>transistors</phrase>, two for the regenerative pair and two to reset the circuit. Its <phrase>design</phrase> eliminates the need for current mirrors which <phrase>results</phrase> in improved speed and accuracy when compared against the WTA approach. It also has the advantage that it does not require extensive circuitry to 
Assessing the contribution of shallow and deep <phrase>knowledge</phrase> sources for <phrase>word sense disambiguation</phrase> (2010) Assessing the contribution of shallow and deep <phrase>knowledge</phrase> sources for <phrase>word sense disambiguation</phrase>, Abstract. Corpus-based techniques have proved to be very beneficial in the development of efficient and accurate approaches to <phrase>word sense disambiguation</phrase> (WSD) despite the fact that they generally represent relatively shallow <phrase>knowledge</phrase>. It has always been thought, however, that WSD could also benefit from deeper <phrase>knowledge</phrase> sources. We describe a novel approach to WSD using <phrase>inductive logic programming</phrase> to learn theories from <phrase>first-order logic</phrase> representations that allows corpus-based evidence to be combined with any kind of <phrase>background knowledge</phrase>. This approach has been shown to be effective over several disambiguation tasks using a combination of deep and shallow <phrase>knowledge</phrase> sources. Is it important to understand the contribution of the various <phrase>knowledge</phrase> sources used in such a system. This <phrase>paper</phrase> investigates the contribution of nine <phrase>knowledge</phrase> sources to the performance of the disambiguation models <phrase>produced</phrase> for the SemEval-2007 <phrase>English</phrase> lexical sample task. The outcome of this analysis will assist future work on WSD in concentrating on the most useful <phrase>knowledge</phrase> sources.
<phrase>Knowledge Transfer</phrase> in <phrase>Deep convolutional</phrase> <phrase>Neural Nets</phrase> <phrase>Knowledge transfer</phrase> is widely held to be a primary mechanism that enables humans to quickly learn new complex concepts when given only small <phrase>training sets</phrase>. In this <phrase>paper</phrase>, we apply <phrase>knowledge transfer</phrase> to deep con-volutional <phrase>neural nets</phrase>, which we argue are particularly well suited for <phrase>knowledge transfer</phrase>. Our initial <phrase>results</phrase> demonstrate that components of a trained deep convolu-tional <phrase>neural net</phrase> can constructively transfer <phrase>information</phrase> to another such net. Furthermore, this transfer is completed in such a way that one can envision creating a net that could learn new concepts throughout its <phrase>lifetime</phrase>.
A <phrase>Generative Model</phrase> for Multi-<phrase>Dialect</phrase> Representation In the <phrase>era</phrase> of <phrase>deep learning</phrase> several unsupervised models have been developed to capture the key features in unlabeled handwritten <phrase>data</phrase>. Popular among them is the <phrase>Restricted Boltzmann Machines</phrase> (RBM). However, due to the novelty in handwritten multi-<phrase>dialect</phrase> <phrase>data</phrase>, the RBM may fail to generate an efficient representation. In this <phrase>paper</phrase> we propose a <phrase>generative model</phrase> the Mode Synthesizing Machine (<phrase>MSM</phrase>) for on-line representation of <phrase>real life</phrase> handwritten multi-<phrase>dialect</phrase> <phrase>language</phrase> <phrase>data</phrase>. The <phrase>MSM</phrase> takes advantage of the hierarchical representation of the modes of a <phrase>data</phrase> distribution using a two-point error update to learn a <phrase>sequence</phrase> of representative multi-<phrase>dialects</phrase> in a generative way. Experiments were performed to evaluate the performance of the <phrase>MSM</phrase> over the RBM with the former attaining much <phrase>lower</phrase> error values than the latter on both <phrase>independent</phrase> and mixed <phrase>data set</phrase>.
Learn to Swing Up and Balance a Real Pole Based on Raw Visual <phrase>Input Data</phrase> For the challenging pole balancing task we propose a system which uses raw visual <phrase>input data</phrase> for <phrase>reinforcement learning</phrase> to evolve a control strategy. Therefore we use a <phrase>neural network</phrase> a deep autoencoder to encode the <phrase>camera</phrase> images and thus the system states in a <phrase>low dimensional</phrase> <phrase>feature space</phrase>. The system is compared to controllers that work directly on the motor <phrase>sensor</phrase> <phrase>data</phrase>. We show that the performances of both systems are settled in the same <phrase>order</phrase> of <phrase>magnitude</phrase>.
Winner-Take-All Autoencoders In this <phrase>paper</phrase>, we propose a winner-take-all method for learning hierarchical sparse representations in an unsupervised <phrase>fashion</phrase>. We first introduce <phrase>fully-connected</phrase> winner-take-all autoencoders which use <phrase>mini</phrase>-batch <phrase>statistics</phrase> to directly enforce a <phrase>lifetime</phrase> sparsity in the activations of the <phrase>hidden units</phrase>. We then propose the convo-lutional winner-take-all autoencoder which combines the benefits of convolutional architectures and autoencoders for learning shift-invariant sparse representations. We describe a way to <phrase>train</phrase> convolutional autoencoders <phrase>layer by layer</phrase>, where in addition to <phrase>lifetime</phrase> sparsity, a spatial sparsity within each feature map is achieved using winner-take-all <phrase>activation functions</phrase>. We will show that winner-take-all <phrase>au</phrase>-toencoders can be used to to learn deep sparse representations from the MNIST, CIFAR-10, ImageNet, Street View <phrase>House</phrase> Numbers and <phrase>Toronto</phrase> Face datasets, and achieve competitive classification performance.
<phrase>Recurrent Neural Networks</phrase> for Multivariate <phrase>Time Series</phrase> with Missing Values Many multivariate <phrase>time series</phrase> <phrase>data</phrase> in practical applications, such as <phrase>health care</phrase>, geoscience, and <phrase>biology</phrase>, are characterized by a <phrase>variety</phrase> of missing values. It has been noted that the missing patterns and values are often correlated with the <phrase>target</phrase> <phrase>labels</phrase>, a.k.a., missingness is informative, and there is significant interest to explore methods which <phrase>model</phrase> them for <phrase>time series</phrase> prediction and other related tasks. In this <phrase>paper</phrase>, we develop novel <phrase>deep learning</phrase> models based on Gated Recurrent Units (<phrase>GRU</phrase>), a <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>recurrent neural network</phrase>, to handle missing observations. Our <phrase>model</phrase> takes two representations of missing patterns, i.e., masking and time duration, and effectively incorporates them into a deep <phrase>model</phrase> <phrase>architecture</phrase> so that it not only captures the <phrase>long</phrase>-term temporal dependencies in <phrase>time series</phrase>, but also utilizes the missing patterns to improve the prediction <phrase>results</phrase>. Experiments of <phrase>time series</phrase> <phrase>classification tasks</phrase> on <phrase>real-world</phrase> clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve <phrase>state</phrase>-of-<phrase>art</phrase> performance on these tasks and provide useful insights for <phrase>time series</phrase> with missing values.
Learning as MAP Inference in Discrete <phrase>Graphical</phrase> Models We present a new formulation for <phrase>binary classification</phrase>. Instead of relying on convex losses and regularizers such as in SVMs, <phrase>logistic regression</phrase> and boosting, or instead non-convex but continuous formulations such as those encountered in <phrase>neural networks</phrase> and <phrase>deep belief</phrase> networks, our framework entails a non-convex but discrete formulation, where estimation amounts to finding a MAP configuration in a <phrase>graphical model</phrase> whose potential functions are <phrase>low-dimensional</phrase> discrete <phrase>surrogates</phrase> for the misclassification loss. We argue that such a discrete formulation can naturally account for a number of issues that are typically encountered in either the convex or the continuous non-convex approaches, or both. By reducing the learning problem to a MAP inference problem, we can immediately translate the guarantees available for many inference settings to the learning problem itself. We empirically demonstrate in a number of experiments that this approach is promising in dealing with issues such as severe <phrase>label</phrase> noise, while still having global optimality guarantees. Due to the discrete <phrase>nature</phrase> of the formulation , it also allows for direct regularization through <phrase>cardinality</phrase>-based penalties, such as the 0 pseudo-norm, thus providing the ability to perform <phrase>feature selection</phrase> and <phrase>trade</phrase>-off interpretability and predictability in a prin-cipled manner. We also outline a number of open problems arising from the formulation.
Asynchronous Participatory Exams: <phrase>Internet</phrase> <phrase>Innovation</phrase> for Engaging Students I nternet technologies enable us to rethink traditional teaching approaches , which opens the <phrase>flood</phrase>-gates to educational <phrase>innovation</phrase>. Why do exams have to be on <phrase>paper</phrase>? Why can't we ensure the integrity of online exams? Why can't students collaborate on exams, so they can learn from each other? And why can't students make up the exams and grade them, so they can learn through the entire process? Inter-net learning technologies let us explore these questions and, inevitably, can liberate students and instructors from traditional roles and restrictions to increase <phrase>higher-order</phrase> learning. We developed the Asynchronous Learning Networks Participatory Examination approach (or APE) utilizing <phrase>Internet</phrase> technologies to actively engage students online in the entire exam <phrase>life</phrase> cycle, which includes letting them create, answer, grade, and dispute exam questions. Our framework allows instructors to become mentors or facili-tators instead of the dominant <phrase>center</phrase> of attention, as in a traditional class. This process, in turn, could empower students with deep or <phrase>higher-order</phrase> thinking and learning processes. Based on this theory, we conducted an APE <phrase>case study</phrase> with 240 graduate students in an " <phrase>Information</phrase> Systems Principles " class for five semesters. Our study <phrase>results</phrase> show that APE is an effective and innovative online assessment approach, which could be widely adopted in various online and on-cam-The Asynchronous Learning Networks Participatory Examination (APE) is a constructivist approach that fully engages students in the entire exam <phrase>life</phrase> cycle. Students <phrase>design</phrase> and solve exam questions while evaluating their <phrase>peers</phrase>' solutions using an anonymous, structured process enabled by <phrase>Internet</phrase> technologies. APE achieves <phrase>higher-level</phrase> learning by encouraging students to tap into all levels of <phrase>cognitive</phrase> skills. Compared to the traditional exam experiences in most classes, most students prefer APE, enjoy its process, and recommend its use. APE liberates both students and instructors by reengineering the examination process and deepening learning throughout.
<phrase>Data Mining</phrase> Using Surface and Deep Agents Based on <phrase>Neural Networks</phrase> This <phrase>paper</phrase> presents an approach to <phrase>data mining</phrase> based on an <phrase>architecture</phrase> that uses two kinds of <phrase>neural network</phrase>-based agents: (i) an instantaneously-trained surface learning agent that quickly adapts to new modes of operation; and, (<phrase>ii</phrase>) a <phrase>deep learning</phrase> agent that is very accurate within a specific regime of operation. The two agents perform complementary functions that improve the overall performance. The performance of the <phrase>hybrid</phrase> <phrase>architecture</phrase> has been compared with that of a back-propagation network for a <phrase>variety</phrase> of classification problems and found to be <phrase>superior</phrase> based on the RMS error criterion.
Scaling Distributed <phrase>Machine Learning</phrase> with System and <phrase>Algorithm</phrase> Co-<phrase>design</phrase> For a lot of important <phrase>machine learning</phrase> problems, due to the rapid growth of <phrase>data</phrase> and the ever increasing <phrase>model</phrase> complexity, which often manifests itself in the large number of <phrase>model</phrase> parameters, no <phrase>single</phrase> machine can solve them fast enough. Therefore, distributed optimization and inference is becoming more and more inevitable for solving <phrase>large scale</phrase> <phrase>machine learning</phrase> problems in both <phrase>academia</phrase> and <phrase>industry</phrase>. Obtaining an efficient distributed implementation of an <phrase>algorithm</phrase>, however, is far from trivial. Both intensive computational workloads and the volume of <phrase>data</phrase> <phrase>communication</phrase> demand careful <phrase>design</phrase> of distributed computation systems and distributed <phrase>machine learning</phrase> <phrase>algorithms</phrase>. In this <phrase>thesis</phrase>, we focus on the co-<phrase>design</phrase> of <phrase>distributed computing</phrase> systems and distributed optimization <phrase>algorithms</phrase> that are specialized for large <phrase>machine learning</phrase> problems. We propose two <phrase>distributed computing</phrase> frameworks: a parameter server framework which features efficient <phrase>data</phrase> <phrase>communication</phrase>, and MXNet, a multi-<phrase>language</phrase> <phrase>library</phrase> aiming to simplify the development of <phrase>deep neural network</phrase> <phrase>algorithms</phrase>. In less than two years, we have witnessed the wide <phrase>adoption</phrase> of the proposed systems. We believe that as we continue to develop these systems, they will enable more people to take advantage of the power of <phrase>distributed computing</phrase> to <phrase>design</phrase> efficient <phrase>machine learning</phrase> applications to solve <phrase>large-scale</phrase> computational problems. Leveraging the two <phrase>computing</phrase> platforms, we examine a number of distributed <phrase>optimization problems</phrase> in <phrase>machine learning</phrase>. We present new methods to accelerate the training process, such as <phrase>data</phrase> partitioning with better <phrase>locality</phrase> properties, <phrase>communication</phrase> <phrase>friendly</phrase> optimization methods, and more compact <phrase>statistical models</phrase>. We implement the new <phrase>algorithms</phrase> on the two systems and <phrase>test</phrase> on <phrase>large scale</phrase> real <phrase>data</phrase> sets. We successfully demonstrate that careful co-<phrase>design</phrase> of <phrase>computing</phrase> systems and <phrase>learning algorithms</phrase> can greatly accelerate <phrase>large scale</phrase> distributed <phrase>machine learning</phrase>.
Learning to reinforcement learn In recent years <phrase>deep reinforcement learning</phrase> (<phrase>RL</phrase>) systems have attained superhuman performance in a number of <phrase>challenging task</phrase> domains. However, a <phrase>major</phrase> limitation of such applications is their demand for massive amounts of <phrase>training data</phrase>. A critical present objective is thus to develop deep <phrase>RL</phrase> methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-<phrase>reinforcement learning</phrase>. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the <phrase>RL</phrase> setting. What emerges is a system that is trained using one <phrase>RL</phrase> <phrase>algorithm</phrase>, but whose recurrent dynamics implement a second, quite separate <phrase>RL</phrase> procedure. This second, learned <phrase>RL</phrase> <phrase>algorithm</phrase> can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven <phrase>proof-of-concept</phrase> experiments, each of which examines a key <phrase>aspect</phrase> of deep meta-<phrase>RL</phrase>. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for <phrase>neuroscience</phrase>.
Optimizing <phrase>FPGA</phrase>-based <phrase>Accelerator</phrase> <phrase>Design</phrase> for <phrase>Deep Convolutional</phrase> <phrase>Neural Networks</phrase> <phrase>Convolutional neural network</phrase> (<phrase>CNN</phrase>) has been widely employed for <phrase>image recognition</phrase> because it can achieve <phrase>high</phrase> accuracy by emulating behavior of optic nerves in living creatures. Recently, rapid growth of modern applications based on <phrase>deep learning</phrase> <phrase>algorithms</phrase> has further improved <phrase>research</phrase> and implementations. Especially, various <phrase>accelerators</phrase> for deep <phrase>CNN</phrase> have been proposed based on <phrase>FPGA</phrase> platform because it has advantages of <phrase>high</phrase> performance, reconfigurability, and fast development round, etc. Although current <phrase>FPGA</phrase> <phrase>accelerators</phrase> have demonstrated better performance over generic processors, the <phrase>accelerator</phrase> <phrase>design</phrase> space has not been well exploited. One critical problem is that the computation throughput may not well match the <phrase>memory</phrase> bandwidth provided an <phrase>FPGA</phrase> platform. Consequently, existing approaches cannot achieve best performance due to under-utilization of either <phrase>logic</phrase> resource or <phrase>memory</phrase> bandwidth. At the same time, the increasing complexity and <phrase>scalability</phrase> of <phrase>deep learning</phrase> applications aggravate this problem. In <phrase>order</phrase> to overcome this problem, we propose an analytical <phrase>design</phrase> scheme using the roofline <phrase>model</phrase>. For any <phrase>solution</phrase> of a <phrase>CNN</phrase> <phrase>design</phrase>, we quantitatively analyze its <phrase>computing</phrase> throughput and required <phrase>memory</phrase> bandwidth using various optimization techniques, such as loop tiling and transformation. Then, with the help of rooine <phrase>model</phrase>, we can identify the <phrase>solution</phrase> with best performance and lowest <phrase>FPGA</phrase> resource requirement. As a <phrase>case study</phrase>, we implement a <phrase>CNN</phrase> <phrase>accelerator</phrase> on a VC707 <phrase>FPGA</phrase> board and compare it to previous approaches. Our implementation achieves a <phrase>peak</phrase> performance of 61.62 GFLOPS under 100MHz working <phrase>frequency</phrase>, which outperform previous approaches significantly.
<phrase>Research</phrase> on <phrase>Network Traffic</phrase> Identification Based on <phrase>Multi Layer</phrase> <phrase>Perceptron</phrase> In recent years, many <phrase>machine learning</phrase> methods have been used in <phrase>network traffic</phrase> identification.In <phrase>order</phrase> to improve the accuracy and solve some problems of <phrase>network traffic</phrase> identification, this <phrase>paper</phrase> presents a <phrase>multi layer</phrase> <phrase>perceptron</phrase> <phrase>neural network</phrase>-based method for <phrase>network traffic</phrase> identification, and parameters of <phrase>multi-layer</phrase> <phrase>perceptron</phrase> <phrase>neural network</phrase> are analyzed. <phrase>Experimental</phrase> <phrase>results</phrase> show that this method can effectively solve some problems, and can improve the classification correctness. 1. Introduction With the growth of network bandwidth, <phrase>network traffic</phrase> identification as a hot <phrase>research</phrase> topic in the fields of <phrase>network management</phrase> is gradually concerned by researchers at home and abroad. A <phrase>variety</phrase> of new network applications such as <phrase>peer to peer</phrase> (<phrase>P2P</phrase>) are becoming popular, traditional <phrase>traffic identification</phrase> method such as well known <phrase>port</phrase> based or <phrase>deep packet inspection</phrase> are either no longer effective for all type of application. <phrase>Traffic identification</phrase> based ML (<phrase>Machine Learning</phrase>) can obtain more precise identification accuracy and higher recognition efficiency, and you will need to select appropriate <phrase>feature selection</phrase> method, which can select best features according to the impact of the great traffic behavior characteristics. The common <phrase>traffic identification</phrase> methods based on <phrase>machine learning</phrase>, such as: BAYES <phrase>neural network</phrase>, the <phrase>SVM</phrase>, C4.5 decision [1-9], these methods are compared in this <phrase>paper</phrase>, the <phrase>results</phrase> show that the MLP <phrase>algorithm</phrase> (<phrase>Multi Layer</phrase> <phrase>Perceptron</phrase>) has the higher identification accuracy compared with other identification <phrase>algorithm</phrase>, and with the increasing of the <phrase>training samples</phrase> number, the identification rate has growth trend. In summary, <phrase>traffic identification</phrase> <phrase>algorithm</phrase> has its own relevance and limitations, this <phrase>paper</phrase> proposed a multilayer <phrase>neural network</phrase>-based <phrase>traffic identification</phrase> <phrase>algorithm</phrase>, this <phrase>algorithm</phrase> is designed to improve the accuracy of the <phrase>traffic identification</phrase>, in-depth analysis of impact of various parameters on the identification <phrase>results</phrase> in the multilayer <phrase>neural network</phrase>, and provide the reference for further study.
Applications of <phrase>Hybrid</phrase> Fuzzy <phrase>Expert Systems</phrase> in Computer Networks <phrase>Design</phrase> The task of designing and configuring large Computer Networks most suited to a certain application and environment is difficult, as it requires highly specialized technical skills and <phrase>knowledge</phrase>, as well as a <phrase>deep understanding</phrase> of a dynamic commercial market. Current <phrase>expert systems</phrase> have made solid achievements in supporting decision makers, they use prior experience to <phrase>solve problems</phrase> in different domains. <phrase>Hybrid</phrase> fuzzy <phrase>expert systems</phrase> have appeared all over the world proving that integrated fuzzy <phrase>expert systems</phrase>/<phrase>neural networks</phrase> methods replaces <phrase>classical</phrase> hard decision methods and providing better performance. In this <phrase>paper</phrase>, we present an integrated fuzzy expert system, <phrase>machine learning</phrase>, and <phrase>neural networks</phrase> approach to large structured computer networks <phrase>design</phrase> and evaluation. After presenting an overview of the system and the <phrase>major</phrase> <phrase>research</phrase> choices, we describe in detail the sys-tem's modules and present examples of its potential use.
Adaptive Web <phrase>Navigation</phrase> for <phrase>Wireless</phrase> Devices Visitors who browse the web from <phrase>wireless</phrase> <phrase>PDAs</phrase>, <phrase>cell</phrase> <phrase>phones</phrase>, and pagers are frequently stymied by web interfaces optimized for desktop PCs. Simply replacing graphics with text and reformatting tables does not solve the problem, because deep link structures can still require minutes to traverse. In this <phrase>paper</phrase> we develop an <phrase>algorithm</phrase>, MINPATH, that automatically improves <phrase>wireless</phrase> web <phrase>navigation</phrase> by suggesting useful shortcut links in real time. MINPATH finds shortcuts by using a learned <phrase>model</phrase> of web visitor behavior to estimate the savings of shortcut links, and suggests only the few best links. We explore a <phrase>variety</phrase> of predictive models, including <phrase>Nave</phrase> Bayes mixture models and mixtures of <phrase>Markov</phrase> models, and <phrase>report</phrase> <phrase>empirical evidence</phrase> that MINPATH finds useful shortcuts that save substantial navigational effort.
<phrase>Deep belief</phrase> network based <phrase>semantic</phrase> taggers for <phrase>spoken language</phrase> understanding This <phrase>paper</phrase> investigates the use of <phrase>deep belief</phrase> networks (DBN) for <phrase>semantic</phrase> tagging, a <phrase>sequence</phrase> classification task, in <phrase>spoken language</phrase> understanding (SLU). We evaluate the performance of the DBN based <phrase>sequence</phrase> tagger on the well-studied ATIS task and compare our technique to <phrase>conditional random fields</phrase> (CRF), a <phrase>state</phrase>-of-the-<phrase>art</phrase> classifier for <phrase>sequence</phrase> classification. In conjunction with lexical and <phrase>named entity</phrase> features, we also use dependency parser based <phrase>syntactic</phrase> features and part of speech (POS) tags [1]. Under both noisy conditions (output of <phrase>automatic speech recognition</phrase> system) and clean conditions (manual transcriptions), our <phrase>deep belief</phrase> network based <phrase>sequence</phrase> tagger outperforms the best CRF based system described in [1] by an absolute 2% and 1% F-measure, respectively.Upon carrying out an analysis of cases where CRF and DBN models made different predictions, we observed that when discrete features are projected onto a continuous space during <phrase>neural network</phrase> training, the <phrase>model</phrase> learns to cluster these features leading to its improved generalization capability, relative to a CRF <phrase>model</phrase>, especially in cases where some features are either missing or noisy.
Smart <phrase>Textiles</phrase> Prototyping: Participating with New Ideas This <phrase>paper</phrase> outlines our work with children who use our smart <phrase>textiles</phrase> <phrase>construction</phrase> kit in a " living lab ". Our constructionist approach to <phrase>participatory design</phrase> with children is inspired by Allison Druin's " <phrase>Cooperative</phrase> Inquiry ". The main idea is to not only develop <phrase>software</phrase> and hardware for and with children, but more importantly to enable insight and a rich learning experience via deep involvement with new technologies. In a bilateral <phrase>learning environment</phrase> we acknowledge children's ideas for future <phrase>products</phrase> and <phrase>research</phrase> directions. It is therefore essential to equip them with tools that support their imagination, that facilitate <phrase>technology</phrase> immersion, and that allow rapid creation of <phrase>high</phrase>-tech <phrase>prototypes</phrase> with novel materials. Our <phrase>convergent</phrase> approach to developing and employing such a toolkit is rooted in <phrase>education</phrase> and aims at the empowerment of children to express their ideas.
An Interdisciplinary Environment for <phrase>Science</phrase> Learning In <phrase>scholar</phrase> <phrase>curriculum</phrase>, the integration of contents from different learning areas has been always a challenging issue, but with very few practical experimentations. A first attempt appears in the late 1920s under the name " core " [1]. Successively, interdisciplinary and integrated curricula have been widely associated with the <phrase>progressive education</phrase> movement [2]. In the last twenty years, a <phrase>variety</phrase> of approaches and <phrase>case studies</phrase> have been carried out, by combining different disciplines, pedagogical approaches, people and skills showing a deep improvement in <phrase>scholar</phrase> learning. This <phrase>paper</phrase> describes an <phrase>experimental</phrase> project of integrating <phrase>mathematics</phrase>, <phrase>natural science</phrase>, and <phrase>computer science</phrase> (<phrase>technology</phrase> <phrase>education</phrase>) in the right curricular direction for the first level of <phrase>Italian</phrase> <phrase>secondary school</phrase>. The project is aimed to improve learning in all the above disciplines and thus overcome the <phrase>low level</phrase> of <phrase>knowledge</phrase> of them held by <phrase>Italian</phrase> primary level scholars, as it has been recently shown by <phrase>OECD</phrase> <phrase>statistics</phrase> [3]. We show effectiveness of the conceived interdisciplinary approach by means of <phrase>case studies</phrase> in a network of thirty classrooms of 11 years old scholars: the INNOVAMBIENTE project. We describe the considered arguments, including justifications and reasoning. Specifically, we describe how such an interdisciplinary study can improve upon <phrase>traditional approaches</phrase> to <phrase>school</phrase> <phrase>curriculum</phrase>.
<phrase>Book</phrase> Review: Verification of Sequential and Concurrent Programs by Krzysztof R. Apt and Ernst-Riidiger Olderog (<phrase>Springer</phrase>-Verlag New <phrase>York</phrase>, 1997) in pure <phrase>computer science</phrase> and for <phrase>computer science</phrase>-oriented courses on <phrase>computational biology</phrase> '. I suspect, though, that most students will find this <phrase>book</phrase> too detailed and too complicated for thei r needs. Nonetheless, Gusfield's <phrase>book</phrase> is an important summary of the <phrase>state</phrase> of the <phrase>art</phrase> in pattern matchin g and an indicator of the importance biological problems have assumed among many researchers. It will undoubtedly draw new researchers to problems in <phrase>biology</phrase> and will hopefully encourage the m to question the importance of the problems they endeavor to solve. As Gusfield advises in hi s epilogue: `focus more on the biological quality of a computation and not exclusively on speed an d space improvements.. .learn real <phrase>biology</phrase>, <phrase>talk</phrase> extensively to <phrase>biologists</phrase>, and work on problems o f known biological importance '. Good advice and especially apt for those hoping to make an impac t in this exciting field. References [1] R. Cole. Tight bounds on the complexity of the Boyer-Moore <phrase>pattern matching</phrase> <phrase>algorithm</phrase> .toniades. <phrase>Simian</phrase> <phrase>sarcoma</phrase> <phrase>virus</phrase> one <phrase>gene</phrase> v-sis, is derived from the <phrase>gene</phrase> (or <phrase>genes</phrase>) encoding a <phrase>platelet-derived growth factor</phrase>. Specification and verification of programs is increasingly being taught to <phrase>undergraduate</phrase> and graduate <phrase>computer science</phrase> students. Courses along these lines enable students to understand and reaso n about programs as formal objects. Especially when students deal with concurrent programs do the y appreciate that program correctness while not necessarily deep in the sense of <phrase>mathematic</phrase> s can be hard. There are many possibilities to consider and intuitive arguments of correctness ar <phrase>e</phrase> often wrong, hence there is need for program verification .
<phrase>Mobile</phrase> <phrase>Ipv4</phrase> Secure Access to Home Networks <phrase>Mobile</phrase> <phrase>Ipv4</phrase> Secure Access to Home Networks To my parents and sister, for everything. iii ACKNOWLEDGEMENTS It is my pleasure to give my sincere thanks to all the people who have supported and helped me during the pursuit of my Ph.D. <phrase>degree</phrase>. Without their encouragement, I would not have completed this dissertation. First of all, I would like to <phrase>express my deep</phrase> gratitude to my dissertation advisor, Dr. John A. Copeland, for his constant support, guidance, and encouragement through my Ph.D. study. Dr. Copeland has taught me how to not only do <phrase>research</phrase> but also <phrase>lead</phrase> a <phrase>life</phrase>. Under his kind help, I have improved my ability at various aspects. What I have learned from him will be very useful to my future career and <phrase>life</phrase>.
Combining <phrase>linguistic</phrase> and <phrase>statistical analysis</phrase> to extract relations from web documents The <phrase>World Wide Web</phrase> provides a nearly endless source of <phrase>knowledge</phrase>, which is mostly given in <phrase>natural language</phrase>. A first step towards exploiting this <phrase>data</phrase> automatically could be to extract pairs of a given <phrase>semantic</phrase> relation from text documents - for example all pairs of a person and her birthdate. One strategy for this task is to find text patterns that express the <phrase>semantic</phrase> relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this <phrase>paper</phrase>, we show that this approach profits significantly when <phrase>deep linguistic</phrase> structures are used instead of surface text patterns. We demonstrate how <phrase>linguistic</phrase> structures can be represented for <phrase>machine learning</phrase>, and we provide a theoretical analysis of the <phrase>pattern matching</phrase> approach. We show the benefits of our approach by <phrase>extensive experiments</phrase> with our <phrase>prototype</phrase> system L<sc>EILA</sc>.
Strategic <phrase>Technology</phrase> Directions <phrase>Jet Propulsion Laboratory</phrase> <phrase>National Aeronautics and Space Administration</phrase> Technologies are deemed strategic if they strive to address NASA's and JPL's <phrase>grand challenges</phrase> and aspirations. Examples of fundamental challenges that we and our technologies will be called upon to address are: What is the concentration of <phrase>carbon dioxide</phrase> and other <phrase>greenhouse gases</phrase> that the <phrase>atmosphere</phrase> and oceans can absorb without crossing <phrase>climatic</phrase> tipping points? Are there other habitable environments and <phrase>life</phrase> on other bodies in the <phrase>solar</phrase> system and beyond? What are the structures and properties of other planetary systems? What is the <phrase>nature</phrase> of <phrase>dark matter</phrase> and <phrase>dark energy</phrase>, and can <phrase>energy</phrase> be harnessed on <phrase>Earth</phrase> from them? Many overarching technical challenges await us beyond the present horizon, if we are to respond to these and other goals: How can we <phrase>test</phrase>, place, and operate 10m, 20m, 50m, 100m <phrase>radar</phrase>/sub-mm/IR/optical apertures in space? What is the <phrase>technology</phrase> and operations path to provide <phrase>a 10</phrase>-fold bandwidth increase per decade for the <phrase>Deep Space Network</phrase>? How do we provide <phrase>a 10</phrase>-fold increase in <phrase>spacecraft</phrase> power? How do we conduct missions to return samples to <phrase>Earth</phrase> from <phrase>Mars</phrase> and other large <phrase>planets</phrase>, which we must before sending people there? It is difficult to resist a sense of appreciation and pride of what has been achieved in the half-century since the space <phrase>era</phrase> began. Technologies developed to support this development, and the value of the <phrase>science</phrase> return, have been incalculable. It allowed us to time-<phrase>stamp</phrase> the beginning of the <phrase>universe</phrase> and appreciate that the kind of <phrase>matter</phrase> we are made of comprises only about 4% of it, to learn more about our <phrase>solar</phrase> system than all the <phrase>knowledge</phrase> humans had distilled since they began to ponder the sky, to understand more about <phrase>Earth</phrase> and the dynamics of global change from both natural causes and <phrase>human</phrase> activity. Strategic technologies identified in this document also represent <phrase>technology</phrase> capabilities that the <phrase>Jet Propulsion Laboratory</phrase> believes are essential to continuing progress in pursuit of NASA's and JPL's mission, national goals, and in addressing global concerns. Advancing these technologies, that often push the theoretical limits of performance, is a <phrase>major</phrase> challenge, not least because such advances require sustaining an environment of imagination, <phrase>creativity</phrase>, and a <phrase>culture</phrase> of <phrase>innovation</phrase> for decades of dedicated effort at a time when support is provided on much-shorter time scales and sometimes not at all. The balance of what is invested on near-term projects vs. 
Teaching <phrase>Robot</phrase> <phrase>Motion Planning</phrase> <phrase>Robot</phrase> <phrase>motion planning</phrase> is a fairly intuitive and engaging topic, yet it is difficult to teach. The material is taught in <phrase>undergraduate</phrase> and graduate <phrase>robotics</phrase> classes in <phrase>computer science</phrase>, <phrase>electrical engineering</phrase>, <phrase>mechanical engineering</phrase> and aeronautical <phrase>engineering</phrase>, but at an abstract level. <phrase>Deep learning</phrase> could be achieved by having students implement and <phrase>test</phrase> different <phrase>motion planning</phrase> strategies. However, a full implementation of <phrase>motion planning</phrase> <phrase>algorithms</phrase> by undergraduates is practically impossible in the context of a <phrase>single</phrase> class, even by students proficient in <phrase>programming</phrase>. By helping undergraduates grasp <phrase>motion planning</phrase> concepts in series of courses designed for increasing advanced levels, we can open the field to young and enthusiastic talent. This cannot be done by asking students to implement <phrase>motion planning</phrase> <phrase>algorithms</phrase> from scratch or access thousands of lines of code and just figure out how things work. We present an ongoing project to develop microworld <phrase>software</phrase> and a modeling <phrase>curriculum</phrase> that supports <phrase>undergraduate</phrase> acquisition of <phrase>motion planning</phrase> <phrase>knowledge</phrase> and tool use by <phrase>computer science</phrase> and <phrase>engineering</phrase> students.
Deep Self-Taught Learning for Handwritten <phrase>Character Recognition</phrase> Recent theoretical and empirical work in <phrase>statistical machine learning</phrase> has demonstrated the importance of <phrase>learning algorithms</phrase> for <phrase>deep architectures</phrase>, i.e., <phrase>function</phrase> classes obtained by <phrase>composing</phrase> multiple non-linear transformations. Self-taught learning (exploiting unla-beled examples or examples from other <phrase>distributions</phrase>) has already been applied to deep learners, but mostly to show the advantage of unlabeled examples. Here we explore the advantage brought by out-of-distribution examples. For this purpose we developed a powerful generator of <phrase>stochastic</phrase> variations and noise processes for character images, including not only affine transformations but also <phrase>slant</phrase>, local elastic deformations, changes in thickness , background images, grey level changes, contrast, occlusion, and various types of noise. The out-of-distribution examples are obtained from these highly distorted images or by including examples of object classes different from those in the <phrase>target</phrase> <phrase>test</phrase> set. We show that deep learners benefit more from out-of-distribution examples than a corresponding shallow learner, at least in the <phrase>area</phrase> of handwritten <phrase>character recognition</phrase>. In fact, we show that they beat previously <phrase>published results</phrase> and reach <phrase>human</phrase>-level performance on both <phrase>handwritten digit</phrase> classification and 62-class handwritten <phrase>character recognition</phrase>.
Applying <phrase>Deep Belief</phrase> Networks to <phrase>Word Sense Disambiguation</phrase> In this <phrase>paper</phrase>, we applied a novel <phrase>learning algorithm</phrase>, namely, <phrase>Deep Belief</phrase> Networks (DBN) to word sense disambigua-tion (WSD). DBN is a probabilistic gen-erative <phrase>model</phrase> composed of <phrase>multiple layers</phrase> of <phrase>hidden units</phrase>. DBN uses <phrase>Restricted Boltzmann Machine</phrase> (RBM) to greedily <phrase>train</phrase> <phrase>layer by layer</phrase> as a <phrase>pre-training</phrase>. Then, a separate <phrase>fine tuning</phrase> step is employed to improve the discrim-inative power. We compared DBN with various <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>supervised learning</phrase> <phrase>algorithms</phrase> in WSD such as <phrase>Support Vector Machine</phrase> (<phrase>SVM</phrase>), Maximum En-tropy <phrase>model</phrase> (MaxEnt), <phrase>Nave</phrase> Bayes clas-sifier (NB) and Kernel <phrase>Principal Component Analysis</phrase> (KPCA). We used all words in the given paragraph, surrounding context words and part-of-speech of surrounding words as our <phrase>knowledge</phrase> sources. We conducted our experiment on the SENSEVAL-2 <phrase>data set</phrase>. We observed that DBN outperformed all other <phrase>learning algorithms</phrase> .
<phrase>Deep Boltzmann Machines</phrase> We present a new <phrase>learning algorithm</phrase> for Boltz-mann machines that contain many layers of hidden variables. <phrase>Data</phrase>-dependent expectations are estimated using a variational approximation that tends to focus on a <phrase>single</phrase> mode, and <phrase>data</phrase>-<phrase>independent</phrase> expectations are approximated using persistent <phrase>Markov</phrase> chains. The use of two quite different techniques for estimating the two types of expectation that enter into the <phrase>gradient</phrase> of the <phrase>log-likelihood</phrase> makes it practical to learn <phrase>Boltzmann</phrase> machines with multiple <phrase>hidden layers</phrase> and millions of parameters. The learning can be made more efficient by using a <phrase>layer-by-layer</phrase> " <phrase>pre-training</phrase> " phase that allows variational inference to be initialized with a <phrase>single</phrase> bottom-up <phrase>pass</phrase>. We present <phrase>results</phrase> on the MNIST and NORB datasets showing that <phrase>deep Boltzmann machines</phrase> learn good <phrase>generative models</phrase> and perform well on <phrase>handwritten digit</phrase> and <phrase>visual object</phrase> <phrase>recognition tasks</phrase>.
<phrase>Semantic</phrase> Class Learning with Deep Coordinate Structures in <phrase>Web Pages</phrase> In this <phrase>paper</phrase>, we introduce a kind of descriptive structure, the deep coordinate structure, which universally exists in <phrase>web pages</phrase>. And it's used for the task of <phrase>semantic</phrase> class learning. At first, we extract the deep coordinate structures in <phrase>web pages</phrase>, and use the <phrase>seed</phrase> instances to filter them and extract the candidates; during the stage of evaluating the candidates, we build a <phrase>graph</phrase> among the seeds, <phrase>web pages</phrase>, coordinate structures and candidates, and use <phrase>PageRank</phrase> to evaluate the candidates. We conduct experiments on 14 <phrase>semantic</phrase> classes (7 <phrase>Chinese</phrase> classes and 7 <phrase>English</phrase> classes) and consistently achieve <phrase>high</phrase> precisions. We compare our <phrase>results</phrase> with several <phrase>state</phrase>-of-<phrase>art</phrase> systems, and our system is better on some of the <phrase>semantic</phrase> classes.
Learning hierarchical representations for face verification with convolutional <phrase>deep belief</phrase> networks Most modern <phrase>face recognition</phrase> systems rely on a <phrase>feature representation</phrase> given by a <phrase>hand-crafted</phrase> image descriptor, such as Local <phrase>Binary</phrase> Patterns (LBP), and achieve improved performance by combining several such representations. In this <phrase>paper</phrase>, we propose <phrase>deep learning</phrase> as a natural source for obtaining additional, complementary representations. To learn features in <phrase>high</phrase>-resolution images, we make use of convolutional <phrase>deep belief</phrase> networks. Moreover, to take advantage of global structure in an object class, we develop local convolutional <phrase>restricted Boltzmann machines</phrase>, a novel convolutional learning <phrase>model</phrase> that exploits the global structure by not assuming stationarity of features across the image, while maintaining <phrase>scalability</phrase> and robustness to small misalignments. We also present a novel application of <phrase>deep learning</phrase> to descriptors other than <phrase>pixel</phrase> intensity values , such as LBP. In addition, we compare performance of <phrase>networks trained</phrase> using <phrase>unsupervised learning</phrase> against networks with random filters, and empirically show that learning weights not only is necessary for obtaining good <phrase>multi-layer</phrase> representations, but also provides robustness to the choice of the <phrase>network architecture</phrase> parameters. Finally, we show that a recognition system using only representations obtained from <phrase>deep learning</phrase> can achieve comparable accuracy with a system using a combination of <phrase>hand-crafted</phrase> image descriptors. Moreover, by combining these representations , we achieve <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> on a <phrase>real-world</phrase> face verification <phrase>database</phrase>.
Learning for Deep <phrase>Language</phrase> Understanding The <phrase>paper</phrase> addresses the problem of learning to parse sentences to logical representations of their underlying meaning, by inducing a <phrase>syntactic</phrase>-<phrase>semantic</phrase> <phrase>grammar</phrase>. The approach uses a class of grammars which has been <phrase>proven</phrase> to be learnable from representative examples. In this <phrase>paper</phrase>, we introduce tractable <phrase>learning algorithms</phrase> for learning this class of grammars, comparing them in terms of a-priori <phrase>knowledge</phrase> needed by the learner, <phrase>hypothesis</phrase> space and <phrase>algorithm</phrase> complexity. We present <phrase>experimental</phrase> <phrase>results</phrase> on learning tense, <phrase>aspect</phrase>, modality and <phrase>negation</phrase> of verbal constructions.
<phrase>Problem-based Learning</phrase> in <phrase>Law</phrase>: <phrase>Student</phrase> Attitudes This study adopts a contemporary <phrase>research</phrase> approach and seeks to identify, analyse, and address some specific process related problems that third year <phrase>undergraduate</phrase> <phrase>translation</phrase> students' experience when learning <phrase>law</phrase> and solving legal problems. This study sought to answer the following question: Is there any change in the affective outcomes of <phrase>undergraduate</phrase> <phrase>translation</phrase> students with respect to their bias towards surface and deep <phrase>approaches to learning</phrase> before and after the <phrase>problem-based learning</phrase> intervention? The students' progression in <phrase>problem solving</phrase> was measured before and after the <phrase>problem-based learning</phrase> intervention to check for change. The second <phrase>research</phrase> question addresses students attitudes to <phrase>learning environments</phrase> before and after the <phrase>problem-based learning</phrase> intervention.
Structure Weight SYNONYM None DEFINITION In structured text retrieval, the structure of a text component may be used to estimate the relevance of that component. This is done by associating a weight to the structure reflecting its significance when estimating the relevance of the component for a given query. MAIN TEXT Associating weight to the structure of a component in itself is not new, and several investigations have been reported for whole document retrieval. This entry is concerned with structure weights in the context of structured text retrieval, where the aim is to exploit the document structure to return document components, instead of whole documents. In structured text retrieval, not all document components will trigger the same user satisfaction when returned as answers to queries. In the context of structured documents <phrase>markup</phrase> in <phrase>XML</phrase>, some document components, i.e. <phrase>XML</phrase> elements, may not be appropriate to return because they are too small, of a tag type that does not contain informative content, nested too deep in the document logical structure, or for other reasons. When ranking <phrase>XML</phrase> elements, their structure (size, tag type, path, depth, etc.) may prove important. The importance of the element structure is captured through a weight, which can be <phrase>binary</phrase>. Using <phrase>binary</phrase> weights means that an element is (value one) or is not (value zero) considered for indexing and retrieval. The decision can be made by looking at the <phrase>DTD</phrase> 1 of the collection, past relevance <phrase>data</phrase>, and/or the requirements of the application and user scenario. In the selective indexing strategy [3], only elements of types that were found to contain relevant content for previous query sets (relevance <phrase>data</phrase>) are considered. Any elements with a length size less than a given threshold can also be ignored. Weights can be assigned to characteristics of elements, such as length, depth, location in the document logical structure, and so on. For instance, within the <phrase>language</phrase> modelling framework, length has been used as a normalization parameter (weight) incorporated through a <phrase>prior probability</phrase> in the ranking formula [2]. With statistical approaches, the weights are estimated based on <phrase>training data</phrase>, such as past relevance <phrase>data</phrase>. The weights can be determined using <phrase>machine learning</phrase>, and then used in the ranking <phrase>function</phrase>. They can also be directly calculated based on the 1 <phrase>Document Type Definition</phrase>.
A Direct Inversion Scheme for Deep <phrase>Resistivity</phrase> Sounding <phrase>Data</phrase> Using <phrase>Artificial Neural Networks</phrase> Initialization of <phrase>model</phrase> parameters is crucial in the conventional 1D inversion of <phrase>DC</phrase> electrical <phrase>data</phrase>, since a poor guess may result in undesired parameter estimations. In the present work, we investigate the performance of <phrase>neural networks</phrase> in the direct inversion of <phrase>DC</phrase> sounding <phrase>data</phrase>, without the need of a priori <phrase>information</phrase>. We introduce a two-step network approach where the first network identifies the curve type, followed by the <phrase>model</phrase> <phrase>parameter estimation</phrase> using the second network. This approach provides the flexibility to accommodate all the characteristic sounding curve types with a wide <phrase>range</phrase> of <phrase>resistivity</phrase> and thickness. Here we realize a three layer <phrase>feed-forward</phrase> <phrase>neural network</phrase> with fast back propagation <phrase>learning algorithms</phrase> performing well. The <phrase>basic</phrase> <phrase>data</phrase> sets for training and testing were simulated on the basis of available deep <phrase>resistivity</phrase> sounding (DRS) <phrase>data</phrase> from the crystalline terrains of <phrase>south India</phrase>. The optimum network parameters and performance were decided as a <phrase>function</phrase> of the testing error convergence with respect to the network training error. On adequate training, the final weights simulate faithfully to recover <phrase>resistivity</phrase> and thickness on new <phrase>data</phrase>. The small discrepancies noticed, however, are well within the resolvability of <phrase>resistivity</phrase> sounding curve interpretations.
Deep <phrase>Semantic</phrase> Embedding We introduce Deep <phrase>Semantic</phrase> Embedding (DSE), a <phrase>supervised learning</phrase> <phrase>algorithm</phrase> which computes <phrase>semantic</phrase> representation for text documents by respecting their similarity to a given query. Unlike other methods that use <phrase>single</phrase>-layer learning machines, DSE maps word inputs into a <phrase>low-dimensional</phrase> <phrase>semantic</phrase> space with <phrase>deep neural network</phrase>, and achieves a highly nonlinear embedding to <phrase>model</phrase> the <phrase>human perception</phrase> of text <phrase>semantics</phrase>. Through discriminative <phrase>fine-tuning</phrase> of the <phrase>deep neural network</phrase>, DSE is able to encode the relative similarity between relevant/irrelevant document pairs in <phrase>training data</phrase>, and hence learn a reliable ranking score for a query-document pair. We present <phrase>test</phrase> <phrase>results</phrase> on datasets including scientific publications and <phrase>user-generated</phrase> <phrase>knowledge base</phrase>.
<phrase>Unsupervised learning</phrase> of hierarchical representations with convolutional <phrase>deep belief</phrase> networks There has been much interest in <phrase>unsupervised learning</phrase> of hierarchical <phrase>generative models</phrase> such as <phrase>deep belief</phrase> networks (DBNs); however, scaling such models to full-sized, <phrase>high</phrase>-dimensional images remains a difficult problem. To address this problem, we present the <i>convolutional <phrase>deep belief</phrase> network</i>, a hierarchical <phrase>generative model</phrase> that scales to realistic image sizes. This <phrase>model</phrase> is <phrase>translation</phrase>-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is <i>probabilistic max-pooling</i>, a novel technique that shrinks the representations of higher layers in a probabilistically <phrase>sound</phrase> way. Our experiments show that the <phrase>algorithm</phrase> learns useful <phrase>high</phrase>-level <phrase>visual features</phrase>, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual <phrase>recognition tasks</phrase> and show that our <phrase>model</phrase> can perform hierarchical (bottom-up and top-down) inference over full-sized images.
Deep Spatiotemporal <phrase>Feature Learning</phrase> with Application to <phrase>Image Classification</phrase> Deep <phrase>machine learning</phrase> is an emerging framework for dealing with complex <phrase>high</phrase>-dimensionality <phrase>data</phrase> in a hierarchical <phrase>fashion</phrase> which draws some inspiration from biological sources. Despite the notable progress made in the field, there remains a need for an <phrase>architecture</phrase> that can represent temporal <phrase>information</phrase> with the same ease that spatial <phrase>information</phrase> is discovered. In this work, we present new <phrase>results</phrase> using a recently introduced <phrase>deep learning</phrase> <phrase>architecture</phrase> called Deep <phrase>Spatio-Temporal</phrase> Inference Network (<phrase>DeSTIN</phrase>). <phrase>DeSTIN</phrase> is a discriminative <phrase>deep learning</phrase> <phrase>architecture</phrase> that combines concepts from <phrase>unsupervised learning</phrase> for dynamic pattern representation together with <phrase>Bayesian inference</phrase>. In <phrase>DeSTIN</phrase> the spatiotemporal dependencies that exist within the observations are modeled inherently in an unguided manner. Each node models the inputs by means of clustering and simple dynamics modeling while it constructs a belief <phrase>state</phrase> over the distribution of sequences using <phrase>Bayesian inference</phrase>. We demonstrate that <phrase>information</phrase> from the different layers of this hierarchical system can be extracted and utilized for the purpose of pattern classification. Earlier <phrase>simulation</phrase> <phrase>results</phrase> indicated that the framework is highly promising, consequently in this work we expand <phrase>DeSTIN</phrase> to a popular problem, the MNIST <phrase>data set</phrase> of <phrase>handwritten digits</phrase>. The system as a <phrase>preprocessor</phrase> to a <phrase>neural network</phrase> achieves a <phrase>recognition accuracy</phrase> of 97.98% on this <phrase>data set</phrase>. We further show related <phrase>experimental</phrase> <phrase>results</phrase> pertaining to automatic cluster adaptation and termination.
<phrase>Knowledge</phrase>, <phrase>management</phrase>, and <phrase>knowledge management</phrase> in <phrase>business</phrase> operations Purpose The purpose of this <phrase>research</phrase> is to help <phrase>knowledge</phrase> managers systematically grasp ''<phrase>knowledge</phrase> about <phrase>management</phrase> <phrase>knowledge</phrase>'' and get a ''deep and full'' understanding of the <phrase>nature</phrase>, scope and methodologies of <phrase>knowledge management</phrase>. <phrase>Design</phrase>/methodology/approach Through presenting a <phrase>variety</phrase> of perspectives on <phrase>knowledge</phrase>, <phrase>management</phrase>, and <phrase>knowledge management</phrase>, the article explores the essence of <phrase>knowledge management</phrase> in organizations from a perspective of critical <phrase>systems thinking</phrase>. Findings <phrase>Knowledge management</phrase> in <phrase>business</phrase> organizations has the task of managing the activities of <phrase>knowledge</phrase> workers or the transformation and interaction of organizational ''static substance <phrase>knowledge</phrase>'' and ''dynamic process <phrase>knowledge</phrase>'' for ''<phrase>products</phrase>, services, and practical process <phrase>innovation</phrase>'' and, at the same time, ''creating new or justifying existing organizational systematic <phrase>knowledge</phrase>''. <phrase>Knowledge management</phrase> is not simply about <phrase>recording</phrase> and manipulating <phrase>explicit knowledge</phrase>, but needs to address that which is implicit, and from which benefit can therefore be derived only through process rather than content. Originality/value The <phrase>comprehensive</phrase> review and classification of various <phrase>management</phrase> theories will expand both <phrase>knowledge</phrase> managers' and <phrase>knowledge</phrase> workers' understanding of the subject and provide a foundation for building a <phrase>knowledge management</phrase> toolkit in practice. <phrase>Knowledge</phrase> is an important issue for <phrase>business</phrase> organisations. There have been a number of different perspectives from which researchers and practitioners have approached the <phrase>management</phrase> of <phrase>knowledge</phrase>. While the acquisition, transmission, and use of <phrase>knowledge</phrase> has always been an important part of <phrase>human</phrase> affairs (hence the well-established domain of <phrase>epistemology</phrase>), <phrase>Penrose</phrase> (1959), Bell (1973) and Drucker (1993a) provide us with a good basis for relating <phrase>knowledge</phrase> to twenty-first century <phrase>business</phrase> organisations. Drucker symbolically declares <phrase>knowledge</phrase>, as we move into the ''<phrase>knowledge</phrase> <phrase>society</phrase>'' (Drucker, 1993b), as the key resource for individual firms and the key driver of <phrase>competitive advantage</phrase> for developed nations, competing in <phrase>knowledge</phrase>-based industries, living with <phrase>knowledge</phrase> communities and societies. <phrase>Penrose</phrase>, acknowledged as one of the first scholars to recognize the role of <phrase>knowledge</phrase> in <phrase>business</phrase> organisations, saw acquiring <phrase>knowledge</phrase> as a <phrase>social learning</phrase> process: This increase in <phrase>knowledge</phrase> not only causes the productive opportunity of a <phrase>firm</phrase> to change in ways unrelated to changes in the environment, but also contributes to the ''uniqueness'' of the opportunity of each individual <phrase>firm</phrase> (<phrase>Penrose</phrase>, 1959). As did Bell, Drucker proposed the concepts of <phrase>knowledge worker</phrase> and <phrase>knowledge</phrase> work arguing that the first <phrase>knowledge</phrase> workers, Taylor's <phrase>industrial</phrase> engineers, increased the The authors wish to thank the anonymous reviewers' and editor's kind suggestion for improving the quality and readability 
Attention Modeling for <phrase>Face Recognition</phrase> via <phrase>Deep Learning</phrase> <phrase>Face recognition</phrase> is an important <phrase>area</phrase> of <phrase>research</phrase> in <phrase>cognitive science</phrase> and <phrase>machine learning</phrase>. This is the first <phrase>paper</phrase> utilizing <phrase>deep learning</phrase> techniques to <phrase>model</phrase> human's attention for <phrase>face recognition</phrase>. In our attention <phrase>model</phrase> based on <phrase>bilinear</phrase> <phrase>deep belief</phrase> network (DBDN), the <phrase>discriminant</phrase> <phrase>information</phrase> is maximized in a frame of simulating the <phrase>human</phrase> <phrase>visual cortex</phrase> and human's <phrase>perception</phrase>. Comparative experiments demonstrate that from <phrase>recognition accuracy</phrase> our <phrase>deep learning</phrase> <phrase>model</phrase> outperforms both representative benchmark models and existing bio-inspired models. Furthermore, our <phrase>model</phrase> is able to automatically abstract and emphasize the important facial features and patterns which are consistent with the human's attention map.
Plausible Justification <phrase>Trees</phrase>: A Framework for Deep and Dynamic Integration of <phrase>Learning Strategies</phrase> This <phrase>paper</phrase> describes a framework for the deep and dynamic integration of <phrase>learning strategies</phrase>. The framework is based on the idea that each <phrase>single</phrase>-strategy learning method is ultimately the result of certain <phrase>elementary</phrase> inferences instead of integrating <phrase>learning strategies</phrase> at a macro level, we propose to integrate the different inference types that generate individual <phrase>learning strategies</phrase>. The <phrase>paper</phrase> presents a concept learning and theory revision method that was developed in this framework. It allows the system to learn from one or from several (positive and/or negative) examples, and to both generalize and specialize its <phrase>knowledge base</phrase>. The method integrates deeply and dynamically different <phrase>learning strategies</phrase>, depending of the relationship between the input <phrase>information</phrase> and the <phrase>knowledge base</phrase>. It also behaves as a <phrase>single</phrase>-strategy learning method whenever the applicability conditions of such a method are satisfied.
Can the Web Improve the Effectiveness Opperson-<phrase>Centered Learning</phrase>? <phrase>Case Study</phrase> on Teaching and Living Web-<phrase>Engineering</phrase> The fundamental <phrase>hypothesis</phrase> underlying this <phrase>research</phrase> is that <phrase>web-based</phrase> services have the potential to make didactic styles leading to <phrase>deep learning</phrase> processes, such as Person-<phrase>Centered Learning</phrase>, more effective and hence more feasible. Recently, we complemented the Person-Centered Approach by elements of <phrase>e</phrase>-Learning, claiming to multiply the advantages of both constituents. In this <phrase>paper</phrase> we characterize the resulting style we refer to as Person-Centered <phrase>e</phrase>-Learning (PCeL) in <phrase>general</phrase> and present a <phrase>case study</phrase> encompassing a group project performed in a lab-course on 'Web <phrase>Engineering</phrase>'. We found that although the Person-Centered style remains more demanding, its combination with <phrase>e</phrase>-Learning brings about several benefits and furthermore appeals to students.
An <phrase>FPGA</phrase>-based <phrase>accelerator</phrase> for LambdaRank in <phrase>Web search</phrase> engines In modern <phrase>Web search</phrase> engines, <phrase>Neural Network</phrase> (NN)-based learning to rank <phrase>algorithms</phrase> is intensively used to increase the quality of search <phrase>results</phrase>. LambdaRank is one such <phrase>algorithm</phrase>. However, it is hard to be efficiently accelerated by computer clusters or <phrase>GPUs</phrase>, because: (i) the cost <phrase>function</phrase> for the ranking problem is much more complex than that of traditional Back-Propagation(<phrase>BP</phrase>) NNs, and (<phrase>ii</phrase>) no <phrase>coarse-grained</phrase> parallelism exists in the <phrase>algorithm</phrase>. This <phrase>article presents</phrase> an <phrase>FPGA</phrase>-based <phrase>accelerator</phrase> <phrase>solution</phrase> to provide <phrase>high</phrase> <phrase>computing</phrase> performance with <phrase>low power</phrase> consumption. A compact deep pipeline is proposed to handle the complex <phrase>computing</phrase> in the batch updating. The <phrase>area</phrase> scales linearly with the number of <phrase>hidden nodes</phrase> in the <phrase>algorithm</phrase>. We also carefully <phrase>design</phrase> a <phrase>data</phrase> format to enable streaming consumption of the <phrase>training data</phrase> from the host computer. The <phrase>accelerator</phrase> shows up to 15.3X (with <phrase>PCIe</phrase> x4) and 23.9X (with <phrase>PCIe</phrase> x8) speedup compared with the pure <phrase>software</phrase> implementation on datasets from a commercial <phrase>search engine</phrase>.
<phrase>Deep Transfer</phrase>: A <phrase>Markov</phrase> <phrase>Logic</phrase> Approach eople are able to take <phrase>knowledge</phrase> learned in one domain and apply it to an entirely different one. For example, <phrase>Wall Street</phrase> firms often hire <phrase>physicists</phrase> to solve <phrase>finance</phrase> problems. Even though these two domains have superficially nothing in common, training as a <phrase>physicist</phrase> provides <phrase>knowledge</phrase> and skills that are highly applicable in <phrase>finance</phrase> (for example, solving <phrase>differential equations</phrase> and performing <phrase>Monte Carlo</phrase> simulations). Yet standard <phrase>machine-learning</phrase> approaches are unable to do this. For example, a <phrase>model</phrase> learned on <phrase>physics</phrase> <phrase>data</phrase> would not be applicable to <phrase>finance</phrase> <phrase>data</phrase>, because the variables in the two domains are different. Despite the recent interest in <phrase>transfer learning</phrase>, most approaches do not address this problem, instead focusing on modeling either a change of <phrase>distributions</phrase> over the same variables or minor variations of the same domain (for example, different numbers of objects). We call this shallow transfer. Our goal is to perform <phrase>deep transfer</phrase>, which involves generalizing across different domains (that is, between domains with different objects, classes, properties, and relations). Performing <phrase>deep transfer</phrase> requires discovering structural regularities that apply to many different domains, irrespective of their superficial descriptions. For example, two domains may be mod-eled by the same type of equation, and <phrase>solution</phrase> techniques learned in one can be applied in the other. The inability to do this is arguably the biggest gap between current learning systems and humans. n Currently the largest gap between <phrase>human</phrase> and <phrase>machine learning</phrase> is <phrase>learning algorithms</phrase>' inability to perform <phrase>deep transfer</phrase>, that is, generalize from one domain to another domain containing different objects, classes, properties, and relations. We argue that second-<phrase>order</phrase> <phrase>Markov</phrase> <phrase>logic</phrase> is ideally suited for this purpose and propose an approach based on it. Our <phrase>algorithm</phrase> discovers structural regularities in the source domain in the form of <phrase>Markov</phrase> <phrase>logic</phrase> formulas with predicate variables and instantiates these formulas with predicates from the <phrase>target</phrase> domain. Our approach has successfully transferred learned <phrase>knowledge</phrase> among <phrase>molecular biology</phrase> , web, and <phrase>social network</phrase> domains. We believe that an approach to <phrase>deep transfer</phrase> should satisfy three <phrase>desiderata</phrase>. First, it should be <phrase>relational</phrase> in <phrase>order</phrase> to capture properties among different predicates. Second, it should be probabilis-tic, to handle the uncertainty inherent in transfer in a principled way. Lastly, it should be able to express <phrase>knowledge</phrase> in a <phrase>domain-independent</phrase> manner to allow for transfer between domains described by different predicates and types. To meet these requirements, we have developed an approach, called <phrase>deep transfer</phrase> 
Learning <phrase>Deep Generative Models</phrase> with Doubly <phrase>Stochastic</phrase> MCMC We present doubly <phrase>stochastic</phrase> <phrase>gradient</phrase> MCMC, a simple and generic method for (approximate) <phrase>Bayesian inference</phrase> of deep gen-erative models in the collapsed continuous <phrase>parameter space</phrase>. At each MCMC sampling step, the <phrase>algorithm</phrase> randomly draws a <phrase>mini</phrase>-batch of <phrase>data</phrase> samples to estimate the <phrase>gradient</phrase> of log-posterior and further estimates the intractable expectation over <phrase>latent variables</phrase> via a Gibbs sampler or a neural adaptive importance sampler. We demonstrate the effectiveness on learning deep <phrase>sigmoid</phrase> <phrase>belief networks</phrase> (DSBNs). Compared to the <phrase>state</phrase>-of-the-<phrase>art</phrase> methods using <phrase>Gibbs sampling</phrase> with <phrase>data</phrase> augmentation, our <phrase>algorithm</phrase> is much more efficient and manages to learn DSBNs on <phrase>large datasets</phrase>.
How to Discount <phrase>Deep Reinforcement Learning</phrase>: Towards New Dynamic Strategies Using deep <phrase>neural nets</phrase> as <phrase>function</phrase> approximator for <phrase>reinforcement learning</phrase> tasks have recently been shown to be very powerful for <phrase>solving problems</phrase> approaching <phrase>real-world</phrase> complexity such as [1]. Using these <phrase>results</phrase> as a benchmark, we discuss the role that the discount factor may <phrase>play</phrase> in the quality of the learning process of a <phrase>deep Q</phrase>-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of <phrase>neural networks</phrase> when they are used in an approximate <phrase>Dynamic Programming</phrase> setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.
Deep <phrase>auto-encoder</phrase> <phrase>neural networks</phrase> in <phrase>reinforcement learning</phrase> This <phrase>paper</phrase> discusses the effectiveness of deep <phrase>auto-encoder</phrase> <phrase>neural networks</phrase> in visual <phrase>reinforcement learning</phrase> (<phrase>RL</phrase>) tasks. We propose a framework for combining deep <phrase>auto-encoder</phrase> <phrase>neural networks</phrase> (for learning compact feature spaces) with <phrase>recently-proposed</phrase> batch-mode <phrase>RL</phrase> <phrase>algorithms</phrase> (for learning policies). An emphasis is put on the <phrase>data</phrase>-efficiency of this combination and on studying the properties of the feature spaces automatically constructed by the deep <phrase>auto-encoders</phrase>. These feature spaces are empirically shown to adequately resemble existing similarities between observations and allow to learn useful policies. We propose several methods for improving the <phrase>topology</phrase> of the feature spaces making use of task-dependent <phrase>information</phrase> in <phrase>order</phrase> to further facilitate the policy-learning. Finally, we present first <phrase>results</phrase> on successfully learning good control policies using synthesized and real images.
Learning from mistakes: a <phrase>comprehensive</phrase> study on <phrase>real world</phrase> concurrency bug characteristics The <phrase>reality</phrase> of <phrase>multi-core</phrase> hardware has made concurrent programs pervasive. Unfortunately, writing correct concurrent programs is difficult. Addressing this challenge requires advances in multiple directions, including concurrency bug detection, concurrent program testing, concurrent <phrase>programming</phrase> <phrase>model</phrase> <phrase>design</phrase>, etc. Designing effective techniques in all these directions will significantly benefit from a <phrase>deep understanding</phrase> of <phrase>real world</phrase> concurrency bug characteristics. This <phrase>paper</phrase> provides the first (to the best of our <phrase>knowledge</phrase>) <phrase>comprehensive</phrase> <phrase>real world</phrase> concurrency bug characteristic study. Specifically, we have carefully examined concurrency bug patterns, manifestation, and fix strategies of 105 randomly selected <phrase>real world</phrase> concurrency <phrase>bugs</phrase> from 4 representative server and client <phrase>open-source</phrase> applications (<phrase>MySQL</phrase>, <phrase>Apache</phrase>, <phrase>Mozilla</phrase> and <phrase>OpenOffice</phrase>). Our study reveals several interesting findings and provides useful guidance for concurrency bug detection, testing, and concurrent <phrase>programming language</phrase> <phrase>design</phrase>. Some of our findings are as follows: (1) Around one third of the examined non-<phrase>deadlock</phrase> concurrency <phrase>bugs</phrase> are caused by violation to programmers' <phrase>order</phrase> intentions, which may not be easily expressed via synchronization primitives like locks and transactional memories; (2) Around 34% of the examined non-<phrase>deadlock</phrase> concurrency <phrase>bugs</phrase> involve multiple variables, which are not well addressed by existing bug detection tools; (3) About 92% of the examined concurrency <phrase>bugs</phrase> canbe reliably triggered by enforcing certain orders among no more than 4 <phrase>memory</phrase> accesses. This indicates that testing concurrent programs can <phrase>target</phrase> at exploring possible orders among every <phrase>small groups</phrase> of <phrase>memory</phrase> accesses, instead of among all <phrase>memory</phrase> accesses; (4) About 73% of the examinednon-<phrase>deadlock</phrase> concurrency <phrase>bugs</phrase> were not fixed by simply adding or changing locks, and many of the fixes were not correct at the first try, indicating the difficulty of reasoning concurrent execution by programmers.
Learning Where to Attend with <phrase>Deep Architectures</phrase> for Image Tracking We discuss an attentional <phrase>model</phrase> for simultaneous <phrase>object tracking</phrase> and recognition that is driven by gaze <phrase>data</phrase>. Motivated by theories of <phrase>perception</phrase>, the <phrase>model</phrase> consists of two interacting pathways, identity and control, intended to <phrase>mirror</phrase> the what and where pathways in <phrase>neuroscience</phrase> models. The identity pathway models object appearance and performs classification using deep (factored)-<phrase>restricted Boltzmann machines</phrase>. At each point in time, the observations consist of foveated images, with decaying resolution toward the periphery of the gaze. The control pathway models the location, orientation, scale, and speed of the attended object. The posterior distribution of these states is estimated with <phrase>particle</phrase> filtering. Deeper in the control pathway, we encounter an attentional mechanism that learns to select gazes so as to minimize tracking uncertainty. Unlike in our previous work, we introduce gaze selection strategies that operate in the presence of partial <phrase>information</phrase> and on a continuous <phrase>action</phrase> space. We show that a straightforward extension of the existing approach to the partial <phrase>information</phrase> setting <phrase>results</phrase> in poor performance, and we propose an <phrase>alternative</phrase> method based on modeling the reward surface as a <phrase>gaussian process</phrase>. This approach gives good performance in the presence of partial <phrase>information</phrase> and allows us to expand the <phrase>action</phrase> space from a small, discrete set of fixation points to a continuous domain.
Face Attribute Prediction Using Off-The-Shelf <phrase>Deep Learning</phrase> Networks Predicting attributes from face images in the wild is a challenging <phrase>computer vision</phrase> problem. To automatically describe face attributes from face containing images, traditionally one needs to <phrase>cascade</phrase> three technical blocks face localization, facial descriptor <phrase>construction</phrase>, and attribute classification in a pipeline. As a typical <phrase>classification problem</phrase>, face attribute prediction has been addressed using <phrase>deep learning</phrase>. <phrase>Current state</phrase>-of-the-<phrase>art</phrase> performance was achieved by using two cascaded <phrase>Convolutional Neural Networks</phrase> (CNNs), which were specifically trained to learn face localization and attribute description. In this <phrase>paper</phrase>, we experiment with an <phrase>alternative</phrase> way of employing the power of deep representations from CNNs. Combining with conventional face localization techniques, we use off-the-shelf architectures trained for <phrase>face recognition</phrase> to build facial descriptors. Recognizing that the describable face attributes are diverse, our face descriptors are constructed from different levels of the CNNs for different attributes to best facilitate face attribute prediction. Experiments on two <phrase>large datasets</phrase>, LFWA and CelebA, show that our approach is entirely comparable to the <phrase>state</phrase>-of-the-<phrase>art</phrase>. Our findings not only demonstrate an efficient face attribute prediction approach, but also raise an important question: how to leverage the power of off-the-shelf <phrase>CNN</phrase> representations for novel tasks.
Grounded Imagination: Challenge, <phrase>Paradox</phrase> and Inspiration Instruments: Disappearing Days (<phrase>Mobile</phrase> Minds), <phrase>Troubadour</phrase> (<phrase>Creativity</phrase> 2n). Abstract Two Disappearing Days and five <phrase>Troubadour</phrase> workshops have allowed us to explore a dual focus on <phrase>creativity</phrase> within the Disappearing Computer <phrase>Initiative</phrase> (<phrase>DC</phrase>) and the <phrase>paradox</phrase> of 'grounded imagination' in international and multidisciplinary <phrase>design</phrase> teams. We have worked '<phrase>Creativity</phrase> 2 n' addressed a dual focus on <phrase>creativity</phrase> with the <phrase>DC</phrase> programme. Many <phrase>DC</phrase> projects aiming to (1) support <phrase>creativity</phrase> in people's everyday practice and (2) spark <phrase>creativity</phrase> within multidisciplinary <phrase>design</phrase> teams. The concept of 'grounded imagination' describes the <phrase>paradox</phrase> of connecting a <phrase>deep understanding</phrase> of present practices with the process of creating new, appropriate, desirable and innovative future technologies. The methodologies we have designed and applied make for a hands-on approach to these issues. They include: speed-days, focus boards, field packs, inspiration packs, fieldwork, <phrase>design</phrase> interventions and animations. Outcomes for the projects involved and the <phrase>DC</phrase> <phrase>community</phrase> as a whole are: A number of important observations around '<phrase>creativity</phrase> in everyday practice'; a shared understanding of challenges involved in achieving grounded imagination in international and interdisciplinary <phrase>design</phrase> teams; a collection of practical methods that respond to these challenges; and shared cross-project collaborative work experiences. The issues we have addressed are highly relevant and challenging. Finding ways of addressing the challenges is a process, calling for ongoing experimentation and imagination. The practical approaches and methods we have introduced clearly help in achieving grounded imagination. The benefits of bringing together projects, fields, approaches and challenges, identifying common ground, and working together are threefold. First, all participants gain an opportunity to reflect upon their own work from a different perspective. Second, teams learn about a collection of new (and known) methods as they are applied to important project-specific and <phrase>DC</phrase> relevant questions. Third, collectively we have made intense, productive collaboration between (relative) strangers with diverse disciplinary perspectives possible shaping <phrase>DC</phrase> <phrase>community</phrase> experience and intra-project collaboration. Finally, Disappearing Days and Troubadours are effective, but demand that organisers (and participants) devote considerable time over and above the 1.5 days of actual contact to preparation, and reporting.
A <phrase>Mobile Device</phrase> and Online System with Contextual Familiarity and its Effects on <phrase>English</phrase> Learning on <phrase>Campus</phrase> or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or <phrase>commercial advantage and that copies bear</phrase> the full citation on the first page. <phrase>Copyrights</phrase> for components of this work owned by others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to <phrase>post on servers</phrase>, or to redistribute to lists, <phrase>requires prior specific permission</phrase> and/or a fee. Request permissions from the editors at kinshuk@ieee.org. ABSTRACT In this study, a <phrase>mobile device</phrase> and online system, StudentPartner, is proposed to help <phrase>students learn</phrase> <phrase>English</phrase> on <phrase>campus</phrase> using <phrase>multimedia</phrase> and <phrase>GPS</phrase> support. Two activities, exploring the <phrase>campus</phrase> in <phrase>English</phrase> and <phrase>English</phrase> presentation, were designed to stimulate students' deep engagement and interaction with the system. Since students are very familiar with the <phrase>campus</phrase> as context, these activities elicit interest and playfulness. An experiment using the proposed system was conducted on the <phrase>university</phrase> <phrase>campus</phrase> for six months. The <phrase>results</phrase> show that positive perceptions of an activity, especially that of exploring the <phrase>campus</phrase>, significantly influenced users' intentions to utilize the proposed system. In addition, it was found that users' performance in the <phrase>English</phrase> presentation activity was significantly correlated with their achievement in learning. Thus, the proposed system, when combined with these two activities is an effective and enjoyable method of learning <phrase>English</phrase> that utilizes the concepts of contextual familiarity and the exchange of ideas through presentations.
People detection through quantified fuzzy temporal rules The <phrase>knowledge</phrase> about the position and movement of people is of great importance in <phrase>mobile</phrase> <phrase>robotics</phrase> for implementing tasks such as <phrase>navigation</phrase>, mapping, localization, or humanrobot interaction. This <phrase>knowledge</phrase> enhances the robustness, reliability and performance of the <phrase>robot</phrase> control <phrase>architecture</phrase>. In this <phrase>paper</phrase>, a pattern classifier system for the detection of people using <phrase>laser</phrase> <phrase>range</phrase> finders <phrase>data</phrase> is presented. The approach is based on the quantified fuzzy temporal rules (QFTRs) <phrase>knowledge representation and reasoning</phrase> <phrase>paradigm</phrase>, that is able to analyze the <phrase>spatio-temporal</phrase> patterns that are associated to people. The pattern classifier system is a <phrase>knowledge base</phrase> made up of QFTRs that were learned with an <phrase>evolutionary algorithm</phrase> based on the <phrase>cooperative</phrase>-competitive approach together with token competition. A deep <phrase>experimental</phrase> study with a Pioneer <phrase>II</phrase> <phrase>robot</phrase> involving a five-fold <phrase>cross-validation</phrase> and several <phrase>runs</phrase> of the <phrase>genetic algorithm</phrase> has been done, showing a classification rate over 80%. Moreover, the characteristics of the <phrase>tests</phrase> represent complex and realistic conditions (people moving in groups, the <phrase>robot</phrase> moving in part of the experiments, and the existence of static and moving people). The operation of <phrase>mobile</phrase> <phrase>robots</phrase> in real environments, like <phrase>supermarkets</phrase>, <phrase>railway</phrase> <phrase>stations</phrase>, <phrase>hospitals</phrase>, etc., is generally characterized by the existence of people and moving objects in the surrounding. This fact needs to be considered when implementing tasks such as mapping or path planning, since discarding moving objects usually leads to errors and poor performance. The detection of people is particularly important for service <phrase>robots</phrase> and, fundamentally, for humanrobot interaction , where both moving people and also static people have to be detected. The detection of people is highly influenced by the type of <phrase>sensor</phrase> being used. The two types of sensors most widely employed for this purpose are cameras [14] and <phrase>range</phrase> finders (generally, <phrase>laser</phrase> <phrase>range</phrase> finders) [57]. The advantages of <phrase>laser</phrase> <phrase>range</phrase> finders are that they can directly measure objects <phrase>geometry</phrase>, distances <phrase>information</phrase> is accurate, the field of view is large, and <phrase>information</phrase> of the <phrase>probability</phrase> of occupancy of each <phrase>area</phrase> of the environment can be easily obtained. On the contrary, the quantity of <phrase>information</phrase> that can be extracted is <phrase>lower</phrase> than with a <phrase>camera</phrase> and, therefore, distinguishing among objects with similar geometric properties becomes much more difficult. Several proposals have been done for the detection of people with <phrase>laser</phrase> <phrase>range</phrase> finders. They can be grouped into three categories: those that are based on the difference of occupancy between consecutive <phrase>range</phrase> scans [716] 
Self-Annotation for <phrase>fine-grained</phrase> geospatial <phrase>relation extraction</phrase> A great deal of <phrase>information</phrase> on the Web is represented in both textual and structured form. The structured form is machine-readable and can be used to augment the textual <phrase>data</phrase>. We call this augmentation the annotation of texts with relations that are included in the structured <phrase>data</phrase> self-annotation. In this <phrase>paper</phrase>, we introduce self-annotation as a new <phrase>supervised learning</phrase> approach for developing and implementing a system that extracts <phrase>fine-grained</phrase> relations between entities. The main benefit of self-annotation is that it does not require manual labeling. The input of the learned <phrase>model</phrase> is a representation of the <phrase>free</phrase> text, its output struc-tured relations. Thus, the <phrase>model</phrase>, once learned, can be applied to any arbitrary <phrase>free</phrase> text. We describe the challenges for the self-annotation process and give <phrase>results</phrase> for a sample <phrase>relation extraction</phrase> system. To deal with the challenge of <phrase>fine-grained</phrase> relations, we implement and evaluate both shallow and <phrase>deep linguistic</phrase> analysis , focusing on <phrase>German</phrase>.
CS 224D: <phrase>Deep Learning</phrase> for <phrase>NLP</phrase> <phrase>Language</phrase> models compute the <phrase>probability</phrase> of occurrence of a number of words in a particular <phrase>sequence</phrase>. The <phrase>probability</phrase> of a <phrase>sequence</phrase> of
Click-through-based Deep Visual-<phrase>Semantic</phrase> Embedding for Image Search The problem of image search is mostly considered from the perspectives of feature-based <phrase>vector</phrase> <phrase>model</phrase> and image ranker learning. A fundamental issue that underlies the success of these approaches is the similarity learning between query and image. The need of image surrounding texts in feature-based <phrase>vector</phrase> <phrase>model</phrase>, however, makes the similarity sensitive to the quality of text descriptions. On the other, the image ranker learning can suffer from robustness problem, originating from the fact that <phrase>human</phrase> labeled query-image pairs do not always predict user search intention precisely. We demonstrate in this <phrase>paper</phrase> that the above two issues can be well mitigated by jointly exploring visual-<phrase>semantic</phrase> embedding and the use of click-through <phrase>data</phrase>. Specifically, we propose a novel click-through-based deep visual-<phrase>semantic</phrase> embedding (C-DVSE) <phrase>model</phrase> for learning query and image similarity. The proposed <phrase>model</phrase> consists of two components: a <phrase>deep convolutional</phrase> <phrase>neural networks</phrase> followed by an image embedding layer for learning visual embedding, and a <phrase>deep neural networks</phrase> for generating query <phrase>semantic</phrase> embedding. The objective of our <phrase>model</phrase> is to maximize the correlation between <phrase>semantic</phrase> (query) and visual (clicked image) embedding. When the visual-<phrase>semantic</phrase> embedding is learnt, query-image similarity can be directly computed by cosine similarity on this embedding space. On a <phrase>large-scale</phrase> click-based image dataset with 11.7 million queries and one million images, our <phrase>model</phrase> is shown to be powerful for keyword-based image search with <phrase>superior</phrase> performance over several <phrase>state</phrase>-of-the-<phrase>art</phrase> methods.
Promoting Constructive Activities That Support <phrase>Vicarious Learning</phrase> during Computer-based Instruction This article explores several ways computer-based instruction can be designed to support constructive activities and promote deep-level comprehension during <phrase>vicarious learning</phrase>. <phrase>Vicarious learning</phrase>, discussed in the first section, refers to <phrase>knowledge</phrase> acquisition under conditions in which the learner is not the addressee and does not physically interact in any way with the source of the content to be mastered. The second section describes <phrase>cognitive</phrase> <phrase>constructivism</phrase> from the standpoint of schema theory and the work of Bartlett (1932). The next section describes four principles of <phrase>curriculum</phrase> <phrase>design</phrase> that support constructive processes during <phrase>vicarious learning</phrase> and reviews the process of self-explanation and how computer prompted self-explanation supports constructive activities. <phrase>Research</phrase> showing the important role that overhearing deep-level <phrase>reasoning questions</phrase> plays in supporting constructive activities and deep-level learning is also described. In the next section, <phrase>vicarious learning</phrase> supported by deep-level <phrase>reasoning questions</phrase> is contrasted with tutoring as one kind of interactive learning. In the final section, some conclusions are drawn, a few empirical issues are discussed, and two caveats are noted. have created situations in which learners sometimes find themselves trying to understand course content in settings where they are observers (Brennan &
<phrase>Deep Belief</phrase> Networks Are Compact <phrase>Universal</phrase> Approximators <phrase>Deep Belief</phrase> Networks (DBN) are <phrase>generative models</phrase> with many layers of hidden causal variables, recently introduced by Hinton et al. (2006), along with a <phrase>greedy layer-wise</phrase> <phrase>unsupervised learning</phrase> <phrase>algorithm</phrase>. Building on Le Roux and Bengio (2008) and Sutskever and Hinton (2008), we show that deep but narrow generative networks do not require more parameters than shallow ones to achieve <phrase>universal</phrase> approximation. Exploiting the proof technique, we prove that deep but narrow <phrase>feed-forward</phrase> <phrase>neural networks</phrase> with sig-moidal units can represent any <phrase>Boolean</phrase> expression.
Learning to remove <phrase>multipath</phrase> distortions in Time-of-Flight <phrase>range</phrase> images for a <phrase>robotic arm</phrase> setup <phrase>Range</phrase> images captured by Time-of-Flight (ToF) cameras are corrupted with <phrase>multipath</phrase> distortions due to interaction between modulated <phrase>light</phrase> signals and scenes. The interaction is often complicated, which makes a <phrase>model</phrase>-based <phrase>solution</phrase> elusive. We propose a learning-<phrase>based approach</phrase> for removing the <phrase>multipath</phrase> distortions for a ToF <phrase>camera</phrase> in a <phrase>robotic arm</phrase> setup. Our approach is based on <phrase>deep learning</phrase>. We use the <phrase>robotic arm</phrase> to automatically collect a large amount of ToF <phrase>range</phrase> images containing various <phrase>multipath</phrase> distortions. The training images are automatically labeled by leveraging a <phrase>high</phrase> precision structured <phrase>light</phrase> <phrase>sensor</phrase> available only in the training time. In the <phrase>test</phrase> time, we apply the learned <phrase>model</phrase> to remove the <phrase>multipath</phrase> distortions. This allows our <phrase>robotic arm</phrase> setup to enjoy the speed and compact form of the ToF <phrase>camera</phrase> without compromising with its <phrase>range</phrase> measurement errors. We conduct extensive <phrase>experimental</phrase> validations and compare the <phrase>proposed method</phrase> to several baseline <phrase>algorithms</phrase>. The experiment <phrase>results</phrase> show that our <phrase>method achieves</phrase> 55% error reduction in <phrase>range</phrase> estimation and largely outperforms the baseline <phrase>algorithms</phrase>.
Designing Deeper <phrase>Learning Experiences</phrase> for Online Instruction Deeper learning promotes students' active engagement in <phrase>learning environments</phrase>, so they can continuously explore, reflect and produce <phrase>information</phrase> to build complex <phrase>knowledge</phrase> structures. Consequently, deeper learning has become a <phrase>major</phrase> focus of scholarly investigation and debate. Multiple studies have been conducted to describe the characteristics of deeper learning and to determine methods for infusing it into the <phrase>curriculum</phrase>. The <phrase>paper</phrase> starts with a description of deeper learning, reviews the body of existing <phrase>research</phrase> and presents guidelines for deeper learning that can be used in <phrase>online learning</phrase> environments. Deeper learning has been a focus of <phrase>higher education</phrase> discourse for more than three decades. The concept of <phrase>deep learning</phrase> was first mentioned in 1972 by Craik and Lockhart. They argued that <phrase>deep learning</phrase> involves <phrase>higher level</phrase> or active <phrase>cognitive</phrase> processing, as opposed to surface learning where students use <phrase>lower</phrase> level <phrase>cognitive</phrase> functions such as simple memorization or <phrase>rote learning</phrase>. Beattie, Collins, and McInnes (1997) furthered this idea and described deep versus surface learning in more detail: The deep approach, which implies that a <phrase>student</phrase> learns for understanding, is characterized by students who (1) seek to understand the issues and interact critically with the contents of particular teaching materials, (2) relate ideas to previous <phrase>knowledge</phrase> and experience and (3) examine the <phrase>logic</phrase> of the arguments and relate the evidence presented to the conclusions. The surface approach, which implies that a <phrase>student</phrase> learns simply to memorize facts, is characterized by students who (1) try simply to memorize parts of the content of teaching materials and accept the ideas and <phrase>information</phrase> given without question, (2) concentrate on memorizing facts without distinguishing any underlying principles or patterns and (3) are influenced by assessment requirements. Saljo introduced the term deep processing to describe <phrase>student</phrase> engagement with learning tasks (<phrase>Laird</phrase>, Shoup, Kuh, & Schwarz, 2008). In their view, the <phrase>adjective</phrase> deep referred to looking beyond the surface and understanding the underlying meaning of <phrase>knowledge</phrase>. Over the course of 1970s and 1980s, other researchers studied the same and proposed strategies and characteristics for each learning approach. For example, <phrase>Laird</phrase> et al. (2008) identified that students who use deeper learning approaches " read widely, combine a
<phrase>Deep Learning</phrase> for <phrase>NLP</phrase> (without Magic) 1 Abtract <phrase>Machine learning</phrase> is everywhere in today's <phrase>NLP</phrase>, but by and large <phrase>machine learning</phrase> amounts to numerical optimization of weights for <phrase>human</phrase> designed representations and features. The goal of <phrase>deep learning</phrase> is to explore how <phrase>computers</phrase> can take advantage of <phrase>data</phrase> to develop features and representations appropriate for complex interpretation tasks. This tuto-rial aims to <phrase>cover</phrase> the <phrase>basic</phrase> <phrase>motivation</phrase>, ideas, models and <phrase>learning algorithms</phrase> in <phrase>deep learning</phrase> for <phrase>natural language processing</phrase>. Recently, these methods have been shown to perform very well on various <phrase>NLP</phrase> tasks such as <phrase>language</phrase> modeling, POS tagging , <phrase>named entity</phrase> recognition, <phrase>sentiment analysis</phrase> and paraphrase detection, among others. The most attractive quality of these techniques is that they can perform well without any external hand-designed resources or time-intensive feature <phrase>engineering</phrase>. Despite these advantages, many researchers in <phrase>NLP</phrase> are not familiar with these methods. Our focus is on insight and understanding, using <phrase>graphical</phrase> illustrations and simple, intuitive derivations. The goal of the tutorial is to make the inner workings of these techniques transparent, intuitive and their <phrase>results</phrase> in-terpretable, rather than <phrase>black</phrase> boxes labeled " magic here ". The first part of the tutorial presents the basics of <phrase>neural networks</phrase>, neural word vectors, several simple models based on local <phrase>windows</phrase> and the <phrase>math</phrase> and <phrase>algorithms</phrase> of training via <phrase>backpropagation</phrase>. In this section applications include <phrase>language</phrase> modeling and POS tagging. In the second section we present recursive <phrase>neural networks</phrase> which can learn structured <phrase>tree</phrase> outputs as well as <phrase>vector</phrase> representations for phrases and sentences. We <phrase>cover</phrase> both equations as well as applications. We show how training can be achieved by a modified version of the <phrase>backpropagation</phrase> <phrase>algorithm</phrase> introduced before. These modifications allow the <phrase>algorithm</phrase> to work on <phrase>tree</phrase> structures. Applications include <phrase>sentiment analysis</phrase> and paraphrase detection. We also draw connections to recent work in <phrase>semantic</phrase> compositionality in <phrase>vector spaces</phrase>. The principle goal, again, is to make these methods appear intuitive and interpretable rather than mathematically confusing. By this point in the tutorial, the audience members should have a clear understanding of how to build a <phrase>deep learning</phrase> system for word-, sentence-and document-level tasks. The last part of the tutorial gives a <phrase>general</phrase> overview of the different applications of <phrase>deep learning</phrase> in <phrase>NLP</phrase>, including bag of words models. We will provide a discussion of <phrase>NLP</phrase>-oriented issues in mod-eling, interpretation, representational power, and optimization .
Characterizing Social Relations Via <phrase>NLP</phrase>-Based <phrase>Sentiment Analysis</phrase> We investigate and evaluate methods for the characterization of social relations from textual <phrase>communication</phrase> context, using <phrase>e</phrase>-<phrase>mail</phrase> as an example. Social relations are intrinsically characterized by the <phrase>Cartesian product</phrase> of weights on various axes (we employ valuation and intensity as examples). The prediction of these characteristics is performed by application of <phrase>unsupervised learning</phrase> <phrase>algorithms</phrase> on <phrase>meta-data</phrase>, <phrase>communication</phrase> <phrase>statistics</phrase>, and the <phrase>results</phrase> of <phrase>deep linguistic</phrase> analysis of the message body. Classification of sentiment polarity is chosen as the means of <phrase>linguistic</phrase> analysis. We find that prediction accuracy can be improved by introducing limited amounts of additional <phrase>information</phrase>.
Towards performing everyday manipulation activities This article investigates fundamental issues in scaling autonomous personal <phrase>robots</phrase> towards <phrase>open-ended</phrase> sets of everyday manipulation tasks which involve <phrase>high</phrase> complexity and vague job specifications. To achieve this, we propose a control <phrase>architecture</phrase> that synergetically integrates some of the most promising <phrase>artificial intelligence</phrase> (<phrase>AI</phrase>) methods that we consider as necessary for the performance of everyday manipulation tasks in <phrase>human</phrase> living environments: deep representations, probabilistic first-<phrase>order</phrase> learning and reasoning, and transformational planning of reactive behavior all of which are integrated in a coherent <phrase>high</phrase>-level <phrase>robot</phrase> control system: COGITO. We demonstrate the strengths of this combination of methods by realizing, as a <phrase>proof of concept</phrase>, an autonomous personal <phrase>robot</phrase> capable of setting a table efficiently using instructions from the <phrase>world wide web</phrase>. To do so, the <phrase>robot</phrase> translates instructions into <phrase>executable</phrase> <phrase>robot</phrase> plans, debugs its plan to eliminate behavior flaws caused by missing pieces of <phrase>information</phrase> and ambiguities in the instructions, optimizes its plan by revising the course of activity, and infers the most likely job from vague job description using probabilistic reasoning .
Refining Architectures of <phrase>Deep Convolutional</phrase> <phrase>Neural Networks</phrase> <phrase>Deep Convolutional</phrase> <phrase>Neural Networks</phrase> (CNNs) have recently evinced immense success for various <phrase>image recognition</phrase> tasks [11, 27]. However, a question of <phrase>paramount</phrase> importance is somewhat unanswered in <phrase>deep learning</phrase> <phrase>research</phrase> is the selected <phrase>CNN</phrase> optimal for the dataset in terms of accuracy and <phrase>model</phrase> size? In this <phrase>paper</phrase>, we intend to answer this question and introduce a novel strategy that alters the <phrase>architecture</phrase> of a given <phrase>CNN</phrase> for a specified dataset, to potentially enhance the original accuracy while possibly reducing the <phrase>model</phrase> size. We use two operations for <phrase>architecture</phrase> refinement, viz. stretching and symmetrical splitting. Stretching increases the number of <phrase>hidden units</phrase> (nodes) in a given <phrase>CNN</phrase> layer, while a symmetrical <phrase>split</phrase> of say K between two layers separates the input and output channels into K equal groups, and connects only the corresponding <phrase>input-output</phrase> <phrase>channel</phrase> groups. Our procedure starts with a <phrase>pre-trained</phrase> <phrase>CNN</phrase> for a given dataset, and optimally decides the stretch and <phrase>split</phrase> factors across the network to refine the <phrase>architecture</phrase>. We empirically demonstrate the necessity of the two operations. We evaluate our approach on two natural scenes attributes datasets, <phrase>SUN</phrase> Attributes [16] and CAMIT-NSAD [20], with architectures of GoogleNet and VGG-11, that are quite contrasting in their <phrase>construction</phrase>. We justify our choice of datasets, and show that they are interestingly distinct from each other, and together pose a challenge to our <phrase>architectural</phrase> refinement <phrase>algorithm</phrase>. Our <phrase>results</phrase> substantiate the usefulness of the <phrase>proposed method</phrase>.
Interconnections of <phrase>Quantum</phrase>, Machine and <phrase>Human</phrase> Learning In earlier work, this <phrase>author</phrase> has explained the robustness of nonlinear multilayer <phrase>machine learning</phrase> <phrase>algorithms</phrase> in terms of an intrinsic chaos of the <phrase>logistic map</phrase>. Moreover we have connected that dynamics to a spectral concentration which occurs in bounded-to-<phrase>free</phrase> <phrase>quantum</phrase> transitions. From these, one may formulate a fundamental irreversibility common to both machine and <phrase>quantum</phrase> learning. Second, in recent work this <phrase>author</phrase> has treated both the Bell and <phrase>Zeno paradoxes</phrase> of <phrase>quantum</phrase> measurement theory. Deep unresolved issues are exposed and analyzed. A fundamental new theorem on <phrase>quantum mechanical</phrase> reversibility is presented. From such viewpoint , one may see more deeply the issue of <phrase>decoherence</phrase> in any <phrase>quantum computing</phrase> <phrase>architecture</phrase>. Third, in our examinations of <phrase>human</phrase> learning, we compared actual <phrase>human</phrase> decision processes against those of several A.I. learning schemes. We were struck by the repeated tendency of humans to go to great lengths to avoid a choice that includes a contradiction. That will be contrasted with <phrase>quantum</phrase> learning, which permits, at least probabilistically, contradictory conclusions.
Learning <phrase>Deep Neural Networks</phrase> for <phrase>High</phrase> Dimensional Output Problems <phrase>State</phrase>-of-the-<phrase>art</phrase> <phrase>pattern recognition</phrase> methods have difficulty dealing with problems where the <phrase>dimension</phrase> of the output space is large. In this article, we propose a new framework based on <phrase>deep architectures</phrase> (e.g. <phrase>Deep Neural Networks</phrase>) in <phrase>order</phrase> to deal with this issue. <phrase>Deep architectures</phrase> have <phrase>proven</phrase> to be efficient for <phrase>high</phrase> dimensional input problems such as <phrase>image classification</phrase>, due to their ability to embed the input space. The main contribution of this article is the extension of the embedding procedure to both the input and output spaces in <phrase>order</phrase> to easily handle <phrase>high</phrase> dimensional output problems. Using this extension, inter-output dependencies can be modelled efficiently. This provides an interesting <phrase>alternative</phrase> to <phrase>probabilistic models</phrase> such as HMM and CRF. Preliminary experiments on <phrase>toy</phrase> datasets and <phrase>USPS</phrase> character <phrase>reconstruction</phrase> show <phrase>promising results</phrase>.
Thriving as a <phrase>data</phrase> miner in the <phrase>real world</phrase> Meaningful work is a deep <phrase>human</phrase> need. We all yearn to contribute to something greater than ourselves, be listened to, and work alongside <phrase>friendly</phrase> <phrase>peers</phrase>. <phrase>Data mining</phrase> consulting is a powerful way to use technical skills and gain these great side benefits. The power of analytics and its <phrase>high</phrase> return on <phrase>investment</phrase> makes one's expertise welcome virtually everywhere. And the <phrase>variety</phrase> of projects and domains encountered leads to continual learning as new problems are met and solved. Teaching and writing are possible, and there is great satisfaction in seeing one's work actually implemented and used, potentially touching millions. Still, in <phrase>industry</phrase>, one has the joy and hazards of working closely with other humans, where final success can depend as much on others as oneself, and on social as well as technical issues. In my experience, <phrase>business</phrase> <phrase>risk</phrase> strongly outweighs technical <phrase>risk</phrase> in whether a <phrase>solution</phrase> is used. I will share some hard-won <phrase>lessons learned</phrase> on how to best succeed, both technically and socially, in the <phrase>results</phrase>-oriented world of <phrase>industry</phrase>.
Massively Parallel Methods for <phrase>Deep Reinforcement Learning</phrase> We present the first massively distributed <phrase>architecture</phrase> for <phrase>deep reinforcement learning</phrase>. This <phrase>architecture</phrase> uses four main components: parallel <phrase>actors</phrase> that generate new behaviour; parallel learners that are trained from stored experience ; a distributed <phrase>neural network</phrase> to represent the value <phrase>function</phrase> or behaviour policy; and a distributed store of experience. We used our <phrase>architecture</phrase> to implement the <phrase>Deep Q</phrase>-Network <phrase>algorithm</phrase> (DQN) (Mnih et al., 2013). Our distributed <phrase>algorithm</phrase> was applied to 49 <phrase>games</phrase> from <phrase>Atari 2600</phrase> <phrase>games</phrase> from the <phrase>Arcade</phrase> <phrase>Learning Environment</phrase> , using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 <phrase>games</phrase> and also reduced the wall-time required to achieve these <phrase>results</phrase> by an <phrase>order</phrase> of <phrase>magnitude</phrase> on most <phrase>games</phrase>.
Age and <phrase>Gender</phrase> Estimation of Unfiltered Faces This <phrase>paper</phrase> concerns the estimation of facial attributes namely, age and <phrase>gender</phrase> from images of faces acquired in challenging, " in the wild " conditions. This problem has received far less attention than the related problem of <phrase>face recognition</phrase>, and in particular, has not enjoyed the same dramatic improvement in capabilities demonstrated by contemporary <phrase>face recognition</phrase> systems. Here we address this problem by making the following contributions. First, (i), in answer to one of the key problems of age estimation <phrase>research</phrase> absence of <phrase>data</phrase> we offer a unique dataset of face images, labeled for age and <phrase>gender</phrase>, acquired by <phrase>smart-phones</phrase> and other <phrase>mobile</phrase> devices, and uploaded without manual filtering to online image repositories. We show the images in our collection to be more challenging than those offered by other face-photo benchmarks. (<phrase>ii</phrase>) We describe the dropout-<phrase>SVM</phrase> approach used by our system for face attribute estimation, in <phrase>order</phrase> to avoid over-fitting. This method, inspired by the dropout learning techniques now popular with <phrase>deep belief</phrase> networks, is applied here for training <phrase>support vector machines</phrase>, to our <phrase>knowledge</phrase>, for the first time. Finally, (iii), we present a robust face alignment technique which explicitly considers the uncertainties of facial feature detectors. We <phrase>report</phrase> extensive <phrase>tests</phrase> analyzing both the difficulty levels of contemporary benchmarks, as well as the capabilities of our own system. These show our method to outperform <phrase>state</phrase>-of-the-<phrase>art</phrase> by a wide margin.
An Analysis of the <phrase>Education</phrase> Category of Apple's <phrase>App Store</phrase> The mission of the <phrase>Joan Ganz Cooney</phrase> <phrase>Center</phrase> at <phrase>Sesame Workshop</phrase> is to <phrase>harness</phrase> <phrase>digital media</phrase> technologies to advance children's learning. The <phrase>Center</phrase> supports <phrase>action research</phrase>, encourages partnerships to connect <phrase>child development</phrase> experts and educators with <phrase>interactive media</phrase> and <phrase>technology</phrase> leaders, and mobilizes <phrase>public</phrase> and <phrase>private</phrase> <phrase>investment</phrase> in promising and <phrase>proven</phrase> new <phrase>media</phrase> technologies for children. The <phrase>Joan Ganz Cooney</phrase> <phrase>Center</phrase> has a deep commitment toward dissemination of useful and timely <phrase>research</phrase>. Working closely with our Cooney Fellows, national advisors, <phrase>media</phrase> scholars, and practitioners, the <phrase>Center</phrase> publishes <phrase>industry</phrase>, policy, and <phrase>research</phrase> briefs examining key issues in the Jield of <phrase>digital media</phrase> and learning. No part of this publication may be reproduced or transmitted in any form or by any means, <phrase>electronic</phrase> or mechanical, including photocopy, or any <phrase>information</phrase> storage and retrieval system, without permission from the <phrase>Joan Ganz Cooney</phrase> <phrase>Center</phrase> at <phrase>Sesame Workshop</phrase>.
Deep <phrase>Unsupervised Feature Learning</phrase> for <phrase>Natural Language Processing</phrase> Statistical <phrase>natural language processing</phrase> (<phrase>NLP</phrase>) builds models of <phrase>language</phrase> based on statistical features extracted from the input text. We investigate <phrase>deep learning</phrase> methods for <phrase>unsupervised feature learning</phrase> for <phrase>NLP</phrase> tasks. Recent <phrase>results</phrase> indicate that features learned using <phrase>deep learning</phrase> methods are not a <phrase>silver bullet</phrase> and do not always <phrase>lead</phrase> to improved <phrase>results</phrase>. In this work we hypothesise that this is the result of a disjoint training protocol which <phrase>results</phrase> in mismatched word representations and classifiers. We also hypothesise that modelling <phrase>long</phrase>-<phrase>range</phrase> dependencies in the input and (separately) in the output layers would further <phrase>improve performance</phrase>. We suggest methods for overcoming these limitations, which will form part of our final <phrase>thesis</phrase> work.
Special <phrase>Track</phrase> on <phrase>Intelligent Tutoring</phrase> Systems <phrase>Intelligent tutoring</phrase> systems emerge from a multidisciplinary field of study that draws upon <phrase>artificial intelligence</phrase>, <phrase>computer science</phrase>, and <phrase>cognitive science</phrase> to create instructional technologies that offer immediate <phrase>feedback</phrase> and individualized instruction. Broadly construed, most <phrase>intelligent tutoring</phrase> systems can be characterized as having two loops: an outer loop and an inner loop. The outer loop intelligently selects the next relevant task for the <phrase>student</phrase> to complete. The inner loop iterates over individual <phrase>problem-solving</phrase> steps and provides contextually relevant <phrase>feedback</phrase> and instructional guidance. The ultimate goal of an <phrase>intelligent tutoring</phrase> system is to <phrase>promote deep learning</phrase> that persists over time, transfers to new domains, and accelerates future learning. In <phrase>general</phrase>, the goal of the <phrase>track</phrase> is to bring together an international group of scientists to present <phrase>current research</phrase>, <phrase>design</phrase>, and empirical evaluations of their <phrase>tutoring systems</phrase>. This <phrase>track</phrase> is meant to inform researchers on the <phrase>recent developments</phrase> in both the <phrase>design</phrase> and evaluation of <phrase>tutoring systems</phrase>. These projects and <phrase>experimental</phrase> studies identify, investigate, and (begin to) resolve issues that relate to <phrase>intelligent tutoring</phrase> systems. Topics include <phrase>game</phrase>-based, <phrase>narrative</phrase>-based and virtual <phrase>learning environments</phrase>; <phrase>natural language processing</phrase> and dialogue in <phrase>tutoring systems</phrase>; modeling and shaping affective states; <phrase>metacognition</phrase>; gaming the system; ill-defined domains; educational <phrase>data mining</phrase>; authoring tools for non-experts; adaptive educational <phrase>hypermedia</phrase>; collaborative and group learning; open learner modeling; <phrase>ontology engineering</phrase> for educational purposes; novel interfaces ; <phrase>human</phrase> computer interaction in educational settings; <phrase>design</phrase> decisions to increase engagement ; and assistive technologies for learners with special needs.
<phrase>Phase Transitions</phrase>: A New <phrase>Paradigm</phrase> for Evaluating Complexity in Learning and other <phrase>Complex Systems</phrase> The study of the <phrase>computational complexity</phrase> of <phrase>algorithms</phrase> takes a central place in <phrase>computer science</phrase>. Since its beginning, scientists have studied problems from the complexity point of view, categorizing their behaviour into a well-known <phrase>complexity class</phrase> hierarchy. Many interesting problems have proved to be intractable. Among these, a class of problems that are particularly prone to a dramatic increase in <phrase>computational complexity</phrase> with increasing problem size is the class of <phrase>combinatorial</phrase> problems, which include learning problems. Recently, it has been uncovered that computational problems show <phrase>critical phenomena</phrase>, similar to those that emerge in physical many-body systems. In particular, learning problems have shown to be the subject of the emergence of <phrase>phase transitions</phrase>, which allow a new <phrase>paradigm</phrase> for evaluating complexity to be considered. Thus, new <phrase>paradigm</phrase> is based on the notion of " typical " complexity instead of " worst case " complexity. For computational systems, the discovery of a <phrase>phase transition</phrase> has important consequences. First of all, the <phrase>phase transition</phrase> <phrase>region</phrase> contains the most difficult problem instances, for which the <phrase>computational complexity</phrase> increases exponentially when the problem size increases. Then, the <phrase>phase transition</phrase> can be used as a source of " difficult " <phrase>test</phrase> problems for assessing the properties and the power of <phrase>algorithms</phrase>, and to compare them on meaningful problem instances on a parity base. Moreover, very small variations of the problem parameters may induce very large variations in the algorithm's behavior and/or in the types of <phrase>solution</phrase>. Then, the <phrase>knowledge</phrase> of the critical value allows the user to roughly predict the behavior. Moreover, by exploiting further the analogy with physical systems, it is possible to enter into the deep structure of the problem and of its solutions; the system's behavior near the <phrase>phase transition</phrase> allows a microscopic view of the <phrase>solution</phrase> space. This fact not only offers the possibility of a <phrase>deeper understanding</phrase> of <phrase>algorithms</phrase>' properties, but opens the way to the introduction of new effective <phrase>algorithms</phrase>. In this <phrase>talk</phrase>, we will show the far-reaching effects of the presence of <phrase>phase transitions</phrase> in various learning approaches. Moreover, connections will be established with fundamental problem classes in <phrase>computer science</phrase>, such as the <phrase>satisfiability</phrase> problem and the <phrase>constraint satisfaction problem</phrase> classes, and with <phrase>statistical physics</phrase> approaches, which offered some very effective <phrase>algorithms</phrase>. Finally we will show how <phrase>phase transitions</phrase> are rather ubiquitous in both natural and <phrase>artificial</phrase> systems, including <phrase>human</phrase> vision and neural system.
<phrase>Peak</phrase>-load Transmission Pricing for the <phrase>Ieee</phrase> Reliability <phrase>Test</phrase> System <phrase>Peak</phrase>-load Transmission Pricing for the <phrase>Ieee</phrase> Reliability <phrase>Test</phrase> System In <phrase>order</phrase> to facilitate the use of inexpensive generation when the existing transmission system creates obstacle to the optimal power transfer, this <phrase>thesis</phrase> analyzes the <phrase>basic</phrase> <phrase>trade</phrase>-off between using expensive generation and investing in transmission enhancement. Understanding this <phrase>trade</phrase>-off has taken on a new importance as the <phrase>electric utility</phrase> <phrase>industry</phrase> undergoes <phrase>reconstruction</phrase> from being a regulated <phrase>monopoly</phrase> into serving competitive generation. This <phrase>thesis</phrase> continues the work started by Lecinq [8], that has introduced the <phrase>basic</phrase> notions of an optimal transmission system and a <phrase>peak</phrase>-load pricing mechanism capable of recovering the transmission enhancement investments. However, the main contribution of this <phrase>thesis</phrase> is an in depth study of transmission provision and <phrase>peak</phrase>-load pricing on a relatively large power system, namely, a 24 <phrase>bus</phrase> <phrase>IEEE</phrase> Reliability <phrase>Test</phrase> System' (often used as an <phrase>IEEE</phrase> <phrase>test</phrase> standard). In addition, <phrase>MATLAB</phrase>-based <phrase>software</phrase> was developed to accomplish the objective of <phrase>economic efficiency</phrase>, by valuing <phrase>trade</phrase>-offs between the cost of expensive generation and the transmission enhancement cost. In <phrase>order</phrase> to understand the implications of the <phrase>peak</phrase>-load pricing mechanism, various simulations, regarding the effect of the type of transmission pricing on the overall <phrase>economic efficiency</phrase>, were performed using this <phrase>software</phrase> on the <phrase>IEEE</phrase> <phrase>RTS</phrase>. Finally, the <phrase>software</phrase> developed here could be used as a <phrase>basic</phrase> transmission enhancement planning tool. I wish to <phrase>express my deep</phrase> gratitude to Marija Ilic, my advisor and foremost supporter. I have learned a lot working with her during my graduate study at <phrase>MIT</phrase>, and her advice and guidance were invaluable for this work. The financial support provided by the M.I.T. <phrase>Consortium</phrase> on Transmission Provision and Pricing is also greatly appreciated. Finally, I owe myself to my <phrase>family</phrase>, whose confidence and <phrase>love</phrase> were with me all these years. Without them, my personal and <phrase>academic</phrase> accomplishments would be meaningless.
<phrase>Semantic Web</phrase> applications: a framework for <phrase>industry</phrase> and <phrase>business</phrase> exploitation - What is needed for the <phrase>adoption</phrase> of the <phrase>Semantic Web</phrase> from the market and <phrase>industry</phrase> The <phrase>Semantic Web</phrase> <phrase>Research</phrase> has resulted in the last years in significant outcomes. Various industries have adopted <phrase>semantic web</phrase> technologies, while the " <phrase>deep web</phrase> " is still pursuing the critical transformation point, in which the majority of <phrase>data</phrase> found on <phrase>deep web</phrase> will be exploited through <phrase>semantic web</phrase> value layers. In this article we analyze the <phrase>Semantic Web</phrase> applications from a " market " perspective. We are setting the key requirements for <phrase>Real World</phrase> <phrase>information</phrase> systems <phrase>semantic web</phrase> enabled and we discuss the <phrase>major</phrase> difficulties for the <phrase>semantic web</phrase> uptake that has been delayed. This article contributes to the <phrase>literature</phrase> of <phrase>semantic web</phrase> and <phrase>knowledge management</phrase> providing a context for discourse towards best practices on <phrase>semantic web</phrase> based <phrase>information</phrase> systems. (2008) '<phrase>Semantic Web</phrase> applications: A framework for <phrase>industry</phrase> and <phrase>business</phrase> exploitation-What is it needed for a successful <phrase>semantic web</phrase> based application', Int. <phrase>semantic web</phrase>, <phrase>knowledge management</phrase> and <phrase>e</phrase>-learning, with more than 100 publications in these areas. He has co-edited / co-edits, 25 special issues in <phrase>semantic web</phrase>, <phrase>human</phrase>-computer interaction, <phrase>content management</phrase> with special emphasis on <phrase>copyright</phrase> <phrase>management</phrase> and <phrase>e</phrase>-<phrase>business</phrase>, with more than 40 publications in these areas. He has edited the <phrase>book</phrase> " <phrase>Semantic Web</phrase> for <phrase>Business</phrase>: Cases and Applications " , which presents a set of <phrase>Semantic Web</phrase> <phrase>business</phrase> cases.
Exploring adolescent's STEM learning through scaffolded <phrase>game design</phrase> This <phrase>paper</phrase> presents the findings of two <phrase>case studies</phrase> concentrating on the <phrase>learning experiences</phrase> of disadvantaged <phrase>middle school</phrase> children participating in The <phrase>Science</phrase> and <phrase>Art</phrase> of <phrase>Game Design</phrase> (SAGD) and Globaloria, <phrase>learning environments</phrase> intended to teach skills in <phrase>Science</phrase>, <phrase>Technology</phrase>, <phrase>Engineering</phrase>, and <phrase>Mathematics</phrase>, through <phrase>educational game</phrase> <phrase>design</phrase> within informal and <phrase>school</phrase>-based settings. In SAGD, youth are introduced to the STEM principles underlying modern computer <phrase>games</phrase> through a two-part <phrase>curriculum</phrase> that takes a <phrase>reverse engineering</phrase> approach to <phrase>educational game</phrase> <phrase>design</phrase>. It begins with <phrase>Gamestar</phrase> Mechanic, a <phrase>web-based</phrase> <phrase>role-playing game</phrase> that encourages students to think of <phrase>games</phrase> as systems made up of <phrase>game</phrase>-specific components and principles that are learned by playing <phrase>games</phrase>, repairing dysfunctional <phrase>games</phrase>, and creating new ones for sharing and critique in an online communities [1]. The second part encourages them to learn to use these <phrase>design</phrase> principles and apply them to solving of problems requiring computational thinking [2] within the <phrase>design</phrase> of <phrase>games</phrase> centered on STEM subjects using <phrase>Microsoft</phrase> Kodu, a <phrase>Microsoft</phrase> 3D <phrase>game</phrase> creation tool. Globaloria is a <phrase>learning environment</phrase> designed for <phrase>middle school</phrase> classrooms where <phrase>students learn</phrase> STEM concepts in the process of learning to <phrase>design</phrase> computer <phrase>games</phrase> using the <phrase>Adobe Flash</phrase> platform. Globaloria classrooms are designed around constructionist pedagogies, and feature a project-based <phrase>curriculum</phrase> supported by a framework of <phrase>Web 2.0</phrase> technologies, and an <phrase>online community</phrase> of <phrase>school</phrase> classrooms, educators, and <phrase>professional</phrase> <phrase>game</phrase> designers. Using multimodal content and <phrase>discourse analysis</phrase>, the study examined the <phrase>evolution</phrase> of students' STEM learning and <phrase>literacy</phrase> in these two contexts, as a <phrase>function</phrase> of their changes in <phrase>language</phrase> use, <phrase>design</phrase> strategies, and <phrase>game</phrase> artifact <phrase>production</phrase>. Findings suggest that scaffolded <phrase>game design</phrase> can provide an effective context for students to develop deep understandings and engagement with STEM subjects, in forms valued within the <phrase>21st century</phrase> workplace.
<phrase>Large-scale</phrase> Hierarchical Topic Models In the past decade, a number of advances in topic modeling have <phrase>produced</phrase> sophisticated models that are capable of generating hierarchies of topics. One challenge for these models is <phrase>scalability</phrase>: they are incapable of working at the massive scale of millions of documents and hundreds of thousands of terms. We address this challenge with a technique that learns a hierarchy of topics by iteratively applying topic models and processing subtrees of the hierarchy in parallel. This approach has a number of <phrase>scalability</phrase> advantages compared to existing techniques, and shows <phrase>promising results</phrase> in experiments assessing runtime and <phrase>human</phrase> evaluations of quality. We detail extensions to this approach that may further improve hierarchical topic modeling for <phrase>large-scale</phrase> applications. 1 <phrase>Motivation</phrase> With massive datasets and corresponding computational resources readily available, the <phrase>Big Data</phrase> movement aims to provide deep insights into <phrase>real-world</phrase> <phrase>data</phrase>. Realizing this goal can require new approaches to well-studied problems. Complex models that, for example, incorporate many dependencies between parameters have alluring <phrase>results</phrase> for small datasets and <phrase>single</phrase> machines but are difficult to adapt to the <phrase>Big Data</phrase> <phrase>paradigm</phrase>. Topic models are an interesting example of this phenomenon. In the last decade, a number of sophisticated techniques have been developed to <phrase>model</phrase> collections of text, from <phrase>Latent Dirichlet Allocation</phrase> (LDA)[1] through extensions using statistical machinery such as the nested <phrase>Chinese</phrase> <phrase>Restaurant</phrase> Process [2][3] and <phrase>Pachinko</phrase> Allocation[4]. One strength of such approaches is the ability to <phrase>model</phrase> topics in a hierarchical <phrase>fashion</phrase>. Coarse and fine topics are learned jointly, creating a <phrase>synergy</phrase> at <phrase>multiple levels</phrase> from shared parameters. However, past experiments have focused on small corpora, such as sampled abstracts from <phrase>journals</phrase> with only thousands of documents and terms. Operating on the scale of text collections the size of <phrase>Wikipedia</phrase>-millions of documents with millions of terms-is beyond such models using current inference techniques. While complex models have shown strong analytical <phrase>results</phrase>, simpler models that use fewer parameters and produce less structured output are being used at scale. In recent years, many parallel versions of LDA (eg. [5][6][7]) have been developed, capable of summarizing web-scale collections. Although these tools are a boon for analyzing massive amounts of <phrase>data</phrase>, they lack the ability to learn a hierarchical representation of topics. This situation highlights a common dilemma in <phrase>Big Data</phrase>: whether we can have our <phrase>cake</phrase> (models with rich output) and eat it too (operate on massive datasets). We propose a <phrase>solution</phrase> that addresses this 
Notes on <phrase>Algebraic</phrase>-geometric Codes Introduction Ideas from <phrase>algebraic geometry</phrase> became useful in <phrase>coding theory</phrase> after Goppa's <phrase>construction</phrase> [8]. He had the beautiful idea of associating to a curve X defined over F q , the <phrase>finite field</phrase> with q elements, a code C. This code, called <phrase>Algebraic</phrase>-Geometric (AG) code, is constructed from two divisors D and G on X , where one of them, say D, is the sum of n distinct F q-rational points of X. It turns out that the minimum distance d of C satisfies d n deg(G). This is one of the main features of Goppa's <phrase>construction</phrase>. In <phrase>general</phrase> there is no <phrase>lower</phrase> bound available on the minimum distance of a code. This bound is meaningful only if n is large enough, then it is of considerable interest to do <phrase>research</phrase> on curves with " many rational points " ; see e.g. [6]. The purpose of these notes is not to survey the vast body of <phrase>literature</phrase> on AG codes but just to provide a <phrase>short</phrase> and possibly plain introduction to this subject. Hence, we will bypass most of all the underlying <phrase>Algebraic Geometry</phrase>. This has two <phrase>major</phrase> drawbacks: firstly we can deal only with a limited class of AG codes, secondly the deep theorems on which AG codes rely are presented without proof. Nonetheless, we believe that such presentation is somehow more useful to the beginning <phrase>student</phrase>, and we hope that it may give some <phrase>motivation</phrase> to learn the subject in all its depth and beauty. These notes are based on a series of lectures given in May 2003 at the <phrase>Mathematical</phrase> <phrase>Department</phrase> of <phrase>KTH</phrase> in <phrase>Stockholm</phrase>.
Why Does <phrase>Unsupervised Pre-training</phrase> Help <phrase>Deep Learning</phrase>? Much recent <phrase>research</phrase> has been devoted to <phrase>learning algorithms</phrase> for <phrase>deep architectures</phrase> such as <phrase>Deep Belief</phrase> Networks and stacks of <phrase>auto-encoder</phrase> variants with impressive <phrase>results</phrase> being obtained in several areas, mostly on vision and <phrase>language</phrase> datasets. The best <phrase>results</phrase> obtained on <phrase>supervised learning</phrase> tasks often involve an unsu-pervised learning component, usually in an unsu-pervised <phrase>pre-training</phrase> phase. The main question investigated here is the following: why does un-supervised <phrase>pre-training</phrase> work so well? Through extensive experimentation, we explore several possible explanations discussed in the <phrase>literature</phrase> including its <phrase>action</phrase> as a regularizer (Erhan et al., 2009b) and as an aid to optimization (Bengio et al., 2007). Our <phrase>results</phrase> build on the work of Erhan et al. (2009b), showing that <phrase>unsupervised pre-training</phrase> appears to <phrase>play</phrase> predominantly a reg-ularization role in subsequent supervised training. However our <phrase>results</phrase> in an online setting, with a virtually unlimited <phrase>data stream</phrase>, point to a somewhat more nuanced interpretation of the roles of optimization and regularization in the un-supervised <phrase>pre-training</phrase> effect.
IR and <phrase>AI</phrase>: Traditions of Representation and <phrase>Anti</phrase>-representation in <phrase>Information Processing</phrase> The <phrase>paper</phrase> is concerned with the role of conceptual representations in access to <phrase>information</phrase>, as for example, from the <phrase>World Wide Web</phrase>. It contrasts two quite diierent traditions for doing this: <phrase>Information Retrieval</phrase> (IR) and more recently <phrase>Information Extraction</phrase> (IE), a development of the <phrase>natural language processing</phrase> tradition within Artiicial <phrase>Intelligence</phrase> (<phrase>AI</phrase>). The former has been statistical in <phrase>nature</phrase> and largely representation-<phrase>free</phrase> (though we discuss exceptions), while the latter has been based on representations making use of <phrase>ontologies</phrase> and lexicons in <phrase>semantics</phrase> and grammars in <phrase>syntax</phrase>. However, this distinction has been eroded by the growth in recent years of <phrase>machine learning</phrase> methods in IE, which have attempted to match IE performance but with methods less committed to representations: some have no representations, and some seek to learn them automatically from cases of their assignment. We discuss ways of resolving this <phrase>division</phrase> of approaches, a deep and historical issue about the ultimate role of representations in <phrase>information</phrase> access. We suggest that modes of use of the Web (e.g. the use of <phrase>short</phrase> questions by real users rather than the <phrase>long</phrase> artiiciaqueries' that <phrase>statistical methods</phrase> require) will tend to favour representational methods. We then discuss the crucial example of <phrase>question-answering</phrase> in a web environment of <phrase>information</phrase> access, as exempliied in the recent TREC competition <phrase>track</phrase> on <phrase>question answering</phrase>, and suggest that, although indecisive at the moment, this is an ideal forum in which the old issue of conceptual representations may be settled.
Text-col: a Tool for Active <phrase>Reading</phrase> abstract Traditionally, much of the efforts to develop computer-based tools has been concentrated on developing <phrase>production</phrase> or authoring tools, such as word processors, <phrase>drawing</phrase> programs, and so on, and not so many consumption or <phrase>reading</phrase> tools have been developed except <phrase>Web browsers</phrase> and different kinds of <phrase>media</phrase> players. Authoring plays an important role in the learning process, and good tools are needed, but readingespecially active readingand exploring are is at least as important. Traditionally, computer-mediated texts have little support for an active way of <phrase>reading</phrase>. This means that <phrase>reading</phrase> computer-mediated texts, as on the WWW, tends to be a very passive form of <phrase>reading</phrase>. The development of Text-Col addresses this problem. Text-Col is a tool designed to support readers in deep processing of texts by letting the readers change appearance of the <phrase>text based</phrase> on different strategies for categorizing words. Text-Col is a <phrase>reading</phrase> tool and exploring tool aimed to make the <phrase>reading</phrase> process more active.
Using '<phrase>Low-cost</phrase>' Learning Features for <phrase>Pronoun</phrase> Resolution We investigate a <phrase>machine learning</phrase> approach to <phrase>Portuguese</phrase> <phrase>pronoun</phrase> resolution. We presently focus on so-called '<phrase>low-cost</phrase>' learning features readily obtainable from the output of a part-of-speech tagger, and we largely bypass <phrase>deep syntactic</phrase> and <phrase>semantic</phrase> analysis. Preliminary <phrase>results</phrase> show significant improvement in resolution precision and recall, and are comparable to existing <phrase>rule-based</phrase> approaches for the <phrase>Portuguese language</phrase> spoken in <phrase>Brazil</phrase>.
<phrase>High</phrase>-Dimensional <phrase>Probability</phrase> Estimation with Deep <phrase>Density</phrase> Models One of the fundamental problems in <phrase>machine learning</phrase> is the estimation of a <phrase>probability distribution</phrase> from <phrase>data</phrase>. Many techniques have been proposed to study the structure of <phrase>data</phrase>, most often building around the assumption that observations lie on a <phrase>lower</phrase>-dimensional <phrase>manifold</phrase> of <phrase>high</phrase> <phrase>probability</phrase>. It has been more difficult, however, to exploit this insight to build explicit, tractable <phrase>density</phrase> models for <phrase>high</phrase>-dimensional <phrase>data</phrase>. In this <phrase>paper</phrase>, we introduce the deep <phrase>density</phrase> <phrase>model</phrase> (DDM), a new approach to <phrase>density estimation</phrase>. We exploit insights from <phrase>deep learning</phrase> to construct a <phrase>bijective</phrase> map to a representation space, under which the transformation of the distribution of the <phrase>data</phrase> is approximately factorized and has identical and known marginal densities. The simplicity of the latent distribution under the <phrase>model</phrase> allows us to feasibly explore it, and the invertibility of the map to characterize contraction of measure across it. This enables us to compute normalized densities for out-of-sample <phrase>data</phrase>. This combination of tractability and flexibility allows us to tackle a <phrase>variety</phrase> of probabilistic tasks on <phrase>high</phrase>-dimensional datasets, including: rapid computation of normalized densities at <phrase>test</phrase>-time without evaluating a <phrase>partition</phrase> <phrase>function</phrase>; generation of samples without MCMC; and characterization of the joint <phrase>entropy</phrase> of the <phrase>data</phrase>.
Towards the <phrase>Design</phrase> of an <phrase>End-to-End</phrase> Automated System for Image and <phrase>Video</phrase>-based Recognition Over many decades, researchers working in <phrase>object recognition</phrase> have longed for an <phrase>end-to-end</phrase> automated system that will simply accept 2D or 3D image or videos as inputs and output the <phrase>labels</phrase> of objects in the <phrase>input data</phrase>. <phrase>Computer vision</phrase> methods that use representations derived based on geometric, <phrase>radiometric</phrase> and neural considerations and statistical and structural matchers and <phrase>artificial neural network</phrase>-based methods where a <phrase>multi-layer</phrase> network learns the mapping from inputs to class <phrase>labels</phrase> have provided competing approaches for <phrase>image recognition</phrase> problems. Over the last four years, methods based on <phrase>Deep Convolutional</phrase> <phrase>Neural Networks</phrase> (DCNNs) have shown impressive performance improvements on <phrase>object detection</phrase>/recognition challenge problems. This has been made possible due to the availability of large annotated <phrase>data</phrase>, a better understanding of the non-linear mapping between image and class <phrase>labels</phrase> as well as the affordability of <phrase>GPUs</phrase>. In this <phrase>paper</phrase>, we present a brief <phrase>history</phrase> of developments in <phrase>computer vision</phrase> and <phrase>artificial neural networks</phrase> over the last forty years for the problem of image-based recognition. We then present the <phrase>design</phrase> details of a <phrase>deep learning</phrase> system for <phrase>end-to-end</phrase> unconstrained face verification/recognition. Some open issues regarding DCNNs for <phrase>object recognition</phrase> problems are then discussed. We caution the readers that the views expressed in this <phrase>paper</phrase> are from the authors and authors only!
An international approach to developing <phrase>information technology</phrase> (IT) <phrase>literacy</phrase> in schools based on critical <phrase>consciousness</phrase> This <phrase>paper</phrase> argues that <phrase>Information Technology</phrase> (IT) needs to be demystified, through critical <phrase>consciousness</phrase>, for educators to focus on IT as a means to enhance learning. It goes on to define the technical, sociohistorical and <phrase>political</phrase>-ideological levels of IT and to identify four theoretical constructs: tutorial, machine-as-<phrase>human</phrase>, tool kit and <phrase>catalyst</phrase>. Pervading these constructs is the understanding that IT can be used as an ideological agent, which is crucial if we are to connect theoretical foundations with practice. The authors list the foundational premises for a <phrase>pedagogy</phrase> of informatics and give practical examples, which emphasize a process of capacitation. Capacitation refers to an ongoing, active process where teachers work together and become empowered and energized through deep collaboration.
Learning Structured Sparsity in <phrase>Deep Neural Networks</phrase> <phrase>High</phrase> demand for computation resources severely hinders deployment of <phrase>large-scale</phrase> <phrase>Deep Neural Networks</phrase> (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (<phrase>SSL</phrase>) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. <phrase>SSL</phrase> can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-<phrase>friendly</phrase> structured sparsity of DNN to efficiently accelerate the DNN's evaluation. <phrase>Experimental</phrase> <phrase>results</phrase> show that <phrase>SSL</phrase> achieves on <phrase>average</phrase> 5.1 and 3.1 speedups of convolutional layer computation of AlexNet against <phrase>CPU</phrase> and <phrase>GPU</phrase>, respectively, with off-the-shelf <phrase>libraries</phrase>. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve <phrase>classification accuracy</phrase>. The <phrase>results</phrase> show that for CIFAR-10, regularization on layer depth reduces <phrase>a 20</phrase>-layer Deep Residual Network (ResNet) to 18 layers while improves the accuracy from 91.25% to 92.60%, which is still higher than that of original ResNet with 32 layers. For AlexNet, <phrase>SSL</phrase> reduces the error by 1%.
<phrase>Case Studies</phrase> in Teaching <phrase>Mathematical</phrase> Modelling * We discuss our experience in using group projects on <phrase>real world</phrase> problems for teaching <phrase>Mathematical</phrase> Modelling. Three sample problems, which were well tackled by students, are described and we summarize what we've learned from these experience in the form of five rules for teaching modelling. modelling is deemed the key element of our <phrase>curriculum</phrase> in <phrase>Mathematical</phrase> <phrase>Science</phrase> <phrase>major</phrase> and many subjects include project work specifically to enhance student's ability in modelling. Indeed, the only compulsory core subject in the final year is the subject on <phrase>Mathematical</phrase> and Statistical Modelling. I have been involved in team teaching this subject for the past seven years and I consider it the easiest subject to teach, but the hardest subject to teach well. What's easy to teach are the skills and techniques of standard <phrase>mathematical models</phrase> , which are well <phrase>covered</phrase> in standard texts such as [1-2], and this was the approach we adopted early on. In <phrase>order</phrase> to expose students to a wide <phrase>range</phrase> of techniques, we <phrase>covered</phrase> four topics in a semester and required students to carry out group projects for each topic. The workload was intense, and students became well drilled in tackling <phrase>textbook</phrase> problems. However, whenever students were asked to apply the techniques they have learnt to <phrase>real world</phrase> and they invariably froze! As Powell et al. succintly put it, students " are talented in manipulating symbols and following <phrase>textbook</phrase> recipes but terrified of applying <phrase>mathematical</phrase> <phrase>knowledge</phrase> in unknown territory " [3]. This brings us to the hard part, namely, teaching the <phrase>art</phrase> of <phrase>mathematical</phrase> modelling. Indeed, students themselves are aware of the problem as reflected in teaching evaluation <phrase>feedback</phrase>. Thus in the past five years, we adopted a different approach. The subject is now divided into two parts. The first part involved the previous skills-oriented approach, albeit reduced to just 2 topics. The second part is <phrase>problem solving</phrase> and group projects. During class, students are given <phrase>small scale</phrase> problems to work on and the emphasis lies in encouraging discussion and <phrase>brainstorming</phrase>. The problems chosen do not require very deep <phrase>mathematical</phrase> machinery but usually involve subtle tricks in all branches of <phrase>mathematics</phrase>. They serve to arouse <phrase>student</phrase> interests and warm up their <phrase>mathematical</phrase> instincts. Examples include the famous Lloyd's 15-<phrase>puzzle</phrase>, explaining the effect of gaining a day in the 80-days Around the World story, non-transitive <phrase>games</phrase> and well know <phrase>probability</phrase> " <phrase>paradoxes</phrase> ". Students are also given a small list 
Deep Predictive Coding Networks The quality of <phrase>data</phrase> representation in <phrase>deep learning</phrase> methods is directly related to the prior <phrase>model</phrase> imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the <phrase>data</phrase>. To address this issue, we propose deep predictive coding networks, a hierarchical <phrase>generative model</phrase> that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This <phrase>model</phrase> captures the temporal dependencies in time-varying signals and uses top-down <phrase>information</phrase> to modulate the representation in <phrase>lower</phrase> layers. The centerpiece of our <phrase>model</phrase> is a novel procedure to infer sparse states of a dynamic network which is used for <phrase>feature extraction</phrase>. We also extend this <phrase>feature extraction</phrase> block to introduce a pooling <phrase>function</phrase> that captures locally invariant representations. When applied on a natural <phrase>video</phrase> <phrase>data</phrase>, we show that our method is able to learn <phrase>high</phrase>-level <phrase>visual features</phrase>. We also demonstrate the role of the top-down connections by showing the robustness of the proposed <phrase>model</phrase> to structured noise.
Influence of <phrase>syntax</phrase> on prosodic boundary prediction We compare the effectiveness of different <phrase>syntactic</phrase> features and representations for prosodic boundary prediction, setting out to clarify which representations are most suitable for this task. We took a <phrase>machine learning</phrase> approach, and ran a series of eight experiments. The <phrase>results</phrase> show that the representations have different strengths and that a combination yields the best result. We also find that linguistically deep representations do not yield clearly <phrase>superior</phrase> classifiers compared to classifiers obtained by extraction of shallow features.
<phrase>COMA</phrase>-boost: co-operative <phrase>multi agent</phrase> AdaBoost Multi <phrase>feature space</phrase> representation is a common practise in <phrase>computer vision</phrase> applications. Traditional features such as HOG, SIFT, SURF etc., individually encapsulates certain discriminative cues for visual classification. On the other hand, each layer of a <phrase>deep neural network</phrase> generates multi ordered representations. In this <phrase>paper</phrase> we present a novel approach for such multi <phrase>feature representation</phrase> learning using Adaptive Boosting (AdaBoost). <phrase>General</phrase> practise in AdaBoost [8] is to concatenate components of feature spaces and <phrase>train</phrase> base learners to classify examples as correctly/incorrectly classified. We posit that multi <phrase>feature space</phrase> learning should be viewed as a <phrase>derivative</phrase> of <phrase>cooperative</phrase> <phrase>multi agent</phrase> learning. To this end, we propose a <phrase>mathematical</phrase> framework to leverage performance of base learners over each <phrase>feature space</phrase>, gauge a measure of "difficulty" of training space and finally make soft weight updates rather than strict <phrase>binary</phrase> weight updates prevalent in regular AdaBoost. This is made possible by periodically sharing of response states by our learner agents in the boosting framework. Theoretically, such soft weight update policy allows infinite combinations of weight updates on training space compared to only two possibilities in AdaBoost. This opens up the opportunity to identify 'more difficult' examples compared to 'less difficult' examples. We <phrase>test</phrase> our <phrase>model</phrase> on traditional multi <phrase>feature representation</phrase> of MNIST handwritten character dataset and 100-<phrase>Leaves</phrase> classification challenge. We consistently outperform traditional and variants of <phrase>multi view</phrase> boosting in terms of accuracy while margin analysis reveals that <phrase>proposed method</phrase> fosters formation of more confident ensemble of learner agents. As an application of using our <phrase>model</phrase> in conjecture with <phrase>deep neural network</phrase>, we <phrase>test</phrase> our <phrase>model</phrase> on the <phrase>challenging task</phrase> of retinal <phrase>blood vessel</phrase> segmentation from fundus images of DRIVE dataset by using kernel dictionaries from layers of unsupervised trained stacked autoencoder network. Our work opens a new avenue of <phrase>research</phrase> for combining a popular <phrase>statistical machine learning</phrase> <phrase>paradigm</phrase> with deep network architectures.
Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant <phrase>speech recognition</phrase> <phrase>Denoising autoencoders</phrase> (DAs) have shown success in generating robust features for images, but there has been limited work in applying DAs for speech. In this <phrase>paper</phrase> we present a deep denoising autoencoder (DDA) framework that can produce robust speech features for noisy reverberant <phrase>speech recognition</phrase>. The DDA is first <phrase>pre-trained</phrase> as restricted Boltz-mann machines (RBMs) in an unsupervised <phrase>fashion</phrase>. Then it is unrolled to autoencoders, and <phrase>fine-tuned</phrase> by corresponding clean speech features to learn a nonlinear mapping from noisy to clean features. <phrase>Acoustic</phrase> models are retrained using the reconstructed features from the DDA, and <phrase>speech recognition</phrase> is performed. The <phrase>proposed approach</phrase> is evaluated on the CHiME-WSJ0 corpus, and shows a 16-25% absolute improvement on the <phrase>recognition accuracy</phrase> under various SNRs.
<phrase>Workflow</phrase>-based <phrase>Human</phrase>-in-the-Loop <phrase>Data</phrase> Analytics Exploration in <phrase>data</phrase> analytics system requires proficient skills in <phrase>data science</phrase> and a <phrase>deep understanding</phrase> of the domain, both of which are scarce and hard to find. Experienced analysts leverage <phrase>knowledge</phrase> from past experience and solve a problem within minutes, yet the same problem could cost novice analysts days. In this <phrase>paper</phrase> we <phrase>advocate</phrase> a <phrase>human</phrase>-in-the-loop <phrase>data</phrase> analytics system which logs the <phrase>workflow</phrase> of users within the <phrase>data</phrase> analytics system and learns common <phrase>workflow</phrase> patterns using <phrase>graph</phrase> analysis, <phrase>information</phrase> scent, and instance-based learning techniques. Based on the learned common patterns, the system automates laborious processing steps to improve analyst <phrase>productivity</phrase> and provides recommendations based on the <phrase>workflow</phrase> of expert users. This will narrow the <phrase>productivity</phrase> gap and <phrase>lead</phrase> to improved <phrase>usability</phrase>.
<phrase>Deep learning</phrase> in partially-<phrase>labeled data</phrase> streams Of the considerable <phrase>research</phrase> on <phrase>data</phrase> streams, relatively little deals with classification where only some of the instances in the <phrase>stream</phrase> are labeled. Most <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>data-stream</phrase> <phrase>algorithms</phrase> do not have an effective way of dealing with unlabeled instances from the same domain. In this <phrase>paper</phrase> we explore <phrase>deep learning</phrase> techniques that provide important advantages such as the ability to learn incrementally in constant <phrase>memory</phrase>, and from unlabeled examples. We develop two <phrase>deep learning</phrase> methods and explore empirically via a series of empirical evaluations the application to several <phrase>data</phrase> streams scenarios based on real <phrase>data</phrase>. We find that our methods can offer competitive accuracy as compared with existing popular <phrase>data-stream</phrase> learners.
<phrase>Deep Learning</phrase> for <phrase>Chinese</phrase> Word Segmentation and POS Tagging This study explores the feasibility of performing <phrase>Chinese</phrase> word segmentation (<phrase>CWS</phrase>) and POS tagging by <phrase>deep learning</phrase>. We try to avoid task-specific feature <phrase>engineering</phrase>, and use deep layers of <phrase>neural networks</phrase> to discover relevant features to the tasks. We leverage <phrase>large-scale</phrase> <phrase>unlabeled data</phrase> to improve internal representation of <phrase>Chinese characters</phrase>, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to <phrase>state</phrase>-of-the-<phrase>art</phrase> performance with minimal computational cost. We also describe a <phrase>perceptron</phrase>-style <phrase>algorithm</phrase> for training the <phrase>neural networks</phrase>, as an <phrase>alternative</phrase> to <phrase>maximum-likelihood</phrase> method, to speed up the training process and make the <phrase>learning algorithm</phrase> easier to be implemented.
Telerobotic Exploration and Development of the <phrase>Moon</phrase> There has been a debate for the last thirty years about the relative merits of <phrase>human</phrase> versus robotic systems and we argue here that both are essential components for successful <phrase>lunar</phrase> exploration and development. We examine the role of <phrase>robots</phrase> in the next phases of exploration and <phrase>human development</phrase> of the <phrase>Moon</phrase>. The historical use of <phrase>robots</phrase> and humans in exploration is discussed, including <phrase>Apollo</phrase>-<phrase>era</phrase> exploration, <phrase>International Space Station</phrase>, and deep-<phrase>water</phrase> <phrase>petroleum</phrase> exploration. The technological challenges of <phrase>lunar</phrase> operations are addressed in the context of how robotic systems can be designed for robust and flexible operations. System <phrase>design</phrase> recommendations are given based on the <phrase>lessons learned</phrase> from <phrase>terrestrial</phrase> and space <phrase>robotics</phrase>.
Unsupervised Deep <phrase>Feature Extraction</phrase> for <phrase>Remote Sensing</phrase> <phrase>Image Classification</phrase> This <phrase>paper</phrase> introduces the use of <phrase>single</phrase> layer and <phrase>deep convolutional</phrase> networks for <phrase>remote sensing</phrase> <phrase>data analysis</phrase>. Direct application to multi-and hyper-spectral imagery of supervised (shallow or deep) <phrase>convolutional networks</phrase> is very challenging given the <phrase>high</phrase> <phrase>input data</phrase> dimensionality and the relatively small amount of available <phrase>labeled data</phrase>. Therefore, we propose the use of <phrase>greedy layer-wise</phrase> <phrase>unsupervised pre-training</phrase> coupled with a highly efficient <phrase>algorithm</phrase> for <phrase>unsupervised learning</phrase> of sparse features. The <phrase>algorithm</phrase> is rooted on sparse representations and enforces both <phrase>population</phrase> and <phrase>lifetime</phrase> sparsity of the extracted features, simultaneously. We successfully illustrate the expressive power of the extracted representations in several scenarios: classification of aerial scenes, as well as land-use classification in very <phrase>high</phrase> resolution (VHR), or land-<phrase>cover</phrase> classification from multi-and hyper-spectral images. The <phrase>proposed algorithm</phrase> clearly outperforms standard <phrase>Principal Component Analysis</phrase> (<phrase>PCA</phrase>) and its kernel counterpart (kPCA), as well as <phrase>current state</phrase>-of-the-<phrase>art</phrase> <phrase>algorithms</phrase> of aerial classification, while being extremely computationally efficient at learning representations of <phrase>data</phrase>. <phrase>Results</phrase> show that <phrase>single</phrase> layer <phrase>convolutional networks</phrase> can extract powerful <phrase>discriminative features</phrase> only when the <phrase>receptive field</phrase> accounts for neighboring <phrase>pixels</phrase>, and are preferred when the classification requires <phrase>high</phrase> resolution and detailed <phrase>results</phrase>. However, <phrase>deep architectures</phrase> significantly outperform <phrase>single</phrase> layers variants, capturing increasing levels of abstraction and complexity throughout the feature hierarchy. I. INTRODUCTION <phrase>Earth observation</phrase> (EO) through <phrase>remote sensing</phrase> techniques is a <phrase>research</phrase> field where a huge <phrase>variety</phrase> of physical signals is measured from instruments on-board space and airborne platforms. A wide diversity of <phrase>sensor</phrase> characteristics is nowadays available, ranging from medium and very <phrase>high</phrase> resolution (VHR) multispectral imagery to <phrase>hyperspectral</phrase> images that sample the <phrase>electromagnetic spectrum</phrase> with <phrase>high</phrase> detail. These myriad of sensors serve to particularly different objectives, focusing either on obtaining quantitative measurements and estimations of geo-bio-physical variables, or on the identification of materials by the analysis of the acquired images [1][3]. Among all the different <phrase>products</phrase> that can be obtained from the acquired images, classification maps
Teaching <phrase>Logic</phrase> Control Systems to <phrase>Chemical Engineering</phrase> Students <phrase>Logic</phrase> control systems was incorporated into the <phrase>academic</phrase> program of <phrase>Chemical Engineering</phrase> students at Tecnolgico de <phrase>Monterrey</phrase>. Exploiting <phrase>Active Learning</phrase> techniques and own <phrase>educational technology</phrase>, a new <phrase>experimental</phrase> <phrase>automatic control</phrase> course was implemented. Experiences show that students get a <phrase>deep learning</phrase> as result of the activity performed. Also, several abilities and skills are learned and the gap between theory and practice is avoided through the <phrase>experimental</phrase> hands-on sessions. Four technical disciplines were selected as crucial to the progress of the <phrase>chemical industry</phrase> [1]: new chemical <phrase>science</phrase> and <phrase>engineering</phrase> <phrase>technology</phrase>, <phrase>supply chain management</phrase>, <phrase>information</phrase> systems, and <phrase>manufacturing</phrase> operations. Particularly, <phrase>manufacturing</phrase> operations require advances in six key areas where <phrase>information</phrase> and <phrase>process control</phrase> is one of them. Here, a <phrase>high</phrase> <phrase>degree</phrase> of <phrase>automation</phrase> and <phrase>decision making</phrase> is needed. In the early 1980's, the market <phrase>pressure</phrase> for greater product <phrase>variety</phrase> forced a gradual shift from continuous <phrase>manufacturing</phrase> processes to batch <phrase>manufacturing</phrase> processes.. About 50 % of all <phrase>industrial</phrase> processes include <phrase>batch processing</phrase>. Batch processes have one or more process cells for several <phrase>products</phrase> or product variants. These processes demand several complex operations carried out in multiple-purpose equipments on <phrase>plant</phrase> designed for multiple <phrase>products</phrase> batch manufacture. Each <phrase>chemical process</phrase> is defined in terms of a list of ingredients, the recipe, and the instructions to transform this <phrase>information</phrase> into a batch of a given product.
<phrase>Learning Styles</phrase> and Learning Behaviour in the <phrase>Hypermedia</phrase> Environment <phrase>RACE</phrase> - Analysing Computer Log files and Questionnaire <phrase>Data</phrase> The following <phrase>Paper</phrase> describes an experiment which analyses in how far specific learner, set by questionnaire, types show congruent behaviour in a <phrase>concrete</phrase> learning situation. The undertaken a priori subdivision in surface-and deep-strategic learner types based on established models for one hour. Then a <phrase>general</phrase> questionnaire about their learning behaviour in self-regulated learning situations was given. The questionnaire <phrase>data</phrase> (trait measurement) and the process <phrase>data</phrase> from the log files (<phrase>state</phrase> measurement) were evaluated by <phrase>cluster-analysis</phrase> and compared with each other. The experiment shows that both analyses result in a two-cluster <phrase>solution</phrase> with clear allocation of surface-and deep-strategic variables. The congruency between these solutions amounts to approx. 70%, and a significant difference could not be determined ( 2 = 581, 1, p = 446). The comparison of the z-transformed means for the cluster variables points out that the analyses of the process <phrase>data</phrase> may be of a higher selectivity for determining these learning types. Interesting findings concerning the cluster composition can be shown when you include further descriptive variables (expert-novice status, <phrase>state</phrase> of training and computer experience).
<phrase>TerraSAR-X</phrase> Observations of the Recovery <phrase>Glacier</phrase> System, <phrase>Antarctica</phrase> We present a comparison of 1997 Radarsat <phrase>Antarctic</phrase> Mapping Project (RAMP) <phrase>data</phrase> with 2008-09 <phrase>TerraSAR-X</phrase> observations of the Recovery <phrase>Glacier</phrase> System in Coates Land <phrase>Antarctica</phrase> (Figure 1). The Recovery <phrase>Glacier</phrase> system was fully mapped for the first time in 1997 utilizing the then unique attribute of Radarsat to acquire C-<phrase>band</phrase> <phrase>synthetic aperture radar</phrase> (SAR) <phrase>data</phrase> extending to the <phrase>South Pole</phrase>. Through careful mapping of <phrase>shear</phrase> margins and flow stripes, the RAMP measurements revealed for the extent of these <phrase>glaciers</phrase> into the deep interior of <phrase>East Antarctica</phrase>. The northerly portion of the main trunk of Recovery <phrase>Glacier</phrase> and its <phrase>confluence</phrase> with several <phrase>long</phrase> but narrow <phrase>tributary</phrase> <phrase>glaciers</phrase> was also mapped interferometrically. Derived estimates of <phrase>ice sheet</phrase> surface <phrase>velocity</phrase> were used to calculate that about 31 <phrase>km</phrase> 3 /a of <phrase>ice</phrase> are discharged into the Filchner <phrase>Ice Shelf</phrase>. The <phrase>data</phrase> were also used to examine the dynamical interaction between Recovery <phrase>Glacier</phrase> and its <phrase>tributaries</phrase>, where in one case it appears that the main trunk of Recovery <phrase>Glaciers</phrase> exerts a back <phrase>pressure</phrase> on the outward flow of the nearly orthogonally flowing <phrase>tributary</phrase> thereby decreasing its <phrase>velocity</phrase> by about 60-70 m/a near the junction. The Recovery <phrase>Glacier</phrase> system is of considerable scientific interest because of its role in discharging East <phrase>Antarctic</phrase> <phrase>ice</phrase> to the sea and because it has been subsequently learned that the flow of the <phrase>glacier</phrase> is likely controlled by the presence of subglacial <phrase>lakes</phrase> near the onset of faster <phrase>glacier</phrase> flow. Eleven years later in 2008, the next <phrase>high</phrase>-resolution space-borne observations of Recovery <phrase>Glacier</phrase> were acquired using the <phrase>TerraSAR-X</phrase> <phrase>satellite</phrase> and as part of the International Polar Year. Again, we relied on the capability of <phrase>TerraSAR-X</phrase> to acquire X-<phrase>band</phrase> SAR <phrase>data</phrase> extending to <phrase>high</phrase> southerly <phrase>latitudes</phrase> and with very <phrase>high</phrase> spatial resolution. Along with obtaining image <phrase>data</phrase>, repeat <phrase>pass</phrase> <phrase>data</phrase> were acquired for SAR <phrase>interferometry</phrase> and <phrase>high</phrase> quality interferograms have now been created. Initial comparisons between the C-and X-<phrase>band</phrase> image <phrase>data</phrase> show that <phrase>large scale</phrase> patterns of crevasses, <phrase>shear</phrase> margins and <phrase>glacier</phrase> flow stripes are similar between 1997 and 2008. Consistency between the <phrase>data</phrase> sets and the clarity of features like crevasses in the <phrase>TerraSAR-X</phrase> <phrase>data</phrase> enabled us to identify the same <phrase>crevasse</phrase> patterns and individual crevasses in both the RAMP and <phrase>TerraSAR-X</phrase> <phrase>data</phrase>, at least in a few instances. By co-registering the two <phrase>data</phrase> sets using tie points, we applied manual feature retracking to estimate the 11-year <phrase>average</phrase> <phrase>velocity</phrase> of 
Towards Structured <phrase>Deep Neural Network</phrase> for <phrase>Automatic Speech Recognition</phrase> In this <phrase>paper</phrase> we propose the Structured <phrase>Deep Neural Network</phrase> (structured DNN) as a structured and <phrase>deep learning</phrase> framework. This approach can learn to find the best structured object (such as a <phrase>label</phrase> <phrase>sequence</phrase>) given a structured input (such as a <phrase>vector</phrase> <phrase>sequence</phrase>) by globally considering the mapping relationships between the structures rather than item by item. When <phrase>automatic speech recognition</phrase> is viewed as a special case of such a structured learning problem, where we have the <phrase>acoustic</phrase> <phrase>vector</phrase> <phrase>sequence</phrase> as the input and the <phrase>phoneme</phrase> <phrase>label</phrase> <phrase>sequence</phrase> as the output, it becomes possible to comprehensively learn utterance by utterance as a whole, rather than frame by frame. Structured <phrase>Support Vector Machine</phrase> (struc-tured <phrase>SVM</phrase>) was proposed to perform ASR with structured learning previously, but limited by the linear <phrase>nature</phrase> of <phrase>SVM</phrase>. Here we propose structured DNN to use nonlinear transformations in multi-layers as a structured and <phrase>deep learning</phrase> approach. This approach was shown to beat structured <phrase>SVM</phrase> in preliminary experiments on TIMIT.
A search space "<phrase>cartography</phrase>" for guiding <phrase>graph coloring</phrase> heuristics We present a search space analysis and its application in improving <phrase>local search</phrase> <phrase>algorithms</phrase> for the <phrase>graph coloring</phrase> problem. Using a <phrase>classical</phrase> distance measure between colorings, we introduce the following clustering <phrase>hypothesis</phrase>: the <phrase>high</phrase> quality solutions are not randomly scattered in the search space, but rather grouped in clusters within spheres of specific <phrase>diameter</phrase>. We first provide intuitive evidence for this <phrase>hypothesis</phrase> by presenting a projection of a large set of <phrase>local minima</phrase> in the 3D space. An <phrase>experimental</phrase> confirmation is also presented: we introduce two <phrase>algorithms</phrase> that exploit the <phrase>hypothesis</phrase> by guiding an underlying <phrase>Tabu Search</phrase> (TS) process. The first <phrase>algorithm</phrase> (TS-Div) uses a learning process to guide the <phrase>basic</phrase> TS process toward as-yet-unvisited spheres. The second <phrase>algorithm</phrase> (TS-Int) makes deep investigations within a bounded <phrase>region</phrase> by organizing it as a <phrase>tree</phrase>-like structure of connected spheres. We experimentally demonstrate that if such a <phrase>region</phrase> contains a global optimum, TS-Int does not fail in eventually finding it. This pair of <phrase>algorithms</phrase> <phrase>significantly outperforms</phrase> the underlying <phrase>basic</phrase> TS <phrase>algorithm</phrase>; it can even improve some of the best-known solutions ever reported in the <phrase>literature</phrase> (e.g. for dsjc1000.9).
<phrase>Drawing</phrase> and Recognizing <phrase>Chinese Characters</phrase> with <phrase>Recurrent Neural Network</phrase> Recent <phrase>deep learning</phrase> based approaches have achieved <phrase>great success</phrase> on <phrase>handwriting recognition</phrase>. <phrase>Chinese characters</phrase> are among the most widely adopted <phrase>writing systems</phrase> in the world. Previous <phrase>research</phrase> has mainly focused on recognizing handwritten <phrase>Chinese characters</phrase>. However, recognition is only one <phrase>aspect</phrase> for understanding a <phrase>language</phrase>, another challenging and interesting task is to teach a machine to automatically write (pictographic) <phrase>Chinese characters</phrase>. In this <phrase>paper</phrase>, we propose a framework by using the <phrase>recurrent neural network</phrase> (RNN) as both a discriminative <phrase>model</phrase> for recognizing <phrase>Chinese characters</phrase> and a <phrase>generative model</phrase> for <phrase>drawing</phrase> (generating) <phrase>Chinese characters</phrase>. To recognize <phrase>Chinese characters</phrase>, previous methods usually adopt the <phrase>convolutional neural network</phrase> (<phrase>CNN</phrase>) models which require transforming the online handwriting trajectory into image-like representations. Instead, our RNN <phrase>based approach</phrase> is an <phrase>end-to-end</phrase> system which directly deals with the sequential structure and does not require any <phrase>domain-specific</phrase> <phrase>knowledge</phrase>. With the RNN system (combining an LSTM and <phrase>GRU</phrase>), <phrase>state</phrase>-of-the-<phrase>art</phrase> performance can be achieved on the ICDAR-2013 competition <phrase>database</phrase>. Furthermore, under the RNN framework, a conditional <phrase>generative model</phrase> with character embedding is proposed for automatically <phrase>drawing</phrase> recognizable <phrase>Chinese characters</phrase>. The generated characters (in <phrase>vector</phrase> format) are <phrase>human</phrase>-readable and also can be recognized by the discriminative RNN <phrase>model</phrase> with <phrase>high</phrase> accuracy. <phrase>Experimental</phrase> <phrase>results</phrase> verify the effectiveness of using RNNs as both generative and discriminative models for the tasks of <phrase>drawing</phrase> and recognizing <phrase>Chinese characters</phrase>.
Beyond Sharing: Engaging Students in <phrase>Cooperative</phrase> and Competitive <phrase>Active Learning</phrase> or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or <phrase>commercial advantage and that copies bear</phrase> the full citation on the first page. <phrase>Copyrights</phrase> for components of this work owned by others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to <phrase>post on servers</phrase>, or to redistribute to lists, <phrase>requires prior specific permission</phrase> and/or a fee. Request permissions from the editors at kinshuk@ieee.org. ABSTRACT The authors describe their <phrase>design</phrase> for an <phrase>Internet</phrase>-<phrase>based learning environment</phrase> called BeyondShare in which students are encouraged to gain a <phrase>deep understanding</phrase> of the learning material, reflect on the quality of individual constructions through sharing and peer evaluation, and synthesize cross-unit <phrase>knowledge</phrase> by integrating self-and peer-<phrase>produced</phrase> constructions. A sharing typology may consist of <phrase>basic</phrase> sharing, sharing with notification, sharing with <phrase>feedback</phrase>, or sharing with interaction. BeyondShare distinguishes itself by combining sharing activities with task structuring and cooperation/competition to achieve <phrase>active learning</phrase>. A formal evaluation of BeyondShare was performed with a class of 34 <phrase>college</phrase> students who created <phrase>concept maps</phrase> for a computer <phrase>programming language</phrase> and who were encouraged to become active learners by exchanging roles throughout the experiment. Evaluation <phrase>results</phrase> indicate that the students viewed BeyondShare as an easy-to-use environment that motivated them toward <phrase>comprehensive</phrase> <phrase>knowledge</phrase> integration by sharing <phrase>construction</phrase> <phrase>products</phrase> with their <phrase>peers</phrase>. Potential activities and suggested modifications are discussed.
Online Incremental <phrase>Feature Learning</phrase> with <phrase>Denoising Autoencoders</phrase> While determining <phrase>model</phrase> complexity is an important problem in <phrase>machine learning</phrase>, many <phrase>feature learning</phrase> <phrase>algorithms</phrase> rely on <phrase>cross-validation</phrase> to choose an optimal number of features, which is usually challenging for <phrase>online learning</phrase> from a massive <phrase>stream</phrase> of <phrase>data</phrase>. In this <phrase>paper</phrase>, we propose an incremen-tal <phrase>feature learning</phrase> <phrase>algorithm</phrase> to determine the optimal <phrase>model</phrase> complexity for <phrase>large-scale</phrase>, online datasets based on the denoising <phrase>au</phrase>-toencoder. This <phrase>algorithm</phrase> is composed of two processes: adding features and merging features. Specifically, it adds new features to minimize the objective function's residual and merges similar features to obtain a compact <phrase>feature representation</phrase> and prevent over-fitting. Our experiments show that the proposed <phrase>model</phrase> quickly converges to the optimal number of features in a <phrase>large-scale</phrase> on-line setting. In <phrase>classification tasks</phrase>, our <phrase>model</phrase> outperforms the (non-incremental) denoising autoencoder, and deep networks constructed from our <phrase>algorithm</phrase> perform favorably compared to <phrase>deep belief</phrase> networks and stacked de-noising autoencoders. Further, the <phrase>algorithm</phrase> is effective in recognizing new patterns when the <phrase>data</phrase> distribution changes over time in the massive online <phrase>data stream</phrase>.
<phrase>Student</phrase> <phrase>Approaches to Learning</phrase>: an Exploratory Study In this study, the partial least square approach (PLS) is applied to investigate students' <phrase>approaches to learning</phrase> in the framework of online or <phrase>hybrid</phrase> courses. A total of 140 valid responses from students who have finished or are currently enrolled in at least one MIS online or <phrase>hybrid</phrase> course were analyzed using a structural equation <phrase>model</phrase> and the <phrase>results</phrase> are presented herein. Keywords: <phrase>experiential learning</phrase> theory (ELT), students " <phrase>approaches to learning</phrase> (SAL), surface approach (SA), deep approach (DA), Bigg " s study process questionnaire (SPQ).
Multimodal <phrase>Deep Learning</phrase> <phrase>Library</phrase> 1 Multimodal <phrase>Deep Learning</phrase> <phrase>Library</phrase> MDL, Multimodal <phrase>Deep Learning</phrase> <phrase>Library</phrase>, is a <phrase>deep learning</phrase> framework that supports multiple models, and this document explains its <phrase>philosophy</phrase> and functionality. MDL <phrase>runs</phrase> on <phrase>Linux</phrase>, Mac, and <phrase>Unix</phrase> platforms. It depends on <phrase>OpenCV</phrase>.
Deep issues: phenomenal <phrase>data mining</phrase> 75 fact a customer has children is more stable than the fact a particular basket includes Roll-ups. The fact that a customer has <phrase>diabetes</phrase> is more stable than a particular pattern of <phrase>food</phrase> purchases that may allow inferring he or she has <phrase>diabetes</phrase>. The phenomenal facts, partly because they are more stable than observations , are more predictive of future behavior than simple observational facts. The extreme <phrase>positivist</phrase> <phrase>philosophical</phrase> view that <phrase>science</phrase> concerns relations among observations still influences the <phrase>design</phrase> of learning programs, and that's what <phrase>data</phrase> miners are. However, <phrase>science</phrase> never worked that way, neither do babies and neither should <phrase>data mining</phrase> programs; all obtain and use representations of the objects and use observations only as a means to that end. <phrase>Data mining</phrase> involves computer programs that infer relations among different kinds of <phrase>data</phrase> in large <phrase>databases</phrase>. The goal has been to infer useful relations that might not have been noticed or at least could not have been confirmed among this <phrase>data</phrase>. We use the standard example of a <phrase>supermarket chain</phrase> with a <phrase>database</phrase> of all the <phrase>cash register</phrase> receipts for some <phrase>long</phrase> <phrase>period</phrase> of timemany gigabytes of <phrase>data</phrase>. The <phrase>company</phrase> wants to mine this <phrase>database</phrase> for <phrase>information</phrase> useful for improving its <phrase>business</phrase>. S cience and common sense both tell us that the facts about the world are not directly <phrase>observable</phrase> but can be inferred from observations about the effects of actions. What people infer about the world is not just relations among observations , but relations among entities much more stable than observations. For example, 3D objects are more stable than the image on a person's <phrase>retina</phrase>, the <phrase>information</phrase> directly obtained from feeling an object, or an image scanned into a computer. Tracing the <phrase>road</phrase> from <phrase>data</phrase> to phenomenausing a <phrase>supermarket</phrase> environ as the backdropillustrates the essence of <phrase>data mining</phrase> with commonsense <phrase>knowledge</phrase>. Deep Issues 1 Even very young babies have a lot of innate <phrase>knowledge</phrase> of the world. My online article , " The Well-Designed Child, " (www-formal.stanford.edu/jmc/child.html) concerns what innate <phrase>knowledge</phrase> children probably do have about the world and what <phrase>knowledge</phrase> <phrase>robots</phrase> should be given. Elizabeth Spelke investigates innate <phrase>knowledge</phrase> in babies experimentally.
<phrase>Domain Specific</phrase> Languages in <phrase>Software Architecture</phrase> Building a <phrase>software</phrase> product requires <phrase>knowledge</phrase> in both <phrase>software engineering</phrase> and the domain the <phrase>software</phrase> is written for. However, common <phrase>software engineering</phrase> methods in fact exclude <phrase>domain experts</phrase> from the <phrase>architectural</phrase> <phrase>design</phrase> of a <phrase>software</phrase> system, because the generality of the notations defined by these methods require expert <phrase>knowledge</phrase> in the field of <phrase>software engineering</phrase> and especially <phrase>software architecture</phrase>. In <phrase>order</phrase> to avoid this effect, <phrase>domain-specific</phrase> languages (<phrase>DSL</phrase>) have been proposed as an <phrase>alternative</phrase> to the <phrase>general</phrase> purpose languages being normally used in <phrase>software architecture</phrase>. One of the advantages of DSLs is that they are less cryptic and easier to learn for <phrase>domain experts</phrase>. Of course the <phrase>design</phrase> of such a <phrase>language</phrase> requires <phrase>deep understanding</phrase> of the problem domain, on the other hand the existing tools for <phrase>language</phrase> <phrase>design</phrase> are only mastered by a few experts. In <phrase>order</phrase> to make <phrase>DSL</phrase> <phrase>design</phrase> applicable by a broader <phrase>range</phrase> of people-especially <phrase>domain experts</phrase>-we use Montages, a semi-visual meta <phrase>language</phrase> based on the Abstract <phrase>State</phrase> Machines approach. A <phrase>DSL</phrase> can be specified in Montages by means of visual descriptions of the <phrase>language</phrase> constructs. These descriptions can be fed to a <phrase>rapid prototyping</phrase> tool, called GemMex, which automatically generates a <phrase>visual programming</phrase> environment for the specified <phrase>DSL</phrase>.
An <phrase>ontology</phrase>-based, multi-modal platform for the inclusion of marginalized rural communities into the <phrase>knowledge</phrase> <phrase>society</phrase> With the <phrase>information revolution</phrase> that promises to shape the <phrase>21st century</phrase>, <phrase>knowledge</phrase> has become the prime <phrase>commodity</phrase>, very much like land, means of <phrase>production</phrase> and <phrase>capital</phrase> have been at different times in the past. Access to <phrase>information</phrase>, made instantly available by the growth of the <phrase>Internet</phrase>, determines access to economic resources, social participation and better quality of <phrase>life</phrase>. For this reason, the <phrase>knowledge</phrase> stored on the Web and the advantages offered by the spread of <phrase>Information</phrase> and <phrase>Communication</phrase> <phrase>Technology</phrase> (ICT) are equally important for rich societies to prosper and for poor ones to develop. The only difference is that marginalised communities do not have access to the tools and have little control over the content found in the domain of ICTs. In this <phrase>paper</phrase> we describe an intervention to develop the potential of a typical rural <phrase>community</phrase> in <phrase>South Africa</phrase> through ICTs. This involves providing <phrase>Internet</phrase> connectivity and deploying a platform to support <phrase>e-commerce</phrase>, <phrase>e</phrase>-learning, <phrase>e-government</phrase> and <phrase>e</phrase>-<phrase>health</phrase>. The core of the platform is an <phrase>ontology</phrase>-based <phrase>model</phrase> designed to integrate and respond to <phrase>Indigenous</phrase> <phrase>Knowledge</phrase> Systems. This has been achieved by combining a <phrase>deep understanding</phrase> of local <phrase>knowledge</phrase> and <phrase>social networks</phrase> with the use of authoring, <phrase>communication</phrase> and <phrase>ontology</phrase>-<phrase>management</phrase> tools. The primary goal of this new approach is to find a way to make ICT solutions more sensitive to the local context, and therefore more effective. Secondly, we hope to foster a sense of ownership of the project among the <phrase>community</phrase>, by capitalising on local <phrase>knowledge</phrase> and resources.
<phrase>Semantic</phrase> Duplicate Identification with <phrase>Parsing</phrase> and <phrase>Machine Learning</phrase> Identifying duplicate texts is important in many areas like <phrase>plagiarism</phrase> detection, <phrase>information retrieval</phrase>, text summarization, and <phrase>question answering</phrase>. Current approaches are mostly surface-oriented (or use only shallow <phrase>syntactic</phrase> representations) and see each text only as a token list. In this work however, we describe a deep, semantically oriented method based on <phrase>semantic</phrase> networks which are derived by a syntactico-<phrase>semantic</phrase> parser. Semantically identical or similar <phrase>semantic</phrase> networks for each sentence of a given base text are efficiently retrieved by using a specialized index. In <phrase>order</phrase> to detect many kinds of paraphrases the <phrase>semantic</phrase> networks of a candidate text are varied by applying inferences: lexico-<phrase>semantic</phrase> relations, relation <phrase>axioms</phrase>, and meaning postulates. Important phenomena occurring in difficult duplicates are discussed. The deep approach profits from <phrase>background knowledge</phrase>, whose acquisition from corpora is explained briefly. The deep duplicate recognizer is combined with two shallow duplicate recognizers in <phrase>order</phrase> to guarantee a <phrase>high</phrase> recall for texts which are not fully parsable. The evaluation shows that the combined approach preserves recall and increases precision considerably in comparison to traditional shallow methods.
Learning Qualitative Models from <phrase>Physiological</phrase> Signals patient monitoring, qualitative reasoning, <phrase>machine learning</phrase> <phrase>Physiological</phrase> models represent a useful form of <phrase>knowledge</phrase>, but are both difficult and time consuming to construct by hand. Further, most <phrase>physiological</phrase> systems are incompletely understood. This article addresses these two issues with a system that learns qualitative models from <phrase>physiological</phrase> systems. We describe the Genmodel learning system in detail, including the front-end processing and segmenting that transforms a signal into a set of qualitative states. Next we <phrase>report</phrase> <phrase>results</phrase> of experiments on <phrase>data</phrase> obtained from six patients during <phrase>cardiac</phrase> <phrase>bypass surgery</phrase>..Useful models were obtained, representing both normal <phrase>physiology</phrase> and <phrase>pathophysiology</phrase> particular to the patient being monitored. <phrase>Model</phrase> variations across time and across different levels of temporal abstraction and <phrase>fault tolerance</phrase> are examined. Implications for the <phrase>design</phrase> of intelligent monitoring systems and smart alarms are explored. <phrase>Physiological</phrase> models have a central role in <phrase>medical</phrase> <phrase>knowledge</phrase>, encapsulating our understanding of experimentally observed physical processes. These encapsulations <phrase>act</phrase> both as theories whose predictions can be used for further <phrase>research</phrase>, and as clinical models whose predictions assist in delivery of therapy. For computer-based <phrase>decision support systems</phrase>, <phrase>physiological</phrase> models represent a useful form of <phrase>knowledge</phrase> because they encapsulate structural <phrase>information</phrase> of the system and allow deep forms of reasoning techniques to be applied. For example, such models are used in many <phrase>prototype</phrase> intelligent monitoring systems and <phrase>medical</phrase> <phrase>expert systems</phrase>. However, constructing <phrase>physiological</phrase> models by hand is difficult and time consuming. Further, most <phrase>physiological</phrase> systems are incompletely understood. These factors have hindered the development of <phrase>model</phrase>-based reasoning systems for clinical <phrase>decision support</phrase>. Qualitative models <phrase>permit</phrase> useful representations of a system to be developed in the absence of extensive <phrase>knowledge</phrase> of the system. In the <phrase>medical</phrase> domain, such models have been applied to: diagnostic patient monitoring of <phrase>acid</phrase>-base balance [6] qualitative <phrase>simulation</phrase> of the <phrase>iron</phrase> <phrase>metabolism</phrase> mechanism [14] qualitative <phrase>simulation</phrase> of <phrase>urea</phrase> extraction during <phrase>dialysis</phrase> [2] qualitative <phrase>simulation</phrase> of the <phrase>water balance</phrase> mechanism and its disorders [17] qualitative <phrase>simulation</phrase> of the mechanism for regulation of <phrase>blood pressure</phrase> [17] <phrase>Recent developments</phrase> in <phrase>machine learning</phrase> have <phrase>produced</phrase> methods of automatically inducing qualitative models from system behaviors. Applying such techniques to learning <phrase>physiological</phrase> models should not only benefit <phrase>knowledge</phrase> acquisition, but also provide a useful tool for physiologists who need to process vast amounts of <phrase>data</phrase> and induce useful theoretical models of the systems they study. The learning system could also serve as a tool for <phrase>model</phrase>-based diagnosis. 
<phrase>Syntactic</phrase> Well-Formedness Diagnosis and Error-Based Coaching in <phrase>Computer Assisted Language Learning</phrase> using <phrase>Machine Translation</phrase> We present a novel approach to <phrase>Computer Assisted Language Learning</phrase> (CALL), using <phrase>deep syntactic</phrase> parsers and <phrase>semantic</phrase> based <phrase>machine translation</phrase> (MT) in diagnosing and providing explicit <phrase>feedback</phrase> on <phrase>language</phrase> learners' errors. We are currently developing a <phrase>proof of concept</phrase> system showing how <phrase>semantic</phrase>-based <phrase>machine translation</phrase> can, in conjunction with robust computational grammars, be used to interact with students, better understand their <phrase>language</phrase> errors, and help students correct their <phrase>grammar</phrase> through a series of useful <phrase>feedback</phrase> messages and guided <phrase>language</phrase> drills. Ultimately, we aim to prove the viability of a new integrated <phrase>rule-based</phrase> MT approach to disambiguate studentsintended meaning in a CALL system. This is a necessary step to provide accurate coaching on how to correct ungrammatical input, and it will allow us to overcome a current bottleneck in the field an exponential burst of ambiguity caused by ambiguous lexical items (Flickinger, 2010). From the usersinteraction with the system, we will also produce a richly annotated Learner Corpus, annotated automatically with both <phrase>syntactic</phrase> and <phrase>semantic</phrase> <phrase>information</phrase>.
Delay/disruption-tolerant Network Testing Using a Leo <phrase>Satellite</phrase> Delay/Disruption Tolerant Networking (DTN) " bundles " have been proposed for deep-space <phrase>communication</phrase> in the " Interplanetary <phrase>Internet</phrase>. " This <phrase>paper</phrase> describes the first DTN bundle protocol testing from space, using the <phrase>United Kingdom</phrase> Disaster Monitoring <phrase>Constellation</phrase> (<phrase>UK</phrase>-DMC) <phrase>satellite</phrase> in <phrase>Low Earth Orbit</phrase> (LEO). The mismatch problems between the different conditions of the <phrase>private</phrase> dedicated space-to-ground link and the shared, congested, ground-to-ground links are discussed. DTN, with its ability to transfer files on a hop-by-hop basis across different subnets, is presented as a <phrase>technology</phrase> that can be used to alleviate this problem. We describe our operational testing, as well as <phrase>test</phrase> configurations, goals and <phrase>results</phrase>, and <phrase>lessons learned</phrase>. I. INTRODUCTION Delay/Disruption Tolerant Networking (DTN) has been defined as an <phrase>end-to-end</phrase> store-and-<phrase>forward</phrase> <phrase>architecture</phrase> capable of providing communications in highly-stressed network environments. To provide the store-and-<phrase>forward</phrase> service, a " bundle " protocol (<phrase>BP</phrase>) sits at the <phrase>application layer</phrase> of some number of constituent internets, forming a store-and-<phrase>forward</phrase> <phrase>overlay network</phrase> [1]. Key capabilities of the <phrase>BP</phrase> include: Custody-based retransmission the ability to take responsibility for a bundle reaching its final destination Ability to cope with intermittent connectivity. Ability to cope with <phrase>long</phrase> propagation delays. Ability to take advantage of scheduled, predicted, and opportunistic connectivity (in addition to continuous connectivity).
During Thalamocortical Assemblies Monographs of the <phrase>Physiological</phrase> <phrase>Society</phrase> 49 How <phrase>Ion Channels</phrase>, <phrase>Single</phrase> <phrase>Neurons</phrase> and <phrase>Large-scale</phrase> Networks Organize <phrase>Sleep</phrase> Oscillations 2 Monographs of the <phrase>Physiological</phrase> <phrase>Society</phrase> <phrase>sleep</phrase>, the <phrase>mammalian</phrase> <phrase>brain</phrase> generates an orderly progression of low <phrase>frequency</phrase> oscillations. The <phrase>nature</phrase> of these oscillations changes as the <phrase>brain</phrase> moves from <phrase>sleep</phrase> onset into deep <phrase>sleep</phrase>. Although readily measured and recorded, the underlying neural mechanisms involved and the purpose of these oscillations have remained unclear. However, as we learn more about the properties of <phrase>neurons</phrase> in the <phrase>thalamus</phrase> and <phrase>cerebral cortex</phrase> and their interactions , it has become possible to suggest a role for these occurrences. This <phrase>book</phrase> reviews the molecular components and <phrase>ionic</phrase> mechanisms underlying <phrase>sleep</phrase> oscillations, including the properties of <phrase>ion channels</phrase>, <phrase>synaptic</phrase> receptors and the patterns of interconnectivity among <phrase>thalamic</phrase> and <phrase>cortical</phrase> <phrase>neurons</phrase>. These properties have been used to build detailed computational models of thalamocortical assemblies and their <phrase>collective behavior</phrase>. The precision <phrase>experimental</phrase> <phrase>data</phrase> collected has provided a foundation for the study of dynamic activity in the central <phrase>brain</phrase> systems and it is now possible to suggest a role for thalamocortical oscillations in <phrase>memory consolidation</phrase>. Thalamocortical Assemblies is for neuroscientists, neurobiologists, physiologists and other researchers interested in <phrase>sleep</phrase> and <phrase>memory</phrase> processes.
Processing elastic surfaces and related <phrase>gradient</phrase> flows Preface T HIS <phrase>thesis</phrase> would not have been possible without the help, support and inspiration of many people. First of all I owe my deepest gratitude to my <phrase>thesis</phrase> advisor Prof. Dr. Martin Rumpf for all his help, support and guidance. Moreover, I would like to thank him for <phrase>giving me the opportunity</phrase> to widen my perspective through the active participation in workshops and conferences. I am grateful to Prof. Dr. Gerhard Dziuk for <phrase>giving me the opportunity</phrase> to learn about anisotropies when I enjoyed the hospitality of the Institute of <phrase>Applied Mathematics</phrase> at the <phrase>University</phrase> of <phrase>Freiburg</phrase> for several times. I would like to thank him for inspiring discussions on Willmore flow. I thank Prof. Dr. Holger Rauhut for being the co-referee. During the work I always received help from my colleagues at the Institute for Numerical <phrase>Simulation</phrase>, <phrase>University</phrase> of <phrase>Bonn</phrase>. Especially, I would like to thank Benjamin Berkels, Martina Teusner and Orestis Vantzos who patiently answered all my questions on implementational topics as well as theoretical aspects. Furthermore, I am grateful to Orestis Vantzos for helping with the rendering in Figure 4.7. My roommates Stefan von Deylen and Benedict Geihe always endured my moods during the final phase of my <phrase>thesis</phrase>. Additionally, I thank my col-Benedikt Wirth for proof <phrase>reading</phrase> selected chapters of my <phrase>thesis</phrase> and their valuable comments. I want to <phrase>express my gratitude</phrase> to my <phrase>college</phrase> Dr. Martin Lenz for his assistance, particularly in <phrase>GRAPE</phrase> related questions. I had the opportunity to get a deep inside in the <phrase>software</phrase> package <phrase>GRAPE</phrase> that has been developed at the Collaborative <phrase>Research</phrase> <phrase>Center</phrase> 256 at the <phrase>University</phrase> of <phrase>Bonn</phrase> and at the Institute for <phrase>Applied Mathematics</phrase> at the <phrase>University</phrase> of <phrase>Freiburg</phrase> [162, 147, 98]. We developed i.a. visualization tools in cooperation with the Gesellschaft <phrase>fr</phrase> Anlagen-und <phrase>Reaktor</phrase>-sicherheit where I was part of the third project "Weiterentwicklung der Rechenprogramme d 3 f und r 3 t (<phrase>E</phrase>-DuR)" funded by the <phrase>German</phrase> <phrase>Federal</phrase> Ministry of <phrase>Education</phrase> and <phrase>Research</phrase> [90, 91, 95]. I would like to thank Prof. Dr. Dietmar Krner and Mirko Krnkel from the Institute for <phrase>Applied Mathematics</phrase>, <phrase>University</phrase> of <phrase>Freiburg</phrase>, for their hospitality and literally fruitful teamwork on <phrase>programming</phrase> new visualization methods. Most of the visualization in my <phrase>thesis</phrase> was done in <phrase>GRAPE</phrase>. I am grateful to Nathan Litke, Ph.D., for providing me with the pictures in Chapter 7. I especially thank the <phrase>German</phrase> <phrase>Science</phrase> foundation for their financial 
MiRTDL: A <phrase>Deep Learning</phrase> Approach for <phrase>miRNA</phrase> <phrase>Target</phrase> Prediction MicroRNAs miRNAs regulate <phrase>genes</phrase> that are associated with various diseases. To better understand miRNAs, the <phrase>miRNA</phrase> regulatory mechanism needs to be investigated and the real targets identified. Here, we present miRTDL, a new <phrase>miRNA</phrase> <phrase>target</phrase> prediction <phrase>algorithm</phrase> based on <phrase>convolutional neural network</phrase> <phrase>CNN</phrase>. The <phrase>CNN</phrase> automatically extracts essential <phrase>information</phrase> from the <phrase>input data</phrase> rather than completely relying on the input dataset generated artificially when the precise <phrase>miRNA</phrase> <phrase>target</phrase> mechanisms are poorly known. In this work, the constraint relaxing method is first used to construct a balanced training dataset to avoid inaccurate predictions caused by the existing unbalanced dataset. The miRTDL is then applied to 1,606 experimentally validated <phrase>miRNA</phrase> <phrase>target</phrase> pairs. Finally, the <phrase>results</phrase> show that our miRTDL outperforms the existing <phrase>target</phrase> prediction <phrase>algorithms</phrase> and achieves significantly higher sensitivity, specificity and accuracy of 88.43, 96.44, and 89.98 percent, respectively. We also investigate the <phrase>miRNA</phrase> <phrase>target</phrase> mechanism, and the <phrase>results</phrase> show that the complementation features are more important than the others.
Analyzing <phrase>Linguistic</phrase> <phrase>Knowledge</phrase> in Sequential <phrase>Model</phrase> of Sentence Sentence modelling is a fundamental topic in <phrase>computational linguistics</phrase>. Recently, <phrase>deep learning</phrase>-based sequential models of sentence , such as <phrase>recurrent neural network</phrase>, have proved to be effective in dealing with the non-sequential properties of <phrase>human</phrase> <phrase>language</phrase>. However, little is known about how a <phrase>recurrent neural network</phrase> captures <phrase>linguistic</phrase> <phrase>knowledge</phrase>. Here we propose to correlate the <phrase>neuron</phrase> activation pattern of a LSTM <phrase>language</phrase> <phrase>model</phrase> with rich <phrase>language</phrase> features at sequential, lexical and compositional level. Qualitative visualization as well as quantitative analysis under multilingual perspective reveals the effectiveness of gate <phrase>neurons</phrase> and indicates that LSTM learns to allow different <phrase>neurons</phrase> selectively respond to <phrase>linguistic</phrase> <phrase>knowledge</phrase> at different levels. Cross-<phrase>language</phrase> evidence shows that the <phrase>model</phrase> captures different aspects of <phrase>linguistic</phrase> properties for different languages due to the <phrase>variance</phrase> of <phrase>syntactic</phrase> complexity. Additionally, we analyze the influence of modelling strategy on <phrase>linguistic</phrase> <phrase>knowledge</phrase> encoded implicitly in different sequential models.
<phrase>Semi-supervised</phrase> <phrase>deep learning</phrase> by metric embedding Deep networks are successfully used as classification models yielding <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> when trained on a large number of <phrase>labeled samples</phrase>. These models, however, are usually much less suited for <phrase>semi-supervised</phrase> problems because of their tendency to overfit easily when trained on small amounts of <phrase>data</phrase>. In this work we will explore a new training objective that is targeting a <phrase>semi-supervised</phrase> regime with only a small <phrase>subset</phrase> of <phrase>labeled data</phrase>. This criterion is based on a deep metric embedding over distance relations within the set of <phrase>labeled samples</phrase>, together with constraints over the embeddings of the unlabeled set. The final learned representations are discriminative in <phrase>euclidean space</phrase>, and hence can be used with subsequent nearest-neighbor classification using the <phrase>labeled samples</phrase>.
Towards Vision-Based <phrase>Deep Reinforcement Learning</phrase> for Robotic <phrase>Motion Control</phrase> This <phrase>paper</phrase> introduces a <phrase>machine learning</phrase> based system for controlling a robotic manipulator with <phrase>visual perception</phrase> only. The capability to autonomously learn <phrase>robot</phrase> controllers solely from raw-<phrase>pixel</phrase> images and without any <phrase>prior knowledge</phrase> of configuration is shown for the first time. We build upon the success of recent <phrase>deep reinforcement learning</phrase> and develop a system for learning <phrase>target</phrase> reaching with a three-joint <phrase>robot</phrase> manipulator using external visual observation. A <phrase>Deep Q</phrase> Network (DQN) was demonstrated to perform <phrase>target</phrase> reaching after training in <phrase>simulation</phrase>. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing <phrase>camera</phrase> images with synthetic images.
Random <phrase>synaptic</phrase> <phrase>feedback</phrase> weights support error <phrase>backpropagation</phrase> for <phrase>deep learning</phrase> The <phrase>brain</phrase> processes <phrase>information</phrase> through <phrase>multiple layers</phrase> of <phrase>neurons</phrase>. This <phrase>deep architecture</phrase> is representationally powerful, but complicates learning because it is difficult to identify the responsible <phrase>neurons</phrase> when a mistake is made. In <phrase>machine learning</phrase>, the <phrase>backpropagation</phrase> <phrase>algorithm</phrase> assigns blame by multiplying error signals with all the <phrase>synaptic</phrase> weights on each neuron's <phrase>axon</phrase> and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the <phrase>brain</phrase>. Here we demonstrate that this strong <phrase>architectural</phrase> constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random <phrase>synaptic</phrase> weights. This mechanism can transmit teaching signals across <phrase>multiple layers</phrase> of <phrase>neurons</phrase> and performs as effectively as <phrase>backpropagation</phrase> on a <phrase>variety</phrase> of tasks. Our <phrase>results</phrase> help reopen questions about how the <phrase>brain</phrase> could use error signals and dispel <phrase>long</phrase>-held assumptions about algorithmic constraints on learning.
Building a <phrase>Machine Learning</phrase> Based Text Understanding System 1 <phrase>Information Extraction</phrase> from <phrase>Free</phrase> Text Text understanding systems are approaching the point of being a practical <phrase>technology</phrase> as <phrase>long</phrase> as the system is trained for a narrowly defined domain. <phrase>Machine learning</phrase> and statistical approaches can minimize the effort involved in adapting a text understanding system to a new domain. This <phrase>paper</phrase> presents a system whose goal is <phrase>deep understanding</phrase>, limited only by the necessity of designing a formal representation of the <phrase>target</phrase> concepts relevant to the domain. This system is an advance over previous <phrase>machine learning</phrase> based systems because of its richer output representation, and an advance over equally expressive text understanding systems because of its more extensive use of <phrase>machine learning</phrase>. A <phrase>variety</phrase> of systems have been developed in recent years that extract <phrase>information</phrase> from text. None of them attempts <phrase>general</phrase>-purpose understanding, but instead focus on narrowly defined <phrase>information</phrase> needs. A domain is defined as a collection of documents and a clear definition of the <phrase>target</phrase> concepts for that domain. Some systems look for fairly simple types of <phrase>information</phrase> such as scanning seminar announcements for time, location, and <phrase>speaker</phrase> [Freitag, 1998], on-line job postings for position title, location, skills needed, and salary [Califf and Mooney, 1997], or rental ads for number of bedrooms, price, and <phrase>neighborhood</phrase> [Soderland, 1999]. <phrase>E</phrase> verything else in the document is ignored as irrelevant. <phrase>Information</phrase> systems that aim for a <phrase>deeper understanding</phrase> have come out of a series of <phrase>DARPA</phrase> sponsored Message Understanding Conferences. These systems must still be adapted to one domain at a time, but have a richer representation of the <phrase>information</phrase> relevant to the domain. One example is the <phrase>terrorism</phrase> domain from the MUC-4 conference [1992], which consisted of <phrase>news</phrase> stories about <phrase>Latin American</phrase> <phrase>terrorism</phrase> and represented the <phrase>target</phrase> concepts with an output template for each specific event with details about the perpetrators, victims, and physical targets. Another example is the <phrase>joint ventures</phrase> domain of the MUC-5 conference [1993] where the domain consisted of <phrase>news</phrase> stories about joint <phrase>business</phrase> ventures and the <phrase>target</phrase> concept was details about the companies involved, the <phrase>business</phrase> activity, and the ownership of the <phrase>joint venture</phrase>. Work on <phrase>medical</phrase> text understanding systems also aim at <phrase>deep understanding</phrase>. Examples of this are the work of Friedman et al. [1997, 1994] and of <phrase>Taira</phrase> and Soderland [2001], both groups initially developing their systems in the domain of <phrase>radiology</phrase> reports. The <phrase>target</phrase> concept is the findings mentioned in the <phrase>report</phrase> and details about their properties: such 
A better way to learn features: technical perspective A TYPICAL <phrase>MACHINE learning</phrase> program uses weighted combinations of features to discriminate between classes or to predict <phrase>real-valued</phrase> outcomes. The <phrase>art</phrase> of <phrase>machine learning</phrase> is in constructing the features, and a radically new method of creating features constitutes a <phrase>major</phrase> advance. In the 1980s, the new method was <phrase>backpropagation</phrase>, which uses the <phrase>chain rule</phrase> to backpropagate error derivatives through a multilayer, <phrase>feed-forward</phrase>, <phrase>neural network</phrase> and adjusts the weights between layers by following the <phrase>gradient</phrase> of the backpropagated error. This worked well for recognizing simple shapes, such as <phrase>handwritten digits</phrase>, especially in <phrase>convolutional neural networks</phrase> that use local feature detectors replicated across the image. 5 For many tasks, however, it proved extremely difficult to optimize deep <phrase>neural nets</phrase> with many layers of non-linear features, and a huge number of <phrase>labeled training</phrase> cases was required for large <phrase>neural networks</phrase> to generalize well to <phrase>test</phrase> <phrase>data</phrase>. In the 1990s, <phrase>Support Vector Machines</phrase> (SVMs) 8 introduced a very different way of creating features: the user defines a kernel <phrase>function</phrase> that computes the similarity between two input vectors, then a judiciously chosen <phrase>subset</phrase> of the <phrase>training examples</phrase> is used to create " landmark " features that measure how similar a <phrase>test case</phrase> is to each training case. SVMs have a clever way of choosing which training cases to use as landmarks and deciding how to weight them. They work remarkably well on many <phrase>machine learning</phrase> tasks even though the selected features are non-adaptive. The success of SVMs dampened the earlier enthusiasm for <phrase>neural networks</phrase>. More recently, however, it has been shown that <phrase>multiple layers</phrase> of feature detectors can be learned greedily, one layer at a time, by using <phrase>unsupervised learning</phrase> that does not require <phrase>labeled data</phrase>. The features in each layer are designed to <phrase>model</phrase> the statistical structure of the patterns of feature activations in the previous layer. After learning several layers of features this way without paying any attention to the final goal, many of the <phrase>high-level</phrase> features will be irrelevant for any particular task, but others will be highly relevant because <phrase>high</phrase>-<phrase>order</phrase> correlations are the signature of the data's true underlying causes and the <phrase>labels</phrase> are more directly related to these causes than to the raw inputs. A subsequent stage of <phrase>fine-tuning</phrase> using <phrase>backpropagation</phrase> then yields <phrase>neural networks</phrase> that work much better than those trained by <phrase>backpropagation</phrase> alone and better than SVMs for important tasks such as object or <phrase>speech recognition</phrase>. The neural 
<phrase>Peer Assessment</phrase> for <phrase>Action</phrase> Learning of <phrase>Data Structures</phrase> and <phrase>Algorithms</phrase> This <phrase>paper</phrase> describes an experience with use of <phrase>peer assessment</phrase> in tutorials as a tool to <phrase>promote deep learning</phrase> from early stages of a course on <phrase>Data Structures</phrase> and <phrase>Algorithms</phrase>. The goal was to improve the utility of tutorials in encouraging more efficient learning habits. Since assessment forms a key part of the actual <phrase>curriculum</phrase>, tutorial exercises were for credit, but the emphasis was on <phrase>formative assessment</phrase>. The novelty in this approach is that <phrase>peer assessment</phrase> has not been extensively studied in <phrase>Computer Science</phrase> <phrase>Education</phrase> for content of the kind <phrase>covered</phrase> in this course. Evaluation is limited by the fact that other details of the course were changed. Two surveys were conducted, one soon after the first assignment, the other soon after the second assignment. Of various aspects of the course surveyed , the tutorial quizzes were the least popular, but improved in popularity between the two surveys. The overall effect based on <phrase>general</phrase> observation of the class appeared to be positive. <phrase>Results</phrase> were closer to a <phrase>normal distribution</phrase> than for the previous 2 years. Performance in the first assignment, which required understanding of how the theory is applied in a practical situation, suggested that <phrase>deep learning</phrase> had taken place.
Going Deeper with Deep <phrase>Knowledge</phrase> Tracing Over the last couple of decades, there have been a large <phrase>variety</phrase> of approaches towards modeling <phrase>student</phrase> <phrase>knowledge</phrase> within <phrase>intelligent tutoring</phrase> systems. With the booming development of <phrase>deep learning</phrase> and <phrase>large-scale</phrase> <phrase>artificial neural networks</phrase>, there have been empirical successes in a number of <phrase>machine learning</phrase> and <phrase>data mining</phrase> applications, including <phrase>student</phrase> <phrase>knowledge</phrase> modeling. Deep <phrase>Knowledge</phrase> Tracing (DKT), a pioneer <phrase>algorithm</phrase> that utilizes <phrase>recurrent neural networks</phrase> to <phrase>model</phrase> <phrase>student</phrase> learning, reports substantial improvements in prediction performance. To help the EDM <phrase>community</phrase> better understand the promising techniques of <phrase>deep learning</phrase>, we examine DKT alongside two well-studied models for <phrase>knowledge</phrase> modeling, PFA and BKT. In addition to sharing a primer on the internal computational structures of DKT, we also <phrase>report</phrase> on potential issues that arise from <phrase>data</phrase> formatting. We take steps to reproduce the experiments of Deep <phrase>Knowledge</phrase> Tracing by implementing a DKT <phrase>algorithm</phrase> using Google's TensorFlow framework; we also reproduce similar <phrase>results</phrase> on new datasets. We determine that the DKT findings don't hold an overall edge when compared to the PFA <phrase>model</phrase>, when applied to properly prepared datasets that are limited to main (i.e. non-<phrase>scaffolding</phrase>) questions. More importantly, during the investigation of DKT, we not only discovered a <phrase>data quality</phrase> issue in a <phrase>public</phrase> available <phrase>data set</phrase>, but we also detected a vulnerability of DKT at how it handles multiple skill sequences.
Exploring relations among <phrase>college</phrase> students' <phrase>prior knowledge</phrase>, implicit theories of <phrase>intelligence</phrase>, and self-regulated learning in a <phrase>hypermedia</phrase> environment In most cases authors are permitted to post their version of the article (e.g. in Word or Tex form) to their personal <phrase>website</phrase> or <phrase>institutional repository</phrase>. Authors requiring further <phrase>information</phrase> regarding Elsevier's archiving and <phrase>manuscript</phrase> policies are encouraged to visit: a b s t r a c t Researchers and educators continue to explore how to assist students in the acquisition of <phrase>conceptual understanding</phrase> of complex <phrase>science</phrase> topics. While <phrase>hypermedia</phrase> <phrase>learning environments</phrase> (HLEs) afford unique opportunities to display <phrase>multiple representations</phrase> of these often abstract topics, students who do not engage in self-regulated learning (SRL) with HLEs often fail to achieve <phrase>conceptual understanding</phrase>. There is a lack of <phrase>research</phrase> regarding how <phrase>student</phrase> characteristics, such as <phrase>prior knowledge</phrase> and students' implicit theory of <phrase>intelligence</phrase> (ITI), interact with SRL to influence <phrase>academic</phrase> performance. In this study, <phrase>structural equation modeling</phrase> was used to investigate these issues. It was found that <phrase>prior knowledge</phrase> and ITI were related to SRL and performance, and that SRL acted as a benevolent moderator, enhancing the positive effects of <phrase>prior knowledge</phrase> upon learning, and diminishing the negative effects of having a maladaptive ITI. In the <phrase>United States</phrase> and around the world, advancement through scientific understanding has been one of the primary engines of <phrase>economic development</phrase> (<phrase>National Academy of Sciences</phrase>, 2007). Yet students in the <phrase>United States</phrase> continue to struggle with even <phrase>basic</phrase> understanding in <phrase>science</phrase>, as evidenced by an overall decrease in <phrase>average</phrase> <phrase>student</phrase> performance on the <phrase>science</phrase> portion of the National Deep <phrase>conceptual understanding</phrase> in <phrase>science</phrase>, which includes declarative as well as <phrase>procedural knowledge</phrase> of <phrase>complex systems</phrase> (Schraw, 2006), is even less common among <phrase>U.S</phrase>. students. A preponderance of evidence supports the claim that, particularly in <phrase>science</phrase> domains, <phrase>complex systems</phrase> (e.g., the <phrase>circulatory</phrase> system) are difficult for students to learn (Chi, 2005; Hmelo-<phrase>Silver</phrase> & Azevedo, 2006). Yet this <phrase>conceptual understanding</phrase> is necessary if students are to be able to apply and transfer their <phrase>knowledge</phrase> to <phrase>real world</phrase> problems (Roth, 1990). Researchers and educators have studied whether <phrase>hypermedia</phrase> <phrase>learning environments</phrase> (HLEs) might be used to foster <phrase>conceptual understanding</phrase> in <phrase>science</phrase> (Azevedo, 2005; Jacobson, 2008). While the <phrase>multiple representations</phrase> afforded by HLEs can help students acquire sophisticated understanding (Lajoie & Azevedo, 2006), they are most effective when students are able to self-regulate their learning highlight that successful students actively engage in planning, monitoring, and assessing the efficacy of the strategies and operations they use to acquire <phrase>knowledge</phrase>, and that these students 
Analogies, Explanations, and Practice: Examining How Task Types Affect Second <phrase>Language</phrase> <phrase>Grammar</phrase> Learning Self-explanation is an effective instructional strategy for improving <phrase>problem solving</phrase> in <phrase>math</phrase> and <phrase>science</phrase> domains. However, our previous studies, within the domain of second <phrase>language</phrase> <phrase>grammar</phrase> learning, show self-explanation to be no more effective than simple practice; perhaps the metalinguistic challenges involved in explaining using one's non-<phrase>native</phrase> <phrase>language</phrase> are hampering the potential benefits. An <phrase>alternative</phrase> strategy is tutoring using analogical comparisons , which reduces <phrase>language</phrase> difficulties while continuing to encourage feature focusing and deep processing. In this <phrase>paper</phrase>, we investigate adult <phrase>English language</phrase> learners learning the <phrase>English</phrase> article system (e.g. the difference between " a <phrase>dog</phrase> " and " the <phrase>dog</phrase> "). We present the <phrase>results</phrase> of a classroom-based study (N=99) that compares practice-only to two conditions that facilitate deep processing: self-explanation with practice and analogy with practice. <phrase>Results</phrase> show that students in all conditions benefit from the instruction. However, students in the practice-only condition complete the instruction in significantly less time leading to greater learning efficiency. Possible explanations regarding the differences between <phrase>language</phrase> and <phrase>science</phrase> learning are discussed.
Implementing <phrase>Latent Semantic Analysis</phrase> in <phrase>Learning Environments</phrase> with Conversational Agents and Tutorial Dialog We have been developing <phrase>learning environments</phrase> with <phrase>animated</phrase> conversational agents. The agents manage a mixed-<phrase>initiative</phrase> dialog between the learner and the computer system either by a direct conversational interaction or by serving as a navigational guide on a <phrase>web site</phrase>. Two of the systems simulate <phrase>human</phrase> tutors by (a) presenting difficult questions that require deep reasoning, (b) attempting to comprehend the learner's typed input, (c) formulating dialog acts that are sensitive to the learner's contributions, and (d) speaking to the <phrase>student</phrase> with the <phrase>animated</phrase> agent. AutoTutor teaches computer <phrase>literacy</phrase> whereas Why/AutoTutor teaches conceptual <phrase>physics</phrase> (Graesser, VanLehn, <phrase>Rose</phrase>, <phrase>Jordan</phrase>, & Harter, 2001). The <phrase>Human</phrase> Use Regulatory Affairs Advisor (HURAA) teaches officers in the <phrase>military</phrase> about the <phrase>ethical</phrase> use of <phrase>human</phrase> subjects on a <phrase>web site</phrase> with a search facility that accesses documents through questions posed in <phrase>natural language</phrase>. All three systems have used <phrase>Latent Semantic Analysis</phrase> (<phrase>LSA</phrase>) as its primary representation of world <phrase>knowledge</phrase>. <phrase>LSA</phrase> is a statistical technique that compresses a large corpus texts into a space of 100-500 dimensions (Landauer, Foltz, & Laham, 1998). The K-dimensional space is used when evaluating the similarity between any two bags of words, with values ranging from 0 to 1. From the standpoint of AutoTutor and Why/AutoTutor, one bag of words is the set of assertions that a <phrase>student</phrase> expresses within a dialog turn; the other bag of words is the content of the <phrase>curriculum</phrase> script for a particular topic. From the standpoint of HURAA, one bag of words is the learner's query in <phrase>natural language</phrase> and the other is a paragraph in the document space. <phrase>LSA</phrase> has generally been successful in evaluating the quality of <phrase>student</phrase> explanations, in evaluating the quality of <phrase>student</phrase> assertions in tutorial dialog, and in the retrieval of documents from <phrase>natural language</phrase> queries. Successes and failures of <phrase>LSA</phrase> are identified in these three <phrase>learning environments</phrase>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of <phrase>ONR</phrase>, IDA, <phrase>DoD</phrase> or <phrase>NSF</phrase>.
Double Loop Learning in Work based Settings Students in work based settings gain access to a vast amount of <phrase>knowledge</phrase>. This workshop contribution introduces micro articles as a tool to make practical <phrase>knowledge</phrase> persistent. Micro articles are used to establish double loop learning in the sense that the learners not only consume but also provide learning objects. The aspired effects are a deeper reflection for the original learner as well as an increase in the available <phrase>knowledge</phrase> that is also interesting for other learners. 1 Introduction Learning a profession is much more for a <phrase>student</phrase> than the mere memorization of facts. [1], [2], [3] Thus learning a profession can only take place within this profession. However, a pure being within a profession will normally not form experts, but people who are only experienced but do not possess a deep expertise. [3], [4] Our <phrase>general</phrase> <phrase>research</phrase> is therefore about navigating between the faults of a too theoretical and a too little grounded <phrase>education</phrase> and about minimizing the suboptimal. One <phrase>aspect</phrase> of this <phrase>research</phrase> is the <phrase>feedback</phrase> that students in work based settings can give on common theories and their application to practical problems and how this <phrase>feedback</phrase> can be used by other learners. To foster this <phrase>information</phrase> exchange we developed tools like e.g. an <phrase>e</phrase>-<phrase>journal</phrase> which can be found at http://www.e-<phrase>journal</phrase>-of-pbr.info. Yet we found that the format of an <phrase>academic journal</phrase> attracts more the <phrase>academic</phrase> <phrase>writer</phrase> and not primarily the practical user of <phrase>academic</phrase> concepts. Therefore we thought about ways to establish structured but easy to use <phrase>communication</phrase> tools which learners in work based settings can use to share <phrase>information</phrase> about the application, the advantages and the limitations of <phrase>academic</phrase> concepts. This workshop contribution reports our first evaluation of so called micro articles which are a concise and structured means of <phrase>communication</phrase> and thus promising to be used in work based settings.
Tactile <phrase>web browsing</phrase> for blind people <phrase>Information</phrase> on the <phrase>World Wide Web</phrase> becomes more and more important for our <phrase>society</phrase>. For blind people this is a chance to access more <phrase>information</phrase> for their everyday <phrase>life</phrase>. In this <phrase>paper</phrase> we propose novel methods to present <phrase>web pages</phrase> including <phrase>graphical</phrase> <phrase>information</phrase> on a tactile output device. We present a <phrase>Mozilla Firefox</phrase> Extension for the tactile rendering of <phrase>web pages</phrase> and for the handling of user interactions. This approach benefits from the <phrase>Firefox</phrase> built-in web page handling including <phrase>parsing</phrase> of <phrase>HTML</phrase> documents, formatting with <phrase>Cascading Style Sheets</phrase> (<phrase>CSS</phrase>), handling of dynamic web content controlled by <phrase>JavaScript</phrase> code, etc. <phrase>Graphical</phrase> <phrase>information</phrase> can be explored and filtered interactively in a special mode for <phrase>raster</phrase> images and <phrase>Scalable Vector Graphics</phrase> (SVG). <phrase>Mathematical</phrase> expressions encoded in the <phrase>Mathematical</phrase> <phrase>Markup Language</phrase> (<phrase>MathML</phrase>) are transformed directly into <phrase>LaTeX</phrase> or into a notation for blind people. The tactile <phrase>web browser</phrase> supports <phrase>feedback</phrase> that is provided via voice output. 1 <phrase>Motivation</phrase> The development of the <phrase>World Wide Web</phrase> is a great benefit not only for sighted people but also for blind people. The web enables blind people to access <phrase>information</phrase> much easier than by printed <phrase>media</phrase> for example. Potentially the web gives blind people the possibility to access the same <phrase>information</phrase> like sighted people do. In <phrase>e</phrase>-learning courses for example blind people may even take part in ordinary programs of study. Unfortunately there are some problems blind people have to face with when browsing the web. Since <phrase>web pages</phrase> get more and more complex, <phrase>information</phrase> is not only presented in simple text but also as <phrase>graphical</phrase> <phrase>information</phrase> and layout. Especially in scientific <phrase>education</phrase> it is necessary to have access to images, diagrams , and formulas. If these parts are not accessible, blind students may not get a <phrase>deep understanding</phrase> of the relations in the learning material. Currently blind people mainly use screen readers to access <phrase>web pages</phrase>. Screen readers extract the textual <phrase>information</phrase> and linearize it for the output on <phrase>Braille</phrase>
<phrase>Neural Network</phrase>-based Underwater <phrase>Image Classification</phrase> for Autonomous Underwater Vehicles <phrase>Image processing</phrase> has been one of hot issues for <phrase>real world</phrase> <phrase>robot</phrase> applications such as <phrase>navigation</phrase> and visual servoing. In case of underwater <phrase>robot</phrase> application, however, conventional optical <phrase>camera</phrase>-based images have many limitations for real application due to visibility in <phrase>turbid</phrase> <phrase>water</phrase>, image saturation under underwater <phrase>light</phrase> in the deep <phrase>water</phrase>, and <phrase>short</phrase> visible <phrase>range</phrase> in the <phrase>water</phrase>. Thus, most of underwater image applications use <phrase>high</phrase> <phrase>frequency</phrase> <phrase>sonar</phrase> to get precise <phrase>acoustic</phrase> image. There have been some approaches to apply optical <phrase>image processing</phrase> methods to <phrase>acoustic</phrase> image, but performance is still not good enough for automatic classification/recognition. In this <phrase>paper</phrase>, a <phrase>neural network</phrase>-based <phrase>image processing</phrase> <phrase>algorithm</phrase> is proposed for <phrase>acoustic</phrase> <phrase>image classification</phrase>. Especially, shadow of an <phrase>acoustic</phrase> object is mainly used as a cue of the classification. The <phrase>neural network</phrase> classifies a pre-taught image from noisy and/or occlude object images. In <phrase>order</phrase> to get fast learning and retrieving, a Bidirectional <phrase>Associative Memory</phrase> (BAM) is used. It is remarked that the BAM doesn't need many learning trials, but just simple <phrase>multiplication</phrase> of two vectors for generating a correlation matrix. However, because of the simple calculation, it is not guaranteed to learn and recall all <phrase>data set</phrase>. Thus, it is needed to modify the BAM for improving its performance. In this <phrase>paper</phrase>, <phrase>complement</phrase> <phrase>data set</phrase> and weighted learning factor are used to improve the BAM performance. The <phrase>test</phrase> <phrase>results</phrase> show that the <phrase>proposed method</phrase> successfully classified 4 pre-taught object images from various underwater object images with up to 50% of B/W noise.
Exploratory <phrase>Engineering</phrase> in <phrase>Ai</phrase> We regularly see examples of new <phrase>artificial intelligence</phrase> (<phrase>AI</phrase>) capabilities. Google's self-driving <phrase>car</phrase> has safely traversed thousands of miles. Watson beat the <phrase>Jeopardy</phrase>! champions, and <phrase>Deep Blue</phrase> beat the <phrase>chess</phrase> champion. <phrase>Boston</phrase> Dynamics' Big <phrase>Dog</phrase> can walk over uneven terrain and right itself when it <phrase>falls</phrase> over. From many angles, <phrase>software</phrase> can recognize faces as well as people can. As their capabilities improve, <phrase>AI</phrase> systems will become increasingly <phrase>independent</phrase> of humans. We will be no more able to monitor their decisions than we are now able to check all the <phrase>math</phrase> done by today's <phrase>computers</phrase>. No doubt such <phrase>automation</phrase> will produce tremendous economic value, but will we be able to trust these advanced autonomous systems with so much capability? For example, consider the autonomous trading programs which <phrase>lost</phrase> <phrase>Knight</phrase> <phrase>Capital</phrase> $440 million (pre-<phrase>tax</phrase>) on August 1st, 2012, requiring the <phrase>firm</phrase> to quickly raise $400 million to avoid <phrase>bankruptcy</phrase>. 1 This event undermines a common view that <phrase>AI</phrase> systems cannot cause much harm because they will only ever be tools of <phrase>human</phrase> <phrase>masters</phrase>. Autonomous trading programs make millions of trading decisions per day, and they were given sufficient capability to nearly <phrase>bankrupt</phrase> one of the largest traders in <phrase>U.S</phrase>. equities. Today, <phrase>AI</phrase> <phrase>safety engineering</phrase> mostly consists in a combination of <phrase>formal methods</phrase> and testing. Though powerful, these methods lack foresight: they can be applied only to particular extant systems. We describe a third, complementary approach which aims to predict the (potentially hazardous) properties and behaviors of broad classes of future <phrase>AI</phrase> agents, based on their <phrase>mathematical structure</phrase> (e.g. <phrase>reinforcement learning</phrase>). Such projects hope to discover methods "for determining whether the behavior of learning agents [will remain] within the bounds of pre-specified constraints... after learning." 2 We call this approach "exploratory <phrase>engineering</phrase> in <phrase>AI</phrase>."
<phrase>Pattern Matching</phrase> Using Layered Strifa for <phrase>Intrusion Detection</phrase> With the advent and explosive growth of the global <phrase>Internet</phrase> adaptive/automatic network intrusion and <phrase>anomaly detection</phrase> in wide <phrase>area</phrase> <phrase>data</phrase> networks is fast gaining critical <phrase>research</phrase> and practical importance. In <phrase>order</phrase> to detect intrusions in a network, need efficient IDS. <phrase>Deep packet inspection</phrase> (DPI) has the ability to inspect both packet headers and payloads to identify the attack signatures in <phrase>order</phrase> to protect <phrase>Internet</phrase> systems. <phrase>Regular expression</phrase> matching, despite its flexibility and efficiency in attack detection, brings <phrase>high</phrase> computation and storage complexities to NIDSs, making packet processing a bottleneck. Stride finite <phrase>automata</phrase> (StriFA), a new <phrase>family</phrase> of finite <phrase>automata</phrase>, to accelerate both string matching and <phrase>regular expression</phrase> matching with reduced <phrase>memory</phrase> consumption. To increase the efficiency of StriFA, a layered approach of attack detection by using KDD 99 <phrase>DARPA</phrase> dataset is integrated with StriFA. We demonstrate that attack detection accuracy can be achieved by using StriFA and <phrase>high</phrase> efficiency by implementing the Layered Approach. I. INTRODUCTION Intrusions are the abnormal events happening in the computer system or network which attempts to compromise the confidentiality and availability of <phrase>data</phrase> or a system or a network. Intrusions are caused by attackers who seek to gain extra prerogatives by getting at a system from the <phrase>internet</phrase>; however they may be unauthorized user or the authorized users misusing their prerogatives. <phrase>Intrusion detection</phrase> is the mechanism of supervising events occurring in the networks to detect the abnormal behaviours of events i.e. intrusions. The most common approaches in <phrase>intrusion detection</phrase> system are <phrase>anomaly detection</phrase> and misuse detection. <phrase>Anomaly detection</phrase> can identify the activities that vary from the common behaviour, and thus have the potential to detect novel attacks. An approach for detecting intrusions is to conceptualize both the normal and the known attack patterns for training a system, then performing classification of the <phrase>test</phrase> <phrase>data</phrase>. It integrates the advantages of both the signature-based and the anomaly-based detections, known as the <phrase>Hybrid</phrase> System. <phrase>Hybrid</phrase> systems are effective, subject to the categorization method used. They can be used to classify the unseen or new instances when occur, and then they assign one of the known classes to every <phrase>test</phrase> instance, because during training the system learns patterns and features from all the classes. But the only problem with the <phrase>hybrid</phrase> systems is the availability of labelled <phrase>data</phrase>. However, <phrase>data</phrase> requirement is also a concern for the signature, and the anomaly-based
An analytic approach to better understanding and <phrase>management</phrase> of coronary surgeries In most cases authors are permitted to post their version of the article (e.g. in Word or Tex form) to their personal <phrase>website</phrase> or <phrase>institutional repository</phrase>. Authors requiring further <phrase>information</phrase> regarding Elsevier's archiving and <phrase>manuscript</phrase> policies are encouraged to visit: a b s t r a c t a r t i c l <phrase>e</phrase> i n f o Keywords: <phrase>Heart disease</phrase> <phrase>Coronary artery bypass surgery</phrase> (CABG) Clinical <phrase>decision support systems</phrase> Survival prediction <phrase>Data mining</phrase> <phrase>Machine learning</phrase> <phrase>Sensitivity analysis</phrase> Demand for <phrase>high</phrase>-quality, affordable <phrase>healthcare</phrase> services increasing with the aging <phrase>population</phrase> in the US. In <phrase>order</phrase> to cope with this situation, decision makers in <phrase>healthcare</phrase> (managerial, administrative and/or clinical) need to be increasingly more effective and efficient at what they do. Along with expertise, <phrase>information</phrase> and <phrase>knowledge</phrase> are the other key sources for better decisions. <phrase>Data mining</phrase> techniques are becoming a popular tool for extracting <phrase>information</phrase>/<phrase>knowledge</phrase> hidden deep into large <phrase>healthcare</phrase> <phrase>databases</phrase>. In this study, using a large, feature-rich, nationwide inpatient <phrase>databases</phrase> along with four popular <phrase>machine learning</phrase> techniques, we developed predictive models; and using an <phrase>information</phrase> <phrase>fusion</phrase> based <phrase>sensitivity analysis</phrase> on these models, we explained the surgical outcome of a patient undergoing a coronary artery bypass <phrase>grafting</phrase>. In this study, <phrase>support vector machines</phrase> <phrase>produced</phrase> the best prediction <phrase>results</phrase> (87.74%) followed by <phrase>decision trees</phrase> and <phrase>neural networks</phrase>. Studies like this illustrate the fact that accurate prediction and better understanding of such complex <phrase>medical</phrase> interventions can potentially <phrase>lead</phrase> to more favorable outcomes and optimal use of limited <phrase>healthcare</phrase> resources. In recent years, <phrase>healthcare</phrase> has become one of the most spoken issues that have a direct impact on quality of <phrase>life</phrase> in the US and abroad. While the demand for <phrase>healthcare</phrase> services is increasing with the aging <phrase>population</phrase>, the supply side is having serious problems to keep up with the needed level and quality of service. In <phrase>order</phrase> to close the gap, <phrase>healthcare</phrase> systems ought to significantly improve their operational efficacy (i.e., effectiveness and efficiency). Effectiveness (doing the right thing, e.g., diagnosing and treating accurately) and efficiency (doing it the right way, e.g., using least amount of resources and using the least amount of time) are the two fundamental pillars upon which the <phrase>healthcare</phrase> system can be revived [16]. One promising way to improve the <phrase>healthcare</phrase> efficacy is to take advantage of advanced modeling techniques and large and feature rich <phrase>data</phrase> sources (true reflections of <phrase>medical</phrase> and <phrase>healthcare</phrase> experiences) to support accurate and 
Gated Softmax Classification We describe a " log-<phrase>bilinear</phrase> " <phrase>model</phrase> that computes class <phrase>probabilities</phrase> by combining an input <phrase>vector</phrase> multiplicatively with a <phrase>vector</phrase> of <phrase>binary</phrase> <phrase>latent variables</phrase>. Even though the <phrase>latent variables</phrase> can take on exponentially many possible combinations of values, we can efficiently compute the exact <phrase>probability</phrase> of each class by marginalizing over the <phrase>latent variables</phrase>. This makes it possible to get the exact <phrase>gradient</phrase> of the <phrase>log likelihood</phrase>. The <phrase>bilinear</phrase> score-functions are defined using a three-dimensional weight <phrase>tensor</phrase>, and we show that factorizing this <phrase>tensor</phrase> allows the <phrase>model</phrase> to encode invariances inherent in a task by learning a <phrase>dictionary</phrase> of invariant basis functions. Experiments on a set of benchmark problems show that this fully <phrase>probabilistic model</phrase> can achieve classification performance that is competitive with (kernel) SVMs, <phrase>backpropagation</phrase>, and <phrase>deep belief</phrase> nets.
Contrasts in learning: a collaborative evaluation by practitioners and students This <phrase>paper</phrase> documents some of the learning emanating from a learner-centred evaluation of a change from face-to-face to online lectures in an Organisational Behaviour course at <phrase>RMIT University</phrase>. An <phrase>academic</phrase> and learning <phrase>technology</phrase> mentor conducted the evaluation, working as co-researchers with 'Managing Change' students. Primarily, the evaluation aimed to illuminate the experience of using the online lectures, as perceived by <phrase>Engineering</phrase> students. As preliminary <phrase>research</phrase> to a wider study , the evaluation was also formative. The findings suggest that the <phrase>Engineering</phrase> students appreciated the flexibility of online lectures. However, they devalued the online experience because it lacked the interaction of face-to-face lectures and consequently adopted a surface approach to learning. The strong message emerging from the <phrase>student</phrase> evaluation <phrase>data</phrase> was the importance of social interaction for understanding organisational behaviour concepts. The practitioners could not initially 'see' this message, such were the demands of managing the <phrase>logistics</phrase> of the change to online delivery and the lack of adequate organisational support. The surface level of learning for the <phrase>Engineering</phrase> students is starkly contrasted with the deep level of learning for the researchers, leading to transformation for the Managing Change students and eventually the practitioners. Context This evaluation is focussed around an introductory course in organisational behaviour. The course is a core first-year unit for several <phrase>undergraduate</phrase> <phrase>Business</phrase> programs, as well as being offered to <phrase>Engineering</phrase> and <phrase>Science</phrase> undergraduates as part of their <phrase>double-degree</phrase> program. The initial emphasis of the studyto trial the delivery of lectures onlinesits within a broader strategic framework to 'renew' (<phrase>RMIT</phrase>, 2000) large courses (often involving in excess of six hundred students) for flexible delivery. Other demands upon the lecturers who deliver the course include catering for diverse <phrase>student</phrase> demographics and, increasingly, the requirement to deliver the course in a <phrase>variety</phrase> of remote and offshore (in particular <phrase>Asian</phrase>) locations: a fully online offering of the Organisational Behaviour course is scheduled for delivery in <phrase>Vietnam</phrase> in 2002 as part of a <phrase>flagship</phrase> <phrase>Bachelor</phrase> of <phrase>Commerce</phrase> program. Introduction to <phrase>Management</phrase>, a compulsory course for <phrase>Engineering</phrase> students, is a lecture-only offering of the Organisational Behaviour course (in contrast, <phrase>Business</phrase> students also attend tutorials). It was decided to conduct the trial of online lectures with this group of eighty <phrase>Engineering</phrase> students, rather than the full group of <phrase>Business</phrase> students, as the smaller <phrase>student</phrase> numbers would be more manageable. As well as accommodating the imperative from policymakers of the Universityto ensure that the <phrase>Bachelor</phrase> 
<phrase>Physics</phrase> and <phrase>Playstation</phrase> Too: Learning <phrase>Physics</phrase> with Computer <phrase>Games</phrase> Computer <phrase>games</phrase> seem to captivate the imagination and attention of contemporary teenagers. If only the <phrase>energy</phrase>, <phrase>motivation</phrase>, fun and exhilaration they enjoy from playing <phrase>games</phrase> on their <phrase>PC</phrase>, or on <phrase>consoles</phrase> such as <phrase>PlayStation 2</phrase>, <phrase>GameCube</phrase>, <phrase>XBOX</phrase>, and <phrase>Dreamcast</phrase>, could be captured in learning <phrase>physics</phrase>! Andrew Stapleton, a doctoral <phrase>student</phrase> under Dr. Peter Taylor's supervision at <phrase>Curtin</phrase> University's <phrase>Science</phrase> and <phrase>Mathematics Education</phrase> Centre, believes it can and explores how in this <phrase>paper</phrase>. The <phrase>paper</phrase> discusses both the use of <phrase>games</phrase> and simulations in <phrase>physics</phrase> classrooms and their <phrase>design</phrase> as legitimate means for learning <phrase>physics</phrase>. Based on theoretical and methodological insights Andrew gained from his doctoral <phrase>research</phrase> into the <phrase>design</phrase> of <phrase>multimedia</phrase> for conceptual learning of <phrase>physics</phrase>, the <phrase>paper</phrase> considers the role of <phrase>design</phrase> in the <phrase>physics</phrase> <phrase>curriculum</phrase>. <phrase>Physics</phrase>, <phrase>education</phrase> and <phrase>design</phrase> are brought together in dialogue in an endeavour to provide <phrase>physics</phrase> educators with new perspective and opportunities for their students to learn <phrase>physics</phrase>. To illustrate the possibility of learning <phrase>physics</phrase> with computer <phrase>games</phrase>, the presentation will include examples from SR <phrase>Voyager</phrase>, a <phrase>multimedia</phrase> <phrase>prototype</phrase> Andrew developed for his <phrase>research</phrase>, which aims to promote conceptual learning of the <phrase>physics</phrase> of <phrase>special relativity</phrase>. SR <phrase>Voyager</phrase> is a <phrase>game</phrase> <phrase>prototype</phrase> designed to promote <phrase>conceptual understanding</phrase> of <phrase>relativistic</phrase> phenomena. It was designed with a <phrase>spirit</phrase> of co-participation between the researcher (Andrew) and a <phrase>university</phrase> <phrase>physics</phrase> course <phrase>lecturer</phrase>, resulting in pedagogical strategies (e.g., two reference frames simultaneously on-screen) and concepts (e.g., time dilation) that were deemed particularly important to the lecturer's program of instruction. In this way, the <phrase>lecturer</phrase> became a co-designer and maintained input into the project's emergent <phrase>design</phrase>. Thus, the <phrase>game</phrase> supplements existing content (i.e., lectures/tutorials) and <phrase>traditional approaches</phrase> to learning <phrase>special relativity</phrase> (i.e., <phrase>mathematical</phrase> formalisms), and provides students with innovative ways of experiencing and understanding <phrase>special relativity</phrase>. The <phrase>narrative</phrase> gameplay leads students on a voyage into deep space on spacestation SR <phrase>Voyager</phrase> as a consequence of a malfunctioning navicomputer. By taking the role of spacestation <phrase>captain</phrase>, the <phrase>student</phrase> can "manually" <phrase>override</phrase> the navicomputer by playing simulations and solving puzzles, eventually gaining access to the control centre and returning the <phrase>station</phrase> to <phrase>Earth orbit</phrase>. Students are encouraged to record events and experiences in the Captain's Log, which also provides a <phrase>narrative</phrase> commentary and
Deep content-based <phrase>music</phrase> recommendation Automatic <phrase>music</phrase> recommendation has become an increasingly relevant problem in recent years, since a lot of <phrase>music</phrase> is now sold and consumed digitally. Most <phrase>recommender systems</phrase> rely on <phrase>collaborative filtering</phrase>. However, this approach suffers from the cold start problem: it fails when no usage <phrase>data</phrase> is available, so it is not effective for recommending new and unpopular songs. In this <phrase>paper</phrase>, we propose to use a latent factor <phrase>model</phrase> for recommendation, and predict the latent factors from <phrase>music</phrase> audio when they cannot be obtained from usage <phrase>data</phrase>. We compare a traditional approach using a bag-of-words representation of the audio signals with <phrase>deep convolutional</phrase> <phrase>neural networks</phrase>, and evaluate the predictions quantitatively and qualitatively on the Million <phrase>Song</phrase> Dataset. We show that using predicted latent factors produces sensible recommendations, despite the fact that there is a large <phrase>semantic</phrase> gap between the characteristics of a <phrase>song</phrase> that affect user preference and the corresponding <phrase>audio signal</phrase>. We also show that <phrase>recent advances</phrase> in <phrase>deep learning</phrase> translate very well to the <phrase>music</phrase> recommendation setting, with deep con-volutional <phrase>neural networks</phrase> significantly outperforming the traditional approach.
<phrase>Problem-Based Learning</phrase> in <phrase>Mathematics</phrase> - A tool for Developing Students' Conceptual <phrase>Knowledge</phrase> <phrase>Mathematics</phrase> teachers must teach students not only to <phrase>solve problems</phrase> but also to learn about <phrase>mathematics</phrase> through <phrase>problem solving</phrase>. 1 While " many students may develop procedural fluency they often lack the deep <phrase>conceptual understanding</phrase> necessary to solve new problems or make connections between <phrase>mathematical</phrase> ideas. " 2 This presents a challenge for teachers: <phrase>problem-based learning</phrase> (<phrase>PBL</phrase>) provides opportunities for teachers to meet this challenge. <phrase>PBL</phrase> exists as a teaching method grounded in the ideals of <phrase>constructivism</phrase> and <phrase>student</phrase>-centred learning. When using <phrase>PBL</phrase>, teachers help students to focus on <phrase>solving problems</phrase> within a <phrase>real-life</phrase> context, encouraging them to consider the situation in which the problem exists when trying to find solutions. 3 The majority of <phrase>research</phrase> examining <phrase>PBL</phrase> focuses on its use in <phrase>medical schools</phrase>, with the key features being (a) the use of collaborative small-group work, (b) a <phrase>student</phrase>-centred approach, (c) the <phrase>teacher</phrase> as facilitator and (d) the use of <phrase>real-life</phrase> problems as the organizing focus. 4 In the <phrase>medical</phrase> <phrase>arena</phrase>, groups of students are given a set of realistic patient symptoms and expected to <phrase>research</phrase> possible diagnoses and courses of treatment; groups work independently, developing and answering their own questions. If, during this diagnostic phase, a group is unsuccessful in addressing key issues, the instructor notes this on their assessment but does not provide the <phrase>solution</phrase>. 4 In the classroom setting, it is this <phrase>aspect</phrase> of <phrase>PBL</phrase> which presents the most signifcant challenge, requiring teachers to shift from direct instruction to supporting students organize their own learning. 5 What kind of <phrase>mathematics</phrase> problems help students develop deep, <phrase>conceptual understanding</phrase>? <phrase>Research</phrase> Tells Us Many students lack a <phrase>deep understanding</phrase> of <phrase>mathematical</phrase> concepts. Classroom teachers find it difficult both to develop a <phrase>real-life</phrase> hook for students and to allow students to work through <phrase>problem solving</phrase> independently. <phrase>PBL</phrase> is a promising approach not only to build <phrase>mathematics</phrase> understanding but also to <phrase>test</phrase> students' conceptual <phrase>knowledge</phrase>. <phrase>PBL</phrase> requires teachers to present students with multifaceted, <phrase>real-life</phrase> problems and to <phrase>act</phrase> as facilitators supporting students in organizing their own learning.
Automatic forum analysis: a thorough method of assessing the importance of posts, discussion threads and of users' involvement Building a dependable hierarchy of users and posts by analyzing forums proves to be a complex and time consuming task, while its importance increases with the wider spread of <phrase>Computer Supported</phrase> <phrase>Collaborative Learning</phrase> (CSCL) environments. In this context, building a system that automatically assesses posts or utterances, predefined discussion threads and users seemed more than appropriate since no system addresses in detail this kind of task. In <phrase>order</phrase> to obtain a holistic perspective and a deep and thorough analysis, multiple factors were evaluated and a multistep <phrase>algorithm</phrase> was developed. Our approach uses a multi-layered <phrase>architecture</phrase> that best fits the task at hand. Manual adjustments of each weight were conducted in <phrase>order</phrase> to best fit the obtained <phrase>results</phrase> with those of <phrase>human</phrase> evaluators. Although improvements can be included in our approach, current <phrase>results</phrase> are optimistic by surpassing existing and conventional methods (e.g. number of posts, creation date of the account, votes given by other users, the moderator's appreciation).
Multiresolution <phrase>Deep Belief</phrase> Networks Motivated by the observation that coarse and fine resolutions of an image reveal different structures in the underlying visual phenomenon , we present a <phrase>model</phrase> based on the <phrase>Deep Belief</phrase> Network (DBN) which learns features from the multiscale representation of images. A <phrase>Laplacian</phrase> <phrase>Pyramid</phrase> is first constructed for each image. DBNs are then trained separately at each level of the <phrase>pyramid</phrase>. Finally, a top level RBM combines these DBNs into a <phrase>single</phrase> network we call the Mul-tiresolution <phrase>Deep Belief</phrase> Network (MrDBN). Experiments show that MrDBNs generalize better than standard DBNs on NORB classification and TIMIT <phrase>phone recognition</phrase>. In the domain of generative learning, we demonstrate the superiority of MrDBNs at <phrase>model</phrase>-ing face images.
Harnessing Folksonomies for Resource Classification 2011 Arkaitz Zubiaga Mendialdua This work is <phrase>licensed</phrase> under the <phrase>Creative Commons</phrase> Attribution-ShareAlike 3.0 License. To view a copy of this license, visit "A <phrase>free culture</phrase> has been our past, but it will only be our future if we change the path we are on right now." <phrase>Lawrence Lessig</phrase> "<phrase>Free software</phrase> is a <phrase>matter</phrase> of <phrase>liberty</phrase>, not price. To understand the concept, you should think of <phrase>free</phrase> as in <phrase>free speech</phrase>, not as in <phrase>free</phrase> <phrase>beer</phrase>." <phrase>Richard Stallman</phrase> "The only valid <phrase>censorship</phrase> of ideas is the right of people not to listen." Tommy Smothers Acknowledgments First and foremost I would like to earnestly thank my advisors, Vctor and Raquel, who have been helping me throughout this <phrase>research</phrase>. Their hard work guiding me has enabled me to acquire a deep insight into both <phrase>research</phrase> and the field of <phrase>social media</phrase> <phrase>mining</phrase>. Their guidance has been of vital importance for pursuing first the <phrase>Master</phrase> <phrase>Thesis</phrase>, and the <phrase>PhD</phrase> <phrase>Thesis</phrase> afterward. Since the moment I decided to work on the novel field of <phrase>social media</phrase>, which was also new to them, they responded with a very willing and excellent attitude to assist with this project. I would also like to thank my colleagues at the <phrase>University</phrase>, with whom I have discussed many <phrase>research</phrase> matters, that allowed me to grow as a researcher. Furthermore, all the leisure times we enjoyed together were of utmost importance to help me feel welcome and comfortable in the <phrase>department</phrase>. I would especially like to thank my <phrase>academic</phrase> brother Alberto, who has worked alongside me. With Alberto I have learned lots of new things and together worked on significant <phrase>research</phrase> that was presented at an international conference. I am also very grateful to Markus and <phrase>Christian</phrase>, from the <phrase>Graz University of Technology</phrase>, with whom I had the pleasure of cooperating. They kindly hosted me in <phrase>Graz</phrase>, <phrase>Austria</phrase>, during my <phrase>research</phrase> stay. The stay was helpful in many aspects, such as getting to know other researchers who work in the same field as I, and sharing our thoughts and <phrase>knowledge</phrase>. It was especially useful to produce a <phrase>sound</phrase> joint <phrase>research</phrase> work which was presented at a consolidated and prestigious international conference. I deeply appreciate and want to thank Jake for his kindness and effort in reviewing this dissertation. Jake provided a handful of tips and suggestions to improve the <phrase>English</phrase> in this document. I also want to thank all the 
The Fifth Workshop on the <phrase>Economics</phrase> of <phrase>Information Security</phrase> (WEIS 2006) Do we spend enough on <phrase>hunting</phrase> down bad guys on the <phrase>Internet</phrase>? Do we not spend enough? Or do we spend too much? One of the most exciting and rapidly-growing fields at the boundary between <phrase>technology</phrase> and the <phrase>social sciences</phrase> is the <phrase>economics</phrase> of <phrase>information security</phrase>. Many <phrase>security</phrase> and <phrase>privacy</phrase> failures are not purely technical: for example, the person best placed to protect a system may be poorly motivated if the costs of system failure fall on others. Many pressing problems, such as spam, are unlikely to be solved by purely technical means, as they have economic and policy aspects too. Building dependable systems also raises questions such as open versus closed systems, the pricing of vulnerabilities and the <phrase>frequency</phrase> of patching. Theconomics of <phrase>bugs</phrase>' are of growing importance to both vendors and users. Now that both <phrase>crime</phrase> and conflict are becoming virtualised, many of the <phrase>lessons learned</phrase> by <phrase>information security</phrase> <phrase>economists</phrase> may travel to other <phrase>security</phrase> applications, such as <phrase>law</phrase> enforcement. <phrase>Law</phrase> enforcement concerns such as traffic analysis impact on <phrase>information security</phrase> costs and practices in turn. For these and other reasons, the <phrase>confluence</phrase> between <phrase>information security</phrase> and <phrase>economics</phrase> is of growing importance. <phrase>Information security</phrase> mechanisms are increasingly used not just to protect against malicious attacks, but also to protect monopolies, differentiate <phrase>products</phrase> and segment markets. There are deep questions about the <phrase>economics</phrase> and <phrase>politics</phrase> of <phrase>DRM</phrase>, of locking printers to the maker's cartridges, and of practices such as <phrase>region</phrase> coding. Original <phrase>research</phrase> papers are sought for the Fifth Workshop on the <phrase>Economics</phrase> of <phrase>Information Security</phrase>. Topics of interest include the dependability of <phrase>open source</phrase> and <phrase>free software</phrase>, the interaction of networks with <phrase>crime</phrase> and conflict, the <phrase>economics</phrase> of <phrase>digital rights management</phrase> and <phrase>trusted computing</phrase>, liability and <phrase>insurance</phrase>, reputation, <phrase>privacy</phrase>, <phrase>risk perception</phrase>, the <phrase>economics</phrase> of trust, the return on <phrase>security</phrase> <phrase>investment</phrase>, and economic perspectives on spam.
Teaching <phrase>Functional Programming</phrase> to <phrase>Professional</phrase> .net Developers <phrase>Functional programming</phrase> is often taught at <phrase>universities</phrase> to first-year or second-year students and most of the teaching materials have been written for this audience. With the recent rise of <phrase>functional programming</phrase> in the <phrase>industry</phrase>, it becomes important to teach functional concepts to <phrase>professional</phrase> developers with deep <phrase>knowledge</phrase> of other paradigms, most importantly <phrase>object-oriented</phrase>. We present our experience with teaching <phrase>functional programming</phrase> and F# to experienced .NET developers through a <phrase>book</phrase> <phrase>Real-World</phrase> <phrase>Functional Programming</phrase> and commercially offered F# trainings. The most important novelty in our approach is the use of C# for relating functional F# with <phrase>object-oriented</phrase> C# and for introducing some of the functional concepts. By presenting principles such as immutability, <phrase>higher-order</phrase> functions and functional types from a different perspective, we are able to build on existing <phrase>knowledge</phrase> of <phrase>professional</phrase> developers. This contrasts with a common approach that asks students to forget everything they know about <phrase>programming</phrase> and think completely differently. We believe that our observations are relevant for <phrase>train</phrase>-ings designed for practitioners, but perhaps also for students who explore functional relatively late in the <phrase>curriculum</phrase>. 1 Introduction Until recently, <phrase>object-oriented</phrase> languages such as <phrase>Java</phrase>, C# and <phrase>C++</phrase> were dominating the <phrase>industry</phrase>. Despite this fact, functional languages have been successfully taught at <phrase>universities</phrase> and interesting approaches to teaching functional languages have been developed. Professionals approached <phrase>functional programming</phrase> with the intention to learn about different style of thinking, with the aim of becoming better developers by broadening their horizons. With the recent rise of mixed functional languages such as F# and Scala and the inclusion of functional features in mainstream languages, the audience interested in <phrase>functional programming</phrase> changes. Developers more often want to add pragmatic <phrase>functional programming</phrase> to their toolbox, learn how it can be applied to their <phrase>daily</phrase> tasks and understand how it relates to the patterns they regularly use. We believe that the most efficient way to teach <phrase>functional programming</phrase> to this new audience is to relate <phrase>functional programming</phrase> to what they already know and build on this existing <phrase>knowledge</phrase>. In particular, we discuss the following approaches:
On the Effects of Spam Filtering and <phrase>Incremental Learning</phrase> for Web-Supervised Visual Concept Classification <phrase>Deep neural networks</phrase> have been successfully applied to the task of visual concept classification. However, they require a large number of <phrase>training examples</phrase> for learning. Although <phrase>pre-trained</phrase> <phrase>deep neural networks</phrase> are available for some domains, they usually have to be <phrase>fine-tuned</phrase> for an envisaged <phrase>target</phrase> domain. Recently, some approaches have been suggested that are aimed at incrementally (or even endlessly) learning <phrase>visual concepts</phrase> based on Web <phrase>data</phrase>. Since tags of Web images are often noisy, normally some filtering mechanisms are employed in <phrase>order</phrase> to remove ``spam'' images that are not appropriate for training. In this <phrase>paper</phrase>, we investigate several aspects of a web-supervised system that has to be adapted to another <phrase>target</phrase> domain: 1.) the effect of <phrase>incremental learning</phrase>, 2.) the effect of spam filtering, and 3.) the behavior of particular concept classes with respect to 1.) and 2.). The <phrase>experimental</phrase> <phrase>results</phrase> provide some insights under which conditions <phrase>incremental learning</phrase> and spam filtering are useful.
Heterogeneous chip <phrase>multiprocessor</phrase> architectures for <phrase>big data</phrase> applications Emerging <phrase>big data</phrase> analytics applications require a significant amount of server computational power. The costs of building and running a <phrase>computing</phrase> server to process <phrase>big data</phrase> and the capacity to which we can scale it are driven in large part by those computational resources. However, <phrase>big data</phrase> applications share many characteristics that are fundamentally different from traditional desktop, parallel, and scale-out applications. <phrase>Big data</phrase> analytics applications rely heavily on specific deep <phrase>machine learning</phrase> and <phrase>data mining</phrase> <phrase>algorithms</phrase>, and are running a complex and deep <phrase>software</phrase> stack with various components (e.g. <phrase>Hadoop</phrase>, Spark, MPI, Hbase, <phrase>Impala</phrase>, <phrase>MySQL</phrase>, Hive, <phrase>Shark</phrase>, <phrase>Apache</phrase>, and MangoDB) that are bound together with a runtime <phrase>software</phrase> system and interact significantly with I/O and <phrase>OS</phrase>, exhibiting <phrase>high</phrase> computational intensity, <phrase>memory</phrase> intensity, I/O intensity and control intensity. Current server designs, based on <phrase>commodity</phrase> homogeneous processors, will not be the most efficient in terms of performance/<phrase>watt</phrase> for this emerging class of applications. In other domains, heterogeneous architectures have emerged as a promising <phrase>solution</phrase> to enhance <phrase>energy</phrase>-efficiency by allowing each application to run on a core that matches resource needs more closely than a one-size-fits-all core. A heterogeneous <phrase>architecture</phrase> integrates cores with various micro-architectures and <phrase>accelerators</phrase> to provide more opportunity for efficient workload mapping. In this work, through methodical investigation of power and performance measurements, and <phrase>comprehensive</phrase> system level characterization, we demonstrate that a heterogeneous <phrase>architecture</phrase> combining <phrase>high</phrase> performance big and <phrase>low power</phrase> little cores is required for efficient <phrase>big data</phrase> analytics applications processing, and in particular in the presence of <phrase>accelerators</phrase> and near real-time performance constraints.
Extending the <phrase>Undergraduate</phrase> <phrase>Computer Science</phrase> <phrase>Curriculum</phrase> to Include <phrase>Information Retrieval</phrase> and <phrase>Data Mining</phrase> We describe our progress extending the <phrase>undergraduate</phrase> <phrase>Computer Science</phrase> (CS) <phrase>curriculum</phrase> to include a <phrase>deep understanding</phrase> of <phrase>information retrieval</phrase> (IR) and <phrase>data mining</phrase> (DM). Instead of simply understanding how to build applications using tools involving IR and DM, students build these tools and learn the relevant <phrase>algorithms</phrase> implemented in these tools. Some novel approaches exist in our work. We include a hands-on lab setting where students use the tools they have built to perform experiments that could ultimately extend the field. Hence, undergraduates have firsthand <phrase>knowledge</phrase> of performing <phrase>research</phrase> in <phrase>Computer Science</phrase> using a <phrase>scientific method</phrase>. Secondly, we have a rigorous set of evaluation criteria developed by our <phrase>Psychology</phrase> <phrase>department</phrase> that will evaluate how well <phrase>students learn</phrase> using our novel approaches. Ultimately, we believe these two courses warrant consideration into standards developed for the <phrase>undergraduate</phrase> CS <phrase>curriculum</phrase>.
Deep Speech: Scaling up <phrase>end-to-end</phrase> <phrase>speech recognition</phrase> We present a <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>speech recognition</phrase> system developed using <phrase>end-to-end</phrase> <phrase>deep learning</phrase>. Our <phrase>architecture</phrase> is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to <phrase>model</phrase> background noise, <phrase>reverberation</phrase>, or <phrase>speaker</phrase> variation, but instead directly learns a <phrase>function</phrase> that is robust to such effects. We do not need a <phrase>phoneme</phrase> <phrase>dictionary</phrase>, nor even the concept of a " <phrase>phoneme</phrase>. " Key to our approach is a well-optimized RNN training system that uses multiple <phrase>GPUs</phrase>, as well as a set of novel <phrase>data</phrase> synthesis techniques that allow us to efficiently obtain a large amount of varied <phrase>data</phrase> for training. Our system, called Deep Speech, outperforms previously <phrase>published results</phrase> on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full <phrase>test</phrase> set. Deep Speech also handles challenging noisy environments better than widely used, <phrase>state</phrase>-of-the-<phrase>art</phrase> commercial speech systems.
Document <phrase>engineering</phrase>: a preferred partner discipline in <phrase>knowledge management</phrase> After 20 years of investigation and application of <phrase>Knowledge Management</phrase> (<phrase>KM</phrase>) there are still various views and expectations on it resulting from its trans-disciplinary character. It is a kind of meta-discipline with a lot of partner disciplines, e.g. personnel development, <phrase>organization</phrase>, process and <phrase>quality management</phrase>, <phrase>information management</phrase>, document <phrase>engineering</phrase> and <phrase>communication</phrase>. The reason is the complex character of <phrase>knowledge</phrase> itself, which is defined in <phrase>KM</phrase> as the capability for effective <phrase>action</phrase>. A <phrase>major</phrase> <phrase>dimension</phrase> of this capability is naturally the content <phrase>dimension</phrase>, i.e. which <phrase>knowledge</phrase> <phrase>area</phrase> or object-activity-domain is it about, e.g. "document <phrase>engineering</phrase>". In any <phrase>knowledge</phrase> <phrase>area</phrase> the <phrase>knowledge</phrase> has three types of <phrase>carriers</phrase>: individuals with their experiences, <phrase>education</phrase> and inherent capabilities, groups like teams and communities with their <phrase>compound</phrase> capabilities based on joint understanding and networked complementary capabilities and finally <phrase>information</phrase>, carrying more or less codified and documented <phrase>knowledge</phrase>. Across all three <phrase>knowledge</phrase> <phrase>carriers</phrase> three questions or dimensions of <phrase>knowledge</phrase> quality are interesting in any <phrase>knowledge</phrase> <phrase>area</phrase>, which is important, e.g. for a <phrase>business</phrase>: "How deep or profound is it, e.g. the level of expertise of a <phrase>subject matter</phrase> expert or a best practice description? " How much is it distributed and inter-connected, e.g. which experts, groups and documents are involved and how?" How is it codified and documented, e.g. the quality of defining, structuring and documenting the content? This is the <phrase>starting point</phrase> for <phrase>KM</phrase>: it provides adequate processes or instruments to improve or adjust the <phrase>knowledge</phrase> quality to the needs, e.g. of a <phrase>business</phrase>. But there are already the various partner disciplines of <phrase>KM</phrase> active to support, e.g. learning and training, inter-connection by collaboration, <phrase>information</phrase> formalizing and distribution - why do we still need <phrase>KM</phrase>? The partner disciplines may have profound capabilities in their fields, but they are driving a kind of one-dimensional <phrase>KM</phrase>. The full power of <phrase>KM</phrase> is to combine their solutions to more powerful <phrase>multi-dimensional</phrase> approaches.
Adaptive Treatment of <phrase>Epilepsy</phrase> via Batch-mode <phrase>Reinforcement Learning</phrase> This <phrase>paper</phrase> highlights the crucial role that modern <phrase>machine learning</phrase> techniques can <phrase>play</phrase> in the optimization of treatment strategies for patients with chronic disorders. In particular, we focus on the task of optimizing a <phrase>deep-brain stimulation</phrase> strategy for the treatment of <phrase>epilepsy</phrase>. The challenge is to choose which stimulation <phrase>action</phrase> to apply, as a <phrase>function</phrase> of the observed <phrase>EEG</phrase> signal, so as to minimize the <phrase>frequency</phrase> and duration of seizures. We apply recent techniques from the <phrase>reinforcement learning</phrase> literaturenamely fitted Q-iteration and extremely <phrase>randomized</phrase> treesto learn an optimal stimulation policy using <phrase>labeled training</phrase> <phrase>data</phrase> from <phrase>animal</phrase> <phrase>brain</phrase> tissues. Our <phrase>results</phrase> show that these methods are an effective means of reducing the incidence of seizures, while also minimizing the amount of stimulation applied. If these <phrase>results</phrase> carry over to the <phrase>human</phrase> <phrase>model</phrase> of <phrase>epilepsy</phrase>, the impact for patients will be substantial.
DeepMPC: Learning Deep Latent Features for <phrase>Model</phrase> Predictive Control Designing controllers for tasks with complex non-linear dynamics is extremely challenging, time-consuming, and in many cases, infeasible. This difficulty is exacerbated in tasks such as robotic <phrase>food</phrase>-cutting, in which dynamics might vary both with environmental properties, such as material and tool class, and with time while acting. In this work, we present DeepMPC, an online real-time <phrase>model</phrase>-predictive control approach designed to handle such difficult tasks. Rather than hand-<phrase>design</phrase> a dynamics <phrase>model</phrase> for the task, our approach uses a novel <phrase>deep architecture</phrase> and <phrase>learning algorithm</phrase>, learning controllers for complex tasks directly from <phrase>data</phrase>. We validate our method in experiments on a <phrase>large-scale</phrase> dataset of 1488 material cuts for 20 diverse classes, and in 450 <phrase>real-world</phrase> robotic experiments, demonstrating significant improvement over several other approaches.
Learning through inquiry: <phrase>student</phrase> difficulties with online course-based Material This study investigates the <phrase>case-based</phrase> learning experience of 133 <phrase>undergraduate</phrase> <phrase>veterinarian</phrase> <phrase>science</phrase> students. Using qualitative methodologies from <phrase>relational</phrase> <phrase>Student</phrase> Learning <phrase>Research</phrase> , variation in the quality of the learning experience was identified, ranging from coherent , deep, quality experiences of the cases, to experiences that separated significant aspects, such as the online <phrase>case histories</phrase>, <phrase>laboratory</phrase> <phrase>test</phrase> <phrase>results</phrase>, and annotated images emphasizing symptoms, from the meaning of the experience. A key outcome of this study was that a significant percentage of the students surveyed adopted a poor approach to learning with online resources in a blended experience even when their overall learning experience was related to cohesive conceptions of <phrase>veterinary</phrase> <phrase>science</phrase>, and that the difference was even more marked for less successful students. The outcomes from the study suggest that many students are unsure of how to approach the use of online resources in ways that are likely to maximise benefits for learning in blended experiences, and that the benefits from <phrase>case-based</phrase> learning such as authenticity and <phrase>active learning</phrase> can be threatened if issues closely associated with qualitative variation arising from incoherence in the experience are not addressed.
Merry <phrase>Christmas</phrase> and Happy New Year Dear <phrase>Friends</phrase> and <phrase>Family</phrase>: I'm again writing this letter on New Year's Day, 2009, celebrating at home, with some fluffy <phrase>snow</phrase> and recovering from a wonderful <phrase>Christmas</phrase>. We are worried for our cousin Al <phrase>LaTour</phrase>, who had a terrible fall from a ladder and is in the <phrase>hospital</phrase> in <phrase>Minneapolis</phrase> recovering from his injuries. We send Al, Kathy and their kids, and dear Aunt Joan and cousin Michael, our best wishes for a happy and speedy recovery. We know that Al and Kathy's deep friendships and <phrase>family</phrase> support will help him get better. I'm picking up Joseph from <phrase>Toronto</phrase> <phrase>Airport</phrase>, where he has spent a few days with Uncle Dave and Aunt Maria in <phrase>Edmonton</phrase>. What a treat for Joseph to spend some adult time with <phrase>family</phrase>, and learn about the authentic <phrase>Italian</phrase> wing of the <phrase>family</phrase>! We had a quiet and fun <phrase>Christmas</phrase>, which was needed, as both kids had a difficult fall with <phrase>health</phrase> issues for each of them, hopefully making progress for a good new year. So, Joseph is now a sophomore in <phrase>high school</phrase>, studying <phrase>chemistry</phrase>, <phrase>Shakespeare</phrase>, and enjoying <phrase>Boy Scouts</phrase> and Tae Kwon Do. Claire is 18! And she will graduate from <phrase>School</phrase> 84 this year, as the <phrase>school</phrase> is reconstructed in a <phrase>long</phrase> awaited renovation. Claire and Joe were featured in a great article in the <phrase>Buffalo News</phrase>, showing her driving her new power chair down the hall. Over the year, we enjoyed a good deal of great travels and this year's letter is dedicated to some wonderful trips. Claire and Joe spent <phrase>Easter</phrase> in <phrase>Detroit</phrase>, with Grandma Anne, Aunt Sarah, Uncle Chris, Uncle Ed and Aunt Robin. While in <phrase>Detroit</phrase>, Carol repainted and decorated Claire's room in a <phrase>nice</phrase> <phrase>pink</phrase> motif and Claire has been working in her room on writing a <phrase>long</phrase> story. She works on writing this story with her aides, Michelle and Carla. Melissa continues as Claire's longest serving <phrase>aide</phrase>, and all three seem a great part of the <phrase>family</phrase>. Joe made a trip to <phrase>Hamburg</phrase>, <phrase>Germany</phrase> for some consulting in April, a beautiful <phrase>city</phrase> in springtime. But, our biggest trip was a <phrase>family</phrase> vacation to <phrase>Paris</phrase> and <phrase>Normandy</phrase>, <phrase>France</phrase>. We flew with Aunt Sandy to <phrase>Paris</phrase>, and toured all the <phrase>major</phrase> sites, Joseph and Joe climbing both the <phrase>Eiffel Tower</phrase> and <phrase>Notre Dame</phrase>. We visited the <phrase>louvre</phrase>, where Claire was treated to a close up view of the Mona 
<phrase>Education</phrase> and <phrase>Social Justice</phrase> Project <phrase>Canada</phrase> <phrase>Guatemala</phrase> <phrase>Senegal</phrase> created within the Office of the <phrase>President</phrase> in 2006, is dedicated to the interdisciplin-<phrase>ary</phrase> study of <phrase>religion</phrase>, <phrase>ethics</phrase>, and <phrase>public</phrase> <phrase>life</phrase>. Through <phrase>research</phrase>, teaching, and service, the <phrase>center</phrase> explores global challenges of <phrase>democracy</phrase> and <phrase>human rights</phrase>; economic and social development; international <phrase>diplomacy</phrase>; and interreligious understanding. Two premises guide the center's work: that a deep examination of <phrase>faith</phrase> and values is critical to address these challenges, and that the open engagement of <phrase>religious</phrase> and <phrase>cultural</phrase> traditions with one another can promote <phrase>peace</phrase>. seeks to advance <phrase>justice</phrase> and the common good through promoting and integrating <phrase>community</phrase>-based <phrase>research</phrase>, teaching, and service by collaborating with diverse partners and communities. CSJ works in three key areas: <phrase>community</phrase> and <phrase>public</phrase> service, <phrase>curriculum</phrase> and <phrase>pedagogy</phrase>, and <phrase>research</phrase>. Through such critical and engaged work, <phrase>George-town</phrase> builds on its tradition of <phrase>academic</phrase> excellence and contributes in singular ways to the <phrase>Jesuit</phrase> ideal of <phrase>justice</phrase> <phrase>education</phrase> and <phrase>action</phrase> " for the glory of <phrase>God</phrase> and the well-being of humankind. " This <phrase>report</phrase> reflects on the sixth year of the <phrase>Education</phrase> and <phrase>Social Justice</phrase> Project, which provided four <phrase>Georgetown University</phrase> students with fellowships to travel to <phrase>South Korea</phrase>, <phrase>Guatemala</phrase>, <phrase>Senegal</phrase>, and <phrase>Canada</phrase> to conduct in-depth examinations of innovative educational initiatives, with a focus on the work of <phrase>Jesuit</phrase> institutions. We are learning more every day about the deep connections between global challenges of <phrase>poverty</phrase> and <phrase>education</phrase>. Only through better access to <phrase>education</phrase> will the world's poor be able to seize opportunities in an increasingly global <phrase>economy</phrase>. While policy analysts have documented the widespread failure of governments to meet this imperative, we still know relatively little about successful local efforts <phrase>led</phrase> by <phrase>religious</phrase> communities to advance economic and social development through <phrase>education</phrase>. In <phrase>order</phrase> to engage <phrase>Georgetown</phrase> undergraduates and build <phrase>knowledge</phrase> in this critical <phrase>area</phrase>, Under faculty guidance, <phrase>Education</phrase> and <phrase>Social Justice</phrase> fellows gather <phrase>information</phrase> through interviews, analyze best practices, and share their reports and conclusions with a wider global audience. The project is made possible through the generous support of Rodney <phrase>Jacob</phrase> (MSB'86, <phrase>JD</phrase>'89) and other members of the <phrase>Georgetown</phrase> <phrase>community</phrase>. During its sixth year, the project awarded fellowships to four students who spent three weeks with institutions engaged in efforts to promote <phrase>social justice</phrase> through <phrase>education</phrase>. Caitlin Snell worked in <phrase>Espanola</phrase>, <phrase>Ontario</phrase>, <phrase>Canada</phrase>, with the Anishinabe Spiritual Centre to <phrase>research</phrase> how <phrase>cultural</phrase> <phrase>education</phrase> programs are being used by <phrase>Canadian</phrase> <phrase>Indian</phrase> and <phrase>Catholic</phrase> institutions to recover <phrase>lost</phrase> <phrase>culture</phrase> and foster <phrase>student</phrase> 
Understanding Childrens <phrase>Programming</phrase> as Poor <phrase>Learning Environments</phrase> Poor <phrase>learning environments</phrase> (PLEs) encourage personal commitment and <phrase>deep understanding</phrase>. In this <phrase>paper</phrase>, I argue that childrens <phrase>programming</phrase> should be understood from the viewpoint of PLEs.
<phrase>Engineering</phrase> <phrase>Education</phrase> <phrase>Research</phrase> <phrase>Aids</phrase> Instruction 1175 EDUCATIONFORUM D iscipline-based <phrase>education</phrase> <phrase>research</phrase> seeks to marry deep <phrase>knowledge</phrase> of the discipline with similarly deep <phrase>knowledge</phrase> of learning and <phrase>pedagogy</phrase> (1, 2) and may encourage <phrase>college</phrase> and <phrase>university</phrase> <phrase>faculty members</phrase> to bring more rigor to classroom instruction. For example, the <phrase>physics</phrase>-based <phrase>education</phrase> <phrase>research</phrase> <phrase>community</phrase> has used tools such as the Force Concept Inventory (3) to determine that students taught with interactive teaching techniques developed from discipline-based <phrase>education</phrase> <phrase>research</phrase> [such as Peer Instruction (4, 5)] better understand the concepts of <phrase>Newtonian physics</phrase> than do students taught in a lecture-based format. Within the <phrase>engineering</phrase> <phrase>community</phrase>, the ultimate aims of such <phrase>research</phrase> include the creation of <phrase>education</phrase> programs that attract more, and more diverse , students to the study of <phrase>engineering</phrase>; retain more of the students who are enrolled; deepen students' understanding of <phrase>engineering</phrase> concepts; broaden students' appreciation of engineering's role in meeting the needs of a global <phrase>society</phrase>; and better prepare students for further study or <phrase>professional</phrase> practice. In pursuing these aims, <phrase>research</phrase> in <phrase>engineering</phrase> <phrase>education</phrase> looks beyond questions solely devoted to teaching, learning , and assessment; it also examines issues associated with faculty rewards (6) and the organizational dynamics of <phrase>engineering</phrase> departments (7, 8). Although not all <phrase>engineering</phrase> faculty will engage in such <phrase>research</phrase>, we contend that all should learn and benefit from its findings. Our commitment to <phrase>engineering</phrase>-based <phrase>education</phrase> <phrase>research</phrase> does not devalue the work of researchers in colleges of <phrase>education</phrase> or that of <phrase>cognitive</phrase> or social scientists. Rather, we emphasize the maturation of the <phrase>engineering</phrase> <phrase>education</phrase> <phrase>research</phrase> <phrase>community</phrase> and the increased value attached to the emerging field by the <phrase>academic</phrase> <phrase>engineering</phrase> <phrase>community</phrase>. In the 1990s, centers for <phrase>research</phrase> on <phrase>engineering</phrase> <phrase>education</phrase> opened on several campuses , with foci ranging from foundational <phrase>research</phrase> to innovative approaches to <phrase>curriculum</phrase> , learning, teaching, and assessment. In 2001, the <phrase>National Science Foundation</phrase> initiated support for Centers for Learning and Teaching focused on <phrase>engineering</phrase> and <phrase>technology</phrase> <phrase>education</phrase> (9, 10). In 2002, the <phrase>National Academy of Engineering</phrase> shifted from solely providing advice to actively enhancing <phrase>engineering</phrase> <phrase>education</phrase> by fostering the growth of a <phrase>community</phrase> of engineers <phrase>conducting</phrase> <phrase>education</phrase> <phrase>research</phrase> and translating <phrase>research</phrase> <phrase>results</phrase> into more effective and efficient educational practices (11). More recently, departments of <phrase>engineering</phrase> <phrase>education</phrase> were opened at <phrase>Purdue</phrase> (2004), <phrase>Virginia Polytechnic Institute and State University</phrase> (2004), and <phrase>Utah State</phrase> (2005) and announced at <phrase>Clemson</phrase> (2007). The emergence of these departments marks the transition from isolated individual researchers to <phrase>academic</phrase> communities 
A highly scalable <phrase>Restricted Boltzmann Machine</phrase> <phrase>FPGA</phrase> implementation <phrase>Restricted Boltzmann Machines</phrase> (RBMs) the <phrase>building block</phrase> for newly popular <phrase>Deep Belief</phrase> Networks (DBNs) are a promising new tool for <phrase>machine learning</phrase> practitioners. However, <phrase>future research</phrase> in applications of DBNs is hampered by the considerable computation that training requires. In this <phrase>paper</phrase>, we describe a novel <phrase>architecture</phrase> and <phrase>FPGA</phrase> implementation that accelerates the training of <phrase>general</phrase> RBMs in a scalable manner, with the goal of producing a system that <phrase>machine learning</phrase> researchers can use to investigate ever-larger networks. Our <phrase>design</phrase> uses a highly efficient, fully-pipelined <phrase>architecture</phrase> based on 16-<phrase>bit</phrase> <phrase>arithmetic</phrase> for performing RBM training on an <phrase>FPGA</phrase>. We show that only 16-<phrase>bit</phrase> <phrase>arithmetic</phrase> precision is necessary, and we consequently use embedded hardware multiply-and-add (MADD) units. We present performance <phrase>results</phrase> to show that a speedup of 25-30X can be achieved over an optimized <phrase>software</phrase> implementation on a <phrase>high</phrase>-end <phrase>CPU</phrase>.
Open <phrase>Hypermedia</phrase> <phrase>Management</phrase> for <phrase>E</phrase>-learning in the <phrase>Humanities</phrase> <phrase>Knowledge</phrase>-based <phrase>hypermedia</phrase> systems are used in scholarly <phrase>communication</phrase> since more than 2000 years, e.g. the <phrase>Babylonian Talmud</phrase>. Many different computer-based <phrase>hypermedia</phrase> systems have been successfully created for the <phrase>humanities</phrase>, too. But, cross-<phrase>university</phrase> or even <phrase>university</phrase>-wide use of these systems for <phrase>e</phrase>-learning purposes in the <phrase>humanities</phrase> is seldom if ever. In this <phrase>paper</phrase>, we argue, that only a deep analysis of <phrase>hypermedia</phrase> usage in scholarly <phrase>communication</phrase> and teaching can disclose non-trivial technical and <phrase>community</phrase>-aware requirements for open <phrase>e</phrase>-learning systems in the <phrase>humanities</phrase>. With these requirements we analyze some of the existing solutions and provide a <phrase>case study</phrase> for a cross-<phrase>university</phrase> <phrase>hypermedia</phrase> <phrase>e</phrase>-learning system called <phrase>MECCA</phrase>.
Spatial <phrase>Language</phrase> Processing for Assistive <phrase>Robots</phrase> with "Deep" Chunking and <phrase>Semantic</phrase> Grammars This <phrase>paper</phrase> presents a <phrase>semantic</phrase> spatial <phrase>language</phrase> <phrase>grammar</phrase> and a novel chunking method that allows nested structures to be encoded as a <phrase>single</phrase> <phrase>label</phrase>. The proposed <phrase>semantic</phrase> <phrase>grammar</phrase>, when used with a <phrase>cognitive architecture</phrase> described elsewhere, makes it possible for a <phrase>mobile robot</phrase> to follow complex, <phrase>human</phrase>-generated spatial descriptions for a fetch task. The <phrase>semantic</phrase> <phrase>grammar</phrase> is based on an interdisciplinary analysis of a corpus of <phrase>human</phrase> generated indoor spatial <phrase>language</phrase>. The "deep" chunking method facilitates encoding deep grammatical structures into a <phrase>single</phrase>-level <phrase>label</phrase>. The <phrase>proposed method</phrase> has been successfully used by an autonomous agent in a <phrase>virtual environment</phrase> as well as by a physical <phrase>mobile robot</phrase>. The deep chunking approach allows fast, feature-based <phrase>machine learning</phrase> methods usually used for shallow chunking to be used for deep nested <phrase>parsing</phrase> which better supports real-time interaction with a <phrase>robot</phrase>. Preliminary accuracy <phrase>results</phrase> are presented along with possible improvements and additional applications.
What do the <phrase>basal ganglia</phrase> do? A modeling perspective <phrase>Basal ganglia</phrase> (BG) constitute a network of seven deep <phrase>brain</phrase> nuclei involved in a <phrase>variety</phrase> of crucial <phrase>brain</phrase> functions including: <phrase>action</phrase> selection, <phrase>action</phrase> gating, reward based learning, motor preparation, timing, etc. In spite of the immense amount of <phrase>data</phrase> available today, researchers continue to wonder how a <phrase>single</phrase> deep <phrase>brain</phrase> circuit performs such a bewildering <phrase>range</phrase> of functions. Computational models of BG have focused on individual functions and fail to give an integrative picture of BG <phrase>function</phrase>. A <phrase>major</phrase> breakthrough in our understanding of BG <phrase>function</phrase> is perhaps the insight that activities of mesencephalic <phrase>dopaminergic</phrase> cells represent some form of 'reward' to the <phrase>organism</phrase>. This insight enabled application of tools from '<phrase>reinforcement learning</phrase>,' a branch of <phrase>machine learning</phrase>, in the study of BG <phrase>function</phrase>. Nevertheless, in spite of these bright spots, we are far from the goal of arriving at a <phrase>comprehensive</phrase> understanding of these 'mysterious nuclei.' A <phrase>comprehensive</phrase> <phrase>knowledge</phrase> of BG <phrase>function</phrase> has the potential to radically alter treatment and <phrase>management</phrase> of a <phrase>variety</phrase> of BG-related <phrase>neurological disorders</phrase> (<phrase>Parkinson's disease</phrase>, Huntington's <phrase>chorea</phrase>, etc.) and neuropsychiatric disorders (<phrase>schizophrenia</phrase>, obsessive compulsive disorder, etc.) also. In this article, we review the existing modeling <phrase>literature</phrase> on BG and hypothesize an integrative picture of the <phrase>function</phrase> of these nuclei.
A Readability Checker with <phrase>Supervised Learning</phrase> Using <phrase>Deep Syntactic</phrase> and <phrase>Semantic</phrase> Indicators Checking for readability or simplicity of texts is important for many institutional and individual users. Formulas for approximately measuring text readability have a <phrase>long</phrase> tradition. Usually, they exploit surface-oriented indicators like sentence length, word length, word <phrase>frequency</phrase>, etc. However, in many cases, this <phrase>information</phrase> is not adequate to realistically approximate the <phrase>cognitive</phrase> difficulties a person can have to understand a text. Therefore we use <phrase>deep syntactic</phrase> and <phrase>semantic</phrase> indicators in addition. The <phrase>syntactic</phrase> <phrase>information</phrase> is represented by a dependency <phrase>tree</phrase>, the <phrase>semantic</phrase> <phrase>information</phrase> by a <phrase>semantic network</phrase>. Both representations are automatically generated by a deep syntactico-<phrase>semantic</phrase> analysis. A global readability score is determined by applying a nearest neighbor <phrase>algorithm</phrase> on 3,000 ratings of 300 <phrase>test</phrase> persons. The evaluation showed, that the <phrase>deep syntactic</phrase> and <phrase>semantic</phrase> indicators <phrase>lead</phrase> to quite comparable <phrase>results</phrase> to most surface-based indicators. Finally, a <phrase>graphical user interface</phrase> has been developed which highlights difficult-to-read text passages, depending on the individual indicator values, and displays a global readability score.
An Image Dataset of Text Patches in Everyday Scenes This <phrase>paper</phrase> describes a dataset containing small images of text from everyday scenes. The purpose of the dataset is to support the development of new automated systems that can detect and analyze text. Although much <phrase>research</phrase> has been devoted to text detection and recognition in scanned documents, relatively little attention has been given to text detection in other types of images, such as photographs that are posted on <phrase>social-media</phrase> sites. This new dataset, known as COCO-Text-Patch, contains approximately 354,000 small images that are each labeled as " text " or " non-text ". This dataset particularly addresses the problem of text verification, which is an essential stage in the <phrase>end-to-end</phrase> text detection and recognition pipeline. In <phrase>order</phrase> to evaluate the utility of this dataset, it has been used to <phrase>train</phrase> two deep <phrase>convolution</phrase> <phrase>neural networks</phrase> to distinguish text from non-text. One network is inspired by the GoogLeNet <phrase>architecture</phrase>, and the second one is based on CaffeNet. Accuracy levels of 90.2% and 90.9% were obtained using the two networks, respectively. All of the images, <phrase>source code</phrase>, and <phrase>deep-learning</phrase> trained models described in this <phrase>paper</phrase> will be publicly available 5 .
<phrase>Deep learning</phrase> of visual control policies This <phrase>paper</phrase> discusses the effectiveness of deep auto-encoding <phrase>neural nets</phrase> in visual <phrase>reinforcement learning</phrase> (<phrase>RL</phrase>) tasks. We describe a new <phrase>algorithm</phrase> and give <phrase>results</phrase> on succesfully learning policies directly on synthesized and real images without a predefined <phrase>image processing</phrase>. Furthermore , we present a thorough evaluation of the learned feature spaces.
<phrase>Ai</phrase> <phrase>Grand Challenges</phrase> for <phrase>Education</phrase> <phrase>Ai</phrase> and <phrase>Education</phrase> <phrase>Artificial intelligence</phrase> impacts growth and <phrase>productivity</phrase> in many industries (e.g., transportation, <phrase>communication</phrase>, <phrase>commerce</phrase>, and <phrase>finance</phrase>). However, one painful exception is <phrase>education</phrase>; today, very few <phrase>AI</phrase>-based learning systems are consistently used in classrooms or homes. Yet the potential for an impact on <phrase>education</phrase> is great: today's instructional <phrase>software</phrase> now routinely tailors learning to individual needs, connects learners together, provides access to <phrase>digital</phrase> materials, supports decentralized learning tools and engages students in meaningful ways. As a <phrase>society</phrase> we have <phrase>great expectations</phrase> from the educational establishment (<phrase>train</phrase> employees, support scientific and artistic development, transmit <phrase>culture</phrase>, etc.) and yet, no <phrase>matter</phrase> how much is achieved, <phrase>society</phrase> continues to expect even more from <phrase>education</phrase>. The current educational environment (fixed classrooms, repeated lectures and static printed textbooks) is clearly not capable of either serving <phrase>society</phrase> nor of flexibly changing for the future. Classrooms and printed textbooks are especially inappropriate for people who use <phrase>technology</phrase> on a <phrase>daily</phrase> basis. For example, <phrase>digital</phrase> natives learn and work at twitch speed, through <phrase>parallel processing</phrase>, with graphics and connected to others (vs. stand alone) (Beavis, 2010). For these <phrase>digital</phrase> natives, <phrase>information</phrase> is instantly available, change is constant, distance and time do not <phrase>matter</phrase>, and <phrase>multimedia</phrase> <phrase>entertainment</phrase> is omnipresent. No wonder schools and classrooms are boring! <phrase>Research</phrase> into the <phrase>learning sciences</phrase> and <phrase>neuroscience</phrase> provides essential insights into the intricacies of learning and neural processes underlying learning, offering clues to further refine individual instruction. For example, students who work in teams on motivating and challenging group projects learn more; students who immediately apply what they learn retain more; and students who receive help from <phrase>human</phrase> tutors who answer questions quickly, in ways that reflect <phrase>deep understanding</phrase> of the learner's background, strengths and weaknesses, learn more. Applying these new insights about <phrase>human</phrase> learning in <phrase>digital</phrase> <phrase>learning environments</phrase> requires far deeper <phrase>knowledge</phrase> about <phrase>human</phrase> <phrase>cognition</phrase>, including dramatically more effective constructivist and active instructional strategies. <phrase>AI</phrase> techniques are essential for developing representations and reasoning about these new <phrase>cognitive</phrase> insights and for providing a richer appreciation of how people learn and for measuring collaborative activity. <phrase>Artificial intelligence</phrase> will be a <phrase>game</phrase> changer in <phrase>education</phrase>. In fact, <phrase>education</phrase> and <phrase>AI</phrase> can be seen as two sides of the same <phrase>coin</phrase>: <phrase>education</phrase> helps <phrase>students learn</phrase> and extend the accumulated <phrase>knowledge</phrase> of a <phrase>society</phrase> and <phrase>AI</phrase> provides techniques to understand the mechanisms underlying thought and intelligent behavior. This <phrase>special issue</phrase> of <phrase>AI</phrase> <phrase>Magazine</phrase> describes the use of <phrase>AI</phrase> <phrase>technology</phrase> 
Mirrored <phrase>Language</phrase> Structure and Innate <phrase>Logic</phrase> of the <phrase>Human Brain</phrase> as a <phrase>Computable</phrase> <phrase>Model</phrase> of the <phrase>Oracle</phrase> <phrase>Turing Machine</phrase> We wish to present a mirrored <phrase>language</phrase> structure (<phrase>MLS</phrase>) and four <phrase>logic</phrase> rules determined by this structure for the <phrase>model</phrase> of a <phrase>computable</phrase> <phrase>Oracle</phrase> <phrase>Turing machine</phrase>. <phrase>MLS</phrase> has novel features that are of considerable biological and computational significance. It suggests an <phrase>algorithm</phrase> of relation learning and recognition (RLR) that enables the deterministic <phrase>computers</phrase> to simulate the mechanism of the <phrase>Oracle</phrase> <phrase>Turing machine</phrase>, or P = NP in a <phrase>mathematical</phrase> term. A concept of mirrored <phrase>language</phrase> structure for the <phrase>human brain</phrase> has already been proposed by <phrase>Chomsky</phrase> [4] as <phrase>Universal Grammar</phrase> (UG). His <phrase>model</phrase> consists of a hierarchical (deep and surface) dual <phrase>language</phrase> structure and a possible set of innate rules. He also proposed the concept that <phrase>language</phrase> is the <phrase>mirror</phrase> of the mind [3]. His <phrase>model</phrase> has been well acknowledged. The challenge that remains is to determine the <phrase>universal</phrase> rules between deep and surface <phrase>language</phrase>. A concept of mirrored hierarchical <phrase>language</phrase> structure for the <phrase>Oracle</phrase> <phrase>Turing machine</phrase> was proposed by <phrase>Turing</phrase> [11]. Turing's <phrase>model</phrase> can be roughly described as follows: A <phrase>language</phrase> L consists of two languages: <phrase>Oracle</phrase> <phrase>language</phrase> L o and <phrase>Turing</phrase> <phrase>language</phrase> L t. The <phrase>member</phrase> x of L t can be accepted or rejected correctly by L o as <phrase>member</phrase> y of L o. This <phrase>model</phrase> has been only an abstract concept, and has not been implemented due to lack of an efficient means [5, 6]. The present RLR approach is to apply a <phrase>model</phrase> of the <phrase>human brain</phrase> [10] as a <phrase>computable</phrase> <phrase>model</phrase> of the <phrase>Oracle</phrase> <phrase>Turing machine</phrase>. The <phrase>human brain</phrase> <phrase>model</phrase> has a pair of " mirrored " languages denoted by L p = L c , where p stands for <phrase>perceptual</phrase> and c for conceptual. That is, there exists correspondent relations between the two languages denoted by L p p = c L c. In this structure, the <phrase>member</phrase> |c| of <phrase>language</phrase> L c is the class of the <phrase>member</phrase> |p| of <phrase>language</phrase> L p denoted by L p |p| |c| L c , where |p| and |c| denote the length of p and c, respectively. That is, there also exists <phrase>member</phrase>-class relations between the two languages, where the <phrase>member</phrase> of the <phrase>perceptual</phrase> <phrase>language</phrase> is also the <phrase>member</phrase> of the members of the conceptual <phrase>language</phrase> iteratively, shown as Fig.1: Figure 1. The continuum of <phrase>member</phrase>-class relations between <phrase>perceptual</phrase> and conceptual <phrase>language</phrase> This mirrored structure embeds four innate <phrase>cognitive</phrase> <phrase>logic</phrase> rules. They 
<phrase>Deep Learning</phrase> using Robust Interdependent Codes We investigate a simple yet effective method to introduce inhibitory and <phrase>excitatory</phrase> interactions between units in the layers of a <phrase>deep neural network</phrase> classifier. The method is based on the <phrase>greedy layer-wise</phrase> procedure of <phrase>deep learning</phrase> <phrase>algorithms</phrase> and extends the denoising autoencoder (Vincent et al., 2008) by adding asymmetric lateral connections between its hidden coding units, in a manner that is much simpler and computationally more efficient than previously proposed approaches. We present experiments on two <phrase>character recognition</phrase> problems which show for the first time that lateral connections can significantly improve the classification performance of deep networks.
Educational interaction in <phrase>distance learning</phrase> The <phrase>basic</phrase> definition of <phrase>distance learning</phrase> (<phrase>DL</phrase>) considers that the <phrase>teacher</phrase> and the students are separate in the spatial <phrase>dimension</phrase> and that this distance is filled by using technological resources. Recent technological developments, allowing an increasing level of interaction between users, have implicated a deep change in the educational system. Aim of these studies was to analyze the implementation of interaction in a <phrase>DL</phrase> system with one-way <phrase>video</phrase> and two-way audio channels and to investigate the effect of interaction on <phrase>psychological</phrase> variables. Final <phrase>results</phrase> suggest that the possibility of interaction in <phrase>distance learning</phrase> arouses a greater <phrase>degree</phrase> of attention, interest, participation, concentration, satisfaction and perceived efficacy. The distance modality with interaction takes his place at the intermediate level between the traditional face to face lesson and <phrase>video</phrase>-recorded lesson or <phrase>distance learning</phrase> without interaction. The possibility of interaction during the lesson is a <phrase>basic</phrase> factor for the success of a distance course.
Induction of Integrated View for <phrase>XML</phrase> <phrase>Data</phrase> with Heterogeneous DTDs This <phrase>paper</phrase> proposes a novel approach to integrating heterogeneous <phrase>XML</phrase> DTDs. With this approach, an <phrase>information</phrase> agent can be easily extended to integrate heterogeneous <phrase>XML</phrase>-based contents and perform <phrase>federated search</phrase>. Based on a <phrase>tree</phrase> <phrase>grammar</phrase> inference technique, this approach derives an integrated view of <phrase>XML</phrase> DTDs in an <phrase>information</phrase> integration framework. The derivation takes advantages of naming and structural similarities among DTDs in similar domains. The complete approach consists of three main steps. (1) <i><phrase>DTD</phrase> clustering</i> clusters DTDs in similar domains into classes. (2) <i>Schema learning</i> applies a <phrase>tree</phrase> <phrase>grammar</phrase> inference technique to generate a set of <phrase>tree</phrase> <phrase>grammar</phrase> rules from the DTDs in a class from the previous step. (3) <i>Minimization</i> optimizes the rules generated in the previous step and transforms them into an integrated view. We have implemented the <phrase>proposed approach</phrase> into a system called <i>DEEP</i> and tested the system on <phrase>artificial</phrase> and real domains. The <phrase>experimental</phrase> <phrase>results</phrase> reveal that this system can effectively and efficiently integrate radically different DTDs.
An evaluation of <phrase>student</phrase> learning in a web-supported unit on <phrase>plant</phrase> diversity This <phrase>paper</phrase> reports on a study of the effectiveness of a <phrase>basic</phrase> <phrase>botany</phrase> unit, which has been enhanced by online support materials. A <phrase>WebCT</phrase> site was developed to provide <phrase>digital</phrase> access to materials studied in practical sessions. Whereas previously, students only saw practical material once, now they have access to <phrase>high</phrase> quality, colour images, to study at their leisure. The online materials were evaluated in the context of the unit of study. The <phrase>nature</phrase> of the unit encouraged students to adopt a surface learning approach, but the evaluation process encouraged teaching staff to explore <phrase>alternative</phrase> teaching strategies. Students found the online materials very useful, and some students used them for deep <phrase>approaches to learning</phrase>. However, the majority of use of the online materials was for 'cramming' prior to examinations. The use of the online materials, in this case, <phrase>reinforced</phrase> the surface-learning <phrase>nature</phrase> of the unit.
Beyond <phrase>Nouns</phrase> and <phrase>Verbs</phrase> beyond <phrase>Nouns</phrase> and <phrase>Verbs</phrase> During the past decade, <phrase>computer vision</phrase> <phrase>research</phrase> has focused on constructing image based appearance models of objects and <phrase>action</phrase> classes using large <phrase>databases</phrase> of examples (positive and negative) and <phrase>machine learning</phrase> to construct models. Visual inference however involves not only detecting and recognizing objects and actions but also extracting rich relationships between objects and actions to form storylines or plots. These relationships also improve <phrase>recognition performance</phrase> of appearance-based models. Instead of identifying individual objects and actions in isolation, such systems improve recognition rates by augmenting appearance based models with contextual models based on object-object, <phrase>action</phrase>-<phrase>action</phrase> and object-<phrase>action</phrase> relationships. In this <phrase>thesis</phrase>, we look at the problem of using contextual <phrase>information</phrase> for recognition from three different perspectives: (a) Representation of Contextual Models (b) Role of <phrase>language</phrase> in learning <phrase>semantic</phrase>/contextual models (c) Learning of contextual models from weakly <phrase>labeled data</phrase>. Our work departs from the traditional view of visual and contextual learning where individual detectors and relationships are learned separately. Our work focuses on simultaneous learning of visual appearance and contextual models from richly annotated, weakly labeled datasets. Specifically, we show how rich annotations can be utilized to constrain the learning of visually grounded models of <phrase>nouns</phrase>, prepositions and comparative <phrase>adjectives</phrase> from weakly <phrase>labeled data</phrase>. I will also show how visually grounded models of prepositions and comparative <phrase>adjectives</phrase> can be utilized as contextual models for scene analysis. We also present storyline models for interpretation of videos. Storyline models go beyond pair-wise contextual models and represent <phrase>higher order</phrase> constraints by allowing only specific possible <phrase>action</phrase> sequences (stories). Visual inference using storyline models involve inferring the " plot " of the <phrase>video</phrase> (<phrase>sequence</phrase> of actions) and recognizing individual activities in the plot. Dedication This <phrase>thesis</phrase> is dedicated to my wonderful <phrase>family</phrase> (my parents, sister and my fiance) for all the unconditional <phrase>love</phrase>, guidance, and support that they have always given me. I <phrase>love</phrase> you all! <phrase>ii</phrase> Acknowledgments I owe my gratitude to all the people who have made this <phrase>thesis</phrase> possible and who have made my graduate experience one that I will cherish forever. It is impossible to remember all, however I have tried and I apologize to those I've inadvertently left out. First and foremost I'd like to thank my advisor, <phrase>Professor</phrase> Larry Davis without whom this <phrase>thesis</phrase> would not have been possible. Larry's <phrase>knowledge</phrase> of the <phrase>literature</phrase>, deep insight into problems, and fresh ideas and surprising intuition has made him a great 
The <phrase>Architecture</phrase> of <phrase>Human</phrase>-like <phrase>General</phrase> <phrase>Intelligence</phrase> By exploring the relationships between different AGI architectures, one can work toward a holistic <phrase>cognitive</phrase> <phrase>model</phrase> of <phrase>human</phrase>-level <phrase>intelligence</phrase>. In this vein, here an integrative <phrase>architecture</phrase> diagram for <phrase>human</phrase>-like <phrase>general</phrase> <phrase>intelligence</phrase> is proposed, via merging of lightly modified version of prior diagrams including Aaron Sloman's <phrase>high</phrase>-level <phrase>cognitive</phrase> <phrase>model</phrase>, Stan Franklin and the <phrase>LIDA</phrase> group's <phrase>model</phrase> of <phrase>working memory</phrase> and the <phrase>cognitive</phrase> cycle, Joscha <phrase>Bach</phrase> and <phrase>Dietrich</phrase> Drner's Psi <phrase>model</phrase> of motivated <phrase>action</phrase> and <phrase>cognition</phrase>, James Al-bus's three-hierarchy intelligent <phrase>robotics</phrase> <phrase>model</phrase>, and the author's prior work on <phrase>cognitive</phrase> <phrase>synergy</phrase> in deliberative thought and <phrase>metacognition</phrase>, along with ideas from <phrase>deep learning</phrase> and <phrase>computational linguistics</phrase>. The purpose is not to propose an actual merger of the various AGI systems considered, but rather to highlight the points of compatibility between the different approaches, as well as the differences of both focus and substance. The result is perhaps the most <phrase>comprehensive</phrase> <phrase>architecture</phrase> diagram of <phrase>human</phrase>-<phrase>cognition</phrase> yet <phrase>produced</phrase>, tying together all key aspects of <phrase>human intelligence</phrase> in a coherent way that is not tightly bound to any particular <phrase>cognitive</phrase> or AGI theory. Finally, the question of the dynamics associated with the <phrase>architecture</phrase> is considered, including the potential that <phrase>human</phrase>-level <phrase>intelligence</phrase> requires <phrase>cognitive</phrase> <phrase>synergy</phrase> between these various components is considered; and the possibility of a " trickiness " <phrase>property</phrase> causing the <phrase>intelligence</phrase> of the overall system to be badly suboptimal if any of the components are missing or insufficiently <phrase>cooperative</phrase>. One idea emerging from these dynamic consideration is that implementing the whole inte-grative <phrase>architecture</phrase> diagram may be necessary for achieving anywhere near <phrase>human</phrase>-level, <phrase>human</phrase>-like <phrase>general</phrase> <phrase>intelligence</phrase>.
<phrase>Coffee</phrase> Crop Recognition Using Multi-scale <phrase>Convolutional Neural Networks</phrase> Identifying crops from <phrase>remote sensing</phrase> images is a fundamental to know and monitor land-use. However, manual identification is expensive and maybe impracticable given the amount <phrase>data</phrase>. Automatic methods, although interesting, are highly dependent on the quality of extracted features, since encoding the spatial features in an efficient and robust <phrase>fashion</phrase> is the key to generating discriminatory models. Even though many visual descriptors have been proposed or successfully used to encode spatial features, in some cases, more specific description are needed. <phrase>Deep learning</phrase> has achieved very good <phrase>results</phrase> in some tasks, mainly boosted by the <phrase>feature learning</phrase> performed which allows the method to extract specific and adaptable <phrase>visual features</phrase> depending on the <phrase>data</phrase> In this <phrase>paper</phrase>, we propose two multi-scale methods, based on <phrase>deep learning</phrase>, to identify <phrase>coffee</phrase> crops. Specifically, we propose the <phrase>Cascade</phrase> <phrase>Convolutional Neural Networks</phrase>, or simply CCNN, that identifies crops considering a hierarchy of networks and, also, propose the Iterative <phrase>Convolutional Neural Network</phrase>, called ICNN, which feeds a same network with <phrase>data</phrase> several times. We conducted a systematic evaluation of the proposed <phrase>algorithms</phrase> using a <phrase>remote sensing</phrase> dataset. The experiments show that the proposed methods outperform the baseline consistent of <phrase>state</phrase>-of-the-<phrase>art</phrase> components by a factor that <phrase>ranges</phrase> from 3 to 6%, in terms of <phrase>average</phrase> accuracy.
Emotional agent in serious <phrase>game</phrase> (DINO) In this <phrase>paper</phrase>, we introduce an novel emotional agent system in 3D <phrase>virtual world</phrase> based on <phrase>OCC</phrase> (Ortony, Clore and Collins) theory, FCM (Fuzzy <phrase>Cognitive Map</phrase>) and GoalNet. The agent system is designed based on Goal Net <phrase>model</phrase>. Emotional modeling and <phrase>decision making</phrase> are based on <phrase>OCC</phrase> and FCM inference. Emotions modeled by the <phrase>OCC</phrase> <phrase>model</phrase> are incorporated into FCM inference. It has been shown <phrase>emotion</phrase> has a great impact in <phrase>decision making</phrase>. The proposed agent system has been applied to <phrase>design</phrase> <phrase>human</phrase>-like <phrase>dinosaur</phrase> agents in the <phrase>research</phrase> project " Immersion and Embodied Learning: Traces of <phrase>Dinosaurs</phrase> in <phrase>Earth</phrase> System <phrase>Science</phrase> " supported by National <phrase>Research</phrase> Foundation (NRF) <phrase>Singapore</phrase>. In this project, we have developed an agent mediated immersive Interactive <phrase>Digital Media</phrase> that appropriately recreates and replays the traces of Earth's <phrase>history</phrase> using <phrase>intelligent agent</phrase> <phrase>technology</phrase> and 3D multiuser environment. The <phrase>virtual world</phrase> and emotional <phrase>dinosaur</phrase> agents are shown in the <phrase>demo</phrase>. The experiments conducted by students in <phrase>secondary schools</phrase> in <phrase>Singapore</phrase> have shown the emotional agents enable <phrase>deep learning</phrase> of <phrase>earth science</phrase>.
<phrase>Deep belief</phrase> networks 1 Abstract Recently, Hinton et al.[6] derived a way to perform fast, greedy learning of <phrase>deep belief</phrase> networks (DBN) one layer at a time, with the top two layers forming an undirected <phrase>bipartite graph</phrase> (associate <phrase>memory</phrase>). The learning procedure consists of training a stack of <phrase>Restricted Boltzmann Machines</phrase> (RBM's) each having only one layer of latent (hidden) feature detectors. The learned feature activations of one RBM are used as the " <phrase>data</phrase> " for training the next RBM in the stack. The important <phrase>aspect</phrase> of this <phrase>layer-wise</phrase> training procedure is that, provided the number of features per layer does not decrease, [6] showed that each extra layer increases a variational <phrase>lower</phrase> bound on the log <phrase>probability</phrase> of <phrase>data</phrase>. So <phrase>layer-by-layer</phrase> training can be repeated several times 1 to learn a deep, hierarchical <phrase>model</phrase> in which each layer of features captures strong <phrase>high</phrase>-<phrase>order</phrase> correlations between the activities of features in the layer below. We will discuss three ideas based on greedily learning a hierarchy of features: Nonlinear <phrase>Dimensionality Reduction</phrase>. The DBN framework allows us to make nonlinear autoencoders work considerably better [7] than widely used methods such as <phrase>PCA</phrase>, <phrase>SVD</phrase>, and LLE. The standard way to <phrase>train</phrase> autoencoders is to use <phrase>backpropagation</phrase> to reduce the <phrase>reconstruction</phrase> error. It is difficult, however, to optimize the weights in non-linear autoencoders that have multiple <phrase>hidden layers</phrase> with many million parameters [3, 5]. We use our greedy <phrase>learning algorithm</phrase> to pretrain autoencoders. This pretraining stage discovers useful features efficiently. After the pretraining stage, the <phrase>model</phrase> is " unfolded " to produce encoder and decoder networks that initially use the same weights. The global <phrase>fine-tuning</phrase> stage then uses <phrase>backpropagation</phrase> through the whole autoencoder to fine-tune the weights for optimal <phrase>reconstruction</phrase>. The key idea is that the greedy <phrase>learning algorithm</phrase> will perform a global search for a good, sensible <phrase>region</phrase> in the <phrase>parameter space</phrase>. Therefore, with this pretraining, we will already have a good <phrase>data</phrase> <phrase>reconstruction</phrase> <phrase>model</phrase>. <phrase>Backpropagation</phrase> is better at local <phrase>fine-tuning</phrase> of the <phrase>model</phrase> parameters than global search. So further training of the entire autoencoder using <phrase>backpropagation</phrase> will result in a good local optimum. Learning <phrase>Semantic</phrase> <phrase>Address Space</phrase> (<phrase>SAS</phrase>) for Fast Document Retrieval. Most of the existing text retrieval <phrase>algorithms</phrase> in one way or another rely on comparing a given query document to all other documents from the large document collection, retrieving the most relevant ones. Typically, the larger the word <phrase>vocabulary</phrase> and the size 
Developing <phrase>Learning by Teaching</phrase> Environments That Support Self-Regulated Learning Betty's <phrase>Brain</phrase> is a teachable agent system in the domain of <phrase>river</phrase> <phrase>ecosystems</phrase> that combines <phrase>learning by teaching</phrase> and self-regulation strategies to <phrase>promote deep learning</phrase> and understanding. Scaffolds in the form of <phrase>hypertext</phrase> resources , a Mentor agent, and a set of quiz questions help novice <phrase>students learn</phrase> and self-assess their own <phrase>knowledge</phrase>. The computational <phrase>architecture</phrase> is implemented as a <phrase>multi-agent</phrase> system to allow flexible and incremental <phrase>design</phrase>, and to provide a more realistic social context for interactions between students and the teachable agent. An extensive study that compared three versions of this system: a <phrase>tutor</phrase> only version, <phrase>learning by teaching</phrase>, and <phrase>learning by teaching</phrase> with self-regulation strategies demonstrates the effectiveness of <phrase>learning by teaching</phrase> environments, and the impact of self-regulation strategies in improving preparation for learning among novice learners.
<phrase>Visual Representations</phrase> for <phrase>Fine-grained</phrase> Categorization <phrase>Visual Representations</phrase> for <phrase>Fine-grained</phrase> Categorization Abstract <phrase>Visual Representations</phrase> for <phrase>Fine-grained</phrase> Categorization Permission to make <phrase>digital</phrase> or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or <phrase>commercial advantage and that copies bear</phrase> this notice and the full citation on the first page. To copy otherwise, to republish, to <phrase>post on servers</phrase> or to redistribute to lists, <phrase>requires prior specific permission</phrase>. In contrast to <phrase>basic</phrase>-level <phrase>object recognition</phrase>, <phrase>fine-grained</phrase> categorization aims to distinguish between subordinate categories, such as different <phrase>animal</phrase> breeds or <phrase>species</phrase>, <phrase>plant</phrase> <phrase>species</phrase> or man-made product models. The problem can be extremely challenging due to the subtle differences in the appearance of certain parts across related categories and often requires distinctions that must be conditioned on the object pose for reliable identification. Discrimi-<phrase>native</phrase> markings are often highly localized, leading traditional <phrase>object recognition</phrase> approaches to struggle with the large pose variations often present in these domains. <phrase>Face recognition</phrase> is the classic case of <phrase>fine-grained</phrase> recognition, and it is noteworthy that the best <phrase>face recognition</phrase> methods jointly discover facial landmarks and extract features from those locations. We propose pose-normalized representations, which align training exemplars, either piece-wise by part or globally for the whole object, effectively factoring out differences in pose and in <phrase>camera</phrase> viewing angle. I first present the methods of using the idea of pose-normalization for two related applications: <phrase>human</phrase> attribute classification and person recognition beyond frontal face. Following the recent success of <phrase>deep learning</phrase>, we use <phrase>deep convolutional</phrase> features as <phrase>feature representations</phrase>. Next, I will introduce the part-based RCNN method as an extension of <phrase>state</phrase>-of-<phrase>art</phrase> <phrase>object detection</phrase> method RCNN for <phrase>fine-grained</phrase> categorization. The <phrase>model</phrase> learns both whole-object and part detectors, and enforces learned geometric constraints between them. I will also show the <phrase>results</phrase> of using the recent compact <phrase>bilinear</phrase> features to generate the pose-normalized representations. However, bottom-up <phrase>region</phrase> proposals is limited by hand-engineered features and in the final work, I will present a fully <phrase>convolution</phrase> deep network, trained <phrase>end-to-end</phrase> for part localization and <phrase>fine-grained</phrase> classification.
<phrase>Unsupervised feature learning</phrase> for audio classification using convolutional <phrase>deep belief</phrase> networks In recent years, <phrase>deep learning</phrase> approaches have gained significant interest as a way of building hierarchical representations from <phrase>unlabeled data</phrase>. However, to our <phrase>knowledge</phrase>, these <phrase>deep learning</phrase> approaches have not been extensively studied for auditory <phrase>data</phrase>. In this <phrase>paper</phrase>, we apply convolutional <phrase>deep belief</phrase> networks to audio <phrase>data</phrase> and empirically evaluate them on various audio <phrase>classification tasks</phrase>. In the case of speech <phrase>data</phrase>, we show that the <phrase>learned features</phrase> correspond to <phrase>phones</phrase>/<phrase>phonemes</phrase>. In addition, our <phrase>feature representations</phrase> learned from unlabeled audio <phrase>data</phrase> show very good performance for multiple audio <phrase>classification tasks</phrase>. We hope that this <phrase>paper</phrase> will inspire more <phrase>research</phrase> on <phrase>deep learning</phrase> approaches applied to a wide <phrase>range</phrase> of audio <phrase>recognition tasks</phrase>.
How and what <phrase>university</phrase> <phrase>students learn</phrase> through online and face-to-face discussion: conceptions, intentions and approaches This <phrase>paper</phrase> reports a phenomenographic investigation into students' experiences of learning through discussion both online and face to face (F2F). The study context was a second-year <phrase>undergraduate</phrase> course in <phrase>psychology</phrase> for social work in which the <phrase>teacher</phrase> had designed discussion tasks to begin in F2F mode and to continue online. A combination of <phrase>open-ended</phrase> questionnaires and semi-structured interviews was used to investigate students' conceptions of what they were learning, their intentions and their <phrase>approaches to learning</phrase> through discussion. Analysis of the interview and <phrase>open-ended</phrase> questionnaire <phrase>data</phrase> identified a number of qualitatively different conceptions, intentions and <phrase>approaches to learning</phrase> through discussion. Associations were found between what students thought they were learning through discussions , their <phrase>approaches to learning</phrase> through discussion and their course grade. Students with a cohesive conception and students adopting a deep approach (to learning through <phrase>online discussion</phrase>) got better course grades. There was no significant difference between deep and surface approaches to F2F discussion and course grade. The outcomes of this study have implications for the <phrase>design</phrase> of online and F2F discussion tasks and in particular for helping students adopt richer conceptions of what they stand to gain through discussion.
<phrase>Fusion</phrase> Based Holistic <phrase>Road</phrase> Scene Understanding This <phrase>paper</phrase> addresses the problem of holistic <phrase>road</phrase> scene understanding based on the integration of visual and <phrase>range</phrase> <phrase>data</phrase>. To achieve the grand goal, we propose an approach that jointly tackles object-level <phrase>image segmentation</phrase> and <phrase>semantic</phrase> <phrase>region</phrase> labeling within a <phrase>conditional random field</phrase> (CRF) framework. Specifically, we first generate <phrase>semantic</phrase> object hypotheses by clustering 3D points, learning their prior appearance models, and using a <phrase>deep learning</phrase> method for reasoning their <phrase>semantic</phrase> categories. The learned priors, together with spatial and geometric contexts, are incorporated in CRF. With this formulation, visual and <phrase>range</phrase> <phrase>data</phrase> are fused thoroughly, and moreover, the coupled segmentation and <phrase>semantic</phrase> labeling problem can be inferred via <phrase>Graph</phrase> Cuts. Our approach is validated on the challenging KITTI dataset that contains diverse complicated <phrase>road</phrase> scenarios. Both quantitative and qualitative evaluations demonstrate its effectiveness.
<phrase>Lessons Learned</phrase>: Reflections of a <phrase>University</phrase> <phrase>President</phrase> William Bowen reflects on the lessons he learned while he was <phrase>president</phrase> of <phrase>Princeton university</phrase> in New <phrase>Jersey</phrase> from 1972 to 1988, and <phrase>president</phrase> of the andrew W. <phrase>mellon Foundation</phrase> in New <phrase>York</phrase> from 1988 to 2006. He shares advice on fund-raising, hiring, managing <phrase>faculty members</phrase> and interacting with trustees. and he reveals his experience of shepherding the elite <phrase>university</phrase> through the <phrase>civil-rights</phrase> movement and the <phrase>Vietnam War</phrase>, a <phrase>period</phrase> during which he helped to expand the faculty, especially in the <phrase>life</phrase> sciences. <phrase>gender</phrase> differences have deep and <phrase>tangled</phrase> roots, according to <phrase>neuroscientist</phrase> Donald Pfaff. although <phrase>genetic</phrase> and biological factors such as <phrase>neuroanatomy</phrase> contribute to this <phrase>dichotomy</phrase>, he argues, they do not dominate. <phrase>Cultural</phrase> influences, including experiences of stress throughout various stages of our lives, may be just as large and affect males and females in varied ways. Differences between the sexes, both physical and mental, result from a combination of <phrase>genetics</phrase> and environment that operates on many levels to influence behavioural mechanisms. Living longer may not be such a good thing, cautions <phrase>bioethicist</phrase> anthony Farrant. although breakthroughs in <phrase>medical</phrase> <phrase>biotechnology</phrase> have the potential to extend our lives and make them healthier, he disputes the idea that <phrase>immortality</phrase> is desirable and cautions that the ready availability of such enhancements will diminish the value we put on reaching old age. increasing <phrase>longevity</phrase> will challenge the <phrase>fair</phrase> distribution of resources, especially <phrase>health care</phrase>. ultimately, he says, these pressures will undermine the idea that all people are fundamentally equal, and thus threaten the good <phrase>life</phrase>. One in seven of the world's <phrase>population</phrase> is <phrase>short</phrase> of <phrase>food</phrase>. Lack of <phrase>political</phrase> will is the main reason for not addressing hunger, explain <phrase>medical</phrase> <phrase>scientist</phrase> John Butterly and environmental <phrase>scientist</phrase> Jack shepherd. as well as describing the <phrase>biology</phrase> of <phrase>human nutrition</phrase> and <phrase>famine</phrase>, they examine the <phrase>political</phrase> and historical factors that cause hunger and <phrase>malnutrition</phrase> to remain <phrase>major</phrase> <phrase>health</phrase> problems today despite advances in <phrase>science</phrase> and <phrase>technology</phrase> and the proliferation of humanitarian efforts. The origins of <phrase>science</phrase> in <phrase>ancient greece</phrase> are explored by classicist stephen Bertman. He looks beyond the familiar names such as <phrase>euclid</phrase> and <phrase>Pythagoras</phrase> to lesser-known figures, including the mapmaker <phrase>anaximander</phrase> and <phrase>alchemist</phrase> maria the Jewess, popularly known for inventing the <phrase>eponymous</phrase> <phrase>bain-marie</phrase> <phrase>water</phrase> <phrase>bath</phrase> and various pieces of chemical apparatus, including the still. Bertman argues that the <phrase>greeks</phrase> owe their scientific success to their belief in an ordered <phrase>universe</phrase>, the rules of which 
Mechanical Verification of <phrase>Concurrency Control</phrase> and Recovery Protocols Mechanical Verification of <phrase>Concurrency Control</phrase> and Recovery Protocols / Mechanical Verification of <phrase>Concurrency Control</phrase> and Recovery Protocols Dmitri Aleksandrovich Chkliaev The work in this <phrase>thesis</phrase> has been carried out under the auspices of the <phrase>research</phrase> <phrase>school</phrase> <phrase>IPA</phrase> (Institute for <phrase>Programming</phrase> <phrase>research</phrase> and Algorithmics). Acknowledgments Having reached the end of my project, I would like to thank everybody who made it possible for me to write this <phrase>thesis</phrase>. First of all, I wish to <phrase>express my deep</phrase> gratitude to direct supervisors of this Ph.D. project Jozef Hooman and Peter van der Stok, who organized the project and invited me to <phrase>Eindhoven</phrase> to carry it out. Without their <phrase>daily</phrase> help and advise I would not have gotten very far. Secondly, I thank Dieter <phrase>Hammer</phrase> for his excellent supervision of the Technical Applications (<phrase>TT</phrase>) group and for serving as my first supervisor. I very much enjoyed the pleasant <phrase>atmosphere</phrase> in the <phrase>TT</phrase> group and <phrase>nice</phrase> social contacts with its members. Former <phrase>TT</phrase> members Richard Kelleners, <phrase>Marja</phrase> de Vroome and Elsa Gelis Escala have been especially helpful and entertaining. Our former <phrase>secretary</phrase> <phrase>Marja</phrase> de Vroome found an <phrase>apartment</phrase> for me where I still <phrase>live</phrase>. My former roommates at TUE Paul van Gorp and Maarten Bodlaender helped me to learn <phrase>UNIX</phrase> during the first year of my project. I am also grateful to Tineke van <phrase>den Bosch</phrase> and Michel Reniers for helping to prepare the <phrase>Dutch</phrase> summary of the <phrase>thesis</phrase>. My special thanks go to Alexei Sintotski, who has been a perfect roommate and friend for 3 years and assisted me with many problems related to <phrase>software</phrase> and hardware. We had a lot of interesting discussions in room HG 6.85 and turned it into a <phrase>center</phrase> of intense activity in the <phrase>area</phrase> of <phrase>formal specification</phrase> and verification. I am very grateful to Jos Baeten for offering me an excellent new job, a postdoc position in the group of <phrase>Formal Methods</phrase>. I am also grateful to him and my new supervisor Erik de Vink for giving me enough time to work on the final version of the <phrase>thesis</phrase>. I thank members of the the <phrase>reading</phrase> committee Dieter <phrase>Hammer</phrase> (first supervisor), Jos Baeten (second supervisor), Jozef Hooman (co-supervisor), Peter van der Stok and Peter Apers, for their timely approval of this <phrase>thesis</phrase> and many useful remarks. I also thank all other members of the doctoral committee for their agreement to serve in the committee. Finally, I thank my parents <phrase>Alexander</phrase> and Larisa, other <phrase>family</phrase> members and all my <phrase>friends</phrase> for their motivating support during my stay in 
Intelligent <phrase>Hybrid</phrase> Man-<phrase>machine Translation</phrase> Evaluation Intelligent <phrase>Hybrid</phrase> Man-<phrase>machine Translation</phrase> Evaluation Presented By ACKNOWLEDGEMENT First, I thank <phrase>ALLAH</phrase> for giving me the ability to complete the work on this <phrase>thesis</phrase>. Noha Yousri for their advising and help during working on the <phrase>thesis</phrase>. This work could not have reached this phase, if it were not for their support, advice and guidance. I would like to acknowledge my gratitude for <phrase>Microsoft</phrase> Advanced <phrase>Technology</phrase> Lab in <phrase>Cairo</phrase> (ATLC). We have benefited greatly from their support to run our experiments. Also, I am deeply grateful for my <phrase>fellow</phrase> <phrase>TAs</phrase> for their <phrase>company</phrase> and the experience I shared with them. In particular, I am grateful to: Ahmed Essam, Mahmoud Fouad and Victor Zakhary for their sincere help and support during my experience as a TA. Last but not the least, I thank my wife, my brother and my sisters for their continuous support and encouragement. vii <phrase>ABSTRACT Machine</phrase> <phrase>Translation</phrase> (MT) has grasped a lot of attention in <phrase>translation</phrase> communities during the recent years and become a crucial part in almost all <phrase>search engines</phrase>. However, the widespread of MT <phrase>technology</phrase> depends on the trust associated with its outputs. Different approaches have been introduced to address the issues of evaluating translations from one <phrase>natural language</phrase> to another. Automatic metrics have been developed to predict the quality of MT outputs. Although these metrics are efficient in terms of speed, the existence of reference translations is assumed. Another <phrase>research</phrase> direction, known as Quality Estimation (QE), was proposed to exploit <phrase>human</phrase> assessments for evaluation based on <phrase>machine learning</phrase> techniques and without reference translations. Both of automatic metrics and QE approaches have drawbacks. Automatic metrics paid little attention to capture any <phrase>information</phrase> at <phrase>linguistic</phrase> levels further than lexical. Therefore, these metrics are considered superficial. On the other hand, QE approaches rely only on <phrase>human</phrase> assessments which are much more expensive to obtain. Moreover, <phrase>human</phrase> assessments can vary for the same translated sentence. In this <phrase>thesis</phrase>, the drawbacks of these two directions are addressed. We extracted a set of <phrase>linguistic</phrase> and <phrase>data</phrase>-driven features from <phrase>parallel corpora</phrase> to evaluate MT outputs. The advantages of these features are twofold. First, they provide a <phrase>deep linguistic</phrase> insight which addresses a key issue in automatic metrics. Second, these features are extracted from <phrase>parallel corpora</phrase> without the need for expensive <phrase>human</phrase> assessments. The <phrase>experimental</phrase> evaluation shows that our proposed system outperforms <phrase>state</phrase>-of-the-<phrase>art</phrase> automatic metrics in terms of accuracy. Moreover, if <phrase>human</phrase> assessments are available, the <phrase>proposed approach</phrase> can benefit from 
Multi-Modal <phrase>Hybrid</phrase> <phrase>Deep Neural Network</phrase> for Speech Enhancement <phrase>Deep Neural Networks</phrase> (DNN) have been successful in enhancing noisy speech signals. Enhancement is achieved by learning a nonlinear mapping <phrase>function</phrase> from the features of the corrupted speech signal to that of the reference clean speech signal. The quality of predicted features can be improved by providing additional side <phrase>channel</phrase> <phrase>information</phrase> that is robust to noise, such as visual cues. In this <phrase>paper</phrase> we propose a novel <phrase>deep learning</phrase> <phrase>model</phrase> inspired by insights from <phrase>human</phrase> audio <phrase>visual perception</phrase>. In the proposed unified <phrase>hybrid</phrase> <phrase>architecture</phrase>, features from a <phrase>Convolution</phrase> <phrase>Neural Network</phrase> (<phrase>CNN</phrase>) that processes the visual cues and features from a <phrase>fully connected</phrase> DNN that processes the <phrase>audio signal</phrase> are integrated using a Bidirectional <phrase>Long</phrase> <phrase>Short-Term Memory</phrase> (BiLSTM) network. The parameters of the <phrase>hybrid</phrase> <phrase>model</phrase> are jointly learned using <phrase>backpropagation</phrase>. We compare the quality of enhanced speech from the <phrase>hybrid</phrase> models with those from traditional DNN and BiLSTM models.
Improving <phrase>CNN</phrase> Performance with Min-Max Objective In this <phrase>paper</phrase>, we propose a novel method to improve <phrase>object recognition</phrase> accuracies of convolu-tional <phrase>neural networks</phrase> (CNNs) by embedding the proposed Min-Max objective into a <phrase>high</phrase> layer of the models during the training process. The Min-Max objective explicitly enforces the learned object feature maps to have the minimum compactness for each object <phrase>manifold</phrase> and the maximum margin between different object <phrase>manifolds</phrase>. The Min-Max objective can be universally applied to different <phrase>CNN</phrase> models with negligible additional computation cost. Experiments with shallow and deep models on four <phrase>benchmark datasets</phrase> including CIFAR-10, CIFAR-100, SVHN and MNIST demonstrate that <phrase>CNN</phrase> models trained with the Min-Max objective achieve remarkable performance improvements compared to the corresponding baseline models.
A <phrase>Weakly Supervised</phrase> Learning Approach for <phrase>Spoken Language</phrase> Understanding In this <phrase>paper</phrase>, we present a <phrase>weakly supervised</phrase> learning approach for <phrase>spoken language</phrase> understanding in <phrase>domain-specific</phrase> dialogue systems. We <phrase>model</phrase> the task of <phrase>spoken language</phrase> understanding as a successive <phrase>classification problem</phrase>. The first classifier (topic classifier) is used to identify the topic of an input utterance. With the restriction of the recognized <phrase>target</phrase> topic, the second classifier (<phrase>semantic</phrase> classifier) is trained to extract the corresponding slot-value pairs. It is mainly <phrase>data</phrase>-driven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for <phrase>spoken language</phrase>. Most importantly , it allows the <phrase>employment</phrase> of <phrase>weakly supervised</phrase> strategies for training the two classifiers. We first apply the training strategy of combining <phrase>active learning</phrase> and self-training (Tur et al., 2005) for topic classifier. Also, we propose a practical method for <phrase>bootstrapping</phrase> the topic-dependent <phrase>semantic</phrase> classifiers from a small amount of labeled sentences. Experiments have been conducted in the context of <phrase>Chinese</phrase> <phrase>public</phrase> transportation <phrase>information</phrase> inquiry domain. The <phrase>experimental</phrase> <phrase>results</phrase> demonstrate the effectiveness of our proposed SLU framework and show the possibility to reduce <phrase>human</phrase> labeling efforts significantly.
Clickers and <phrase>Deep Learning</phrase>: an <phrase>Oxymoron</phrase>? The use of clicker <phrase>technology</phrase> as a tool for promoting learning, let along <phrase>deep learning</phrase>, is hotly debated and has its proponents and opponents. With the question still open, this <phrase>paper</phrase> examines the use of this <phrase>technology</phrase> in fostering <phrase>critical thinking</phrase> and other <phrase>higher-order</phrase> learning and <phrase>team-building</phrase> skills, in the context of two entry-level <phrase>psychology</phrase> courses. The <phrase>results</phrase> obtained indicate that from the perspectives of both the <phrase>student</phrase> and the instructor, the integration of this <phrase>technology</phrase> was positively perceived to enhance the learning process and the acquisition of these skills. It would seem these concepts are, after all, not antithetical.
<phrase>Genetic algorithms</phrase> for evolving <phrase>deep neural networks</phrase> In recent years, <i><phrase>deep learning</phrase></i> methods applying <phrase>unsupervised learning</phrase> to <phrase>train</phrase> deep layers of <phrase>neural networks</phrase> have achieved remarkable <phrase>results</phrase> in numerous fields. In the past, many <phrase>genetic algorithms</phrase> based methods have been successfully applied to training <phrase>neural networks</phrase>. In this <phrase>paper</phrase>, we extend previous work and propose a GA-assisted method for <phrase>deep learning</phrase>. Our <phrase>experimental</phrase> <phrase>results</phrase> indicate that this GA-assisted approach improves the performance of a deep autoencoder, producing a sparser <phrase>neural network</phrase>.
Collaborative <phrase>Feature Learning</phrase> from <phrase>Social Media</phrase> Image <phrase>feature representation</phrase> plays an essential role in <phrase>image recognition</phrase> and related tasks. The <phrase>current state</phrase>-of-the-<phrase>art</phrase> <phrase>feature learning</phrase> <phrase>paradigm</phrase> is <phrase>supervised learning</phrase> from <phrase>labeled data</phrase> [3], which surpasses other well-known <phrase>hand-crafted</phrase> feature based methods [4, 5]. However, this <phrase>paradigm</phrase> requires <phrase>large datasets</phrase> with category <phrase>labels</phrase> to <phrase>train</phrase> properly, which limits its applicability to new problem domains where <phrase>labels</phrase> are hard to obtain. In this <phrase>paper</phrase>, we ask an interesting <phrase>research</phrase> question: Are category-level <phrase>labels</phrase> the only way for <phrase>data</phrase> driven <phrase>feature learning</phrase>? There is a surge of <phrase>social media</phrase> <phrase>websites</phrase> in the last ten years. Most <phrase>social media</phrase> <phrase>websites</phrase> such as <phrase>Pinterest</phrase> have been collecting content <phrase>data</phrase> that the users share as well as behavior <phrase>data</phrase> of the users. User behavior <phrase>data</phrase> are the activities of individual users, such as likes, comments, or view histories and they carry rich <phrase>information</phrase> about corresponding content <phrase>data</phrase>. For instance, two photos of a similar style on <phrase>Pinterest</phrase> tend to be pinned by the same user. If we aggregate the user behavior <phrase>data</phrase> across many users, we may recover interesting properties of the content. For instance, the photos liked by a group of users of similar interests tend to have very similar styles. Approach. We propose a new <phrase>paradigm</phrase> for <phrase>data</phrase> driven image <phrase>feature learning</phrase> which we call collaborative <phrase>feature learning</phrase>. It is a <phrase>major</phrase> departure from the existing paradigms on <phrase>feature learning</phrase> such as <phrase>supervised learning</phrase> in that we do not rely on category <phrase>labels</phrase> at all. The main idea is to learn image features from user behavior <phrase>data</phrase> on <phrase>social media</phrase>. In particular, we use the user behavior <phrase>data</phrase> collected on <phrase>social media</phrase> to recover latent representations of individual images and learn a feature transformation from the images to the recovered latent representations. The <phrase>proposed approach</phrase> is a framework that unifies latent <phrase>factor analysis</phrase> and <phrase>deep convolutional</phrase> <phrase>neural network</phrase> for image <phrase>feature learning</phrase> from <phrase>social media</phrase>. We focus on the simple form of user-item view <phrase>data</phrase> in this work to keep our <phrase>feature learning</phrase> framework <phrase>general</phrase>. Figure 1 provides a <phrase>high</phrase>-level overview of our approach. Given a set of content items I = the corresponding user-item view <phrase>data</phrase> is in the format of a matrix between I and U, which is denoted as V R <phrase>MN</phrase>. To handle the sparsity and noise in V , and extract compact latent <phrase>information</phrase> from it, we use <phrase>collaborative filtering</phrase> for implicit <phrase>feedback</phrase> <phrase>data</phrase> [2] 
Technologies That Make You Smile: Adding Humor to <phrase>Text-Based</phrase> Applications Carlo Strapparava, Istituto per la ricerca scientifica <phrase>e</phrase> Tecnologica N atural language's creative genres are traditionally considered to be outside the scope of computational modeling. Computational <phrase>linguists</phrase> have paid little attention to humor in particular because it is puzzling by <phrase>nature</phrase>. However, given the importance of humor in our <phrase>daily</phrase> lives and <phrase>computers</phrase> in our work and <phrase>entertainment</phrase>, studies related to computational humor will become increasingly significant in fields such as <phrase>human</phrase>-computer interaction, intelligent interactive <phrase>entertainment</phrase>, and computer-assisted <phrase>education</phrase>. Previous work in computational humor has focused mainly on humor generation, 1,2 and little <phrase>research</phrase> has addressed developing systems for automatic humor recognition 3 (see the " Related Work on Computational Humor " sidebar). This is not surprising because, computationally, humor recognition appears to be significantly more subtle and difficult than humor generation. Moreover, the absence of very large collections of humorous texts has hindered the development of systems that use humor in <phrase>text-based</phrase> applications. Consequently, few such systems are available. In this article, we explore computational <phrase>ap</phrase>-proaches' applicability to the recognition and use of verbally expressed humor. Particularly, we focus on three important <phrase>research</phrase> questions related to this problem: Can we automatically gather large collections of humorous texts? Can we automatically recognize humor in text? And can we automatically insert humorous add-ons into existing applications? One-<phrase>liners</phrase> versus <phrase>long</phrase> jokes Because a deep comprehension of all humor styles is probably too ambitious for existing computational capabilities, we restricted our investigation to one-<phrase>liners</phrase>. A one-liner is a <phrase>short</phrase> sentence with <phrase>comic</phrase> effects and an interesting <phrase>linguistic</phrase> structure: simple <phrase>syntax</phrase>, deliberate use of <phrase>rhetoric</phrase> devices (such as <phrase>alliteration</phrase> or rhyme), and frequent use of creative <phrase>language</phrase> constructions meant to attract the reader's attention. For instance, " I'm not a <phrase>vegetarian</phrase> because I <phrase>love</phrase> animals, I'm a <phrase>vegetarian</phrase> because I hate <phrase>plants</phrase> " is an example of a one-liner. Although longer jokes can have a relatively complex <phrase>narrative structure</phrase>, a one-liner must produce the humorous effect in one <phrase>shot</phrase>, with few words. This makes one-<phrase>liners</phrase> particularly suitable for automatic learning settings because the humor-producing features are guaranteed to be present in the first (and only) sentence. Large amounts of <phrase>training data</phrase> can potentially make the learning process more accurate and at the same time provide insights into how increasingly larger <phrase>data</phrase> sets can affect classification precision. However, we found that manually constructing a very large one-liner <phrase>data set</phrase> was problematic because most <phrase>Web sites</phrase> 
In All Likelihood, <phrase>Deep Belief</phrase> Is Not Enough <phrase>Statistical models</phrase> of <phrase>natural images</phrase> provide an important tool for researchers in the fields of <phrase>machine learning</phrase> and <phrase>computational neuroscience</phrase>. The canonical measure to quantitatively assess and compare the performance of <phrase>statistical models</phrase> is given by the likelihood. One class of <phrase>statistical models</phrase> which has recently gained increasing popularity and has been applied to a <phrase>variety</phrase> of complex <phrase>data</phrase> is formed by <phrase>deep belief</phrase> networks. Analyses of these models, however, have often been limited to qualitative analyses based on samples due to the computationally intractable <phrase>nature</phrase> of their likelihood. Motivated by these circumstances, the present article introduces a consistent <phrase>estimator</phrase> for the likelihood of <phrase>deep belief</phrase> networks which is computationally tractable and simple to apply in practice. Using this <phrase>estimator</phrase>, we quantitatively investigate a <phrase>deep belief</phrase> network for natural <phrase>image patches</phrase> and compare its performance to the performance of other models for natural <phrase>image patches</phrase>. We find that the <phrase>deep belief</phrase> network is outperformed with respect to the likelihood even by very simple mixture models.
Learning to Rank Acknowledgements I take it as an honour to work with Assist. Prof. Olcay Taner Yldz. He shared his deep <phrase>knowledge</phrase> with me and guided me throughout my <phrase>thesis</phrase>. Thanks a million to him for his help an <phrase>patience</phrase>. Abstract The web has grown so rapidly in the last decade and it brought the need for proper ranking. Learning to rank (LTR) is the collection of <phrase>machine learning</phrase> technologies that construct a ranking <phrase>model</phrase> using <phrase>training data</phrase>. The <phrase>model</phrase> can sort documents according to their degrees of relevance or preference. In this <phrase>thesis</phrase>, we introduce LTR technologies and divide them into three approaches: the point-wise, pair-wise and list-wise. We review the theoritical aspects of each category and introduce the representative <phrase>algorithms</phrase> of them. We also introduce a new LTR method GRwC which uses classification and <phrase>graph</phrase> <phrase>algorithms</phrase>. We reduce the ranking problem to a two class <phrase>classification problem</phrase> and apply KNN <phrase>algorithm</phrase> on a modified LTR dataset. We compared it with the popular ranking <phrase>algorithm</phrase> RankingSVM.
<phrase>Semantic</phrase> Feature <phrase>Mining</phrase> for <phrase>Video</phrase> Event Understanding Content-based <phrase>video</phrase> understanding is extremely difficult due to the <phrase>semantic</phrase> gap between <phrase>low-level</phrase> vision signals and the various <phrase>semantic</phrase> concepts (object, <phrase>action</phrase>, and scene) in videos. Though <phrase>feature extraction</phrase> from videos has achieved significant progress, most of the previous methods rely only on <phrase>low-level</phrase> features, such as the appearance and motion features. Recently, visual-<phrase>feature extraction</phrase> has been improved significantly with <phrase>machine-learning</phrase> <phrase>algorithms</phrase>, especially <phrase>deep learning</phrase>. However, there is still not enough work focusing on extracting <phrase>semantic</phrase> features from videos directly. The goal of this article is to adopt unlabeled videos with the help of text descriptions to learn an embedding <phrase>function</phrase>, which can be used to extract more effective <phrase>semantic</phrase> features from videos when only a few <phrase>labeled samples</phrase> are available for <phrase>video</phrase> recognition. To achieve this goal, we propose a novel embedding <phrase>convolutional neural network</phrase> (ECNN). We evaluate our <phrase>algorithm</phrase> by comparing its performance on three challenging benchmarks with several popular <phrase>state</phrase>-of-the-<phrase>art</phrase> methods. Extensive <phrase>experimental</phrase> <phrase>results</phrase> show that the proposed ECNN consistently and <phrase>significantly outperforms</phrase> the <phrase>existing methods</phrase>.
<phrase>Native</phrase> <phrase>Xml</phrase> Support for Semi-structured Probabilistic <phrase>Data Management</phrase> 1. Abstract / Plan SPOQL (Semi-structured Probabilistic Object <phrase>Query Language</phrase>) is the current method used to extract <phrase>data</phrase> from shredded <phrase>XML</phrase> files from a particular probabilistic <phrase>data</phrase> storage system that our <phrase>professor</phrase>, Alex Dekhtyar, uses. However, the <phrase>database</phrase> system itself could benefit from being upgraded, and we have proposed that the freely-<phrase>downloadable</phrase> <phrase>database management</phrase> system (<phrase>DBMS</phrase>) MonetDB be used in place of the current <phrase>DBMS</phrase>. MonetDB, however, does not support SPOQL in any way. In <phrase>order</phrase> to utilize MonetDB as a <phrase>DBMS</phrase> for Alex's probabilistic <phrase>data</phrase>, SPOQL queries must be translated into <phrase>XQuery</phrase> queries commands that MonetDB can handle. In this course, Team <phrase>Blue</phrase> will spend the majority of their seven to eight weeks of effort learning <phrase>XQuery</phrase> and SPOQL in <phrase>order</phrase> to effectively translate between the two languages. Once these languages have been sufficiently mastered, code will be written that will allow the <phrase>results</phrase> of <phrase>XQuery</phrase> queries to match the <phrase>results</phrase> of SPOQL queries, mirroring the functionality of SPOQL in <phrase>XQuery</phrase>. Once all SPOQL functionality is translated into <phrase>XQuery</phrase>, Team <phrase>Blue</phrase> will populate a MonetDB <phrase>database</phrase> with the probabilistic <phrase>data</phrase> and compare the two languages' functionalities. Team <phrase>Blue</phrase> must translate the functionality embodied by SPOQL into <phrase>XQuery</phrase> statements in <phrase>order</phrase> for the <phrase>DBMS</phrase> to be switched over to MonetDB for <phrase>Professor</phrase> Alex Dekhtyar. This task involves learning SPOQL and <phrase>XQuery</phrase> on a deep level, learning how to interact with MonetDB, and writing code to effectively translate SPOQL queries into <phrase>XQuery</phrase> queries. The project most <phrase>falls</phrase> under the '<phrase>Research</phrase> Project' category of the two project categories discussed in class.
The <phrase>Architecture</phrase> of Why2-Atlas: A <phrase>Coach</phrase> for Qualitative <phrase>Physics</phrase> <phrase>Essay</phrase> Writing The Why2-Atlas system teaches qualitative <phrase>physics</phrase> by having students write paragraph-<phrase>long</phrase> explanations of simple mechanical phenomena. The <phrase>tutor</phrase> uses <phrase>deep syntactic</phrase> analysis and abductive <phrase>theorem proving</phrase> to convert the student's <phrase>essay</phrase> to a proof. The proof formalizes not only what was said, but the likely beliefs behind what was said. This allows the <phrase>tutor</phrase> to uncover misconceptions as well as to detect missing correct parts of the explanation. If the <phrase>tutor</phrase> finds such a flaw in the <phrase>essay</phrase>, it conducts a dialogue intended to remedy the missing or misconceived beliefs, then asks the <phrase>student</phrase> to correct the <phrase>essay</phrase>. It often takes several iterations of <phrase>essay</phrase> correction and dialogue to get the <phrase>student</phrase> to produce an acceptable explanation. <phrase>Pilot</phrase> subjects have been run, and an evaluation is in progress. After explaining the <phrase>research</phrase> questions that the system addresses, the bulk of the <phrase>paper</phrase> describes the system's <phrase>architecture</phrase> and operation. 1 Objectives The Why2 project has three objectives. The first is to build and evaluate qualitative <phrase>physics</phrase> tutors where all <phrase>student</phrase> <phrase>communication</phrase> is via <phrase>natural language</phrase> text. In particular, we will compare their performance to <phrase>text-based</phrase> expository instruction and <phrase>human</phrase> tutoring. <phrase>Text-based</phrase> <phrase>natural language</phrase> (<phrase>NL</phrase>) <phrase>technology</phrase> has improved significantly but is still far from perfect. Are the inevitable disfluencies so confusing that they significantly retard learning, or are they no worse than the disfluencies of <phrase>human</phrase>-to-<phrase>human</phrase> <phrase>text-based</phrase> tutoring? The second objective is to compare several different <phrase>NL</phrase> processing techniques. In particular, we are collaborating with the AutoTutor <phrase>Research</phrase> Group [1, 2] who are building a version of Why2 using <phrase>latent semantic analysis</phrase>, a statistical technique. Our version of Why2 is called Why2-Atlas; their version is called Why2-AutoTutor. Why2-Atlas is based on <phrase>deep syntactic</phrase> analysis and compositional <phrase>semantics</phrase>. It is more difficult to build but may yield better performance. The third objective is to develop authoring tools that facilitate development of <phrase>NL</phrase>-based <phrase>tutoring systems</phrase>. Whereas a typical tutoring system's size is roughly determined by the amount of material it covers, an <phrase>NL</phrase>-based tutor's size is also a <phrase>function</phrase> of the diversity of the words and <phrase>linguistic</phrase> structures that students use.
Multivariate <phrase>Bernoulli Distribution</phrase> Models Multivariate <phrase>Bernoulli Distribution</phrase> Models Acknowledgments First and most importantly, I would like to express my deepest gratitude toward my advisor <phrase>Professor</phrase> Grace Wahba. Her guidance and encouragement through my <phrase>PhD</phrase> study into various <phrase>statistical machine learning</phrase> methods is the key factor to the success of this dissertation. Grace is a brilliant and passionate <phrase>statistician</phrase>, and her insightful ideas in both statistical theories and applications inspire me. It is a great honor and privilege to have the opportunity to work closely and learn from her. This work is also the product of collaboration with a number of researchers. In particular, I would like to thank <phrase>Professor</phrase> Stephen Wright from <phrase>Department</phrase> of <phrase>Computer Science</phrase> for his guidance in computation. Without him, the proposed models in this <phrase>thesis</phrase> would not be solved with efficient optimization techniques. In addition, I am grateful to other professors in my <phrase>thesis</phrase> committee. I benefit from <phrase>Professor</phrase> S undz Keles' expertise in <phrase>biostatistics</phrase> and her valuable ideas in the Thursday group. On the other hand, <phrase>Professor</phrase> Peter Qian and Sijian Wang raised questions with deep <phrase>perception</phrase> and helped greatly improve the <phrase>thesis</phrase>. I am also greatly influenced by <phrase>Professor</phrase> Karl Rohe and Xinwei <phrase>Deng</phrase> for their improving suggestion to this work. <phrase>ii</phrase> I want to thank Xiwen <phrase>Ma</phrase> and <phrase>Shilin</phrase> Ding for their effort on our collaborative projects. The current and past fellows graduate students in the Thursday group helped me in various ways: and <phrase>Chen Zuo</phrase>. The synergies gained from discussions during the Thursday group meetings were beneficial to my <phrase>PhD</phrase> <phrase>research</phrase> study and made my <phrase>life</phrase> at <phrase>Madison</phrase> much more enjoyable. <phrase>Wisconsin</phrase>-<phrase>Madison</phrase>. My solid background is built from receiving rigid training here in both theories and applications of <phrase>statistics</phrase>, which are not only beneficial to my <phrase>research</phrase> but also priceless for my career. What's more, support and understanding from my dear parents Jifang Wang and Guanying Cui, and my girlfriend Xinxin Yu all these years is <phrase>paramount</phrase> for my success. I own them all I have and all I am about to have.
Inference of <phrase>Low-Dimensional</phrase> Latent Structure in <phrase>High</phrase>-Dimensional <phrase>Data</phrase> (EE) Abstract The problem of learning a latent <phrase>model</phrase> for sparse or <phrase>low-dimensional</phrase> representation of <phrase>high</phrase>-dimensional <phrase>data</phrase> has attracted significant attention for many years. This <phrase>thesis</phrase> focuses on learning latent models for sparse or <phrase>low-dimensional</phrase> representation of images, dynamic <phrase>data</phrase>, and documents with <phrase>Bayesian</phrase> nonparametrics. The <phrase>thesis</phrase> consists of three parts. First, nonparametric <phrase>Bayesian</phrase> methods are considered for recovery of imagery based upon compressive measurements. A truncated beta-<phrase>Bernoulli process</phrase> is employed to infer an appropriate <phrase>dictionary</phrase> for the <phrase>test</phrase> <phrase>data</phrase>, and also for image recovery. In the context of compressive sensing, <phrase>significant improvements</phrase> in image recovery are manifested using learned dictionaries, relative to using standard orthonormal image expansions. The compressive-measurement projections are also optimized for the learned <phrase>dictionary</phrase>. Spatial interrelationships within imagery are exploited through use of the <phrase>Dirichlet</phrase> and <phrase>probit</phrase> stick-breaking processes. Several example <phrase>results</phrase> are presented, with comparisons to other <phrase>state</phrase>-of-the-<phrase>art</phrase> methods in the <phrase>literature</phrase>. Second, hierarchical <phrase>Bayesian</phrase> methods are employed to learn a reversible statistical embedding. The proposed embedding procedure is connected to spectral embedding methods (e.g., <phrase>diffusion</phrase> maps and Isomap), yielding a new statistical spectral framework. The <phrase>proposed approach</phrase> allows one to discard the <phrase>training data</phrase> when embedding new <phrase>data</phrase>, allows synthesis of <phrase>high</phrase>-dimensional <phrase>data</phrase> from the embedding space, and provides accurate estimation of the latent-space dimensionality. Hier-iv archical <phrase>Bayesian</phrase> methods are also developed to learn a nonlinear dynamic <phrase>model</phrase> in the <phrase>low-dimensional</phrase> embedding space, allowing joint analysis of multiple types of dynamic <phrase>data</phrase>, sharing strength and inferring interrelationships. In addition to analyzing dynamic <phrase>data</phrase>, the learned <phrase>model</phrase> also yields effective synthesis. Example <phrase>results</phrase> are presented for statistical embedding, latent-space dimensionality estimation , and analysis and synthesis of <phrase>high</phrase>-dimensional (dynamic) <phrase>motion-capture</phrase> <phrase>data</phrase>. Third, a new hierarchical <phrase>tree</phrase>-based topic <phrase>model</phrase> is developed, based on nonpara-metric <phrase>Bayesian</phrase> techniques. The <phrase>model</phrase> has two unique attributes: (i) a child node in the <phrase>tree</phrase> may have more than one parent, with the goal of eliminating redundant sub-topics deep in the <phrase>tree</phrase>; and (<phrase>ii</phrase>) parsimonious sub-topics are manifested, by removing redundant usage of words at multiple scales. The depth and width of the <phrase>tree</phrase> are unbounded within the prior, with a retrospective sampler employed to adap-tively infer the appropriate <phrase>tree</phrase> size based upon the corpus under study. Excellent quantitative <phrase>results</phrase> are manifested on five standard <phrase>data</phrase> sets, and the inferred <phrase>tree structure</phrase> is also found to be highly interpretable.
Teaching CS <phrase>unplugged</phrase> in the <phrase>high school</phrase> (with limited success) CS <phrase>Unplugged</phrase> is a set of <phrase>active learning</phrase> activities designed to introduce fundamental <phrase>computer science</phrase> principles without the use of <phrase>computers</phrase>. The program has gained significant <phrase>momentum</phrase> in recent years, with proponents citing deep engagement and enjoyment benefits. With these benefits in mind, we initiated a one-year outreach program involving a local <phrase>high school</phrase>, using the CS <phrase>Unplugged</phrase> program as the foundation. To our disappointment, the <phrase>results</phrase> were at odds with our enthusiasm --- significantly. In this <phrase>paper</phrase>, we describe our approach to adapting the CS <phrase>Unplugged</phrase> materials for use at the <phrase>high school</phrase> level, present our experiences teaching it, and summarize the <phrase>results</phrase> of our evaluation.
Deep Deformable Part Models The deformable parts <phrase>model</phrase> (DPM) [6] serves as a key component in most modern <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>object detection</phrase> systems. At a <phrase>high</phrase> level, the DPM composes a <phrase>single</phrase> object <phrase>model</phrase> by learning to detect and assemble parts of an object. Most modern systems employing the DPM employ densely computed <phrase>Histogram</phrase> of Oriented Gradients [5] features at training time. Despite the success of HOG features in many applications , we believe that more sophisticated features derived from <phrase>unsupervised feature learning</phrase> processes can boost DPM performance. Recent work in <phrase>unsupervised feature learning</phrase> demonstrates that <phrase>single</phrase> <phrase>neurons</phrase> in a deep network can discriminate between object classes [10]. However, most published work on such <phrase>deep learning</phrase> approaches have succeeded only on simple <phrase>image classification</phrase> tasks (e.g. CIFAR, MNIST). In this project, we explore the empirical performance of the deep deformable parts <phrase>model</phrase> (D-DPM), which uses deep features in the DPM in lieu of HOG features. We evaluate the efficacy of the D-DPM on the PASCAL <phrase>VOC</phrase> 2007 <phrase>object detection</phrase> task. To the best of our <phrase>knowledge</phrase>, no other work has successfully employed deep features on detection tasks involving datasets with as much variation as PASCAL.
An Iterative Approach to Text Segmentation We present divSeg, a novel method for text segmentation that iteratively splits a portion of text at its weakest point in terms of the connectivity strength between two adjacent parts. To search for the weakest point, we apply two different measures: one is based on <phrase>language</phrase> modeling of text segmentation and the other, on the interconnectivity between two segments. Our <phrase>solution</phrase> produces a deep and narrow <phrase>binary tree</phrase> a dynamic object that describes the structure of a text and that is fully adaptable to a user's segmentation needs. We treat it as a separate task to flatten the <phrase>tree</phrase> into a broad and shallow hierarchy either through <phrase>supervised learning</phrase> of a document set or explicit input of how a text should be segmented. The rich structure of our created <phrase>tree</phrase> further allows us to segment documents at varying levels such as topic, sub-topic, etc. We evaluated our new <phrase>solution</phrase> on a set of 265 articles from <phrase>Discover magazine</phrase> where the topic structures are unknown and need to be discovered. Our <phrase>experimental</phrase> <phrase>results</phrase> show that the iterative approach has the potential to generate better segmentation <phrase>results</phrase> than several leading baselines, and the separate flattening step allows us to adapt the <phrase>results</phrase> to different levels of details and user preferences.
Transactional <phrase>Information</phrase> Systems - <phrase>Book</phrase> Review of the <phrase>book</phrase> The <phrase>book</phrase> gives both a <phrase>comprehensive</phrase> overview and an in-depth presentation of the field of transactional <phrase>data processing</phrase> covering the latest findings of the <phrase>research</phrase> <phrase>community</phrase> as well as practical experiences with <phrase>state</phrase>-of-the <phrase>art</phrase> <phrase>algorithms</phrase> and efficient implementation techniques. It reflects the advances of the field, particularly over the last decade. In the formal foundations, the authors emphasize a novel approach that unifies the discussion of <phrase>concurrency control</phrase> and recovery aspects. This is a significant improvement over traditional text books that <phrase>cover</phrase> the latter more on the level of imple-menation details rather than giving a solid formal background , integrated with the body of <phrase>concurrency control</phrase> theory. Also, their approach extends nicely from a standard page-oriented <phrase>model</phrase> to a multi-level, object <phrase>model</phrase> of <phrase>transaction processing</phrase>. The <phrase>book</phrase> develops the necessary formal background and does not require a deep <phrase>database</phrase> background. The authors argue that the scope of transaction <phrase>technology</phrase> is wider than <phrase>classical</phrase> <phrase>database systems</phrase>, aiming at new fields, such as today's <phrase>e-commerce</phrase> or <phrase>communication</phrase> systems. From time to time, the authors dive deeper into formal arguments, but the text always allows to skip details. The reader is further guided through the material by pairs of "Goal and Overview"-"<phrase>Lessons Learned</phrase>" sections as well as illustrative examples, extraordinarily <phrase>comprehensive</phrase> bibliographic notes, and a suite of exercises of varying complexity in each chapter. The two constituents of transactional <phrase>technology</phrase>, con-<phrase>currency</phrase> control and recovery, are presented in the two core Parts <phrase>II</phrase> and III of the <phrase>book</phrase>. Part I starts out with a broad discussion of background and <phrase>motivation</phrase>, ending in the presentation of the two computational models for transactional processing, the page and the object <phrase>model</phrase>, and the definitions of the <phrase>basic</phrase> notions. Also, this part provides the necessary understanding of <phrase>database</phrase> system concepts for those readers not familiar with that material. Part IV is devoted to the coordination of distributed transactions and the last, Part V, gives an outlook on topics not <phrase>covered</phrase> in the <phrase>book</phrase>. The notions and techniques of <phrase>concurrency control</phrase> in Part <phrase>II</phrase> are presented in eight chapters (3-10). Chapter 3 introduces potential problems, the notions of schedules and histories, defines the several correctness criteria in terms of final <phrase>state</phrase>, view, conflict, and commit <phrase>serializability</phrase>, and elaborates on the (inclusion) relationships between the corresponding classes of histories. Next, <phrase>algorithms</phrase> for guaranteeing correct execution of concurrent transactions are given in Chapter 4, among them are the 
Improving <phrase>Cross-Cultural Communication</phrase> Through Collaborative Technologies The <phrase>paper</phrase> discusses an original <phrase>research</phrase> project in the <phrase>area</phrase> of <phrase>education</phrase> and cross-<phrase>cultural</phrase> <phrase>rhetoric</phrase> on the use of persuasive <phrase>digital</phrase> technologies to enable intercultural competencies among students and teachers across globally-distributed teams. The <phrase>paper</phrase> outlines the methodology for the <phrase>research</phrase>, including the use of <phrase>video</phrase> conferences, collaborative <phrase>blogs</phrase>, a project <phrase>wiki</phrase>, webforums, and <phrase>Google</phrase> documents, and presents the findings on how such <phrase>information</phrase> and <phrase>communication</phrase> technologies can influence people to approach <phrase>cross-cultural communication</phrase> with greater <phrase>political</phrase> understanding, <phrase>ethical</phrase> awareness, and intercultural competencies in <phrase>order</phrase> to bring about improved international and social relations. The <phrase>paper</phrase> presents statistical <phrase>data</phrase> pertaining to qualitative and quantitative assessment of project outcomes; it situates the project within current debates in <phrase>intercultural communication</phrase> and <phrase>digital</phrase> <phrase>pedagogy</phrase>; and it concludes with a projection on the <phrase>scalability</phrase> and <phrase>sustainability</phrase> of using <phrase>computers</phrase> to change <phrase>human</phrase> attitudes and behaviors in positive ways in an international context. 1 Introduction Through a Wallenberg Global Learning Network (WGLN) grant, our project aims to contribute new learning in the fields of <phrase>education</phrase> and cross-<phrase>cultural</phrase> <phrase>rhetoric</phrase> through application of persuasive <phrase>digital</phrase> technologies as the mode and apparatus for changing attitudes about cultures and for empowering users to develop intercultural competencies as a means for improving <phrase>international relations</phrase>, social relations, <phrase>political</phrase> understanding, and trust in educational and <phrase>cultural</phrase> exchanges. In this <phrase>paper</phrase>, we offer an international perspective on the use of persuasive <phrase>technology</phrase> in creating what in the <phrase>literature</phrase> is termed " intercultural competencies " among students and teachers across globally-distributed teams. The past two decades have witnessed an explosion of interest in <phrase>globalization</phrase>, transnational studies, and <phrase>cultural</phrase> codes of <phrase>communication</phrase> and the concurrent scholarly attention to developing better methods of implementing technological tools in educational settings. Yet, a key problem remains: how best to use <phrase>information</phrase> and <phrase>communication</phrase> technologies (or ICTs) to offer students hands-on learning of transnational and intercultural differences. To address this problem, our WGLN project " Developing Intercultural Competencies through Collaborative <phrase>Rhetoric</phrase> " experimented with innovative uses of <phrase>technology</phrase> by bringing together students at <phrase>Stanford</phrase> and rebro <phrase>Universities</phrase> in globally-distributed teams to analyze <phrase>rhetorical</phrase> artifacts (speeches, advertisements, <phrase>architectural</phrase> landmarks, representations of nationhood) with the aim of facilitating both practical and <phrase>deep learning</phrase> of effective <phrase>cross-cultural communication</phrase> skills and transnational <phrase>cultural</phrase> understanding.
<phrase>Digital</phrase> Techniques for <phrase>Etruscan</phrase> Graves: the Etruscanning Project Etruscanning is a project founded by the <phrase>European Commission</phrase> and it focuses on the investigation of new digitization and presentation techniques, in <phrase>order</phrase> to recreate the original context of the <phrase>Etruscan</phrase> graves. Several <phrase>digital</phrase> techniques have been applied for the stages of digitization, virtual <phrase>restoration</phrase> and <phrase>reconstruction</phrase> and <phrase>communication</phrase>. The possibility of working on two different tombs allows us to deep two specific approaches and to diversify the final real-time applications. This project represents an interesting opportunity to create a <phrase>concrete</phrase> link between <phrase>research</phrase> and <phrase>communication</phrase> in the field of virtual <phrase>museums</phrase>, testing the effective impact in terms of <phrase>cultural</phrase> transmission, learning and appreciation both in non-linear <phrase>narrative</phrase> plots conception and in novel <phrase>metaphors</phrase> of interaction. From a technological point of view the most innovative result of the project is the implementation of natural interaction interfaces, allowing the <phrase>public</phrase> to move and interact with objects inside the <phrase>virtual environment</phrase>.
Automatically learning gazetteers from the <phrase>deep web</phrase> <phrase>Wrapper induction</phrase> faces a dilemma: To reach web scale, it requires automatically generated examples, but to produce accurate <phrase>results</phrase>, these examples must have the quality of <phrase>human</phrase> annotations. We resolve this conflict with <phrase>AMBER</phrase>, a system for fully automated <phrase>data</phrase> extraction from result pages. In contrast to previous approaches, <phrase>AMBER</phrase> employs <phrase>domain specific</phrase> gazetteers to discern <phrase>basic</phrase> domain attributes on a page, and leverages repeated occurrences of similar attributes to group related attributes into records rather than relying on the noisy structure of the DOM. With this approach <phrase>AMBER</phrase> is able to identify records and their attributes with almost perfect accuracy (&gt;98%) on a large sample of <phrase>websites</phrase>. To make such an approach feasible at scale, <phrase>AMBER</phrase> automatically learns domain gazetteers from a small <phrase>seed</phrase> set. In this demonstration, we show how <phrase>AMBER</phrase> uses the repeated structure of records on <phrase>deep web</phrase> result pages to learn such gazetteers. This is only possible with a highly accurate extraction system. Depending on its <phrase>parametrization</phrase>, this learning process <phrase>runs</phrase> either fully automatically or with <phrase>human</phrase> interaction. We show how <phrase>AMBER</phrase> bootstraps a gazetteer for <phrase>UK</phrase> locations in 4 iterations: From a small <phrase>seed</phrase> sample we achieve 94.4% accuracy in recognizing <phrase>UK</phrase> locations in the $4th$ iteration.
Selecting <phrase>Receptive Fields</phrase> in Deep Networks Recent <phrase>deep learning</phrase> and <phrase>unsupervised feature learning</phrase> systems that learn from <phrase>unlabeled data</phrase> have achieved <phrase>high</phrase> performance in benchmarks by using extremely large architectures with many features (<phrase>hidden units</phrase>) at each layer. Unfortunately, for such large architectures the number of parameters can grow quadratically in the width of the network, thus necessitating hand-coded " <phrase>local receptive</phrase> fields " that limit the number of connections from <phrase>lower</phrase> <phrase>level features</phrase> to higher ones (e.g., based on spatial <phrase>locality</phrase>). In this <phrase>paper</phrase> we propose a fast method to choose these connections that may be incorporated into a wide <phrase>variety</phrase> of unsupervised training methods. Specifically, we choose <phrase>local receptive</phrase> fields that group together those <phrase>low-level</phrase> features that are most similar to each other according to a pairwise similarity metric. This approach allows us to <phrase>harness</phrase> the advantages of <phrase>local receptive</phrase> fields (such as improved <phrase>scalability</phrase>, and reduced <phrase>data</phrase> requirements) when we do not know how to specify such <phrase>receptive fields</phrase> by hand or where our unsupervised training <phrase>algorithm</phrase> has no obvious generalization to a topographic setting. We produce <phrase>results</phrase> showing how this method allows us to use even simple unsuper-vised training <phrase>algorithms</phrase> to <phrase>train</phrase> successful multi-layered networks that achieve <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> on CIFAR and <phrase>STL</phrase> datasets: 82.0% and 60.1% accuracy, respectively.
The Segmentation of the <phrase>Left Ventricle</phrase> of the <phrase>Heart</phrase> From <phrase>Ultrasound</phrase> <phrase>Data</phrase> Using <phrase>Deep Learning</phrase> Architectures and <phrase>Derivative</phrase>-Based Search Methods We present a new <phrase>supervised learning</phrase> <phrase>model</phrase> designed for the automatic segmentation of the <phrase>left ventricle</phrase> (LV) of the <phrase>heart</phrase> in <phrase>ultrasound</phrase> images. We address the following problems inherent to <phrase>supervised learning</phrase> models: 1) the need of a large set of training images; 2) robustness to imaging conditions not present in the <phrase>training data</phrase>; and 3) complex search process. The innovations of our approach reside in a formulation that decouples the rigid and nonrigid detections, <phrase>deep learning</phrase> methods that <phrase>model</phrase> the appearance of the LV, and efficient <phrase>derivative</phrase>-based search <phrase>algorithms</phrase>. The functionality of our approach is evaluated using a <phrase>data set</phrase> of diseased cases containing 400 annotated images (from 12 sequences) and another <phrase>data set</phrase> of normal cases comprising 80 annotated images (from two sequences), where both sets present <phrase>long</phrase> <phrase>axis</phrase> views of the LV. Using several error measures to compute the <phrase>degree</phrase> of similarity between the manual and automatic segmentations, we show that our method not only has <phrase>high</phrase> sensitivity and specificity but also presents variations with respect to a <phrase>gold standard</phrase> (computed from the manual annotations of two experts) within interuser variability on a <phrase>subset</phrase> of the diseased cases. We also compare the segmentations <phrase>produced</phrase> by our approach and by two <phrase>state</phrase>-of-the-<phrase>art</phrase> LV segmentation models on the <phrase>data set</phrase> of normal cases, and the <phrase>results</phrase> show that our approach produces segmentations that are comparable to these two approaches using only 20 training images and increasing the <phrase>training set</phrase> to 400 images causes our approach to be generally more accurate. Finally, we show that efficient search methods reduce up to tenfold the complexity of the method while still producing competitive segmentations. In the future, we plan to include a dynamical <phrase>model</phrase> to improve the performance of the <phrase>algorithm</phrase>, to use semisupervised learning methods to reduce even more the dependence on rich and large <phrase>training sets</phrase>, and to <phrase>design</phrase> a shape <phrase>model</phrase> less dependent on the <phrase>training set</phrase>.
<phrase>Pastel</phrase>: Pattern-driven Adaptive Simulations 2 Radical Adaptation We propose a new kind of <phrase>learning environment</phrase> called an adaptive <phrase>simulation</phrase> that more deeply explores and exploits the potential of simulations as pedagogical and explanatory tools. In an adaptive <phrase>simulation</phrase>, the <phrase>simulation</phrase> configuration is not fixed but rather can be modified by an instructional agent for optimal pedagogical effect. Types of adaptations include manipulations of <phrase>simulation</phrase> time and <phrase>state</phrase>, changes in representation to facilitate explanations and/or task performance, and adjustments in <phrase>simulation</phrase> complexity by the addition and/or removal of components. We briefly describe a system we are developing called <phrase>PASTEL</phrase> that is designed to enable these kinds of adaptations. Open <phrase>research</phrase> issues include precisely how to perform these adaptations and when to employ them for optimal effect. There is a <phrase>long</phrase> <phrase>history</phrase> of interest in simulations as <phrase>learning environments</phrase>, primarily because of their ability to support conceptual exploration and learning-by-doing in authentic, <phrase>risk</phrase>-<phrase>free</phrase>, immersive settings. In the face of mounting global challenges requiring deep insight into the complexities of economic, biological, and social systems, interest in simulations as exploratory conceptual tools only appears to be growing [1]. Although widely regarded as having great promise, simulations can have a number of associated learning problems, primarily having to do with learner understanding, <phrase>management</phrase> of system complexity, and the lack of built-in instructional support [2]. In our work, we are striving for a tight linkage between the <phrase>design</phrase> of simulations and the <phrase>design</phrase> of associated instructional components. Specifically, we are considering what value may result if the <phrase>simulation</phrase> itself is put under much more direct and masterful control of an instructional agent. In developing our vision of adaptive simulations that can reconfigure themselves radically in service of instructional goals, we are investigating three broad categories of adaptation:
The nested <phrase>chinese</phrase> <phrase>restaurant</phrase> process and <phrase>bayesian</phrase> nonparametric inference of topic hierarchies We present the nested <phrase>Chinese</phrase> <phrase>restaurant</phrase> process (nCRP), a <phrase>stochastic process</phrase> that assigns <phrase>probability distributions</phrase> to ensembles of infinitely deep, infinitely branching <phrase>trees</phrase>. We show how this <phrase>stochastic process</phrase> can be used as a prior distribution in a <phrase>Bayesian</phrase> nonparametric <phrase>model</phrase> of document collections. Specifically, we present an application to <phrase>information retrieval</phrase> in which documents are modeled as paths down a random <phrase>tree</phrase>, and the <phrase>preferential attachment</phrase> dynamics of the nCRP leads to clustering of documents according to sharing of topics at <phrase>multiple levels</phrase> of abstraction. Given a corpus of documents, a posterior inference <phrase>algorithm</phrase> finds an approximation to a posterior distribution over <phrase>trees</phrase>, topics and allocations of words to levels of the <phrase>tree</phrase>. We demonstrate this <phrase>algorithm</phrase> on collections of scientific abstracts from several <phrase>journals</phrase>. This <phrase>model</phrase> exemplifies a recent trend in <phrase>statistical machine learning</phrase>&#8212;the use of <phrase>Bayesian</phrase> nonparametric methods to infer <phrase>distributions</phrase> on flexible <phrase>data structures</phrase>.
Learning from Organizational Experience Learning from Organizational Experience Learning-inaction , the cyclical interplay of thinking and doing, is increasingly important for organizations as environments and required capabilities become more complex and interdependent. <phrase>Organizational learning</phrase> involves both a desire to learn and supportive structures and mechanisms. We draw upon three <phrase>case studies</phrase> from the <phrase>nuclear power</phrase> and chemical industries to illustrate a four-stage <phrase>model</phrase> of <phrase>organizational learning</phrase>: (1) local stage of decentralized learning by individuals and work groups, (2) control stage of fixing problems and complying with rules, (3) open stage of acknowledgement of doubt and <phrase>motivation</phrase> to learn, and (4) <phrase>deep learning</phrase> stage of skillful inquiry and systemic mental
Searching for <phrase>Memory</phrase> In Search of <phrase>Memory</phrase>: The Emergence of a New <phrase>Science</phrase> of Mind. Eric R. <phrase>Kandel</phrase>. WW Norton (2006) 352 pp., $29.95 <phrase>hardcover</phrase>. You don't have to understand <phrase>Italian</phrase> or <phrase>French</phrase> to enjoy an <phrase>opera</phrase>. You can simply let the rich textures of the melodies , the pageantry of the <phrase>production</phrase>, and the passion of the <phrase>composer</phrase> carry you along on a <phrase>musical</phrase> journey. Likewise, you don't have to understand <phrase>biology</phrase> to enjoy this remarkable <phrase>book</phrase> by the preeminent <phrase>neuroscientist</phrase> <phrase>Eric Kandel</phrase>, In Search of <phrase>Memory</phrase>. You can simply allow yourself to be carried along during the unfolding of his <phrase>life</phrase> story, a multicolored <phrase>tapestry</phrase> of experiences, which began in <phrase>Vienna</phrase> as a young <phrase>Jewish</phrase> boy driven from his home by <phrase>Nazi</phrase> policemen, and which culminated, many years later, in <phrase>Stockholm</phrase>, where he received the <phrase>Nobel Prize</phrase> for his work on the biological basis of <phrase>memory</phrase>. Now, of course, if you happen to understand <phrase>Italian</phrase> or <phrase>French</phrase>, it can greatly add to your enjoyment of an <phrase>opera</phrase>, since you can grasp the story that the <phrase>composer</phrase> was actually telling through the <phrase>lyrics</phrase>. And if you have some background in <phrase>biology</phrase>, or even better, <phrase>neuroscience</phrase>, then this <phrase>book</phrase> takes on added value as a marvelous tutorial in the <phrase>history</phrase> and significance of many of the fundamental scientific discoveries that serve as the pillars of modern <phrase>neuroscience</phrase>. Either way, <phrase>Kandel</phrase> allows his readers to share, often in remarkably intimate detail, the events that profoundly shaped his personal and scientific development. He is a man of many <phrase>passions</phrase>. The three that emerge and are intertwined throughout this <phrase>book</phrase> are his deep appreciation of his <phrase>Jewish</phrase> heritage, his <phrase>love</phrase> of <phrase>history</phrase>, and his passion for <phrase>science</phrase>. These three themes constantly recur as we join him on his journey In Search Of <phrase>Memory</phrase>. A Shiny <phrase>Blue</phrase> <phrase>Car</phrase> The <phrase>book</phrase> opens with a riveting <phrase>history</phrase> of <phrase>Vienna</phrase> during Hitler's rise to power during the 1930s. <phrase>Kandel</phrase> was a young <phrase>Jewish</phrase> boy there when the <phrase>city</phrase> became, in a very <phrase>short</phrase> <phrase>period</phrase> of time, filled with hatred for him and all other <phrase>Jews</phrase>. That hatred is made palpable in the most compelling pages of the entire <phrase>book</phrase>, as we learn of Kandel's delight in receiving a shiny <phrase>blue</phrase> <phrase>toy</phrase> <phrase>car</phrase> on November 7, 1938, his ninth birthday. He played with this <phrase>battery</phrase>-operated <phrase>car</phrase> for hours on end. But two days later, on the evening on November 9, his world was changed forever. As he was to learn later, this was the evening of Kristallnachtliterally, ''the Night of Broken <phrase>Glass</phrase>''so named 
Deep Mixtures of Factor Analysers An efficient way to learn deep <phrase>density</phrase> models that have many layers of <phrase>latent variables</phrase> is to learn one layer at a time using a <phrase>model</phrase> that has only one layer of <phrase>latent variables</phrase>. After learning each layer, samples from the posterior <phrase>distributions</phrase> for that layer are used as <phrase>training data</phrase> for learning the next layer. This approach is commonly used with <phrase>Restricted Boltzmann Machines</phrase>, which are undirected <phrase>graphical</phrase> models with a <phrase>single</phrase> <phrase>hidden layer</phrase>, but it can also be used with Mixtures of Factor Analysers (MFAs) which are <phrase>directed</phrase> <phrase>graphical</phrase> models. In this <phrase>paper</phrase>, we present a <phrase>greedy layer-wise</phrase> <phrase>learning algorithm</phrase> for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels, learning and inference are much more efficient in a DMFA and the sharing of each <phrase>lower</phrase>-level factor loading matrix by many different <phrase>higher level</phrase> MFAs prevents overfit-ting. We demonstrate empirically that DM-FAs learn better <phrase>density</phrase> models than both MFAs and two types of <phrase>Restricted Boltzmann Machine</phrase> on a wide <phrase>variety</phrase> of datasets.
Learning Sparse, Distributed Representations using the Hebbian Principle The " fire together, <phrase>wire</phrase> together " Hebbian <phrase>model</phrase> is a central principle for learning in <phrase>neuroscience</phrase>, but surprisingly, it has found limited applicability in modern <phrase>machine learning</phrase>. In this <phrase>paper</phrase>, we take a first step towards bridging this gap, by developing flavors of competitive Hebbian learning which produce sparse, distributed neural codes using online adaptation with minimal tuning. We propose an unsupervised <phrase>algorithm</phrase>, termed Adaptive Hebbian Learning (<phrase>AHL</phrase>). We illustrate the distributed <phrase>nature</phrase> of the learned representations via output <phrase>entropy</phrase> computations for synthetic <phrase>data</phrase>, and demonstrate <phrase>superior</phrase> performance, compared to standard alternatives such as autoencoders, in training a <phrase>deep convolutional</phrase> net on standard image datasets.
Learning Runtime Parameters in Computer Systems with Delayed Experience Injection Learning effective configurations in computer systems without hand-crafting models for every parameter is a <phrase>long</phrase>-standing problem. This <phrase>paper</phrase> investigates the use of <phrase>deep reinforcement learning</phrase> for runtime parameters of <phrase>cloud</phrase> <phrase>databases</phrase> under latency constraints. <phrase>Cloud</phrase> services serve up to thousands of concurrent requests per second and can adjust critical parameters by leveraging performance metrics. In this work, we use continuous <phrase>deep reinforcement learning</phrase> to learn optimal cache expirations for HTTP caching in <phrase>content delivery</phrase> networks. To this end, we introduce a technique for asynchronous experience <phrase>management</phrase> called delayed experience injection, which facilitates delayed reward and next-<phrase>state</phrase> computation in concurrent environments where measurements are not immediately available. Evaluation <phrase>results</phrase> show that our approach based on normalized advantage functions and asynchronous <phrase>CPU</phrase>-only training outperforms a statistical <phrase>estimator</phrase>.
<phrase>Speech Synthesis</phrase> for Portable Devices A Text-To-Speech (<phrase>TTS</phrase>) <phrase>synthesiser</phrase> is a computer-based system that should be able to read any text aloud, whether it was directly introduced in the computer by an operator or scanned and submitted to an <phrase>Optical Character Recognition</phrase> (<phrase>OCR</phrase>) system. This project presents the <phrase>design</phrase> and implementation of a speech <phrase>synthesiser</phrase> for portable devices. The <phrase>TTS</phrase> implementation was developed for J2ME. This development involved the conversion of a desktop <phrase>TTS</phrase> program by <phrase>Sun Microsystems</phrase> Inc. called FreeTTS. The implementation also involved the partial development of <phrase>Java</phrase> <phrase>Sound</phrase> API's for J2ME on <phrase>Windows CE</phrase>. The <phrase>TTS</phrase> implementation on a <phrase>PDA</phrase>, apart from being platform <phrase>independent</phrase> produces the same <phrase>sound</phrase> quality with a far less powerful processor than its desktop counterparts. The implementation is written for J2ME, and is also compatible with JDK 1.3 on desktop <phrase>computers</phrase>. The program has been tested on a <phrase>Dell</phrase> <phrase>Axim</phrase> <phrase>X5</phrase> <phrase>PDA</phrase> running <phrase>Windows CE</phrase>, and has been tested on a wide <phrase>range</phrase> of desktop <phrase>computers</phrase> and <phrase>operating systems</phrase> including: <phrase>Windows 2000</phrase>, <phrase>Windows XP</phrase>, <phrase>FreeBSD</phrase> and <phrase>Linux</phrase>. i Acknowledgements " ...very little do we have and enclose which we can call our own in the deep sense of the word. We all have to accept and learn, either from our predecessors or from our contemporaries. Even the greatest genius would not have achieved much if he had wished to extract everything from inside himself. But there are many good people, who do not understand this, and spend half their lives wondering in darkness with their dreams of originality. I have known artists who were proud of not having followed any <phrase>teacher</phrase> and of owing everything only to their own genius. Such fools! " [<phrase>Goethe</phrase>, Conversations with Eckermann, 17 Feb 1832] First of all, I would like to thank all of the staff in DIT Kevin Street, in particular Dr. Fred Mtenzi, the project supervisor for all the guidance he provided throughout this work. I appreciate the input of the <phrase>computer science</phrase> classes, FT228, FT225 and DT226 for their continued discussion, support and assisting with <phrase>software testing</phrase>. Thanks to NSIcom who issued a license for the use of their J2ME <phrase>virtual machine</phrase> in this project, and also for answering questions relating to the hardware and processing limitations of portable devices. Great thanks are expressed to both <phrase>Donald Knuth</phrase> [31], the <phrase>author</phrase> of T <phrase>E</phrase> X, and also to <phrase>Leslie Lamport</phrase> [34] for writing L A T <phrase>E</phrase> X, 
Clustering Learning for Robotic Vision We present the clustering learning technique applied to <phrase>multi-layer</phrase> feedforward <phrase>deep neural networks</phrase>. We show that this <phrase>unsupervised learning</phrase> technique can compute network filters with only a few minutes and a much reduced set of parameters. The goal of this <phrase>paper</phrase> is to promote the technique for <phrase>general</phrase>-purpose robotic vision systems. We <phrase>report</phrase> its use in static image datasets and <phrase>object tracking</phrase> datasets. We show that <phrase>networks trained</phrase> with clustering learning can outper-form large <phrase>networks trained</phrase> for many hours on complex datasets.
<phrase>Deep architectures</phrase> for <phrase>protein</phrase> contact map prediction <phrase>MOTIVATION</phrase> Residue-residue contact prediction is important for <phrase>protein structure prediction</phrase> and other applications. However, the accuracy of current contact predictors often barely exceeds 20% on <phrase>long</phrase>-<phrase>range</phrase> contacts, falling <phrase>short</phrase> of the level required for <phrase>ab initio</phrase> structure prediction. <phrase>RESULTS</phrase> Here, we develop a novel <phrase>machine learning</phrase> approach for contact map prediction using three steps of increasing resolution. First, we use 2D recursive <phrase>neural networks</phrase> to predict coarse contacts and orientations between <phrase>secondary</phrase> structure elements. Second, we use an <phrase>energy</phrase>-based method to align <phrase>secondary</phrase> structure elements and predict contact <phrase>probabilities</phrase> between <phrase>residues</phrase> in contacting <phrase>alpha-helices</phrase> or strands. Third, we use a <phrase>deep neural network</phrase> <phrase>architecture</phrase> to organize and progressively refine the prediction of contacts, integrating <phrase>information</phrase> over both space and time. We <phrase>train</phrase> the <phrase>architecture</phrase> on a large set of non-redundant <phrase>proteins</phrase> and <phrase>test</phrase> it on a large set of non-homologous domains, as well as on the set of <phrase>protein domains</phrase> used for contact prediction in the two most recent CASP8 and CASP9 experiments. For <phrase>long</phrase>-<phrase>range</phrase> contacts, the accuracy of the new CMAPpro predictor is close to 30%, a significant increase over existing approaches. AVAILABILITY CMAPpro is available as part of the SCRATCH suite at http://scratch.proteomics.ics.uci.edu/. CONTACT pfbaldi@uci.edu SUPPLEMENTARY <phrase>INFORMATION</phrase> Supplementary <phrase>data</phrase> are available at <phrase>Bioinformatics</phrase> online.
The <phrase>Fusion</phrase> of <phrase>Deep Learning</phrase> Architectures and <phrase>Particle</phrase> Filtering Applied to Lip Tracking This work introduces a new <phrase>pattern recognition</phrase> <phrase>model</phrase> for segmenting and tracking lip contours in <phrase>video</phrase> sequences. We formulate the problem as a <phrase>general</phrase> non-rigid <phrase>object tracking</phrase> method, where the computation of the expected segmentation is based on a filtering distribution. This is a difficult task because one has to compute the expected value using the whole <phrase>parameter space</phrase> of segmentation. As a result, we compute the expected segmentation using sequential <phrase>Monte Carlo</phrase> sampling methods, where the filtering distribution is approximated with a proposal distribution to be used for sampling. The key contribution of this <phrase>paper</phrase> is the formulation of this proposal distribution using a new observation <phrase>model</phrase> based on <phrase>deep belief</phrase> networks and a new transition <phrase>model</phrase>. The efficacy of the <phrase>model</phrase> is demonstrated in publicly available <phrase>databases</phrase> of <phrase>video</phrase> sequences of people talking and <phrase>singing</phrase>. Our method produces <phrase>results</phrase> comparable to <phrase>state</phrase>-of-the-<phrase>art</phrase> models, but showing potential to be more robust to imaging conditions .
<phrase>Feature Learning</phrase> in Dynamic Environments: Modeling the <phrase>Acoustic</phrase> Structure of <phrase>Musical</phrase> <phrase>Emotion</phrase> While <phrase>emotion</phrase>-based <phrase>music</phrase> <phrase>organization</phrase> is a natural process for humans, quantifying it empirically proves to be a very difficult task, and as such no dominant <phrase>feature representation</phrase> for <phrase>music</phrase> <phrase>emotion</phrase> recognition has yet emerged. Much of the difficulty in developing <phrase>emotion</phrase>-based features is the ambiguity of the <phrase>ground-truth</phrase>. Even using the smallest time window, opinions about <phrase>emotion</phrase> are bound to vary and reflect some disagreement between listeners. In previous work, we have modeled <phrase>human</phrase> response <phrase>labels</phrase> to <phrase>music</phrase> in the <phrase>arousal</phrase>-valence (A-V) <phrase>emotion</phrase> space with time-varying <phrase>stochastic</phrase> <phrase>distributions</phrase>. Current methods for automatic detection of <phrase>emotion</phrase> in <phrase>music</phrase> seek performance increases by combining several feature domains (e.g. <phrase>loudness</phrase>, <phrase>timbre</phrase>, <phrase>harmony</phrase>, <phrase>rhythm</phrase>). Such work has focused largely in <phrase>dimensionality reduction</phrase> for minor classification performance gains, but has provided little insight into the relationship between audio and emotional associations. In this work, we seek to employ <phrase>regression</phrase>-based <phrase>deep belief</phrase> networks to learn features directly from <phrase>magnitude</phrase> spectra. Taking into account the dynamic <phrase>nature</phrase> of <phrase>music</phrase>, we investigate combining multiple timescales of ag-gregated <phrase>magnitude</phrase> spectra as a basis for <phrase>feature learning</phrase>.
Localizing transient faults using dynamic <phrase>bayesian</phrase> networks Transient faults are a <phrase>major</phrase> concern in today's deep sub-<phrase>micron</phrase> <phrase>semiconductor</phrase> <phrase>technology</phrase>. These faults are rare but they have been known to cause catastrophic system-level failures. Transient errors often occur due to physical effects on deployed systems and hence, diagnosis of transient errors must be performed over manufactured chips or systems assembled from <phrase>black</phrase>-box components where arbitrary instrumentation of the system is not possible and hence, the system <phrase>state</phrase> is only partially <phrase>observable</phrase>. Further, these systems are often composed of components that are third <phrase>party</phrase> IP which further adds opaque-ness to the system. In this <phrase>paper</phrase>, we propose a probabilistic approach to localize transient faults in space and time for such partially <phrase>observable</phrase> systems. From a set of correct traces and a failure trace, we seek to locate the faulty component and the cycle of operation at which the fault occurred. Our technique uses correct system traces over monitored components of the system to learn a dynamic <phrase>Bayesian network</phrase> (DBN) summarizing the temporal dependencies across the monitored components. This DBN is augmented with different error hypotheses allowed by the fault <phrase>model</phrase>. The most probable explanation (MPE) among these hypotheses corresponds to the most likely location of the error. We evaluated the effectiveness of our technique on a set of ISCAS89 benchmarks and a <phrase>router</phrase> <phrase>design</phrase> used in on-chip networks in a <phrase>multi-core</phrase> <phrase>design</phrase>.
Implicit <phrase>Density Estimation</phrase> by Local Moment Matching to Sample from <phrase>Auto-Encoders</phrase> Recent work suggests that some <phrase>auto-encoder</phrase> variants do a good job of capturing the local <phrase>manifold</phrase> structure of the unknown <phrase>data</phrase> generating <phrase>density</phrase>. This <phrase>paper</phrase> contributes to the <phrase>mathematical</phrase> understanding of this phenomenon and helps define better justified sampling <phrase>algorithms</phrase> for <phrase>deep learning</phrase> based on <phrase>auto-encoder</phrase> variants. We consider an MCMC where each step samples from a Gaussian whose mean and <phrase>covariance matrix</phrase> depend on the previous <phrase>state</phrase>, defines through its asymptotic distribution a <phrase>target</phrase> <phrase>density</phrase>. First, we show that good choices (in the sense of consistency) for these mean and <phrase>covariance</phrase> functions are the local expected value and local <phrase>covariance</phrase> under that <phrase>target</phrase> <phrase>density</phrase>. Then we show that an <phrase>auto-encoder</phrase> with a contractive penalty captures estimators of these local moments in its <phrase>reconstruction</phrase> <phrase>function</phrase> and its <phrase>Jacobian</phrase>. A contribution of this work is thus a novel <phrase>alternative</phrase> to <phrase>maximum-likelihood</phrase> <phrase>density estimation</phrase>, which we call local moment matching. It also justifies a <phrase>recently proposed</phrase> sampling <phrase>algorithm</phrase> for the Contractive <phrase>Auto-Encoder</phrase> and extends it to the Denoising <phrase>Auto-Encoder</phrase>.
Learning Deep Representation Without Parameter Inference for Nonlinear <phrase>Dimensionality Reduction</phrase> Unsupervised <phrase>deep learning</phrase> is one of the most powerful representation learning techniques. Restricted Boltzman machine, <phrase>sparse coding</phrase>, regularized <phrase>auto-encoders</phrase>, and <phrase>convolutional neural networks</phrase> are pioneering <phrase>building blocks</phrase> of <phrase>deep learning</phrase>. In this <phrase>paper</phrase>, we propose a new <phrase>building block</phrase> -- distributed random models. The <phrase>proposed method</phrase> is a special full implementation of the product of experts: (i) each expert owns multiple <phrase>hidden units</phrase> and different experts have different numbers of <phrase>hidden units</phrase>; (<phrase>ii</phrase>) the <phrase>model</phrase> of each expert is a k-<phrase>center</phrase> clustering, whose k-centers are only uniformly sampled examples, and whose output (i.e. the <phrase>hidden units</phrase>) is a sparse code that only the similarity values from a few nearest neighbors are reserved. The relationship between the pioneering <phrase>building blocks</phrase>, several notable <phrase>research</phrase> branches and the <phrase>proposed method</phrase> is analyzed. <phrase>Experimental</phrase> <phrase>results</phrase> show that the proposed deep <phrase>model</phrase> can learn better representations than <phrase>deep belief</phrase> networks and meanwhile can <phrase>train</phrase> a much larger network with much less time than <phrase>deep belief</phrase> networks.
Evaluating the Contribution of Top-Down <phrase>Feedback</phrase> and Post-Learning <phrase>Reconstruction</phrase> <phrase>Deep generative models</phrase> and their associated top-down <phrase>architecture</phrase> are gaining popularity in <phrase>neuroscience</phrase> and <phrase>computer vision</phrase>. In this <phrase>paper</phrase> we link our previous work with regulatory <phrase>feedback</phrase> networks to <phrase>generative models</phrase>. We show that generative model's and regulatory <phrase>feedback</phrase> model's equations can share the same fixed points. Thus, phenomena observed using regulatory <phrase>feedback</phrase> can also apply to <phrase>generative models</phrase>. This suggests that <phrase>generative models</phrase> can also be developed to identify mixtures of patterns, address problems associated with binding, and display the ability to estimate numerosity.
Neural Decision <phrase>Forests</phrase> for <phrase>Semantic</phrase> Image Labelling In this work we present Neural Decision <phrase>Forests</phrase>, a novel approach to jointly tackle <phrase>data</phrase> representation-and dis-criminative learning within <phrase>randomized</phrase> <phrase>decision trees</phrase>. <phrase>Recent advances</phrase> of <phrase>deep learning</phrase> architectures demonstrate the power of embedding representation learning within the classifier An idea that is intuitively supported by the hierarchical <phrase>nature</phrase> of the decision <phrase>forest</phrase> <phrase>model</phrase> where the input space is typically left unchanged during training and testing. We <phrase>bridge</phrase> this gap by introducing <phrase>randomized</phrase> <phrase>Multi-Layer</phrase> Perceptrons (rMLP) as new <phrase>split</phrase> nodes which are capable of learning non-linear, <phrase>data</phrase>-specific representations and taking advantage of them by finding optimal predictions for the emerging child nodes. To prevent <phrase>overfitting</phrase>, we i) randomly select the image <phrase>data</phrase> fed to the input layer, <phrase>ii</phrase>) automatically adapt the rMLP <phrase>topology</phrase> to meet the complexity of the <phrase>data</phrase> arriving at the node and iii) introduce an 1-norm based regularization that additionally sparsifies the network. The key findings in our experiments on three different <phrase>semantic</phrase> image labelling datasets are consistently improved <phrase>results</phrase> and significantly compressed <phrase>trees</phrase> compared to conventional classification <phrase>trees</phrase>.
Analysis, <phrase>design</phrase>, and optimization of cellular <phrase>neural networks</phrase> Acknowledglllents I wish to <phrase>express my deep</phrase> gratitude to my <phrase>thesis</phrase> advisor Prof. George S. Moschytz for providing an excellent <phrase>research</phrase> environment at the Signal and <phrase>Information Processing</phrase> <phrase>Laboratory</phrase> (<phrase>ISI</phrase>) and for the precious freedom he gave me to tryout my own ideas. His engaged interest and hours of <phrase>proofreading</phrase> substantially improved my publications and this <phrase>thesis</phrase>. Also, I am greatly indebted to him for the many opportunities I had in the course of my <phrase>thesis</phrase> to present my work at various occasions. I would like to thank Prof. Leon O. Chua for his stimulating lectures at <phrase>ISI</phrase> in the summers 1996 and 1998, for his immense enthusiasm for the cellular <phrase>neural network</phrase>, and for serving as co-examiner of this <phrase>thesis</phrase>. I also wish to thank Prof. Hari Reddy for introducing me to delta operators and for our fruitful discussions. I was very fortunate to share the office with Bahram Mirzai when I started to work on my project; I profited from many enriching discussions on CNNs and other important subjects. I am also indebted to Drahos1av Lfm for providing the necessary insight into the physica11imitations of <phrase>CNN</phrase> hardware. Dr. Armin Friedli and Michae1 Vollmer from the <phrase>Swiss</phrase> <phrase>Center</phrase> for Scientific <phrase>Computing</phrase> showed great interest in my <phrase>simulation</phrase> problems. I gratefully acknowledge their assistance with the Paragon <phrase>Supercomputer</phrase>. I enjoyed and learned a lot from the collaboration with Pio Balmelli and Ruben Notari during the supervision of their <phrase>diploma</phrase> project on <phrase>stochastic optimization</phrase> of CNNs. In an outstanding semester project, Simon Moser and Eric Pfaffhauser developed a <phrase>Java</phrase> <phrase>CNN</phrase> simulator that is now being used by numerous <phrase>CNN</phrase> researchers and lecturers throughout the world. Finally, I would like to extend my thanks to all colleagues at <phrase>ISI</phrase> for providing a very pleasant environment and a stimulating <phrase>atmosphere</phrase>. Abstract Cellular <phrase>neural networks</phrase> (CNNs) constitute a class of recurrent and locally coupled arrays of identical <phrase>dynamical systems</phrase> (cells). The underlying equation governing the dynamics of each <phrase>cell</phrase> is nonlinear and the cells are assumed to operate in parallel. The connectivity among the cells is determined by a set of parameters denoted as a template set. A specific task is implemented by determining the appropriate template set. <phrase>Signal processing</phrase> via CNNs only becomes efficient if the network is implemented in analog hardware. In view of the physicallinlitations that analog implementations entail, robust operation of a <phrase>CNN</phrase> chip with respect to parameter variations has 
<phrase>Machine Learning</phrase> Proof Access <phrase>Machine Learning</phrase> <phrase>Proof-of-concept</phrase> for Opportunistic <phrase>Spectrum</phrase> Concept for Opportunistic <phrase>Spectrum</phrase> The set of 5G requirements show that future <phrase>radio</phrase> systems should answer to network capabilities in terms of : capacity, <phrase>spectrum</phrase> for future evolutions, fixed-<phrase>mobile</phrase> convergence, integration of 3GPP and and robustness, cost efficiency, etc. In parallel, new user experiences <phrase>area</phrase>, from static to <phrase>high</phrase>-speed-throughput per user/ application, E2E to the need of new enablers for <phrase>business</phrase> as: <phrase>Internet</phrase> of Things (<phrase>IoT</phrase>), V2V communications. This <phrase>ecosystem</phrase> points out that Opportunistic <phrase>Spectrum</phrase> 5G network optimization. The <phrase>telecommunication</phrase> <phrase>community</phrase> is working on the at the Horizon 2020 (H2020). This future system would answer to the incr traffic issued from the new services (3D TVHD, gaming, <phrase>virtual reality</phrase>, smart cities, <phrase>e</phrase> and new user experiences. Connectivity should be possible at Any Time, Any Where and Any Device (ATAWAD). Thus, 5G should allow a set higher capacity higher <phrase>spectrum</phrase> efficiency <phrase>spectrum</phrase> agility <phrase>low power</phrase> consumption Higher network coverage flexibility for future evolutions fixed-<phrase>mobile</phrase> convergence integration of 3GPP and non 3GPP RATs resilience and robustness cost efficiency support for <phrase>high</phrase> down-to very low <phrase>bit</phrase> Sometimes, some answers to fulfill all these requirements are system's capacity with higher coverage without increasing the transmit power seems So, the system optimization has to be considered in <phrase>general</phrase>. <phrase>Spectrum</phrase> is one of the key issues expensive. 5G requirements show that future <phrase>radio</phrase> systems should answer to network capabilities <phrase>spectrum</phrase> efficiency, <phrase>spectrum</phrase> agility, <phrase>low power</phrase> consumption <phrase>mobile</phrase> convergence, integration of 3GPP and non 3GPP RATs, resilience , etc. new user experiences will appear such as: homogenous experience over the coverage-trains <phrase>velocity</phrase>, from outdoor to deep-indoor and hroughput per user/ application, E2E i latency of a few ms, connectivity transparency to the need of new enablers for <phrase>business</phrase> as: <phrase>Internet</phrase> of Things (<phrase>IoT</phrase>), V2V <phrase>ii</phrase> , D2D Opportunistic <phrase>Spectrum</phrase> Access should <phrase>play</phrase> a <phrase>telecommunication</phrase> <phrase>community</phrase> is working on the definition of future 5G that should take place at the Horizon 2020 (H2020). This future system would answer to the increase in <phrase>mobile</phrase> <phrase>data</phrase> issued from the new services (3D TVHD, gaming, <phrase>virtual reality</phrase>, smart cities, <phrase>e</phrase> and new user experiences. Connectivity should be possible at Any Time, Any Where and Any Device (ATAWAD). Thus, 5G should allow a set of requirements such as: higher <phrase>spectrum</phrase> efficiency Higher network coverage flexibility for future evolutions <phrase>mobile</phrase> convergence integration of 3GPP and non 3GPP RATs resilience and robustness to very low <phrase>bit</phrase>-rates ulfill 
An Analysis of <phrase>Single</phrase>-Layer Networks in <phrase>Unsupervised Feature Learning</phrase> A great deal of <phrase>research</phrase> has focused on <phrase>algorithms</phrase> for learning features from unla-beled <phrase>data</phrase>. Indeed, much progress has been made on <phrase>benchmark datasets</phrase> like NORB and CIFAR by employing increasingly complex <phrase>unsupervised learning</phrase> <phrase>algorithms</phrase> and deep models. In this <phrase>paper</phrase>, however, we show that several simple factors, such as the number of <phrase>hidden nodes</phrase> in the <phrase>model</phrase>, may be more important to achieving <phrase>high</phrase> performance than the <phrase>learning algorithm</phrase> or the depth of the <phrase>model</phrase>. Specifically, we will apply several off-the-shelf <phrase>feature learning</phrase> <phrase>algorithms</phrase> (sparse <phrase>auto-encoders</phrase>, sparse RBMs, <phrase>K-means clustering</phrase> , and Gaussian mixtures) to CIFAR, NORB, and <phrase>STL</phrase> datasets using only <phrase>single</phrase>-layer networks. We then present a detailed analysis of the effect of changes in the <phrase>model</phrase> setup: the <phrase>receptive field</phrase> size, number of <phrase>hidden nodes</phrase> (features), the step-size (" stride ") between extracted features, and the effect of whitening. Our <phrase>results</phrase> show that large numbers of <phrase>hidden nodes</phrase> and dense <phrase>feature extraction</phrase> are critical to achieving <phrase>high</phrase> performanceso critical, in fact, that when these parameters are pushed to their limits, we achieve <phrase>state</phrase>-of-the-<phrase>art</phrase> performance on both CIFAR-10 and NORB using only a <phrase>single</phrase> layer of features. More surprisingly, our best performance is based on <phrase>K-means clustering</phrase> , which is extremely fast, has no hyper-parameters to tune beyond the <phrase>model</phrase> structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously <phrase>published results</phrase> on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively).
Organising <phrase>Collaborative Learning</phrase> Spaces for <phrase>Knowledge</phrase> <phrase>Construction</phrase>: <phrase>Deep Learning</phrase> and Online Behaviour This study examines how <phrase>wiki</phrase> <phrase>technology</phrase> can be used to create an environment for <phrase>deep learning</phrase>, encourage reflections and develop positive online behaviour during group project work. The <phrase>paper</phrase> highlights (1) how the <phrase>wiki</phrase> spaces could be organized to support the project groups to focus on depth of learning; (2) how the use of <phrase>wiki</phrase> support students' learning; and (3) what online behaviour were needed when using <phrase>wiki</phrase>. It was found that the organized spaces were able to scaffold the learning, allow students to take responsibility of their spaces and improve readability of the pages. The students were also able to benefit from the work of the other project groups. The students articulated that <phrase>wiki</phrase> as a collaborative environment was able to support them in their process of learning, <phrase>spirit</phrase> of collaboration and sharing of <phrase>knowledge</phrase>, as well as <phrase>motivation</phrase> and engagement for learning. They expressed that integrity and teamwork were the two most important online behaviour needed in shared <phrase>collaborative learning</phrase> environment.
Dropout: a simple way to prevent <phrase>neural networks</phrase> from <phrase>overfitting</phrase> Deep <phrase>neural nets</phrase> with a large number of parameters are very powerful <phrase>machine learning</phrase> systems. However, <phrase>overfitting</phrase> is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with <phrase>overfitting</phrase> by combining the predictions of many different large <phrase>neural nets</phrase> at <phrase>test</phrase> time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the <phrase>neural network</phrase> during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At <phrase>test</phrase> time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a <phrase>single</phrase> unthinned network that has smaller weights. This significantly reduces <phrase>overfitting</phrase> and gives <phrase>major</phrase> improvements over other regularization methods. We show that dropout improves the performance of <phrase>neural networks</phrase> on <phrase>supervised learning</phrase> tasks in vision, <phrase>speech recognition</phrase>, <phrase>document classification</phrase> and <phrase>computational biology</phrase>, obtaining <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> on many benchmark <phrase>data</phrase> sets.
Assessing the Assessment: an <phrase>Empirical Study</phrase> of an <phrase>Information</phrase> Systems Development Subject <phrase>Universities</phrase> need to respond to a change in the mix of <phrase>student</phrase> profiles from a majority of the traditional academically committed <phrase>student</phrase> to a majority of the students who seek a qualification for a job. The latter type of <phrase>student</phrase> lacks <phrase>study skills</phrase> such as an ability to structure his learning experience and to engage actively and continuously with the material to be learnt. This study reports on the application of one <phrase>formative assessment</phrase> strategy in response to the change in the mix of <phrase>student</phrase> profiles. The aims were to encourage students to engage in <phrase>deep learning</phrase>; increase a student's responsibility for learning by providing a structure for the learning effort; and encouraging continuous study. The <phrase>results</phrase> indicate that the aims were to some extent achieved. Consideration of <phrase>formative assessment</phrase>, alternatives that might improve our response to the change, and areas for further <phrase>research</phrase> are identified.
Learning a Deep <phrase>Hybrid</phrase> <phrase>Model</phrase> for <phrase>Semi-Supervised</phrase> Text Classification We present a novel <phrase>fine-tuning</phrase> <phrase>algorithm</phrase> in a deep <phrase>hybrid</phrase> <phrase>architecture</phrase> for <phrase>semi-supervised</phrase> text classification. During each increment of the <phrase>online learning</phrase> process , the <phrase>fine-tuning</phrase> <phrase>algorithm</phrase> serves as a top-down mechanism for pseudo-jointly modifying <phrase>model</phrase> parameters following a bottom-up generative learning <phrase>pass</phrase>. The resulting <phrase>model</phrase>, trained under what we call the Bottom-Up-Top-Down <phrase>learning algorithm</phrase> , is shown to outperform a <phrase>variety</phrase> of competitive models and baselines trained across a wide <phrase>range</phrase> of splits between supervised and unsupervised <phrase>training data</phrase>.
Zero-<phrase>Shot</phrase> Learning for <phrase>Semantic</phrase> Utterance Classification We propose a novel zero-<phrase>shot</phrase> learning method for <phrase>semantic</phrase> utterance classification (SUC). It learns a classifier f : X Y for problems where none of the <phrase>semantic</phrase> categories Y are present in the <phrase>training set</phrase>. The framework uncovers the link between categories and utterances through a <phrase>semantic</phrase> space. We show that this <phrase>semantic</phrase> space can be learned by <phrase>deep neural networks</phrase> trained on large amounts of <phrase>search engine</phrase> query log <phrase>data</phrase>. What's more, we propose a novel method that can learn discriminative <phrase>semantic</phrase> features without supervision. It uses the zero-<phrase>shot</phrase> learning framework to guide the learning of the <phrase>semantic</phrase> features. We demonstrate the effectiveness of the zero-<phrase>shot</phrase> <phrase>semantic</phrase> <phrase>learning algorithm</phrase> on the SUC dataset collected by [1]. Furthermore, we achieve <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> by combining the <phrase>semantic</phrase> features with a supervised method.
Unsupervised Multilingual Learning for Morphological Segmentation For centuries, the deep connection between languages has brought about <phrase>major</phrase> discoveries about <phrase>human</phrase> <phrase>communication</phrase>. In this <phrase>paper</phrase> we investigate how this powerful source of <phrase>information</phrase> can be exploited for unsuper-vised <phrase>language</phrase> learning. In particular, we study the task of morphological segmentation of multiple languages. We present a non-parametric <phrase>Bayesian</phrase> <phrase>model</phrase> that jointly induces <phrase>morpheme</phrase> segmentations of each <phrase>language</phrase> under consideration and at the same time identifies cross-lingual <phrase>morpheme</phrase> patterns , or abstract <phrase>morphemes</phrase>. We apply our <phrase>model</phrase> to three <phrase>Semitic languages</phrase>: <phrase>Arabic</phrase>, He-brew, <phrase>Aramaic</phrase>, as well as to <phrase>English</phrase>. Our <phrase>results</phrase> demonstrate that learning morphological models in <phrase>tandem</phrase> reduces error by up to 24% relative to monolingual models. Furthermore , we provide evidence that our joint <phrase>model</phrase> achieves better performance when applied to languages from the same <phrase>family</phrase>.
Rank Aggregation for Similar Items The problem of combining the ranked preferences of many experts is an old and surprisingly deep problem that has gained renewed importance in many <phrase>machine learning</phrase>, <phrase>data mining</phrase>, and <phrase>information retrieval</phrase> applications. Effective rank aggregation becomes difficult in <phrase>real-world</phrase> situations in which the rankings are noisy, incomplete, or even disjoint. We address these difficulties by extending several standard methods of rank aggregation to consider similarity between items in the various ranked lists, in addition to their rankings. The intuition is that similar items should receive similar rankings, given an appropriate measure of similarity for the domain of interest. In this <phrase>paper</phrase>, we propose several <phrase>algorithms</phrase> for merging ranked lists of items with defined similarity. We establish evaluation criteria for these <phrase>algorithms</phrase> by extending previous definitions of distance between ranked lists to include the role of similarity between items. Finally, we <phrase>test</phrase> these new methods on both synthetic and <phrase>real-world</phrase> <phrase>data</phrase>, including <phrase>data</phrase> from an application in keywords expansion for sponsored search advertisers. Our <phrase>results</phrase> show that incorporating similarity <phrase>knowledge</phrase> within rank aggregation can significantly improve the performance of several standard rank aggregation methods, especially when used with noisy, incomplete, or disjoint rankings.
Surface Web <phrase>Semantics</phrase> for Structured <phrase>Natural Language Processing</phrase> Surface Web <phrase>Semantics</phrase> for Structured <phrase>Natural Language Processing</phrase> Abstract Surface Web <phrase>Semantics</phrase> for Structured <phrase>Natural Language Processing</phrase> Permission to make <phrase>digital</phrase> or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or <phrase>commercial advantage and that copies bear</phrase> this notice and the full citation on the first page. To copy otherwise, to republish, to <phrase>post on servers</phrase> or to redistribute to lists, <phrase>requires prior specific permission</phrase>. <phrase>Research</phrase> in <phrase>natural language processing</phrase> (<phrase>NLP</phrase>) is aimed at making machines automatically understand <phrase>natural language</phrase>. Incorporating world <phrase>knowledge</phrase> <phrase>semantics</phrase> is becoming an increasingly crucial requirement for resolving deep, complex decisions in most <phrase>NLP</phrase> tasks today, e.g., <phrase>question answering</phrase>, <phrase>syntactic</phrase> <phrase>parsing</phrase>, coreference resolution, and <phrase>relation extraction</phrase>. Structured <phrase>NLP</phrase> corpora such as treebanks are too small to encode much of this <phrase>knowledge</phrase>, so instead, we turn to the vast Web, and access its <phrase>information</phrase> via a diverse collection of Web <phrase>n-gram</phrase> counts (of size 4 billion, and 500x larger than <phrase>Wikipedia</phrase>). Shallow cues from this large <phrase>n-gram</phrase> dataset, when harnessed in a structured learning setting, help reveal deep <phrase>semantics</phrase>. In this <phrase>thesis</phrase>, we address various important facets of the <phrase>semantics</phrase> problem from indirect <phrase>semantics</phrase> for sentence-level <phrase>syntactic</phrase> ambiguities, and <phrase>semantics</phrase> as specific <phrase>knowledge</phrase> for discourse-level coreference ambiguities, to structured acquisition of <phrase>semantic</phrase> taxonomies from text, and <phrase>fine-grained</phrase> <phrase>semantics</phrase> such as intensity <phrase>order</phrase>. These facets represent structured <phrase>NLP</phrase> tasks which have a combinatorially large decision space. Hence, in <phrase>general</phrase>, we adopt a struc-tured learning approach, incorporating surface <phrase>Web-based</phrase> <phrase>semantic</phrase> cues as intuitive features on the full space of decisions. The feature weights are then learned automatically based on a discrim-inative training approach. Empirically, for each facet, we see <phrase>significant improvements</phrase> over the corresponding <phrase>state</phrase>-of-the-<phrase>art</phrase>. In the first part of this <phrase>thesis</phrase>, we show how <phrase>Web-based</phrase> features can be powerful cues to resolving complex <phrase>syntactic</phrase> ambiguities. We develop surface <phrase>n-gram</phrase> features over the full <phrase>range</phrase> of <phrase>syntactic</phrase> attachments, encoding both lexical affinities as well as paraphrase-based cues to <phrase>syntactic</phrase> structure. These features, when encoded into full-scale, discriminative dependency and constituent parsers, correct a <phrase>range</phrase> of error types. In the next part, we address <phrase>semantic</phrase> ambiguities in discourse-level coreference resolution, again using Web <phrase>n-gram</phrase> features that capture a <phrase>range</phrase> of world <phrase>knowledge</phrase> cues to hypernymy, <phrase>semantic</phrase> compatibility, and <phrase>semantic</phrase> context, as well as <phrase>general</phrase> lexical co-occurrence. When added to a <phrase>state</phrase>-of-the-<phrase>art</phrase> coreference baseline, these Web features provide <phrase>significant improvements</phrase> on multiple datasets and metrics. 2 In 
Image Colorization Using a <phrase>Deep Convolutional</phrase> <phrase>Neural Network</phrase> In this <phrase>paper</phrase>, we present a novel approach that uses <phrase>deep learning</phrase> techniques for colorizing <phrase>grayscale</phrase> images. By utilizing a <phrase>pre-trained</phrase> <phrase>convolutional neural network</phrase>, which is originally designed for <phrase>image classification</phrase>, we are able to separate content and style of different images and recombine them into a <phrase>single</phrase> image. We then propose a method that can add colors to a <phrase>grayscale</phrase> image by combining its content with style of a color image having <phrase>semantic</phrase> similarity with the <phrase>grayscale</phrase> one. As an application, to our <phrase>knowledge</phrase> the first of its kind, we use the <phrase>proposed method</phrase> to colorize images of ukiyo-<phrase>ea</phrase> <phrase>genre</phrase> of <phrase>Japanese</phrase> paintingand obtain interesting <phrase>results</phrase>, showing the potential of this method in the growing field of computer-assisted <phrase>art</phrase>.
Algorithmic Questions for Pregroup Grammars 4 <phrase>Parsing</phrase> Pregroup Grammars 47 4.1 Problems with <phrase>Parsing</phrase> Pregroup Grammars . . . . . . . . . . . . . . . 47 Acknowledgments I wish to <phrase>express my deep</phrase> gratitude to my Promotor Prof. Wojciech Buszkowski for his time he devoted to me. He oered me so much advice, help and encouragement. He patiently supervised me. I have learned a lot from him. I would like to thank my Master's <phrase>thesis</phrase> advisor, Dr Krzysztof Jassem, who encouraged me to continue my work and has always been willing to help. I thank my colleagues at the Faculty of <phrase>Mathematics</phrase> and <phrase>Computer Science</phrase> of <phrase>Adam Mickiewicz University</phrase>, especially late Dariusz Okoowski, talking to whom was always very encouraging and inspiring. Last but not least, this work would not have been achieved without the support and understanding of my <phrase>family</phrase>. I thank my husband who had to keep our <phrase>house</phrase> and take care of children when only at home and have always encouraged me to continue my work. Without support of my parents and sisters I would not be able to concentrate on my work.
Promoting <phrase>vicarious learning</phrase> of <phrase>physics</phrase> using deep questions with explanations Two experiments explored the role of vicarious " self " explanations in facilitating <phrase>student</phrase> learning gains during computer-presented instruction. In Exp. 1, <phrase>college</phrase> students with low or <phrase>high</phrase> <phrase>knowledge</phrase> on Newton's laws were tested in four conditions: (a) <phrase>monologue</phrase> (M), (b) questions (Q), (c) explanation (<phrase>E</phrase>), and (d) question explanation (Q <phrase>E</phrase>). Those with low pre-<phrase>experimental</phrase> <phrase>knowledge</phrase> levels showed marginally significant yet consistently greater gains than those with <phrase>high</phrase> levels and condition Q <phrase>E</phrase> outperformed the other three (M, Q, <phrase>E</phrase>). Among those with <phrase>high</phrase> <phrase>knowledge</phrase>, the Q <phrase>E</phrase> presentations actually inhibited learning. In Exp. 2, <phrase>high school</phrase> <phrase>physics</phrase> students in standard and honors classes were studied during their introduction to Newton's laws. Brief (12 min) computer videos that introduced key <phrase>Newtonian</phrase> concepts preceded <phrase>teacher</phrase> presentations in seven <phrase>daily</phrase> sessions. Both standard and honors students who received Q <phrase>E</phrase> presentations prior to regular classroom activities learned more in <phrase>daily</phrase> sessions than those who received either M or Q presentations. It was concluded that when key concepts are introduced in the context of deep questions along with explanations new learning was facilitated both in vicarious environments and in subsequent standard classroom activities.
Learning to Understand <phrase>Web Site</phrase> Update Requests We experimentally evaluate components of a system that learns to analyze <phrase>natural-language</phrase> requests to update <phrase>information</phrase> on a <phrase>database</phrase>-backed <phrase>web-site</phrase>. Our <phrase>long</phrase>-term goal is to develop a system which can adapt to changes in the distribution of requests as the underlying <phrase>database schema</phrase> changesin <phrase>short</phrase>, a system that performs deep analysis of text in a domain of discourse that is limited, but which grows over time. We describe a scheme for decomposing request-understanding into a <phrase>sequence</phrase> of entity recognition and text <phrase>classification tasks</phrase>, each of which can be solved using standard learning methods. We then present <phrase>experimental</phrase> <phrase>results</phrase> that quantify how well these tasks can be learned.
Affective Learning Companions Developing <phrase>learning experiences</phrase> that facilitate <phrase>self-actualization</phrase> and <phrase>creativity</phrase> is among the most important goals of our <phrase>society</phrase> in preparation for the future. To facilitate <phrase>deep understanding</phrase> of a new concept-to facilitate learning-learners must have the opportunity to develop multiple and flexible perspectives. The process of becoming an expert involves failure, understanding failure, and the <phrase>motivation</phrase> to move onward. Meta-<phrase>cognitive</phrase> awareness and personal strategies can <phrase>play</phrase> a role in developing an individual's ability to persevere through failure and combat other diluting influences. This <phrase>thesis</phrase> will <phrase>center</phrase> upon the development of a theory for using affective sensing and appropriate <phrase>relational</phrase> agent interactions to support learning and meta-<phrase>cognitive</phrase> strategies for perseverance through failure.
<phrase>Recurrent Neural Networks</phrase> for <phrase>Noise Reduction</phrase> in Robust ASR Recent work on <phrase>deep neural networks</phrase> as <phrase>acoustic</phrase> models for <phrase>automatic speech recognition</phrase> (ASR) have demonstrated substantial performance improvements. We introduce a <phrase>model</phrase> which uses a deep recurrent <phrase>auto encoder</phrase> <phrase>neural network</phrase> to denoise input features for robust ASR. The <phrase>model</phrase> is trained on <phrase>stereo</phrase> (noisy and clean) audio features to predict clean features given noisy input. The <phrase>model</phrase> makes no assumptions about how noise affects the signal, nor the existence of distinct noise environments. Instead, the <phrase>model</phrase> can learn to <phrase>model</phrase> any type of <phrase>distortion</phrase> or additive noise given sufficient <phrase>training data</phrase>. We demonstrate the <phrase>model</phrase> is competitive with existing feature denoising approaches on the Aurora2 task, and out-performs a <phrase>tandem</phrase> approach where deep networks are used to predict <phrase>phoneme</phrase> posteriors directly.
<phrase>Internet</phrase> and the <phrase>Digital Divide</phrase> The <phrase>Internet</phrase> provides less benefit to the underdeveloped countries than to the developed ones. This holds true even when network access is similar in both countries. We argue that network access is not enough to reap full benefit from the <phrase>Internet</phrase>; therefore, simple <phrase>universal</phrase> access to the <phrase>Internet</phrase> in underdeveloped countries is not enough to close the breach of the <phrase>Digital Divide</phrase>. <phrase>Agriculture</phrase>, industrialization, and informatization revolutions are with no doubt historical <phrase>productivity</phrase> landmarks. Tribes and societies can be divided based on the fact that they have or have not accomplished each of those landmarks. We still can find a few tribes that never learned <phrase>agriculture</phrase>; tribes merely dedicated to <phrase>hunting</phrase> and gathering. Some of these tribes can still be found deep into the <phrase>Amazon</phrase>. Their <phrase>productivity</phrase> barely covers the <phrase>basic</phrase> alimentary needs of the individual and that of their young children. On the other hand, the <phrase>productivity</phrase> of <phrase>agricultural</phrase> tribes covers the requirements of more children, for a longer <phrase>period</phrase> of time, and that of a few relatives that cannot work due to their old age or other physical impediments. And in some cases, it even extends to <phrase>cover</phrase> the necessities of another class of people or cast among those tribes that instead of physically working in the field they dedicate themselves to intellectual work. Intellectual work (observations, <phrase>research</phrase>, planning, and <phrase>leadership</phrase>) that derives in higher levels of overall <phrase>productivity</phrase> from those doing the physical <phrase>labor</phrase>. Industrialized societies increased their overall <phrase>productivity</phrase> by several <phrase>orders of magnitude</phrase> in comparison to <phrase>agricultural</phrase> ones [16]. Societies that are now undergoing the <phrase>information revolution</phrase> are advancing yet at even higher <phrase>orders of magnitude</phrase>. Unfortunately, with those great leaps accomplished by a few societies, the breach between them and those that have not yet accomplished the prior stages is becoming larger and larger. A breach that is not only limited to <phrase>productivity</phrase> but also to <phrase>culture</phrase> and forms of social <phrase>organization</phrase> [3, 4]. The breach or <phrase>division</phrase> between the societies that are involved in the accelerated process of informatization and those who are left behind in inferior stages of <phrase>productivity</phrase> is what we call the <phrase>digital divide</phrase> [2]. This divide is replacing the current <phrase>division</phrase> between the so called industrialized and not industrialized countries which had in turn replaced the breach between the developed and the underdeveloped countries. Unfortunately countries have tended to stay in the same side of the breach no <phrase>matter</phrase> 
An <phrase>Organizational Learning</phrase> Approach to Domain Analysis As the application of computer <phrase>technology</phrase> continues to proliferate and diversify, the identification and understanding of application domains is becoming increasingly important to <phrase>software development</phrase> methodologies. Domain analysis techniques have been developed to accumulate and formalize the <phrase>knowledge</phrase> necessary for successful <phrase>software</phrase> reuse. These techniques have been shown to be useful, but suffer from defining the domain too restrictively, burying important relationships deep in domain taxonomies, and prohibiting flexible identification of domains with common issues. Techniques are needed that dynamically detect recurring patterns of activities in development projects, This <phrase>paper</phrase> presents a method for developing and refining the <phrase>knowledge</phrase> and experience accumulated by a development <phrase>organization</phrase> so it can learn from previous efforts. A <phrase>case-based</phrase> repository of project experiences supports the re-use and refinement of <phrase>domain knowledge</phrase> to reduce duplicate effort, build on successful efforts, and avoid repeating mistakes in the process of building quality <phrase>software</phrase> systems.
A Task-specific Approach for Crawling the <phrase>Deep Web</phrase> There is a great amount of valuable <phrase>information</phrase> on the web that cannot be accessed by conventional crawler engines. This portion of the web is usually known as the <phrase>Deep Web</phrase> or the <phrase>Hidden Web</phrase>. Most probably, the <phrase>information</phrase> of highest value contained in the <phrase>deep web</phrase>, is that behind web forms. In this <phrase>paper</phrase>, we describe a <phrase>prototype</phrase> <phrase>hidden-web</phrase> crawler able to access such content. Our approach is based on providing the crawler with a set of domain definitions, each one describing a specific <phrase>data</phrase>-collecting task. The crawler uses these descriptions to identify relevant query forms and to learn to execute queries on them. We have tested our techniques for several <phrase>real world</phrase> tasks, obtaining a <phrase>high</phrase> <phrase>degree</phrase> of effectiveness. I. INTRODUCTION Crawlers are <phrase>software</phrase> programs that automatically traverse the web, retrieving pages to build a searchable index of their content. Conventional crawlers receive as input a set of "<phrase>seed</phrase>" pages and recursively obtain new ones by locating and traversing their outbound links. Crawling techniques have <phrase>led</phrase> the <phrase>construction</phrase> of highly successful commercial <phrase>web search</phrase> engines. Nevertheless, conventional web crawlers cannot access to a significant fraction of the web, which is usually called the " <phrase>hidden web</phrase> " or the " <phrase>deep web</phrase> ". The problem of crawling the " <phrase>hidden web</phrase> " can be divided into two challenges:-Crawling the " server-side " <phrase>hidden web</phrase>. Many <phrase>websites</phrase> offer query forms to access the contents of an underlying <phrase>database</phrase>. Conventional crawlers cannot access these pages because they do not know how to execute queries on those forms.-Crawling the " <phrase>client-side</phrase> " <phrase>hidden web</phrase>. Many <phrase>websites</phrase> use techniques such as <phrase>client-side scripting</phrase> languages and session maintenance mechanisms. Most conventional crawlers are unable to handle this kind of pages. Several works have tried to characterize the <phrase>hidden web</phrase> [4],
Learning <phrase>kinematics</phrase> in <phrase>elementary</phrase> grades using agent-based computational modeling: a <phrase>visual programming</phrase>-<phrase>based approach</phrase> Integrating computational modeling and <phrase>programming</phrase> with learning and teaching <phrase>physics</phrase> is a non-trivial challenge for educational designers. In this <phrase>paper</phrase>, we attempt to address this challenge by presenting ViMAP, a new <phrase>visual-programming language</phrase> and modeling platform for learning <phrase>kinematics</phrase>, and its underlying <phrase>design</phrase> principles. We then <phrase>report</phrase> a study conducted with 3<sup>rd</sup> and 4<sup>th</sup> grade students which shows that using ViMAP, they were able to develop a) deep conceptual understandings of <phrase>kinematics</phrase> and b) relevant <phrase>programming</phrase> and computational modeling practices. We also identify how the <phrase>design</phrase> principles supported the development of these understandings and practices as students engaged in learning activities that integrated modeling, <phrase>programming</phrase> and <phrase>physics</phrase>.
A <phrase>Self-Organizing Map</phrase> Based <phrase>Navigation</phrase> System Autonomous underwater vehicles (AUVs) have great advantages for activities in <phrase>deep sea</phrase>, and expected as the attractive tool. However, AUVs have various problems which should be solved. In this <phrase>paper</phrase>, the <phrase>Self-Organizing Map</phrase> (SOM) is applied as the clustering method for the <phrase>navigation</phrase> system. The SOM is known as one of the effective methods to extract the principle feature from many parameters and decrease the <phrase>dimension</phrase> of parameters. Through the competitive <phrase>learning algorithms</phrase>, the obtained map is tuned to express specific features of the input signals. We have been investigating the possibility of <phrase>navigation</phrase> system based on SOM through simulations are experiments with an <phrase>AUV</phrase> called "Twin-Burger". The <phrase>learning algorithm</phrase> of usual SOM is <phrase>unsupervised learning</phrase>. However, <phrase>supervised learning</phrase> <phrase>algorithms</phrase> should be introduced because the relationship between distances <phrase>information</phrase> and desirable behavior of the <phrase>robot</phrase>, that is, the relationship from inputs to outputs should be acquired and learned. In this <phrase>paper</phrase>, a <phrase>supervised learning</phrase> <phrase>algorithm</phrase> is introduced into SOM and a method to adapt the local map to its environment by learning and evaluating the trajectory of <phrase>robot</phrase> is proposed. In the <phrase>proposed method</phrase>, the "initial map" is made static and <phrase>digital</phrase> vale as teaching <phrase>data</phrase>. In <phrase>order</phrase> to include more <phrase>information</phrase> of environment in the initial map, the trajectories of <phrase>robot</phrase> are evaluated, and the evaluation is utilized in the learning process. This method enables the map to have both the effect of dynamics of <phrase>robot</phrase> and environmental <phrase>information</phrase>. The efficiency of the method is investigated through the simulations and experiments. I. INTRODUCTION Autonomous underwater vehicles (AUVs) have great advantages for activities in <phrase>deep sea</phrase>, such as wide <phrase>area</phrase> survey [1], inspection and <phrase>cable</phrase> tracking [2] and expected as the attractive tool for their extreme environment. However, AUVs have various problems which should be solved. For example, <phrase>motion control</phrase>, acquisition of sensors <phrase>information</phrase>, determination of <phrase>action</phrase>, <phrase>navigation</phrase> without collision, self-localization and so on. Therefore, it is desirable for AUVs to learn their behavior corresponding to various situations. Development of the <phrase>navigation</phrase> system without the collision to the obstacles is one of the important problems for the <phrase>mobile</phrase> <phrase>robots</phrase>. In <phrase>order</phrase> to navigate the <phrase>robots</phrase> without collision, the distances to environment are effective <phrase>information</phrase> to recognize the local situation of the <phrase>robot</phrase>. Because these variables can be obtained handily from the <phrase>ultrasonic</phrase> <phrase>range</phrase> sensors and so on. The distances to the
<phrase>Neuronal</phrase> dynamics of sensory coding: the legacy of Jose Pedro Segundo <phrase>Neuronal</phrase> dynamics refers to the temporal behaviors that <phrase>nervous</phrase> systems exhibit. By coding we ask what and how does the <phrase>neuronal</phrase> activity represent our sensory experience, emotions, mental decisions and actions. A measure of activity or <phrase>information</phrase> that ignores temporal structure, say the <phrase>average</phrase> over time, gives a zeroth <phrase>order</phrase> indication of our world or our interpretation/representation of it. It <phrase>falls</phrase> upon on us to reach deeper, to consider the potential for encoding that is offered by the dynamical features of activity. What resources from the brain's wetware enable coding in a dynamical <phrase>fashion</phrase>? What are the <phrase>basic</phrase> components, the <phrase>neurons</phrase>, <phrase>synapses</phrase>, circuits dynamically capable of, how do they respond to dynamic stimuli? How does the <phrase>brain</phrase> exploit these resources to extract dynamical features and to encode <phrase>information</phrase> in spike trains, in multi-unit <phrase>spatio-temporal</phrase> activity patterns and across <phrase>brain</phrase> areas and via transformations from the periphery to higher associative areas? A dynamical understanding of <phrase>brain function</phrase> builds on <phrase>knowledge</phrase> of how systems respond to inputs, in particular, biologically significant input. In the early 1960's, the stimulation <phrase>paradigm</phrase> of the day was heavily influenced by the emerging field of <phrase>cybernetics</phrase> with its use of linear <phrase>systems theory</phrase>. The relevance of this <phrase>knowledge</phrase> to true <phrase>brain function</phrase> always depends on how well one can relate it to the in vivo situation. From this point of view, sensory systems, along with motor systems, have always presented a distinct advantage because one has a relatively clear idea of what constitutes a relevant input, and what the output is used forin contrast e.g. to cells buried deep into associative cortices. Our current understanding of neural activity, from the sub-cellular to the whole <phrase>brain</phrase> level, has benefited from the close and cross-disciplinary interaction of practitioners interested in <phrase>neuronal</phrase> dynamicsnotably, neurophysiolo-gists, and applied <phrase>mathematicians</phrase> (or engineers or <phrase>physicists</phrase>) versed in the theory of linear/nonlinear dynamics and of <phrase>time series</phrase> and point process analysis. It has been particularly advanced by those practitioners who were able to reach out to the " other side " , learning enough of each other's <phrase>language</phrase> as well as the potential and limitations of each other's approaches. The theorists' approaches can provide, moreover , the conceptual frameworks that enable us to generalize across system levels and that expose, by way of reduction, the mechanistic essence of dynamics in coding. This <phrase>special issue</phrase> on <phrase>Neuronal</phrase> Dynamics of Sensory Coding highlights insights that can emerge 
Pedestrian Detection with Unsupervised Multi-stage <phrase>Feature Learning</phrase> Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of <phrase>deep learning</phrase> methods to vision, we <phrase>report</phrase> <phrase>state</phrase>-of-the-<phrase>art</phrase> and competitive <phrase>results</phrase> on all <phrase>major</phrase> pedestrian datasets with a <phrase>convolutional network</phrase> <phrase>model</phrase>. The <phrase>model</phrase> uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape <phrase>information</phrase> with local distinctive motif <phrase>information</phrase>, and an unsupervised method based on convolutional <phrase>sparse coding</phrase> to pre-<phrase>train</phrase> the filters at each stage.
BINGO! and DAFFODIL: Personalized Exploration of <phrase>Digital Libraries</phrase> and Web Sources Daffodil is a <phrase>digital library</phrase> system targeted at strategic support of advanced users during the <phrase>information</phrase> search process. It provides user-customizable " stratagems " for exploring and managing <phrase>digital library</phrase> objects with <phrase>meta data</phrase> annotations over a federation of heterogeneous <phrase>digital libraries</phrase>. Bingo! is a focused crawler that learns how to gather thematically relevant documents from the Web and <phrase>Deep-Web</phrase> sources. This <phrase>paper</phrase> presents a coupling <phrase>architecture</phrase> for Daffodil and Bingo! that allows advanced users to explore <phrase>digital libraries</phrase> and Web sources in a <phrase>comprehensive</phrase> and coherent way. Starting from a user's interest profile in Daffodil, Bingo! is instructed to find thematically similar documents on the Web, leading to <phrase>high</phrase>-quality recommendations that reach beyond the <phrase>information</phrase> that can be directly found in <phrase>digital libraries</phrase>. Our <phrase>experimental</phrase> studies demonstrate that this coupling does indeed <phrase>lead</phrase> to a powerful tool suite that improves the precision and recall compared to conventional <phrase>library</phrase> and <phrase>Web search</phrase> mechanisms.
Deep Representation for Abnormal <phrase>Event Detection</phrase> in Crowded Scenes Abnormal <phrase>event detection</phrase> is extremely important, especially for <phrase>video</phrase> <phrase>surveillance</phrase>. Nowadays, many detectors have been proposed based on <phrase>hand-crafted</phrase> features. However, it remains challenging to effectively distinguish abnormal events from normal ones. This <phrase>paper</phrase> proposes a deep representation based <phrase>algorithm</phrase> which extracts features in an unsupervised <phrase>fashion</phrase>. Specially, appearance, texture, and <phrase>short</phrase>-term motion features are automatically learned and fused with stacked <phrase>denoising autoencoders</phrase>. Subsequently, <phrase>long</phrase>-term temporal clues are modeled with a <phrase>long</phrase> <phrase>short-term memory</phrase> (LSTM) recurrent network, in <phrase>order</phrase> to discover meaningful regularities of <phrase>video</phrase> events. The abnormal events are identified as samples which disobey these regularities. Moreover, this <phrase>paper</phrase> proposes a spatial <phrase>anomaly detection</phrase> strategy via <phrase>manifold</phrase> ranking, aiming at excluding false alarms. Experiments and comparisons on <phrase>real world</phrase> datasets show that the <phrase>proposed algorithm</phrase> outperforms <phrase>state</phrase> of the <phrase>arts</phrase> for the abnormal <phrase>event detection</phrase> problem in crowded scenes.
Detecting Hazardous Events from Sequential <phrase>Data</phrase> with Multilayer Architectures Multivariate <phrase>time series</phrase> <phrase>data</phrase> <phrase>play</phrase> an important role in many domains, including real-time monitoring systems. In this <phrase>paper</phrase>, we focus on multilayer neural architectures that are capable of learning <phrase>high</phrase> level representations from raw <phrase>data</phrase>. This includes our previous <phrase>solution</phrase> based on <phrase>Recurrent Neural Networks</phrase> with <phrase>Long</phrase> <phrase>Short-Term Memory</phrase> (LSTM) cells. We build upon this work and present improved methods that aim to achieve higher prediction quality and better generalization to other similar tasks. We apply new deep neural architec-tures, minimize feature <phrase>engineering</phrase> and explore different ways of <phrase>model selection</phrase>. In particular, our focus on architectures includes networks with attention mechanism and <phrase>convolutional networks</phrase>. We tackle <phrase>overfitting</phrase> challenges in a presence of concept drift.
Query-Oriented Multi-Document Summarization via Unsupervised <phrase>Deep Learning</phrase> Extractive style query oriented multi document summariza tion generates the summary by extracting a proper set of sentences from multiple documents based on the pre given query. This <phrase>paper</phrase> proposes a novel multi document summa rization framework via <phrase>deep learning</phrase> <phrase>model</phrase>. This uniform framework consists of three parts: concepts extraction, summary generation, and <phrase>reconstruction</phrase> validation, which work together to achieve the largest coverage of the docu ments content. A new query oriented extraction technique is proposed to concentrate distributed <phrase>information</phrase> to <phrase>hidden units</phrase> <phrase>layer by layer</phrase>. Then, the whole <phrase>deep architecture</phrase> is fi ne tuned by minimizing the <phrase>information</phrase> loss of reconstruc tion validation. According to the concentrated <phrase>information</phrase>, <phrase>dynamic programming</phrase> is used to seek most informative set of sentences as the summary. Experiments on three bench mark datasets demonstrate the effectiveness of the <phrase>proposed framework</phrase> and <phrase>algorithms</phrase>.
Intelligent <phrase>Simulation</phrase>-based <phrase>Tutor</phrase> for <phrase>Flight Training</phrase> Today's <phrase>military</phrase> flight simulators have dramatically reduced the cost of training by providing cheaper, effective alternatives to training on a real <phrase>aircraft</phrase>. However, <phrase>flight training</phrase> is still limited by the availability of instructor pilots. The <phrase>adage</phrase> " practice makes perfect " is nowhere truer than in the learning psychomotor skills such as flying. Ideally, trainees should be able to practice flying skills on their own to <phrase>complement</phrase> instructor-<phrase>led</phrase> training. Most flight simulators do not have any automated assessment and tutoring facilities, making them ineffective as self-paced <phrase>learning environments</phrase>. The <phrase>Army</phrase> has funded pioneering <phrase>research</phrase> on developing automated tutors for <phrase>flight training</phrase>, specifically for training initial-entry rotor-wing pilots. An early <phrase>rule-based</phrase> system, called the IFT (Intelligent Flight <phrase>Trainer</phrase>), monitored trainees' flight performance and provided adaptive coaching. It provided instructional assistance by regulating the challenge level of a flight task, and through overt spoken <phrase>feedback</phrase> to inform trainees when they are flying out of <phrase>range</phrase> of specified flight parameters. Evaluations showed that while this system was effective in improving flying skills, it was inflexible in terms of it assessment and instruction strategies. The <phrase>Army</phrase> is currently funding <phrase>research</phrase> on a next generation automatic flight <phrase>trainer</phrase>, called <phrase>AIS</phrase>-IFT, that improves upon the IFT. <phrase>AIS</phrase>-IFT is designed to be flexible and extensible in terms of assessment and tutoring procedures. A visual authoring tool lets SMEs and course designers modify or create powerful instructional behavior with little <phrase>programming</phrase> effort. Whereas the previous effort had the instructional approach embedded deep in the tutoring system, the new approach separate the specific instructional strategies from the ITS <phrase>infrastructure</phrase>, thus empowering SMEs and course authors to create a <phrase>tutor</phrase> with <phrase>pedagogy</phrase> that is customized to their domain. His graduate work focused on " map building " , whereby an <phrase>autonomous robot</phrase> combines sensory <phrase>information</phrase> and actions it performs in <phrase>order</phrase> to build and localize in a map of its environment. Dr. Remolina's <phrase>research</phrase> interest includes <phrase>intelligent tutoring</phrase> systems, planning, <phrase>simulation</phrase> and common sense reasoning. She has a strong background in a wide <phrase>variety</phrase> of <phrase>Artificial Intelligence</phrase> techniques, including <phrase>Intelligent Tutoring</phrase> Systems, and <phrase>Machine Learning</phrase>. Her <phrase>research</phrase> interests include application of <phrase>Artificial Intelligence</phrase> techniques to <phrase>Education</phrase> <phrase>Technology</phrase> with a focus on addressing motivational, affective, and meta-<phrase>cognitive</phrase> issues. Dr. Ramachandran has headed several <phrase>intelligent tutoring</phrase> system development efforts for k-12 <phrase>education</phrase> and <phrase>military</phrase> training. She is currently heading an effort to develop an <phrase>intelligent tutoring</phrase> system for training <phrase>medical</phrase> teams and 
<phrase>Unsupervised Feature Learning</phrase> and <phrase>Deep Learning</phrase>: A Review and New Perspectives ! Abstract The success of <phrase>machine learning</phrase> <phrase>algorithms</phrase> generally depends on <phrase>data</phrase> representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the <phrase>data</phrase>. Although <phrase>domain knowledge</phrase> can be used to help <phrase>design</phrase> representations, learning can also be used, and the quest for <phrase>AI</phrase> is motivating the <phrase>design</phrase> of more powerful representation-<phrase>learning algorithms</phrase>. This <phrase>paper</phrase> reviews recent work in the <phrase>area</phrase> of <phrase>unsupervised feature learning</phrase> and <phrase>deep learning</phrase>, covering advances in <phrase>probabilistic models</phrase>, <phrase>manifold</phrase> learning, and <phrase>deep learning</phrase>. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for <phrase>computing</phrase> representations (i.e., inference), and the geometrical connections between representation learning, <phrase>density estimation</phrase> and <phrase>manifold</phrase> learning.
Modeling <phrase>Freeway</phrase> Lane Changing Behavior Modeling <phrase>Freeway</phrase> Lane Changing Behavior Drivers continuously evaluate the surrounding traffic and the roadway environment, and make decisions about lanes and travel speed. The objective of this <phrase>thesis</phrase> is to develop a lane changing <phrase>model</phrase> that can be used in microscopic traffic <phrase>simulation</phrase> models to capture drivers' lane changing behavior. Lane change is modeled as a <phrase>sequence</phrase> of four steps: decision to consider a lane change, choice of left or right lane, search for an acceptable gap to execute the decision, and performing the lane change maneuver. First, a decision is made whether a driver will consider <phrase>changing lanes</phrase>. If a decision to consider <phrase>changing lanes</phrase> is made, a lane is chosen from the alternatives. Finally, the gap acceptance <phrase>model</phrase> determines whether the available gap in the <phrase>target</phrase> lane is sufficient for a safe merging and lane change can be completed. A discrete choice framework is used to <phrase>model</phrase> the lane changing behavior. The framework allows for modeling the impact of different elements of the traffic and roadway environment on driver behavior. The <phrase>model</phrase> is applied in the special case of merging from an on-ramp. <phrase>Results</phrase> from the estimation of the parameters show that in addition to the gap length, other important factors that affect drivers gap acceptance behavior are relative speed, distance remaining to the point at which lane change must be complete, and delay in completing merging. Finally, the estimated <phrase>model</phrase> is tested in a micro-<phrase>simulation</phrase> environment. ACKNOWLEDGEMENT I acknowledge with deep gratitude the guidance and constant inspiration provided by my was a privilege to work with them. I have learned a lot from them during the course of this <phrase>research</phrase>. My special thanks goes to <phrase>Qi</phrase> Yang-an extremely helpful and always encouraging friend. Sometimes he came out of his way to help me that made me feel guilty lot of times. I wish him success in <phrase>life</phrase>. grateful to the <phrase>Center</phrase> for Transportation Studies at <phrase>MIT</phrase> for awarding me admission with financial aid without which <phrase>MIT</phrase> would have been a dream place to me. I would like to thank all my <phrase>friends</phrase> that made my stay at <phrase>MIT</phrase> an enjoyable experience <phrase>Department</phrase> for the friendship and their support. And finally, I wish I knew the exact words to express my indebtedness to Lubna, my wife, for her constant support, care, and endless <phrase>love</phrase>, to my parents for having <phrase>faith</phrase> in me and their <phrase>love</phrase>, encouragement, and constant inspiration that helped me outgrow 
<phrase>Web-based</phrase> <phrase>Medical</phrase> Teaching using a <phrase>Multi-Agent</phrase> System <phrase>Web-based</phrase> teaching via <phrase>Intelligent Tutoring</phrase> Systems (ITSs) is considered as one of the most successful enterprises in <phrase>artificial intelligence</phrase>. Indeed, there is a <phrase>long</phrase> list of ITSs that have been tested on humans and have <phrase>proven</phrase> to facilitate learning, among which we may find the well-tested and known tutors of <phrase>algebra</phrase>, <phrase>geometry</phrase>, and computer languages. These ITSs use a <phrase>variety</phrase> of computational paradigms, as <phrase>production</phrase> systems, <phrase>Bayesian</phrase> networks, schema-templates, <phrase>theorem proving</phrase>, and explanatory reasoning. The next generation of ITSs are expected to go one step further by adopting not only more intelligent interfaces but will focus on integration. This article will describe some particularities of a tutoring system that we are developing to simulate conversational dialogue in the <phrase>area</phrase> of <phrase>Medicine</phrase>, that enables the integration of highly heterogeneous sources of <phrase>information</phrase> into a coherent <phrase>knowledge base</phrase>, either from the tutor's point of view or the development of the discipline in itself, i.e. the system's content is created automatically by the <phrase>physicians</phrase> as their <phrase>daily</phrase> work goes on. This will encourage students to articulate lengthier answers that exhibit deep reasoning, rather than to deliver straight tips of shallow <phrase>knowledge</phrase>. The goal is to take advantage of the normal functioning of the <phrase>health care</phrase> units to build on the <phrase>fly</phrase> a <phrase>knowledge base</phrase> of cases and <phrase>data</phrase> for teaching and <phrase>research</phrase> purposes.
A deep <phrase>language</phrase> <phrase>model</phrase> for <phrase>software</phrase> code Existing <phrase>language</phrase> models such as n-grams for <phrase>software</phrase> code often fail to capture a <phrase>long</phrase> context where dependent code elements scatter far apart. In this <phrase>paper</phrase>, we propose a novel approach to build a <phrase>language</phrase> <phrase>model</phrase> for <phrase>software</phrase> code to address this particular issue. Our <phrase>language</phrase> <phrase>model</phrase>, partly inspired by <phrase>human</phrase> <phrase>memory</phrase>, is built upon the powerful <phrase>deep learning</phrase>-based <phrase>Long</phrase> <phrase>Short Term Memory</phrase> <phrase>architecture</phrase> that is capable of learning <phrase>long</phrase>-term dependencies which occur frequently in <phrase>software</phrase> code. <phrase>Results</phrase> from our intrinsic evaluation on a corpus of <phrase>Java</phrase> projects have demonstrated the effectiveness of our <phrase>language</phrase> <phrase>model</phrase>. This work contributes to realizing our vision for DeepSoft, an <phrase>end-to-end</phrase>, generic <phrase>deep learning</phrase>-based framework for modeling <phrase>software</phrase> and its development process.
Controlled <phrase>Natural Language</phrase> - 5th International Workshop, CNL 2016, <phrase>Aberdeen</phrase>, <phrase>UK</phrase>, July 25-27, 2016, Proceedings In the <phrase>era</phrase> of <phrase>Big Data</phrase> and <phrase>Deep Learning</phrase>, a common view is that statistical and <phrase>machine learning</phrase> approaches are the only way to cope with the robust and scalable <phrase>information extraction</phrase> and summarization. Manning [1] compares <phrase>Deep Learning</phrase> with a <phrase>tsunami</phrase> at the shores of <phrase>Computational Linguistics</phrase>, raising a question if this is the end for the linguistically oriented approaches. Consequently, this question is relevant also to the special interest group on Controlled <phrase>Natural Language</phrase> (CNL). It has been <phrase>recently proposed</phrase> that the CNL approach could be scaled up, building on the concept of embedded CNL [2] and, thus, allowing for CNL-based <phrase>information extraction</phrase> from e.g. normative or <phrase>medical</phrase> texts that are rather controlled by <phrase>nature</phrase> but still infringe the boundaries of CNL or the <phrase>target</phrase> formalism [3]. It has also been demonstrated that CNL can serve as an efficient and user-<phrase>friendly</phrase> interface for <phrase>Big Data</phrase> end-point querying [4; 5], or for <phrase>bootstrapping</phrase> robust <phrase>NL</phrase> interfaces [6]. as well as for tailored multilingual <phrase>natural language generation</phrase> from the retrieved <phrase>data</phrase> [4]. In this position <phrase>paper</phrase>, we focus on the issue of multi-document storyline summari-zation, and generation of story highlights a task in the Horizon 2020 <phrase>Big Data</phrase> project SUMMA 1 (Scalable Understanding of Multilingual <phrase>MediA</phrase>). For this use case, the <phrase>information extraction</phrase> process, i.e., the <phrase>semantic</phrase> <phrase>parsing</phrase> of input texts cannot be approached by CNL: <phrase>large-scale</phrase> <phrase>media</phrase> monitoring is not limited to a particular domain, and the input sources vary from newswire texts to <phrase>radio</phrase> and <phrase>TV</phrase> transcripts to <phrase>user-generated content</phrase> in <phrase>social networks</phrase>. Robust <phrase>machine learning</phrase> techniques are necessary instead to map the arbitrary input sentences to their meaning representation in terms of PropBank and FrameNet [7], or the emerging Abstract Meaning Representation , AMR [8], which is based on PropBank with <phrase>named entity</phrase> recognition and linking via <phrase>DBpedia</phrase> [9]. AMR <phrase>parsing</phrase> has reached 67% accuracy (the F 1 score) on open-domain texts, which is <phrase>a level</phrase> acceptable for automatic summarization [10]. Although it is arguable if CNL can be exploited to approach the robust wide-coverage <phrase>semantic</phrase> <phrase>parsing</phrase> for use cases like <phrase>media</phrase> monitoring, its potential becomes much more obvious in the opposite direction: generation of story highlights from the summarized (pruned) AMR <phrase>graphs</phrase>. An example of possible input and expected output is given in Figure 1. While novel methods for AMR-based abstractive 2 summarization begin to appear [11], full text generation from AMR is still recognized 
Deep Blind <phrase>Compressed Sensing</phrase> This work addresses the problem of extracting deeply <phrase>learned features</phrase> directly from compressive measurements. There has been no work in this <phrase>area</phrase>; existing <phrase>deep learning</phrase> tools only give good <phrase>results</phrase> when applied on the full signal (that too usually after pre-processing). These techniques require the signal to be reconstructed first. In this work we show that by learning directly from the compressed domain, considerably better <phrase>results</phrase> can be obtained. This work extends the <phrase>recently proposed</phrase> framework of deep matrix factorization in combination with blind <phrase>compressed sensing</phrase>; hence the term 'deep blind <phrase>compressed sensing</phrase>'. <phrase>Simulation</phrase> experiments have been carried out on imaging via <phrase>single</phrase> <phrase>pixel</phrase> <phrase>camera</phrase>, under-sampled biomedical signals (arising in <phrase>wireless</phrase> body <phrase>area</phrase> network) and compressive <phrase>hyperspectral imaging</phrase>. In all cases, the superiority of our proposed deep blind <phrase>compressed sensing</phrase> can be envisaged.
Deep <phrase>Model</phrase> Based Transfer and <phrase>Multi-task</phrase> Learning Deep <phrase>Model</phrase> Based Transfer and <phrase>Multi-task</phrase> Learning for Biological <phrase>Image Analysis</phrase> A central theme in learning from image <phrase>data</phrase> is to develop appropriate representations for the specific task at hand. Traditional methods used handcrafted local features combined with <phrase>high</phrase>-level <phrase>image representations</phrase> to generate image-level representations. Thus, a practical challenge is to determine what features are appropriate for specific tasks. For example, in the study of <phrase>gene expression</phrase> patterns in <phrase>Drosophila melanogaster</phrase>, texture features based on wavelets were particularly effective for determining the developmental stages from in situ hybridization (ISH) images. Such image representation is however not suitable for <phrase>controlled vocabulary</phrase> (CV) term annotation because each CV term is often associated with only a part of an image. Here, we developed problem-<phrase>independent</phrase> <phrase>feature extraction</phrase> methods to generate hierarchical representations for ISH images. Our approach is based 1 Zhang et al. on the <phrase>deep convolutional</phrase> <phrase>neural networks</phrase> (CNNs) that can <phrase>act</phrase> on image <phrase>pixels</phrase> directly. To make the extracted features generic, the models were trained using a natural image set with millions of labeled examples. These models were transferred to the ISH image domain. To account for the differences between the source and <phrase>target</phrase> domains, we propose a partial <phrase>transfer learning</phrase> scheme in which only part of the source <phrase>model</phrase> is transferred. We employed <phrase>multi-task</phrase> learning method to fine-tune the <phrase>pre-trained</phrase> models with labeled ISH images. <phrase>Experimental</phrase> <phrase>results</phrase> showed that <phrase>feature representations</phrase> computed by deep models based on transfer and <phrase>multi-task</phrase> learning significantly outperformed other methods for annotating <phrase>gene expression</phrase> patterns at different stage <phrase>ranges</phrase>. We also demonstrated that the intermediate layers of deep models <phrase>produced</phrase> the best <phrase>gene expression</phrase> pattern representations.
<phrase>Deep Learning</phrase> <phrase>Layer-wise</phrase> Learning of Feature Hierarchies Hierarchical <phrase>neural networks</phrase> for <phrase>object recognition</phrase> have a <phrase>long</phrase> <phrase>history</phrase>. In recent years, novel methods for incrementally learning a hierarchy of features from unlabeled inputs were proposed as good <phrase>starting point</phrase> for supervised training. These <phrase>deep learning</phrase> methods together with the advances of parallel computersmade it possible to successfully attack problems that were not practical before, in terms of depth and input size. In this article, we introduce the reader to the <phrase>basic</phrase> concepts of <phrase>deep learning</phrase>, discuss selected methods in detail, and present application examples from <phrase>computer vision</phrase> and <phrase>speech recognition</phrase>.
<phrase>Communication</phrase>-aware <phrase>motion planning</phrase> in fading environments In this <phrase>paper</phrase> we create a framework to <phrase>model</phrase> and characterize the impact of time-varying fading <phrase>communication</phrase> links on the performance of a <phrase>mobile</phrase> <phrase>sensor</phrase> network. We propose <phrase>communication</phrase>-aware <phrase>motion-planning</phrase> strategies, where each node incorporates statistical learning of <phrase>communication</phrase> link qualities, such as <phrase>Signal to Noise Ratio</phrase> (SNR) and correlation characteristics, into its <phrase>motion-planning</phrase> <phrase>function</phrase>. We show that while uncorrelated fading channels can ruin the overall performance, the introduced natural <phrase>randomization</phrase> can potentially help the nodes leave deep fade spots. We furthermore show that highly correlated deep fades, on the other hand, can degrade the performance drastically for a <phrase>long</phrase> <phrase>period</phrase> of time. We then propose a randomizing <phrase>motion-planning</phrase> strategy that can help the nodes leave highly correlated deep fades.
Szemerdi's Regularity Lemma and Its Applications to Pairwise Clustering and Segmentation Szemerdi's regularity lemma is a deep result from <phrase>extremal graph theory</phrase> which states that every <phrase>graph</phrase> can be well-approximated by the <phrase>union</phrase> of a constant number of random-like <phrase>bipartite</phrase> <phrase>graphs</phrase>, called regular pairs. Although the original proof was non-constructive, efficient (i.e., <phrase>polynomial</phrase>-time) <phrase>algorithms</phrase> have been developed to determine regular <phrase>partitions</phrase> for arbitrary <phrase>graphs</phrase>. This <phrase>paper</phrase> reports a first attempt at applying Szemerdi's result to <phrase>computer vision</phrase> and <phrase>pattern recognition</phrase> problems. Motivated by a powerful <phrase>auxiliary</phrase> result which, given a partitioned <phrase>graph</phrase>, allows one to construct a small reduced <phrase>graph</phrase> which inherits many properties of the original one, we develop a two-step pairwise clustering strategy in an attempt to reduce computational costs while preserving satisfactory <phrase>classification accuracy</phrase>. Specifically, Szemerdi's partitioning process is used as a preclustering step to substantially reduce the size of the input <phrase>graph</phrase> in a way which takes advantage of the strong notion of edge-<phrase>density</phrase> regularity. Clustering is then performed on the reduced <phrase>graph</phrase> using standard <phrase>algorithms</phrase> and the solutions obtained are then mapped back into the original <phrase>graph</phrase> to create the final groups. <phrase>Experimental</phrase> <phrase>results</phrase> conducted on standard <phrase>benchmark datasets</phrase> from the <phrase>UCI</phrase> <phrase>machine learning</phrase> repository as well as on <phrase>image segmentation</phrase> tasks confirm the effectiveness of the <phrase>proposed approach</phrase>.
<phrase>Neuronal</phrase> Adaptation for Sampling-Based Probabilistic Inference in <phrase>Perceptual</phrase> Bistability It has been argued that <phrase>perceptual</phrase> multistability reflects probabilistic inference performed by the <phrase>brain</phrase> when sensory input is ambiguous. Alternatively, more traditional explanations of multistability refer to <phrase>low-level</phrase> mechanisms such as <phrase>neuronal</phrase> adaptation. We employ a Deep <phrase>Boltzmann</phrase> Machine (<phrase>DBM</phrase>) <phrase>model</phrase> of <phrase>cortical</phrase> processing to demonstrate that these two different approaches can be combined in the same framework. Based on <phrase>recent developments</phrase> in <phrase>machine learning</phrase> , we show how <phrase>neuronal</phrase> adaptation can be understood as a mechanism that improves probabilistic, sampling-based inference. Using the ambiguous <phrase>Necker cube</phrase> image, we analyze the <phrase>perceptual</phrase> switching exhibited by the <phrase>model</phrase>. We also examine the influence of spatial attention, and explore how <phrase>binocular rivalry</phrase> can be modeled with the same approach. Our work joins earlier studies in demonstrating how the principles underlying <phrase>DBMs</phrase> relate to <phrase>cortical</phrase> processing, and offers novel perspectives on the neural implementation of approximate probabilistic inference in the <phrase>brain</phrase>.
CALMsystem: A Conversational Agent for Learner Modelling This <phrase>paper</phrase> describes a system which incorporates <phrase>natural language</phrase> technologies, <phrase>database</phrase> manipulation and educational theories in <phrase>order</phrase> to offer learners a Negotiated Learner <phrase>Model</phrase>, for integration into an <phrase>Intelligent Tutoring</phrase> System. The system presents the learner with their learner <phrase>model</phrase>, offering them the opportunity to compare their own beliefs regarding their capabilities with those inferred by the system. A conversational agent, or " chatbot " has been developed to allow the learner to negotiate over the representations held about them using <phrase>natural language</phrase>. The system aims to support the metacognitive goals of self-assessment and reflection, which are increasingly seen as key to learning and are being incorporated into <phrase>UK</phrase> educational policy. The <phrase>paper</phrase> describes the <phrase>design</phrase> of the system, and reports a user trial, in which the chatbot was found to support users in increasing the accuracy of their self-assessments, and in reducing the number of discrepancies between system and user beliefs in the learner <phrase>model</phrase>. Some <phrase>lessons learned</phrase> in the development have been highlighted and <phrase>future research</phrase> and experimentation directions are outlined. <phrase>Intelligent Tutoring</phrase> Systems (ITS) provide their users with an adaptive <phrase>learning environment</phrase>, with personalized tutoring and testing customised to meet the needs of the individual <phrase>student</phrase>. This adaptation is based on the contents of the learner <phrase>model</phrase>, a representation of the student's <phrase>knowledge</phrase>, gaps in understanding and misconceptions. Traditional ITSs have not made the contents of the learner <phrase>model</phrase> visible to the learner. However, it has been argued that an Open Learner <phrase>Model</phrase> (i.e. one that can be inspected by the <phrase>student</phrase>) can offer opportunities for learner reflection, <phrase>metacognition</phrase> and <phrase>deep learning</phrase>, which may enhance learning (e.g. [1], [2], [3], [4] and [5]), as well as improving the accuracy of the learner <phrase>model</phrase>. Educational theorists have emphasised the importance of learner reflection ([6], [7] and [8]). Some researchers have developed Open Learner Models (<phrase>OLM</phrase>) that
Experiences in teaching an educational user-level <phrase>operating systems</phrase> implementation project The importance of a <phrase>comprehensive</phrase> implementation component for <phrase>undergraduate</phrase> <phrase>Operating Systems</phrase> (<phrase>OS</phrase>) courses cannot be understated. Students not only develop deep insight and understanding of <phrase>OS</phrase> fundamentals, but they also learn key <phrase>software engineering</phrase> skills that only a large development project, such as implementing an <phrase>OS</phrase>, can teach. There are clear benefits to traditional <phrase>OS</phrase> projects where students program or alter real (<phrase>Linux</phrase>) kernel source or extend educational <phrase>OS</phrase> implementations; however, in our experience, <phrase>bootstrapping</phrase> such a project is a huge undertaking that may not be accessible in many classrooms. In this <phrase>paper</phrase>, we describe a different approach to the <phrase>OS</phrase> implementation assignment: A user-level Operating System <phrase>simulation</phrase> based on <phrase>UNIX</phrase> preemptive signaling and threading constructs called ucontext. We believe that this variation of the implementation assignment provides many of the same educational benefits as traditional <phrase>low-level</phrase> projects without many of the expensive start-up costs. This project has been taught for a number of years at the <phrase>University</phrase> of <phrase>Pennsylvania</phrase> and was recently overhauled for the Fall 2011 semester. This <phrase>paper</phrase> describes the current version of the project and our experiences teaching it to a class of 54 students.
Recurrent <phrase>Convolutional Networks</phrase> for Pulmonary Nodule Detection in CT Imaging <phrase>Computed tomography</phrase> (CT) generates a stack of cross-sectional images covering a <phrase>region</phrase> of the body. The visual assessment of these images for the identification of potential abnormalities is a challenging and time consuming task due to the large amount of <phrase>information</phrase> that needs to be processed. In this article we propose a deep <phrase>artificial</phrase> <phrase>neu</phrase>-ral <phrase>network architecture</phrase>, ReCTnet, for the fully-automated detection of pulmonary nodules in <phrase>CT scans</phrase>. The <phrase>architecture</phrase> learns to distinguish nodules and normal structures at the <phrase>pixel</phrase> level and generates three-dimensional <phrase>probability</phrase> maps highlighting areas that are likely to harbour the objects of interest. Convolutional and recurrent layers are combined to learn expressive <phrase>image representations</phrase> exploiting the spatial dependencies across axial slices. We demonstrate that lever-aging intra-slice dependencies substantially increases the sensitivity to detect pulmonary nodules without inflating the false positive rate. On the publicly available LIDC/IDRI dataset consisting of 1,018 annotated <phrase>CT scans</phrase>, ReCTnet reaches a detection sensitivity of 90.5% with an <phrase>average</phrase> of 4.5 false positives per scan. Comparisons with a competing multi-<phrase>channel</phrase> <phrase>convolutional neural network</phrase> for multi-slice segmentation and other published methodologies using the same dataset provide evidence that ReCTnet offers significant performance gains.
Fast <phrase>Algorithms</phrase> for <phrase>Convolutional Neural Networks</phrase> We derive a new class of fast <phrase>algorithms</phrase> for <phrase>convolutional neural networks</phrase> using Winograd's minimal filtering <phrase>algorithms</phrase>. Specifically we derive <phrase>algorithms</phrase> for network layers with 3 3 kernels, which are the preferred kernel size for <phrase>image recognition</phrase> tasks. The best of our <phrase>algorithms</phrase> reduces <phrase>arithmetic</phrase> complexity up to 4X compared with direct convolu-tion, while using small block sizes with limited transform overhead and <phrase>high</phrase> computational intensity. By comparison, <phrase>FFT</phrase> based <phrase>convolution</phrase> requires larger block sizes and significantly greater transform overhead to achieve an equal complexity reduction. We measure the accuracy of our <phrase>algorithms</phrase> to be sufficient for <phrase>deep learning</phrase> and inference with fp32 or fp16 <phrase>data</phrase>. Also, we demonstrate the practical application of our approach with a simple <phrase>CPU</phrase> implementation of our slowest <phrase>algorithm</phrase> using the <phrase>Intel</phrase> <phrase>Math</phrase> Kernel <phrase>Library</phrase>, and <phrase>report</phrase> VGG network inference <phrase>results</phrase> that are 2.6X as fast as Caffe with an effective utilization of 109%. We believe these are the highest utilization convnet inference <phrase>results</phrase> to date, and that they can be improved significantly with more implementation effort. We also believe that the new <phrase>algorithms</phrase> lend themselves equally well to <phrase>GPU</phrase> and <phrase>FPGA</phrase> implementations for both training and inference.
Deep Bottleneck Features for <phrase>Spoken Language</phrase> Identification A key problem in <phrase>spoken language</phrase> identification (LID) is to <phrase>design</phrase> effective representations which are specific to <phrase>language</phrase> <phrase>information</phrase>. For example, in recent years, representations based on both phonotactic and <phrase>acoustic</phrase> features have <phrase>proven</phrase> their effectiveness for LID. Although advances in <phrase>machine learning</phrase> have <phrase>led</phrase> to <phrase>significant improvements</phrase>, LID performance is still lacking, especially for <phrase>short</phrase> duration speech utterances. With the <phrase>hypothesis</phrase> that <phrase>language</phrase> <phrase>information</phrase> is weak and represented only latently in speech, and is largely dependent on the statistical properties of the speech content, existing representations may be insufficient. Furthermore they may be susceptible to the variations caused by different speakers, specific content of the speech segments, and background noise. To address this, we propose using Deep Bottleneck Features (DBF) for spoken LID, motivated by the success of <phrase>Deep Neural Networks</phrase> (DNN) in <phrase>speech recognition</phrase>. We show that DBFs can form a <phrase>low-dimensional</phrase> compact representation of the original inputs with a powerful descriptive and discriminative capability. To evaluate the effectiveness of this, we <phrase>design</phrase> two <phrase>acoustic</phrase> models, termed DBF-<phrase>TV</phrase> and parallel DBF-<phrase>TV</phrase> (PDBF-<phrase>TV</phrase>), using a DBF based i-<phrase>vector</phrase> representation for each speech utterance. <phrase>Results</phrase> on <phrase>NIST</phrase> <phrase>language</phrase> recognition evaluation 2009 (LRE09) show <phrase>significant improvements</phrase> over <phrase>state</phrase>-of-the-<phrase>art</phrase> systems. By fusing the output of phonotactic and <phrase>acoustic</phrase> approaches, we achieve an EER of 1.08%, 1.89% and 7.01% for 30 s, 10 s and 3 s <phrase>test</phrase> utterances respectively. Furthermore, various DBF configurations have been extensively evaluated, and an optimal system proposed.
Involving <phrase>Industry</phrase> Professionals in <phrase>Empirical Studies</phrase> with Students <phrase>Empirical studies</phrase> are often carried out with students because they are viewed as inexpensive subjects for <phrase>pilot</phrase> studies. Though the <phrase>literature</phrase> has mostly focused on their <phrase>external validity</phrase>, we believe that there are a number of other issues that need to be investigated in <phrase>empirical studies</phrase> with students. In our past <phrase>research</phrase>, we have identified four viewpoints, each of which needs to be taken into account when carrying out successful <phrase>empirical studies</phrase> with students: researchers, teachers, students, and <phrase>industry</phrase> professionals. Each viewpoint can be seen as a stakeholder of an <phrase>empirical study</phrase> with students, with specific and possibly conflicting goals. The stakeholders also have risks from participating in <phrase>empirical studies</phrase>, which need to be identified and minimized. At any rate, the final goal of carrying out <phrase>empirical studies</phrase> with students is carrying out <phrase>empirical studies</phrase> in <phrase>industrial</phrase> organizations and establishing collaborations with them. It is therefore useful to involve <phrase>industry</phrase> professionals in <phrase>empirical studies</phrase> with students, and they should actually <phrase>play</phrase> all of the stakeholders' roles. Professionals as students. This is the case of <phrase>industrial</phrase> training or continuous <phrase>education</phrase>, and it may be the case of the participation of <phrase>industrial</phrase> professionals in <phrase>university</phrase> <phrase>software engineering</phrase> classes. This could help establish a strong <phrase>communication</phrase> <phrase>channel</phrase> between <phrase>academia</phrase> and <phrase>industry</phrase> by showing empirical <phrase>software engineering</phrase> may provide value added to them. Professionals as customers. Professionals can <phrase>play</phrase> the role of the customers for the <phrase>empirical studies</phrase> with students in <phrase>software engineering</phrase> classes. This may not entail any direct or deep involvement with the <phrase>empirical study</phrase> itself. Showing interesting <phrase>results</phrase> may help establish a good collaboration. Professionals as researchers. Empirical investigations may be at least partially designed and run in the context of <phrase>software engineering</phrase> courses by <phrase>industrial</phrase> professionals who are interested in the <phrase>research</phrase> <phrase>results</phrase>. Here, the <phrase>degree</phrase> of involvement is certainly higher than in the previous case, as is the interest in cooperating with <phrase>academia</phrase>. Professionals as teachers. Professionals may be invited by <phrase>academic</phrase> institutions to share their expertise with students, for a few lessons in a class or teaching entire classes. In this context, professionals may be willing to carry out studies with students and even be the main driving force behind them. Being both teachers and researchers, professionals will have to find an optimal <phrase>trade</phrase>-off between the conflicting goals of either role. Carrying out <phrase>empirical studies</phrase> with students is a learning experience for professionals (as 
Trust Inference in Online <phrase>Social Networks</phrase> We study the problem of trust inference in signed <phrase>social networks</phrase>, in which, in addition to rating items, users can also indicate their disposition towards each other through directional signed links. We explore the problem in a semisupervised setting, where given a small fraction of signed edges we classify the remaining edges by leveraging contextual <phrase>information</phrase> (i.e. the users' ratings). In <phrase>order</phrase> to <phrase>model</phrase> user behavior, we use <phrase>deep learning</phrase> <phrase>algorithms</phrase> i.e. a variation of <phrase>Restricted Boltzmann machine</phrase> and Autoencoders for user encoding and edge classification respectively. We evaluate our approach on a <phrase>large-scale</phrase> <phrase>real-world</phrase> dataset and show that it outperforms <phrase>state</phrase>-of-the <phrase>art</phrase> methods.
<phrase>Digital Humanities</phrase> 2010 <phrase>Digital Humanities</phrase> Internships: Creating a <phrase>Model</phrase> Ischool-<phrase>digital Humanities</phrase> <phrase>Center</phrase> Partnership Creative partnership between <phrase>computer science</phrase> and the <phrase>humanities</phrase> what we now call "<phrase>digital humanities</phrase>" is the <phrase>cornerstone</phrase> of the <phrase>digital revolution</phrase>. Cathy Davidson writes (2008) that "perhaps we need to see <phrase>technology</phrase> and the <phrase>humanities</phrase> not as a <phrase>binary</phrase> but as two sides of a necessarily interdependent, conjoined, and mutually constitutive set of intellectual, educational, social, <phrase>political</phrase>, and economic practices." Significant educational challenges exist, however, in creating a cadre of professionals who understand the intellectual context of <phrase>digital humanities</phrase> <phrase>research</phrase> and who are also capable of building the supporting <phrase>infrastructure</phrase> of <phrase>digital</phrase> collections, tools, and services (de Smedt 2002). The <phrase>American Council of Learned Societies</phrase>' groundbreaking <phrase>report</phrase> Our <phrase>Cultural</phrase> <phrase>Commonwealth</phrase>: <phrase>Report</phrase> of the Commission on <phrase>Cyberinfrastructure</phrase> for the <phrase>Humanities</phrase> and <phrase>Social Sciences</phrase> focuses attention on the need to "cultivate <phrase>leadership</phrase> in support of <phrase>cyberinfrastructure</phrase> from within the <phrase>humanities</phrase> and <phrase>social sciences</phrase>, encourage <phrase>digital</phrase> <phrase>scholarship</phrase>, develop and maintain <phrase>open standards</phrase> and robust tools, and create extensive and reusable <phrase>digital</phrase> collections" (<phrase>ACLS</phrase> 2006, p. 4). An international network of <phrase>digital humanities</phrase> centers creates and develops access to the <phrase>digital</phrase> documents, images, languages, <phrase>sound</phrase>, and <phrase>film</phrase> that constitute the <phrase>human</phrase> record and facilitate its understanding. In a quite separate but potentially <phrase>symbiotic</phrase> movement, graduate schools of <phrase>information</phrase> in the <phrase>United States</phrase> and elsewhere are producing technologically sophisticated professionals with deep backgrounds in and commitments to the <phrase>humanities</phrase>. Schools of <phrase>Information</phrase>, or "iSchools" have emerged from a two-decade <phrase>long</phrase> <phrase>era</phrase> of consolidation and reform, during which traditional schools of <phrase>library science</phrase> struggled with irrelevancy, diminished scale, and a fundamental societal transformation in the use of new and emerging technologies (Sawyer 2008). In <phrase>North America</phrase>, twenty-four iSchools have formed a <phrase>caucus</phrase> (http: //www.ischools.org/) to advance a common agenda regarding the future of <phrase>information</phrase> studies. John <phrase>Unsworth</phrase> (2007) notes that <phrase>digital humanities</phrase> centers can establish new working relationships between <phrase>humanities</phrase> faculty and iSchool programs. iSchool faculty "are about half from other disciplines, and <phrase>humanities</phrase> <phrase>computing</phrase> is very much about <phrase>information</phrase> <phrase>organization</phrase>, <phrase>ontologies</phrase>, taxonomies, schema, preservation, <phrase>interface design</phrase>, and other issues that are studied and taught in [iSchool] programs. The [iSchool] connection also would help to activate the <phrase>NEH</phrase>/IMLS connection, as well as the <phrase>NSF</phrase> <phrase>cyberinfrastructure</phrase> connection." While the move to develop <phrase>digital humanities</phrase> centers has demonstrated great successes, it has also meant the development of a number of unique but remote archives that are in danger of being <phrase>lost</phrase>. <phrase>Universities</phrase> in this 
Introducing Tpack Technological <phrase>Pedagogical Content Knowledge</phrase> What is TPACK? Technological <phrase>Pedagogical Content Knowledge</phrase> (TPACK) attempts to capture some of the essential qualities of <phrase>knowledge</phrase> required by teachers for <phrase>technology</phrase> integration in their teaching. At the <phrase>heart</phrase> of the TPCK framework, is the complex interplay of three primary forms of <phrase>knowledge</phrase>: Content (C), <phrase>Pedagogy</phrase> (P), & <phrase>Technology</phrase> (T) and their intersections. Considering the intersections we get:-<phrase>Pedagogical Content Knowledge</phrase> (PCK): Taking P and C together, we get <phrase>knowledge</phrase> of <phrase>pedagogy</phrase> that is applicable to the teaching of specific content-Technological Content <phrase>Knowledge</phrase> (TCK) At the intersection of T and C is the <phrase>knowledge</phrase> of the relationship between <phrase>technology</phrase> and content-Technological Pedagogical <phrase>Knowledge</phrase> (TPK)-the existence, components and capabilities of various technologies as they are used in the settings of <phrase>teaching and learning</phrase> And finally, at the intersection of all three <phrase>knowledge</phrase> domains is-Technological <phrase>Pedagogical Content Knowledge</phrase> (TPCK) The intersection of all three components characteristic of true <phrase>technology</phrase> integration and the negotiation of the relationships between these three components of <phrase>knowledge</phrase>. True <phrase>technology</phrase> integration is understanding and negotiating the relationships between these three components, as they <phrase>play</phrase> out in specific contexts. A <phrase>teacher</phrase> capable of negotiating these relationships represents a form of expertise different from, and greater than, the <phrase>knowledge</phrase> of a disciplinary expert (say a <phrase>mathematician</phrase> or a <phrase>historian</phrase>), a <phrase>technology</phrase> expert (a computer <phrase>scientist</phrase>) and a pedagogical expert (an experienced <phrase>educator</phrase>). Effective <phrase>technology</phrase> integration for <phrase>pedagogy</phrase> around specific <phrase>subject matter</phrase> requires developing sensitivity to the dynamic, transactional, relationship between all three components. There is no <phrase>single</phrase> technological <phrase>solution</phrase> that applies for every <phrase>teacher</phrase>, every course, or every view of teaching. Rather, solutions lie in the ability of a <phrase>teacher</phrase> to flexibly navigate the spaces defined by these key elements. The critical role of <phrase>teacher</phrase> <phrase>creativity</phrase>: Most technological tools (think word-processors or compasses) have not been designed for educational purposes. Moreover the rapid rate of <phrase>technology</phrase> change (think <phrase>Web 2.0</phrase> services on hand held devices) leads to increasing <phrase>pressure</phrase> on teachers to keep up with these new tools and devise new ways to integrate them in their teaching. This means that teachers need to go beyond " functional fixedness " and instead need to creatively repurpose these tools to make them pedagogically viable. This requires a <phrase>deep understanding</phrase> not just of the <phrase>technology</phrase> but also of the content to be <phrase>covered</phrase>, the pedagogical approaches to be supported and the contexts within which teaching/learning is to happen. Situational <phrase>creativity</phrase>, 
Generalized Conflict Learning for <phrase>Hybrid</phrase> Discrete/linear Optimization Generalized Conflict Learning for <phrase>Hybrid</phrase> Discrete/linear Optimization Conflict-<phrase>directed</phrase> search <phrase>algorithms</phrase> have formed the core of practical, <phrase>model</phrase>-based reasoning systems for the last three decades. In many of these applications there is a series of discrete constraint <phrase>optimization problems</phrase> and a conflict-<phrase>directed</phrase> <phrase>search algorithm</phrase>, which uses conflicts in the <phrase>forward</phrase> search step to focus search away from known infeasibilities and towards the optimal <phrase>solution</phrase>. In the <phrase>arena</phrase> of <phrase>model</phrase>-based <phrase>autonomy</phrase>, discrete systems, like deep space probes, have given way to more agile systems, such as coordinated vehicle control, which must robustly control their continuous dynamics. Controlling these systems requires optimizing over continuous, as well as discrete variables, using linear and non-linear as well as logical constraints. This <phrase>paper</phrase> explores the development of <phrase>algorithms</phrase> for solving <phrase>hybrid</phrase> discrete/linear <phrase>optimization problems</phrase> that use conflicts in the <phrase>forward</phrase> search direction, generalizing from the conflict-<phrase>directed</phrase> search <phrase>algorithms</phrase> of <phrase>model</phrase>-based reasoning. We introduce a novel <phrase>algorithm</phrase> called Generalized Conflict-<phrase>directed</phrase> Branch and Bound (<phrase>GCD</phrase>-BB). <phrase>GCD</phrase>-BB extends traditional Branch and Bound (B&B), by first constructing conflicts from nodes of the search <phrase>tree</phrase> that are found to be infeasible or sub-optimal, and then by using these conflicts to guide the <phrase>forward</phrase> search away from known infeasible and sub-optimal states. We evaluate <phrase>GCD</phrase>-BB empirically on a <phrase>range</phrase> of <phrase>test</phrase> problems of coordinated air vehicle control. <phrase>GCD</phrase>-BB demonstrates a substantial improvement in performance compared to a traditional B&B <phrase>algorithm</phrase>, applied to either disjunctive linear programs or an equivalent <phrase>binary</phrase> <phrase>integer</phrase> program encoding. Acknowledgments First of all, I would like to thank my advisor, <phrase>Brian Williams</phrase>, for his guidance and encouragement on my <phrase>research</phrase> and working so hard with me to make the <phrase>thesis</phrase> deadline. I would like to thank my caring roommates, Caroline Maier and Jit Kee Chin, especially Caroline, for feeding and taking care of me when I was overwhelmed by work, and cheering me up when I was down. They are not just my roommates; they are my <phrase>family</phrase>. I would like to thank my parents, for their unconditional <phrase>love</phrase> and support, and for their care and <phrase>patience</phrase> during the time when I was stuck in <phrase>China</phrase> for 9 months. I would like to thank MERS group, for making our lab a comfortable and stimulating part of my <phrase>life</phrase>. Especially, Thomas Laut, for providing <phrase>test</phrase> problems for my <phrase>algorithm</phrase>, and giving immediate and helpful comments on my <phrase>thesis</phrase>, Lars <phrase>Blackmore</phrase>, for the insightful discussions we had, the comments he gave on the early draft of my 
Deep-level <phrase>acoustic</phrase>-to-articulatory mapping for DBN-HMM based <phrase>phone recognition</phrase> In this <phrase>paper</phrase> we experiment with methods based on <phrase>Deep Belief</phrase> Networks (DBNs) to recover measured articulatory <phrase>data</phrase> from speech <phrase>acoustics</phrase>. Our <phrase>acoustic</phrase>-to-articulatory mapping (AAM) processes go through multi-layered and hierarchical (i.e., deep) representations of the <phrase>acoustic</phrase> and the articulatory domains obtained through <phrase>unsupervised learning</phrase> of DBNs. The <phrase>unsupervised learning</phrase> of DBNs can serve two purposes: (i) <phrase>pre-training</phrase> of the <phrase>Multi-layer</phrase> Perceptrons that perform AAM; (<phrase>ii</phrase>) transformation of the articulatory domain that is recovered from <phrase>acoustics</phrase> through AAM. The recovered artic-ulatory features are combined with MFCCs to compute phone posteriors for <phrase>phone recognition</phrase>. Tested on the MOCHA-TIMIT corpus, the recovered articulatory features, when combined with MFCCs, <phrase>lead</phrase> to up to a remarkable 16.6% relative phone error reduction w.r.t. a phone recognizer that only uses MFCCs.
<phrase>Neural Network</phrase> Based Multi-Factor Aware Joint Training for Robust <phrase>Speech Recognition</phrase> Although great progress has been made in <phrase>automatic speech recognition</phrase> ASR, significant performance degradation still exists in noisy environments. In this <phrase>paper</phrase>, a novel factor-aware training framework, named <phrase>neural network</phrase>-based multifactor aware joint training, is proposed to improve the <phrase>recognition accuracy</phrase> for noise robust <phrase>speech recognition</phrase>. This approach is a structured <phrase>model</phrase> which integrates several different functional modules into one computational deep <phrase>model</phrase>. We explore and extract <phrase>speaker</phrase>, phone, and environment factor representations using <phrase>deep neural networks</phrase> DNNs, which are integrated into the main ASR DNN to improve <phrase>classification accuracy</phrase>. In addition, the hidden activations in the main ASR DNN are used to improve factor extraction, which in turn helps the ASR DNN. All the <phrase>model</phrase> parameters, including those in the ASR DNN and factor extraction DNNs, are jointly optimized under the multitask learning framework. Unlike prior traditional techniques for the factor-aware training, our approach requires no explicit separate stages for factor extraction and adaptation. Moreover, the proposed <phrase>neural network</phrase>-based multifactor aware joint training can be easily combined with the conventional factor-aware training which uses the explicit factors, such as i-<phrase>vector</phrase>, noise <phrase>energy</phrase>, and T60 value to obtain additional improvement. The <phrase>proposed method</phrase> is evaluated on two main noise robust tasks: the AMI <phrase>single</phrase> distant <phrase>microphone</phrase> task in which <phrase>reverberation</phrase> is the main concern, and the Aurora4 task in which multiple noise types exist. Experiments on both tasks show that the proposed <phrase>model</phrase> can significantly reduce word <phrase>error rate</phrase> WER. The best configuration achieved more than 15% relative reduction in WER over the baselines on these two tasks.
<phrase>Concrete</phrase> <phrase>Programming</phrase>: the Use of Small <phrase>Robots</phrase> in <phrase>Primary Schools</phrase> Small <phrase>robots</phrase> are very simple <phrase>computers</phrase> that can move autonomously. Their use in <phrase>primary schools</phrase> allows pupils to have <phrase>concrete</phrase> yet full <phrase>programming</phrase> experiences at the age in which <phrase>Piaget</phrase> situates the <phrase>concrete</phrase> operational stage of <phrase>cognitive development</phrase>. Indeed, for their first robotic activities, pupils think of paths where the <phrase>robot</phrase> moves <phrase>forward</phrase>, decides which direction to go when getting to a crossroad or repeats part of its previous trip. In planning a path for their <phrase>robot</phrase>, children walk it themselves thus finding out its successive parts and related features. Pupils in our classes use NQCBaby and NXCJunior <phrase>programming languages</phrase> for the <phrase>formal specification</phrase> of robot's behaviour. These languages are textual languages, <phrase>mother-tongue</phrase>-based and <phrase>Logo</phrase>-like, in <phrase>order</phrase> to be oriented to children rather than to <phrase>robots</phrase>. Thus, when <phrase>programming</phrase> small <phrase>robots</phrase>, children are introduced to a deep computer competence because they deal with the <phrase>basic</phrase> blocks of algorithmics (<phrase>sequence</phrase>, selection and iteration) and learn how to specify them.
Detecting Sentence Boundaries in <phrase>Sanskrit</phrase> Texts The <phrase>paper</phrase> applies a deep <phrase>recurrent neural network</phrase> to the task of sentence boundary detection in <phrase>Sanskrit</phrase>, an important, yet underresourced ancient <phrase>Indian</phrase> <phrase>language</phrase>. The <phrase>deep learning</phrase> approach improves the F scores set by a metrical baseline and by a <phrase>Conditional Random Field</phrase> classifier by more than 10%.
Facilitating Co-<phrase>Construction</phrase> by Social Practice <phrase>Design</phrase>: a Case of Geodistributed Employees Facing the <phrase>Design</phrase> of <phrase>Model</phrase>-Based Enterprise Systems This <phrase>paper</phrase> describes the experiences of practicing social practice <phrase>design</phrase> (<phrase>SPD</phrase>) activities with user groups, in geographically distributed, collaborating <phrase>manufacturing</phrase> companies, struggling with the introduction of <phrase>model</phrase> based enterprise systems. Within a <phrase>European</phrase> project, we observed and routinely analyzed ongoing development and assessment work of <phrase>model</phrase> based technologies and methodologies, in these companies. Based on an <phrase>ethnographic</phrase> study of Modelling sessions and Validation sessions, we performed an in depth analysis of people <phrase>semantic</phrase> and pragmatic perspectives (a necessary and needed 'second step back'), and identified core disconnects on modelling concept use and <phrase>language</phrase>, and on motivations and goals, between <phrase>technology</phrase> designers modellers-and <phrase>domain experts</phrase> users-clearly hindering project progress-. These disconnects were addressed with user groups in the form of <phrase>SPD</phrase> sessions ('second <phrase>order</phrase>' activities), which consisted mostly in a series of <phrase>design</phrase> <phrase>game</phrase> and scenario-building workshops, enriched by open conversations and perspective sharing and comparison. The <phrase>paper</phrase> describes how these <phrase>SPD</phrase> sessions facilitated the creation of sense making and trust, enabling participants to engage and learn, and to <phrase>act</phrase> as change agents in the project, opening the way to co-<phrase>construction</phrase> of solutions with other <phrase>actors</phrase>. Observations of Modelling and Validation sessions showed that participants could not automatically build on a <phrase>deep understanding</phrase> of modelling and its <phrase>trade</phrase>-offs; they adopted the representational conventions they had learned to use. Lack of sense making and lack of co-<phrase>construction</phrase> were observed, along with lack of facilitation for genuinely participative conditions. Modeller-guided Modelling sessions showed no appropriation of object decomposition and relationship structures by <phrase>domain experts</phrase>, nor contribution from users to <phrase>leadership</phrase> in the modelling process; only imposition of hierarchical structures by modellers, in the midst of a <phrase>cloud</phrase> of mistrust and suspicion. Validation sessions of the <phrase>model-based</phrase> approach showed that 'common' users do not perceive the value of the approach, as they have not been helped to gain a <phrase>conceptual understanding</phrase> of modelling, of the <phrase>trade</phrase>-offs of abstractions, and of how a <phrase>model</phrase> may productively interact with work practices. In this distributed project, different concepts of various user groups all conflicted with modellers' concept. The <phrase>SPD</phrase> facilitation interventions helped participants in stepping back from the " official view " of the work process created in the course of the project, and in focussing more on their own experiences, opening up for <phrase>creativity</phrase>. <phrase>SPD</phrase> events were grounded in the belief that, when it comes to one's own things, people with no special <phrase>knowledge</phrase> 
A framework for <phrase>games</phrase> <phrase>literacy</phrase> and understanding <phrase>games</phrase> Based on <phrase>research</phrase> that studied the challenges and difficulties faced by students taking <phrase>games</phrase> studies and <phrase>game design</phrase> courses, we propose that, while many students enrolled in <phrase>games</phrase> <phrase>education</phrase> programs are adept at playing <phrase>games</phrase>, they are usually neither <phrase>games</phrase> literate nor do they have a <phrase>deep understanding</phrase> of <phrase>games</phrase>. In this article we provide a framework that can be used to evaluate and assess <phrase>games</phrase> <phrase>literacy</phrase>. Using Gee's notion of <phrase>literacy</phrase>, we propose that a <phrase>deep understanding</phrase> of <phrase>games</phrase> involves having the ability to explain, discuss, describe, frame, situate, interpret, and/or position <phrase>games</phrase> (1) in the context of <phrase>human</phrase> <phrase>culture</phrase> (<phrase>games</phrase> as a <phrase>cultural</phrase> artifacts), (2) in the context of other <phrase>games</phrase>, (3) in the context of the technological platform on which they are executed, (4) and by deconstructing them and understanding their components, how they interact, and how they facilitate certain experiences in players. We describe each of these aspects and also discuss two educational lenses that can be used to help contextualize what it means to understand and learn about <phrase>games</phrase> as well as support <phrase>games</phrase> <phrase>literacy</phrase> in students.
<phrase>Knowledge</phrase> Details in Web Forums: How <phrase>High</phrase> or Low above the Ground? While <phrase>entertainment</phrase> web forums provide a dynamic medium for interaction, not many researchers feel the need to go deeply into the contents. One of the reasons behind this attitude lies on a widely perceived assumption that web forums do not deal with <phrase>knowledge</phrase> <phrase>matter</phrase> and have the <phrase>inclination</phrase> to take place only as small <phrase>talk</phrase>. In this <phrase>paper</phrase> we will mainly inspect the components of <phrase>knowledge</phrase> as are being handled by the six servicemen of Kipling's 5W1H framework. Based on our observation on a learning zone that contains 35 forum topics and 789 messages in a web forum (http://asamboi.org), we found that at least several topics were dealing with deep <phrase>knowledge</phrase> contents. From our analysis, we have found that the depth of <phrase>knowledge</phrase> details is reasonably significant particularly when responding to specific question demand. On <phrase>average</phrase> the depth of details obtained were ranked as 'Who' Siapa, 'What' <phrase>Apa</phrase>, 'How' Bagaimana, 'Why' Kenapa, 'Where' <phrase>Mana</phrase> and 'When' Bila once sorted descendingly. We conclude that web forums are a good <phrase>web-resource</phrase> for <phrase>digital</phrase> age <phrase>learning styles</phrase> as it provides detailed <phrase>knowledge</phrase> for networked learning.
Statistical Soft <phrase>Error Rate</phrase> (SSER) Analysis for Scaled <phrase>CMOS</phrase> Designs This article re-examines the soft error effect caused by <phrase>radiation</phrase>-induced particles beyond the <phrase>deep submicron</phrase> regime. Considering the impact of process variations, <phrase>voltage</phrase> pulse widths of transient faults are found no longer monotonically diminishing after propagation, as they were formerly. As a result, the soft error rates in scaled <phrase>electronic</phrase> designs escape traditional static analysis and are seriously underestimated. In this article we formulate the statistical soft <phrase>error rate</phrase> (SSER) problem and present two frameworks to cope with the aforementioned sophisticated issues. The <i>table-lookup</i> framework captures the change of transient-fault <phrase>distributions</phrase> implicitly by using a <phrase>Monte-Carlo</phrase> approach, whereas the <i>SVR-learning</i> framework does the task explicitly by using statistical learning theory. <phrase>Experimental</phrase> <phrase>results</phrase> show that both frameworks can more accurately estimate SERs than static approaches do. Meanwhile, the SVR-learning framework outperforms the table-lookup framework in both SER accuracy and runtime.
Boosting textual compression in optimal linear time We provide a <phrase>general</phrase> boosting technique for Textual <phrase>Data Compression</phrase>. Qualitatively, it takes a good compression <phrase>algorithm</phrase> and turns it into an <phrase>algorithm</phrase> with a better compression performance guarantee. It displays the following remarkable properties: (a) it can turn <i>any memoryless</i> compressor into a compression <phrase>algorithm</phrase> that uses the &#8220;best possible&#8221; contexts; (b) it is very simple and <i>optimal</i> in terms of time; and (c) it admits a <phrase>decompression</phrase> <phrase>algorithm</phrase> again optimal in time. To the best of our <phrase>knowledge</phrase>, this is the first boosting technique displaying these properties.Technically, our boosting technique builds upon three main ingredients: the Burrows--Wheeler Transform, the <phrase>Suffix Tree</phrase> <phrase>data structure</phrase>, and a <phrase>greedy algorithm</phrase> to process them. Specifically, we show that there exists a proper <phrase>partition</phrase> of the Burrows--Wheeler Transform of a string <i>s</i> that shows a deep <phrase>combinatorial</phrase> relation with the <i>k</i>th <phrase>order</phrase> <phrase>entropy</phrase> of <i>s</i>. That <phrase>partition</phrase> can be identified via a greedy processing of the <phrase>suffix tree</phrase> of <i>s</i> with the aim of minimizing a proper <phrase>objective function</phrase> over its nodes. The final compressed string is then obtained by compressing individually each <phrase>substring</phrase> of the <phrase>partition</phrase> by means of the base compressor we wish to boost.Our boosting technique is inherently <phrase>combinatorial</phrase> because it does not need to assume any prior <phrase>probabilistic model</phrase> about the source emitting <i>s</i>, and it does not deploy any training, <phrase>parameter estimation</phrase> and learning. Various corollaries are derived from this main achievement. Among the others, we show analytically that using our booster, we get better compression <phrase>algorithms</phrase> than some of the best existing ones, that is, LZ77, LZ78, PPMC and the ones derived from the Burrows--Wheeler Transform. Further, we settle analytically some <phrase>long</phrase>-standing open problems about the algorithmic structure and the performance of BWT-based compressors. Namely, we provide the first <phrase>family</phrase> of BWT <phrase>algorithms</phrase> that do not use Move-To-Front or <phrase>Symbol</phrase> Ranking as a part of the compression process.
0 Th World <phrase>Congress</phrase> on Structural and Multidisciplinary Optimization <phrase>Probability</phrase> Collectives for Solving <phrase>Truss</phrase> Structure Problems 1. Abstract The approach of <phrase>Probability</phrase> Collectives (<phrase>PC</phrase>) in the <phrase>Collective Intelligence</phrase> (<phrase>COIN</phrase>) framework is one of the emerging <phrase>Artificial Intelligence</phrase> approaches dealing with the complex problems in a distributed way. It decomposes the entire system into subsystems and treats them as a group of learning, rational and self interested agents or a <phrase>Multi-Agent</phrase> System (MAS). These agents iteratively select their strategies to optimize their individual local goal which also makes the system to achieve the global optimum. The approach of <phrase>PC</phrase> has been tested and validated by solving a <phrase>variety</phrase> of practical problems in continuous domain. This <phrase>paper</phrase> demonstrates the ability of <phrase>PC</phrase> solving 2-D space <phrase>truss</phrase> structure and 3-D <phrase>truss</phrase> structure <phrase>design</phrase> problems with discrete as well as continuous variables. The approach is shown to be producing competent and sufficiently robust <phrase>results</phrase>. The associated strengths, weaknesses are also discussed. The <phrase>solution</phrase> to these problems indicates that the approach of <phrase>PC</phrase> can be further efficiently applied to solve a <phrase>variety</phrase> of practical/<phrase>real world</phrase> problems. 2. 3. Introduction In the framework of <phrase>Collective Intelligence</phrase> (<phrase>COIN</phrase>), the <phrase>Artificial Intelligence</phrase> (<phrase>AI</phrase>) tool referred to as <phrase>Probability</phrase> Collectives (<phrase>PC</phrase>) is becoming popular for modeling and controlling distributed <phrase>Multi-Agent</phrase> System (MAS) [1-15]. It was inspired from a sociophysics viewpoint, with deep connections to <phrase>Game Theory</phrase>, <phrase>Statistical Physics</phrase>, and Optimization [1, 2]. According to [1, 2, 9-11], the key characteristics of the <phrase>PC</phrase> methodology such as its ability to accommodate discrete and continuous variables as well as irregular and noisy functions, tolerance to subsystem/agent failure, ability to provide sensitivity <phrase>information</phrase> and ability to handle uncertainty in terms of <phrase>probability</phrase>, use of <phrase>homotopy</phrase> <phrase>function</phrase> to make the <phrase>solution</phrase> jump out of possible <phrase>local minima</phrase>, ability to avoid the <phrase>tragedy</phrase> of <phrase>commons</phrase>, <phrase>high</phrase> <phrase>scalability</phrase>, ability to achieve unique <phrase>Nash Equilibrium</phrase>, etc. makes it a very competitive choice over other contemporary <phrase>algorithms</phrase>. The approach of <phrase>PC</phrase> has been applied in variegated areas such as <phrase>airplane</phrase> fleet assignment problem [12] and various cases of the Multiple Traveling Salesmen Problems (MTSPs) [4, 7], continuous constrained problems such as benchmark <phrase>test</phrase> problems [7, 9, 13-15], two variations of the <phrase>Circle</phrase> Packing Problem (CPP) [5], <phrase>Sensor</phrase> Network Coverage Problem [10] as well as <phrase>fault-tolerant</phrase> system in association with the CPP [11]. Furthermore, the segmented <phrase>beam</phrase> problem [8], multimodal, nonlinear and non-separable <phrase>test</phrase> problems comparing the performance with <phrase>Genetic Algorithm</phrase> (GA) [24] as well as joint optimization of the routing and <phrase>resource allocation</phrase> 
Multimodal Feature <phrase>Fusion</phrase> for 3D Shape Recognition and Retrieval hree-dimensional shapes are used extensively in fields such as mechanical <phrase>design</phrase>, <phrase>multimedia</phrase> <phrase>games</phrase>, <phrase>architecture</phrase>, and <phrase>medical diagnosis</phrase>. 1 All of these applications need to store, recognize , and retrieve 3D models effectively and automatically. Because the characteristics of 3D shapes differ from those of text and images, traditional classification and retrieval techniques cannot be applied directly to 3D objects. Hence, 3D shape analysis remains a challenging issue. Researchers have proposed numerous solutions to 3D shape recognition, matching, and retrieval problems. 1,2 Although tremendous advancements have been made, current methods are still far from satisfactory for applying 3D objects in more realms. <phrase>Geometry</phrase>-and view-based methods, for example, only use partial <phrase>information</phrase> from a 3D object (see the " <phrase>Geometry</phrase>-and View-Based Methods for 3D Shape Analysis " sidebar). <phrase>Geometry</phrase>-based methods use the complex <phrase>topological</phrase> structure and geometric properties of the 3D <phrase>model</phrase> but ignore the visual similarities between 3D objects. Conversely, view-based methods only consider the visual characteristics of a <phrase>model</phrase> from different viewing angles. These methods neglect either the extrinsic features or intrinsic properties of 3D objects. What will happen if we combine different modality <phrase>information</phrase> in a creative and effective way? <phrase>Information</phrase> in the <phrase>real world</phrase> has various manifestation modalities. Each of these typically carries different <phrase>information</phrase> and is rarely <phrase>independent</phrase> of others. For example, <phrase>video</phrase> contains visual and audio signals, images are often associated with captions and tags, and 3D models can be described by multiview images captured from different angles and 3D shape features. Because these totally different modalities depict the same object, some highly nonlinear relationships exist between them. However, different modalities have different representations and structures. For example, images are often represented with <phrase>real-valued</phrase> <phrase>pixel</phrase> intensities or the outputs of feature detectors, whereas 3D shapes are usually represented with 3D features that contain <phrase>information</phrase> about geometric attributes and <phrase>topological</phrase> structures. This makes it hard to discover the nonlinear relationships between features across modalities. This article proposes fusing the different modality <phrase>data</phrase> of 3D shapes into a <phrase>deep learning</phrase> framework. Our core idea is to better mine the deep correlations of different modalities. <phrase>High-level</phrase> features are first extracted using two <phrase>deep belief</phrase> networks (DBNs), one for <phrase>geometry</phrase>-based modality with the input of a <phrase>geodesic</phrase>-aware bag of features (GA-BoF) and the other for view-based modality with the input of a bag of visual feature (BoVF). 3 We then use a restricted Boltz-<phrase>mann machine</phrase> (RBM) 4 to associate the 
How to Intelligently Distribute <phrase>Training Data</phrase> to Multiple Compute Nodes: Distributed <phrase>Machine Learning</phrase> via Submodular Partitioning In this <phrase>paper</phrase> we investigate the problem of <phrase>training data</phrase> partitioning for parallel learning of <phrase>statistical models</phrase>. Motivated by [10], we utilize submodular functions to <phrase>model</phrase> the utility of <phrase>data</phrase> subsets for training <phrase>machine learning</phrase> classifiers and formulate this problem mathematically as submodular partitioning. We introduce a simple and scalable <phrase>greedy algorithm</phrase> that near-optimally solves the submodular partitioning problem. We empirically demonstrate the efficacy of the <phrase>proposed algorithm</phrase> to obtain <phrase>data</phrase> partitioning for distributed optimization of convex and <phrase>deep neural network</phrase> objectives. Empirical evidences suggest that the intelligent <phrase>data</phrase> partitioning <phrase>produced</phrase> by the <phrase>proposed framework</phrase> leads to faster convergence in the case of distributed <phrase>convex optimization</phrase>, and better resulting models in the case of parallel <phrase>neural network</phrase> training.
Learning to Learn with <phrase>Compound</phrase> HD Models We introduce HD (or " Hierarchical-Deep ") models, a new compositional learning <phrase>architecture</phrase> that integrates <phrase>deep learning</phrase> models with structured hierarchical <phrase>Bayesian</phrase> models. Specifically we show how we can learn a hierarchical <phrase>Dirichlet</phrase> process (HDP) prior over the activities of the top-<phrase>level features</phrase> in a Deep Boltz-<phrase>mann Machine</phrase> (<phrase>DBM</phrase>). This <phrase>compound</phrase> HDP-<phrase>DBM</phrase> <phrase>model</phrase> learns to learn novel concepts from very few <phrase>training examples</phrase>, by learning <phrase>low-level</phrase> generic features, <phrase>high-level</phrase> features that capture correlations among <phrase>low-level</phrase> features, and a category hierarchy for sharing priors over the <phrase>high-level</phrase> features that are typical of different kinds of concepts. We present efficient learning and inference <phrase>algorithms</phrase> for the HDP-<phrase>DBM</phrase> <phrase>model</phrase> and show that it is able to learn new concepts from very few examples on CIFAR-100 <phrase>object recognition</phrase>, handwritten <phrase>character recognition</phrase> , and <phrase>human</phrase> <phrase>motion capture</phrase> datasets.
Facilitating Constructive Alignment in Power <phrase>Systems Engineering</phrase> <phrase>Education</phrase> Using <phrase>Free and Open-Source Software</phrase> This <phrase>paper</phrase> describes how the use of <phrase>Free and Open Source Software</phrase> (FOSS) can facilitate the application of constructive alignment theory in power <phrase>systems engineering</phrase> <phrase>education</phrase> by enabling the <phrase>deep learning</phrase> approach in power system analysis courses. With this aim, the <phrase>paper</phrase> describes the authors' approach in using the Power System Analysis Toolbox (<phrase>PSAT</phrase>) for <phrase>undergraduate</phrase> and graduate <phrase>education</phrase>. Interviews with former students reveal the positive impact that the use of FOSS in <phrase>general</phrase>, and <phrase>PSAT</phrase> in particular, had on their learning and how it has influenced their <phrase>professional</phrase> <phrase>life</phrase>. Constructive alignment, <phrase>free and open-source software</phrase>, functioning <phrase>knowledge</phrase> , learning activities, power system analysis.
<phrase>Traffic Classification</phrase> through Joint <phrase>Distributions</phrase> of Packet-Level <phrase>Statistics</phrase> Interest in <phrase>traffic classification</phrase>, in both <phrase>industry</phrase> and <phrase>academia</phrase>, has dramatically grown in the past few years. <phrase>Research</phrase> is devoting great efforts to statistical approaches using robust features. In this <phrase>paper</phrase> we propose a classification approach based on the joint distribution of Packet Size (PS) and Inter-Packet Time (IPT) and on <phrase>machine-learning</phrase> <phrase>algorithms</phrase>. Provided <phrase>results</phrase>, obtained using different real traffic traces, demonstrate how the <phrase>proposed approach</phrase> is able to achieve <phrase>high</phrase> (<phrase>byte</phrase>) accuracy (<phrase>till</phrase> 98%) and how the new discriminating features have properties of robustness, which suggest their use in the <phrase>design</phrase> of classification/identification approaches robust to traffic <phrase>encryption</phrase> and protocol <phrase>obfuscation</phrase>. I. INTRODUCTION <phrase>Network traffic</phrase> classification is fundamental to build <phrase>knowledge</phrase> on the use of network links, but it is also a crucial functionality to impose <phrase>security</phrase> or quality-of-service policies, to perform <phrase>accounting</phrase>, and many other relevant tasks. <phrase>Traditional approaches</phrase> based on <phrase>transport</phrase>-layer protocol ports are becoming increasingly unreliable and Deep Payload Inspection (DPI) approaches using packet payload have to cope with increasing network link speed and <phrase>privacy</phrase> issues. At the same time, finding <phrase>alternative</phrase> solutions has been demonstrated to be a non-immediate task. In recent years, <phrase>research</phrase> <phrase>community</phrase> and networking <phrase>industry</phrase> have investigated and developed several approaches. None of them solves the problem definitively and all of them show some drawbacks related to issues such as on-the-field applicability and reliability [1], [2], [3], [4], [5], [6], [7]. Moreover, while the <phrase>state</phrase> of the <phrase>art</phrase> is rapidly improving, <phrase>Internet</phrase> protocols and applications are continuously evolving (<phrase>encryption</phrase> is a notable example closely related to <phrase>traffic classification</phrase>) opening new challenges. In this <phrase>paper</phrase>, we propose a novel approach considering the joint distribution of Packet Size (PS) and Inter-Packet Time (IPT). We apply a strong <phrase>discretization</phrase> to the estimate of their joint <phrase>Probability Density Function</phrase> (<phrase>PDF</phrase>) and using <phrase>machine-learning</phrase> <phrase>algorithms</phrase> like K-Nearest Neighbor (K-NN) and <phrase>Support Vector Machines</phrase> (<phrase>SVM</phrase>). Our approach is heavily based on the following <phrase>research</phrase> result: <phrase>network traffic</phrase> from different applications shows distinctive properties when the traffic is analyzed at packet-level, that is, in terms of PS and
Towards open <phrase>ontology</phrase> learning and filtering Open <phrase>ontology</phrase> learning is the process of extracting a domain <phrase>ontology</phrase> from a <phrase>knowledge</phrase> source in an unsupervised way. Due to its unsupervised <phrase>nature</phrase>, it requires filtering mechanisms to rate the importance and correctness of the extracted <phrase>knowledge</phrase>. This <phrase>paper</phrase> presents OntoCmaps, a <phrase>domain independent</phrase> and open <phrase>ontology</phrase> learning tool that extracts deep <phrase>semantic</phrase> representations from corpora. OntoCmaps generates rich conceptual representations in the form of <phrase>concept maps</phrase> and proposes an innovative filtering mechanism based on metrics from <phrase>graph theory</phrase>. Our <phrase>results</phrase> show that using metrics such as Betweenness, <phrase>PageRank</phrase>, <phrase>Hits</phrase> and <phrase>Degree</phrase> centrality outperforms the <phrase>results</phrase> of standard <phrase>text-based</phrase> metrics (TF-<phrase>IDF</phrase>, Term <phrase>Frequency</phrase>) for concept identification. We propose voting schemes based on these metrics that provide a good performance in relationship identification, which again provides better <phrase>results</phrase> (in terms of precision and F-measure) than other traditional metrics such as <phrase>Frequency</phrase> of co-occurrences. The approach is evaluated against a <phrase>gold standard</phrase> and is compared to the <phrase>ontology</phrase> learning tool Text2Onto. The OntoCmaps generated <phrase>ontology</phrase> is more expressive than Text2Onto <phrase>ontology</phrase> especially in conceptual relationships and leads to better <phrase>results</phrase> in terms of precision, recall and F-measure.
The Role of <phrase>Software</phrase> Process Modeling in Planning <phrase>Industrial</phrase> Measurement Programs Measurement is a necessary prerequisite for <phrase>software</phrase> process improvement. However, few guidelines exist for systematic planning of measurement programs within <phrase>software</phrase> projects. We <phrase>advocate</phrase> beginning with goal-oriented measurement as expressed in the GQM <phrase>paradigm</phrase>. When applying GQM, meaningful refinement of measurement goals requires a <phrase>deep understanding</phrase> of the organization's <phrase>software development</phrase> processes. <phrase>Software</phrase> process modeling can <phrase>act</phrase> as a means to gain this understanding. We provide guidelines that <phrase>state</phrase> how <phrase>software</phrase> processes should be modeled in <phrase>order</phrase> to support the introduction of measurement in a <phrase>software</phrase> project. The process models are used to derive non-intrusive <phrase>data</phrase> collection procedures that satisfy the needs of multiple measurement goals and thereby minimize the <phrase>data</phrase> collection overhead. Finally, we present the <phrase>lessons learned</phrase> from jointly applying <phrase>software</phrase> process modeling and measurement technologies in two <phrase>industrial</phrase> <phrase>software</phrase> projects.
Predicting <phrase>Speaker</phrase> Head Nods and the Effects of Affective <phrase>Information</phrase> During face-to-face conversation, our body is continually in motion, displaying various head, gesture, and posture movements. Based on findings describing the communicative functions served by these <phrase>nonverbal</phrase> behaviors, many virtual agent systems have modeled them to make the virtual agent look more effective and believable. One <phrase>channel</phrase> of <phrase>nonverbal</phrase> behaviors that has received less attention is head movements, despite the important functions served by them. The goal for this work is to build a <phrase>domain-independent</phrase> <phrase>model</phrase> of speaker's head movements that could be used to generate head movements for virtual agents. In this <phrase>paper</phrase>, we present a <phrase>machine learning</phrase> approach for learning models of head movements by focusing on when <phrase>speaker</phrase> head nods should occur, and conduct evaluation studies that compare the nods generated by this work to our previous approach of using handcrafted rules [1]. To learn patterns of <phrase>speaker</phrase> head nods, we use a gesture corpus and rely on the <phrase>linguistic</phrase> and af-fective features of the utterance. We describe the <phrase>feature selection</phrase> process and training process for learning <phrase>hidden Markov models</phrase> and compare the <phrase>results</phrase> of the learned models under varying conditions. The <phrase>results</phrase> show that we can predict <phrase>speaker</phrase> head nods with <phrase>high</phrase> precision (.84) and recall (.89) rates, even without a deep representation of the surface text and that using affective <phrase>information</phrase> can help improve the prediction of the head nods (precision: .89, recall: .90). The evaluation study shows that the nods generated by the <phrase>machine learning</phrase> approach are perceived to be more natural in terms of nod timing than the nods generated by the <phrase>rule-based</phrase> approach.
Designing <phrase>Curriculum</phrase> to Meet National Standards emphasis on memorizing decontextualized scientific facts and more emphasis on students investigating the everyday world and developing <phrase>deep understanding</phrase> from their inquiries. Broadly conceived, inquiry refers to " the diverse ways in which scientists study the natural world and propose explanations based on the evidence derived from their work " (<phrase>NRC</phrase>, 1996, p. 23). By emphasizing <phrase>scientific inquiry</phrase>, the standards challenge the <phrase>education</phrase> and <phrase>science</phrase> communities to transform the very <phrase>heart</phrase> of students' experiences in <phrase>science</phrase> classrooms. In support of the standards, new approaches to <phrase>science</phrase> instruction feature inquiry as essential for <phrase>student</phrase> learning (Krajcik et. The <phrase>spirit</phrase> of the <phrase>science</phrase> <phrase>education</phrase> standards represents a dramatic shift in what and how <phrase>science</phrase> is taught in k-12 classrooms. In <phrase>order</phrase> to enable teachers to accomplish the ambitious agenda advocated by <phrase>AAAS</phrase> and <phrase>NRC</phrase>, educational researchers and <phrase>professional</phrase> educators need to create a <phrase>research</phrase> and development program to support reform (<phrase>Marx</phrase>, et al., 1998). Such an agenda needs to address the full <phrase>range</phrase> of issues associated with reform: <phrase>curriculum</phrase> and <phrase>pedagogy</phrase>, <phrase>management</phrase> and policy, <phrase>teacher</phrase> <phrase>professional</phrase> development, new learning technologies, and <phrase>community</phrase> engagement. By 3 studying the intersection of these issues and developing programs of <phrase>research</phrase>-based practice around them, partnerships of researchers and educators can begin to create the know how to help teachers meet the new standards (Blumenfeld, in press). In this <phrase>paper</phrase> we <phrase>report</phrase> our work on one of these issuescurriculum materials to support reform. Researchers at the <phrase>University</phrase> of <phrase>Michigan</phrase> have been working together with the <phrase>Detroit Public Schools</phrase> to reform <phrase>science</phrase> <phrase>education</phrase> for middle schools. The collaborative work between DPS and UM takes place within two projects funded by the <phrase>National Science Foundation</phrase>-the <phrase>Detroit</phrase> <phrase>Urban</phrase> Systemic Program and the <phrase>Center</phrase> for Learning Technologies in <phrase>Urban</phrase> Schools (LeTUS), which takes as its core challenge the <phrase>infusion</phrase> of <phrase>technology</phrase> to support learning into <phrase>urban</phrase> classrooms. We are documenting situations that influence <phrase>technology</phrase> acquisition, exploring how <phrase>technology</phrase> can be embedded in <phrase>science</phrase> curricula, identifying problems that present barriers to success, and finding local solutions to these problems. When we began this collaborative effort, we found that a <phrase>major</phrase> challenge for imbedding <phrase>technology</phrase> use in <phrase>urban</phrase> schools was the lack of <phrase>curriculum</phrase> materials that match <phrase>science</phrase> content with the appropriate use of learning technologies. To meet this challenge it became necessary to develop materials that simultaneously are suitable for use in schools that serve diverse populations, promote inquiry, are based in <phrase>research</phrase> 
The <<phrase>E</phrase>-<phrase>Game</phrase>> Project: Facilitating the Development of Educational <phrase>Adventure Games</phrase> <phrase>Game</phrase> based learning can be seen as an interesting approach to learning, but the complexity of today's <phrase>videogames</phrase> demands from developers a robust foundation in a <phrase>variety</phrase> of technologies. On the other hand, the educational part of <phrase>game</phrase> based learning requires the active participation of field experts. The collaboration between those experts and <phrase>game</phrase> developers is always difficult. The <<phrase>e</phrase>-<phrase>Game</phrase>> project addresses this problem by offering an authoring environment for educational <phrase>adventure games</phrase> that does not demand a deep formation in <phrase>Information</phrase> and <phrase>Communication</phrase> Technologies. The <phrase>author</phrase> (an expert in a specific field) only needs to write documents that describe the contents of the <phrase>videogame</phrase> following the <<phrase>e</phrase>-<phrase>Game</phrase>> <phrase>XML</phrase> <phrase>syntax</phrase> and feed them to the <phrase>engine</phrase>. In turn, the <phrase>engine</phrase> produces a fully functional <phrase>game</phrase> from those documents.
International Workshop on <phrase>Evolutionary</phrase> <phrase>Rule-Based</phrase> <phrase>Machine Learning</phrase> Workshop (IWERML) Welcome & <phrase>Organization</phrase> It is our great pleasure to welcome you to the International Workshop on <phrase>Evolutionary</phrase> <phrase>Rule-Based</phrase> <phrase>Machine Learning</phrase> Workshop held in conjunction with GECCO 2016. In previous years, this workshop has been hosted as the International Workshop on Learning Classifiers Systems (IWLCS). Since the previous edition, it has been expanded to include all forms of <phrase>evolutionary</phrase> <phrase>rule-based</phrase> <phrase>machine learning</phrase> systems to include a wider set of <phrase>research</phrase> topics and participation. LCSs originated in work by <phrase>Holland</phrase> in the 70's, and aimed to create <phrase>cognitive</phrase> systems that used <phrase>evolutionary computation</phrase> to learn to perform a certain task by interacting with its environment. Since that time, the LCS <phrase>paradigm</phrase> has broadened greatly into an algorithmic framework encompassing many representations, rule discovery mechanisms, and credit assignment schemes. LCS applications have varied greatly including problems in <phrase>data mining</phrase>, classification, <phrase>function</phrase> approximation, <phrase>reinforcement learning</phrase>, and on-line control. Classifier systems are an active <phrase>area</phrase> of <phrase>research</phrase>, fueled by novel reinventions of the LCS <phrase>architecture</phrase> such as those introduced in Wilson's accuracy-based XCS. IWLCS was initiated in 1992 held at the <phrase>NASA</phrase> <phrase>Johnson Space Center</phrase> in <phrase>Houston</phrase>, <phrase>Texas</phrase>. Since 1999, the workshop has been held yearly in conjunction with PPSN in 2000 and 2002, and in conjunction with GECCO in 1999, 2001 and from 2003 until the present. This year's workshop will include two invited papers from eminent researchers, a hands-on lab session to get familiar with LCS, and a panel discussion to answer questions and outline challenges and future directions for LCSs in the age of <phrase>deep learning</phrase>.
Circuit <phrase>Design</phrase> for <phrase>Logic</phrase> <phrase>Automata</phrase> Circuit <phrase>Design</phrase> for <phrase>Logic</phrase> <phrase>Automata</phrase> The <phrase>Logic</phrase> <phrase>Automata</phrase> <phrase>model</phrase> is a <phrase>universal</phrase> <phrase>distributed computing</phrase> structure which pushes parallelism to the <phrase>bit</phrase>-level extreme. This new <phrase>model</phrase> drastically differs from conventional computer architectures in that it exposes, rather than hides, the <phrase>physics</phrase> underlying the computation by accomodating <phrase>data processing</phrase> and storage in a local and distributed manner. Based on <phrase>Logic</phrase> <phrase>Automata</phrase>, highly scalable <phrase>computing</phrase> struc-trues for <phrase>digital</phrase> and analog processing have been developed; and they are verified at the <phrase>transistor</phrase> level in this <phrase>thesis</phrase>. The Asynchronous <phrase>Logic</phrase> <phrase>Automata</phrase> (ALA) <phrase>model</phrase> is derived by adding the temporal <phrase>locality</phrase>, i.e., the asynchrony in <phrase>data</phrase> exchanges, in addition to the spacial <phrase>locality</phrase> of the <phrase>Logic</phrase> <phrase>Automata</phrase> <phrase>model</phrase>. As a demonstration of this incrementally extensible, clockless structure, we designed an ALA <phrase>cell</phrase> <phrase>library</phrase> in 90 nm <phrase>CMOS</phrase> <phrase>technology</phrase> and established a " pick-and-place " <phrase>design</phrase> flow for fast ALA circuit layout. The work flow gracefully aligns the description of computer programs and circuit realizations, providing a simpler and more scalable <phrase>solution</phrase> for <phrase>Application Specific Integrated Circuit</phrase> (<phrase>ASIC</phrase>) designs, which are currently limited by global contraints such as the <phrase>clock</phrase> and <phrase>long</phrase> interconnects. The potential of the ALA circuit <phrase>design</phrase> flow is tested with example applications for <phrase>mathematical</phrase> operations. The same <phrase>Logic</phrase> <phrase>Automata</phrase> <phrase>model</phrase> can also be augmented by relaxing the <phrase>digital</phrase> states into analog ones for interesting analog computations. The Analog <phrase>Logic</phrase> <phrase>Automata</phrase> (AnLA) <phrase>model</phrase> is a merge of the Analog <phrase>Logic</phrase> principle and the <phrase>Logic</phrase> <phrase>Automata</phrase> arhitecture, in which efficient processing is embedded onto a scalable <phrase>construction</phrase>. In <phrase>order</phrase> to study the unique <phrase>property</phrase> of this mixed-signal <phrase>computing</phrase> structure, we designed and fabricated an AnLA <phrase>test</phrase> chip in AMI 0.5m <phrase>CMOS</phrase> <phrase>technology</phrase>. Chip <phrase>tests</phrase> of an AnLA Noise-Locked Loop (<phrase>NLL</phrase>) circuit as well as application <phrase>tests</phrase> of AnLA <phrase>image processing</phrase> and <phrase>Error-Correcting Code</phrase> (ECC) decoding, show large potential of the AnLA structure. Acknowledgments I would like to acknowledge the support of MIT's <phrase>Center</phrase> for <phrase>Bits</phrase> and <phrase>Atoms</phrase> and its sponsors. Thank you to my <phrase>thesis</phrase> supervisor, <phrase>Neil Gershenfeld</phrase>, for his intriguing guidance and encouragement over the past two years. His wide <phrase>knowledge</phrase> span and deep insight have been inspiring me to keep learning and thinking; and his openness to ideas has encouraged me to always seek better solutions to problems I encounter. Thank you for directing the <phrase>Center</phrase> for <phrase>Bits</phrase> and <phrase>Atoms</phrase> and the <phrase>Physics</phrase> and <phrase>Media</phrase> <phrase>Research</phrase> Group, which provide me great <phrase>intellectual freedom</phrase> and wide vision. He has 
<phrase>Pattern Recognition</phrase> in <phrase>Bioinformatics</phrase> Every accumulation of <phrase>data</phrase> in its raw form holds obscure patterns. <phrase>Pattern recognition</phrase> deals with the <phrase>science</phrase> of transforming and classifying entities on the basis of these patterns. It is a vast field as it deals with <phrase>data</phrase> from diverse sources. <phrase>Data</phrase> can be of <phrase>single</phrase> dimensional <phrase>nature</phrase> as in case of <phrase>stock exchanges</phrase> and <phrase>sound</phrase>, two-dimensional as in case of images, and even multidimensional. It has many applications, for example, in <phrase>medical</phrase> <phrase>science</phrase>, it provides origins for <phrase>computer-aided</phrase> diagnosis (<phrase>CAD</phrase>) which supports <phrase>medical</phrase> practitioners in interpretations and finding of diseases. It has other typical applications: <phrase>automatic speech recognition</phrase>; recognition of text in various categories; and automatic recognition of <phrase>human</phrase> faces. Moreover, the <phrase>genetic</phrase> and <phrase>protein structure</phrase> in <phrase>living organisms</phrase> form intrinsic patterns. <phrase>Data</phrase> collected from the decomposition of these <phrase>proteins</phrase> help to identify them and hence to classify the <phrase>protein</phrase>. The ultimate objective is to make machines ideally as intelligent as humans in recognizing such patterns which help to form automated systems for conduction of routine matters. <phrase>Bioinformatics</phrase> deals with development of <phrase>algorithms</phrase> and <phrase>software</phrase> for understanding the biological <phrase>data</phrase>. For analyzing and interpretation of the biological <phrase>data</phrase>, <phrase>bioinformatics</phrase> uses <phrase>mathematics</phrase>, <phrase>statistics</phrase>, computer, and <phrase>engineering</phrase>. There exists a lot of work in <phrase>molecular biology</phrase> using various approaches of <phrase>bioinformatics</phrase> like <phrase>image processing</phrase> and <phrase>machine learning</phrase>. <phrase>Bioinformatics</phrase> not just deals with application of <phrase>pattern recognition</phrase> for <phrase>protein</phrase> classification but it also incorporates use of <phrase>computational intelligence</phrase> in <phrase>protein</phrase> sequencing, <phrase>gene expression</phrase>, <phrase>comparative genomics</phrase>, <phrase>mutation</phrase>, <phrase>disease</phrase> <phrase>genetics</phrase>, and molecular interactive networks. In this <phrase>special issue</phrase>, we focused on <phrase>innovation</phrase> of cutting edge <phrase>technology</phrase> in the fields of <phrase>pattern recognition</phrase> and <phrase>bioinformatics</phrase> with multidimensional scope. The articles published in this issue contain various aspects of PARE. Y. Ren et al. propose an <phrase>algorithm</phrase> Autoregressive <phrase>Bayesian</phrase> spectral <phrase>Regression</phrase> (ABSR) to estimate rhythmicity of a <phrase>gene expression</phrase> profile with <phrase>short</phrase> <phrase>time series</phrase>. G. Zhao et al. propose a <phrase>supervised learning</phrase>-based <phrase>Chinese</phrase> Visible <phrase>Human</phrase> (CVH) <phrase>brain</phrase> tissues segmentation method that uses stacked autoencoder (SAE) to automatically learn the deep <phrase>feature representations</phrase>. A. Butt et al. present a computationally intelligent technique used for the prediction of <phrase>membrane protein</phrase>. They use statistical moments for extracting features. Furthermore, for prediction of <phrase>membrane protein</phrase>, multilayer <phrase>neural network</phrase> is trained based on back-propagation. S. Qadri et al. describe <phrase>pattern recognition</phrase> for the classification of five land <phrase>cover</phrase> patterns <phrase>data</phrase> from <phrase>remote sensing</phrase> images. These land patterns are used quantitatively in 
<phrase>Research</phrase> Statement Combining Ensembles for <phrase>Semi-supervised</phrase> Learning [Webpage] [Link to CV] My <phrase>research</phrase> has centered around <phrase>machine learning</phrase>, which has exploded in the past two decades. The field has addressed <phrase>cornerstone</phrase> problems that recur in myriad application areas, and solved them with <phrase>general</phrase>, useful <phrase>algorithms</phrase> that are ultimately <phrase>state</phrase>-of-the-<phrase>art</phrase> across many of these areas given enough <phrase>data</phrase>, using <phrase>statistics</phrase> and other <phrase>mathematics</phrase>. I have been motivated during my <phrase>PhD</phrase> by this remarkable <phrase>data</phrase>-driven combination of generality and practicality, working to devise new <phrase>algorithms</phrase> that exploit <phrase>data structure</phrase> in practical learning scenarios. I have focused on two specific problem areas which share the above characteristics, answering the following questions over the course of my <phrase>PhD</phrase>: 1. <phrase>Semi-Supervised</phrase> Learning with Ensembles of Predictors: How can we devise an efficient, practical, and interpretable <phrase>algorithm</phrase> that uses large quantities of <phrase>unlabeled data</phrase> to best put together the predictions of a collection of classifiers of varying competence? 2. Sequential <phrase>Algorithms</phrase> and Stopping: How can we robustly adapt traditional statistical <phrase>hypothesis</phrase> <phrase>tests</phrase> to <phrase>report</phrase> accurate <phrase>results</phrase> when the sample size is unknown? How can we use as few samples as possible to detect an effect of unknown <phrase>magnitude</phrase>? These areas have interested me because they recur frequently in common practical problems, and yet are still new enough to researchers to present many <phrase>basic</phrase> fresh challenges. I now discuss my work in each of them in the first two sections. In the third section, I discuss my future interests, which involve applications of such <phrase>algorithms</phrase> to interdisciplinary <phrase>research</phrase>. In the fundamental learning problem of (<phrase>binary</phrase>) classification, each datum has one of two <phrase>labels</phrase>; the learner is required to predict those of some <phrase>unlabeled data</phrase>, given a set of <phrase>labeled data</phrase>. Many widely-used methods exist for this problem, like linear predictors, <phrase>decision trees</phrase>, <phrase>Bayesian</phrase> classifiers, and deep <phrase>neural nets</phrase>. A typical <phrase>workflow</phrase> might involve using the <phrase>labeled data</phrase> to <phrase>train</phrase> several different such <phrase>algorithms</phrase> and estimate their error rates, and finally choosing the best of the <phrase>algorithms</phrase> with which to predict on the <phrase>unlabeled data</phrase>. Justifying this empirical <phrase>risk</phrase> minimization procedure is part of the <phrase>bedrock</phrase> of <phrase>classical</phrase> learning theory ([Vapnik, 1982]). However, practical cutting-edge usage very frequently involves aspects not addressed in this account, which I have focused on in my work. a) First and foremost, reliably <phrase>labeled data</phrase> are expensive to obtain in many applications in <phrase>medicine</phrase>, <phrase>NLP</phrase>, and other areas the phrase "<phrase>big data</phrase>" in classification often solely signifies abundant unlabeled 
Understanding the Principles of Recursive <phrase>Neural networks</phrase>: A Generative Approach to Tackle <phrase>Model</phrase> Complexity Recursive <phrase>Neural Networks</phrase> are non-linear adaptive models that are able to learn deep structured <phrase>information</phrase>. However, these models have not yet been broadly accepted. This fact is mainly due to its inherent complexity. In particular, not only for being extremely complex <phrase>information processing</phrase> models, but also because of a computational expensive learning phase. The most popular training method for these models is back-propagation through the structure. This <phrase>algorithm</phrase> has been revealed not to be the most appropriate for structured processing due to problems of convergence, while more sophisticated training methods enhance the speed of convergence at the expense of increasing significantly the computational cost. In this <phrase>paper</phrase>, we firstly perform an analysis of the underlying principles behind these models aimed at understanding their computational power. Secondly, we propose an approximate second <phrase>order</phrase> <phrase>stochastic</phrase> <phrase>learning algorithm</phrase>. The <phrase>proposed algorithm</phrase> dynamically adapts the learning rate throughout the training phase of the network without incurring excessively expensive computational effort. The <phrase>algorithm</phrase> operates in both on-line and batch modes. Furthermore, the resulting learning scheme is robust against the vanishing gradients problem. The advantages of the <phrase>proposed algorithm</phrase> are demonstrated with a <phrase>real-world</phrase> application example.
Best-effort <phrase>Data Integration</phrase> 2 Best-effort <phrase>Data Integration</phrase> This position statement makes the case that, since " exact " <phrase>data integration</phrase> is <phrase>AI</phrase> complete, we should seriously consider " best-effort " <phrase>data integration</phrase>. I will start by briefly summarizing the development of the field of <phrase>data integration</phrase>. Next, I discuss best-effort <phrase>data integration</phrase> as one of the next logical <phrase>research</phrase> directions, then <phrase>sketch</phrase> a simple observation that can be leveraged to examine the topic systematically and to understand current work. Finally, I describe <phrase>current research</phrase> on the topics at the <phrase>University</phrase> of <phrase>Wisconsin</phrase>-<phrase>Madison</phrase>, and list open questions that we are considering. The field of <phrase>data integration</phrase> can be roughly classified as having gone through four overlapping stages (based strictly on my personal perspectives): " Group Grope " (up to 1990): This was when we realized that <phrase>data integration</phrase> is an important problem. Numerous solutions were proposed, and the field started to acquire a " feel " for the various aspects of the problem. A clear distinction was also made between application/process integration and <phrase>data integration</phrase>. Foundational Development (1990-2000): A clear foundation was laid down for <phrase>data integration</phrase>. The mediator <phrase>model</phrase> was proposed and gained widespread acceptance. The various components of the <phrase>model</phrase> (<phrase>e</phrase>. Building on the Foundation (1998-today): Problems regarding various integration components are intensively studied. " One hundred <phrase>flowers</phrase> bloom " for theoretical development, query The <phrase>Rubber</phrase> Meets the <phrase>Road</phrase> (1998-today): The <phrase>lessons learned</phrase> from the above stages are applied in a <phrase>wave</phrase> of <phrase>Internet</phrase> startups. <phrase>Data integration</phrase> " branches " out into many application domains, including bio-informatics, geo-spatial domains, <phrase>hydrology</phrase>, <phrase>intelligence analysis</phrase>, and the <phrase>Deep Web</phrase>. The field takes a step back, to gain perspectives. In perspective, one of the key lessons we have gained is that <phrase>data integration</phrase> is hard, much harder than we thought (" intractable " or " <phrase>AI</phrase> complete " , as many have said). A <phrase>major</phrase> reason for this, I believe, is that so far we have mostly tried to achieve exact, precise <phrase>data integration</phrase>. This made much sense in the early days, when like <phrase>relational</phrase> <phrase>data management</phrase>, <phrase>data integration</phrase> is targeted
Theories for Deep Change in Affect-sensitive <phrase>Cognitive</phrase> Machines: A Constructivist <phrase>Model</phrase> There is interplay between emotions and learning, but this interaction is far more complex than previous learning theories have articulatedthis interplay interacts with other realms (e.g., <phrase>information</phrase> realm, attention realm, <phrase>entertainment</phrase> realm). This article proffers a novel <phrase>model</phrase> by which to regard the interplay of emotions upon learning in <phrase>light</phrase> of other factors that must be considered, and discusses the larger practical aim of crafting computer-based models that will recognize a learner's affective <phrase>state</phrase> and respond appropriately to it so that learning will proceed at an optimal pace.
<phrase>Data Mining</phrase> Predictive Technique: Cart Introduction: <phrase>Data Mining</phrase> is an emerging <phrase>technology</phrase> for the automatic extraction of patterns, associations for large <phrase>data</phrase> sets. It is one of the key technologies which enable <phrase>business</phrase> to select, filter, and correlate <phrase>data</phrase> automatically. Classification and Prediction are two forms of <phrase>data analysis</phrase> that can be used to categorize applications and in prediction. Here I discuss one predictive technique i.e. Classification And <phrase>Regression</phrase> Tress (CART). Features of CART: i. It is a <phrase>tree</phrase> structured <phrase>statistical analysis</phrase>. <phrase>ii</phrase>. It is easy to use. iii. It uses an intuitive, <phrase>windows-based</phrase> interface, making it accessible to both learning and statistical <phrase>research</phrase>. iv. It provides stable performance and reliable <phrase>results</phrase> as it is based on <phrase>machine learning</phrase> and statistical <phrase>research</phrase>. v. It is a <phrase>proven</phrase> statistical methodology and digs deep into date using advanced techniques. It uses a backward <phrase>pruning</phrase> technique that identifies the <phrase>tree structure</phrase> of the <phrase>data</phrase>.
Interactive Spoken Content Retrieval by <phrase>Deep Reinforcement Learning</phrase> User-machine interaction is important for spoken content retrieval. For text content retrieval, the user can easily scan through and select on a list of retrieved item. This is impossible for spoken content retrieval, because the retrieved items are difficult to show on screen. Besides, due to the <phrase>high</phrase> <phrase>degree</phrase> of uncertainty for <phrase>speech recognition</phrase>, the retrieval <phrase>results</phrase> can be very noisy. One way to counter such difficulties is through user-machine interaction. The machine can take different actions to interact with the user to obtain better retrieval <phrase>results</phrase> before showing to the user. The suitable actions depend on the retrieval status, for example requesting for extra <phrase>information</phrase> from the user, returning a list of topics for user to select, etc. In our previous work, some <phrase>hand-crafted</phrase> states estimated from the present retrieval <phrase>results</phrase> are used to determine the proper actions. In this <phrase>paper</phrase>, we propose to use <phrase>Deep-Q</phrase>-Learning techniques instead to determine the machine actions for interactive spoken content retrieval. <phrase>Deep-Q</phrase>-Learning bypasses the need for estimation of the <phrase>hand-crafted</phrase> states, and directly determine the best <phrase>action</phrase> base on the present retrieval status even without any <phrase>human</phrase> <phrase>knowledge</phrase>. It is shown to achieve significantly better performance compared with the previous <phrase>hand-crafted</phrase> states.
Broad Coverage Multilingual Deep Sentence Generation with a <phrase>Stochastic</phrase> Multi-Level Realizer Most of the known <phrase>stochastic</phrase> sentence generators use <phrase>syntactically</phrase> annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from <phrase>semantic</phrase> (predicate-argument) structures. To be able to deal with <phrase>semantic</phrase> structures, <phrase>stochastic</phrase> generators require semantically annotated, or, even better, multilevel annotated corpora. Only then can they deal with such crucial generation issues as sentence planning, <phrase>linearization</phrase> and mor-phologization. Multilevel annotated corpora are increasingly available for multiple languages. We take advantage of them and propose a multilingual deep <phrase>stochastic</phrase> sentence realizer that mirrors the <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>research</phrase> in <phrase>semantic</phrase> <phrase>parsing</phrase>. The realizer uses an <phrase>SVM</phrase> <phrase>learning algorithm</phrase>. For each pair of adjacent levels of annotation , a separate decoder is defined. So far, we evaluated the realizer for <phrase>Chinese</phrase>,
Efficient Transformation-Based <phrase>Parsing</phrase> In transformation-based <phrase>parsing</phrase>, a finite <phrase>sequence</phrase> of <phrase>tree</phrase> rewriting rules are checked for application to an input structure. Since in practice only a small percentage of rules are applied to any particular structure, the naive <phrase>parsing</phrase> <phrase>algorithm</phrase> is rather inefficient. We exploit this sparseness in rule applications to derive an <phrase>algorithm</phrase> two to three <phrase>orders of magnitude</phrase> faster than the standard <phrase>parsing</phrase> <phrase>algorithm</phrase>. 1 Introduction The idea of using transformational rules in <phrase>natural language</phrase> analysis dates back at least to Chore-sky, who attempted to define a set of transformations that would apply to a word <phrase>sequence</phrase> to map it from deep structure to surface structure (see (<phrase>Chomsky</phrase>, 1965)). Transformations have also been used in much of generative <phrase>phonology</phrase> to capture contextual variants in pronunciation, starting with (<phrase>Chomsky</phrase> and <phrase>Halle</phrase>, 1968). More recently , transformations have been applied to a diverse set of problems, including <phrase>part of speech tagging</phrase>, pronunciation network creation, preposi-tional phrase attachment disambiguation, and <phrase>parsing</phrase> , under the <phrase>paradigm</phrase> of transformation-based error-driven learning In this <phrase>paradigm</phrase>, rules can be learned automatically from a training corpus, instead of being written by hand. Transformation-based systems are typically deter-ministic. Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded, and the next rule is processed until the last rule in the list has been processed. Since for each rule the application <phrase>algorithm</phrase> must check for a matching at all possible sites to see whether the rule can apply, these systems run in O(rrpn) time, where 7r is the number of rules, p is the cost of a <phrase>single</phrase> rule matching , and n is the size of the input structure. While this <phrase>results</phrase> in fast processing, it is possible to create much faster systems. In (Roche and Schabes, 1995), a method is described for converting a list of transformations that operates on <phrase>strings</phrase> into a determin-istic finite <phrase>state</phrase> <phrase>transducer</phrase>, resulting in an optimal tagger in the sense that tagging requires only one <phrase>state</phrase> transition per word, giving a linear time tag-ger whose run-time is <phrase>independent</phrase> of the number and size of rules. In this <phrase>paper</phrase> we consider transformation-based <phrase>parsing</phrase>, introduced in (Brill, 1993), and we improve upon the O(Trpn) time <phrase>upper</phrase> bound.. In transformation-based <phrase>parsing</phrase>, an ordered <phrase>sequence</phrase> of <phrase>tree</phrase>-rewriting rules (<phrase>tree</phrase> transformations) are applied to an initial parse structure for an input sentence , to derive the final parse structure. We observe 
<phrase>Educational Technology</phrase> in Introductory <phrase>College</phrase> <phrase>Physics</phrase> <phrase>Teaching and Learning</phrase>: The Importance of Students <phrase>Perception</phrase> and Performance <phrase>E</phrase> Ed du uc ca at <phrase>ti</phrase> io on na al l T Te ec ch hn no ol lo og gy y i in n I In <phrase>nt</phrase> tr <phrase>ro</phrase> od du uc ct to or ry y C Co ol ll le eg <phrase>ge</phrase> <phrase>e</phrase> P <phrase>Ph</phrase> hy <phrase>ys</phrase> si ic cs s T Te <phrase>ea</phrase> ac ch hi in ng g a an nd d L Le <phrase>ea</phrase> ar <phrase>rn</phrase> ni in ng g: : T Th he <phrase>e</phrase> I Im <phrase>mp</phrase> po or rt ta an <phrase>nc</phrase> ce <phrase>e</phrase> o of f S St tu ud de en <phrase>nt</phrase> ts s' ' P Pe <phrase>er</phrase> rc ce <phrase>ep</phrase> pt <phrase>ti</phrase> io on n a an nd d P Pe <phrase>er</phrase> rf fo or rm <phrase>ma</phrase> an <phrase>nc</phrase> ce <phrase>e</phrase> Abstract In this <phrase>paper</phrase> the researcher explored how introductory <phrase>physics</phrase> <phrase>student</phrase> perceptions about learning <phrase>physics</phrase> and their perspectives about <phrase>physics</phrase> instructors' presentational formats might be developed. Within a constructivist framework, it is of fundamental importance that the educators understand and address <phrase>student</phrase> expectations about effective instructional methods and <phrase>educational technology</phrase>-integrated curricula in particular. The researcher also investigated the likely impact of <phrase>student</phrase> expectations of <phrase>learning outcomes</phrase> as part of the implications of improving the approach towards teaching. Introduction <phrase>Physics</phrase> is a <phrase>science</phrase> composed of well-founded expectations of how the natural world should behave, and it uses the tool of <phrase>mathematics</phrase> to describe these behaviors (Foster, 2000). <phrase>Physics</phrase> learning therefore involves observing phenomena, quantifying the observations, and synthesizing the <phrase>results</phrase> into theories (<phrase>Williams</phrase>, 1999). The traditional approach to instruction is to teach <phrase>physics</phrase> through <phrase>solving problems</phrase>. Students of <phrase>physics</phrase> are expected to learn both the descriptive or conceptual side of <phrase>physics</phrase> and its predictive, <phrase>problem-solving</phrase>, and logical reasoning aspects. Because learning logical thinking is difficult, many students try to memorize formulas and recall <phrase>results</phrase> from the homework problems at <phrase>test</phrase> time. <phrase>Hammer</phrase> (1994) reported that many <phrase>students learn</phrase> by rote because they have a naive conception of what it means to understand <phrase>physics</phrase>. Formulas and equations are important for <phrase>physics</phrase> because physical quantities have to be calculated by using them. However, if students cannot understand the <phrase>physics</phrase> behind the formulas, they usually will not be able to solve the problem. Elby (1999) focused on another cause of these study habits because many students believed that a <phrase>deep understanding</phrase> of <phrase>physics</phrase> is not necessary to obtain <phrase>high</phrase> grades in 
Joint and Individual <phrase>Knowledge</phrase> Elaboration in Cscl <phrase>Knowledge</phrase> Elaboration in Cscl Joint <phrase>Knowledge</phrase> Elaboration in Cscl This <phrase>case study</phrase> aims to illustrate the sequential process of joint and individual <phrase>knowledge</phrase> elaboration in <phrase>computer-supported</phrase> <phrase>collaborative learning</phrase> (CSCL). Six <phrase>Dutch</phrase> <phrase>secondary school</phrase> students (three males, three females) participated in the three-week experiment. They were paired based on self-selection. Each dyad was asked to work on moderately-structured problems concerning <phrase>Newtonian mechanics</phrase>. With the help of elaboration values, students' online interactions were <phrase>categorized</phrase> and sequentially plotted. Three dyads showed three different patterns of individual <phrase>knowledge</phrase> elaboration. Group is the learning agent in <phrase>collaborative learning</phrase> (Suthers, 2006). <phrase>Problem solving</phrase> process can be regarded as a joint process of <phrase>knowledge</phrase> elaboration made up of numerous meaningful artifacts, such as utterances, <phrase>visual representations</phrase>, etc. In <phrase>computer-supported</phrase> <phrase>collaborative learning</phrase> (CSCL), verbal and visual interchanges in students' interaction are of ultimate importance for students' joint <phrase>knowledge</phrase> elaboration. To solve a problem collaboratively, highly elaborative messages are important for group success. As Van <phrase>Boxtel</phrase> (2000) posited that joint <phrase>knowledge</phrase> elaboration is a process within which all participants should contribute to the <phrase>knowledge</phrase> elaboration verbally and propositionally. <phrase>Collaborative learning</phrase> involves individual <phrase>cognitive</phrase> elaboration, and will not reduce it (Stahl, Koschmann & Suthers, 2006). But there exists a qualitative difference in individual involvement. Each person has unique situated <phrase>prior knowledge</phrase>, and their <phrase>knowledge</phrase> elaboration may vary in the <phrase>degree</phrase> of <phrase>cognitive</phrase> engagement. Methodology Six tenth graders (three females, three males) from a <phrase>Dutch</phrase> <phrase>secondary school</phrase> participated in the synchronous CSCL experiment. Students were paired based on self-selection. The scope was limited to <phrase>average</phrase> students. Students were spread into different rooms to avoid face-to-face contact. The content of students' interaction messages was analyzed, and each was endowed an <phrase>integral</phrase>,-1, 0 or +1. This was roughly in line with Kumpulainen and Mutanen's (1999) three <phrase>cognitive</phrase> processing modes that acknowledged that procedural processing referred to the routine execution of task without improving the ideas (value=0). Interpretative or exploratory processing referred to students' deep engagement in <phrase>problem solving</phrase> activity (value=+1), while off-task activity referred to those absent-minded activities or off-task social <phrase>talk</phrase> (value=-1). We aggregated numbers of messages one by one sequentially, and plotted the sum to illustrate the process of joint <phrase>knowledge</phrase> elaboration. Then we added up the numbers of each individual to trace the process of individual <phrase>knowledge</phrase> elaboration in CSCL. Let's take one problem as an example. Sandy-Carol dyad spent 15 " 34' on it. Their joint <phrase>knowledge</phrase> elaboration (Figure 1) showed that the girls almost hadn't talked anything 
Accelerated Parallelizable <phrase>Neural Network</phrase> <phrase>Learning Algorithm</phrase> for <phrase>Speech Recognition</phrase> We describe a set of novel, batch-mode <phrase>algorithms</phrase> we developed recently as one key component in scalable, <phrase>deep neural network</phrase> based <phrase>speech recognition</phrase>. The essence of these <phrase>algorithms</phrase> is to structure the <phrase>single</phrase>-<phrase>hidden-layer</phrase> <phrase>neural network</phrase> so that the <phrase>upper</phrase>-layer's weights can be written as a deterministic <phrase>function</phrase> of the <phrase>lower</phrase>-layer's weights. This structure is effectively exploited during training by plugging in the deterministic <phrase>function</phrase> to the least square error <phrase>objective function</phrase> while calculating the gradients. Accelerating techniques are further exploited to make the weight updates move along the most promising directions. The experiments on TIMIT frame-level phone and phone-<phrase>state</phrase> classification show strong <phrase>results</phrase>. In particular, the <phrase>error rate</phrase> is strictly monotonically dropping as the <phrase>mini</phrase>-batch size increases. This demonstrates the potential for the proposed batch-mode <phrase>algorithms</phrase> in <phrase>large scale</phrase> <phrase>speech recognition</phrase> since they are easily parallelizable across <phrase>computers</phrase>.
Adaptive Hyperman: a Customizable <phrase>Hypertext</phrase> System for Reference Manuals We are interested in facilitating <phrase>information</phrase> access from large volume of reference <phrase>information</phrase> contained in technical and operational manuals. When a large volume of <phrase>information</phrase> is used on an everyday basis to perform one's job, the problem for users is to build and maintain an accurate <phrase>cognitive</phrase> <phrase>model</phrase> of the <phrase>information</phrase> in <phrase>order</phrase> to access it quickly. The problem is not to perform a search to discover new <phrase>information</phrase>, since they have already learned the <phrase>information</phrase> during their training. We have developed an adaptive <phrase>hypertext</phrase> system to help <phrase>Space Shuttle</phrase> flight controllers access operations documents in <phrase>mission control center</phrase>. We describe this intelligent system, called Adaptive HyperMan, which lets users incorporate their representation of the content and <phrase>organization</phrase> of documents over time. It provides sophisticated annotations and hyperlinking capabilities to end-users, and integrates an adaptive indexing and retrieval <phrase>engine</phrase> for managing annotations. This novel feature lets users assign topics to annotations, retrieve annotations by topics, and provide relevance <phrase>feedback</phrase> over time. Besides memorizing user inputs, the indexing <phrase>engine</phrase> also learns to generalize user inputs in <phrase>order</phrase> to facilitate retrieval from similar topics. We describe the Adaptive HyperMan system, then show how it provides a virtual "goody <phrase>book</phrase>" facility to flight controllers, and supports collaborative work. 1 The Problem We are interested in facilitating access to large volume of reference <phrase>information</phrase> contained in technical and operational manuals. This type of <phrase>information</phrase> is used in many fields by professionals like <phrase>airplane</phrase> <phrase>mechanics</phrase>, pilots, <phrase>astronauts</phrase>, <phrase>Space Shuttle</phrase> flight controllers, power <phrase>plant</phrase> controllers, <phrase>lawyers</phrase>, <phrase>doctors</phrase>, etc. These persons need to use of large amount of technical <phrase>information</phrase> to perform their everyday job. However, as opposed to more traditional <phrase>information retrieval</phrase> tasks, they do not often need to search for <phrase>information</phrase> to answer their queries. They rather already know most of the <phrase>information</phrase> and where it is stored (it is part of their training), and just need to access it quickly to use it as a backup to their <phrase>memory</phrase> (to prevent <phrase>human</phrase> errors). For example, we are working with <phrase>Space Shuttle</phrase> flight controllers. It takes years of training to become a <phrase>flight controller</phrase> in the <phrase>Space Shuttle</phrase> <phrase>Mission Control Center</phrase>. As part of this training, people learn to use a large corpus of documentation to <phrase>solve problems</phrase>. They develop a deep <phrase>knowledge</phrase> of the <phrase>organization</phrase> and content of these manuals in <phrase>order</phrase> to access the proper sections as quickly as possible. This <phrase>knowledge</phrase>, or 
Sense of the deep Sense of the Deep is a <phrase>multi-user</phrase> interactive installation that employs a unique tactile controller to collaboratively manipulate 3D holograms in real time. Participants immerse both hands in closed chambers containing a wet <phrase>gelatin</phrase> substance. As they explore the sensual <phrase>matter</phrase> they also learn how to utilize it as an interface. Up to four participants can work together to <phrase>mold</phrase> and reshape the <phrase>fluid</phrase> visuals.
<phrase>Human</phrase> <phrase>Liver Cancer</phrase> Classification using <phrase>Microarray</phrase> <phrase>Gene Expression</phrase> <phrase>Data</phrase> <phrase>Cancer</phrase> is one of the dreadful diseases, which causes considerable <phrase>death</phrase> rate in humans. <phrase>Cancer</phrase> is featured by an irregular, unmanageable growth that may demolish and attack neighboring healthy body tissues or somewhere else in the body. There are dissimilar techniques lives for the naming of <phrase>cancer</phrase> but none of those techniques afford considerable accuracy of detection. Therefore a new method is highly essential for the <phrase>cancer</phrase> classification with improved accuracy. <phrase>Gene expression</phrase> profiling by <phrase>microarray</phrase> method has been emerged as an efficient technique for classification and diagnostic prediction of <phrase>cancer</phrase> nodules. In recent times, <phrase>DNA microarray</phrase> technique has gained more attraction in both scientific and in <phrase>industrial</phrase> fields. The <phrase>DNA</phrase> microarrays are utilized in this <phrase>paper</phrase> for the purpose of identifying the presence of <phrase>cancer</phrase>. Statistical ranking has also been used for effective <phrase>cancer</phrase> classification. The most widely used ranking schemes are <phrase>ANOVA</phrase>, T-score and Enrichment Score. But, these existing techniques suffer from the drawbacks of less accuracy, complexity etc. This <phrase>paper</phrase> uses <phrase>liver cancer</phrase> <phrase>data set</phrase> for experimentation of the proposed technique. The classifier used here is <phrase>SVM</phrase> and FNN. The <phrase>experimental</phrase> <phrase>results</phrase> shows that the proposed technique has the ability to classify the <phrase>cancer</phrase> cells significantly when compared to the conventional methods of <phrase>cancer</phrase> classification. 1. INTRODUCTION <phrase>Cancer</phrase> is one of the dreadful diseases found in most of the living being, which is one of the challenging studies for <phrase>research</phrase> in the 20th century. There has been lot of proposals from various researchers on <phrase>cancer</phrase> classification and detailed study is still on in the domain of <phrase>cancer</phrase> classification. In <phrase>order</phrase> to gain deep insight into the <phrase>cancer</phrase> <phrase>classification problem</phrase>, it is necessary to take a closer look at the problem, the proposed solutions and the related issues all together. There have been various investigations available in the <phrase>literature</phrase> on the <phrase>classification problem</phrase> by the statistical, <phrase>machine learning</phrase> and <phrase>database</phrase> <phrase>research</phrase> <phrase>community</phrase>. However, <phrase>gene</phrase> classification as a new <phrase>area</phrase> of <phrase>research</phrase> has new challenges due to its unique problem <phrase>nature</phrase>. Some of the challenges are summarized below: The unique <phrase>nature</phrase> of the available <phrase>gene expression</phrase> <phrase>data set</phrase> is the foremost challenge. Third challenge is the huge number of irrelevant attributes (<phrase>genes</phrase>). Fourth challenge arises from the application domain of <phrase>cancer</phrase> classification. Though Accuracy plays a vital factor in <phrase>cancer</phrase> classification, the biological relevancy is another key criterion, as any biological <phrase>information</phrase> exposed during the process can help in 
EPOpt: Learning Robust <phrase>Neural Network</phrase> Policies Using <phrase>Model</phrase> Ensembles Sample complexity and <phrase>safety</phrase> are <phrase>major</phrase> challenges when learning policies with <phrase>reinforcement learning</phrase> for <phrase>real-world</phrase> tasks, especially when the policies are represented using rich <phrase>function</phrase> approximators like <phrase>deep neural networks</phrase>. <phrase>Model</phrase>-based methods where the <phrase>real-world</phrase> <phrase>target</phrase> domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real <phrase>data</phrase> with simulated <phrase>data</phrase>. However, discrepancies between the simulated source domain and the <phrase>target</phrase> domain pose a challenge for simulated training. We introduce the EPOpt <phrase>algorithm</phrase>, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad <phrase>range</phrase> of possible <phrase>target</phrase> domains, including unmodeled effects. Further, the <phrase>probability distribution</phrase> over source domains in the ensemble can be adapted using <phrase>data</phrase> from <phrase>target</phrase> domain and approximate <phrase>Bayesian</phrase> methods, to progressively make it a better approximation. Thus, learning on a <phrase>model</phrase> ensemble, along with source <phrase>domain adaptation</phrase>, provides the benefit of both robustness and learning/adaptation.
Concurrent Inference of Topic Models and Distributed <phrase>Vector</phrase> Representations Topic modeling techniques have been widely used to uncover dominant themes hidden inside an unstructured document collection. Though these techniques first originated in the probabilistic analysis of word <phrase>distributions</phrase>, many <phrase>deep learning</phrase> approaches have been adopted recently. In this <phrase>paper</phrase>, we propose a novel <phrase>neural network</phrase> based <phrase>architecture</phrase> that produces distributed representation of topics to capture topical themes in a dataset. Unlike many <phrase>state</phrase>-of-the-<phrase>art</phrase> techniques for generating distributed representation of words and documents that directly use neighboring words for training, we leverage the outcome of a sophisticated <phrase>deep neural network</phrase> to estimate the topic <phrase>labels</phrase> of each document. The networks, for topic modeling and generation of distributed representations , are trained concurrently in a cascaded style with better runtime without sacrificing the quality of the topics. <phrase>Empirical studies</phrase> reported in the <phrase>paper</phrase> show that the distributed representations of topics represent intuitive themes using smaller dimensions than conventional topic modeling approaches.
Modal Definability in <phrase>Topology</phrase> Modal definability in <phrase>topology</phrase> Master's <phrase>thesis</phrase> Acknowledgments During the year spent in <phrase>Amsterdam</phrase> at ILLC I have been lucky to be supported by many. The proper acknowledgment of everyone would have resulted in a separate <phrase>book</phrase>. I will only name a few, with the hope that the others will find my gratitude in between the lines. I am greatly indebted to Dr. Yde Venema, my supervisor. If it were not for his constant care and <phrase>professional</phrase> guidance, I would never have finished this <phrase>thesis</phrase>. His questions, suggestions and advises were crucial for creating this text. I appreciate his personal, not only <phrase>professional</phrase>, assistance. My deep gratitude goes to Prof. Johan van Benthem. Besides being one of the most inspiring teachers I have ever had, besides the helpful comments and interesting questions on this <phrase>thesis</phrase>, it was by his support that my studies at ILLC came possible at all. I could never afford living in <phrase>Amsterdam</phrase> without the financial support of the <phrase>Spinoza</phrase> project " <phrase>Logic</phrase> in <phrase>Action</phrase> ". There are no words that can express my very special thanks to Prof. Leo Esakia. His patient <phrase>love</phrase>, careful support and wise advises have helped me greatly to carry on when my <phrase>soul</phrase> was the heaviest. He taught me <phrase>Logic</phrase>. In the severe conditions of the lack of <phrase>elementary</phrase> means of comfort which our <phrase>country</phrase> has experienced, he managed to keep his seminars going, most skillfully guiding us through the breathtaking beauty of <phrase>mathematical</phrase> abstractions. I would like to thank all the attendants of those seminars-I have learned much of what is presented here from them. Dr. Guram Bezhanishvili was a <phrase>member</phrase> of those seminars as well. I would like to thank him especially. He made many helpful comments and suggestions on the earlier versions of the <phrase>thesis</phrase>, showed me how to shorten several proofs. His detailed analysis were the pieces of <phrase>professional</phrase> work and manifestations of a <phrase>friendly</phrase> support. They have always amazed me with precision and expertize. Many many thanks to Dr. Maarten <phrase>Marx</phrase> and his beautiful <phrase>family</phrase>. Thanks Maarten for being close and understanding friend and offering cordial support , <phrase>professional</phrase> advice, a shelter when I most needed those. The following people have supported the development of this <phrase>thesis</phrase> and/or my studies at ILLC: all my teachers and <phrase>nice</phrase> staff at ILLC. Thanks to my <phrase>family</phrase> and <phrase>friends</phrase> for making this <phrase>life</phrase> bearable. Abstract This <phrase>thesis</phrase> is mainly concerned 
Parallelizable Sampling of <phrase>Markov</phrase> <phrase>Random Fields</phrase> <phrase>Markov</phrase> <phrase>Random Fields</phrase> (MRFs) are an important class of <phrase>probabilistic models</phrase> which are used for <phrase>density estimation</phrase>, classification , denoising, and for constructing <phrase>Deep Belief</phrase> Networks. Every application of an MRF requires addressing its inference problem , which can be done using deterministic inference methods or using <phrase>stochastic</phrase> <phrase>Markov Chain Monte Carlo</phrase> methods. In this <phrase>paper</phrase> we introduce a new <phrase>Markov Chain</phrase> transition operator that updates all the variables of a pairwise MRF in parallel by using <phrase>auxiliary</phrase> Gaussian variables. The proposed MCMC operator is extremely simple to implement and to parallelize. This is achieved by a formal equivalence result between arbitrary pairwise MRFs and a particular type of <phrase>Restricted Boltzmann Machine</phrase>. This result also implies that the later can be learned in place of the former without any loss of modeling power, a possibility we explore in experiments .
<phrase>Design</phrase> for <phrase>Art</phrase> and Leisure The <phrase>paper</phrase> is a reflection about the compelling yet difficult <phrase>nature</phrase> of <phrase>design</phrase> and evaluation of <phrase>entertainment</phrase> or <phrase>edutainment</phrase> systems. Such systems are not designed to help users perform work tasks or save time. They should encourage users to spend time and enjoy the interaction, and their ultimate mission is to engage the user and stimulate learning. The <phrase>nature</phrase> of this kind of systems imposes the definition of a theoretical and methodological policy of <phrase>design</phrase> that mediates betwecn <phrase>design</phrase> visions and user needs. In the <phrase>paper</phrase> we describe the ftamework adopted to <phrase>design</phrase> and develop a <phrase>tourist</phrase> guidc that transforms the <phrase>user experience</phrase> from one of consultation, whether with an audio guide, a <phrase>multimedia</phrase> <phrase>kiosk</phrase>, a <phrase>CD ROM</phrase> or even a <phrase>book</phrase>, to one of immersion in a rich <phrase>information</phrase> environment. The <phrase>technology</phrase> used allows to overlay multiple <phrase>information</phrase> structures over the physical world in a non intrusive <phrase>fashion</phrase>, opening up new possibilities for creative <phrase>design</phrase>. We believe that the success of the "new generation" systems for <phrase>tourist</phrase> applications, especially those exploiting advanced technologies, strongly relies on a <phrase>design</phrase> <phrase>philosophy</phrase> that mediates between a deep and continuous focus on the users and ~nnovative <phrase>design</phrase> visions.
minedICE: A <phrase>Knowledge</phrase> Discovery Platform for Neurophysiological <phrase>Artificial Intelligence</phrase> In this <phrase>paper</phrase> we present the minedICE T M computer <phrase>architecture</phrase> and network comprised of neurological instruments and <phrase>artificial intelligence</phrase> (<phrase>AI</phrase>) agents. It's called minedICE because <phrase>data</phrase> that is " mined " via IntraCortical <phrase>Electroencephalography</phrase> (<phrase>ICE</phrase>) located deep inside the <phrase>human brain</phrase> procures (mined) <phrase>knowledge</phrase> to a <phrase>Decision Support</phrase> System (DSS) that is read by a <phrase>neurosurgeon</phrase> located either at the bedside of the patient or at a geospatially remote location. The DSS system 1) alerts the <phrase>neurosurgeon</phrase> when a severe neurological event is occurring in the patient and 2) identifies the severe neurological event. The <phrase>neurosurgeon</phrase> may choose to provide <phrase>feedback</phrase> to the <phrase>AI</phrase> agent which controls the confidence level of the association rules and thereby teaches the learning component of minedICE.
Learning Quadrotor Dynamics Using <phrase>Neural Network</phrase> for Flight Control Traditional learning approaches proposed for controlling quadrotors or <phrase>helicopters</phrase> have focused on improving performance for specific trajectories by iteratively improving upon a nominal controller, for example learning from demonstrations , iterative learning, and <phrase>reinforcement learning</phrase>. In these schemes, however, it is not clear how the <phrase>information</phrase> gathered from the training trajectories can be used to synthesize controllers for more <phrase>general</phrase> trajectories. Recently, the efficacy of <phrase>deep learning</phrase> in inferring <phrase>helicopter</phrase> dynamics has been shown. Motivated by the generalization capability of <phrase>deep learning</phrase>, this <phrase>paper</phrase> investigates whether a <phrase>neural network</phrase> based dynamics <phrase>model</phrase> can be employed to synthesize control for trajectories different than those used for training. To <phrase>test</phrase> this, we learn a quadrotor dynamics <phrase>model</phrase> using only translational and only rotational training trajectories, each of which can be controlled independently, and then use it to simultaneously control the yaw and position of a quadrotor, which is non-trivial because of nonlinear couplings between the two motions. We validate our approach in experiments on a quadrotor <phrase>testbed</phrase>.
A Classification-Based <phrase>Polyphonic</phrase> <phrase>Piano</phrase> Transcription Approach Using Learned <phrase>Feature Representations</phrase> Recently <phrase>unsupervised feature learning</phrase> methods have shown great promise as a way of extracting features from <phrase>high</phrase> dimensional <phrase>data</phrase>, such as image or audio. In this <phrase>paper</phrase>, we apply <phrase>deep belief</phrase> networks to <phrase>musical</phrase> <phrase>data</phrase> and evaluate the learned <phrase>feature representations</phrase> on classification-based poly-phonic <phrase>piano</phrase> transcription. We also suggest a way of training classifiers jointly for multiple notes to improve training speed and classification performance. Our method is evaluated on three <phrase>public</phrase> <phrase>piano</phrase> datasets. The <phrase>results</phrase> show that the <phrase>learned features</phrase> outperform the baseline features, and also our method gives significantly better frame-level accuracy than other <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>music</phrase> transcription methods.
<phrase>Brain</phrase> Oscillatory Activity during Spatial <phrase>Navigation</phrase>: Theta and Gamma Activity Link Medial Temporal and <phrase>Parietal</phrase> Regions <phrase>Brain</phrase> oscillatory correlates of spatial <phrase>navigation</phrase> were investigated using blind source separation (BSS) and standardized low resolution <phrase>electromagnetic</phrase> tomography (sLORETA) analyses of 62-<phrase>channel</phrase> <phrase>EEG</phrase> recordings. Twenty-five participants were instructed to navigate to distinct landmark buildings in a previously learned <phrase>virtual reality</phrase> <phrase>town</phrase> environment. <phrase>Data</phrase> from periods of <phrase>navigation</phrase> between landmarks were subject to BSS analyses to obtain source components. Two of these <phrase>cortical</phrase> sources were found to exhibit significant spectral power differences during <phrase>navigation</phrase> with respect to a resting eyes open condition and were subject to source localization using sLORETA. These two sources were localized as a right <phrase>parietal</phrase> component with gamma activation and a right medial-temporal-<phrase>parietal</phrase> component with activation in theta and gamma bandwidths. The <phrase>parietal</phrase> gamma activity was thought to reflect visuospatial processing associated with the task. The medial-temporal-<phrase>parietal</phrase> activity was thought to be more specific to the navigational processing, representing the integration of ego- and allo-centric representations of space required for successful <phrase>navigation</phrase>, suggesting theta and gamma oscillations may have a role in integrating <phrase>information</phrase> from <phrase>parietal</phrase> and medial-temporal regions. Theta activity on this medial-temporal-<phrase>parietal</phrase> source was positively correlated with more efficient <phrase>navigation</phrase> performance. <phrase>Results</phrase> are discussed in <phrase>light</phrase> of the depth and proposed closed field structure of the <phrase>hippocampus</phrase> and potential implications for <phrase>scalp</phrase> <phrase>EEG</phrase> <phrase>data</phrase>. The findings of the present study suggest that appropriate BSS methods are ideally suited to minimizing the effects of volume conduction in noninvasive recordings, allowing more accurate exploration of deep <phrase>brain</phrase> processes.
Speech <phrase>Emotion</phrase> Recognition Using <phrase>CNN</phrase> <phrase>Deep learning</phrase> systems, such as <phrase>Convolutional Neural Networks</phrase> (CNNs), can infer a hierarchical representation of <phrase>input data</phrase> that facilitates categorization. In this <phrase>paper</phrase>, we propose to learn affect-salient features for Speech <phrase>Emotion</phrase> Recognition (SER) using semi-<phrase>CNN</phrase>. The training of semi-<phrase>CNN</phrase> has two stages. In the first stage, unlabeled samples are used to learn candidate features by contractive <phrase>convolutional neural network</phrase> with <phrase>reconstruction</phrase> penalization. The candidate features, in the second step, are used as the input to semi-<phrase>CNN</phrase> to learn affect-salient, <phrase>discriminative features</phrase> using a novel <phrase>objective function</phrase> that encourages the feature saliency, <phrase>orthogonality</phrase> and <phrase>discrimination</phrase>. Our experiment <phrase>results</phrase> on <phrase>benchmark datasets</phrase> show that our approach leads to stable and robust <phrase>recognition performance</phrase> in complex scenes (e.g., with <phrase>speaker</phrase> and environment <phrase>distortion</phrase>), and outperforms several well-established SER features.
Scaling <phrase>Learning Algorithms</phrase> towards <phrase>Ai</phrase> One <phrase>long</phrase>-term goal of <phrase>machine learning</phrase> <phrase>research</phrase> is to produce methods that are applicable to highly complex tasks, such as <phrase>perception</phrase> (vision, audition), reasoning , intelligent control, and other <phrase>artificially intelligent</phrase> behaviors. We argue that in <phrase>order</phrase> to progress toward this goal, the <phrase>Machine Learning</phrase> <phrase>community</phrase> must endeavor to discover <phrase>algorithms</phrase> that can learn highly complex functions, with minimal need for <phrase>prior knowledge</phrase>, and with minimal <phrase>human</phrase> intervention. We present <phrase>mathematical</phrase> and <phrase>empirical evidence</phrase> suggesting that many popular approaches to non-parametric learning, particularly <phrase>kernel methods</phrase>, are fundamentally limited in their ability to learn complex <phrase>high</phrase>-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a <phrase>single</phrase> layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second , we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (<phrase>manifold</phrase> learning) and <phrase>semi-supervised</phrase> kernel machines. Using empirical <phrase>results</phrase> on invariant <phrase>image recognition</phrase> tasks, <phrase>kernel methods</phrase> are compared with <phrase>deep architectures</phrase>, in which <phrase>lower</phrase>-<phrase>level features</phrase> or concepts are progressively combined into more abstract and <phrase>higher-level</phrase> representations. We argue that <phrase>deep architectures</phrase> have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in <phrase>order</phrase> to make progress on the kind of complex tasks required for <phrase>artificial intelligence</phrase>.
Integrative Graduate <phrase>Education</phrase> and <phrase>Research</phrase> Traineeship (igert) Interdisciplinary Training Program Program Faculty Eligibility Requirements The <phrase>University</phrase> of <phrase>Minnesota</phrase> is an <phrase>equal opportunity</phrase> <phrase>educator</phrase> and employer. Become the future of systems neuroengineering. The <phrase>University</phrase> of <phrase>Minnesota</phrase> is home to an Integrative Graduate <phrase>Education</phrase> and <phrase>Research</phrase> Traineeship (IGERT) program in Systems Neuroengineering, sponsored by the <phrase>National Science Foundation</phrase> (<phrase>NSF</phrase>). Bright, <phrase>high</phrase>-achieving students who are admitted to a <phrase>University</phrase> of Minnesota's Ph.D. program in <phrase>Biomedical Engineering</phrase>, <phrase>Electrical Engineering</phrase>, <phrase>Mechanical Engineering</phrase>, or <phrase>Neuroscience</phrase> are eligible for this prestigious training program. The program provides a generous <phrase>stipend</phrase> and tuition coverage as well as access to cutting-edge <phrase>research</phrase> in neuroengineering. <phrase>Minnesota</phrase> is also home to the largest collection of <phrase>medical device</phrase> manufacturers in the world and our program provides opportunities for IGERT trainees to gain practical experience working with these companies. Through our <phrase>education</phrase> and <phrase>research</phrase>-training <phrase>model</phrase>, students in our program learn to develop the skills to revolutionize neurotechnologies and advance our understanding of <phrase>neuroscience</phrase> processes underlying these technologies. The Systems Neuroengineering IGERT Program has over 40 outstanding <phrase>faculty members</phrase> who have made significant contributions to neural decoding, neuromodulation, neural interfacing, and <phrase>neuroimaging</phrase> <phrase>research</phrase>, and who are committed to graduate training. Many of them are world class leaders who have shaped where the field is in cutting-edge <phrase>research</phrase>, including noninvasive <phrase>brain</phrase>-computer interface controlling <phrase>quadcopter</phrase>, <phrase>deep-brain stimulation</phrase>, <phrase>high</phrase> field <phrase>MRI</phrase> imaging, and dynamic <phrase>brain mapping</phrase>. Choice of <phrase>research</phrase> advisors from over 40 participating training faculty across <phrase>engineering</phrase> and <phrase>brain</phrase> sciences Joint faculty mentoring and team advising of <phrase>research</phrase> Tailored neuroengineering graduate <phrase>curriculum</phrase> Lab rotations in <phrase>engineering</phrase> and <phrase>basic</phrase>/clinical <phrase>brain</phrase> sciences <phrase>Industrial</phrase> <phrase>internship</phrase> rotations Generous <phrase>stipend</phrase> ($30,000/year for up to two years on IGERT program) and tuition coverage
Towards Less Biased <phrase>Web Search</phrase> <phrase>Web search</phrase> engines now serve as essential assistant to help users make decisions in different aspects. Delivering correct and impartial <phrase>information</phrase> is a crucial functionality for <phrase>search engines</phrase> as any false <phrase>information</phrase> may <phrase>lead</phrase> to unwise decision and thus undesirable consequences. Unfortunately, a recent study revealed that <phrase>Web search</phrase> engines tend to provide biased <phrase>information</phrase> with most <phrase>results</phrase> supporting users' beliefs conveyed in queries regardless of the truth. In this <phrase>paper</phrase> we propose to alleviate bias in <phrase>Web search</phrase> through predicting the topical polarity of documents, which is the overall tendency of one document regarding whether it supports or disapproves the belief in query. By applying the prediction to balance search <phrase>results</phrase>, users would receive less biased <phrase>information</phrase> and therefore make wiser decision. To achieve this goal, we propose a novel textual segment extraction method to distill and generate document <phrase>feature representation</phrase>, and leverage <phrase>convolution</phrase> <phrase>neural network</phrase>, an effective <phrase>deep learning</phrase> approach, to predict topical polarity of documents. We conduct <phrase>extensive experiments</phrase> on a set of queries with <phrase>medical</phrase> indents and demonstrate that our <phrase>model</phrase> performs empirically well on identifying topical polarity with satisfying accuracy. To our best <phrase>knowledge</phrase>, our work is the first on investigating the mitigation of bias in <phrase>Web search</phrase> and could provide directions on <phrase>future research</phrase>.
The <phrase>Manifold</phrase> <phrase>Tangent</phrase> Classifier We combine three important ideas present in previous work for building classi-<phrase>fiers</phrase>: the <phrase>semi-supervised</phrase> <phrase>hypothesis</phrase> (the input distribution contains <phrase>information</phrase> about the classifier), the unsupervised <phrase>manifold</phrase> <phrase>hypothesis</phrase> (<phrase>data</phrase> <phrase>density</phrase> concentrates near <phrase>low-dimensional</phrase> <phrase>manifolds</phrase>), and the <phrase>manifold</phrase> <phrase>hypothesis</phrase> for classification (different classes correspond to disjoint <phrase>manifolds</phrase> separated by low <phrase>density</phrase>). We exploit a novel <phrase>algorithm</phrase> for capturing <phrase>manifold</phrase> structure (<phrase>high</phrase>-<phrase>order</phrase> contractive <phrase>auto-encoders</phrase>) and we show how it builds a <phrase>topological</phrase> atlas of charts, each <phrase>chart</phrase> being characterized by the principal singular vectors of the <phrase>Jacobian</phrase> of a representation mapping. This representation <phrase>learning algorithm</phrase> can be stacked to yield a <phrase>deep architecture</phrase>, and we combine it with a <phrase>domain knowledge</phrase>-<phrase>free</phrase> version of the TangentProp <phrase>algorithm</phrase> to encourage the classifier to be insensitive to local directions changes along the <phrase>manifold</phrase>. Record-breaking classification <phrase>results</phrase> are obtained.
How <phrase>Open Source Software</phrase> Works: " <phrase>Free</phrase> " User-to-user Assistance <phrase>Research</phrase> into <phrase>free and open source software</phrase> development projects has so far largely focused on how the <phrase>major</phrase> tasks of <phrase>software development</phrase> are organized and motivated. But a complete project requires the execution of " mundane but necessary " tasks as well. In this <phrase>paper</phrase>, we explore how the mundane but necessary task of field support is organized in the case of <phrase>Apache</phrase> server <phrase>software</phrase>, and why some project participants are motivated to provide this service <phrase>gratis</phrase> to others. We find that the <phrase>Apache</phrase> field support system functions effectively. We also find that, when we <phrase>partition</phrase> the help system into its component tasks, 99% of the effort expended by <phrase>information</phrase> providers in fact returns direct learning benefits to those providers. This finding considerably reduces the <phrase>puzzle</phrase> of why <phrase>information</phrase> providers are willing to perform this task " for <phrase>free</phrase>. " Implications are discussed. Thau for sharing their deep insights into the functioning of the <phrase>Apache</phrase> <phrase>free</phrase>/<phrase>open source</phrase> <phrase>community</phrase> and thank Ben Ho and <phrase>Starling</phrase> Hunter for their helpful inputs during <phrase>paper</phrase> development. We also thank the participants on the <phrase>Usenet newsgroup</phrase> comp.infosystems.www.servers.unix who responded to our online survey. Doctoral <phrase>student</phrase> support from the <phrase>Boston Consulting Group</phrase> is gratefully acknowledged.
Error Analysis and Handling in <phrase>Arabic</phrase> ICALL Systems <phrase>Arabic</phrase> is a Semitic <phrase>language</phrase> that is rich in its <phrase>morphology</phrase> and <phrase>syntax</phrase>. The very numerous and complex <phrase>grammar</phrase> rules of the <phrase>language</phrase> could be confusing even for <phrase>Arabic</phrase> <phrase>native</phrase> speakers. Many <phrase>Arabic</phrase> intelligent <phrase>computer-assisted language-learning</phrase> (ICALL) systems have neither deep error analysis nor sophisticated error handling. In this <phrase>paper</phrase>, we <phrase>report</phrase> an attempt at developing an error analyzer and error handler for <phrase>Arabic</phrase> as an important part of the <phrase>Arabic</phrase> ICALL system. In this system, the learners are encouraged to construct sentences freely in various contexts and are guided to recognize by themselves the errors or inappropriate usage of their <phrase>language</phrase> constructs. We used <phrase>natural language processing</phrase> (<phrase>NLP</phrase>) tools such as a morphological analyzer and a <phrase>syntax</phrase> analyzer for error analysis and to give <phrase>feedback</phrase> to the learner. Furthermore, we propose a mechanism of correction by the learner, which allows the learner to correct the typed sentence independently. This will result in the learner being able to figure out what the error is. Examples of error analysis and error handling will be given and will illustrate how the system works.
<phrase>Large Scale</phrase> Estimation in Cyberphysical Systems using Streaming <phrase>Data</phrase>: a <phrase>Case Study</phrase> with <phrase>Smartphone</phrase> Traces Controlling and analyzing cyberphysical and <phrase>robotics</phrase> systems is increasingly becoming a <phrase>Big Data</phrase> challenge. Pushing this <phrase>data</phrase> to, and processing in the <phrase>cloud</phrase> is more efficient than on-board processing. However, current <phrase>cloud-based</phrase> solutions are not suitable for the latency requirements of these applications. We present a new concept, Discretized Streams or D-Streams, that enables massively scalable computations on streaming <phrase>data</phrase> with latencies as <phrase>short</phrase> as a second. We experiment with an implementation of D-Streams on top of the Spark <phrase>computing</phrase> framework. We demonstrate the usefulness of this concept with a novel <phrase>algorithm</phrase> to estimate vehicular traffic in <phrase>urban</phrase> networks. Our online EM <phrase>algorithm</phrase> can estimate traffic on a very large <phrase>city network</phrase> (the <phrase>San Francisco Bay Area</phrase>) by processing tens of thousands of observations per second, with a latency of a few seconds. Note to Practitioners This work was driven by the need to estimate vehicular traffic at a <phrase>large scale</phrase>, in an online setting, using <phrase>commodity</phrase> hardware. <phrase>Machine Learning</phrase> <phrase>algorithms</phrase> combined with streaming <phrase>data</phrase> are not new, but it still requires deep expertise both in <phrase>Machine Learning</phrase> and in Computer Systems to achieve <phrase>large scale</phrase> computations in a tractable manner. The Streaming Spark project aims at providing an interface that abstracts out all the technical details of the computation platform (<phrase>cloud</phrase>, HPC, <phrase>workstation</phrase>, etc.). As shown in this work, Streaming Spark is suitable for implementing and calibrating non-trivial <phrase>algorithms</phrase> on a large cluster, and provides an intuitive yet powerful <phrase>programming</phrase> interface. The readers are invited to refer to the <phrase>source code</phrase> referred in this article for more examples. This <phrase>article presents</phrase> <phrase>algorithms</phrase> to sample and compute densities for Gamma <phrase>random variables</phrase> restricted to a <phrase>hyperplane</phrase> (i.e. <phrase>distributions</phrase> of the form <phrase>Ti</phrase>| j jTj = d with Tj independant Gamma <phrase>distributions</phrase>). It is common in this case to use Gaussian <phrase>random variables</phrase> because of <phrase>closed form</phrase> solutions to solve. If one considers positive valued <phrase>distributions</phrase> with heavy tails, our formulas using gamma <phrase>distributions</phrase> may be more suitable.
Learnable Visual Markers We propose a new approach to designing visual markers (analogous to <phrase>QR-codes</phrase>, markers for <phrase>augmented reality</phrase>, and robotic fiducial tags) based on the advances in deep generative networks. In our approach, the markers are obtained as color images synthesized by a deep network from input <phrase>bit</phrase> <phrase>strings</phrase>, whereas another deep network is trained to recover the <phrase>bit</phrase> <phrase>strings</phrase> back from the photos of these markers. The two networks are trained simultaneously in a joint <phrase>backpropagation</phrase> process that takes characteristic <phrase>photometric</phrase> and geometric distortions associated with marker fabrication and marker scanning into account. Additionally, a styl-ization loss based on <phrase>statistics</phrase> of activations in a pretrained classification network can be inserted into the learning in <phrase>order</phrase> to shift the marker appearance towards some texture <phrase>prototype</phrase>. In the experiments, we demonstrate that the markers obtained using our approach are capable of retaining <phrase>bit</phrase> <phrase>strings</phrase> that are <phrase>long</phrase> enough to be practical. The ability to automatically adapt markers according to the usage scenario and the desired capacity as well as the ability to combine <phrase>information</phrase> encoding with artistic stylization are the unique properties of our approach. As a byproduct, our approach provides an insight on the structure of patterns that are most suitable for recognition by ConvNets and on their ability to distinguish composite patterns.
Probabilistic <phrase>Algorithm</phrase> for List Viterbi Decoding <phrase>Digital</phrase> <phrase>communication</phrase> has seen tremendous growth in the past two decades. A <phrase>digital</phrase> <phrase>communication</phrase> system can be broken down into the following components: <phrase>channel</phrase> encoder, <phrase>communication</phrase> <phrase>channel</phrase>, and <phrase>channel</phrase> decoder. The encoder adds structured redundancy into the <phrase>data</phrase>, the <phrase>communication</phrase> <phrase>channel</phrase> introduces noise into the system while the decoder performs <phrase>algorithms</phrase> to estimate the original <phrase>data</phrase> from a noisy one. There exist numerous encoding and decoding <phrase>algorithms</phrase>, each of which is characterized by various parameters like accuracy, <phrase>memory</phrase>, <phrase>computational complexity</phrase> and domain of applicability. Convolutional codes are a class of <phrase>channel</phrase> codes that are used in a <phrase>variety</phrase> of applications like <phrase>CDMA</phrase>, <phrase>GSM</phrase>-cellular, deep space <phrase>communication</phrase>, 802.11 <phrase>wireless</phrase> <phrase>LANs</phrase> etc. The widely used decoding <phrase>algorithm</phrase> for convolutional codes is the <phrase>Viterbi algorithm</phrase>. A <phrase>Viterbi algorithm</phrase> produces a <phrase>maximum-likelihood</phrase> estimate of the original <phrase>data</phrase>. A list-<phrase>Viterbi algorithm</phrase> (of list size L) produces L best estimates of the original <phrase>data</phrase>. Although <phrase>results</phrase> show an improvement in performance when a list-Viterbi decoder is used as opposed to a Viterbi decoder, implementing a list-Viterbi decoder can prove to be expensive in terms of <phrase>memory</phrase> and computations. In this <phrase>thesis</phrase>, we propose an novel probabilistic <phrase>algorithm</phrase> to emulate a list-Viterbi de-coder. The <phrase>motivation</phrase> of the <phrase>algorithm</phrase> stems from the enormous complexity and <phrase>memory</phrase> requirements involved in a list-<phrase>Viterbi algorithm</phrase>, and the inexistence of an off-the-shelf list-Viterbi module. Our probabilistic <phrase>algorithm</phrase> uses an off-the-shelf <phrase>Viterbi algorithm</phrase> module, and using prior <phrase>information</phrase>, 'pins' the noisy <phrase>sequence</phrase> a number of times to recreate the list. We present a complexity comparison of the probabilistic <phrase>algorithm</phrase> against the list-<phrase>Viterbi algorithm</phrase>. We conclude the <phrase>thesis</phrase> by addressing some of the issues that arise while applying the algoithm to convolutional codes. <phrase>ii</phrase> Acknowledgements I am indebted to my advisor, <phrase>Professor</phrase> Stark <phrase>Draper</phrase> for his guidance and encouragement. His commitment and drive to the highest standards of <phrase>research</phrase> and his dedication to the vocation of mentorship have been inspiring. His simultaneous keen attention to minute technical details and broad overview has enormously helped shape my <phrase>research</phrase>. I have learned greatly from the several discussions with him, and hope to inherit his style of approach to <phrase>research</phrase> and <phrase>problem solving</phrase>.
Global Refinement of <phrase>Random Forest</phrase> Y X <phrase>Training Samples</phrase> Rf 2k <phrase>Leaf</phrase> Nodes Rf 10k <phrase>Leaf</phrase> Nodes Ours 2k <phrase>Leaf</phrase> Nodes Rf 100k <phrase>Leaf</phrase> Nodes <phrase>Random forest</phrase> is well known as one of the best learning methods. In spite of its <phrase>great success</phrase>, it also has certain drawbacks: the <phrase>heuristic</phrase> learning rule does not effectively minimize the global training loss; the <phrase>model</phrase> size is usually too large for many real applications. To address the issues, we propose two techniques, global refinement and global <phrase>pruning</phrase>, to improve a <phrase>pre-trained</phrase> <phrase>random forest</phrase>. The proposed global refinement jointly relearns the <phrase>leaf</phrase> nodes of all <phrase>trees</phrase> under a global <phrase>objective function</phrase> so that the complementary <phrase>information</phrase> between multiple <phrase>trees</phrase> is well exploited. In this way, the fitting power of the <phrase>forest</phrase> is significantly enhanced. The global <phrase>pruning</phrase> is developed to reduce the <phrase>model</phrase> size as well as the over-fitting <phrase>risk</phrase>. The refined <phrase>model</phrase> has better performance and smaller storage cost, as verified in <phrase>extensive experiments</phrase>. <phrase>Random forest</phrase> [2] is one of the most popular learning methods and has many ideal properties: 1) it is simple to understand and implement; 2) it is strong in handling non-linearity and <phrase>outliers</phrase>; 3) it is <phrase>friendly</phrase> to parallel training and large <phrase>data</phrase>; and 4) it is fast in testing. Recently, it has <phrase>proven</phrase> extremely successful on important applications in <phrase>data mining</phrase> [12] and <phrase>computer vision</phrase> [6, 11]. In spite of its <phrase>great success</phrase>, <phrase>random forest</phrase> has certain insufficiency from both theoretical and practical viewpoints. Theoretically, the <phrase>heuristic</phrase> learning of <phrase>random forest</phrase> is suboptimal in terms of minimizing training error. Specifically, each individual <phrase>tree</phrase> is learnt independently and greedily. Such learning does not fully utilize complementary <phrase>information</phrase> among different <phrase>trees</phrase>. Practically, for complex real problems [3, 4, 6, 11], deep <phrase>trees</phrase> are usually required to fit the <phrase>training data</phrase> well. This <phrase>results</phrase> in <phrase>high</phrase> storage cost, which is a serious issue especially for <phrase>embedded devices</phrase> such as <phrase>mobile phone</phrase> or <phrase>Kinect</phrase>. While <phrase>tree</phrase> <phrase>pruning</phrase> can reduce the <phrase>tree</phrase> size, <phrase>existing methods</phrase> [7, 8] are independently performed on individual <phrase>trees</phrase> and could degrade the performance of <phrase>random forest</phrase>. To address the above problems, we propose a simple and effective method to refine a <phrase>pre-trained</phrase> <phrase>random forest</phrase>. We notice that the learning and prediction of <phrase>random forest</phrase> is inconsistent: the learning of individual <phrase>trees</phrase> is <phrase>independent</phrase> but the prediction averages all <phrase>trees</phrase>' outputs. The loss functions implied from these two processes are actually different. This limits the fitting power of <phrase>random forest</phrase>. To alleviate such inconsistency, we discard the old values stored in all <phrase>tree</phrase> <phrase>leaves</phrase> 
Image Prediction for Limited-angle Tomography via <phrase>Deep Learning</phrase> with <phrase>Convolutional Neural Network</phrase> Limited angle problem is a challenging issue in <phrase>x-ray</phrase> <phrase>computed tomography</phrase> (CT) field. Iterative <phrase>reconstruction</phrase> methods that utilize the additional prior can suppress artifacts and improve image quality, but unfortunately require increased computation time. An interesting way is to restrain the artifacts in the images reconstructed from the practical filtered back projection (FBP) method. Frikel and Quinto have proved that the streak artifacts in FBP <phrase>results</phrase> could be characterized. It indicates that the artifacts created by FBP method have specific and similar characteristics in a stationary limited-angle scanning configuration. Based on this understanding, this work aims at developing a method to extract and suppress specific artifacts of FBP reconstructions for limited-angle tomography. A <phrase>data</phrase>-driven learning-based method is proposed based on a <phrase>deep convolutional</phrase> <phrase>neural network</phrase>. An <phrase>end-to-end</phrase> mapping between the FBP and artifact-<phrase>free</phrase> images is learned and the implicit features involving artifacts will be extracted and suppressed via nonlinear mapping. The qualitative and quantitative evaluations of <phrase>experimental</phrase> <phrase>results</phrase> indicate that the <phrase>proposed method</phrase> show a Corresponding authors: stable and prospective performance on artifacts reduction and detail recovery for limited angle tomography. The presented strategy provides a simple and efficient approach for improving image quality of the <phrase>reconstruction</phrase> <phrase>results</phrase> from limited projection <phrase>data</phrase>.
Spectral classification using <phrase>convolutional neural networks</phrase> There is a great need for accurate and autonomous spectral classification methods in <phrase>astrophysics</phrase>. This <phrase>thesis</phrase> is about training a <phrase>convolutional neural network</phrase> (ConvNet) to recognize an object class (<phrase>quasar</phrase>, <phrase>star</phrase> or <phrase>galaxy</phrase>) from one-<phrase>dimension</phrase> spectra only. <phrase>Author</phrase> developed several scripts and C programs for datasets preparation, preprocessing and postprocessing of the <phrase>data</phrase>. EBLearn <phrase>library</phrase> (developed by Pierre Sermanet and Yann LeCun) was used to create ConvNets. Application on dataset of more than 60000 spectra yielded success rate of nearly 95%. This <phrase>thesis</phrase> conclusively proved great potential of <phrase>convolutional neural networks</phrase> and <phrase>deep learning</phrase> methods in <phrase>astrophysics</phrase>. I'd like to thank my girlfriend Elena Lindiov for her moral support during <phrase>research</phrase> that <phrase>led</phrase> to writing this <phrase>thesis</phrase>. I declare that I wrote my <phrase>diploma</phrase> <phrase>thesis</phrase> independently and exclusively using sources cited. I agree with borrowing the work and its <phrase>publishing</phrase>.
Voice conversion from non-<phrase>parallel corpora</phrase> using variational <phrase>auto-encoder</phrase> We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require <phrase>parallel corpora</phrase>, phonetic alignments , or explicit frame-wise correspondence for learning conversion functions or for synthesizing a <phrase>target</phrase> <phrase>spectrum</phrase> with the aid of alignments. However, these requirements gravely limit the scope of practical applications of SC due to scarcity or even unavailability of <phrase>parallel corpora</phrase>. We propose an SC framework based on variational <phrase>auto-encoder</phrase> which enables us to exploit non-<phrase>parallel corpora</phrase>. The framework comprises an encoder that learns <phrase>speaker</phrase>-<phrase>independent</phrase> phonetic representations and a decoder that learns to reconstruct the designated <phrase>speaker</phrase>. It removes the requirement of <phrase>parallel corpora</phrase> or phonetic alignments to <phrase>train</phrase> a spectral conversion system. We <phrase>report</phrase> objective and subjective evaluations to validate our <phrase>proposed method</phrase> and compare it to SC methods that have access to aligned corpora. I. INTRODUCTION Voice conversion is a technique that converts the perceived identity of <phrase>speaker</phrase> of a given utterance. A typical case is that, when one wants to convert his or her voices into a celebrity's, it is required that <phrase>linguistic</phrase> contents and other <phrase>speaker</phrase>-unrelated <phrase>information</phrase> remain unchanged after conversion. A complete voice conversion system involves many tasks. In this study, we devote our focus on spectral conversion (SC) and leave inspection on prosody outside the scope of this <phrase>paper</phrase>. A wide <phrase>variety</phrase> of techniques have been applied to spectral conversion, including <phrase>Gaussian mixture</phrase> models (GMM) [1] [3], <phrase>frequency</phrase> <phrase>warping</phrase> [4], [5], <phrase>deep neural networks</phrase> (DNN) [6][8], and exemplar-based approaches [9], [10]. Most of these methods demand aligned source-<phrase>target</phrase> pairs of frames or alignments of phonetic states to <phrase>train</phrase> conversion functions or adaptation transformations. The most widely adopted approach is to align source and <phrase>target</phrase> frames using dynamic time <phrase>warping</phrase> (DTW) technique. However, DTW fails to work if <phrase>parallel corpora</phrase> are unavailable. Many techniques have been conceived to align source and <phrase>target</phrase> frames in non-<phrase>parallel corpora</phrase>. The most intuitive way is to apply a speech recognizer to the utterances, and proceed with explicit alignment or <phrase>model</phrase> adaptation [11], [12]. Applying speech recognizers to each utterance gives every frame a phonetic <phrase>label</phrase> (usually of phonetic states). It is particularly suitable for <phrase>model</phrase>-based voice conversion techniques because they can readily utilize these labeled frames [13]. The problem with this frame-wise, <phrase>model-based</phrase> approach is that it does not
Sentence Compression as a Step in Summarization or an <phrase>Alternative</phrase> Path in Text Shortening The originality of this work leads in tackling text compression using an unsuper-vised method, based on a <phrase>deep linguistic</phrase> analysis, and without resorting on a learning corpus. This work presents a system for dependent <phrase>tree</phrase> <phrase>pruning</phrase>, while preserving the <phrase>syntactic</phrase> coherence and the main informational contents, and <phrase>led</phrase> to an operational <phrase>software</phrase>, named COLIN. Experiment <phrase>results</phrase> show that our compressions get honorable satisfaction levels, with a mean <phrase>compression ratio</phrase> of 38 %.
Confessions of a <phrase>human</phrase>-centered designer or a day out in the sunshine. I learned how to empathize with her, and I felt it to be imperative that someone create <phrase>products</phrase> and services that would give her an easier way to interact with her world. I was ready to step up to the task. Fast-<phrase>forward</phrase> in time. Armed with a freshly minted <phrase>master's degree</phrase> in <phrase>interaction design</phrase>, I had been formally trained in user <phrase>research</phrase> methods to gain a <phrase>deep understanding</phrase> of people's spoken and unspoken needs. I joined <phrase>e</phrase>-lab, her when she cried out for help the <phrase>bus</phrase> driver would often fail to see that she was waiting for the <phrase>bus</phrase> and drive past her. Over the next few months, I got to be <phrase>friends</phrase> with Mabel, keeping her <phrase>company</phrase> when I spotted her on the corner and helping her <phrase>flag</phrase> down the <phrase>bus</phrase>. I saw the world and her many frustrations in navigating it through her eyes. I saw her fear and helplessness and experienced it firsthand. I also saw her happiness and pleasure in the simple things like a meal from her I never intended to be a designer. I wanted to be a famous <phrase>artist</phrase>. As an <phrase>undergraduate</phrase> <phrase>art</phrase> <phrase>student</phrase>, I created a series of fake <phrase>products</phrase> that did absolutely nothing as a statement of disdain about <phrase>consumerism</phrase> in the modern world. But then I experienced a situation of real need, the eye-opening <phrase>revelation</phrase> that started my journey as a real designer. An elderly lady, stooped into a full bow from <phrase>osteoporosis</phrase>, often waited for the <phrase>bus</phrase> across the street from my <phrase>apartment</phrase>. I first noticed Jodi Forlizzi is an associate <phrase>professor</phrase> of <phrase>design</phrase> and <phrase>human</phrase>-computer interaction at <phrase>carnegie mellon university</phrase>. She also leads the <phrase>human</phrase>-Systems interaction Group of the Quality of <phrase>life</phrase> <phrase>Technology</phrase> <phrase>center</phrase>, an <phrase>NSF</phrase> <phrase>engineering</phrase> <phrase>research</phrase> <phrase>center</phrase>.
<phrase>High</phrase>-performance <phrase>Neural Networks</phrase> for <phrase>Visual Object</phrase> Classification <phrase>High</phrase>-performance <phrase>Neural Networks</phrase> for <phrase>Visual Object</phrase> Classification We present a fast, fully parameterizable <phrase>GPU</phrase> implementation of <phrase>Convolutional Neural Network</phrase> variants. Our feature extractors are neither carefully designed nor pre-<phrase>wired</phrase>, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best <phrase>published results</phrase> on benchmarks for object classification (NORB, CIFAR10) and <phrase>handwritten digit</phrase> recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. <phrase>Test</phrase> error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.
<phrase>Natural Language Processing</phrase> Tools for <phrase>Computer Assisted Language Learning</phrase> This <phrase>paper</phrase> illustrates the usefulness of <phrase>natural language processing</phrase> (<phrase>NLP</phrase>) tools for <phrase>computer assisted language learning</phrase> (CALL) through the presentation of three <phrase>NLP</phrase> tools integrated within a CALL <phrase>software</phrase> for <phrase>French</phrase>. These tools are (i) a sentence structure viewer; (<phrase>ii</phrase>) an error diagnosis system; and (iii) a conjugation tool. The sentence structure viewer helps <phrase>language</phrase> learners grasp the structure of a sentence, by providing lexical and grammatical <phrase>information</phrase>. This <phrase>information</phrase> is derived from a <phrase>deep syntactic</phrase> analysis. Two different outputs are presented. The error diagnosis system is composed of a <phrase>spell checker</phrase>, a <phrase>grammar</phrase> checker, and a coherence checker. The <phrase>spell checker</phrase> makes use of alpha-codes, <phrase>phonological</phrase> reinterpre-tation, and some <phrase>ad hoc</phrase> rules to provide correction proposals. The <phrase>grammar</phrase> checker employs constraint relaxation and <phrase>phonological</phrase> reinterpretation as diagnosis techniques. The coherence checker compares the underlying "<phrase>semantic</phrase>" structures of a stored answer and of the learners' input to detect <phrase>semantic</phrase> discrepancies. The conjugation tool is a resource with enhanced capabilities when put on an <phrase>electronic</phrase> format, enabling searches from <phrase>inflected</phrase> and ambiguous <phrase>verb</phrase> forms. 1 Introduction <phrase>Computer assisted language learning</phrase> (CALL) emerged in the early days of <phrase>computers</phrase>. Since the early 1960's, CALL <phrase>software</phrase> was designed and implemented. In the drill-and-practice days of <phrase>language</phrase> teaching methodology, what could, better than a computer, repeatedly and relentlessly propose exercises to the learners? Since then, advances in <phrase>second language acquisition</phrase> (SLA) have helped the shift of interest from repetitive exercises to more communicative tasks. The <phrase>range</phrase> of exercises offered in CALL <phrase>software</phrase> has always been constrained by the kind of <phrase>feedback</phrase> judged necessary for the learning process and its availability on a computer. Thus, too often, exercises tend to elicit answers easily classified as right or wrong. This leads to all sorts of <phrase>multiple-choice</phrase>-questions, fill-in-the-blanks, and similar exercise types which <phrase>computers</phrase> can very easily correct automatically. For slightly more complex exercises requiring the learners to write a few words, either no <phrase>feedback</phrase> is provided or <phrase>pattern matching</phrase>
Sum-Product Networks: A New <phrase>Deep Architecture</phrase> The key limiting factor in <phrase>graphical model</phrase> inference and learning is the complexity of the <phrase>partition</phrase> <phrase>function</phrase>. We thus ask the question: what are the most <phrase>general</phrase> conditions under which the <phrase>partition</phrase> <phrase>function</phrase> is tractable? The answer leads to a new kind of <phrase>deep architecture</phrase>, which we call sum-product networks (SPNs). SPNs are <phrase>directed</phrase> <phrase>acyclic</phrase> <phrase>graphs</phrase> with variables as <phrase>leaves</phrase>, sums and <phrase>products</phrase> as internal nodes, and weighted edges. We show that if an SPN is complete and consistent it represents the <phrase>partition</phrase> <phrase>function</phrase> and all marginals of some <phrase>graphical model</phrase>, and give <phrase>semantics</phrase> to its nodes. Essentially all tractable <phrase>graphical</phrase> models can be cast as SPNs, but SPNs are also strictly more <phrase>general</phrase>. We then propose <phrase>learning algorithms</phrase> for SPNs, based on <phrase>backpropagation</phrase> and EM. Experiments show that inference and learning with SPNs can be both faster and more accurate than with standard deep networks. For example, SPNs perform face completion better than <phrase>state</phrase>-of-the-<phrase>art</phrase> deep networks for this task. SPNs also have intriguing potential connections to the <phrase>architecture</phrase> of the <phrase>cortex</phrase>.
Interactions: authenticity, complexity, and <phrase>design</phrase> on the nuances of what makes us humanmat-ters of <phrase>cognitive psychology</phrase>, social interaction, and the desire for emotional <phrase>resonance</phrase>. This issue of interactions unpacks all of these ideas, exploring the <phrase>gestalt</phrase> of interaction design's influence. Sarah Kettley, a researcher and an <phrase>artist</phrase>, is most interested in understanding the relationship between wearable <phrase>computing</phrase> and body adornment. She writes about the relationship between craftsmanship and authenticity, illustrating a potential divide between <phrase>design</phrase> and craft. We can see a similar exploration of the ethereal in William Odom, Richard <phrase>Banks</phrase>, and Dave Kirk's piece " Reciprocity, Deep Storage, and Letting Go: Opportunities for Designing Interactions with Inherited <phrase>Digital</phrase> Materials " ; these authors are looking to understand " how the <phrase>digital</phrase> residue of a person's <phrase>life</phrase> could become the <phrase>property</phrase> of someone else and be representative of a person after they have passed on. " And Liz Danzico's column focuses on the <phrase>nature</phrase> of <phrase>serendipity</phrase> and <phrase>design</phrase>. If <phrase>design</phrase> is careful planning, and <phrase>serendipity</phrase> is a desired <phrase>state</phrase> of unplannedness, what can interaction designers learn from serendipityand what can we reap-propriate in our work? Several pieces in this issue look less at how to integrate <phrase>human</phrase> qualities into <phrase>design</phrase>, and instead at how to evaluate the qualities of <phrase>design</phrase> on <phrase>human</phrase> problems. Steve Baty examines why <phrase>design</phrase> is suitable forand perhaps best prepared forhandling complex problems, and Graham Pullin and Andrew Cook show how this form of <phrase>problem solving</phrase> can lend new capabilities to those with a form of <phrase>disability</phrase>. In Dana Chisnell's review of Pullin's <phrase>book</phrase>, <phrase>Design</phrase> Meets <phrase>Disability</phrase>, she explains that the text is " a call to <phrase>action</phrase> against an old way of thinking, in which <phrase>design</phrase> for <phrase>disability</phrase> is solving a <phrase>medical</phrase> <phrase>engineering</phrase> problem rather than meeting a <phrase>cultural</phrase>, societal challenge. " Our old <phrase>friends</phrase> Don <phrase>Norman</phrase> and Jakob Nielsen take gestural interfaces to task, exploring the usabilityor lack thereofin a number of the most popular touch devices we've come to take for granted. Our <phrase>cover</phrase> story offers a thoughtful reflection on the qualities of affinityof emotional attraction , <phrase>nostalgia</phrase>, identity, and <phrase>language</phrase>. <phrase>Matthew</phrase> <phrase>Jordan</phrase> describes affinity as the " emotional connection someone feels for a product or service as driven by these notions of beauty and identity. Unlike usefulness and <phrase>usability</phrase>, affinity is about unexplained desire or want. It is often irrational, <phrase>fluid</phrase>, and intense. Affinity is the opposite of aversion, and affinity is always positive. Along with usefulness 
Learning to Detect User Activity and Availability from a <phrase>Variety</phrase> of <phrase>Sensor</phrase> <phrase>Data</phrase> Using a networked <phrase>infrastructure</phrase> of easily available sensors and context-processing components, we are developing applications for the support of workplace interactions. Notions of activity and availability are learned from labeled <phrase>sensor</phrase> <phrase>data</phrase> based on a <phrase>Bayesian</phrase> approach. The <phrase>higher-level</phrase> <phrase>information</phrase> on the users is then automatically derived from <phrase>low-level</phrase> <phrase>sensor</phrase> <phrase>information</phrase> in <phrase>order</phrase> to facilitate informal <phrase>ad hoc</phrase> communications between peer workers in an office environment. 1. Opportunistic meetings In an office environment, unplanned interactions <phrase>play</phrase> an important role in the exchange of <phrase>information</phrase> as well as for the social cohesion of a group, their importance should not be neglected, as they are often <phrase>instrumental</phrase> to the <phrase>organization</phrase> [10]. In contrast to simply spontaneous encounters, opportunistic meetings occur where one person has a reason to <phrase>talk</phrase> with another but has not arranged to meet in advance and thus either seeks them out or <phrase>waits</phrase> until an encounter occurs by chance to raise the issue [9]. However, there is always the danger that the encounter might not take place (or not in a timely manner), leaving the issue unaddressed. In this <phrase>paper</phrase> we consider how such meetings, referred to here as IFFI's (for 'informal face-to-face interactions'), might be facilitated. In the first place by alleviating the burden of finding an appropriate time and place to meet while taking into account each person's constraints and activities. Facilitating IFFI's differs from making a rendezvous, since it enables a deep interleaving with current ongoing activities so as to support the setting-up of an interaction in near real-time. Secondly, these interactions should be supported in situ and in an <phrase>ad-hoc</phrase> manner, as they cannot be logistically prearranged. For instance, by discovering surrounding devices, such as printers or screens, that may support the interaction by providing convenient access to relevant documents. Finally, putting in perspective series of such interactions helps users to better articulate their work, for instance by reminding them of previous related interactions with the same persons or on the same topic. As described above, these interactions are opportunistic in <phrase>nature</phrase>, and therefore depend upon the ability of the people involved to correctly interpret explicit and implicit cues. For instance, a colleague might be available to answer a <phrase>short</phrase> question if he has just finished a phone call and has not yet turned back to continue work on his presentation slides. On the other hand he might not be available if he has closed 
Automatic Identification of Instrument Classes in <phrase>Polyphonic</phrase> and Poly-Instrument Audio We present and compare several models for automatic identification of instrument classes in <phrase>polyphonic</phrase> and poly-instrument audio. The goal is to be able to identify which categories of instrument (<phrase>Strings</phrase>, <phrase>Woodwind</phrase>, <phrase>Guitar</phrase> , <phrase>Piano</phrase>, etc.) are present in a given audio example. We use a <phrase>machine learning</phrase> approach to solve this task. We constructed a system to generate a large <phrase>database</phrase> of musically relevant poly-instrument audio. Our <phrase>database</phrase> is generated from hundreds of instruments classified in 7 categories. <phrase>Musical</phrase> audio examples are generated by <phrase>mixing</phrase> multi-<phrase>track</phrase> <phrase>MIDI</phrase> files with thousands of instrument combinations. We compare three different classifiers : a <phrase>Support Vector Machine</phrase> (<phrase>SVM</phrase>), a <phrase>Multilayer Perceptron</phrase> (MLP) and a <phrase>Deep Belief</phrase> Network (DBN). We show that the DBN tends to outperform both the <phrase>SVM</phrase> and the MLP in most cases.
Complex Events Detection Using <phrase>Data</phrase>-Driven Concepts Automatic <phrase>event detection</phrase> in a large collection of uncon-strained videos is a challenging and important task. The key issue is to describe <phrase>long</phrase> complex <phrase>video</phrase> with <phrase>high</phrase> level <phrase>semantic</phrase> descriptors, which should find the regularity of events in the same category while distinguish those from different categories. This <phrase>paper</phrase> proposes a novel unsupervised approach to discover <phrase>data</phrase>-driven concepts from multi-modality signals (audio, scene and motion) to describe <phrase>high</phrase> level <phrase>semantics</phrase> of videos. Our methods consists of three main components: we first learn the <phrase>low-level</phrase> features separately from three modalities. Secondly we discover the <phrase>data</phrase>-driven concepts based on the <phrase>statistics</phrase> of <phrase>learned features</phrase> mapped to a <phrase>low dimensional</phrase> space using <phrase>deep belief</phrase> nets (DBNs). Finally, a compact and robust sparse representation is learned to jointly <phrase>model</phrase> the concepts from all three modalities. Extensive <phrase>experimental</phrase> <phrase>results</phrase> on large in-the-wild dataset show that our <phrase>proposed method</phrase> <phrase>significantly outperforms</phrase> <phrase>state</phrase>-of-the-<phrase>art</phrase> methods.
Menu <phrase>Design</phrase> with Visual <phrase>Momentum</phrase> for Compact Smart <phrase>Products</phrase> Users of compact smart <phrase>products</phrase> with small screens often have trouble learning the menu structure. If they cannot <phrase>master</phrase> the menu structure, users are not able to fully utilize the <phrase>products</phrase>. It is argued in this <phrase>paper</phrase> that using visual <phrase>momentum</phrase> in menu representation <phrase>design</phrase> helps users develop effective mental maps of menu structures and promotes learning of the <phrase>user interface</phrase>. To assess the effect of visual <phrase>momentum</phrase> in this study, four types of menu representations were developed. Additionally, two menu hierarchies, two types of <phrase>function</phrase> key layout, and two types of <phrase>function</phrase> key labeling were assessed to examine the effects of menu <phrase>dimension</phrase> and compatibility. <phrase>Experimental</phrase> <phrase>results</phrase> indicated that participants using a partial menu map with visual <phrase>momentum</phrase> <phrase>design</phrase> performed the best, and participants using a partial menu map without visual <phrase>momentum</phrase> performed the poorest, even worse than those-using command-only representation. The <phrase>results</phrase> also showed that the menu <phrase>navigation</phrase> problem appeared to be particularly significant with a deep menu hierarchy. Actual or potential applications of this <phrase>research</phrase> include menu representation <phrase>design</phrase> for compact smart <phrase>products</phrase>.
<phrase>Deep transfer</phrase> via second-<phrase>order</phrase> <phrase>Markov</phrase> <phrase>logic</phrase> Standard inductive learning requires that training and <phrase>test</phrase> instances come from the same distribution. <phrase>Transfer learning</phrase> seeks to remove this restriction. In shallow transfer, <phrase>test</phrase> instances are from the same domain, but have a different distribution. In <phrase>deep transfer</phrase>, <phrase>test</phrase> instances are from a different domain entirely (i.e., described by different predicates). Humans routinely perform <phrase>deep transfer</phrase>, but few learning systems, if any, are capable of it. In this <phrase>paper</phrase> we propose an approach based on a form of second-<phrase>order</phrase> <phrase>Markov</phrase> <phrase>logic</phrase>. Our <phrase>algorithm</phrase> discovers structural regularities in the source domain in the form of <phrase>Markov</phrase> <phrase>logic</phrase> formulas with predicate variables, and instantiates these formulas with predicates from the <phrase>target</phrase> domain. Using this approach, we have successfully transferred learned <phrase>knowledge</phrase> among <phrase>molecular biology</phrase>, <phrase>social network</phrase> and Web domains. The discovered patterns include broadly useful properties of predicates, like <phrase>symmetry</phrase> and transitivity, and relations among predicates, such as various forms of homophily.
<phrase>Color Space</phrase> Transformation Network Deep networks have become very popular over the past few years. The main reason for this widespread use is their excellent ability to learn and predict <phrase>knowledge</phrase> in a very easy and efficient way. <phrase>Convolutional neural networks</phrase> and <phrase>auto-encoders</phrase> have become the normal in the <phrase>area</phrase> of imaging and <phrase>computer vision</phrase> achieving unprecedented <phrase>BLOCKIN BLOCKIN</phrase> accuracy <phrase>BLOCKIN BLOCKIN</phrase> levels <phrase>BLOCKIN BLOCKIN</phrase> in <phrase>BLOCKIN BLOCKIN</phrase> many <phrase>BLOCKIN BLOCKIN</phrase> applications. <phrase>BLOCKIN BLOCKIN</phrase> The <phrase>BLOCKIN BLOCKIN</phrase> most <phrase>BLOCKIN BLOCKIN</phrase> common <phrase>BLOCKIN BLOCKIN</phrase> strategy <phrase>BLOCKIN BLOCKIN</phrase> is to build and <phrase>train</phrase> networks with many layers by tuning their hyper-parameters. While this approach has <phrase>proven</phrase> to be a successful way to build robust <phrase>deep learning</phrase> schemes <phrase>BLOCKIN BLOCKIN</phrase> it <phrase>BLOCKIN BLOCKIN</phrase> suffers <phrase>BLOCKIN BLOCKIN</phrase> from <phrase>BLOCKIN BLOCKIN</phrase> <phrase>high</phrase> <phrase>BLOCKIN BLOCKIN</phrase> complexity. <phrase>BLOCKIN BLOCKIN</phrase> In <phrase>BLOCKIN BLOCKIN</phrase> this <phrase>BLOCKIN BLOCKIN</phrase> <phrase>paper</phrase> <phrase>BLOCKIN BLOCKIN</phrase> we <phrase>BLOCKIN BLOCKIN</phrase> introduce <phrase>BLOCKIN BLOCKIN</phrase> a <phrase>BLOCKIN BLOCKIN</phrase> module <phrase>BLOCKIN BLOCKIN</phrase> that learns <phrase>color space</phrase> transformations within a network. Given a large dataset of colored images the <phrase>color space</phrase> transformation module tries to learn <phrase>color space</phrase> transformations that increase overall <phrase>classification accuracy</phrase>. This module has shown to increase overall accuracy for the same network <phrase>design</phrase> and to achieve faster BLOCKIN
<phrase>Long</phrase>-term <phrase>anti</phrase>-kindling effects of desynchronizing <phrase>brain</phrase> stimulation: a theoretical study In a modeling study we show that desynchronization stimulation may have powerful <phrase>anti</phrase>-kindling effects. For this, we incorporate spike-timing-dependent plasticity into a generic network of coupled phase oscillators, which serves as a <phrase>model</phrase> network of synaptically interacting <phrase>neurons</phrase>. Two states may coexist under spontaneous conditions: a <phrase>state</phrase> of uncorrelated firing and a <phrase>state</phrase> of pathological synchrony. Appropriate stimulation protocols make the network learn or unlearn the pathological <phrase>synaptic</phrase> interactions, respectively. Low-<phrase>frequency</phrase> periodic pulse <phrase>train</phrase> stimulation causes a kindling. Permanent <phrase>high</phrase>-<phrase>frequency</phrase> stimulation, used as golden standard for <phrase>deep brain stimulation</phrase> in medically <phrase>refractory</phrase> movement disorders, basically freezes the <phrase>synaptic</phrase> weights. In contrast, desynchronization stimulation, e.g., by means of a multi-site coordinated reset, has powerful <phrase>long</phrase>-term <phrase>anti</phrase>-kindling effects and enables the network to unlearn pathologically strong <phrase>synaptic</phrase> interactions. We propose desynchronization stimulation for the therapy of movement disorders and epilepsies.
The Blessing and the Curse of the Multiplicative Updates <phrase>Online learning</phrase> is a <phrase>major</phrase> branch in the <phrase>computer science</phrase> field of <phrase>machine learning</phrase>. On the surface, <phrase>online learning</phrase> seems completely unrelated to <phrase>evolution</phrase>, but this chapter will show that there are in fact deep analogies. Our goal is to exploit these analogies to provide new insights into both <phrase>evolution</phrase> and <phrase>online learning</phrase>. The <phrase>basic</phrase> task in <phrase>online learning</phrase> is to process a discrete <phrase>stream</phrase> of <phrase>data</phrase>. Each piece of the <phrase>data stream</phrase> (example) consists of an instance and a <phrase>label</phrase> for the instance. The instances are vectors of a fixed <phrase>dimension</phrase> and the <phrase>labels</phrase> are <phrase>real valued</phrase>. Each trial processes one instance <phrase>vector</phrase> and its <phrase>label</phrase>. The <phrase>algorithm</phrase> first receives the instance <phrase>vector</phrase>. It then must produce a <phrase>label</phrase> for the instance <phrase>vector</phrase>. Typically this is done by maintaining one weight per <phrase>dimension</phrase> of the instance vectors and predicting the <phrase>label</phrase> using the <phrase>dot product</phrase> of the instance <phrase>vector</phrase> and the current weight <phrase>vector</phrase>. After predicting, the <phrase>online learning</phrase> <phrase>algorithm</phrase> receives the " true " <phrase>label</phrase> of the instance <phrase>vector</phrase>. For example, the instance <phrase>vector</phrase> might be the predictions of five experts on whether it will <phrase>rain</phrase> tomorrow (where 1 stands for <phrase>rain</phrase> and 0 for no <phrase>rain</phrase>). The <phrase>algorithm</phrase> maintains five weights where the ith weight is the current belief that the ith expert is the best expert. The maintained weight <phrase>vector</phrase> is a <phrase>probability</phrase> <phrase>vector</phrase>, and the algorithm's own prediction for <phrase>rain</phrase> might be the weighted <phrase>average</phrase> (<phrase>dot product</phrase>) of the predictions of the experts. The true <phrase>label</phrase> is 1 or 0, depending whether it actually rained or not on the next day. The <phrase>algorithm</phrase> is constructed to predict the <phrase>labels</phrase> " accurately " for each instance as it arrives, and to improve accuracy as more instances accumulate.
APC Forum: Leveraging Emerging <phrase>Digital</phrase> <phrase>Technology</phrase> at <phrase>BP</phrase> Executive Summary This APC Forum is the first in a series of columns from the Advanced Practices Council (APC) of the <phrase>Society</phrase> for <phrase>Information Management</phrase> (SIM). SIM's APC is an exclusive forum for senior IT executives who value directing and applying pragmatic <phrase>research</phrase>, exploring emerging IT issues in depth, and learning different, global perspectives from colleagues in other industries. This APC Forum presents the thoughts of P. P. Darukhanavala (<phrase>Daru</phrase>), CTO of <phrase>BP</phrase>. He shared his company's <phrase>digital</phrase> <phrase>technology</phrase> scanning practices with his <phrase>fellow</phrase> APC members at a recent APC meeting. In particular, this Forum describes the work of the <phrase>digital</phrase> <phrase>technology</phrase> scanning team that <phrase>Daru</phrase> created at <phrase>BP</phrase>. Heads a team responsible for matching external <phrase>technology</phrase> innovations to BP's <phrase>business</phrase> requirements. One of the world's largest <phrase>energy</phrase> companies, providing customers with <phrase>fuel</phrase> for transportation, <phrase>energy</phrase> for <phrase>heat</phrase> and <phrase>light</phrase>, <phrase>retail</phrase> services, and petrochemicals <phrase>products</phrase> for everyday items. At <phrase>BP</phrase>, IT is not seen as just a service or <phrase>function</phrase>. It is seen as an activity that can transform the <phrase>business</phrase>. As BP's CTO, <phrase>Daru</phrase> recognized his role in helping his <phrase>business</phrase> colleagues use <phrase>digital</phrase> <phrase>innovation</phrase> to gain and sustain <phrase>competitive advantage</phrase>. <phrase>Daru</phrase> created a <phrase>digital</phrase> <phrase>technology</phrase> scanning team in IT and charged it with identifying the best sources of relevant and cost-effective <phrase>digital</phrase> <phrase>technology</phrase> to meet the needs of BP's <phrase>business</phrase> units, which are in over 100 countries across six continents. The team's goal is to identify truly significant opportunities for <phrase>BP</phrase>. From the outset, <phrase>Daru</phrase> realized that his small team could not possibly keep up with rapidly changing <phrase>technology</phrase> developments on its own. Therefore he adopted a " <phrase>venture capital</phrase> " <phrase>model</phrase> of <phrase>digital</phrase> scanning, whereby the team, with its deep <phrase>knowledge</phrase> of BP's diverse businesses and broad appreciation of <phrase>technology</phrase>, links to an <phrase>ecosystem</phrase> of external <phrase>knowledge</phrase> sources. Team members meet regularly with <phrase>business</phrase> executives to <phrase>ferret</phrase> out opportunities in which <phrase>digital</phrase> <phrase>technology</phrase> could provide <phrase>competitive advantage</phrase>. They seek answers MISQE is sponsored by:
<phrase>Time series</phrase> forecasting using a <phrase>deep belief</phrase> network with <phrase>restricted Boltzmann machines</phrase> <phrase>Multi-layer</phrase> <phrase>perceptron</phrase> (MLP) and other <phrase>artificial neural networks</phrase> (ANNs) have been widely applied to <phrase>time series</phrase> forecasting since 1980s. However, for some problems such as initialization and local <phrase>optima</phrase> existing in applications, the improvement of ANNs is, and still will be the most interesting study for not only <phrase>time series</phrase> forecasting but also other intelligent <phrase>computing</phrase> fields. In this study, we propose a method for <phrase>time series</phrase> prediction using Hinton & Salakhutdinov's <phrase>deep belief</phrase> nets (DBN) which are probabilistic generative <phrase>neural network</phrase> composed by <phrase>multiple layers</phrase> of <phrase>restricted Boltzmann machine</phrase> (RBM). We use a 3-layer deep network of RBMs to capture the feature of input space of <phrase>time series</phrase> <phrase>data</phrase>, and after pretraining of RBMs using their <phrase>energy</phrase> functions, <phrase>gradient descent</phrase> training, i.e., back-propagation <phrase>learning algorithm</phrase> is used for <phrase>fine-tuning</phrase> connection weights between " visible layers " and " <phrase>hidden layers</phrase> " of RBMs. To decide the sizes of <phrase>neural networks</phrase> and the learning rates, <phrase>Kennedy</phrase> & Eberhart's <phrase>particle swarm optimization</phrase> (PSO) is adopted during the training processes. Furthermore, " trend removal " , a preprocessing to the original <phrase>data</phrase>, is also approached in the forecasting experiment using CATS benchmark <phrase>data</phrase>. Additionally, approximating and <phrase>short</phrase>-term prediction of chaotic <phrase>time series</phrase> such as Lorenz chaos and <phrase>logistic map</phrase> were also applied by the <phrase>proposed method</phrase>.
Identifying <phrase>Arbitrage</phrase> Opportunities in <phrase>E</phrase>-markets A market is in equilibrium if there is no opportunity for <phrase>arbitrage</phrase>, ie: <phrase>risk</phrase>-<phrase>free</phrase>, or low-<phrase>risk</phrase>, profit. The majority of real markets are not in equilibrium. A project is investigating the market <phrase>evolutionary</phrase> process in a particular <phrase>electronic</phrase> market that has been constructed in an ongoing collaborative <phrase>research</phrase> project between a <phrase>university</phrase> and a <phrase>software house</phrase>. The way in which <phrase>actors</phrase> (buyers, sellers and others) use the market will be influenced by the <phrase>information</phrase> available to them. In this experiment, <phrase>data mining</phrase> and filtering techniques are used to distil both individual signals drawn from the markets and signals from the <phrase>Internet</phrase> into meaningful advice for the <phrase>actors</phrase>. The goal of this experiment is first to learn how <phrase>actors</phrase> will use the advice available to them to identify <phrase>arbitrage</phrase> opportunities, and second how the market will evolve through entrepreneurial intervention. In this <phrase>electronic</phrase> market a multiagent <phrase>process management</phrase> system is used to manage all market transactions including those that drive the market <phrase>evolutionary</phrase> process. 1 Introduction A three-year project commencing in 2002 at UTS is investigating the mechanisms required to support the <phrase>evolution</phrase> of eMarkets. The <phrase>perturbation</phrase> of market equilibrium through entrepreneurial <phrase>action</phrase> is the essence of market <phrase>evolution</phrase>. <phrase>Entrepreneurship</phrase> relies both on intuition and on <phrase>information</phrase> discovery. The term '<phrase>entrepreneur</phrase>' is used here in its technical sense [1]. Market <phrase>evolution</phrase> is a deep issue, and so a more tractable goal has been adopted initially. The initial goal of this project is to identify <phrase>arbitrage</phrase> opportunities in the <phrase>stock</phrase> options market based solely on <phrase>information</phrase> that can be derived automatically from <phrase>market data</phrase> and newsfeeds such as those <phrase>produced</phrase> by <phrase>Reuters</phrase>. This initial goal has been chosen because the problem is well defined and does not involve intangible intuition to the <phrase>degree</phrase> necessary for market <phrase>evolution</phrase>. Even so, intricate machinery is required to support even a simplistic investigation of <phrase>arbitrage</phrase> opportunities. The systems developed may not even be successful in identifying opportunities of financial significance. They will however constitute a first iteration <phrase>design</phrase> of systems to support market <phrase>evolution</phrase>, and that is the rationale for this initial phase of the project. The overall project aims to derive fundamental insight into how <phrase>e</phrase>-markets evolve. To achieve this it addresses the problem of identifying timely <phrase>information</phrase> for <phrase>e</phrase>-markets with their rapid, pervasive and massive flows of <phrase>data</phrase>. This <phrase>information</phrase> is distilled from individual signals in the markets themselves and from signals observed on 
Clusdm: a Multiple Criteria <phrase>Decision Making</phrase> Method for Heterogeneous <phrase>Data</phrase> Sets Pel meu estimat Toni Acknowledgements I would like to thank all those people who have made this <phrase>thesis</phrase> possible. First of all, to my Ph.D. <phrase>director</phrase>, Dr. Vicen Torra, who introduced me in the <phrase>research</phrase> field a <phrase>long</phrase> time ago, and who has always been there to give me precious comments. I also acknowledge the advice of my supervisor, Dr. Ulises Corts, who has helped me in all the steps that <phrase>lead</phrase> to this dissertation. Fruitful discussions with other researchers have influenced this <phrase>thesis</phrase>, like the ideas provided by Dr. Karina Gibert or the comments received in conferences or reviews. But this work has been possible with the help of many other people. Especially my husband, Toni, who has always believed that I could achieve those goals that seemed impossible to me. My parents, who let me study in <phrase>Tarragona</phrase> and <phrase>Barcelona</phrase> and encouraged me to continue with the Ph.D. studies. My sister, who has always listened to my discussions. Finally, this work would not have been possible without the help of the people in the <phrase>Software</phrase> <phrase>Department</phrase> of Universitat Politcnica de <phrase>Catalunya</phrase>, who have always provided me assistance. As well as, the people of the <phrase>Department</phrase> of <phrase>Computer Science</phrase> and <phrase>Mathematics</phrase> of Universitat Rovira i Virgili, where this <phrase>research</phrase> has been done. In particular, the members of the Banzai and Crises groups who have always helped me. ABSTRACT This <phrase>thesis</phrase> presents a new methodology for <phrase>decision making</phrase>. In particular, we have studied the problems that consider more than one criterion, which is known as Multiple Criteria <phrase>Decision Making</phrase> (MDCM) or Multiple Criteria Decision Aid (MCDA). The difference relies on the fact of imitating the behaviour of the decision maker (i.e. develop a method that makes decisions) or giving to the decision maker some additional <phrase>information</phrase> that allows him to understand the mechanism of solving decisions (i.e. the decision maker can learn from the use of the method). Our proposal fits better in the MCDA approach, but has also similarities with the MCDM perspective. On one hand, the method we have designed is <phrase>independent</phrase> enough to not require a <phrase>deep understanding</phrase> of the process by the decision maker. On the other hand, we have carefully studied the process and the method is able to extract <phrase>knowledge</phrase> about the <phrase>decision problem</phrase>, which is given to the user to let him know any special characteristics of the <phrase>data</phrase> analysed. ClusDM is a 
Deep, Big, Simple <phrase>Neural Nets</phrase> for <phrase>Handwritten Digit</phrase> Recognition Good old online <phrase>backpropagation</phrase> for plain multilayer perceptrons yields a very low 0.35% <phrase>error rate</phrase> on the MNIST <phrase>handwritten digits</phrase> benchmark. All we need to achieve this best result so far are many <phrase>hidden layers</phrase>, many <phrase>neurons</phrase> per layer, numerous deformed training images to avoid <phrase>overfitting</phrase>, and <phrase>graphics cards</phrase> to greatly speed up learning.
<phrase>Model</phrase>-based Performance Assessment <phrase>Model</phrase>-based Performance Assessment <phrase>Model</phrase>-based Performance Assessment Performance-assessment Learning Models <phrase>Problem Solving</phrase> Self-regulation <phrase>Communication</phrase> Content Understanding Collaboration Learning The findings and opinions expressed in this <phrase>report</phrase> do not reflect the position or policies of t h <phrase>e</phrase> There is concern in many quarters about the type and level of <phrase>knowledge</phrase> our children are acquiring in schools. Study) assessments, many students do poorly. Further, there is a growing concern in <phrase>American</phrase> <phrase>business</phrase> and <phrase>industry</phrase> (<phrase>U.S</phrase>. 1992) that young people entering the workforce are not adequately prepared for the world of work. These concerns are a <phrase>major</phrase> source of what is now almost a decade of effort to restructure what students are taught and the ways in which we can accurately assess their learning. For example, <phrase>government</phrase> policy makers have attempted to address these issues, beginning with the 1989 National achievement towards the Goals (see Goals 2000) on a national and <phrase>state</phrase> level, also advised that standards of learning be set so that students progress towards the Goals could be determined. Goal 3, which states that all students will leave grades 4, 8, and 12 having demonstrated competency over challenging <phrase>subject matter</phrase>. . ., was a particular focal point for the development of standards. The National <phrase>Education</phrase> Goals Panel also pointed to the need to define more specifically what constitutes challenging <phrase>subject matter</phrase> and competency i n itfor instance, the kind of learning characterized by <phrase>higher order</phrase> thinking skills, deep content <phrase>knowledge</phrase> within and across subject areas, <phrase>problem-solving</phrase> abilityand the need to determine how that competency would be measured. Standards now have been developed for eight of the nine subject areas <phrase>listed</phrase> But goals for national educational achievement and resulting standards to describe those goals will not tell us how well our children are doing unless we also measure their progress in learning the content of the standards. To do this, new kinds of <phrase>tests</phrase> are being created, called performance assessments, in which students engage in tasks that may require significant amounts of time and i n which they are asked to communicate their understanding of content, of process Performance assessment also has been described by its proponents as a <phrase>major</phrase> strategy to assist teachers to improve the learning of their students. This piece will describe both the values ascribed to performance assessment and the <phrase>major</phrase> criticisms of assessment that have developed in the last few years of exploration. One approach, <phrase>model</phrase>-based performance assessment, will be described as a way to remedy and to avoid criticisms of performance assessment. Part 
<phrase>Unsupervised Pre-training</phrase> with <phrase>Sequence</phrase> <phrase>Reconstruction</phrase> Loss for Deep <phrase>Relation Extraction</phrase> Models <phrase>Relation extraction</phrase> models based on <phrase>deep learning</phrase> have been attracting a lot of attention recently. Little <phrase>research</phrase> is carried out to reduce their need of <phrase>labeled training</phrase> <phrase>data</phrase>. In this work, we propose an unsu-pervised <phrase>pre-training</phrase> method based on the <phrase>sequence</phrase>-to-<phrase>sequence</phrase> <phrase>model</phrase> for deep <phrase>relation extraction</phrase> models. The <phrase>pre-trained</phrase> models need only half or even less <phrase>training data</phrase> to achieve equivalent performance as the same models without <phrase>pre-training</phrase>.
Left-corner <phrase>Parsing</phrase> <phrase>Algorithm</phrase> for Uniication Grammars iii Acknowledgements Like many Ph.D. graduates will say, writing a <phrase>thesis</phrase> and doing the <phrase>research</phrase> that comes with it is a solitary <phrase>business</phrase>. Fortunately I had many people who gave me kind support and warm encouragement throughout this process. I owe a <phrase>debt</phrase> of sincere gratitude to each of those people. First, my advisor Dr. Steve Lytinen, who <phrase>directed</phrase> my study which <phrase>led</phrase> to this <phrase>thesis</phrase>. From day one of my Ph.D. program at <phrase>DePaul</phrase> <phrase>CTI</phrase>, he made every <phrase>single</phrase> step with me, from writing code to proving lemmas. The work described in this <phrase>thesis</phrase> has beneetted greatly from his intuition in identifying problems and solving them. In this regard, this <phrase>thesis</phrase> is considered to be his as well as mine. Second, my mentor Dr. <phrase>David</phrase> Miller, who nourished my scholarly development during my Master's and Ph.D. programs. His strong top-down view of how pieces t together, as well as the <phrase>deep understanding</phrase> of what each piece is, also his preciseness at the detailed level, has innuenced me greatly in my graduate studies. After all, this <phrase>thesis</phrase> in essence is my best eeort to become his true <phrase>student</phrase>. Third, Dr. Tom Muscarello and Dr. Bill Rounds, who reviewed this <phrase>thesis</phrase>. From early in my Ph.D. program, Dr. Muscarello has kindly accepted me and given me encouragement. His view of what's really important from a practical standpoint applies beyond <phrase>academic</phrase> studies, and it is something I can learn a lot from, particularly in my future \real" work after the <phrase>degree</phrase>. Dr. Rounds, a <phrase>Professor</phrase> at <phrase>University</phrase> of <phrase>Michigan</phrase>, is a highly respected researcher in <phrase>Computational Linguistics</phrase> and <phrase>AI</phrase>. It is my honor to have him as my committee <phrase>member</phrase>. There are quite a few people, my <phrase>friends</phrase>, colleagues and other <phrase>faculty members</phrase>, who gave me encouragement from the sideline. Their kind support meant a lot to me, and helped me go through the process which required a signiicant amount of perseverance. I sincerely want to thank Joseph Morgan, Sotiris Skevoulis and Charles Sykes for their support and friendship. Countless hours of chats and laughs with them, who were struggling through the same process, made the <phrase>long</phrase>, no-fun <phrase>life</phrase> of a Ph.D. <phrase>student</phrase> bearable, and kept me motivated. Another thanks to Kathy Rossi, who has put up with me when I was out of line, and stayed as a great friend. I would also like to thank all <phrase>CTI</phrase> faculty and staa 
<phrase>Digital</phrase> <phrase>Storytelling</phrase> within Virtual Environments: "The Battle of <phrase>Thermopylae</phrase>" Until recently virtual environments and <phrase>videogame</phrase> applications rarely incorporated any deep <phrase>cultural</phrase> or educational principles. <phrase>Current research</phrase> in the <phrase>area</phrase> of <phrase>virtual reality</phrase> applications, clearly indentify that they are extremely motivating for learners and therefore can be employed as an innovative, more accessible framework to deliver <phrase>education</phrase> while at the same time entertain the <phrase>public</phrase>. On the other hand, <phrase>recent advances</phrase> in <phrase>videogame</phrase> applications and <phrase>human</phrase> computer interfaces, demonstrate that a combination of <phrase>game</phrase> applications with effective learning principles and intuitive <phrase>human</phrase> computer interfaces could potentially transform virtual environments to a significant educational tool that could significantly facilitate the learning process. This <phrase>paper</phrase> describes an interactive <phrase>virtual reality</phrase> application that we developed for the <phrase>museum</phrase> of <phrase>Thermopylae</phrase> located at the site of the original battle, near the <phrase>city</phrase> of <phrase>Lamia</phrase> in <phrase>Greece</phrase>. We utilized <phrase>storytelling</phrase> techniques and principles of modern <phrase>videogames</phrase> to disseminate historical <phrase>knowledge</phrase> about the battle and the associated legends. We present the hardware and <phrase>software</phrase> components comprising the proposed installation, while we elaborate over the educational techniques designed to reinforce the strength of <phrase>virtual reality</phrase> <phrase>technology</phrase> as a mean of designing educational experiences in the context of <phrase>cultural heritage</phrase> related <phrase>information</phrase>. 1 Introduction Enabled by the advent of interactive <phrase>digital media</phrase>, interactive <phrase>digital</phrase> <phrase>storytelling</phrase> redefines the experience of <phrase>narrative</phrase> by allowing its audience to actively participate in the story. Moreover, the <phrase>design</phrase> of interactive <phrase>storytelling</phrase> scenarios in a <phrase>virtual reality</phrase> environment, exhibits social <phrase>cognitive</phrase> and technical challenges, that need to be addressed in <phrase>order</phrase> to educate and entertain the <phrase>public</phrase>. Creating VR environments for <phrase>storytelling</phrase> requires the development of complex <phrase>software</phrase>, <phrase>mixing</phrase> multiple disciplines: computer <phrase>games</phrase>, graphics and <phrase>engineering</phrase>, <phrase>physics</phrase> <phrase>simulation</phrase>, pedagogical approaches, and significant experience in the <phrase>area</phrase> of the <phrase>education</phrase>. The
Searching for Product Experience Attributes in Online <phrase>Information</phrase> Sources Rechercher les attributs d'exprience d'un produit dans des sources d'information en ligne Abstract Using the Web, consumers not only find product characteristics from manufacturers and sellers; they can also exchange opinions with other third <phrase>parties</phrase>. Learning about such " experience attributes " builds confidence in purchasing decisions and establishes trust between <phrase>parties</phrase> in transactions. However, little is known about the search process for these experience attributes. In the current study, 65 participants researched a product, reporting its characteristics as well as salient experience attributes. By analyzing their search diaries and questionnaire responses, patterns for finding product characteristics versus experience attributes were compared. Precision on task was the same, however, product characteristics searches were likely to originate at <phrase>search engines</phrase>. Seller-dominated sites were favored over <phrase>independent</phrase> sites in searches for product characteristics, while the opposite was true for experience attributes. Finally, searches for experience attributes were not as broad or deep as those for product characteristics, suggesting that consumers focus their attention on fewer sources. Rsum Les consommateurs utilisent le Web pour se renseigner non seulement sur les caractristiques d'un produit mais galement ses attributs d'exprience. La prsente tude examine le processus de recherche de tels attributs. En analysant les historiques de leur recherche et les rponses <phrase>au</phrase> questionnaire, des diffrences saillantes entre le processus de recherche des caractristiques du produit et celui des attributs d'exprience ont t identifies.
Hierarchical Matching Pursuit for <phrase>Image Classification</phrase>: <phrase>Architecture</phrase> and Fast <phrase>Algorithms</phrase> Extracting good representations from images is essential for many <phrase>computer vision</phrase> tasks. In this <phrase>paper</phrase>, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy <phrase>layer-by-layer</phrase> using an efficient matching pursuit en-coder. It includes three modules: batch (<phrase>tree</phrase>) orthogonal matching pursuit, spatial <phrase>pyramid</phrase> max pooling, and contrast normalization. We investigate the <phrase>architecture</phrase> of HMP, and show that all three components are critical for good performance. To speed up the orthogonal matching pursuit, we propose a batch <phrase>tree</phrase> orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large <phrase>dictionary</phrase>. HMP is scalable and can efficiently handle full-size images. In addition, HMP enables linear <phrase>support vector machines</phrase> (SVMs) to match the performance of nonlinear SVMs while being scal-able to <phrase>large datasets</phrase>. We compare HMP with many <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>algorithms</phrase> including convolutional <phrase>deep belief</phrase> networks, SIFT based <phrase>single</phrase> layer <phrase>sparse coding</phrase> , and <phrase>kernel based</phrase> <phrase>feature learning</phrase>. HMP consistently yields <phrase>superior</phrase> accuracy on three types of visual recognition problems: <phrase>object recognition</phrase> (<phrase>Caltech</phrase>-101), scene recognition (<phrase>MIT</phrase>-Scene), and static event recognition (UIUC-<phrase>Sports</phrase>).
<phrase>Cognitive</phrase> visualisations and the <phrase>design</phrase> of learning technologies This <phrase>paper</phrase> discusses <phrase>cognitive</phrase> visualisations as a <phrase>design</phrase> approach in which the affordances of currently available learning technologies are used to reify or make explicit mental representations about conceptually challenging <phrase>knowledge</phrase>. Conceptual visualisations are intended to help learners develop understandings of a domain through dynamic <phrase>visual representations</phrase> that are based on <phrase>cognitive</phrase> <phrase>research</phrase> into the <phrase>nature</phrase> of <phrase>cognitive</phrase> frameworks or mental models, in particular, models with the main characteristic of <phrase>nave</phrase> or intuitive ideas and of expert understandings. A rationale for <phrase>cognitive</phrase> visualisations is presented, followed by a discussion of two examples of <phrase>cognitive</phrase> visualisations in contrasting domains that were developed using different implementing technologies. A set of issues for <phrase>future research</phrase> in this <phrase>area</phrase> is also considered. and he has been involved with organisational and international consulting activities. His <phrase>research</phrase> interests have focused on the <phrase>design</phrase> of learning technologies to foster deep <phrase>conceptual understanding</phrase>, conceptual change, and <phrase>knowledge transfer</phrase> in challenging conceptual domains. Most recently, his work has explored <phrase>cognitive</phrase> and learning issues related to understanding new scientific perspectives emerging from the study of complex and <phrase>dynamical systems</phrase>. 1 <phrase>Cognitive</phrase> visualisations and the <phrase>design</phrase> of learning technologies There are many challenges facing learners from <phrase>school</phrase>-age students to <phrase>professional</phrase> adults who must come to robust and useable understandings of challenging <phrase>knowledge</phrase> and skills for the <phrase>21st century</phrase>. The appropriate <phrase>design</phrase> and use of learning technologies for use in rich and stimulating <phrase>learning environments</phrase> holds considerable promise to help learners meet these challenges [1]. Whereas there is a wide <phrase>range</phrase> of <phrase>design</phrase> frameworks that may be used for the principled development of learning technologies, this <phrase>paper</phrase>
A Generative Process for Contractive <phrase>Auto-Encoders</phrase> The contractive <phrase>auto-encoder</phrase> learns a representation of the <phrase>input data</phrase> that captures the local <phrase>manifold</phrase> structure around each <phrase>data</phrase> point, through the leading singular vectors of the <phrase>Jacobian</phrase> of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a <phrase>high</phrase>-<phrase>density</phrase> <phrase>region</phrase> of the input space. This <phrase>paper</phrase> proposes a procedure for generating samples that are consistent with the local structure captured by a contrac-tive <phrase>auto-encoder</phrase>. The associated stochas-tic process defines a distribution from which one can sample, and which experimentally appears to converge quickly and mix well between modes, compared to Restricted Boltz-mann Machines and <phrase>Deep Belief</phrase> Networks. The intuitions behind this procedure can also be used to <phrase>train</phrase> the second layer of contraction that pools <phrase>lower</phrase>-<phrase>level features</phrase> and learns to be invariant to the local directions of variation discovered in the first layer. We show that this can help learn and represent invari-ances present in the <phrase>data</phrase> and improve classification error.
<phrase>Incremental Learning</phrase> for <phrase>Fine-Grained</phrase> <phrase>Image Recognition</phrase> This <phrase>paper</phrase> considers the problem of <phrase>fine-grained</phrase> <phrase>image recognition</phrase> with a growing <phrase>vocabulary</phrase>. Since in many <phrase>real world</phrase> applications we often have to add a new object category or visual concept with just a few images to learn from, it is crucial to develop a method that is able to generalize the recognition <phrase>model</phrase> from existing classes to new classes. <phrase>Deep convolutional</phrase> <phrase>neural networks</phrase> are capable of constructing powerful <phrase>image representations</phrase>; however, these networks usually rely on a logistic <phrase>loss function</phrase> that cannot handle the <phrase>incremental learning</phrase> problem. In this <phrase>paper</phrase>, we present a new method that can efficiently learn a new class given only a limited number of <phrase>training examples</phrase>, which we evaluate on the problems of <phrase>food</phrase> and <phrase>clothing</phrase> recognition. To illustrate the performance of our <phrase>proposed method</phrase> on the task of recognizing different kinds of <phrase>food</phrase>, when using only 1.3\% of <phrase>training examples</phrase> per category we achieved about 73\% of the performance (as measured by <phrase>F1</phrase>-score) compared to when using all available <phrase>training data</phrase>.
Exposing <phrase>Technology</phrase> Impact via <phrase>Enterprise Architecture</phrase> Exposing <phrase>Technology</phrase> Impact via <phrase>Enterprise Architecture</phrase> <phrase>Albert Schweitzer</phrase> -the Spiritual <phrase>Life</phrase> (1947) Exposing <phrase>Technology</phrase> Impact via <phrase>Enterprise Architecture</phrase> Exposing <phrase>Technology</phrase> Impact via <phrase>Enterprise Architecture</phrase> " The highest <phrase>knowledge</phrase> is to know that we are surrounded by <phrase>mystery</phrase>. " Acknowledgements This <phrase>Master</phrase> <phrase>Thesis</phrase> document is my final and last document to finally finish my ICT in <phrase>Business</phrase> study! Initially I was apprehensive because the expected outcome from this exploratory <phrase>research</phrase> was not clear. Now looking back, I am more than satisfied with my achievements of this <phrase>research</phrase> and the impression I leave at <phrase>Intel</phrase>. First of all I want to thank my <phrase>university</phrase> supervisor Dr. Luuk Groenewegen for his time, conversations (on and off topic), <phrase>feedback</phrase> and most important his <phrase>faith</phrase> in me and my <phrase>research</phrase>. Also I want to thank my second <phrase>university</phrase> supervisor Drs Werner Heijstek for his constructive <phrase>feedback</phrase>. My <phrase>Master</phrase> <phrase>Thesis</phrase> is based on my <phrase>research</phrase> performed at <phrase>Intel</phrase>. I would like to thank my <phrase>company</phrase> supervisor Matty Bakkeren for all his time, guidance, conversations, advice, <phrase>patience</phrase>, resources, <phrase>business</phrase> network possibilities, trust, full support and for driving in his <phrase>BMW</phrase>! Furthermore I want to thank Henk van den Eeckhout for granting me that wonderful intern position, such that I could join his Sales team and the ETS team of Jean-Laurent Philippe. Henk and Jean-Laurent thank you for all your effort, support, the approval to join the ETS training in <phrase>Munich</phrase> and for making my <phrase>thesis</phrase> a priority in the team! I want to thank all other sales and technical colleagues who helped me in one way or another during this year, especially Peter van Rossum for our conversations during the travels to <phrase>Groningen</phrase>, Marc Beckers and Steve Davies for providing me the <phrase>information</phrase> I needed and for taking the time to answer all of my questions about <phrase>Intel</phrase> <phrase>vPro</phrase>. I also want to thank my <phrase>Intel</phrase> logistic colleagues for embracing me as a sales and <phrase>marketing</phrase> <phrase>member</phrase> in the logistic team! Special thanks Tony, Peter en Wouter for making my time at <phrase>Intel</phrase> <phrase>Amsterdam</phrase> fun! Thanks to Henk Heusdens who made it possible to have my own desk, to return to every day! Last but not least I want to thank all my close <phrase>family</phrase> and <phrase>friends</phrase> who supported me during my <phrase>internship</phrase> and provided me the time to work on my <phrase>thesis</phrase>! It is time for me to move on and dive into the next challenge! As my <phrase>manager</phrase> would say: " You will be thrown into the deep (again), learn to swim. Make sure you do not drown! " 
Street-fighting <phrase>Mathematics</phrase> I have read Street-Fighting <phrase>Mathematics</phrase> twice and most of the sections five or six times. I find myself working, and struggling with, problems from this <phrase>book</phrase> as much as I did with homework problems when I was a <phrase>student</phrase>. It is not that the material in this <phrase>book</phrase> is difficult; in fact the aim of the <phrase>book</phrase> is to provide simple tools for approximating solutions to complicated problems. The difficulty lies in the fact that the ideas this <phrase>book</phrase> presents are at times completely foreign to my way of thinking. Learning to see problems the way Mahajan sees them takes deep thought, time, and practice, but that is what makes Street-Fighting <phrase>Mathematics</phrase> an enjoyable read that provides an enlightening look at <phrase>solving problems</phrase>. At only 134 pages in length, the <phrase>book</phrase> is small and covers only six <phrase>major</phrase> topics, but those topics have kept me busy for months. They are <phrase>dimensional analysis</phrase>, easy cases, lumping, pictorial proofs, taking out the big part, and reasoning by analogy. Along the way, Mahajan, a <phrase>physicist</phrase> by training, solves problems involving everyday calculations, <phrase>geometry</phrase>, <phrase>calculus</phrase>, <phrase>differential equations</phrase> , <phrase>topology</phrase>, and <phrase>physics</phrase>. He even finds a <phrase>solution</phrase> for the Navier-Stokes equations involving falling cones using nothing more than dimen-Brent Deschamp is assistant <phrase>professor</phrase> of <phrase>mathematics</phrase> at the <phrase>South Dakota School of Mines and Technology</phrase>. His sional analysis and easy cases. The list of topics is <phrase>short</phrase>, but those topics are powerful. To illustrate some of these ideas, I present a few examples. Consider the <phrase>integral</phrase> 1 x 2 dx. The most common method for solving this <phrase>integral</phrase> would be to use a <phrase>trigonometric</phrase> substitution such as x = (1/ ) cos . Instead, Mahajan shows how <phrase>dimensional analysis</phrase> can be used to see how the parameter influences the <phrase>solution</phrase>. In <phrase>order</phrase> to simplify notation, assign the <phrase>dimension</phrase> of length, L, to x. Since everything under the radical must have the same <phrase>dimension</phrase> and since 1 is dimen-sionless, it must be that has <phrase>dimension</phrase> L 2. We now need to determine the <phrase>dimension</phrase> of the entire <phrase>integral</phrase>. The integrand is <phrase>dimensionless</phrase>, and the differential dx, which represents a small quantity of x, will have <phrase>dimension</phrase> L. Finally, the <phrase>integral</phrase> <phrase>symbol</phrase> represents both a limit and a sum, which are both <phrase>dimensionless</phrase>, and thus summing terms of <phrase>dimension</phrase> L yields a quantity with <phrase>dimension</phrase> L. Also, the <phrase>integral</phrase> will produce a <phrase>function</phrase> of 
Multilingual Deep Lexical Acquisition for HPSGs via Supertagging We propose a <phrase>conditional random field</phrase>-based method for supertagging, and apply it to the task of learning new lexical items for HPSG-based precision grammars of <phrase>English</phrase> and <phrase>Japanese</phrase>. Using a pseudo-likelihood approximation we are able to scale our <phrase>model</phrase> to hundreds of supertags and tens-of-thousands of training sentences. We show that it is possible to achieve start-of-the-<phrase>art</phrase> <phrase>results</phrase> for both languages using maximally <phrase>language</phrase>-<phrase>independent</phrase> lexical features. Further, we explore the performance of the models at the type-and token-level, demonstrating their <phrase>superior</phrase> performance when compared to a unigram-based base-line and a transformation-based learning approach.
Smart decisions: an <phrase>architectural</phrase> <phrase>design</phrase> <phrase>game</phrase> <phrase>Architecture</phrase> <phrase>design</phrase> is notoriously difficult to teach and to learn. Most competent <phrase>architects</phrase> in <phrase>industry</phrase> have deep <phrase>knowledge</phrase> won from <phrase>long</phrase> years of experience. But if we want <phrase>architecture</phrase> <phrase>design</phrase> to be methodical and repeatable, we need better methods for teaching it. Simply waiting for an aspiring <phrase>architect</phrase> to accumulate 10 or 20 years of experience is not acceptable if we believe that <phrase>software engineering</phrase> is a true <phrase>engineering</phrase> discipline. In this <phrase>paper</phrase> we describe our experiences with the development of a <phrase>game</phrase> that <phrase>aids</phrase> in teaching <phrase>architecture</phrase> <phrase>design</phrase>, specifically <phrase>design</phrase> employing the Attribute-Driven <phrase>Design</phrase> method. We discuss our approach to creating the <phrase>game</phrase>, and the "<phrase>design</phrase> concepts catalog" that provides the <phrase>knowledge base</phrase> for the <phrase>game</phrase>. Finally, we <phrase>report</phrase> on our experiences with deploying the <phrase>game</phrase>, and the (enthusiastic) assessments and <phrase>feedback</phrase> that we have received from <phrase>industrial</phrase> and <phrase>academic</phrase> participants.
Quantization based Fast Inner Product Search We propose a quantization <phrase>based approach</phrase> for fast approximate Maximum Inner Product Search (MIPS). Each <phrase>database</phrase> <phrase>vector</phrase> is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a <phrase>database</phrase> <phrase>vector</phrase> is approximated as the sum of inner <phrase>products</phrase> with the subspace quantizers. Different from <phrase>recently proposed</phrase> LSH approaches to MIPS, the <phrase>database</phrase> vectors and queries do not need to be augmented in a higher dimensional <phrase>feature space</phrase>. We also provide a theoretical analysis of the <phrase>proposed approach</phrase>, consisting of the concentration <phrase>results</phrase> under mild assumptions. Furthermore, if a small sample of example queries is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. <phrase>Experimental</phrase> <phrase>results</phrase> on a <phrase>variety</phrase> of datasets including those arising from <phrase>deep neural networks</phrase> show that the <phrase>proposed approach</phrase> <phrase>significantly outperforms</phrase> the existing <phrase>state</phrase>-of-the-<phrase>art</phrase>.
Smart <phrase>computing</phrase> for <phrase>large scale</phrase> visual <phrase>data</phrase> sensing and processing Smart <phrase>computing</phrase> is an emerging multidisciplinary <phrase>area</phrase>, aiming to use <phrase>computing</phrase> <phrase>technology</phrase> to <phrase>design</phrase> smart methods, build smart systems, and make <phrase>human</phrase> <phrase>life</phrase> better. Visual signal plays the most important role in the <phrase>communication</phrase> and interaction between <phrase>human</phrase> and the surrounding world, while the past decade has witnessed the rapid development of <phrase>digital imaging</phrase> and transmission technologies. It is estimated that by 2015 the <phrase>U.S</phrase>. consumers will capture more than 100 billion <phrase>digital pictures</phrase> annually. In many practical vision applications, how to smartly collect the desired <phrase>data</phrase> and how to <phrase>design</phrase> smart <phrase>algorithms</phrase> to analyze the collected large amount of visual <phrase>data</phrase> have become very challenging issues. Meanwhile, the advancement of <phrase>cloud computing</phrase>, <phrase>social computing</phrase>, <phrase>machine learning</phrase> and <phrase>artificial intelligence</phrase> technologies are bringing smart <phrase>computing</phrase> to a newer <phrase>dimension</phrase> and improving our ways of living. This <phrase>special issue</phrase> invites original papers on topics that are related to smart <phrase>computing</phrase> for <phrase>large scale</phrase> visual <phrase>data</phrase> sensing and processing. It provides an effective forum for researchers and engineers from a broad <phrase>range</phrase> of disciplines to exchange their creative ideas, share recent <phrase>research</phrase> advances, and identify future directions in response to <phrase>grand challenges</phrase> in intelligent <phrase>computing</phrase> technologies for <phrase>large scale</phrase> vision problems. This <phrase>special issue</phrase> will significantly benefit a large <phrase>variety</phrase> of audience from both <phrase>academia</phrase> and <phrase>industry</phrase>. The topics of this <phrase>special issue</phrase> include, but are not limited to: Visual <phrase>data</phrase> sensing and processing with new sensors <phrase>Large scale</phrase> visual dataset <phrase>benchmarking</phrase> <phrase>High</phrase> quality visual <phrase>data</phrase> <phrase>reconstruction</phrase> Visual <phrase>feature representation</phrase> learning Big visual <phrase>data</phrase> classification <phrase>Deep learning</phrase> for vision applications Vision applications for <phrase>smartphone</phrase> Pervasive <phrase>computing</phrase> for vision <phrase>Cloud computing</phrase> for vision <phrase>Active learning</phrase> for visual <phrase>data</phrase> sensing and processing
From the Programmer's Apprentice to <phrase>Human</phrase>-<phrase>Robot</phrase> Interaction: Thirty Years of <phrase>Research</phrase> on <phrase>Human</phrase>-Computer Collaboration We summarize the continuous thread of <phrase>research</phrase> we have conducted over the past thirty years on <phrase>human</phrase>-computer collaboration. This <phrase>research</phrase> reflects many of the themes and issues in operation in the greater field of <phrase>AI</phrase> over this <phrase>period</phrase>, such as <phrase>knowledge representation and reasoning</phrase>, planning and intent recognition, learning, and the interplay of <phrase>human</phrase> theory and computer <phrase>engineering</phrase>. Figure 1 illustrates our overall <phrase>research</phrase> methodology, which has been to <phrase>model</phrase> <phrase>human</phrase>-computer collaboration on what is known about <phrase>human</phrase>-<phrase>human</phrase> collaboration. Furthermore we have focused almost exclusively on the special case of two copresent collaborators, i.e., where each collaborator is able both to communicate with and observe the actions of the other. Examples of such collaborations include two <phrase>mechanics</phrase> working on a <phrase>car</phrase> <phrase>engine</phrase> together or two computer users working on a <phrase>spreadsheet</phrase> together. To a first approximation, our approach has been simply to <phrase>substitute</phrase> a computer agent for one of the <phrase>human</phrase> collaborators, keeping as much else the same as possible. Due to space limitations, we will not attempt to review all <phrase>research</phrase> on <phrase>human</phrase>-computer collaboration, but limit ourselves to viewing this topic through the <phrase>lens</phrase> of our own work and that of our immediate collaborators. Consistent with this, note that bibliography below contains only publications by ourselves and our immediate collaborators. The chronology of our <phrase>research</phrase> begins in 1976 with the publication of Rich and Shrobe's joint M.S. <phrase>thesis</phrase> on the Pro-grammer's Apprentice [1,3]: " As compared to <phrase>automatic programming</phrase> <phrase>research</phrase>, the programmer's apprentice emphasizes a <phrase>cooperative</phrase> relationship between the computer and the <phrase>human</phrase> <phrase>programmer</phrase>... " Shortly thereafter, Sidner began work on modeling how <phrase>natural language</phrase> is used in the context of pairs (and later groups) of people achieving tasks together. Her first <phrase>paper</phrase> on this topic dealt with the interpretation of discourse purposes in the Personal Assistant <phrase>Language</phrase> Understanding Program [2]. Under the direction of Rich and Shrobe, and later Waters, the Programmer's Apprentice project [4,15,16] lived at the <phrase>MIT</phrase> <phrase>AI</phrase> Lab from 1976 until Rich and Waters left <phrase>MIT</phrase> in 1991. Even though the concept of <phrase>human</phrase>-computer collaboration was the <phrase>bedrock</phrase> of the project, we never developed a focus of attention SharedPlans mutual beliefs communicate observe Shared Object(s) manipulate observe manipulate Figure 1: Modeling <phrase>human</phrase>-computer collaboration on <phrase>human</phrase>-<phrase>human</phrase> collaboration. deep theoretical understanding of what collaboration meant. Instead, most of the Programmer's Apprentice <phrase>research</phrase> concentrated on how to represent and reason with the shared <phrase>knowledge</phrase> necessary for successful <phrase>human</phrase>-computer 
Helping Educators Harvest <phrase>Internet</phrase> Resources: The Development of <phrase>Technology</phrase> Integration Support Tools The availability of new <phrase>internet</phrase> technologies has reached deep into the educational system with educators being the real gatekeepers of their impact on <phrase>teaching and learning</phrase>. Access to the <phrase>internet</phrase> has been accelerated by national mandates and <phrase>State</phrase> <phrase>legislation</phrase> requiring <phrase>internet access</phrase> for all students in the educational <phrase>arena</phrase>. Through national and <phrase>state</phrase> funding, this <phrase>evolution</phrase> holds promise for new ways of <phrase>teaching and learning</phrase>. However, stories abound about how <phrase>computers</phrase> sit idle or are used for glorified seatwork. With this <phrase>evolution</phrase> come several questions that must be addressed to prevent unused, misused or underused <phrase>technology</phrase> in the classroom. How can educators balance <phrase>school</phrase>-level support structures with <phrase>teaching and learning</phrase> issues through the strategic integration of <phrase>technology</phrase> resources into <phrase>teaching and learning</phrase>? How do educators create web-enhanced <phrase>learning environments</phrase> that effectively integrate teaching objectives and learning with <phrase>technology</phrase> resources? This <phrase>paper</phrase> describes how three <phrase>educator</phrase> support tools, ID-PRISM, Web-Enhanced <phrase>Learning Environment</phrase> Strategies (WELES), and the lesson plan structure of Kids as Airborne Mission Scientists (KaAMS), were created from <phrase>research</phrase> designed to investigate these questions. 1. INTEGRATING <phrase>INTERNET</phrase> TECHNOLOGIES INTO <phrase>EDUCATION</phrase> Today, <phrase>internet</phrase> <phrase>technology</phrase> enables the integration of web resources into classrooms to enhance the interactive and social <phrase>nature</phrase> of instruction. Recently, researchers have investigated inter-, intra-and <phrase>extranet</phrase> Findings suggested that these interactive and <phrase>technology</phrase>-enhanced <phrase>learning strategies</phrase> engaged learners more fully in instruction and facilitated their ability to comprehend and construct personal <phrase>knowledge</phrase>. Evidence was also found that the use of <phrase>internet</phrase> technologies in the classroom promoted openness, sharing, and involvement in students' own learning, increased <phrase>academic</phrase> achievement, developed <phrase>social skills</phrase>, aided in the mainstreaming of handicapped students, reduced <phrase>ethnic</phrase> tensions, increased <phrase>self-esteem</phrase>, and predicted students' interests in <phrase>science</phrase> careers <phrase>research</phrase> has demonstrated that the possibilities for web use in the classroom extend far beyond the individualized drill-and-practice and <phrase>entertainment</phrase> scenarios of the past.
Learning <phrase>Semantic</phrase> <phrase>Image Representations</phrase> at a <phrase>Large Scale</phrase> Learning <phrase>Semantic</phrase> <phrase>Image Representations</phrase> at a <phrase>Large Scale</phrase> Learning <phrase>Semantic</phrase> <phrase>Image Representations</phrase> at a <phrase>Large Scale</phrase> Learning <phrase>Semantic</phrase> <phrase>Image Representations</phrase> at a <phrase>Large Scale</phrase> Permission to make <phrase>digital</phrase> or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or <phrase>commercial advantage and that copies bear</phrase> this notice and the full citation on the first page. To copy otherwise, to republish, to <phrase>post on servers</phrase> or to redistribute to lists, <phrase>requires prior specific permission</phrase>. I present my work towards learning a better <phrase>computer vision</phrase> system that learns and generalizes object categories better, and behaves in ways closer to what <phrase>human</phrase> behave. Specifically, I focus on two key components of such a system: learning better features, and revisiting existing problem statements. For the first component, I propose and analyze novel <phrase>receptive field</phrase> learning and <phrase>dictionary</phrase> learning methods, mathematically justified by the Nystrm sampling theory, that learn more compact and effective features for <phrase>object recognition</phrase> tasks. For the second component, I propose to combine otherwise independently developed <phrase>computer vision</phrase> and <phrase>cognitive science</phrase> studies, and present the first <phrase>large-scale</phrase> system that allows <phrase>computers</phrase> to learn and generalize closer to what a <phrase>human</phrase> learner will do. I also provide a <phrase>large-scale</phrase> <phrase>human</phrase> behavior <phrase>database</phrase>, which will hopefully enable further <phrase>research</phrase> along this <phrase>research</phrase> direction. Following the recent success of <phrase>convolutional neural networks</phrase>, I present and release a well-engineered framework for <phrase>general</phrase> <phrase>deep learning</phrase> <phrase>research</phrase>, and provide an extensive analysis on the generality of deep features learned from the <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>CNN</phrase> pipeline: whether they serve as a <phrase>general</phrase>-purpose visual descriptor that could be adopted in various applications, and <phrase>future research</phrase> directions made possible by such <phrase>general</phrase> features.
Enacting <phrase>Risk</phrase> in <phrase>Independent</phrase> Technological <phrase>Innovation</phrase> Institute for <phrase>Management</phrase> of <phrase>Innovation</phrase> and <phrase>Technology</phrase> Enacting <phrase>Risk</phrase> in <phrase>Independent</phrase> Technological <phrase>Innovation</phrase> The present study aims at investigating the role of <phrase>risk</phrase> in the activity of <phrase>independent</phrase> technological venturing. Altogether 12 deep-interviews were conducted with technological entrepreneurs, who had taken part in the inventive, developmental and the commercialization phases of a <phrase>technology</phrase>-based <phrase>innovation</phrase> process. The interviews revealed a number of enactment approaches through which these innovators encountered and affected (dealt with or transformed) <phrase>risk</phrase> within the <phrase>innovation</phrase> process. Factors thus developed from the empirical material included: <phrase>human capital</phrase>, pace and priority, the world moves, activating <phrase>social networks</phrase>, <phrase>risk</phrase> learning, <phrase>risk</phrase> incrementalism, maintaining <phrase>venture</phrase> agility, and creating and sustaining <phrase>autonomy</phrase>. The <phrase>paper</phrase> presents a theoretical contextualization as to the significance of these factors, and finally suggests a number of ways in which these may be interpreted for the benefit of <phrase>innovation management</phrase>.
Acquiring Mastery Training for Mastery Minimal Task Interruption & Control Many opportunities for attaining mastery exist within a typical <phrase>workflow</phrase>. By leveraging focus-shifts and providing the user with complete control over the learning process, both critical elements of any " as-you-go " learning method, the HotKeyCoach learning <phrase>model</phrase> demonstrates that the <phrase>production</phrase> <phrase>paradox</phrase> can be overcome by experienced computer users. After formal training, a professional's <phrase>education</phrase> continues during on-the-job activities. Expertise is achieved by learning case-by-case how to apply a <phrase>professional</phrase> framework to a problem. Learning to use efficiently the appropriate technologies requires mastery. Mastery, a <phrase>state</phrase> of " knowing " how to productively work through the <phrase>technology</phrase>, is a problem of skill acquisition. To achieve it requires an understanding of the <phrase>conceptual model</phrase> that underlies an application, i.e. learning the operations and methods that best apply to different kinds of situations. As the user becomes more efficient and knowledgeable about the articulation work it takes to use the application, she can remain within the flow of the activity for longer periods of interrupted time. By developing a deep <phrase>conceptual model</phrase> of the system, the amount of work required to accomplish goals diminishes, resulting in a more efficient utilization of the system. Learning to <phrase>master</phrase> work-mediated <phrase>technology</phrase>, however, is complicated by the <phrase>production</phrase> <phrase>paradox</phrase> (Carroll & Rosson, 1987): learning more effective and efficient methods for using <phrase>technology</phrase> <phrase>results</phrase> in mastery in the <phrase>long</phrase>-term; however, spending time to acquire mastery in the <phrase>short</phrase>-term interferes with <phrase>productivity</phrase>. To preserve the flow of activity, the user must be able to concentrate on her activity with minimal interruption (Bederson, 2004). The tool we present, HotKeyCoach (HKC) develops a <phrase>model</phrase> for the incremental attainment of mastery that is integrated into the work activity of the user and minimizes task interruption. HKC provides the individual with an opportunity to gradually acquire mastery of <phrase>technology</phrase> during the course of her normal work activity, while ensuring she maintains complete control of the learning process. In support of the attainment of mastery, the HKC <phrase>model</phrase> turns <phrase>user interface</phrase> actions into learning events (VanLehn, 1996), leveraging the many learning opportunities for attaining mastery of the work-mediated <phrase>technology</phrase>. The learning events either introduce new material or provide the learner with the capability to practice previously introduced material. Bdker, (Bdker, 1995), defines two types of interruptions that redirect the user's attention: breakdowns and focus-shifts. The key difference between a breakdown and a focus-shift is the affect upon the flow of the central 
Learning and Imitation in Heterogeneous <phrase>Robot</phrase> Groups C-lab <phrase>Report</phrase> Learning and Imitation in Heterogeneous <phrase>Robot</phrase> Groups (now <phrase>Siemens AG</phrase>) and the <phrase>University</phrase> of <phrase>Paderborn</phrase> under the auspices of the <phrase>State</phrase> of <phrase>North-Rhine Westphalia</phrase>. C-LAB's vision is based on the fundamental premise that the gargantuan challenges thrown up by the transition to a future <phrase>information society</phrase> can only be met through global cooperation and deep interworking of theory and practice. This is why, under one roof, staff from the <phrase>university</phrase> and from <phrase>industry</phrase> cooperate closely on joint projects within a common <phrase>research</phrase> and development <phrase>organization</phrase> together with international partners. In doing so, C-LAB concentrates on those innovative subject areas in which cooperation is expected to <phrase>bear</phrase> particular <phrase>fruit</phrase> for the partners and their <phrase>general</phrase> well-being. Alle Rechte sind vorbehalten. Insbesondere ist die bernahme in maschinenlesbare Form sowie das Speichern in Informationssystemen, <phrase>auch</phrase> auszugsweise, nur <phrase>mit</phrase> schriftlicher Genehmigung der <phrase>Siemens AG</phrase> und der Universitt <phrase>Paderborn</phrase> gestattet. All rights reserved. In particular, the content of this document or extracts thereof are only permitted to be transferred into machine-readable form and stored in <phrase>information</phrase> systems when written consent has been obtained from <phrase>Siemens AG</phrase> and the <phrase>University</phrase> of <phrase>Paderborn</phrase> werden kann, ohne vorher auf die Imitationsdaten zuzugreifen (Abschnitt 4).
A Fast <phrase>Learning Algorithm</phrase> for <phrase>Deep Belief</phrase> Nets We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected <phrase>belief nets</phrase> that have many <phrase>hidden layers</phrase>. Using complementary priors, we derive a fast, <phrase>greedy algorithm</phrase> that can learn deep, <phrase>directed</phrase> <phrase>belief networks</phrase> one layer at a time, provided the top two layers form an undirected <phrase>associative memory</phrase>. The fast, <phrase>greedy algorithm</phrase> is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-<phrase>sleep</phrase> <phrase>algorithm</phrase>. After <phrase>fine-tuning</phrase>, a network with three <phrase>hidden layers</phrase> forms a very good <phrase>generative model</phrase> of the joint distribution of <phrase>handwritten digit</phrase> images and their <phrase>labels</phrase>. This <phrase>generative model</phrase> gives better digit classification than the best discriminative <phrase>learning algorithms</phrase>. The <phrase>low-dimensional</phrase> <phrase>manifolds</phrase> on which the digits lie are modeled by <phrase>long</phrase> ravines in the <phrase>free-energy</phrase> <phrase>landscape</phrase> of the top-level <phrase>associative memory</phrase>, and it is easy to explore these ravines by using the <phrase>directed</phrase> connections to display what the <phrase>associative memory</phrase> has in mind.
Empowering <phrase>Student</phrase> <phrase>Leadership</phrase> Beliefs: an Exploratory Study <phrase>Leadership</phrase> beliefs contribute to behaviors and attitudes. The purposes for <phrase>conducting</phrase> this study were 1) to gain an understanding of <phrase>undergraduate</phrase> students' <phrase>leadership</phrase> beliefs, 2) to implement three distinct <phrase>leadership</phrase> modules into an introductory <phrase>textiles</phrase> and <phrase>clothing</phrase> course, and 3) to assess the modules' effectiveness in promoting empowering <phrase>leadership</phrase> beliefs. The study used quantitative and qualitative methods (n=76). Findings suggest undergraduates' perceptions of <phrase>leadership</phrase> encompass trait and situational perspectives of <phrase>leadership</phrase>. The modules influenced students' understanding of the varied definitions of <phrase>leadership</phrase> and empowered them to consider that the behaviors, beliefs, and attitudes of <phrase>leadership</phrase> were attainable. <phrase>Leadership</phrase> development is a focus of schools, <phrase>universities</phrase>, and businesses, yet " there are almost as many different definitions of <phrase>leadership</phrase> as there are persons who have attempted to define the concept " (Stogdill, 1974, p.7). <phrase>Research</phrase> on the topic has taken many approaches, with a plethora of models, theories, and perceptions seeking to define and explain the <phrase>leadership</phrase> concept (Gregoire & Arendt, 2004; Shertzer & Shuh, 2004). <phrase>Leadership</phrase>, while not clearly defined, is a sought-after trait among employers searching for job candidates and a necessary ability once students are employed (Frazier, 2007). The importance of <phrase>leadership</phrase> qualities such as creating and managing change, learning to learn, and interpersonal skills such as collaboration are firmly established in the <phrase>literature</phrase> According to The Partnership for <phrase>21st Century</phrase> Skills, which is a <phrase>consortium</phrase> of <phrase>business</phrase>, <phrase>education</phrase>, and <phrase>media</phrase> groups, there is deep concern among <phrase>U.S</phrase>. employers that young people are not adequately prepared with the skills to compete and thrive in the global <phrase>economy</phrase> (2006). An overwhelming 81% of employers polled indicated that <phrase>leadership</phrase> was " very important " for new entrants with a four-year <phrase>college</phrase> <phrase>diploma</phrase>. For both two-year and four-year <phrase>college</phrase> graduates, lack of <phrase>leadership</phrase> was the second most frequently reported applied skill " deficiency " (Partnership for <phrase>21 st Century</phrase> Skills, 2006). Although many authors and employers have stressed the need for strong <phrase>leadership</phrase> skills for workforce readiness, a clear definition of <phrase>leadership</phrase> remains elusive (Frazier, 2007). A student's definition of <phrase>leadership</phrase> may <phrase>play</phrase> a significant role in whether the <phrase>student</phrase> perceives herself as a leader (Shertzer & Schuh, 2004). Astin and Astin (2000) contend that some perceptions of <phrase>leadership</phrase> promote constraining beliefs that limit <phrase>student</phrase> participation in <phrase>leadership</phrase> experiences. They conclude that <phrase>leadership</phrase> development programs should focus on instilling empowering beliefs in <phrase>college</phrase> students, and they define empowering beliefs as liberating 
<phrase>Kernel-Based</phrase> Machines for Abstract and Easy Modeling of Automatic Learning The modeling of system <phrase>semantics</phrase> (in several ICT domains) by means of pattern analysis or <phrase>relational</phrase> learning is a product of latest <phrase>results</phrase> in statistical learning theory. For example, the modeling of <phrase>natural language</phrase> <phrase>semantics</phrase> expressed by text, images, speech in <phrase>information</phrase> search (e.g. <phrase>Google</phrase>, <phrase>Yahoo</phrase>,..) or <phrase>DNA sequence</phrase> labeling in <phrase>Bioinformatics</phrase> represent distinguished cases of successful use of <phrase>statistical machine learning</phrase>. The reason of this success is due to the ability to overcome the <phrase>concrete</phrase> limitations of <phrase>logic</phrase>/<phrase>rule-based</phrase> approaches to <phrase>semantic</phrase> modeling: although, from a <phrase>knowledge</phrase> <phrase>engineer</phrase> perspective, rules are natural methods to encode system <phrase>semantics</phrase>, noise, ambiguity and errors affecting <phrase>dynamic systems</phrase>, prevent such approached from being effective, e.g. they are not flexible enough. In contrast, statistical <phrase>relational</phrase> learning, applied to representations of system states, i.e. <phrase>training examples</phrase>, can produce <phrase>semantic</phrase> models of system behavior based on a large number attributes. As the values of the latter are automatically learned, they reflect the flexibility of statistical settings and the overall <phrase>model</phrase> is robust to unexpected system condition changes. Unfortunately, while attribute weight and their relations with other attributes can be automatically learned from examples, their <phrase>design</phrase> for representing the <phrase>target</phrase> object (e.g. a system <phrase>state</phrase>) has to be manually carry out. This requires expertise, intuition and deep <phrase>knowledge</phrase> about the expected system behavior. A typical difficult task is for example the conversion of structures into attribute-value representations. <phrase>Kernel Methods</phrase> are powerful techniques designed within the statistical learning theory. They can be used in <phrase>learning algorithms</phrase> in place of attributes, thus simplifying object representation. More specifically, kernel functions can define structural and <phrase>semantic</phrase> similarities between objects (e.g. states) at abstract level, replacing the similarity defined in terms of attribute overlap. In this chapter, we provide the <phrase>basic</phrase> notions of <phrase>machine learning</phrase> along with latest theoretical <phrase>results</phrase> obtained in recent years. First, we show traditional and <phrase>simple machine</phrase> <phrase>learning algorithms</phrase> based on attribute-value representations and <phrase>probability</phrase> notions such as the Naive Bayes and the <phrase>Decision Tree</phrase> classifiers. Second, we introduce the PAC learning theory and the <phrase>Perceptron</phrase> <phrase>algorithm</phrase> to provide the readers with essential concepts of modern <phrase>machine learning</phrase>. Finally, we use the above background to illustrate a simplified theory of <phrase>Support Vector Machines</phrase>, which, along with the <phrase>kernel methods</phrase>, are the ultimate product of the statistical learning theory.
Robust <phrase>Video</phrase> Synchronization using Unsupervised <phrase>Deep Learning</phrase> Figure 1: Our novel unsupervised training <phrase>algorithm</phrase> is used to <phrase>train</phrase> a convnet to temporally align two videos captured in different environment settings. The <phrase>neural network</phrase> learns a content-aware similarity metric by embedding each frame into a <phrase>feature space</phrase> which is <phrase>independent</phrase> from factors like <phrase>weather</phrase>, traffic or seasons. For the extraction of a <phrase>video</phrase> alignment, we deploy a Dijkstra-based <phrase>algorithm</phrase>. Abstract Aligning <phrase>video</phrase> sequences is a fundamental yet still unsolved component for a wide <phrase>range</phrase> of applications in <phrase>computer graphics</phrase> and vision. Especially when targeting <phrase>video</phrase> clips containing an extensively varying appearance. Using <phrase>recent advances</phrase> in <phrase>deep learning</phrase> , we present a scalable and robust method for <phrase>computing</phrase> optimal non-linear temporal <phrase>video</phrase> alignments. The presented <phrase>algorithm</phrase> learns to retrieve and match similar <phrase>video</phrase> frames from input sequences without any <phrase>human</phrase> interaction or additional annotations in an unsupervised <phrase>fashion</phrase>. An iterative scheme is presented which leverages on the <phrase>nature</phrase> of the videos themselves in <phrase>order</phrase> to remove the need for <phrase>labels</phrase>. We incorporate a variation of Dijkstra's <phrase>shortest-path</phrase> <phrase>algorithm</phrase> for extracting meaningful <phrase>training examples</phrase> as well as a robust <phrase>video</phrase> alignment. While previous methods assume similar settings as <phrase>weather</phrase> conditions, <phrase>season</phrase> and illumination , our approach is able to robustly align videos regardless of such noise. This provides new ways of <phrase>compositing</phrase> non-seasonal <phrase>video</phrase> clips from <phrase>data</phrase> recorded months apart.
Learning in <phrase>High</phrase> Gear: Building on the <phrase>literature</phrase> of dynamic capability and <phrase>organizational learning</phrase>, we examine strategy execution in hyper-competition as a problem of how organizations can re-configure their learning capability to match with their radically different learning demands. Organizations in hyper-competitive environments face an increasing gap between their learning opportunities and needs, and actual learning performance. In <phrase>order</phrase> to survive they must improve their absorptive capacity so that they can learn simultaneously broad, deep and fast. We define such a learning contingency as hyper-learning. To do so, the <phrase>organization</phrase> must systematically interlace explorationthat seeks to maximize learning breadth and exploitationthat seeks to maximize learning depth. Unlike in traditional learning cycles, exploration and exploitation during periods of hyper-learning are not insulated from each other through time or structure. We explore seven <phrase>software</phrase> firms engaged in Web system development during the heyday on dot.com frenzy and investigate how these companies were able to hyper-learn. We distinguish two mechanisms to speed up exploration: distributed gate-keeping and extended <phrase>grafting</phrase> of external <phrase>knowledge</phrase>; and two mechanisms to speed up exploitation: simple <phrase>design patterns</phrase> and peer networks. These mechanisms were nearly uniformly recognized in all studied organizations. We also examine the systemic configuration and patterning of these activities, which enables organizations to learn in <phrase>high</phrase> gear. This <phrase>organizational learning</phrase> <phrase>model</phrase> is contrasted with the <phrase>punctuated equilibrium</phrase> <phrase>model</phrase> of learning articulated in mainstream strategy <phrase>research</phrase>. Finally some implications for <phrase>future research</phrase> and <phrase>management</phrase> practice are drawn. material is granted provided that the copies are not made or distributed for direct <phrase>commercial advantage and that copies bear</phrase> this <phrase>copyright</phrase> notice and a full citation. Abstracting with credit is permitted. Written permission from the authors and possibly a fee are required prior to making any other use of this material.
Documenting Frameworks to Assist Application Developers A framework is a collection of abstract classes that provides an <phrase>infrastructure</phrase> common to a <phrase>family</phrase> of applications. The <phrase>design</phrase> of the framework xes certain roles and responsibilities amongst the classes, as well as standard protocols for their collaboration. The variability within the <phrase>family</phrase> of applications is factored into so-called \hotspots" 16], and the framework provides simple mechanisms to customize each hotspot. Customizing is typically done by subclassing an existing class of the framework and overriding a small number of methods. Sometimes, however, the framework insists that the customization preserves a protocol of collaboration between several subclasses, so customization requires the parallel development of these subclasses and certain of their methods. A framework exists to support the development of a <phrase>family</phrase> of applications. Reuse involves an application developer, or team of application developers, customizing the framework to construct one <phrase>concrete</phrase> application. Typically a framework is developed by expert designers who have a deep <phrase>knowledge</phrase> of the application domain and <phrase>long</phrase> experience of <phrase>software design</phrase>. On the other hand, a typical application developer who reuses the framework is less experienced and less knowledgeable of the domain. This is the situation desired by organizations since they wish to leverage the expertise of their core <phrase>software</phrase> designers. However, a framework is not an easy thing to understand when one rst uses it: the <phrase>design</phrase> is very abstract, to factor out commonality; the <phrase>design</phrase> is incomplete, requiring additional subclasses to create an application; the <phrase>design</phrase> provides exibility for several hotspots, not all of which are needed in the application at hand; and the collaborations and the resulting dependencies between classes can be indirect and obscure. The large <phrase>learning curve</phrase> faced by the rst-time user of a framework is a serious impediment to successfully reaping the beneets of reuse. How can an <phrase>organization</phrase> address this problem? Clearly, the development process for frameworks is one <phrase>area</phrase> where answers might be found. One quality criteria for a framework is that it should be as simple as possible to understand and customize.
<phrase>Multi-dimensional</phrase> Baker Maps for Chaos Based Image <phrase>Encryption</phrase> DEDICATION To the most cryptic of them all...to Enigma Himself To him, whose cryptic smile is decrypted In ways unique to each, Every decryption correct Unlike in the ordinary, Making this phenomenon A unique <phrase>cipher</phrase>. CERTIFICATE This is to certify that this project <phrase>report</phrase> entitled " <phrase>Multi-Dimensional</phrase> Baker Maps for Chaos Based Image <phrase>Encryption</phrase> " being submitted by Sri. Sai Cha-ran K. in partial fulfillment of the requirements for the award of the <phrase>degree</phrase> <phrase>Master</phrase> of <phrase>Technology</phrase> in <phrase>Computer Science</phrase> is a record of bonafide <phrase>research</phrase> work carried out by him under my supervision and guidance during the <phrase>academic</phrase> year 2007-08 in the <phrase>campus</phrase>. To the best of my <phrase>knowledge</phrase>, the <phrase>results</phrase> embodied in this project have not formed the basis of any work submitted to any other <phrase>University</phrase> or Institute for the award of any <phrase>Diploma</phrase> or <phrase>Degree</phrase>. Rao for the administrative support provided with regard to the project formalities. I <phrase>express my deep</phrase> sense of gratitude to my supervisor Prof. V. Chan-drasekaran for his infectious enthusiasm and for being such a wonderful spring of ideas. Most of the work herein would not have been possible if not for his out-of-the box ideas and lateral, non-linear, 'chaotic' thinking. I would also like to express my thankfulness to Sri <phrase>Uday Kiran</phrase>, <phrase>lecturer</phrase> at DMACS for the wonderful course work on <phrase>Algorithms</phrase> and Complexity and also for timely advice regarding <phrase>data-structures</phrase> and implementation issues. Sri S. Balasub-ramanian, <phrase>research</phrase> <phrase>scholar</phrase> at DMACS was very kind in helping me out with some numerical <phrase>algorithms</phrase>. The critical review and invaluable comments of Sri Srikanth Khanna, <phrase>research</phrase> <phrase>scholar</phrase> at DMACS have helped shaped this <phrase>thesis</phrase> and the publications that were communicated to various conferences. Mr. Krishnamoorthy was absolutely forthcoming in his support with <phrase>software</phrase> , hardware and invaluable <phrase>computing</phrase> advice. Mr. Raghunath Sarma's support with lab and inspirational resources have been indispensible. I would like to <phrase>express my gratitude</phrase> to the staff of DMACS for shaping me through my <phrase>tenure</phrase> here at the <phrase>department</phrase>. In particular, I would like to thank my HoD, Prof. K.S. Sridharan for his wonderful and efficient administrative and <phrase>infrastructure</phrase> support. I would like to thank Prof. G.V. Special thanks to Mr. Ravi Iyer for his discussions, insights and gyaan on various issues and for showing interest in my work. Learning was never more interesting and deep! I cannot forget the wonderful tidbits provided by Sri <phrase>Lakshmi</phrase> Narayan. Thanks are due 
Deep <phrase>Image Retrieval</phrase>: Learning Global Representations for Image Search We propose a novel approach for instance-level <phrase>image retrieval</phrase>. It produces a global and compact fixed-length representation for each image by aggregating many <phrase>region</phrase>-wise descriptors. In contrast to previous works employing <phrase>pre-trained</phrase> deep networks as a <phrase>black</phrase> box to produce features, our method leverages a <phrase>deep architecture</phrase> trained for the specific task of <phrase>image retrieval</phrase>. Our contribution is twofold: (i) we leverage a ranking framework to learn <phrase>convolution</phrase> and projection weights that are used to build the <phrase>region</phrase> features; and (<phrase>ii</phrase>) we employ a <phrase>region</phrase> proposal network to learn which regions should be pooled to form the final global descriptor. We show that using clean <phrase>training data</phrase> is key to the success of our approach. To that aim, we use a <phrase>large scale</phrase> but noisy landmark dataset and develop an automatic cleaning approach. The proposed <phrase>architecture</phrase> produces a global image representation in a <phrase>single</phrase> <phrase>forward pass</phrase>. Our approach <phrase>significantly outperforms</phrase> previous approaches based on global descriptors on standard datasets. It even surpasses most prior works based on costly local descriptor indexing and spatial verification 1 .
Zero <phrase>Knowledge</phrase> Protocols and Multiparty Computation on the Amortized Complexity of Zero <phrase>Knowledge</phrase> Protocols for Multiplicative Relations Introduction This progress <phrase>report</phrase> gathers two very different papers in the <phrase>research</phrase> field of <phrase>cryptography</phrase>. Our choice of the topics to present was based on two <phrase>major</phrase> considerations: First, both works carry deep <phrase>mathematical</phrase> aspects, ranging from <phrase>algebraic geometry</phrase> applied to secret sharing schemes to <phrase>Galois theory</phrase> applied to multiparty computation. The second <phrase>motivation</phrase> is the interest from a <phrase>cryptographic</phrase> perspective: our goal in those two papers was to improve the most recent <phrase>results</phrase> on topics of <phrase>major</phrase> interest, namely on the amortized complexity of zero <phrase>knowledge</phrase> protocols [CD09] and multiparty computation with dishonest majority [BDOZ11]. Given the different <phrase>nature</phrase> of the two topics, we prefer to make separate introduction to each of the two subjects: In Chapter 2 we present a zero-<phrase>knowledge</phrase> protocol aimed to prove multiplicative relations of three elements of a <phrase>finite field</phrase> and generalise it to a protocol proving <phrase>algebraic</phrase> relations; we also describe a similar protocol that proves multiplicative relations over the integers. In Chapter 3 we construct an implementation of secure multiparty computation against active dishonest majority. Such a <phrase>construction</phrase> is inspired by the work of Bendlin et al. [BDOZ11], but it is based on a somewhat homomorphic <phrase>encryption</phrase> scheme specifically designed to the purpose. In Chapter 4 we present a description of some of the most relevant topics we are researching on. The notions of commitment schemes and zero-<phrase>knowledge</phrase> proofs are among the most fundamental in the theory and practice of <phrase>cryptographic</phrase> protocols. Intuitively, a commitment scheme provides a way for a prover to put a value x in a locked box and commit to x by giving this box to a verifier. Later the prover can choose to open the box by giving away the key to the box. A <phrase>bit</phrase> more precisely, a commitment c = com pk (x, r) is a <phrase>function</phrase> of the committed value x, a <phrase>public</phrase> key pk and a random value r from some suitable domain. Commitments must be hiding: from c and pk it is hard to decide the value of x, and binding: it is hard to produce a commitment and open it in two different ways, In a zero-<phrase>knowledge</phrase> protocol, a prover wants to convince a verifier that some statement is true, such that the verifier learns nothing except the validity of the assertion. Typically, the prover claims that an input string u is in a <phrase>language</phrase> L, and after the interaction, the 
DeepCoder: Learning to Write Programs We develop a first line of attack for solving <phrase>programming</phrase> competition-style problems from <phrase>input-output</phrase> examples using <phrase>deep learning</phrase>. The approach is to <phrase>train</phrase> a <phrase>neural network</phrase> to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the <phrase>programming languages</phrase> <phrase>community</phrase>, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an <phrase>order</phrase> of <phrase>magnitude</phrase> speedup over the strong non-augmented baselines and a <phrase>Recurrent Neural Network</phrase> approach, and that we are able to <phrase>solve problems</phrase> of difficulty comparable to the simplest problems on <phrase>programming</phrase> competition <phrase>websites</phrase>.
Learning with Diagrams: Effects on Inferences and the Integration of <phrase>Information</phrase> Students studied materials about the <phrase>human</phrase> <phrase>heart</phrase> and <phrase>circulatory</phrase> system using either (a) text only, (b) text with simple diagrams, or (c) text with detailed diagrams. During learning, students self-explained [1] the materials. Explanations were transcribed, separated into propositions, and analyzed according to the type of learning process they represented. <phrase>Results</phrase> demonstrated that diagrams promoted inference generation but did not affect other learning processes (such as elaboration or comprehension monitoring). However, only simple diagrams promoted generation of inferences that integrated domain <phrase>information</phrase>. <phrase>Results</phrase> indicate that diagrams may be useful because they guide the learner to engage in the <phrase>cognitive processes</phrase> required for <phrase>deep understanding</phrase>. In recent years there has been growing interest and enthusiasm regarding the addition of visual resources to educational materials. The overall conclusion of previous <phrase>research</phrase> on text with pictures is that the addition of visual material improves students' memories and understanding [2]. In addition, Mayer and his colleagues have identified a number of principles that describe situations in which <phrase>multimedia</phrase> materials are most effective [3]. Although such principles are useful in identifying conditions that can maximize <phrase>multimedia</phrase> benefits, there is little evidence as to why diagrams improve <phrase>memory</phrase> and learning. The goals of this <phrase>research</phrase> were to determine: (a) if the comprehension processes of learners using text and diagrams were different from learners using text only, and (b) whether diagram complexity would influence comprehension processes. A simple text about the <phrase>heart</phrase> and <phrase>circulatory</phrase> system was used alone or in conjunction with a series of diagrams from one of two types: diagrams that were simplified to emphasize the functional aspects of the <phrase>heart</phrase> (see Figure 1a), or more detailed diagrams that depicted the correct <phrase>anatomy</phrase> of the <phrase>heart</phrase> in addition to its functional aspects (see Figure 1b). During learning, students self-explained the materials [1] and the resulting verbal protocols were separated into a series of complex propositions [4]. Two raters scored propositions as: paraphrases (statements that reflected <phrase>information</phrase> from the current * Kirsten <phrase>Butcher</phrase> has moved to a joint appointment at the
The scientific basis for prediction <phrase>research</phrase> In recent years there has been a huge growth in using statistical and <phrase>machine learning</phrase> methods to find useful prediction systems for <phrase>software</phrase> engineers. Of particular interest is predicting project effort and duration and defect behaviour. Unfortunately though <phrase>results</phrase> are often promising no <phrase>single</phrase> technique dominates and there are clearly complex interactions between technique, training methods and the problem domain. Since we lack deep theory our <phrase>research</phrase> is of necessity <phrase>experimental</phrase>. Minimally, as scientists, we need reproducible studies. We also need comparable studies. I will show through a <phrase>meta-analysis</phrase> of many primary studies that we are not presently in that situation and so the scientific basis for our collective <phrase>research</phrase> remains in doubt. By way of remedy I will argue that we need to address these issues of reporting protocols and expertise plus ensure blind analysis is routine.
Transparent-supported <phrase>radiance</phrase> <phrase>regression</phrase> <phrase>function</phrase> A modified RRF[Ren et al. 2013] rendering method called TsRRF is presented in this <phrase>paper</phrase>, which support <phrase>global illumination</phrase> in realtime for scenes with moving transparent objects. The key idea of this method is to augment the map between object and scene. There are two kinds of method to augment this map. First, we choose different attributes which can represent the true color of an object and the relationship with the whole scene in space, and at the same time in <phrase>order</phrase> to get these attributes, we use GPGPU to get real time <phrase>information</phrase>. Second we use <phrase>deep learning</phrase> to get the most important <phrase>information</phrase> from the sample <phrase>data</phrase> which can decrease the <phrase>overfitting</phrase>. In <phrase>order</phrase> to get more details and make full use of the sample <phrase>data</phrase>, we not only <phrase>partition</phrase> the scene by position, but also <phrase>partition</phrase> by object, and we will use different TsRRF to render different <phrase>light</phrase> effect like reflection or <phrase>refraction</phrase>. The network <phrase>forward</phrase> propagate process will also be put into the <phrase>GPU</phrase> and use the parallel feature to calculate quickly. As a result, the modified method works well when dealing with the transparent objects and have a real time effect.
A Robust Method for Transcript Quantification with <phrase>RNA-Seq</phrase> <phrase>Data</phrase> The advent of <phrase>high</phrase> throughput <phrase>RNA-seq</phrase> <phrase>technology</phrase> allows deep sampling of the <phrase>transcriptome</phrase>, making it possible to characterize both the diversity and the abundance of transcript isoforms. Accurate abundance estimation or transcript quantification of isoforms is critical for downstream differential analysis (e.g., healthy vs. diseased cells) but remains a challenging problem for several reasons. First, while various types of <phrase>algorithms</phrase> have been developed for abundance estimation, <phrase>short</phrase> reads often do not uniquely identify the transcript isoforms from which they were sampled. As a result, the quantification problem may not be identifiable, i.e., lacks a unique transcript <phrase>solution</phrase> even if the read maps uniquely to the <phrase>reference genome</phrase>. In this article, we develop a <phrase>general linear model</phrase> for transcript quantification that leverages reads spanning multiple splice junctions to ameliorate identifiability. Second, <phrase>RNA-seq</phrase> reads sampled from the <phrase>transcriptome</phrase> exhibit unknown position-specific and <phrase>sequence</phrase>-specific biases. We extend our method to simultaneously learn bias parameters during transcript quantification to improve accuracy. Third, transcript quantification is often provided with a candidate set of isoforms, not all of which are likely to be significantly expressed in a given tissue type or condition. By resolving the linear system with <phrase>LASSO</phrase>, our approach can infer an accurate set of dominantly expressed transcripts while <phrase>existing methods</phrase> tend to assign positive expression to every candidate isoform. Using simulated <phrase>RNA-seq</phrase> datasets, our method demonstrated better quantification accuracy and the inference of dominant set of transcripts than <phrase>existing methods</phrase>. The application of our method on real <phrase>data</phrase> experimentally demonstrated that transcript quantification is effective for differential analysis of transcriptomes.
Promoting a Ubiquitous <phrase>E</phrase>-learning Framework <phrase>Mobile</phrase> learning development is, most of the time, the privilege of practitioners with support from a technical team. The <phrase>deep learning</phrase> curve involved understanding the <phrase>native</phrase> <phrase>mobile</phrase> <phrase>operating systems</phrase> and developing <phrase>mobile</phrase> applications keeps most of the non-technical teachers and researchers away. We explore what <phrase>innovation</phrase> <phrase>mobile</phrase> technologies and <phrase>Web 2.0</phrase> can bring that allows teachers to <phrase>author</phrase> their own <phrase>mobile learning</phrase> environment and how best we can leverage the emerging technologies to better prepare students for the <phrase>digital</phrase> world. We hope our study will help build teachers' confidence and skills through learning the <phrase>affordance</phrase> of <phrase>mobile Web</phrase> 2.0 technologies in the process of developing <phrase>mobile learning</phrase> environments for their courses. In this <phrase>paper</phrase> we will describe our <phrase>design</phrase>-based <phrase>research</phrase> in our efforts to 1) develop a generic <phrase>mobile</phrase> learning based platform for all (MEPA) that allows easy access to device's core functions as well as <phrase>web-based</phrase> learning contents and social tools; 2) encourage instructors without a <phrase>programming</phrase> background to implement these functions for <phrase>teaching and learning</phrase>; 3) document <phrase>lessons learned</phrase> to inform <phrase>future research</phrase>.
<phrase>Hybrid</phrase> <phrase>Neural Network</phrase> <phrase>Architecture</phrase> for On-Line Learning Approaches to machine <phrase>intelligence</phrase> based on <phrase>brain</phrase> models have stressed the use of <phrase>neural networks</phrase> for generalization. Here we propose the use of a <phrase>hybrid</phrase> <phrase>neural network</phrase> <phrase>architecture</phrase> that uses two kind of <phrase>neural networks</phrase> simultaneously: (i) a surface learning agent that quickly adapt to new modes of operation; and, (<phrase>ii</phrase>) a <phrase>deep learning</phrase> agent that is very accurate within a specific regime of operation. The two networks of the <phrase>hybrid</phrase> <phrase>architecture</phrase> perform complementary functions that improve the overall performance. The performance of the <phrase>hybrid</phrase> <phrase>architecture</phrase> has been compared with that of back-propagation perceptrons and the CC and FC networks for chaotic <phrase>time-series</phrase> prediction, the CATS benchmark <phrase>test</phrase>, and <phrase>smooth function</phrase> approximation. It has been shown that the <phrase>hybrid</phrase> <phrase>architecture</phrase> provides a <phrase>superior</phrase> performance based on the RMS error criterion.
A Survey on <phrase>Information Retrieval</phrase>, Text Categorization, and Web Crawling This <phrase>paper</phrase> is a survey discussing <phrase>Information Retrieval</phrase> concepts, methods, and applications. It goes deep into the document and query modelling involved in IR systems, in addition to pre-processing operations such as removing stop words and searching by synonym techniques. The <phrase>paper</phrase> also tackles text categorization along with its application in <phrase>neural networks</phrase> and <phrase>machine learning</phrase>. Finally, the <phrase>architecture</phrase> of web crawlers is to be discussed shedding the <phrase>light</phrase> on how <phrase>internet</phrase> <phrase>spiders</phrase> index web documents and how they allow users to search for items on the web.
Informal <phrase>Human</phrase> <phrase>Mathematical</phrase> Reasoning We encounter <phrase>mathematics</phrase> in every <phrase>aspect</phrase> of our lives. Some of the deepest and greatest insights into reasoning were made in <phrase>mathematics</phrase>. Hence, it is not surprising that emulating such powerful reasoning on machines is one of the important and difficult aims of <phrase>artificial intelligence</phrase> and <phrase>automated reasoning</phrase>. <phrase>Human</phrase> <phrase>mathematicians</phrase> often use diagrams to better convey problems and generate intuitive and easily understandable solutions. They also often learn <phrase>general</phrase> solutions from examples of solutions to related problems. Sometimes, they may use analogy or <phrase>symmetry</phrase> in <phrase>solving problems</phrase>. My <phrase>research</phrase> is in the exploration of the <phrase>nature</phrase> of such informal reasoning. Informal <phrase>human</phrase> reasoning is very powerful, yet its potential has largely not been exploited in the <phrase>design</phrase> of mechanised reasoning systems (i.e., systems which use some <phrase>logic</phrase> formalism to (semi-) automatically <phrase>solve problems</phrase>). This can perhaps be explained by the fact that we do not have a <phrase>deep understanding</phrase> of informal techniques and their use in <phrase>problem solving</phrase>. In <phrase>order</phrase> to advance further the <phrase>state</phrase> of the <phrase>art</phrase> of <phrase>automated reasoning</phrase> systems, I think it is important to integrate some of the informal <phrase>human</phrase> reasoning techniques with the <phrase>proven</phrase> successful formal techniques, such as different types of <phrase>logic</phrase>. This will not only make the reasoning systems more powerful, but such systems can then serve as tools with which we can study and explore the <phrase>nature</phrase> of <phrase>human</phrase> reasoning. My aim is to formalise and emulate, in particular, <phrase>human</phrase> reasoning with diagrams and <phrase>human</phrase> learning, on machines. Theorems in <phrase>automated theorem proving</phrase> are usually proved with formal logical proofs, so called symbolic proofs. However, there is a <phrase>subset</phrase> of problems which humans can prove by the use of geometric operations on diagrams, so called diagrammatic proofs. Figure 1 presents an example of a diagrammatic proof of a theorem concerning the sum of odd naturals . The proof consists of repeatedly applying lcuts to a square (an lcut removes an ell shape which is formed from two adjacent sides of a square see Figure 1).
<phrase>Semantic</phrase> Image Search From Multiple Query Images This <phrase>paper</phrase> presents a novel search <phrase>paradigm</phrase> that uses multiple images as input to perform <phrase>semantic search</phrase> of images. While earlier focuses on using <phrase>single</phrase> or multiple query images to retrieve images with views of the same instance, the proposed <phrase>paradigm</phrase> uses each query image to discover <phrase>text-based</phrase> descriptors that are leveraged to find the common concepts that are implicitly shared by all of the query images and retrieves images considering the found concepts. Our implementation uses <phrase>high</phrase> level <phrase>visual features</phrase> extracted from a <phrase>deep convolutional</phrase> network to retrieve images similar to each query input. These images have associated text previously generated by implicit <phrase>crowdsourcing</phrase>. A Bag of Words (BoW) textual representation of each query image is built from the associated text of the retrieved similar images. A learned <phrase>vector space</phrase> representation of <phrase>English</phrase> words extracted from a corpus of 100 billion words allows <phrase>computing</phrase> the conceptual similarity of words. The words that represent the input images are used to find new words that share conceptual similarity across all the input images. These new words are combined with the representations of the input images to obtain a BoW textual representation of the search, which is used to perform <phrase>image retrieval</phrase>. The retrieved images are re-ranked to enhance visual similarity with respect to any of the input images. Our experiments show that the concepts found are meaningful and that they retrieve correctly 72.43% of the images from the top 25, along with user ratings performed in the cases of study.
Learning hierarchical invariant <phrase>spatio-temporal</phrase> features for <phrase>action</phrase> recognition with <phrase>independent</phrase> subspace analysis Previous work on <phrase>action</phrase> recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the <phrase>video</phrase> domain. In this <phrase>paper</phrase> , we propose using <phrase>unsupervised feature learning</phrase> as a way to learn features directly from <phrase>video</phrase> <phrase>data</phrase>. More specifically , we present an extension of the <phrase>Independent</phrase> Subspace Analysis <phrase>algorithm</phrase> to learn invariant <phrase>spatio-temporal</phrase> features from unlabeled <phrase>video</phrase> <phrase>data</phrase>. We discovered that, despite its simplicity, this method performs surprisingly well when combined with <phrase>deep learning</phrase> techniques such as stacking and <phrase>convolution</phrase> to learn hierarchical representations. By replacing hand-designed features with our <phrase>learned features</phrase> , we achieve classification <phrase>results</phrase> <phrase>superior</phrase> to all previous <phrase>published results</phrase> on the Hollywood2, <phrase>UCF</phrase>, <phrase>KTH</phrase> and <phrase>YouTube</phrase> <phrase>action</phrase> recognition datasets. On the challenging Hollywood2 and <phrase>YouTube</phrase> <phrase>action</phrase> datasets we obtain 53.3% and 75.8% respectively, which are approximately 5% better than the current best <phrase>published results</phrase>. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned <phrase>spatio-temporal</phrase> features here: http://ai.stanford.edu/wzou/
On Adding <phrase>Question Answering</phrase> to an Adaptive <phrase>Programming Language</phrase> <phrase>Tutor</phrase> Our aim is to produce an <phrase>e</phrase>-<phrase>tutor</phrase> that behaves like an experienced colleague looking over your shoulder as you learn a <phrase>language</phrase>. Our <phrase>e</phrase>-<phrase>tutor</phrase>, the Intelligent <phrase>Verilog</phrase> <phrase>Compiler</phrase> (IVC) uses an explainer with the <phrase>compiler</phrase> to check <phrase>syntax</phrase> and explain errors. A <phrase>model</phrase> checker is used to check the dynamic <phrase>semantics</phrase> of pre-defined exercises. The <phrase>motivation</phrase> for the explainer is to spot and <phrase>report</phrase> typographical and other errors that get in the way of understanding <phrase>Verilog</phrase>, whilst encouraging <phrase>deep learning</phrase> by not providing the answer directly. This models the way that a <phrase>human</phrase> supervisor leads a <phrase>student</phrase> towards a correct <phrase>solution</phrase>, and implements the concept of <phrase>deep learning</phrase> Fig. 1. The <phrase>student</phrase> has typed (<=) instead of (=) at line 24. The voogle component generates links back into the tutorial whilst the explainer (in <phrase>red</phrase> text) points out the error.
Developing a <phrase>Total Quality Management</phrase> Framework for <phrase>Healthcare</phrase> Organizations Tqm Definition in <phrase>Healthcare</phrase> <phrase>Quality management</phrase> has become an important issue in <phrase>healthcare</phrase> organizations (<phrase>hospitals</phrase>) during the last couple of decades. The increased attention to quality is due to governmental regulations, influence of customers, and <phrase>hospital</phrase> <phrase>management</phrase> initiatives. So, the role of <phrase>government</phrase> as the main provider of <phrase>healthcare</phrase> (HC) services has changed. Additionally, the <phrase>healthcare</phrase> market is changing from a <phrase>producer</phrase>-oriented to a customer-oriented market due to the increasing influence of customers and <phrase>public</phrase> pressures. As a consequence, the patient is becoming a customer for the <phrase>healthcare</phrase> organizations, or more likely a direct strategic partner who participates in a <phrase>decision-making</phrase> process. The changes in environment, <phrase>society</phrase>, and <phrase>political</phrase> policies have significant impacts on <phrase>management</phrase> in <phrase>hospitals</phrase> as well. There are many difficulties in managing <phrase>healthcare</phrase> organizations in a competitive marketplace with a little support from official bodies especially in a small <phrase>country</phrase> like <phrase>Jordan</phrase>. The purpose of this <phrase>paper</phrase> is to provide a framework for implementing the <phrase>total quality management</phrase> concept that is compatible with the local <phrase>culture</phrase> of <phrase>Jordan</phrase>. Theoretical Background <phrase>Total quality management</phrase> is a system that makes quality the responsibility of all clinicians and administrators throughout the <phrase>health care</phrase> <phrase>organization</phrase>. In TQM, systems are established to prevent clinical and administrative problems, increase patient satisfaction, continuously improve the organization's processes, and provide <phrase>healthcare</phrase> services as good, or better, then those of the competitors. Customer focus, error prevention, employee participation, teamwork, systemization, <phrase>leadership</phrase> and continuous quality improvement are TQM <phrase>universal</phrase> <phrase>management</phrase> concepts that can be transferred to any <phrase>business</phrase> setting. TQM uses quality as the fundamental measurement metric, continuous improvement as the <phrase>philosophy</phrase> and employee involvement as the approach. Therefore, TQM programs in <phrase>healthcare</phrase> can be measured, without the need for a deep <phrase>knowledge</phrase> of the <phrase>industry</phrase> unique circumstances. In <phrase>healthcare</phrase> services there are three definitions distinguished TQM from other approaches: One is that TQM is a " <phrase>Comprehensive</phrase> strategy of organizational and attitude change for enabling personnel to learn and use quality methods, in <phrase>order</phrase> to reduce costs and meet the requirements of patients and other customers. (Ovretveit, 2000) A second definition by Donabedian refers to quality as " the maximization of patient's satisfaction considering all profits and losses to be faced in a <phrase>healthcare</phrase> procedure " (Donabedian, 1989). A third definition given by US theories (William & Johnson, 2013) emphasized that TQM is a <phrase>management</phrase> method: " TQM/CQI Continuous Quality Improvement is simultaneously two things: a <phrase>management</phrase> <phrase>philosophy</phrase> 
<phrase>Human</phrase> <phrase>Gait</phrase> Recognition using <phrase>Deep Neural Networks</phrase> In this <phrase>paper</phrase>, we present a <phrase>deep learning</phrase> pipeline consisting of Deep Stacked <phrase>Auto-Encoders</phrase> stacked below Softmax classifier for classifying <phrase>human</phrase> <phrase>gait</phrase> features extracted using CA-<phrase>SIA</phrase> dataset. The <phrase>human</phrase> <phrase>gait</phrase> silhouettes were pre-processed for noise removal, segmented for body points extraction by which the required features were extracted. The missing <phrase>gait</phrase> points were estimated using <phrase>linear interpolation</phrase>. The features were scaled and classified using the pipeline. The performance of our <phrase>proposed approach</phrase> was also compared with the performances obtained using <phrase>Artificial Neural Networks</phrase> and using <phrase>Support Vector Machines</phrase>. We have obtained a <phrase>recognition accuracy</phrase> of 92.30%, 95.26% and 99.0% for <phrase>Support Vector Machines</phrase>, <phrase>Artificial Neural Networks</phrase>, and the <phrase>proposed approach</phrase> respectively.
An Efficient Platform for <phrase>Large-Scale</phrase> <phrase>MapReduce</phrase> Processing <phrase>ii</phrase> Acknowledgments I would like to <phrase>express my deep</phrase> gratitude to all the people who have helped and supported my <phrase>master</phrase> study during these years. I give sincere thanks to my adviser, Dr. Vassil Roussev, for his kind guidance and supervision of my study. I have learned a lot from Dr. Roussev, from <phrase>knowledge</phrase> and skills for <phrase>academic</phrase> <phrase>research</phrase> to attitude and strategies for being a wise person. It is always my pleasure to work with him. I am grateful to all the professors in my <phrase>thesis</phrase> committee. Besides my adviser, I want to thank Drs. Golden <phrase>Richard III</phrase> and Shengru Tu for their precious suggestions about my <phrase>master</phrase> <phrase>research</phrase> and advices about my <phrase>life</phrase> here. I especially thank Dr. <phrase>Mahdi</phrase> Abdelguerfi who cared much about my <phrase>life</phrase> and <phrase>family</phrase> during the whole <phrase>period</phrase> of my study, and Ms. Jeanne Boudreaux, our <phrase>department</phrase> <phrase>secretary</phrase>, for her hard work and kind help. I appreciate the understanding and encouragement of my wife Zhiyu Zhao and my parents. Without their care throughout these years it would be impossible for me to complete this <phrase>master</phrase> study. iii Contents Abstract vi 1 Introduction 1
Adaptation and <phrase>Personalization</phrase> in <phrase>Web-based</phrase> Learning Support Systems In <phrase>order</phrase> to achieve optimal efficiency in a learning process, individual learner needs his/her own personalized assistance. For a <phrase>web-based</phrase> open and dynamic <phrase>learning environment</phrase>, personalized support for learners becomes more important. This <phrase>paper</phrase> demonstrates how to realize personalized learning support in dynamic and heterogeneous <phrase>learning environments</phrase> by utilizing Adaptive Web technologies. We focus on course <phrase>personalization</phrase> in terms of contents and teaching materials that is according to each student's needs and capabilities. To accomplish this, a <phrase>conceptual model</phrase> based on the <phrase>Knowledge</phrase> Structure is presented. Using the hierarchy and association rules of the concepts, we can organize courses and lessons as a <phrase>multi-layer</phrase> <phrase>knowledge network</phrase>, which has a reasonable classification and interdependent relations among the <phrase>knowledge</phrase>. With retrieval based on concept and association among the concepts, we propose a framework of <phrase>knowledge</phrase> structure based visualization tool for representing a dynamic learning process to support students' <phrase>deep learning</phrase>, efficient tutoring and collaboration in <phrase>web-based</phrase> <phrase>learning environment</phrase>.
<phrase>Feature Representation</phrase> for Online Signature Verification <phrase>Biometrics</phrase> systems have been used in a wide <phrase>range</phrase> of applications and have improved people <phrase>authentication</phrase>. Signature verification is one of the most common biometric methods with techniques that employ various specifications of a signature. Recently, <phrase>deep learning</phrase> has achieved <phrase>great success</phrase> in many fields, such as image, sounds and text processing. In this <phrase>paper</phrase>, <phrase>deep learning</phrase> method has been used for <phrase>feature extraction</phrase> and <phrase>feature selection</phrase>, which has enormous impact on the accuracy of signature verification. This <phrase>paper</phrase> presents a method based on self-taught learning, in which a sparse autoencoder attempts to learn <phrase>discriminative features</phrase> of signatures from a large unlabeled signature dataset. Then, the features learned are employed to present users' signatures by creating a <phrase>model</phrase> for each user based on user genuine signatures. Finally, users' signatures are classified using a one-class classifier. The <phrase>proposed method</phrase> is <phrase>independent</phrase> on signature datasets thanks to self-taught learning. The features have been learned from 17,500 signatures (<phrase>ATVS</phrase> dataset) and verification process of the proposed system is evaluated on SVC2004 and SUSIG signature datasets, which contain genuine and skilled <phrase>forgery</phrase> signatures. The <phrase>experimental</phrase> <phrase>results</phrase> indicate significant error reduction and accuracy enhancement in comparison with <phrase>state</phrase> of the <phrase>art</phrase> counterparts.
The Equivalence of Learning Paths in Early <phrase>Science</phrase> Instruction: Effects of Direct Instruction and Discovery Learning In a study with 112 third and fourth grade children, we measured the relative effectiveness of discovery learning and direct instruction at two points in the learning process (a) during the initial acquisition of the <phrase>basic</phrase> <phrase>cognitive</phrase> objective: a procedure for designing and interpreting simple, unconfounded experiments, and (b) during the subsequent transfer and application of this <phrase>basic</phrase> skill to more diffuse and authentic reasoning associated with the evaluation of <phrase>science fair</phrase> posters. We found not only that many more children learned from direct instruction than from discovery learning, but also that when asked to make broader, richer scientific judgments the (many) children who learned about <phrase>experimental</phrase> <phrase>design</phrase> from direct instruction performed as well as those (few) children who discovered the method on their own. These <phrase>results</phrase> challenge predictions derived from the presumed superiority of discovery approaches to teaching young children <phrase>basic</phrase> procedures for early scientific investigations. A widely accepted claim in the <phrase>science</phrase> and <phrase>mathematics education</phrase> <phrase>community</phrase> is the constructivist idea that discovery learning, as opposed to direct instruction, is the best way to get deep and lasting understanding of scientific phenomena and procedures, particularly for young children. " The premise of <phrase>constructivism</phrase> implies that the <phrase>knowledge</phrase> students construct on their own, for example, is more valuable than the <phrase>knowledge</phrase> modeled for them; told to them; or shown, demonstrated, or explained to them by a <phrase>teacher</phrase> " (Loveless,1998, p. 285). Advocates of discovery learning concur with Piaget's claim that " each time one prematurely teaches a child something he could have discovered for himself the child is kept from inventing it and consequently from understanding it completely " (<phrase>Piaget</phrase>, 1970, p. 715). Moreover, they argue that children who acquire <phrase>knowledge</phrase> on their own are more likely to apply and extend that <phrase>knowledge</phrase> than those who receive direct There are pragmatic, empirical, and theoretical grounds for questioning this position. Pragmatically, it is clear that most of what students (and teachers and scientists) know about <phrase>science</phrase> was taught to them, rather than discovered by them. Empirical challenges to the <phrase>general</phrase> enthusiasm for discovery learning come from studies demonstrating that <phrase>teacher</phrase>-centered methods using direct instruction are highly effective 1986). In particular, direct instruction has been shown to be an efficient way to teach procedures that are difficult for students to discover on their own, such as those involved in <phrase>geometry</phrase>, <phrase>algebra</phrase>, and computer Finally, the theoretical basis for the predicted superiority of 
Developing <phrase>student</phrase> help desk consultants: a skill-based modular approach <phrase>Student</phrase> Help Desk consultants are an essential resource in most <phrase>college</phrase> IT departments. Hiring and training these students presents special challenges: consultants need a set of skills that is both wide and deep, and supervisors may be unsure which skills to hire for and which to <phrase>train</phrase> for, how to provide training, and how to measure competency. At <phrase>Duquesne University</phrase> we have created a system for developing skilled consultants that is based on: how easy a skill is to teach; which mode of teaching best lends itself to a particular skill; and the need to accommodate different <phrase>learning styles</phrase> in the students.We first grouped the skills into five <phrase>core competency</phrase> areas: technical <phrase>knowledge</phrase>, <phrase>troubleshooting</phrase> and <phrase>problem-solving</phrase>, <phrase>knowledge</phrase> of our structure and procedure, customer service skills, and <phrase>professional</phrase> behavior. We then identified five modalities for acquiring these skills in our consultants, being:<ul><li>hire people with the skill;</li><li>provide classroom training;</li><li><phrase>train</phrase> through <phrase>apprenticeship</phrase>;</li><li>rely on <phrase>independent</phrase> learning; or</li><li>provide ongoing training.</li></ul>.Different skill sets are emphasized within each modality, but most are addressed across several modalities so as to ensure mastery and accommodate different <phrase>learning styles</phrase>. For example, attitude and interpersonal skills are heavily weighted in the hiring criteria. Formal instruction in customer service skills is then provided in a classroom setting and application of these skills is emphasized in the <phrase>apprenticeship</phrase> phase of training. Finally, customer service skills may be further developed through <phrase>independent</phrase> learning and ongoing training.We use this approach to structure all phases of consultant development: candidate selection, training, evaluation, and ongoing support. This <phrase>poster</phrase> presentation provides an overview of the analysis and plan, together with a selection of the instruments (forms, <phrase>tests</phrase>, etc.) that we use in each phase.
Learning <phrase>Object-oriented Design</phrase> by Creating <phrase>Games</phrase> As a youth taking my first steps toward learning how to program a computer, I remember how exciting it was to write a program that printed the first 100 prime numbers. Nowadays, computer programs with fancy interfaces that provide access to <phrase>music</phrase>, <phrase>video</phrase>, and <phrase>games</phrase> have become commonplace, and such <phrase>results</phrase> no longer fascinate novice programmers. Instead, initiates want to be the ones who create these complex and attractive programs. Unfortunately, even with the most wonderful application-development tools available, creating these programs requires a huge amount of work and a <phrase>deep understanding</phrase> of the computer system, <phrase>programming language</phrase>, available <phrase>libraries</phrase>, and development tool used. How then do we transform <phrase>computing</phrase> consumers into creators? <phrase>LOGO</phrase> AND <phrase>LEGO</phrase> <phrase>Logo</phrase> (www.logosurvey.co.uk) and its many variants provide the classic example of a <phrase>programming language</phrase> aimed at creating interest among youngsters. Primarily seen as a <phrase>language</phrase> to make drawings, with <phrase>Logo</phrase>, the user steers a virtual <phrase>turtle</phrase> to draw shapes onscreen. Even the <phrase>basic</phrase> program can make fancy drawings this way, while modern versions extend Logo's possibilities considerably. For today's users, spoiled by <phrase>console</phrase> and computer <phrase>games</phrase>, <phrase>Logo</phrase> is no longer flashy enough, however. Steering a virtual <phrase>turtle</phrase> can't possibly compare with steering a <phrase>real robot</phrase>, which probably accounts for much of <phrase>Lego MindStorms</phrase>' success (www.legomindstorms.com). With the admittedly limited <phrase>software</phrase> that comes with this system, users can create and program their own <phrase>robots</phrase>. Fortunately, third-<phrase>party</phrase> developers have written complete <phrase>programming languages</phrase> for these <phrase>robots</phrase>, most notably NQC The main disadvantages of using <phrase>robots</phrase> to learn <phrase>programming</phrase> are their expense and limited <phrase>programming</phrase> possibilities. On the other hand, <phrase>robots</phrase> do provide great vehicles for explaining concepts such as sensing, control loops, and parallel tasks. 1 A shorter version of this <phrase>paper</phrase> appeared under the title Teaching <phrase>Computer Science</phrase> through <phrase>Game Design</phrase> in the April 2004 issue of <phrase>IEEE</phrase> Computer.
Teaching <phrase>computer science</phrase> through problems, not solutions Regardless of the course topic, every instructor in a <phrase>computing</phrase> field endeavors to engage their students in deep <phrase>problem-solving</phrase> and <phrase>critical thinking</phrase>. One of the specific <phrase>learning outcomes</phrase> throughout our <phrase>computer science</phrase> <phrase>curriculum</phrase> is the development of <phrase>independent</phrase>, capable <phrase>problem solving</phrase> and we believe good <phrase>pedagogy</phrase> can bring such about. Our experiences indicate to us that students improve their ability to analyze and solve complex computational problems when we pursue pedagogies that support them in developing these skills incrementally. Specifically, we pursue a <phrase>problem-based learning</phrase> approach that we apply individually in each course as well as across the entire <phrase>curriculum</phrase> of our <phrase>department</phrase>, instead of solely considering our <phrase>pedagogy</phrase> on a course-by-course basis. 1. What is <phrase>Problem-Based Learning</phrase>? <phrase>Problem-Based Learning</phrase> (<phrase>PBL</phrase>) is a <phrase>pedagogy</phrase> that centers <phrase>student</phrase> learning around <phrase>open-ended</phrase>, <phrase>student</phrase>-driven problems facilitated by an instructor in <phrase>order</phrase> to achieve the <phrase>learning outcomes</phrase> of a course. It appeals to a <phrase>cognitive</phrase> <phrase>constructivist epistemology</phrase> which concludes from study and experience that learners gain more through relating educational material to their own <phrase>real-life</phrase> experiences, and that such experience informs their ability to conceptualize content (Duffy & Jonnasen, 1992). <phrase>Constructivism</phrase> calls for learning opportunities that are experiential, active, collaborative, and that also develop <phrase>problem-solving</phrase> skills (Jonnasen, 2000). The goal here for the learner is not to passively absorb and regurgitate <phrase>information</phrase>; but rather to actively engage with the content, work through it with others, relate to it through an analysis with personal experience, and effectively <phrase>solve problems</phrase> with the corresponding <phrase>knowledge</phrase> gained. Thus the ultimate goal is the development of <phrase>critical-thinking</phrase> abilities. This of course means that the <phrase>student</phrase> is an active participant in the learning process (Bonwell & Eison, 1991). The result is a necessary relaxing of the traditional classroom structure so that students can pursue ideas in a <phrase>fashion</phrase> that makes sense to them individually, rather than the specific prescribed approach that the instructor may have in mind. Indeed, many approaches could be relevant for attaining the <phrase>knowledge</phrase> developed by the intellectual task at hand. So, students need to be <phrase>free</phrase> to develop those <phrase>knowledge</phrase> constructions in their own way. This does not mean that there is no structure to the process as some might suggest (Kirschner, Sweller & Clark, 2006). But rather, that a looser structure governs the endeavor and allows the <phrase>student</phrase> to maneuver in several different directions under the guidance of an engaged instructor. Of course, there 
<phrase>Boltzmann</phrase> Machines and <phrase>Denoising Autoencoders</phrase> for Image Denoising Image denoising based on a <phrase>probabilistic model</phrase> of local <phrase>image patches</phrase> has been employed by various researchers, and recently a deep (denoising) autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as a good <phrase>model</phrase> for this. In this <phrase>paper</phrase>, we propose that another popular <phrase>family</phrase> of models in the field of <phrase>deep learning</phrase>, called <phrase>Boltzmann</phrase> machines, can perform image denoising as well as, or in certain cases of <phrase>high</phrase> level of noise, better than <phrase>denoising autoencoders</phrase>. We empirically evaluate the two models on three different sets of images with different types and levels of noise. Throughout the experiments we also examine the effect of the depth of the models. The experiments confirmed our claim and revealed that the performance can be improved by adding more <phrase>hidden layers</phrase>, especially when the level of noise is <phrase>high</phrase>.
<phrase>Randomness</phrase> and Non-<phrase>determinism</phrase> <phrase>Exponentiation</phrase> makes the difference between the <phrase>bit</phrase>-size of this line and the number ( 2 300) of particles in the known <phrase>Universe</phrase>. The expulsion of exponential time <phrase>algorithms</phrase> from Computer Theory in the 60's broke its <phrase>umbilical cord</phrase> from <phrase>Mathematical Logic</phrase>. It created a deep gap between deterministic computation and formerly its unremarkable tools <phrase>randomness</phrase> and non-<phrase>determinism</phrase>. Little did we learn in the past decades about the power of either of these two <phrase>basic</phrase> " freedoms " of computation, but some vague pattern is emerging in relationships between them. The pattern of similar techniques <phrase>instrumental</phrase> for quite different <phrase>results</phrase> in this <phrase>area</phrase> seems even more interesting. Ideas like multilinear and low-<phrase>degree</phrase> multivariate <phrase>polynomials</phrase>, <phrase>Fourier</phrase> transformation over low-periodic groups seem very illuminating. The <phrase>talk</phrase> surveyed some recent <phrase>results</phrase>. One of them, given in a stronger form than previously published, is described below. |x| will denote the length of string x. Let P be the set of fast, i.e. <phrase>computable</phrase> in time T f (x) = |x| O(1) , <phrase>algorithms</phrase> f (x) on <phrase>binary</phrase> <phrase>strings</phrase>. [Blum Micali 82, Yao 82] proposed a fast deterministic way to generate " nearly perfect " <phrase>randomness</phrase>, using the idea of a hard core or hidden <phrase>bit</phrase>. They assume certain length preserving functions f P to be one-way (OWF), i.e. infeasible to invert (a non-deterministically easy task). Suppose it is hard to compute from f (x) not only x but even its one <phrase>bit</phrase> b(x) {1}, b P. Moreover, assume that even guessing b(x) with any noticeable correlation is infeasible. If f is <phrase>bijective</phrase>, f (x) and b(x) are both random and appear to be <phrase>independent</phrase> to any feasible <phrase>test</phrase>, thus increasing the initial amount |x| of <phrase>randomness</phrase> by one <phrase>bit</phrase>. Then, a <phrase>short</phrase> random <phrase>seed</phrase> x can be transformed into an arbitrary <phrase>long</phrase> string (1), (2),. . .: (i) = b(f (i) (x)). Such <phrase>passes</phrase> any feasible <phrase>randomness</phrase> <phrase>test</phrase>. [Goldreich Levin 89] showed that every OWF f has such a hidden <phrase>bit</phrase> with <phrase>security</phrase> of f and b polynomially related. It also gives more details on the definitions below. Here this result is strengthened to yield the same <phrase>security</phrase> for f and b. Let P be the set of probabilistic <phrase>algorithms</phrase> A(x, ) using <phrase>coin</phrase>-flips {0, 1} I N and running in <phrase>average</phrase> over time <phrase>E</phrase> T A(x,) = |x| O(1). An <phrase>inverter</phrase> I P for f 
Interleaved Text/Image Deep <phrase>Mining</phrase> on a <phrase>Large-Scale</phrase> <phrase>Radiology</phrase> <phrase>Database</phrase> for Automated Image Interpretation Despite tremendous progress in <phrase>computer vision</phrase>, there has not been an attempt to apply <phrase>machine learning</phrase> on very <phrase>large-scale</phrase> <phrase>medical</phrase> image <phrase>databases</phrase>. We present an interleaved text/image <phrase>deep learning</phrase> system to extract and mine the <phrase>semantic</phrase> interactions of <phrase>radiology</phrase> images and reports from a national <phrase>research</phrase> hospital's Picture Archiving and <phrase>Communication</phrase> System. With <phrase>natural language processing</phrase>, we mine a collection of 216K representative two-dimensional images selected by clinicians for diagnostic reference and match the images with their descriptions in an automated manner. We then employ a <phrase>weakly supervised</phrase> approach using all of our available <phrase>data</phrase> to build models for generating approximate interpretations of patient images. Finally, we demonstrate a more strictly supervised approach to detect the presence and absence of a number of frequent <phrase>disease</phrase> types, providing more specific interpretations of patient scans. A relatively small amount of <phrase>data</phrase> is used for this part, due to the challenge in gathering quality <phrase>labels</phrase> from large raw text <phrase>data</phrase>. Our work shows the feasibility of <phrase>large-scale</phrase> learning and prediction in <phrase>electronic</phrase> patient records available in most modern clinical institutions. It also demonstrates the <phrase>trade</phrase>-offs to consider in designing <phrase>machine learning</phrase> systems for analyzing large <phrase>medical</phrase> <phrase>data</phrase>.
Implementation of Training <phrase>Convolutional Neural Networks</phrase> <phrase>Deep learning</phrase> refers to a shining branch of <phrase>machine learning</phrase> that is based on learning levels of representations. <phrase>Convolutional Neural Networks</phrase> (<phrase>CNN</phrase>) is one kind of <phrase>deep neural network</phrase>. It can study concurrently. In this article, we use <phrase>convolutional neural network</phrase> to implement the typical <phrase>face recognition</phrase> problem which can overcome the influence of pose or resolution in <phrase>face recognition</phrase>. Then, a parallel strategy was proposed in section4. In addition, by measuring the actual time of <phrase>forward</phrase> and backward <phrase>computing</phrase>, we analysed the maximal speed up and parallel efficiency theoretically. 1. INTRODUTION <phrase>Deep learning</phrase> refers to a subfield of <phrase>machine learning</phrase> that is based on learning levels of representations, corresponding to a hierarchy of features, factors or concepts, where higher-lever concepts are defined from <phrase>lower</phrase>-lever ones, and the same <phrase>lower</phrase>-lever concepts can help to define many higher-lever concepts. <phrase>Deep learning</phrase> is learning <phrase>multiple levels</phrase> of representation and abstraction, helps to understand the <phrase>data</phrase> such as images, audio and text. The concept of <phrase>Deep Learning</phrase> comes from the study of <phrase>Artificial Neural Network</phrase>, <phrase>Multilayer Perceptron</phrase> which contains more <phrase>hidden layers</phrase> is a <phrase>Deep Learning</phrase> structure. In the late 1980s, the <phrase>invention</phrase> of Back Propagation <phrase>algorithm</phrase> used in <phrase>Artificial Neural Network</phrase> brings hope to <phrase>machine learning</phrase> and creates a trend of <phrase>machine learning</phrase> based on <phrase>statistical models</phrase>. In the 1990s, a <phrase>variety</phrase> of Shallow Learning models have been proposed such as <phrase>Support Vector Machines</phrase> (<phrase>SVM</phrase>), Boosting, <phrase>Logistic Regression</phrase> (LR). The structure of these models can be seen as one hidden node (<phrase>SVM</phrase>, Boosting), or no <phrase>hidden nodes</phrase> (LR). These models gained a <phrase>great success</phrase> both in theoretical analysis and applications. In 2006, <phrase>Geoffrey Hinton</phrase> who is the <phrase>professor</phrase> of <phrase>University</phrase> of <phrase>Toronto</phrase>, <phrase>Canada</phrase> and the <phrase>dean</phrase> of <phrase>machine learning</phrase> and his students Ruslan Salakhutdinov published an article in " <phrase>Science</phrase> " , <phrase>led</phrase> to a trend of <phrase>machine learning</phrase> in <phrase>academia</phrase> and <phrase>industry</phrase>. The article had two points: 1) <phrase>Artificial Neural Network</phrase> with multiple <phrase>hidden layers</phrase> has an excellent ability of characteristic learning. The
Automatically Generating Discussion Questions Automatic question generation can support instruction and learning. However, work to date has <phrase>produced</phrase> mostly " shallow " questions that fall <phrase>short</phrase> of supporting <phrase>deep learning</phrase> and discussion. We propose an extension to a <phrase>state</phrase>-of-the-<phrase>art</phrase> question generation system that allows it to produce deep, subjective questions suitable for group discussion. We evaluate the questions generated by this system against a panel of experienced judges, and find that our approach fares significantly better than the baseline system.
Stacked <phrase>Denoising Autoencoders</phrase>: Learning Useful Representations in a Deep Network with a Local Denoising Criterion We explore an original strategy for building deep networks, based on stacking layers of <phrase>denoising autoencoders</phrase> which are trained locally to denoise corrupted versions of their inputs. The resulting <phrase>algorithm</phrase> is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly <phrase>lower</phrase> classification error, thus bridging the performance gap with <phrase>deep belief</phrase> networks (DBN), and in several cases surpassing it. <phrase>Higher level</phrase> representations learnt in this purely unsupervised <phrase>fashion</phrase> also help boost the performance of subsequent <phrase>SVM</phrase> classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, <phrase>denoising autoencoders</phrase> are able to learn Gabor-like edge detectors from natural <phrase>image patches</phrase> and larger <phrase>stroke</phrase> detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful <phrase>higher level</phrase> representations.
In at the Deep End: An Activity-<phrase>Led</phrase> Introduction to First Year <phrase>Creative Computing</phrase> Misconceptions about the <phrase>nature</phrase> of the <phrase>computing</phrase> disciplines pose a serious problem to <phrase>university</phrase> faculties that offer <phrase>computing</phrase> degrees, as students enrolling on their programmes may come to realise that their expectations are not met by <phrase>reality</phrase>. This frequently <phrase>results</phrase> in the students' early disengagement from the subject of their degrees which in turn can <phrase>lead</phrase> to excessive 'wastage', i.e. reduced retention. In this <phrase>paper</phrase> we <phrase>report</phrase> on our <phrase>academic</phrase> group's attempts within <phrase>creative computing</phrase> degrees at a <phrase>UK</phrase> <phrase>university</phrase> to counter these problems through the introduction of a six week <phrase>long</phrase> project that newly enrolled students embark on at the very beginning of their studies. This group project provides a breadth-first, activity-<phrase>led</phrase> introduction to their chosen <phrase>academic</phrase> discipline, aiming to increase <phrase>student</phrase> engagement while providing a stimulating learning experience with the overall goal to increase retention. We present the methods and <phrase>results</phrase> of two iterations of these projects in the 2009/2010 and 2010/2011 <phrase>academic</phrase> years, and conclude that the approach worked well for these cohorts, with students expressing increased interest in their chosen discipline, in addition to noticeable improvements in retention following the first year of the students' studies.
Estimation of static formation temperatures in <phrase>geothermal</phrase> wells by using an <phrase>artificial neural network</phrase> approach Keywords: Horner method <phrase>Geothermal energy</phrase> LevenbergMarquardt <phrase>algorithm</phrase> <phrase>Borehole</phrase> drilling Bottom-hole <phrase>temperature</phrase> <phrase>Artificial intelligence</phrase> Shut-in time a b s t r a c t An <phrase>artificial neural network</phrase> (ANN) approach was used to develop a new predictive <phrase>model</phrase> for the calculation of static formation <phrase>temperature</phrase> (SFT) in <phrase>geothermal</phrase> wells. A three-layer ANN <phrase>architecture</phrase> was successfully trained using a <phrase>geothermal</phrase> <phrase>borehole</phrase> <phrase>database</phrase>, which contains ''statistically normal-ised'' SFT estimates. These estimates were inferred from seven analytical methods commonly used in <phrase>geothermal</phrase> <phrase>industry</phrase>. Bottom-hole <phrase>temperature</phrase> (BHT) measurements and shut-in times were used as main input variables for the ANN training. Transient <phrase>temperature</phrase> gradients were used as <phrase>secondary</phrase> variables. The LevenbergMarquardt (LM) <phrase>learning algorithm</phrase>, the hyperbolic <phrase>tangent</phrase> <phrase>sigmoid</phrase> <phrase>transfer function</phrase> and the linear <phrase>transfer function</phrase> were used for the ANN optimisation. The best <phrase>training data</phrase> set was obtained with an ANN <phrase>architecture</phrase> composed by five <phrase>neurons</phrase> in the <phrase>hidden layer</phrase>, which made possible to predict the SFT with a satisfactory efficiency (R 2 4 0.95). A suitable accuracy of the ANN <phrase>model</phrase> was achieved with a percentage error less than 7 5%. The SFTs predicted by the ANN <phrase>model</phrase> were statistically analyzed and compared with ''true'' SFTs measured in synthetic experiments and actual BHT logs collected in <phrase>geothermal</phrase> boreholes during <phrase>long</phrase> shut-in times. These <phrase>data</phrase> sets were processed both to validate the new ANN <phrase>model</phrase> and to avoid bias. The SFT estimates inferred from the ANN validation process were in good agreement (R 2 40.95) with the ''true'' SFT <phrase>data</phrase> reported for synthetic and field experiments. The <phrase>results</phrase> suggest that the new ANN <phrase>model</phrase> could be used as a practical tool for the reliable prediction of SFT in <phrase>geothermal</phrase> wells using BHT and shut-in time as <phrase>input data</phrase> only. The exploitation of <phrase>geothermal</phrase> resources for producing <phrase>electricity</phrase> requires the drilling of deep boreholes in the most suitable thermal regions of <phrase>geothermal</phrase> fields (Saito et al., 1998; Davis and Michaelides, 2009). The <phrase>borehole</phrase> drilling is a complex process in which a constant thermal anomaly (added to a circulating <phrase>drilling mud</phrase>) affects the original <phrase>rock</phrase>-formation <phrase>temperature</phrase> surrounding the <phrase>borehole</phrase> (Fomin et al., 2003, 2005). Once the <phrase>borehole</phrase> drilling is completed, the resulting thermal recovery is evaluated by analyzing build-up bottom-hole <phrase>temperature</phrase> (BHT) and shut-in time measurements (Santoyo et al., 2000). BHT <phrase>data</phrase> are measured at different shut-in times during the <phrase>borehole</phrase> drilling operations (Espinosa-Paredes and Espinosa-Martinez, 2009). BHTs are usually costly due to the use of sophisticated 
The <phrase>Cognitive</phrase> Representation of <phrase>Computer-supported</phrase> Instructional Tools Two studies are reported whose aim was to assess whether undergraduates' <phrase>cognitive</phrase> representations of the <phrase>psychological</phrase> correlates of <phrase>computer-supported</phrase> instructional tools (CSIT) vary according to the features of tools themselves. A questionnaire investigating participants' conceptions about motivational and emotional aspects of learning through CSIT, behavior during the learning process, required capacities, mental operations, <phrase>metacognition</phrase>, preferred style of thinking, <phrase>cognitive</phrase> benefits and <phrase>learning outcomes</phrase> was employed in both studies. In Study 1 undergraduates were requested to <phrase>judge</phrase> to what extent such issues are involved in different kinds (virtual simulations, Web forums, and so on) of CSIT, whereas in Study 2 the same issues were considered by making reference to different dimensions (<phrase>hypertext</phrase>, <phrase>multimedia</phrase>, and so forth) of CSIT. <phrase>Results</phrase> showed that students have a well-defined and deep-rooted conception about what CSIT can introduce into a learning process, by attributing different <phrase>psychological</phrase> correlates to different kinds of tools. However, undergraduates failed to recognize that distinct dimensions of CSIT involve different <phrase>psychological</phrase> correlates. Implications of these findings for <phrase>education</phrase> and tool designing are discussed. From Goals to Effects: "Objective" and "Subjective" Perspectives <phrase>Computer-supported</phrase> instructional tools (CSIT) are often presented as instruments that teachers, trainers, and tutors might adopt in <phrase>order</phrase> to try to <phrase>solve problems</phrase> that they encounter in their job, particularly to <phrase>lead</phrase> trainees to overcome the difficulties that they experience in learning. The main question that educators ask when someone proposes to them to employ technological devices is: Do this tool actually produce the expected outcomes? (Giles, 2003). A way to conceptualize this question is the following. There is a need that requires to be satisfied (for example, rising the level of trainees' <phrase>motivation</phrase>) or a goal to be reached (for instance, improving students' understanding of a hard concept). Instructors look for, or devise by themselves, a tool which should help students in satisfying that need or in achieving that goal. Then they induce learners to use that tool. The outcomes <phrase>produced</phrase> by the tool are detected in <phrase>order</phrase> to <phrase>test</phrase> the alleged efficacy of the tool. Such an approach stresses the importance of the characteristics of the tool to be employed, assuming that the higher is its quality, the better are the learning <phrase>results</phrase>. In this perspectives the way of employing a tool derives directly from the affordances of the tool: What was "put" into the tool (provided that it was phenomenologically salient), that will be used properly by students, 
Learning from Text with Diagrams: Promoting <phrase>Mental Model</phrase> Development and Inference Generation Learning with Text and Diagrams Two experiments investigated <phrase>learning outcomes</phrase> and comprehension processes when students learned about the <phrase>heart</phrase> and <phrase>circulatory</phrase> system using (a) text only, (b) text with simplified diagrams designed to highlight important structural relations, or (c) text with more detailed diagrams reflecting a more accurate representation. Experiment 1 found that both types of diagrams supported <phrase>mental model</phrase> development, but simplified diagrams best supported factual learning. Experiment 2 replicated learning effects from Experiment 1 and tested the influence of diagrams on novices' comprehension processes. Protocol analyses indicated that both types of diagrams supported inference generation and reduced comprehension errors, but simplified diagrams most strongly supported <phrase>information</phrase> integration during learning. <phrase>Visual representations</phrase> appear to be most effective when they are designed to support the <phrase>cognitive processes</phrase> necessary for deep comprehension. As <phrase>multimedia</phrase> <phrase>technology</phrase> becomes increasingly popular in formal and informal educational settings, the importance of <phrase>research</phrase> investigating learning with visual and verbal materials takes on added value. Understanding the ways in which visual materials influence learning will be essential to developing <phrase>multimedia</phrase> tools with consistent and predictable benefits. Advancing <phrase>technology</phrase> has meant that <phrase>multimedia</phrase> often now includes complex forms of interactive and computationally intensive presentations; however, <phrase>multimedia</phrase> can be more simply defined as any presentation that includes verbal and visual <phrase>information</phrase> (Mayer, 2001). In practice, <phrase>basic</phrase> types of multimediasuch as pictures and textstill appear to be frequently used. Currently, many <phrase>digital</phrase> and print materials use pictures, diagrams, and text as their primary <phrase>communication</phrase> format. But how does the visual representation of <phrase>information</phrase> influence learning? Can changes in comprehension processes account for the impact of diagrams on learning? The purpose of this <phrase>research</phrase> was to investigate potential effects of different diagram representations on students' <phrase>learning outcomes</phrase> and comprehension processes when diagrams were added to a <phrase>science</phrase> text. Early <phrase>research</phrase> on pictures and text consistently demonstrated that students learned more after <phrase>reading</phrase> illustrated versus nonil-lustrated text (for a review, see Levie & Lentz, 1982). These studies predominantly administered <phrase>memory</phrase> measures for the source materials, including <phrase>multiple-choice</phrase> and fill-in-the-blank <phrase>tests</phrase>. But a <phrase>long</phrase> <phrase>history</phrase> of <phrase>cognitive</phrase> <phrase>research</phrase> has distinguished between rote memorization and <phrase>deeper understanding</phrase> (<phrase>e</phrase>. for a discussion). Deeper learning evidenced by measures that assess application and transfer of informationwas not widely tested in early <phrase>research</phrase> on illustrations. However, one early set of studies (see Dwyer, 1967, 1968, 1975) did <phrase>test</phrase> both <phrase>memory</phrase> and deep comprehension for an illustrated text. Dwyer (1967, 1968, 1975) found benefits 
Static and Dynamic Task Mapping onto Network on Chip Multiprocessors Static and dynamic task mapping onto network on chip multiprocessors Mapeo esttico y dinmico de tareas en sistemas multiprocesador, basados en redes en circuito integrado Abstract Due to its <phrase>scalability</phrase> and flexibility, Network-on-Chip (<phrase>NoC</phrase>) is a growing and promising <phrase>communication</phrase> <phrase>paradigm</phrase> for <phrase>Multiprocessor</phrase> System-on-Chip (MPSoC) <phrase>design</phrase>. As the <phrase>manufacturing</phrase> process scales down to the <phrase>deep submicron</phrase> domain and the complexity of the system increases, <phrase>fault-tolerant</phrase> <phrase>design</phrase> strategies are gaining increased relevance. This <phrase>paper</phrase> exhibits the use of a <phrase>Population</phrase>-Based <phrase>Incremental Learning</phrase> (PBIL) <phrase>algorithm</phrase> aimed at finding the best mapping solutions at <phrase>design</phrase> time, as well as to finding the optimal remapping <phrase>solution</phrase>, in presence of <phrase>single</phrase>-node failures on the <phrase>NoC</phrase>. The optimization objectives in both cases are the application completion time and the network's <phrase>peak</phrase> bandwidth. A deterministic XY routing <phrase>algorithm</phrase> was used in <phrase>order</phrase> to simulate the traffic conditions in the network which has a 2D mesh <phrase>topology</phrase>. Obtained <phrase>results</phrase> are promising. The <phrase>proposed algorithm</phrase> exhibits a better performance, when compared with other reported approaches, as the problem size increases. Palabras clave: Mapeo de tareas, Sistemas integrados multiprocesador (MPSoC), Redes en circuito integrado (<phrase>NoC</phrase>), Aprendizaje incremental basado en poblacin (PBIL).
Differing Ways That <phrase>Computing</phrase> Academics Understand Teaching This <phrase>paper</phrase> presents first <phrase>results</phrase> from a wide-ranging phenomenographic study of <phrase>computing</phrase> academics' understanding of teaching. These first <phrase>results</phrase> focus upon four areas: the role of lab practical sessions, the experience of teaching success, conceptions of motivating and engaging students, and the granularity of the teacher's focus. The findings are comparable with prior work on the Reproduction for <phrase>academic</phrase>, not-for-profit purposes permitted provided this text is included. understandings of academics in other disciplines. This study was started as part of a workshop on phenomenography. Most participants at the workshop received their first training in phenomenography. This <phrase>paper</phrase> summarises the structure of the workshop. 1 Phenomenography While most readers of this <phrase>paper</phrase> would be familiar with the dual concepts of deep and surface learning, fewer might know that the origins of these concepts lie in phenomenographic <phrase>research</phrase>. Phenomenography is a <phrase>research</phrase> approach that focuses on the qualitatively different ways that people experience, understand,
<phrase>Model</phrase> <phrase>Order</phrase> Reduction for Nonlinear Systems <phrase>Model</phrase> <phrase>Order</phrase> Reduction for Nonlinear Systems This <phrase>thesis</phrase> presents some practical methods for doing <phrase>model</phrase> <phrase>order</phrase> reduction for a <phrase>general</phrase> type of nonlinear systems. Based on quadratic or even higher <phrase>degree</phrase> approximation and <phrase>tensor</phrase> reduction with assistance of Arnoldi type projection, we demonstrate a much better accuracy for the reduced nonlinear system to capture the original behavior than the traditional <phrase>linearization</phrase> method. Acknowledgments I rst want to thank <phrase>Professor</phrase> <phrase>Jacob</phrase> <phrase>White</phrase>, the supervisor of this <phrase>thesis</phrase>, for his very helpful advice and guidance crucial in many stages of the <phrase>research</phrase> done in the <phrase>thesis</phrase> and a lot of <phrase>academic</phrase> advice and also his summer nancial support. And I learned most of my numerical <phrase>knowledge</phrase> and intuition from his 6.336 class. I also want to thank the <phrase>Department</phrase> of <phrase>Mathematics</phrase> for the <phrase>academic</phrase> and nancial support and especially our Graduate Coordinator Linda Okun who gave much help for the administrational procedure. I became a fan for tensors (which is widely used in the main chapters of the <phrase>thesis</phrase>) in the <phrase>multilinear algebra</phrase> class of my highly respected late advisor in <phrase>math</phrase> <phrase>department</phrase> <phrase>Gian-Carlo Rota</phrase>, whose innuence to me is deep and enormous. Frank Wang of <phrase>MIT</phrase> gave me some kind technical help and Junling <phrase>Ma</phrase> of <phrase>Princeton</phrase> helped me with a question on ODE. And I also want to thank several colleagues in the RLE-<phrase>VLSI</phrase> group who ooered help with <phrase>computers</phrase>. The weekly group lunch which further nourished my time there should also be mentioned.
Active <phrase>Semi-Supervised</phrase> Learning Method with <phrase>Hybrid</phrase> <phrase>Deep Belief</phrase> Networks In this <phrase>paper</phrase>, we develop a novel <phrase>semi-supervised learning</phrase> <phrase>algorithm</phrase> called active <phrase>hybrid</phrase> <phrase>deep belief</phrase> networks (AHD), to address the <phrase>semi-supervised</phrase> sentiment <phrase>classification problem</phrase> with <phrase>deep learning</phrase>. First, we construct the previous several <phrase>hidden layers</phrase> using <phrase>restricted Boltzmann machines</phrase> (RBM), which can reduce the <phrase>dimension</phrase> and abstract the <phrase>information</phrase> of the reviews quickly. Second, we construct the following <phrase>hidden layers</phrase> using convolutional <phrase>restricted Boltzmann machines</phrase> (CRBM), which can abstract the <phrase>information</phrase> of reviews effectively. Third, the constructed <phrase>deep architecture</phrase> is <phrase>fine-tuned</phrase> by <phrase>gradient-descent</phrase> based <phrase>supervised learning</phrase> with an exponential <phrase>loss function</phrase>. Finally, <phrase>active learning</phrase> method is combined based on the proposed <phrase>deep architecture</phrase>. We did several experiments on five sentiment classification datasets, and show that AHD is competitive with previous <phrase>semi-supervised learning</phrase> <phrase>algorithm</phrase>. Experiments are also conducted to verify the effectiveness of our <phrase>proposed method</phrase> with different number of labeled reviews and unlabeled reviews respectively.
Learning to Cooperate <phrase>Curriculum</phrase> Vitae His current work in <phrase>machine learning</phrase> has been pursued since 1999 with <phrase>professor</phrase> Dana Ballard. iii Acknowledgments I would like to <phrase>express my gratitude</phrase> to all those who gave me the possibility to complete this <phrase>thesis</phrase>. I want to thank the <phrase>University</phrase> of <phrase>Rochester</phrase> <phrase>Computer Science</phrase> <phrase>department</phrase> for giving me permission to commence this <phrase>thesis</phrase> in the first instance, to do the necessary <phrase>research</phrase> work and to use departmental resources. This <phrase>thesis</phrase> is the result of several years of work whereby I have been accompanied and supported by many people. It is a pleasant <phrase>aspect</phrase> that I have now the opportunity to <phrase>express my gratitude</phrase> for all of them. The first person I would like to thank is my adviser, Dana Ballard, for stimulating suggestions and encouragement helped me in all the time of <phrase>research</phrase> for, giving me the freedom to pursue my <phrase>research</phrase> goals, and writing of this <phrase>thesis</phrase>. I would also like to thank the other members of my <phrase>PhD</phrase> committee who monitored my work and took effort in <phrase>reading</phrase> and providing me with valuable comments on earlier versions of this <phrase>thesis</phrase>: Mitsunori Ogihara, <phrase>Chris Brown</phrase> and Mark Fey. I thank you all. The faculty, students and staff all gave me the feeling of being at home at work. I want to thank them for all their help, support, interest and valuable hints. Especially I am obliged to <phrase>Tao Li</phrase>, Chen Yu, <phrase>Qi</phrase> Li for collaboration in several projects. Also I would like to thank Deqing Chen, Chunqiang <phrase>Tang</phrase> and Xipeng Shen for discussing <phrase>research</phrase> topics and playing <phrase>squash</phrase>. I feel a deep sense of gratitude for my parents always gave me boundless <phrase>love</phrase> and support to follow <phrase>the dream</phrase> I choose. I am grateful for my sister, Chunshi, iv who encouraged me during the years. Especially, I would like to give my special thanks to my wife, Sang, whose patient <phrase>love</phrase> enabled me to complete this work. Abstract <phrase>Game theory</phrase> is not only useful to understand the performance of <phrase>human</phrase> and autonomous <phrase>game</phrase> players, but it is also widely employed to solve <phrase>resource allocation</phrase> problems in distributed <phrase>decision making</phrase> systems. These <phrase>distributed systems</phrase> are mostly referred to as <phrase>multi-agent systems</phrase>. <phrase>Reinforcement learning</phrase> is a promising technique for learning agents to adapt their own strategies in such systems. Most existing <phrase>reinforcement learning</phrase> <phrase>algorithms</phrase> are designed from a <phrase>single</phrase>-agent's perspective and for simplicity assume the environment is stationary, i.e., 
Empirical learning aided by weak <phrase>domain knowledge</phrase> in the form of feature importance Standard <phrase>hybrid</phrase> learners that use <phrase>domain knowledge</phrase> require stronger <phrase>knowledge</phrase> that is hard and expensive to acquire. However, weaker <phrase>domain knowledge</phrase> can benefit from <phrase>prior knowledge</phrase> while being cost effective. Weak <phrase>knowledge</phrase> in the form of feature relative importance (FRI) is presented and explained. Feature relative importance is a <phrase>real valued</phrase> approximation of a feature's importance provided by experts. Advantage of using this <phrase>knowledge</phrase> is demonstrated by IANN, a modified multilayer <phrase>neural network</phrase> <phrase>algorithm</phrase>. IANN is a very simple modification of standard <phrase>neural network</phrase> <phrase>algorithm</phrase> but attains significant performance gains. <phrase>Experimental</phrase> <phrase>results</phrase> in the field of <phrase>molecular biology</phrase> show higher performance over other empirical <phrase>learning algorithms</phrase> including standard <phrase>backpropagation</phrase> and <phrase>support vector machines</phrase>. IANN performance is even comparable to a theory refinement system KBANN that uses stronger <phrase>domain knowledge</phrase>. This shows Feature relative importance can <phrase>improve performance</phrase> of existing empirical <phrase>learning algorithms</phrase> significantly with minimal effort. 1 Introduction Empirical learning methods are the dominant methods for <phrase>supervised learning</phrase> problems. But these methods are dependent on significant amount of <phrase>training data</phrase> and training time to perform well. Furthermore, It has been shown [1] that <phrase>learning algorithms</phrase> simply refine the <phrase>knowledge</phrase> provided through the inductive bias in the <phrase>algorithm</phrase>. So, a learner only learns what it already knows with a better accuracy. Therefore, providing more <phrase>prior knowledge</phrase> greatly improves performance. A learner's learned <phrase>model</phrase> will also usually be much more comprehensible if the learner takes existing <phrase>knowledge</phrase> into account [2]. There has been extensive <phrase>research</phrase> to combine <phrase>prior knowledge</phrase> into <phrase>learning algorithms</phrase> (e.g. Different systems use different forms of <phrase>prior knowledge</phrase>. Many of them use <phrase>propositional logic</phrase> such as KBANN. Eccentric forms of <phrase>prior knowledge</phrase> have also been used such as, derivatives of instances in TangentProp [3]; Certainty factors in <phrase>RAPTURE</phrase> [5]. However, the use of these <phrase>hybrid</phrase> systems has not been prevalent in <phrase>real world</phrase> applications. The main reason for this is the <phrase>high</phrase> cost and difficulty of obtaining <phrase>domain knowledge</phrase> [8] [9]. The <phrase>prior knowledge</phrase> needed in these systems is " deep " and extensive; in fact, they are called theory refinement systems; for they essentially refine an existing <phrase>domain theory</phrase>. Such domain theories are sufficient to classify instances on their own in many cases. This means much more resources and expertise are required to acquire such deep <phrase>prior knowledge</phrase>. But
A Survey on <phrase>Object Detection</phrase> in Optical <phrase>Remote Sensing</phrase> Images <phrase>Object detection</phrase> in optical <phrase>remote sensing</phrase> images, being a fundamental but challenging problem in the field of aerial and <phrase>satellite</phrase> <phrase>image analysis</phrase>, plays an important role for a wide <phrase>range</phrase> of applications and is receiving significant attention in recent years. While enormous methods exist, a deep review of the <phrase>literature</phrase> concerning generic <phrase>object detection</phrase> is still lacking. This <phrase>paper</phrase> aims to provide a review of the recent progress in this field. Different from several previously published surveys that focus on a specific object class such as building and <phrase>road</phrase>, we concentrate on more generic object categories including, but are not limited to, <phrase>road</phrase>, building, <phrase>tree</phrase>, vehicle, <phrase>ship</phrase>, <phrase>airport</phrase>, <phrase>urban-area</phrase>. Covering about 270 publications we survey 1) template matching-based <phrase>object detection</phrase> methods, 2) <phrase>knowledge</phrase>-based <phrase>object detection</phrase> methods, 3) object-based <phrase>image analysis</phrase> (OBIA)-based <phrase>object detection</phrase> methods, 4) <phrase>machine learning</phrase>-based <phrase>object detection</phrase> methods, and 5) five publicly available datasets and three standard evaluation metrics. We also discuss the challenges of current studies and propose two promising <phrase>research</phrase> directions, namely <phrase>deep learning</phrase>-based <phrase>feature representation</phrase> and <phrase>weakly supervised</phrase> learning-based geospatial <phrase>object detection</phrase>. It is our hope that this survey will be beneficial for the researchers to have better understanding of this <phrase>research</phrase> field.
<phrase>Recurrent Neural Networks</phrase> With Limited Numerical Precision <phrase>Recurrent Neural Networks</phrase> (RNNs) produce <phrase>state</phrase>-of-<phrase>art</phrase> performance on many <phrase>machine learning</phrase> tasks but their demand on resources in terms of <phrase>memory</phrase> and computational power are often <phrase>high</phrase>. Therefore, there is a great interest in optimizing the computations performed with these models especially when considering development of specialized <phrase>low-power</phrase> hardware for deep networks. One way of reducing the computational needs is to limit the numerical precision of the network weights and biases, and this will be addressed for the case of RNNs. We present <phrase>results</phrase> from the use of different <phrase>stochastic</phrase> and deterministic reduced precision training methods applied to two <phrase>major</phrase> RNN types, which are then tested on three datasets. The <phrase>results</phrase> show that the <phrase>stochastic</phrase> and deterministic ternarization, pow2-ternarization, and exponential quantization methods gave rise to low-precision RNNs that produce similar and even higher accuracy on certain datasets, therefore providing a path towards training more efficient implementations of RNNs in specialized hardware.
The <phrase>immune</phrase> self: a selectionist theory of recognition, learning, and remembering within the <phrase>immune</phrase> system. In this <phrase>paper</phrase>, I have briefly explored <phrase>metaphors</phrase> shared by the <phrase>immune</phrase> and <phrase>nervous</phrase> systems and shown that this exercise can <phrase>lead</phrase> to the elucidation of common principles of <phrase>organization</phrase>, as well as to predictions concerning how the <phrase>immune</phrase> system functions. <phrase>Metaphor</phrase> itself undoubtedly reflects the way in which we categorize and retrieve <phrase>information</phrase> 44], so it is not surprising that the deep processes of <phrase>language</phrase> tend to sample <phrase>information</phrase> from related <phrase>data</phrase> categories. Although the <phrase>nervous</phrase> and <phrase>immune systems</phrase> are obviously not the same and <phrase>metaphors</phrase> are indeed just that, my primary goal has been to suggest that by virtue of their having evolved in parallel over millions of years, the <phrase>nervous</phrase> and <phrase>immune systems</phrase> currently use the same archetypal principles and strategies to address related challenges in <phrase>information processing</phrase> and retrieval. Ultimately, <phrase>nature</phrase> is <phrase>conservative</phrase>. One need only look at a <phrase>tree</phrase>, a <phrase>river</phrase>, the airways, or the vascular bed in <phrase>order</phrase> to see how a <phrase>fractal</phrase> pattern of repetitive dichotomous branching has been used by each, in <phrase>order</phrase> to optimize the <phrase>transport</phrase> of fluids over large distances [45]. While each system has had to adopt different materials in <phrase>order</phrase> to solve the problem, the shape of their solutions is remarkably alike. In the <phrase>immune</phrase> and <phrase>nervous</phrase> systems, the elements used to produce optimal functional responses are also quite different, but again the solutions have been achieved by comparable strategies. I am certain that these two great systems of <phrase>information processing</phrase>, each responding with vastly different <phrase>kinetics</phrase>, will prove to be far more integrally interdependent than has been previously recognized. For example, should a <phrase>swift</phrase> response by the <phrase>immune</phrase> system be required in an overwhelming invasion by microbial <phrase>pathogens</phrase>, the <phrase>immune</phrase> system may be able to cooperate with the rapidly reacting <phrase>nervous</phrase> system to rid the host of the invaders. In this regard, we have shown that the beta-<phrase>adrenergic</phrase> <phrase>hormone</phrase> <phrase>epinephrine</phrase> rapidly increases the traffic of <phrase>memory</phrase> T-cells to mucosal sites, presumably representing an <phrase>immune</phrase> component of the <phrase>fight-or-flight response</phrase> [46]. Neural <phrase>evolution</phrase> appears to have as its goal the development of more efficient <phrase>information processing</phrase> systems that <phrase>lead</phrase> to higher levels of <phrase>consciousness</phrase>. However, in modern times, <phrase>technologic</phrase> advances in <phrase>information processing</phrase> have rapidly outstripped the slower adaptations that can be made by <phrase>evolution</phrase>. In <phrase>order</phrase> to satisfy his compulsive quest for <phrase>information</phrase>, man has recently developed and recruited the aid of <phrase>computers</phrase>.(ABSTRACT TRUNCATED AT 400 WORDS)
Building User Profiles from Shared Photos In this <phrase>paper</phrase>, we analyze the association between a <phrase>social media</phrase> user's photo content and their interests. Visual content of photos is analyzed using <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>deep learning</phrase> based automatic concept recognition. We compute an aggregate visual concept signature for each user. User tags that have been manually applied to their photos are also used to construct a tf-<phrase>idf</phrase> based signature per user. We also obtain social groups that users join to represent their social interests. In an effort to compare the visual-based versus tag-based user profiles with social interests, we compare corresponding similarity matrices with a reference similarity matrix based on users' group memberships. A random baseline is also included that groups users by random sampling while preserving the actual group sizes. A difference metric is proposed and it is shown that the combination of visual and text features better approximates the group-based similarity matrix than either modality individually. We also validate the visual analysis against the reference inter-user similarity using the <phrase>Spearman rank correlation coefficient</phrase>. Finally we cluster users by their visual signatures and rank clusters using a cluster uniqueness criteria.
<phrase>Knowledge engineering</phrase> in an intelligent environment The concept of <phrase>knowledge engineering</phrase>, starting with its deep association with <phrase>information management</phrase>, still carries multiple, even conflicting interpretations. The most popular one being a structured field that encompasses processes and techniques for <phrase>knowledge</phrase> discovery , indexing, <phrase>organization</phrase>, and <phrase>fusion</phrase>. Where the <phrase>classical</phrase> approach to <phrase>knowledge engineering</phrase> and <phrase>management</phrase> tends to rely on techniques like <phrase>concept maps</phrase>, hy-permedia and <phrase>object-oriented</phrase> <phrase>databases</phrase>, <phrase>computational intelligence</phrase> techniques for core <phrase>knowledge engineering</phrase> activities like <phrase>knowledge</phrase> discovery, <phrase>organization</phrase>, and <phrase>knowledge</phrase> <phrase>fusion</phrase> are rapidly gaining popularity. In the evolved scenario, <phrase>knowledge engineering</phrase> may be interpreted as a field that deals with acquisition, storage and application of <phrase>knowledge</phrase> for a <phrase>range</phrase> of <phrase>knowledge</phrase> intensive tasks whether it is <phrase>decision support</phrase>, learning or <phrase>research</phrase> support. This <phrase>special issue</phrase> on 'Knowl-edge <phrase>Engineering</phrase> in an Intelligent Environment' is an attempt to present some of the latest theoretical and application developments in the field of <phrase>knowledge engineering</phrase>. This <phrase>special issue</phrase> comprises of four papers on different aspects of <phrase>knowledge management</phrase> and is organized as follows. In the first <phrase>paper</phrase> Jermol et al. present a virtual enterprise <phrase>model</phrase> used in networking international expert teams from <phrase>academia</phrase> and <phrase>business</phrase> in the <phrase>area</phrase> of <phrase>data mining</phrase> and <phrase>decision support</phrase>. The <phrase>knowledge management</phrase> aspects of <phrase>business intelligence</phrase> as implemented in the virtual enterprise <phrase>model</phrase> are analyzed in terms of appropriate <phrase>business</phrase> organizational and <phrase>management</phrase> models. Further, <phrase>construction</phrase> of a <phrase>knowledge</phrase> map of the available tools, expertise and collaborative work procedures, <phrase>cognitive</phrase> authority in collaborative work <phrase>management</phrase>, as well as the network <phrase>intelligence</phrase> <phrase>aspect</phrase> of the virtual enterprise endeavor are discussed. Authors made use of a <phrase>European</phrase> virtual enterprise as a <phrase>case study</phrase> to illustrate some of the <phrase>lessons learned</phrase>. <phrase>Messina</phrase> et al. in the second <phrase>paper</phrase> discuss a rigorous approach to <phrase>engineer</phrase> the <phrase>knowledge</phrase> within intelligent controllers. The key to real-time intelligent control lies in the <phrase>knowledge</phrase> models that the system contains. Authors identified three main classes of <phrase>knowledge</phrase> namely parametric, geometric/iconic, and symbolic and examples are illustrated. Each of these classes provides unique perspectives and advantages for the planning of behaviors by the intelligent system. Since the early eighties, there has been a gradual shift in the focus of development of <phrase>knowledge based systems</phrase> away from the <phrase>rapid prototyping</phrase> techniques that had previously prevailed, toward more structured methodologies, including <phrase>model</phrase> based reasoning and modeling of <phrase>knowledge</phrase> domains. The default standard for the development of these systems has become the CommonKADS methodology. In the 
<phrase>Information Extraction</phrase> from <phrase>Wikipedia</phrase> Using Pattern Learning In this <phrase>paper</phrase> we present solutions for the crucial task of extracting struc-tured <phrase>information</phrase> from massive <phrase>free</phrase>-text resources, such as <phrase>Wikipedia</phrase>, for the <phrase>sake</phrase> of <phrase>semantic</phrase> <phrase>databases</phrase> serving upcoming <phrase>Semantic Web</phrase> technologies. We demonstrate both a <phrase>verb</phrase> frame-<phrase>based approach</phrase> using deep <phrase>natural language processing</phrase> techniques with extraction patterns developed by <phrase>human</phrase> <phrase>knowledge</phrase> experts and <phrase>machine learning</phrase> methods using shallow <phrase>linguistic</phrase> processing. We also propose a method for learning <phrase>verb</phrase> frame-based extraction patterns automatically from <phrase>labeled data</phrase>. We show that <phrase>labeled training</phrase> <phrase>data</phrase> can be <phrase>produced</phrase> with only minimal <phrase>human</phrase> effort by utilizing existing <phrase>semantic</phrase> resources and the special characteristics of <phrase>Wikipedia</phrase>. Custom solutions for <phrase>named entity</phrase> recognition are also possible in this scenario. We present evaluation and comparison of the different approaches for several different relations.
Why Gawk for <phrase>AI</phrase>? Most people are surprised when I tell them what <phrase>language</phrase> we use in our <phrase>undergraduate</phrase> <phrase>AI</phrase> <phrase>programming</phrase> class. That's understandable. We use GAWK. GAWK, Gnu's version of Aho, Weinberger, and Kernighan's old pattern scanning <phrase>language</phrase> isn't even viewed as a <phrase>programming language</phrase> by most people. Like <phrase>PERL</phrase> and <phrase>TCL</phrase>, most prefer to view it as a "<phrase>scripting language</phrase>." It has no objects; it is not functional; it does no built-in <phrase>logic programming</phrase>. Their surprise turns to puzzlement when I confide that (a) while the students are allowed to use any <phrase>language</phrase> they want; (b) with a <phrase>single</phrase> exception, the best work consistently <phrase>results</phrase> from those working in GAWK. i Programmers in C, <phrase>C++</phrase>, and <phrase>LISP</phrase> haven't even been close (we have not seen work in <phrase>PROLOG</phrase> or <phrase>JAVA</phrase>). Why GAWK? There are some quick answers that have to do with the <phrase>pragmatics</phrase> of <phrase>undergraduate</phrase> <phrase>programming</phrase>. Then there are more instructive answers that might be valuable to those who debate <phrase>programming</phrase> paradigms or to those who study the <phrase>history</phrase> of <phrase>AI</phrase> languages. And there are some deep <phrase>philosophical</phrase> answers that expose the <phrase>nature</phrase> of reasoning and symbolic <phrase>AI</phrase>. I think the answers, especially the last ones, can be even more surprising than the observed effectiveness of GAWK for <phrase>AI</phrase>. First it must be confessed that <phrase>PERL</phrase> programmers can cobble together <phrase>AI</phrase> projects well, too. Most of GAWK's attractiveness is reproduced in <phrase>PERL</phrase>, and the success of <phrase>PERL</phrase> forebodes some of the success of GAWK. Both are powerful string-processing languages that allow the <phrase>programmer</phrase> to exploit many of the features of a <phrase>UNIX</phrase> environment. Both provide powerful constructions for manipulating a wide <phrase>variety</phrase> of <phrase>data</phrase> in reasonably efficient ways. Both are interpreted, which can reduce development time. Both have <phrase>short</phrase> learning curves. The GAWK manual can be consumed in a <phrase>single</phrase> lab session and the iThe exception was a PASCAL <phrase>programmer</phrase> who is now an <phrase>NSF</phrase> graduate <phrase>fellow</phrase> getting a Ph.D. in <phrase>mathematics</phrase> at <phrase>Harvard</phrase>. <phrase>language</phrase> can be mastered by the next morning by the <phrase>average</phrase> <phrase>student</phrase>. GAWK's automatic initialization, implicit <phrase>coercion</phrase>, I/O support and lack of pointers forgive many of the mistakes that young programmers are likely to make. Those who have seen C but not mastered it are happy to see that GAWK retains some of the same sensibilities while adding what must be regarded as spoonsful of <phrase>syntactic sugar</phrase>. Some will argue that <phrase>PERL</phrase> has <phrase>superior</phrase> functionality, but for 
Integrating Induction and <phrase>Case-Based Reasoning</phrase>: Methodological Approach and First Evaluations We propose in this <phrase>paper</phrase> a <phrase>general</phrase> framework for integrating inductive and <phrase>case-based reasoning</phrase> (CBR) techniques for diagnosis tasks. We present a set of practical integrated approaches realised between the KATE-Induction <phrase>decision tree</phrase> builder and the PATDEX <phrase>case-based reasoning</phrase> system. The integration is based on the <phrase>deep understanding</phrase> about the weak and strong points of each <phrase>technology</phrase>. This theoretical <phrase>knowledge</phrase> permits to specify the structural possibilities of a <phrase>sound</phrase> integration between the relevant components of each approach. We define different levels of integration called "<phrase>cooperative</phrase>", "<phrase>workbench</phrase>" and "seamless". They realise respectively a tight, medium and strong link between both techniques. <phrase>Experimental</phrase> <phrase>results</phrase> show the appropriateness of these integrated approaches for the treatment of noisy or unknown <phrase>data</phrase>. 1 Introduction Integration of case-base reasoning and other learning paradigms is a growing <phrase>research</phrase> <phrase>area</phrase> today. The representation and use of additional <phrase>domain knowledge</phrase>, e.g., rules or deep causal models, is an important issue for dealing with <phrase>real world</phrase> applications (e.g., [1] and [2]). Numerous <phrase>case-based</phrase> systems have experimented on integrated use of <phrase>problem solving</phrase> methods. Some suggestions for the integration between <phrase>case-based reasoning</phrase> and <phrase>model</phrase>-based <phrase>knowledge</phrase> acquisition are given in [15]. The MOBAL system [24] integrates manual and automatic <phrase>knowledge</phrase> acquisition methods. The CASEY system [17] integrates a <phrase>model</phrase>-based causal reasoning program to diagnose <phrase>heart</phrase> diseases. An example of integrating rules and cases is the <phrase>BOLERO</phrase> system [19]. The MOLTKE approach [8] integrates different kinds of knowledgetechnical and heuristicto deal with a complex technical application (Computerised Numeric Control <phrase>Machining</phrase> Centre). We focus in this <phrase>paper</phrase> on the integration between a <phrase>case-based reasoning</phrase> tool (PATDEX) and an inductive learning tool (KATE-Induction) for diagnosis tasks in complex structured domains. This idea has an origin in Schank approach [30] who wrote: " In essence, <phrase>case-based reasoning</phrase> means no more than reasoning from experience. The issue of whether something is best called a case, or a rule, or a story, is one of understanding how experience get encode in <phrase>memory</phrase> ". In a very simplified
Learning from <phrase>Games</phrase>: HCI <phrase>Design</phrase> Innovations in <phrase>Entertainment</phrase> <phrase>Software</phrase> Computer <phrase>games</phrase> are one of the most successful application domains in the <phrase>history</phrase> of interactive systems. This success has come despite the fact that <phrase>games</phrase> were 'separated at birth' from most of the accepted paradigms for designing usable interactive <phrase>software</phrase>. It is now apparent that this separate and less-constrained environment has allowed for much <phrase>design</phrase> <phrase>creativity</phrase> and many innovations that make <phrase>game</phrase> interfaces highly usable. We analyzed several current <phrase>game</phrase> interfaces looking for ideas that could be applied more widely to <phrase>general</phrase> UIs. In this <phrase>paper</phrase> we present four of these: effortless <phrase>community</phrase>, learning by watching, deep customizability, and <phrase>fluid</phrase> system-<phrase>human</phrase> interaction. These ideas have arisen in <phrase>games</phrase> because of their focus on user performance and user satisfaction, and we believe that they can help to improve the <phrase>usability</phrase> of other types of applications.
<phrase>Recent Advances</phrase> in <phrase>Convolutional Neural Networks</phrase> In the last few years, <phrase>deep learning</phrase> has <phrase>lead</phrase> to very good performance on a <phrase>variety</phrase> of problems, such as <phrase>object recognition</phrase>, <phrase>speech recognition</phrase> and <phrase>natural language processing</phrase>. Among different types of <phrase>deep neural networks</phrase>, <phrase>convolutional neural networks</phrase> have been most extensively studied. Due to the lack of <phrase>training data</phrase> and <phrase>computing</phrase> power in early days, it is hard to <phrase>train</phrase> a large <phrase>high</phrase>-capacity <phrase>convolutional neural network</phrase> without <phrase>overfitting</phrase>. Recently, with the rapid growth of <phrase>data</phrase> size and the increasing power of <phrase>graphics processor</phrase> unit, many researchers have improved the <phrase>convolutional neural networks</phrase> and achieved <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> on various tasks. In this <phrase>paper</phrase>, we provide a broad survey of the <phrase>recent advances</phrase> in <phrase>convolutional neural networks</phrase>. Besides, we also introduce some applications of <phrase>convolutional neural networks</phrase> in <phrase>computer vision</phrase>.
<phrase>Deep Linguistic</phrase> Processing for Spoken Dialogue Systems We describe a framework for <phrase>deep linguistic</phrase> processing for <phrase>natural language understanding</phrase> in task-oriented spoken dialogue systems. The goal is to create domain-<phrase>general</phrase> processing techniques that can be shared across all domains and dialogue tasks, combined with <phrase>domain-specific</phrase> optimization based on an <phrase>ontology</phrase> mapping from the generic LF to the application on-tology. This framework has been tested in six domains that involve tasks such as interactive planning, coordination operations, tutoring, and learning.
<phrase>NICE</phrase>: Non-linear <phrase>Independent</phrase> Components Estimation We propose a <phrase>deep learning</phrase> framework for modeling complex <phrase>high</phrase>-dimensional densities via Non-linear <phrase>Independent</phrase> Component Estimation (<phrase>NICE</phrase>). It is based on the idea that a good representation is one in which the <phrase>data</phrase> has a distribution that is easy to <phrase>model</phrase>. For this purpose, a non-linear deterministic transformation of the <phrase>data</phrase> is learned that maps it to a latent space so as to make the transformed <phrase>data</phrase> conform to a factorized distribution, i.e., resulting in <phrase>independent</phrase> <phrase>latent variables</phrase>. We parametrize this transformation so that <phrase>computing</phrase> the <phrase>determinant</phrase> of the <phrase>Jacobian</phrase> and inverse <phrase>Jacobian</phrase> is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple <phrase>building blocks</phrase>, each based on a <phrase>deep neural network</phrase>. The training criterion is simply the exact <phrase>log-likelihood</phrase>, which is tractable, and unbiased ancestral sampling is also easy. We show that this approach yields good <phrase>generative models</phrase> on four image datasets and can be used for inpainting.
Shallow vs. Deep Sum-Product Networks We investigate the representational power of sum-product networks (computation networks analogous to <phrase>neural networks</phrase>, but whose individual units compute either <phrase>products</phrase> or weighted sums), through a theoretical analysis that compares deep (multiple <phrase>hidden layers</phrase>) vs. shallow (one <phrase>hidden layer</phrase>) architectures. We prove there exist families of functions that can be represented much more efficiently with a deep network than with a shallow one, i.e. with substantially fewer <phrase>hidden units</phrase>. Such <phrase>results</phrase> were not available until now, and contribute to motivate recent <phrase>research</phrase> involving learning of deep sum-product networks, and more generally motivate <phrase>research</phrase> in <phrase>Deep Learning</phrase>.
A Rational <phrase>Design</phrase> Process It's Time to Stop Faking It The <phrase>literature</phrase> on <phrase>Object-Oriented</phrase> <phrase>software development</phrase> processes has proposed a lifecycle which is often found to be at odds with the established <phrase>software engineering</phrase> processes of organizations. In particular, more established <phrase>software engineering</phrase> processes at higher CMM (<phrase>Capability Maturity Model</phrase>) levels often have deep roots in the '<phrase>Waterfall</phrase>' lifecycle Some of these mature processes might be evolving into legacy processes. The emerging <phrase>Software Engineering</phrase> Body of <phrase>Knowledge</phrase> (SWEBOK) is more appropriate than the <phrase>Project Management</phrase> Body of <phrase>Knowledge</phrase> (PMBOK) as a course of study for potential project managers of <phrase>software</phrase> projects. PMBOK has a tendency to emphasize scope <phrase>management</phrase> and task decomposition, while SWEBOK focus on <phrase>requirements analysis</phrase> and <phrase>architectural</phrase> <phrase>design</phrase>. <phrase>Recent developments</phrase> in <phrase>Object-Oriented</phrase> <phrase>software engineering</phrase> assert that an emphasis on requirements rather than scope, and on <phrase>architecture</phrase> rather than tasks leads to <phrase>superior software</phrase> development processes. Specifically, organizations should not demand detailed fixed scope, cost and schedule plans at the beginning of a significant <phrase>software development</phrase> effort. There is a lesson to be learn from the building <phrase>industry</phrase>, which allocates up to half of the overall development time to <phrase>architectural</phrase> <phrase>design</phrase>, and does not create a controlled project environment until the <phrase>construction</phrase> phase. When developing <phrase>e-commerce</phrase> applications, <phrase>requirements analysis</phrase> and system <phrase>architecture</phrase> remain critical, but they should be expanded to a broader context. The <phrase>business plan</phrase> replaces a project plan in <phrase>e-commerce</phrase>; <phrase>marketing</phrase> (and even sales) drive <phrase>requirements analysis</phrase>; and <phrase>architectural</phrase> <phrase>design</phrase> should be broadened to include <phrase>infrastructure</phrase> <phrase>design</phrase>. <phrase>E-commerce</phrase> system <phrase>design</phrase> encompasses system-wide issues, including hardware, networks, purchased components and partnerships, as well as interfaces with back-end fulfillment and collection capabilities.
A <phrase>multimedia</phrase> <phrase>literacy</phrase> series: the <phrase>cognition</phrase> and <phrase>technology</phrase> group at <phrase>Vanderbilt</phrase> The Little <phrase>Planet</phrase> <phrase>Literacy</phrase> Series is a <phrase>multimedia</phrase> <phrase>language</phrase> and <phrase>literacy</phrase> program for beginning readers. Its no-floor, no-ceiling <phrase>design</phrase> is learner-centered, based on five years of <phrase>research</phrase> with a <phrase>range</phrase> of students, including those at <phrase>risk</phrase> of <phrase>school</phrase> failure and those well on the <phrase>road</phrase> to learning to read. The <phrase>research</phrase> and <phrase>prototypes</phrase> for the series were developed at <phrase>Vanderbilt University</phrase>, then <phrase>licensed</phrase> to Little <phrase>Planet</phrase> <phrase>Publishing</phrase> in <phrase>Nashville</phrase>, Tenn. for development and distribution. Institute for Studies in <phrase>Education</phrase>). The program is structured around <phrase>video</phrase> stories that anchor a series of activities targeting deep comprehension, composition, and oral <phrase>language</phrase>, along with traditional print-based skills.
Sequential <phrase>Deep Learning</phrase> for <phrase>Human</phrase> <phrase>Action</phrase> Recognition We propose in this <phrase>paper</phrase> a fully automated deep <phrase>model</phrase>, which learns to classify <phrase>human</phrase> actions without using any <phrase>prior knowledge</phrase>. The first step of our scheme, based on the extension of Convo-lutional <phrase>Neural Networks</phrase> to 3D, automatically learns <phrase>spatio-temporal</phrase> features. A <phrase>Recurrent Neural Network</phrase> is then trained to classify each <phrase>sequence</phrase> considering the temporal <phrase>evolution</phrase> of the <phrase>learned features</phrase> for each timestep. <phrase>Experimental</phrase> <phrase>results</phrase> on the <phrase>KTH</phrase> dataset show that the <phrase>proposed approach</phrase> outperforms existing deep models, and gives comparable <phrase>results</phrase> with the best related works.
Learning Causal Laws Causal Grammars Attempts to characterize people's causal <phrase>knowledge</phrase> of a domain in terms of causal network structures miss a key level of abstraction: the laws that allow people to formulate meaningful causal network hypotheses, and thereby learn and reason about novel causal systems so effectively. We outline a preliminary framework for modeling causal laws in terms of generative grammars for causal networks. We then present an experiment showing that causal grammars can be learned rapidly in a novel domain and used to support one-<phrase>shot</phrase> inferences about the unobserved causal properties of new objects. Finally, we give a <phrase>Bayesian analysis</phrase> explaining how causal grammars may be induced from the limited <phrase>data</phrase> available in our experiments. Recently there has been substantial progress in understanding how people learn causal relations, or causal networks connecting multiple causes and effects. Here we construe causal network broadly to include any collection of (<phrase>domain-specific</phrase>) causal beliefs that can be represented as a set of nodes and a set of (<phrase>directed</phrase>) links between nodes. Nodes may represent objects, properties of or relations between objects, or events. Links may have different causal <phrase>semantics</phrase> depending on the <phrase>semantics</phrase> of the nodes. For instance, the network N 0 (Figure 1) might represent some aspects of a person's <phrase>knowledge</phrase> about several common diseases, their effects (symptoms), and causes (risky behaviors). Our <phrase>thesis</phrase> here is that attempts to characterize peo-ple's causal <phrase>knowledge</phrase> of a domain primarily in terms of such network structures (e.g., Gopnik & Glymour, 2002; Rehder, in press), while revealing in some important ways, miss a key level of abstraction: the laws that allow people to formulate meaningful causal network hypotheses, and thereby learn and reason about novel causal systems so effectively. For instance, in Figure 1, there appears to be a common <phrase>domain theory</phrase> underlying networks N 0 , N 1 , and N 2 , which distinguishes them from N 3 , but is not explicitly represented in any of them. We present a framework for representing such abstract causal <phrase>knowledge</phrase>, which we call causal <phrase>grammar</phrase>. The framework is surely incomplete and oversimplified; we view it as merely a first <phrase>pass</phrase> at a deep and hard problem. We also describe an <phrase>experimental</phrase> study of how people learn and use causal grammars, and briefly <phrase>sketch</phrase> a theory of learning based on <phrase>Bayesian inference</phrase>. The networks N 1 and N 2 differ from N 0 in the precise causal links or <phrase>disease</phrase> nodes 
A systematic comparison of flat and standard <phrase>cascade</phrase>-correlation using a <phrase>student</phrase>-<phrase>teacher</phrase> network approximation task <phrase>Cascade</phrase>-correlation (cascor) networks grow by recruiting <phrase>hidden units</phrase> to adjust their computational power to the task being learned. The standard cascor <phrase>algorithm</phrase> recruits each hidden unit on a new layer, creating deep networks. In contrast, the flat cascor variant adds all recruited <phrase>hidden units</phrase> on a <phrase>single</phrase> <phrase>hidden layer</phrase>. <phrase>Student</phrase>-<phrase>teacher</phrase> network approximation tasks were used to investigate the ability of flat and standard cascor networks to learn the <phrase>input-output</phrase> mapping of other, randomly initialized flat and standard cascor networks. For low-complexity approximation tasks, there was no significant performance difference between flat and standard <phrase>student</phrase> networks. Contrary to the common belief that standard cascor does not generalize well due to cascading weights creating deep networks, we found that both standard and flat cascor generalized well on problems of varying complexity. On <phrase>high</phrase>-complexity tasks, flat cascor networks had fewer connection weights and learned with less computational cost than standard networks did.
The brave new world of <phrase>e</phrase>-learning: a department's response to mandated change This <phrase>paper</phrase> offers a case <phrase>history</phrase> of one <phrase>university</phrase> <phrase>academic</phrase> department's experience with the <phrase>pressure</phrase> to create online <phrase>scholarship</phrase> opportunities for <phrase>professional</phrase> educators. As <phrase>tertiary education</phrase> transforms its course delivery to <phrase>Web-based</phrase> learning <phrase>management</phrase> platforms, instructors are challenged to transform career-<phrase>long</phrase> practice suddenly and without warning. The <phrase>department</phrase> in question focuses on <phrase>school</phrase> <phrase>leadership</phrase> and primarily serves mid-career professionals studying at the post-baccalaureate level. Practical and <phrase>political</phrase> challenges encountered by <phrase>faculty members</phrase> and the measures taken to tackle them are addressed. Institutional support initiatives are described, along with perspectives on the consequences of sudden <phrase>technology</phrase> migration to the quality of teaching, learning, and lifestyle. Using various <phrase>data</phrase> sources, faculty and <phrase>student</phrase> responses to institutional change are assessed, with implications for further policy and practice. Introduction In this article we examine the particular challenges of graduate-level <phrase>university</phrase> <phrase>scholarship</phrase> for <phrase>school</phrase> <phrase>leadership</phrase> in a rural, geographically remote <phrase>region</phrase> of <phrase>American</phrase> <phrase>Appalachia</phrase>. Relative to more populated areas, this <phrase>region</phrase> is poorly served with <phrase>technology</phrase> and transportation <phrase>infrastructure</phrase>. Yet it is confronted by similar <phrase>professional</phrase> pressures as more affluent, better served regions. Faced with sharp competition for students from other institutions and severe regional difficulties in travel, the <phrase>university</phrase> that serves as the setting for this experience, <phrase>Western Carolina University</phrase> (WCU), has <phrase>launched</phrase> strategies to make formal graduate <phrase>education</phrase> accessible and engaging to its <phrase>professional</phrase> constituents. WCU's <phrase>Department</phrase> of Educational <phrase>Leadership</phrase> and Foundations (<phrase>ELF</phrase>) deals with demands consistent with <phrase>state</phrase> and nationwide educational <phrase>leadership</phrase> issues, among them the recruitment of qualified personnel to administer <phrase>elementary</phrase> and <phrase>secondary schools</phrase> successfully. Responding to the <phrase>state</phrase>-level elimination of baccalaureate add-on certification for principals over 10 <phrase>years ago</phrase>, <phrase>ELF</phrase> was awarded a <phrase>Master</phrase> of <phrase>School</phrase> Administration (MSA) <phrase>degree</phrase> (a 42 hour program) and a <phrase>doctorate</phrase> in Educational <phrase>Leadership</phrase> (Ed.D., 60 to 66 hours) offered through the <phrase>Department</phrase>. Additionally, a post-Master's specialist certificate (Ed.S., 36 hours) is offered for students who seek a credential beyond the master's and the <phrase>school</phrase> superintendency <phrase>professional</phrase> license. Thus, we address questions of faculty and instructional development from the holistic perspective of institutional change, placing the particular challenges of online efficacy in the larger context of <phrase>teaching and learning</phrase> excellence. How do we improve <phrase>scholarship</phrase> for <phrase>deep understanding</phrase>? What implication does deep <phrase>scholarship</phrase> hold for <phrase>teaching and learning</phrase> in <phrase>general</phrase>? Once these principles are better understood institutionally, where do they point in terms of online, on-site and blended <phrase>pedagogy</phrase>? We expect that this inquiry 
Deformations of IC Structure in <phrase>Test</phrase> and Yield Learning This <phrase>paper</phrase> argues that the existing approaches to modeling and characterization of IC malfunctions are inadequate for <phrase>test</phrase> and yield learning of Deep Sub-<phrase>Micron</phrase> (<phrase>DSM</phrase>) <phrase>products</phrase>. Traditional notions of a spot defect and local and global process variations are analyzed and their shortcomings are exposed. A detailed <phrase>taxonomy</phrase> of process-induced deformations of <phrase>DSM</phrase> IC structures, enabling modeling and characterization of IC malfunctions, is proposed. The blueprint of a roadmap enabling such a characterization is suggested.
Review of "<phrase>Memory</phrase> as a <phrase>Programming</phrase> Concept in C and <phrase>C++</phrase> by Frantisek Franek". <phrase>Cambridge University Press</phrase>, 2004, (<phrase>paperback</phrase>), 0-521-52043-6 <phrase>Eclipse</phrase> is an <phrase>open-source</phrase> <phrase>IBM</phrase>-sponsored extensible <phrase>IDE</phrase> built using <phrase>Java</phrase>, JFace, and SWT (Standard <phrase>Widget Toolkit</phrase>). It offers a <phrase>comprehensive</phrase> <phrase>API</phrase> for developers to write <phrase>plug-ins</phrase>. This massive tome, one of the books in a series written by experts , reviewed by <phrase>Eclipse</phrase> insiders, and published by <phrase>Addison Wesley</phrase> <phrase>Professional</phrase>, just provides an in-depth description of the process involved in building commercial quality <phrase>plug-ins</phrase> for <phrase>Eclipse</phrase> and IBM's <phrase>WebSphere</phrase> Studio <phrase>Workbench</phrase>. As far as I know, this is the first authoritative, complete <phrase>book</phrase> out there on this topic. Perhaps it is still the best one now. It will fill the gap between common <phrase>Eclipse</phrase> development and <phrase>Eclipse</phrase> customization. The authors start the <phrase>book</phrase> with an introduction to the <phrase>Eclipse</phrase> Environment, then gives an outline about how to build a simple plug-in. Each chapter follows a similar <phrase>order</phrase>, and contains an overview at the very start, a summary at the end, and an in-between detailed description with a wealth of diagrams, screen shots, tips, <phrase>API</phrase> listings, and code snippets. It covers the most important topics in an easy to follow style. Along the way, you will get a <phrase>comprehensive</phrase> understanding of how to write <phrase>Eclipse</phrase> <phrase>plug-ins</phrase> using every facility that <phrase>Eclipse</phrase> provides. Granted, as a reviewer, I must admit that there still leave some flaws in it. For example, some of codes and screenshots are a little out of date. Even a minor of places seem have errors. In a few cases, the authors use deprecated <phrase>Java</phrase> <phrase>API</phrase>. Some codes could not run as written. Although the aforemen-tioned flaws indeed exist in the <phrase>book</phrase>, it is still a must read for <phrase>Eclipse</phrase> <phrase>plug-ins</phrase>' developers. The <phrase>book</phrase> is nicely up-to-date, and keeping up with recently released <phrase>Eclipse</phrase> version 3.0. It is definitely not an on-line documentation or a <phrase>cookbook</phrase> like many others. It is written for <phrase>Eclipse</phrase> developers, engineers, commercial customers, students , and researchers. To different people, it means different things. Anyone who wants to gain a <phrase>deep understanding</phrase> of <phrase>Eclipse</phrase> and <phrase>Eclipse</phrase> <phrase>plug-ins</phrase> will find it indispensable and save you a lot of effort. When you read the <phrase>book</phrase>, you may have a good grasp of the entire process of creating <phrase>Eclipse</phrase> <phrase>plug-ins</phrase>. However, if you are not an experienced <phrase>Java</phrase> developer, you should go elsewhere for some other <phrase>Java</phrase> books to renew your <phrase>knowledge</phrase>. After all, when you are building <phrase>plug-ins</phrase> on <phrase>Eclipse</phrase>, there is a lot to learn. 
Learning via Queries Query learning is one of the most important <phrase>active learning</phrase> models that have been studied in the <phrase>literature</phrase>. In this <phrase>thesis</phrase>, we study two types of queries, namely, edge-detecting queries and value-injecting queries, both of which are motivated by biological applications. Motivated by <phrase>PCR</phrase> experiments used in a <phrase>genome sequencing</phrase> problem, edge-detecting queries allow the learner to query whether a set of vertices induces an edge or not. We present <phrase>algorithms</phrase> that learn <phrase>general</phrase> <phrase>graphs</phrase> and uniform hypergraphs with queries linear in the number of edges. The <phrase>algorithms</phrase> are optimal in terms of their dependence on the number of edges. In both problems, we present <phrase>algorithms</phrase> in which queries can be made in a small number of rounds. We also give matching <phrase>upper</phrase> and <phrase>lower</phrase> bounds for the problem of learning almost uniform hypergraphs. Value-injection queries are motivated by experiments that use selective <phrase>gene</phrase> disruption and expression to identify <phrase>gene</phrase> regulatory networks. We propose a new <phrase>model</phrase> of exact learning of <phrase>acyclic</phrase> circuits with value-injection queries. We prove <phrase>lower</phrase> bounds and show that a large class of circuits including NC1 and AC0 are learnable in the new <phrase>model</phrase>. Acknowledgments First and foremost, I would like to express deep and sincere gratitude to my advisor, Dana Angluin. The dissertation would not be possible without her patient guidance and constant encouragement. Her modestness, extraordinary kindness, and truly scientific <phrase>spirit</phrase> make her a forever role <phrase>model</phrase> for me. Those wonderful twice-a-week talks with her have witnessed my growth as a <phrase>student</phrase>, a researcher and a <phrase>scholar</phrase>. I am deeply indebted to her. I gratefully thank my committee members for their invaluable advice. James Aspnes always inspires me with his incredible speed of thinking and terrific sense of humor. Discussions with him were greatly beneficial. Ravi Kannan's enlightening and knowledgeable lectures attracted me to the field of theoretical <phrase>computer science</phrase>. I thank Roni Khardon for using his precious time to read this <phrase>thesis</phrase> and give wonderful comments about its early drafts. My thanks also go to my <phrase>friends</phrase> and <phrase>fellow</phrase> students in the <phrase>department</phrase> for the wonderful time we share together. My former officemate, Michael Elkin, shared his rich <phrase>knowledge</phrase> on <phrase>graph theory</phrase> and gave much needed advice on <phrase>research</phrase>. Numerous rememberable talks with Zheng <phrase>Ma</phrase>, my <phrase>fellow</phrase> <phrase>student</phrase>, <phrase>led</phrase> me to a better understanding of my journey as a <phrase>PhD</phrase> <phrase>student</phrase>. Finally, I can not thank my <phrase>family</phrase> enough for their unconditioned <phrase>love</phrase> 
<phrase>Neural network</phrase>-based <phrase>symbol</phrase> recognition using a few <phrase>labeled samples</phrase> The recognition of pen-based visual patterns such as sketched symbols is amenable to supervised <phrase>machine learning</phrase> models such as <phrase>neural networks</phrase>. However, a sizable, <phrase>labeled training</phrase> corpus is often required to learn the <phrase>high</phrase> variations of freehand sketches. To circumvent the costs associated with creating a large training corpus, improve the <phrase>recognition accuracy</phrase> with only a limited amount of <phrase>training samples</phrase> and accelerate the development of <phrase>sketch</phrase> recognition system for novel <phrase>sketch</phrase> domains, we present a <phrase>neural network</phrase> training protocol that consists of three steps. First, a large pool of unlabeled, synthetic samples are generated from a small set of existing, <phrase>labeled training</phrase> samples. Then, a <phrase>Deep Belief</phrase> Network (DBN) is <phrase>pre-trained</phrase> with those synthetic, unlabeled samples. Finally, the <phrase>pre-trained</phrase> DBN is <phrase>fine-tuned</phrase> using the limited amount of <phrase>labeled samples</phrase> for classification. The training protocol is evaluated against supervised baseline approaches such as the nearest neighbor classifier and the <phrase>neural network</phrase> classifier. The benchmark <phrase>data</phrase> sets used are partitioned such that there are only a few <phrase>labeled samples</phrase> for training, yet a large number of labeled <phrase>test</phrase> cases featuring rich variations. <phrase>Results</phrase> suggest that our training protocol leads to a significant error reduction compared to the baseline approaches. <phrase>Sketch</phrase> understanding [1,2] aims to enable the <phrase>computers</phrase> to interpret man-made, freehand sketches and extract the intended <phrase>information</phrase> underlying the input strokes. Fig. 1 shows two exemplary sketches depicting two <phrase>engineering</phrase> systems and the corresponding <phrase>engineering</phrase> <phrase>model</phrase>. If successful, <phrase>sketch</phrase> understanding could provide a natural <phrase>human</phrase>-computer interface for scenarios in which physical, pen-and-<phrase>paper</phrase> sketches have been routinely used, such as the early ideation process or the classroom instruction. Moreover, <phrase>sketch</phrase> understanding could automate the <phrase>mining</phrase>, <phrase>organization</phrase>, search and critique of the <phrase>information</phrase> embedded in freehand sketches, potentially resulting in a myriad of <phrase>intelligent agents</phrase>, such as a web <phrase>spider</phrase> that crawls through the drawings in online textbooks and lecture notes to learn the <phrase>design</phrase> rules of electrical systems, an archiver that indexes <phrase>brainstorming</phrase> sketches for later retrieval and reuse, and a computer grader for the <phrase>free</phrase>-body diagrams that students draw in their <phrase>statics</phrase> homework. One of the core problems in <phrase>sketch</phrase> understanding is to devise a <phrase>symbol</phrase> recognizer to compute a categorical <phrase>label</phrase> for each segment of the input <phrase>sketch</phrase>. Used in conjunction with a <phrase>sketch</phrase> parser that divides the input <phrase>sketch</phrase> into segments and possibly a post-processor that ensures the consistency of the recognition, an interpretation of the input <phrase>sketch</phrase> can 
Accuracy Evaluation of <phrase>Deep Belief</phrase> Networks with <phrase>Fixed-point Arithmetic</phrase> <phrase>Deep Belief</phrase> Networks (DBNs) are <phrase>state</phrase>-of-<phrase>art</phrase> <phrase>Machine Learning</phrase> techniques and one of the most important <phrase>unsupervised learning</phrase> <phrase>algorithms</phrase>. Training DBNs is computationally intensive which naturally leads to investigate <phrase>FPGA</phrase> <phrase>acceleration</phrase>. <phrase>Fixed-point arithmetic</phrase> can be used when implementing DBNs in <phrase>FPGAs</phrase> to reduce execution time, but it is not clear the implications for accuracy. Previous studies have focused only on <phrase>accelerators</phrase> using some fixed <phrase>bit</phrase>-widths. A contribution of this <phrase>paper</phrase> is to demonstrate the <phrase>bit</phrase>-width effect on various configurations of DBNs in a <phrase>comprehensive</phrase> way by <phrase>experimental</phrase> evaluation. Explicit performance changing points are found using various <phrase>bit</phrase>-widths. The impact of <phrase>sigmoid function</phrase> approximation, required part of DBNs, is evaluated. A <phrase>solution</phrase> of mixed <phrase>bit</phrase>-widths DBN is proposed, fitting the <phrase>bit</phrase>-widths of <phrase>FPGA</phrase> primitives and gaining similar performance to the <phrase>software</phrase> implementation. Our <phrase>results</phrase> provide a guide to inform the <phrase>design</phrase> choices on <phrase>bit</phrase>-widths when implementing DBNs in <phrase>FPGAs</phrase> documenting clearly the <phrase>trade</phrase>-off in accuracy.
<phrase>Dimensionality reduction</phrase> strategy based on <phrase>auto-encoder</phrase> <phrase>Auto-encoder</phrase> is a tricky three-layered <phrase>neural network</phrase>, which constructs the "<phrase>building block</phrase>" of <phrase>deep learning</phrase> that has been demonstrated to achieve good performance in various domains. In this <phrase>paper</phrase>, we focus on auto-encoder's <phrase>dimensionality reduction</phrase> ability, and try to investigate whether <phrase>auto-encoder</phrase> has some kind of good <phrase>property</phrase> that can accumulate when being stacked, thus contribute to the success of <phrase>deep learning</phrase>. We start from <phrase>auto-encoder</phrase>, trying to investigate its ability to reduce the dimensionality and understand the difference between <phrase>auto-encoder</phrase> and <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>dimensionality reduction</phrase> methods. Experiments are conducted on both the synthesized <phrase>data</phrase> for an intuitive understanding and some real datasets, e.g. MNIST dataset and <phrase>Olivetti</phrase> face dataset. The <phrase>results</phrase> show that <phrase>auto-encoder</phrase> can indeed learn something different from other methods.
Instructional, Curricular, and Technological Supports for Inquiry in <phrase>Science</phrase> Classrooms Inquiry is a central component of <phrase>science</phrase> learning (Lunetta, 1997; Roth, 1995). New approaches to <phrase>science</phrase> instruction feature inquiry as essential for <phrase>student</phrase> learning. The assumption is that students need opportunities to find solutions to real problems by asking and refining questions, designing and <phrase>conducting</phrase> investigations, gathering and analyzing <phrase>information</phrase> and <phrase>data</phrase>, making interpretations, <phrase>drawing</phrase> conclusions, and reporting findings. Congruent with argues that "there needs to be a de-emphasis on didactic instruction focusing on memorizing decontextualized scientific facts, and there needs to be new emphasis placed on <phrase>inquiry-based learning</phrase> focusing on having students develop a <phrase>deep understanding</phrase> of <phrase>science</phrase> embedded in the everyday world. " Evidence indicates that students can attain <phrase>deeper understanding</phrase> of <phrase>science</phrase> content and processes when they engage in inquiry (e.g. that inquiry places many <phrase>cognitive</phrase> demands on learners so that they require considerable support to be successful. Students need support to become knowledgeable about content, skilled in using inquiry strategies, proficient at using technological tools, productive in collaborating with others, competent in exercising self-regulation, and motivated to sustain careful and thoughtful work over a <phrase>period</phrase> of time. Describing problems students encounter as they engage in inquiry and finding ways to ameliorate those problems has received considerable attention recently (Hmelo we describe inquiry in more detail, discuss ways to aid students via instructional, <phrase>curriculum</phrase>, and 1. 2 The authors would like to thank Ann <phrase>Rivet</phrase> from the <phrase>University</phrase> of <phrase>Michigan</phrase> for her helpful editorial comments.
PLOW: A Collaborative Task Learning Agent To be effective, an agent that collaborates with humans needs to be able to learn new tasks from humans they work with. This <phrase>paper</phrase> describes a system that learns <phrase>executable</phrase> task models from a <phrase>single</phrase> <phrase>collaborative learning</phrase> session consisting of demonstration, explanation and dialogue. To accomplish this, the system integrates a <phrase>range</phrase> of <phrase>AI</phrase> technologies: deep <phrase>natural language understanding</phrase>, <phrase>knowledge representation and reasoning</phrase>, dialogue systems, plan-ning/agent-based systems and <phrase>machine learning</phrase>. A formal evaluation shows the approach has great promise.
Situated <phrase>design</phrase> for creative, reflective, collaborative, <phrase>technology</phrase>-mediated learning STEM subjects are typically seen as boring, geeky, difficult to learn and with low relevance to <phrase>real life</phrase>. To counter this opinion, we aim to foster engagement with and curiosity about STEM subjects, through an approach to learning that facilitates the <phrase>construction</phrase> of understanding of key, threshold concepts (TCs). To achieve this, we engender <phrase>creativity</phrase> by using performance as a means of expression. We demonstrate how the process of collaboratively crafting a <phrase>video</phrase> to explain a TC students have been introduced to helps them to break down the concept, and through reflection on each piece of <phrase>knowledge</phrase> about it, build understanding about its different aspects, and further develop their <phrase>knowledge</phrase>. We aim through this approach to encourage students to work together to discover, explore, engage in lateral, visual thinking, and therefore develop deep, shared understanding of TCs in STEM subjects.
Two-Layer <phrase>Multiple Kernel</phrase> Learning <phrase>Multiple Kernel</phrase> Learning (MKL) aims to learn kernel machines for solving a real <phrase>machine learning</phrase> problem (e.g. classification) by exploring the combinations of multiple kernels. The traditional MKL approach is in <phrase>general</phrase> " shallow " in the sense that the <phrase>target</phrase> kernel is simply a linear (or convex) combination of some base kernels. In this <phrase>paper</phrase>, we investigate a framework of <phrase>Multi-Layer</phrase> <phrase>Multiple Kernel</phrase> Learning (MLMKL) that aims to learn " deep " kernel machines by exploring the combinations of multiple kernels in a <phrase>multi-layer</phrase> structure, which goes beyond the conventional MKL approach. Through a multiple layer mapping , the proposed MLMKL framework offers higher flexibility than the regular MKL for finding the optimal kernel for applications. As the first attempt to this new MKL framework, we present a Two-Layer <phrase>Multiple Kernel</phrase> Learning (2LMKL) method together with two efficient <phrase>algorithms</phrase> for <phrase>classification tasks</phrase>. We analyze their generalization performances and have conducted an extensive set of experiments over 16 <phrase>benchmark datasets</phrase>, in which encouraging <phrase>results</phrase> showed that our method performed better than the conventional MKL methods.
<phrase>Hallucinations</phrase> in <phrase>Charles Bonnet</phrase> Syndrome Induced by <phrase>Homeostasis</phrase>: a Deep <phrase>Boltzmann</phrase> Machine <phrase>Model</phrase> The <phrase>Charles Bonnet</phrase> Syndrome (<phrase>CBS</phrase>) is characterized by complex vivid visual <phrase>hallucinations</phrase> in people with, primarily, eye diseases and no other neurological <phrase>pathology</phrase>. We present a Deep <phrase>Boltzmann</phrase> Machine <phrase>model</phrase> of <phrase>CBS</phrase>, exploring two core hypotheses: First, that the <phrase>visual cortex</phrase> learns a generative or predic-tive <phrase>model</phrase> of sensory input, thus explaining its capability to generate internal imagery. And second, that <phrase>homeostatic</phrase> mechanisms stabilize <phrase>neuronal</phrase> activity levels, leading to <phrase>hallucinations</phrase> being formed when input is lacking. We reproduce a <phrase>variety</phrase> of qualitative findings in <phrase>CBS</phrase>. We also introduce a modification to the <phrase>DBM</phrase> that allows us to <phrase>model</phrase> a possible role of <phrase>acetylcholine</phrase> in <phrase>CBS</phrase> as mediating the balance of <phrase>feed-forward</phrase> and <phrase>feedback</phrase> processing. Our <phrase>model</phrase> might provide new insights into <phrase>CBS</phrase> and also demonstrates that generative frameworks are promising as hypothetical models of <phrase>cortical</phrase> learning and <phrase>perception</phrase>.
Combining Similarity and Distribution Features to Match Attributes The Web contains much useful semistructued <phrase>information</phrase> which can be organized into web objects, and many of them are commercially valuable. The inner structures of these web objects are highly heterogeneous that web objects from different <phrase>web sites</phrase> <phrase>cover</phrase> different subsets of useful attributes. The complete set of attributes can be mined from <phrase>web pages</phrase> through attribute extraction <phrase>algorithms</phrase>. However, to construct <phrase>high</phrase> quality web object schema, some mined attributes should be merged since they are synonyms for the same concepts. Our <phrase>empirical study</phrase> shows that features used by traditional schema matching and <phrase>deep web</phrase> integration methods are usually <phrase>domain specific</phrase>, so they are not applicable to match attributes extracted from the Web. To overcome this problem, this <phrase>paper</phrase> proposes new features to depict attribute distribution characteristics and uses <phrase>machine learning</phrase> techniques to combine attribute distribution characteristics with attribute similarity features. We empirically compare the <phrase>proposed method</phrase> with <phrase>existing methods</phrase> use other features, and the <phrase>results</phrase> show the effectiveness of our method.
Enhanced <phrase>Gradient</phrase> and Adaptive Learning Rate for Training <phrase>Restricted Boltzmann Machines</phrase> <phrase>Boltzmann</phrase> machines are often used as <phrase>building blocks</phrase> in greedy learning of deep networks. However, training even a simplified <phrase>model</phrase>, known as <phrase>restricted Boltzmann machine</phrase> (RBM), can be extremely laborious: Traditional <phrase>learning algorithms</phrase> often converge only with the right choice of the learning rate scheduling and the scale of the initial weights. They are also sensitive to specific <phrase>data</phrase> representation: An equivalent RBM can be obtained by flipping some <phrase>bits</phrase> and changing the weights and biases accordingly, but traditional learning rules are not invariant to such transformations. Without careful tuning of these training settings, traditional <phrase>algorithms</phrase> can easily get stuck at plateaus or even diverge. In this work, we present an enhanced <phrase>gradient</phrase> which is derived such that it is invariant to <phrase>bit</phrase>-flipping transformations. We also propose a way to automatically adjust the learning rate by maximizing a local likelihood estimate. Our experiments confirm that the proposed improvements yield more stable training of RBMs.
Improved Rgb-d-t Based <phrase>Face Recognition</phrase> Improved Rgb-d-t Based <phrase>Face Recognition</phrase> <phrase>General</phrase> rights <phrase>Copyright</phrase> and <phrase>moral rights</phrase> for the publications made accessible in the <phrase>public</phrase> portal are retained by the authors and/or other <phrase>copyright</phrase> owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights. ? Users may download and print one copy of any publication from the <phrase>public</phrase> portal for the purpose of <phrase>private</phrase> study or <phrase>research</phrase>. ? You may not further distribute the material or use it for any profit-making activity or commercial gain ? You may freely distribute the URL identifying the publication in the <phrase>public</phrase> portal ? Take down policy If you believe that this document breaches <phrase>copyright</phrase> please contact us at vbn@aub.aau.dk providing details, and we will remove access to the work immediately and investigate your claim. Abstract: Reliable <phrase>facial recognition</phrase> systems are of crucial importance in various applications from <phrase>entertainment</phrase> to <phrase>security</phrase>. Thanks to the <phrase>deep-learning</phrase> concepts introduced in the field, a significant improvement in the performance of the unimodal <phrase>facial recognition</phrase> systems has been observed in the recent years. At the same time a multimodal <phrase>facial recognition</phrase> is a promising approach. This <phrase>paper</phrase> combines the latest successes in both directions by applying <phrase>deep learning</phrase> <phrase>Convolutional Neural Networks</phrase> (<phrase>CNN</phrase>) to the multimodal RGB-D-T based <phrase>facial recognition</phrase> problem outperforming previously <phrase>published results</phrase>. Furthermore, a late <phrase>fusion</phrase> of the <phrase>CNN</phrase>-based recognition block with various <phrase>hand-crafted</phrase> features (LBP, HOG, HAAR, HOGOM) is introduced, demonstrating even better <phrase>recognition performance</phrase> on a benchmark RGB-D-T <phrase>database</phrase>. The obtained <phrase>results</phrase> in this <phrase>paper</phrase> show that the <phrase>classical</phrase> engineered features and <phrase>CNN</phrase>-based features can <phrase>complement</phrase> each other for recognition purposes.
<phrase>Learning styles</phrase> and <phrase>approaches to learning</phrase> among <phrase>medical</phrase> undergraduates and postgraduates BACKGROUND The challenge of imparting a large amount of <phrase>knowledge</phrase> within a limited time <phrase>period</phrase> in a way it is retained, remembered and effectively interpreted by a <phrase>student</phrase> is considerable. This has resulted in crucial changes in the field of <phrase>medical</phrase> <phrase>education</phrase>, with a shift from didactic <phrase>teacher</phrase> centered and subject based teaching to the use of interactive, problem based, <phrase>student</phrase> <phrase>centered learning</phrase>. This study tested the <phrase>hypothesis</phrase> that <phrase>learning styles</phrase> (visual, auditory, read/write and kinesthetic) and <phrase>approaches to learning</phrase> (deep, strategic and superficial) differ among first and final year <phrase>undergraduate</phrase> <phrase>medical</phrase> students, and postgraduates <phrase>medical</phrase> trainees. METHODS We used self administered VARK and ASSIST questionnaires to assess the differences in <phrase>learning styles</phrase> and <phrase>approaches to learning</phrase> among <phrase>medical</phrase> undergraduates of the <phrase>University</phrase> of <phrase>Colombo</phrase> and postgraduate trainees of the Postgraduate Institute of <phrase>Medicine</phrase>, <phrase>Colombo</phrase>. <phrase>RESULTS</phrase> A total of 147 participated: 73 (49.7%) first year students, 40 (27.2%) final year students and 34(23.1%) postgraduate students. The majority (69.9%) of first year students had multimodal <phrase>learning styles</phrase>. Among final year students, the majority (67.5%) had multimodal <phrase>learning styles</phrase>, and among postgraduates, the majority were unimodal (52.9%) learners.Among all three groups, the predominant approach to learning was strategic. Postgraduates had significant higher mean scores for deep and strategic approaches than first years or final years (p < 0.05). Mean scores for the superficial approach did not differ significantly between groups. CONCLUSIONS The learning approaches suggest a positive shift towards deep and strategic learning in postgraduate students. However a similar difference was not observed in <phrase>undergraduate</phrase> students from first year to final year, suggesting that their <phrase>curriculum</phrase> may not have influenced learning methodology over a five year <phrase>period</phrase>.
Minimally Supervised Domain-Adaptive Parse Reranking for <phrase>Relation Extraction</phrase> The <phrase>paper</phrase> demonstrates how the generic parser of a minimally supervised <phrase>information extraction</phrase> framework can be adapted to a given task and domain for <phrase>relation extraction</phrase> (RE). For the experiments a generic <phrase>deep-linguistic</phrase> parser was employed that works with a largely <phrase>hand-crafted</phrase> <phrase>head-driven phrase structure grammar</phrase> (HPSG) for <phrase>English</phrase>. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank. It will be shown how the estimated confidence of RE rules learned from the n best parses can be exploited for parse reranking. The acquired rerank-ing <phrase>model</phrase> improves the performance of RE in both training and <phrase>test</phrase> phases with the new first parses. The obtained significant boost of recall does not come from an overall gain in <phrase>parsing</phrase> performance but from an application-driven selection of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic <phrase>parsing</phrase> accuracy actually decreases. The novel method for task-specific parse reranking does not require any annotated <phrase>data</phrase> beyond the <phrase>semantic</phrase> <phrase>seed</phrase>, which is needed anyway for the RE task.
<phrase>Phone Recognition</phrase> with the Mean-<phrase>Covariance</phrase> <phrase>Restricted Boltzmann Machine</phrase> Straightforward application of <phrase>Deep Belief</phrase> Nets (DBNs) to <phrase>acoustic</phrase> modeling produces a rich distributed representation of speech <phrase>data</phrase> that is useful for recognition and yields impressive <phrase>results</phrase> on the <phrase>speaker</phrase>-<phrase>independent</phrase> TIMIT <phrase>phone recognition</phrase> task. However, the first-layer Gaussian-<phrase>Bernoulli</phrase> <phrase>Restricted Boltzmann Machine</phrase> (GRBM) has an important limitation, shared with mixtures of diagonal-<phrase>covariance</phrase> Gaussians: GRBMs treat different components of the <phrase>acoustic</phrase> input <phrase>vector</phrase> as conditionally <phrase>independent</phrase> given the hidden <phrase>state</phrase>. The mean-<phrase>covariance</phrase> <phrase>restricted Boltzmann machine</phrase> (mcRBM), first introduced for modeling <phrase>natural images</phrase> , is a much more representationally efficient and powerful way of modeling the <phrase>covariance</phrase> structure of speech <phrase>data</phrase>. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the <phrase>acoustic</phrase> space. In this work, we use the mcRBM to learn features of speech <phrase>data</phrase> that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone <phrase>error rate</phrase> of 20.5%, which is <phrase>superior</phrase> to all <phrase>published results</phrase> on <phrase>speaker</phrase>-<phrase>independent</phrase> TIMIT to date.
Text Compression Via <phrase>Alphabet</phrase> Re-Representation This <phrase>thesis</phrase> introduces the notion of <phrase>alphabet</phrase> re-representation in the context of text compression. We consider re-representing the <phrase>alphabet</phrase> so that a representation of a character reeects its properties as a predictor of future text. This enables us to use an <phrase>estimator</phrase> from a restricted class to map contexts to predictions of upcoming characters. We describe an <phrase>algorithm</phrase> that uses this idea in conjunction with <phrase>neural networks</phrase>. The performance of our implementation is compared to other compression methods, such as <phrase>UNIX</phrase> compress, <phrase>gzip</phrase>, PPMC, and an <phrase>alternative</phrase> <phrase>neural network</phrase> approach. <phrase>ii</phrase> Acknowledgements First of all, I would like to thank my parents Ivan and Anna for bringing me up in the <phrase>spirit</phrase> of <phrase>science</phrase> admiration and constant pursuit of <phrase>knowledge</phrase>. They, along with a number of teachers throughout my <phrase>education</phrase>, are primarily responsible for the interest and curiosity I have developed towards <phrase>science</phrase>. This <phrase>thesis</phrase> is dedicated to them, and to my brother Ivailo, all of whom made this work possible through the support and encouragement they gave me along the years of my <phrase>education</phrase>. I also ooer my most sincere gratitude to my advisor Jee Vitter who taught me all I know about <phrase>research</phrase> and the way it should be done. I consider myself very fortunate to be given an opportunity to work with him. Through this project Jee has shown me how to attack a problem from diierent angles, how to approach the two sides of a <phrase>coin</phrase> and nd a third if I have to, but not give up... He has been truly an excellent advisor, and the invaluable experience I gained will be an indispensable part of my career as a researcher in the future. Many thanks go to Phil <phrase>Long</phrase> whose great ideas and <phrase>deep understanding</phrase> of learning theory have beneeted this project immensely. He has been of enormous help to me, and without his <phrase>knowledge</phrase> and insight much of this work would not have been completed. Both he and Jee were always there when I needed technical advice. I also wish to thank John Reif and Tassos Markas for serving on my committee and for their comments on various aspects of the project|especially Tassos' suggestions on <phrase>results</phrase> visualization. I have learned a great deal about compression from the class Tassos taught, and for that, I will always be indebted to him. Finally, I thank many of my patient <phrase>fellow</phrase> students for listening when I 
Towards <phrase>Narrative</phrase>-<phrase>Centered Learning</phrase> Environments Because <phrase>narrative</phrase> plays such a central role in <phrase>cognition</phrase> and <phrase>culture</phrase>, <phrase>narrative</phrase>-centered curric-ula have been the subject of increasing attention. By taking advantage of the inherent structure of <phrase>narrative</phrase>, <phrase>narrative</phrase>-<phrase>centered learning</phrase> environments could provide engaging worlds in which students are actively involved in motivating story-building activities. The fundamental <phrase>hypothesis</phrase> of this <phrase>research</phrase> program is that by enabling learners to be co-<phrase>constructors</phrase> of narratives, <phrase>narrative</phrase>-<phrase>centered learning</phrase> environments can promote the deep, connection-building meaning-making activities that define constructivist learning. We outline <phrase>tile</phrase> features of <phrase>narrative</phrase> that support eonstruc-tivist learning, explore the key issues in introducing <phrase>narrative</phrase> into <phrase>learning environments</phrase>, consider how these environments can support one particular <phrase>subject matter</phrase>, <phrase>literacy</phrase> <phrase>education</phrase>, and <phrase>sketch</phrase> the <phrase>research</phrase> agenda required to make <phrase>narrative</phrase>-<phrase>centered learning</phrase> environments a <phrase>reality</phrase>.
Sparse <phrase>Feature Learning</phrase> for <phrase>Deep Belief</phrase> Networks <phrase>Unsupervised learning</phrase> <phrase>algorithms</phrase> aim to discover the structure hidden in the <phrase>data</phrase>, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low <phrase>dimension</phrase>, sparsity, etc). Others are based on approximating <phrase>density</phrase> by stochastically reconstructing the input from the representation. We describe a novel and efficient <phrase>algorithm</phrase> to learn sparse representations , and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a <phrase>Restricted Boltzmann Machine</phrase>. We propose a simple criterion to compare and select different unsupervised machines based on the <phrase>trade</phrase>-off between the <phrase>reconstruction</phrase> error and the <phrase>information</phrase> content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural <phrase>image patches</phrase>. We show that by stacking <phrase>multiple levels</phrase> of such machines and by training sequentially, <phrase>high</phrase>-<phrase>order</phrase> dependencies between the input observed variables can be captured.
<phrase>Binary</phrase> coding of speech spectrograms using a deep <phrase>auto-encoder</phrase> This <phrase>paper</phrase> reports our recent exploration of the <phrase>layer-by-layer</phrase> learning strategy for training a <phrase>multi-layer</phrase> <phrase>generative model</phrase> of patches of speech spectrograms. The top layer of the <phrase>generative model</phrase> learns <phrase>binary</phrase> codes that can be used for efficient compression of speech and could also be used for scalable <phrase>speech recognition</phrase> or rapid speech content retrieval. Each layer of the <phrase>generative model</phrase> is <phrase>fully connected</phrase> to the layer below and the weights on these connections are <phrase>pre-trained</phrase> efficiently by using the contrastive divergence approximation to the <phrase>log likelihood</phrase> <phrase>gradient</phrase>. After <phrase>layer-by-layer</phrase> <phrase>pre-training</phrase> we " unroll " the <phrase>generative model</phrase> to form a deep <phrase>auto-encoder</phrase>, whose parameters are then <phrase>fine-tuned</phrase> using back-propagation. To reconstruct the full-length speech <phrase>spectrogram</phrase>, individual <phrase>spectrogram</phrase> segments predicted by their respective <phrase>binary</phrase> codes are combined using an overlap-and-add method. <phrase>Experimental</phrase> <phrase>results</phrase> on speech <phrase>spectrogram</phrase> coding demonstrate that the <phrase>binary</phrase> codes produce a log-spectral <phrase>distortion</phrase> that is approximately 2 dB <phrase>lower</phrase> than a sub-<phrase>band</phrase> <phrase>vector quantization</phrase> technique over the entire <phrase>frequency</phrase> <phrase>range</phrase> of wide-<phrase>band</phrase> speech.
The Nested <phrase>Chinese</phrase> <phrase>Restaurant</phrase> Process and Hierarchical Topic Models We present the nested <phrase>Chinese</phrase> <phrase>restaurant</phrase> process (nCRP), a <phrase>stochastic process</phrase> which assigns <phrase>probability distributions</phrase> to infinitely-deep, infinitely-branching <phrase>trees</phrase>. We show how this <phrase>stochastic process</phrase> can be used as a prior distribution in a nonparametric <phrase>Bayesian</phrase> <phrase>model</phrase> of document collections. Specifically, we present an application to <phrase>information retrieval</phrase> in which documents are modeled as paths down a random <phrase>tree</phrase>, and the <phrase>preferential attachment</phrase> dynamics of the nCRP leads to clustering of documents according to sharing of topics at <phrase>multiple levels</phrase> of abstraction. Given a corpus of documents, a posterior inference <phrase>algorithm</phrase> finds an approximation to a posterior distribution over <phrase>trees</phrase>, topics and allocations of words to levels of the <phrase>tree</phrase>. We demonstrate this <phrase>algorithm</phrase> on several collections of scientific abstracts. This <phrase>model</phrase> exemplifies a recent trend in statistical machine learningthe use of nonparametric <phrase>Bayesian</phrase> methods to infer <phrase>distributions</phrase> on flexible <phrase>data structures</phrase>.
Does <phrase>computational neuroscience</phrase> need new <phrase>synaptic</phrase> learning paradigms? <phrase>Computational neuroscience</phrase> is dominated by a few paradigmatic models, but it remains an open question whether the existing modelling frameworks are sufficient to explain observed behavioural phenomena in terms of neural implementation. We take learning and <phrase>synaptic plasticity</phrase> as an example and point to open questions, such as one-<phrase>shot</phrase> learning and acquiring <phrase>internal representations</phrase> of the world for flexible planning. Introduction Successful paradigms inspire the thinking of researchers and guide scientific <phrase>research</phrase>, yet their success may block <phrase>independent</phrase> thinking and hinder <phrase>scientific progress</phrase> [1]. Influential learning paradigms in computational neurosci-ence such as the Hopfield <phrase>model</phrase> of <phrase>associative memory</phrase> [2 ], the BienenstockCooperMunro <phrase>model</phrase> for <phrase>receptive field</phrase> development [3 ], or temporal-difference learning for reward-based <phrase>action</phrase> learning [4 ] are of that kind. The question arises whether these and related paradigms in <phrase>machine learning</phrase> will be sufficient to account for the <phrase>variety</phrase> of learning behaviour observed in <phrase>nature</phrase>. In classic approaches to <phrase>machine learning</phrase> and <phrase>artificial neural networks</phrase>, learning from <phrase>data</phrase> is formalized in three different paradigms: supervised, unsupervised and <phrase>reinforcement learning</phrase> [58]. In <phrase>supervised learning</phrase>, each sample <phrase>data</phrase> point (e.g. a <phrase>pixel</phrase> image or measurements for multiple sensors) comes with a <phrase>label</phrase> such as 'this image is a <phrase>cat</phrase>', 'this image is a <phrase>dog</phrase>' (classification task) or for this configuration of sensory <phrase>data</phrase> the correct output is 5.8 (<phrase>regression</phrase> task). The objective of <phrase>supervised learning</phrase> is to optimize parameters of a machine or <phrase>mathematical</phrase> <phrase>function</phrase> that takes a <phrase>data</phrase> point as input and predicts the output, that is, that performs a correct classification or prediction. <phrase>Machine learning</phrase> has developed powerful models and methods, such as <phrase>support vector machines</phrase> [9], Gaussian Processes [10], or <phrase>stochastic gradient descent</phrase> in <phrase>deep neural networks</phrase> [11] that allow to minimize the classification or <phrase>regression</phrase> error. In contrast with the above, in <phrase>unsupervised learning</phrase> we just have multiple sample <phrase>data</phrase> points (<phrase>pixel</phrase> images or <phrase>sensor</phrase> readings), but no notion of correct or incorrect classification. The typical task of such <phrase>machine learning</phrase> <phrase>algorithms</phrase> consists of finding a representation of the <phrase>data</phrase> that would serve as a useful <phrase>starting point</phrase> for further processing. Typical objective functions include compression of the <phrase>data</phrase> into a <phrase>low-dimensional</phrase> space while maximizing the <phrase>variance</phrase> or <phrase>independence</phrase> of the <phrase>data</phrase> under some normali-zation constraints. The fields of <phrase>signal processing</phrase> and <phrase>machine learning</phrase> have developed <phrase>algorithms</phrase> such as <phrase>principal component analysis</phrase> (<phrase>PCA</phrase>) [5], projection pursuit [12], <phrase>independent component analysis</phrase> (ICA) [13,14] and <phrase>sparse coding</phrase> [15], 
<phrase>Construction</phrase> of the Limit Concept with a Computer <phrase>Algebra</phrase> System Students encounter many <phrase>cognitive</phrase> difficulties with limit ideas: sequences never end; functions do not attain their limits; series do not produce a final answer. Limit is further both a process and an object and students usually focus on the process. Studies investigating these difficulties are noted before presenting some <phrase>results</phrase> from a new study that examines the limit conceptions of students learning <phrase>calculus</phrase> concepts with the aid of a computer <phrase>algebra</phrase> system. Differences with <phrase>traditional approaches</phrase> emerge in that process problems are suppressed and the limit as an object appears clearer but this brings its own problems. Limit is a deep notion and each approach highlights and suppresses different facets of the concept. Conceptual difficulties with limits The limit concept is known to cause difficulty in learning. A number of <phrase>research</phrase> studies reveal that students often conceive of the notion of xa lim f (x) or n lim a n not as a static concept but as a dynamic process of 'getting close to' a fixed value, often with the implication of 'never reaching' the limit (see summaries in Cornu (1991) and Tall (1992)). Gray & Tall (1993) considered a wide <phrase>variety</phrase> of instances where a <phrase>symbol</phrase> can ambiguously represent either a process or a concept. They call this a procept. For instance 3+2 might evoke a process of addition, perhaps by counting on two, or the concept of sum. The symbols xa lim f (x) and n lim a n both represent either the process of getting close to a specific value, or the value of the limit itself. The limit is therefore an example of a procept. But unlike the procepts of <phrase>elementary</phrase> <phrase>mathematics</phrase>, which have a built-in <phrase>algorithm</phrase> to calculate the specific value of the concept, the limit value does not have a specific <phrase>universal</phrase> <phrase>algorithm</phrase> that works in all cases and, in some cases, such as lim n 1 / k 2 k =1 n , there may be no simple <phrase>algorithm</phrase>. (In this instance, the theory of <phrase>residues</phrase> in complex integration or a <phrase>sequence</phrase> of key strokes on a computer <phrase>algebra</phrase> system shows the limit to be 2 /6.) The circuitous routes by which limits are calculated in the early stages of the theory exacerbate the difficulties students have with the concept. As Cornu (1981) observed: " <phrase>mathematics</phrase> no longer reduces to calculations and simple <phrase>algebraic</phrase> properties; <phrase>infinity</phrase> intervenes and it is shrouded 
<phrase>Layer-wise</phrase> analysis of deep networks with Gaussian kernels Deep networks can potentially express a learning problem more efficiently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their <phrase>nice</phrase> representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds <phrase>higher-level</phrase> abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers.
<phrase>Bootstrapping</phrase> from <phrase>Game Tree</phrase> Search In this <phrase>paper</phrase> we introduce a new <phrase>algorithm</phrase> for updating the parameters of a heuris-tic evaluation <phrase>function</phrase>, by updating the <phrase>heuristic</phrase> towards the values computed by an alpha-beta search. Our <phrase>algorithm</phrase> differs from previous <phrase>approaches to learning</phrase> from search, such as Samuel's checkers player and the <phrase>TD</phrase>-<phrase>Leaf</phrase> <phrase>algorithm</phrase>, in two key ways. First, we update all nodes in the search <phrase>tree</phrase>, rather than a <phrase>single</phrase> node. Second, we use the outcome of a deep search, instead of the outcome of a subsequent search, as the training signal for the evaluation <phrase>function</phrase>. We implemented our <phrase>algorithm</phrase> in a <phrase>chess</phrase> program Meep, using a linear <phrase>heuristic</phrase> <phrase>function</phrase>. After initialising its weight <phrase>vector</phrase> to small random values, Meep was able to learn <phrase>high</phrase> quality weights from self-<phrase>play</phrase> alone. When tested online against <phrase>human</phrase> opponents , Meep played at a <phrase>master</phrase> level, the best performance of any <phrase>chess</phrase> program with a <phrase>heuristic</phrase> learned entirely from self-<phrase>play</phrase>.
A modeling language's <phrase>evolution</phrase> driven by tight interaction between <phrase>academia</phrase> and <phrase>industry</phrase> <phrase>Domain specific</phrase> languages <phrase>play</phrase> an important role in <phrase>model-driven engineering</phrase> of <phrase>software</phrase>-intensive <phrase>industrial</phrase> systems. A rich body of <phrase>knowledge</phrase> exists on the development of languages, modeling environments, and transformation systems. The understanding of <phrase>architectural</phrase> choices for combining these parts into a feasible <phrase>solution</phrase>, however, is not particularly deep. We <phrase>report</phrase> on an endeavor in the realm of a <phrase>technology transfer</phrase> process from <phrase>academia</phrase> to <phrase>industry</phrase>, where we encountered unexpected influences of the <phrase>architecture</phrase> on the <phrase>modeling language</phrase>. By examining the <phrase>evolution</phrase> of our <phrase>language</phrase> and its <phrase>programming</phrase> interface, we show that these influences mainly stemmed from practical considerations; for identifying these early on, tight interaction between our <phrase>research</phrase> lab and the <phrase>industrial</phrase> partner was key. In addition, we share insights into the practice of cooperating with <phrase>industry</phrase> by presenting essential lessons we learned.
A <phrase>pilot</phrase> study on <phrase>logic</phrase> proof tutoring using hints generated from historical <phrase>student</phrase> <phrase>data</phrase> We have proposed a novel application of <phrase>Markov</phrase> decision processes (MDPs), a <phrase>reinforcement learning</phrase> technique, to automatically generate hints using historical <phrase>student</phrase> <phrase>data</phrase>. Using this technique, we have modified a an existing, non-adaptive <phrase>logic</phrase> proof <phrase>tutor</phrase> called Deep Thought with a Hint <phrase>Factory</phrase> that provides hints on the next step a <phrase>student</phrase> might take. This <phrase>paper</phrase> presents the <phrase>results</phrase> of our <phrase>pilot</phrase> study using Deep Thought with the Hint <phrase>Factory</phrase>, which demonstrate that hints generated from historical <phrase>data</phrase> can support students in writing <phrase>logic</phrase> proofs.
An Overview: <phrase>Intelligent Tutoring</phrase> Systems <phrase>Intelligent tutoring</phrase> systems have been shown to be highly effective at increasing students' performance and <phrase>motivation</phrase>. In this <phrase>paper</phrase>, a brief overview of ITS will be provided and then, an overview of the main components of <phrase>intelligent tutoring</phrase> systems will be discussed in detail. <phrase>Intelligent Tutoring</phrase> System (ITS) is computer system that can support and improve of learning and teaching process in the <phrase>domain knowledge</phrase>. Katie Hafner defined ITS as " an <phrase>intelligent tutoring</phrase> system is <phrase>educational software</phrase> containing an <phrase>artificial intelligence</phrase> component. The <phrase>software</phrase> tracks students' work, Tailoring <phrase>feedback</phrase> and hints along the way. By collecting <phrase>information</phrase> on a particular student's performance, the <phrase>software</phrase> can make inferences about strengths and weaknesses and can suggest additional work. " (Hafner, 2000). Regarding Bloom's <phrase>research</phrase> the improvement of students' learning in one-on-one <phrase>human</phrase> tutoring is more effective than traditional classroom instruction by two standard deviations (Bloom, 1984). This improvement of two standard deviations shows that the students were taught by one-on-one tutoring, on <phrase>average</phrase>, achieves as well as the top (2%) of those were attending classroom instruction. Based on this, the ITSs aim to achieve similar <phrase>results</phrase> to those observed when a <phrase>human</phrase> <phrase>tutor</phrase> teaches a <phrase>student</phrase> individually. The goal of ITS is to enable students to acquire deep <phrase>knowledge</phrase>, gain skills of <phrase>problem solving</phrase>, and work independently. Computer has been used in <phrase>education</phrase> since 1970s. Computer-based training (<phrase>CBT</phrase>) and <phrase>computer aided</phrase> instruction (CAI) were the first educational systems being designed to be used in teaching (<phrase>Beck</phrase>, <phrase>Stern</phrase> and Haugsjaa 1996). Instruction in these systems was presented exactly in the same way to all students for all time. On other hand, <phrase>Intelligent Tutoring</phrase> Systems (ITSs) go further than training simulations to deal with specific user needs and to <phrase>tailor</phrase> Instruction based on the <phrase>student</phrase> profile. Unlike other computer-based training technologies, ITSs combine the <phrase>artificial intelligence</phrase>, <phrase>cognitive science</phrase> and advanced technologies to improve the learning level of students. Additionally, ITSs assess each learner's behaviour within these interactive environments and enhance a <phrase>model</phrase> of their <phrase>knowledge</phrase>, skills and expertise. Based on the learner/<phrase>student</phrase> <phrase>model</phrase>, the ITS tailors instructional strategies, in terms of both the content and style, and provide explanations, hints, examples, demonstrations and practice problems as needed. The first proposed ITS architectures consisted of four main components: a domain module, a <phrase>student</phrase> <phrase>model</phrase>, a <phrase>tutor</phrase> or pedagogical <phrase>model</phrase>, and an interface or <phrase>communication</phrase> module (Self, 1990). Since then, this <phrase>basic</phrase> <phrase>architecture</phrase> has been 
:1 <phrase>Mobile</phrase> Inquiry Learning Experience for Primary <phrase>Science</phrase> Students: a Study of Learning Effectivenessj Cal_390 269..287 This <phrase>paper</phrase> presents the findings of a <phrase>research</phrase> project in which we transformed a primary (grade) 3 <phrase>science</phrase> <phrase>curriculum</phrase> for delivery via <phrase>mobile</phrase> technologies, and a <phrase>teacher</phrase> enacted the lessons over the 2009 <phrase>academic</phrase> year in a class in a <phrase>primary school</phrase> in <phrase>Singapore</phrase>. The students had a total of 21 weeks of the mobilized lessons in <phrase>science</phrase>, which were co-designed by teachers and researchers by tapping into the affordances of <phrase>mobile</phrase> technologies for supporting inquiry learning in and outside of class. We examine the learning effectiveness of the enacted mobilized <phrase>science</phrase> <phrase>curriculum</phrase>. The <phrase>results</phrase> show that among the six mixed-ability classes in primary (grade) 3 in the <phrase>school</phrase>, the <phrase>experimental</phrase> class performed better than other classes as measured by traditional assessments in the <phrase>science</phrase> subject. With mobilized lessons, students were found to learn <phrase>science</phrase> in personal, deep and engaging ways as well as developed positive attitudes towards <phrase>mobile</phrase> learning.
Collaborative <phrase>Sensemaking</phrase> Support: Progressing from Portals and Tools to Collaboration Envelopes <phrase>Sensemaking</phrase> involves incomplete discovery, inaccurate interpretation, and imperfect <phrase>action</phrase> that will fail in someway and likely alter the situation in some unknowable way. <phrase>Sensemaking</phrase> demands intense, deep collaboration with participating agents, who many times are physically distributed and come from different groups and organizations. Incorporating collaboration functionality in a piecemeal approach in different ways as add-ons within a portal-based <phrase>architecture</phrase> can place heavy demands on users to learn, organizations to <phrase>train</phrase>, and ultimately limit the potential of collaboration <phrase>technology</phrase> to achieve organizational goals. It is proposed that individual and group <phrase>sensemaking</phrase> is a better <phrase>starting point</phrase> from which to build architectures to mitigate socio-<phrase>cognitive</phrase> limitations of participating agents collaborating to make sense of things. Three levels of Collaboration Envelopes are presented and <phrase>architectural</phrase> considerations presented to guide development of <phrase>technology</phrase> to better support collaborative <phrase>sensemaking</phrase>.
<phrase>Deep Learning</phrase> using <phrase>Support Vector Machines</phrase> Recently, <phrase>fully-connected</phrase> and <phrase>convolutional neural networks</phrase> have been trained to reach <phrase>state</phrase>-of-the-<phrase>art</phrase> performance on a wide <phrase>variety</phrase> of tasks such as <phrase>speech recognition</phrase>, <phrase>image classification</phrase>, <phrase>natural language processing</phrase> , and <phrase>bioinformatics</phrase> <phrase>data</phrase>. For <phrase>classification tasks</phrase>, much of these " <phrase>deep learning</phrase> " models employ the softmax <phrase>activation functions</phrase> to learn output <phrase>labels</phrase> in 1-of-K format. In this <phrase>paper</phrase>, we demonstrate a small but consistent advantage of replacing soft-max layer with a linear <phrase>support vector machine</phrase>. Learning minimizes a margin-based loss instead of the cross-<phrase>entropy</phrase> loss. In almost all of the previous works, hidden representation of deep networks are first learned using supervised or unsupervised techniques, and then are fed into SVMs as inputs. In contrast to those models, we are proposing to <phrase>train</phrase> all layers of the deep networks by backpropagating gradients through the top level <phrase>SVM</phrase>, learning features of all layers. Our experiments show that simply replacing softmax with linear SVMs gives significant gains on datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge .
Accelerating a <phrase>Random Forest</phrase> Classifier: <phrase>Multi-Core</phrase>, GP-<phrase>GPU</phrase>, or <phrase>FPGA</phrase>? <phrase>Random forest</phrase> classification is a well known <phrase>machine learning</phrase> technique that generates classifiers in the form of an ensemble (" <phrase>forest</phrase> ") of <phrase>decision trees</phrase>. The classification of an input sample is determined by the majority classification by the ensemble. Traditional <phrase>random forest</phrase> classifiers can be highly effective, but classification using a <phrase>random forest</phrase> is <phrase>memory</phrase> bound and not typically suitable for <phrase>acceleration</phrase> using <phrase>FPGAs</phrase> or GP-<phrase>GPUs</phrase> due to the need to traverse large, possibly irregular <phrase>decision trees</phrase>. Recent work at <phrase>Lawrence Livermore National Laboratory</phrase> has developed several variants of <phrase>random forest</phrase> classifiers, including the Compact <phrase>Random Forest</phrase> (CRF), that can generate <phrase>decision trees</phrase> more suitable for <phrase>acceleration</phrase> than traditional <phrase>decision trees</phrase>. Our <phrase>paper</phrase> compares and contrasts the effectiveness of <phrase>FPGAs</phrase>, GP-<phrase>GPUs</phrase>, and <phrase>multi-core</phrase> CPUs for accelerating classification using models generated by compact <phrase>random forest</phrase> <phrase>machine learning</phrase> classifiers. Taking advantage of training <phrase>algorithms</phrase> that can produce compact random <phrase>forests</phrase> composed of many, small <phrase>trees</phrase> rather than fewer, deep <phrase>trees</phrase>, we are able to regularize the <phrase>forest</phrase> such that the classification of any sample takes a deterministic amount of time. This optimization then allows us to execute the classifier in a pipelined or <phrase>single</phrase>-instruction multiple thread (SIMT) <phrase>fashion</phrase>. We show that <phrase>FPGAs</phrase> provide the highest performance <phrase>solution</phrase>, but require a multi-chip / multi-board system to execute even modest sized <phrase>forests</phrase>. GP-<phrase>GPUs</phrase> offer a more flexible <phrase>solution</phrase> with reasonably <phrase>high</phrase> performance that scales with <phrase>forest</phrase> size. Finally, multi-threading via <phrase>OpenMP</phrase> on a <phrase>shared memory</phrase> system was the simplest <phrase>solution</phrase> and provided near linear performance that scaled with core <phrase>count</phrase>, but was still significantly slower than the GP-<phrase>GPU</phrase> and <phrase>FPGA</phrase>.
Learning and the Reflective <phrase>Journal</phrase> in <phrase>Computer Science</phrase> This <phrase>paper</phrase> describes the use of the reflective <phrase>journal</phrase> in a computer <phrase>programming</phrase> course at the <phrase>University of South Australia</phrase>. We describe rationale for the <phrase>journal</phrase> relating it to the contribution it can make to generic skills of <phrase>lifelong learning</phrase>, <phrase>problem-solving</phrase>, <phrase>communication</phrase> and awareness of personal <phrase>learning strategies</phrase>. We also relate it to the Personal <phrase>Software</phrase> Process (<phrase>PSP</phrase>) used by <phrase>industry</phrase> to encourage <phrase>software</phrase> engineers to improve <phrase>productivity</phrase> by 'review', with <phrase>collation</phrase> of <phrase>software</phrase> <phrase>productivity</phrase> metrics and awareness of personal and team level practice. We introduce the detail how students were asked to use the <phrase>journal</phrase> and evaluate its influence on learning. We present a selection of <phrase>student</phrase> reflections on the <phrase>journal</phrase> and summarise attainment comparing attainment in the <phrase>journal</phrase> with other more traditional items of assessment. We conclude that there is a place for the reflective <phrase>journal</phrase> in <phrase>programming</phrase> courses-a number of students reported benefits to their learning along a number of dimensions-and this was supported with evidence both in their <phrase>journal</phrase> itself and by their absolute attainment. We also note that the reflective <phrase>journal</phrase> is not universally accepted (or even recognised as a 'valid' learning activity by some). However, we recommend further use and development of such reflective techniques in <phrase>computer science</phrase> to stimulate good <phrase>software</phrase> practice and <phrase>deep learning</phrase>.
Etmlsi O: a Program That Learns New Heuristics and Domain Concepts the <phrase>Nature</phrase> of Heuristics Iii: Program <phrase>Design</phrase> and <phrase>Results</phrase> The AM program, an early attempt to mechanize learning by discorery, has recently been expanded and extended to set'eral otl, <phrase>er</phrase> task. domains. AM's tdtimate failure apparently was due to its inability to discot'er new, powerftd, domain-<phrase>sl</phrase>~'cific heuristics for the rarious new fiehts it .ncot'ered. At that time, it seemed straight-fonvard to simply add "lleuristics" as one more field in which to let A.~I explore, obsert'e, define, and det'elop. That task-learning new het~ristics by discorery-turned out to be much more diffitcult than was realized initially, and we hat'e just now achiered some szwcesses at it. Along the way it became clearer why A.~I had succeeded in the first place, and why it was so diffuzult to ase the same <phrase>paradigm</phrase> to discot'er new heuristics. In essence, a~l was an atttomatic <phrase>programming</phrase> system, whose primitire actiotts were modificatiotts to pieces of <phrase>LISP</phrase> code, code which represented the characteristic functions of variotes <phrase>math</phrase> concepts. It was only beeattse of the deep relationship between <phrase>LISP</phrase> and Alathematics that these operatiotts (loop unwinding, <phrase>recursion</phrase> elimination , composition, argument elimination, <phrase>function</phrase> substitution, etc.) which were <phrase>basic</phrase> <phrase>LISP</phrase> nlutators also turned out to yieM a <phrase>high</phrase> 'hit rate" of t'iable, useful new <phrase>math</phrase> concepts when applied to preciously-known, useful <phrase>math</phrase> concepts. Hut no such deep relatiottship existed between <phrase>LISP</phrase> and Heuristics, and when the <phrase>basic</phrase> <phrase>automatic programming</phrase> operators were applied to t'iable, useful heuristics, they ahnost always <phrase>produced</phrase> ttseless (often worse than ttseless) new rides. Ot,r work on the <phrase>nature</phrase> of heuristics has enabled the cottstruction of a new <phrase>language</phrase> in which the statement of hettristics is more natural and compact. Briefly, the ~,ocabulary inchtdes many types of conditiotts, actions, and descriptire properties that a <phrase>heuristic</phrase> tnight possess; instead of writing a large lump of <phrase>LIsP</phrase> code to represent the he.ristic, one spreads the same <phrase>information</phrase> out across dozetts of 'slots'. By employing this new <phrase>language</phrase>, the old <phrase>property</phrase> that AM satisfied fortuitously is once again satisfied: the primitive <phrase>syntactic</phrase> operators usually now produce meaningful <phrase>semantic</phrase> t'ariants of what they operate on. The ties to the foundations of lleuretics <phrase>hare</phrase> been engineered into the <phrase>syntax</phrase> at:d t, ocabulary of the new <phrase>language</phrase>, partly by <phrase>design</phrase> and partly by et, olution, much as John McCarthy engineered ties to the foulutations of <phrase>Mathematics</phrase> into <phrase>LISP</phrase>. The EURISKO program embodies this <phrase>language</phrase>, and it is described in this <phrase>paper</phrase>, along with its <phrase>results</phrase> in eight task domains: <phrase>design</phrase> of 
Feasibility Study for <phrase>Procedural Knowledge</phrase> Extraction in Biomedical Documents We propose how to extract <phrase>procedural knowledge</phrase> rather than declarative <phrase>knowledge</phrase> utilizing <phrase>machine learning</phrase> method with deep <phrase>language</phrase> processing features in scientific documents, as well as how to <phrase>model</phrase> it. We show the representation of <phrase>procedural knowledge</phrase> in <phrase>PubMed</phrase> abstracts and provide experiments that are quite promising in that it shows 82%, 63%, 73%, and 70% performances of purpose/solutions (two components of <phrase>procedural knowledge</phrase> <phrase>model</phrase>) extraction, process " s entity identification, entity association, and relation identification between processes respectively, even though we applied strict guidelines in evaluating the performance.
Kernel Analysis of Deep Networks Grgoire Montavon When training deep networks it is common <phrase>knowledge</phrase> that an efficient and well generalizing representation of the problem is formed. In this <phrase>paper</phrase> we aim to elucidate what makes the emerging representation successful. We analyze the <phrase>layer-wise</phrase> <phrase>evolution</phrase> of the representation in a deep network by building a <phrase>sequence</phrase> of deeper and deeper kernels that subsume the mapping performed by more and more layers of the deep network and measuring how these increasingly complex kernels fit the learning problem. We observe that deep networks create increasingly better representations of the learning problem and that the structure of the deep network controls how fast the representation of the task is formed layer after layer.
Deep Lambertian Networks <phrase>Visual perception</phrase> is a challenging problem in part due to illumination variations. A possible <phrase>solution</phrase> is to first estimate an illumination invariant representation before using it for recognition. The object <phrase>albedo</phrase> and surface normals are examples of such representations. In this <phrase>paper</phrase>, we introduce a multilayer <phrase>generative model</phrase> where the <phrase>latent variables</phrase> include the <phrase>albedo</phrase>, surface normals, and the <phrase>light</phrase> source. Combining <phrase>Deep Belief</phrase> Nets with the Lambertian <phrase>reflectance</phrase> assumption , our <phrase>model</phrase> can learn good priors over the <phrase>albedo</phrase> from 2D images. Illumination variations can be explained by changing only the lighting <phrase>latent variable</phrase> in our <phrase>model</phrase>. By transferring learned <phrase>knowledge</phrase> from similar objects, <phrase>albedo</phrase> and surface normals estimation from a <phrase>single</phrase> image is possible in our <phrase>model</phrase>. Experiments demonstrate that our <phrase>model</phrase> is able to generalize as well as improve over standard baselines in one-<phrase>shot</phrase> <phrase>face recognition</phrase>.
Using CSP <phrase>Look-Back</phrase> Techniques to Solve <phrase>Real-World</phrase> <phrase>SAT</phrase> Instances in Tableau and POSIT do not. Combining good techniques for look-ahead and <phrase>look-back</phrase> is likely to give better performance across a broad <phrase>range</phrase> of problems. Similarly , systematic <phrase>algorithms</phrase> with <phrase>look-back</phrase> (or/and look-ahead) are not universally <phrase>superior</phrase> to <phrase>stochastic</phrase> <phrase>algorithms</phrase>. The best <phrase>algorithms</phrase> in either category are bound to be sty-mied by instances of sufficient size and complexity or adversarial structure. Systematic, complete <phrase>algorithms</phrase> have the obvious advantage that only they can <phrase>report</phrase> unsatisfiability. Size-bounded learning is effective when instances have relatively many <phrase>short</phrase> nogoods which can be derived without deep inference. Relevance-bounded learning is effective when many sub-problems corresponding to the current DP assignment also have this <phrase>property</phrase>. <phrase>Phase transition</phrase> instances from Random 3SAT tend to have very <phrase>short</phrase> nogoods [Schrag and Crawford 96], but these seem to require deep inference to derive, and <phrase>look-back</phrase>-enhanced DP is ineffective on them. GSAT is effective for these but not for problems which exhibit strong <phrase>local minima</phrase>. Some researchers have attempted to exploit the distinct advantages of systematic and <phrase>stochastic</phrase> search in <phrase>hybrid</phrase> global/<phrase>local search</phrase> <phrase>algorithms</phrase>. <phrase>Ginsberg</phrase> and McAllester [94] evaluated their partial-<phrase>order</phrase> dynamic <phrase>backtracking</phrase> <phrase>algorithm</phrase> using randomly generated instances with <phrase>crystallographic</phrase> structure, showing that it performed better than unenhanced Tableau. Mazure et al. [96] evaluated a <phrase>hybrid</phrase> <phrase>algorithm</phrase> with interleaved DP and <phrase>local search</phrase> execution using instances from the DIMACS suite, showing that it frequently outperformed capable non-<phrase>hybrid</phrase> DP implementations. Bayardo & Schrag [96] reported that <phrase>look-back</phrase>-enhanced Tableau performed as well as or better than these <phrase>algorithms</phrase> on the same <phrase>crystallographic</phrase> problem space and DIMACS instances. Knowing that our <phrase>look-back</phrase>-enhanced DP exhibits different strengths from <phrase>local search</phrase>, we believe it would be promising to exploit it in an interleaved <phrase>hybrid</phrase> like that of Mazure et al.. Also, given the similarity in performance of <phrase>look-back</phrase> enhanced <phrase>algorithms</phrase> on our <phrase>real-world</phrase> instances and the <phrase>artificial</phrase> ones used by Bayardo and Schrag [96], we speculate that their structured, " exceptionally hard " random problem space may reflect computational difficulties that arise in <phrase>real-world</phrase> problems better than do unstructured spaces like Random 3SAT. Conclusions We have described CSP <phrase>look-back</phrase> enhancements for DP and demonstrated their significant advantages. We feel their performance warrants their being included as options in DP implementations more commonly. Where DP is used in a larger system (for planning, scheduling, circuit processing, <phrase>knowledge representation</phrase>, <phrase>higher-order</phrase> <phrase>theorem proving</phrase>, etc., or in a <phrase>hybrid</phrase> systematic/<phrase>stochastic</phrase> <phrase>SAT</phrase> <phrase>algorithm</phrase>), <phrase>look-back</phrase>-enhanced DP should probably replace unen-hanced DP; 
<phrase>Digital</phrase> <phrase>Storytelling</phrase>: Emotions in <phrase>Higher Education</phrase> In <phrase>tandem</phrase> with the deep structural changes that have taken place in <phrase>society</phrase>, <phrase>education</phrase> must also shift towards a teaching approach focused on learning and the overall development of the <phrase>student</phrase>. The integration of <phrase>technology</phrase> may be the drive to foster the needed changes. We draw on the <phrase>literature</phrase> of pertaining to the role of emotions and <phrase>interpersonal relationships</phrase> in the learning process; the technological <phrase>evolution</phrase> of <phrase>storytelling</phrase> towards <phrase>Digital</phrase> <phrase>Storytelling</phrase> and its connections to <phrase>education</phrase>. We argue <phrase>Digital</phrase> <phrase>Storytelling</phrase> is capable of challenging HE contexts, namely the emotional realm, where the <phrase>private</phrase> vs. <phrase>public</phrase> <phrase>dichotomy</phrase> is more prominent. Ultimately we propose <phrase>Digital</phrase> <phrase>Storytelling</phrase> as the <phrase>aggregator</phrase> capable of personalizing <phrase>Higher Education</phrase> while developing essential skills and competences.
Deep-<phrase>play</phrase>: developing TPACK for <phrase>21st century</phrase> teachers A key complication facing teachers who seek to integrate <phrase>technology</phrase> in their teaching is the fact that most technologies are not designed for educational purposes. Making a tool an <phrase>educational technology</phrase> requires creative input from the <phrase>teacher</phrase> to redesign , or maybe even subvert the original intentions of the designer. The learning <phrase>technology</phrase> by <phrase>design</phrase> (LT/D) framework has been proposed as being an effective instructional technique to develop <phrase>deeper understanding</phrase> of technological <phrase>pedagogical content knowledge</phrase>. In this <phrase>paper</phrase> we expand our description of the LT/D technique to develop what we call a deep-<phrase>play</phrase> <phrase>model</phrase> for <phrase>teacher</phrase> <phrase>professional</phrase> development. The deep-<phrase>play</phrase> <phrase>model</phrase> integrates: a <phrase>pedagogy</phrase> for key <phrase>21st century</phrase> learning skills b content that cuts across disciplines with trans-disciplinary <phrase>cognitive</phrase> tools c <phrase>technology</phrase> by the creative repurposing of tools for pedagogical purposes. understanding the affordances and constraints of new technologies; the <phrase>design</phrase> of <phrase>technology</phrase>-rich, innovative <phrase>learning environments</phrase>; and the <phrase>professional</phrase> development of teachers. Together with Punya Mishra, he developed the technological <phrase>pedagogical content knowledge</phrase> (TPCK/TPACK) framework, first published in 2005. Punya Mishra is a <phrase>Professor</phrase> of <phrase>Educational Technology</phrase> at <phrase>Michigan State University</phrase> where he directs the <phrase>Master</phrase> of <phrase>Arts</phrase> in <phrase>Educational Technology</phrase> programme. He is nationally and internationally recognised for his work in the <phrase>area</phrase> of <phrase>technology</phrase> integration in <phrase>teacher</phrase> <phrase>education</phrase>. This <phrase>led</phrase> to the development (in collaboration with Dr. M.J. <phrase>Koehler</phrase>) of the technological <phrase>pedagogical content knowledge</phrase> (TPACK) framework, which has been described as being 'the most significant advancement in the <phrase>area</phrase> of <phrase>technology</phrase> integration in the past 25 years'. His <phrase>website</phrase> is at http://punyamishra.com/. Emily C. Bouck is an Associate <phrase>Professor</phrase> of Educational Studies in the <phrase>Special Education</phrase> Programme at <phrase>Purdue University</phrase>. Her <phrase>research</phrase> focuses on <phrase>assistive technology</phrase> for students with <phrase>high</phrase> incidence disabilities and issues of <phrase>curriculum</phrase> for <phrase>secondary</phrase> students with <phrase>high</phrase> incidence disabilities. <phrase>research</phrase> is focused on examining the <phrase>nature</phrase> of teaching, thinking, and learning in the new <phrase>ecology</phrase> of <phrase>digital</phrase>, largely web-mediated, <phrase>knowledge</phrase> and <phrase>information</phrase>. Kristen Kereluik is a doctoral <phrase>student</phrase> in the <phrase>Department</phrase> of <phrase>Educational Psychology</phrase> and <phrase>Educational Technology</phrase> in the <phrase>College</phrase> of <phrase>Education</phrase> at <phrase>Michigan State University</phrase>. Her <phrase>research</phrase> interest include <phrase>teaching and learning</phrase> in contexts mediated by <phrase>technology</phrase>. <phrase>research</phrase> interests include motivating students to learn, developing pre-and in-service teachers' technological <phrase>pedagogical content knowledge</phrase>, and understanding motivational aspects of <phrase>online learning</phrase> environments. His <phrase>website</phrase> is at <phrase>Arts</phrase> in <phrase>Educational Technology</phrase> programme at <phrase>Michigan State University</phrase>. Her <phrase>research</phrase> and <phrase>scholarship</phrase> focuses on the use of 
A discriminative deep <phrase>model</phrase> for pedestrian detection with occlusion handling Part-based models have demonstrated their merit in <phrase>object detection</phrase>. However, there is a key issue to be solved on how to integrate the inaccurate scores of part detectors when there are occlusions or large deformations. To handle the imperfectness of part detectors, this <phrase>paper</phrase> presents a probabilistic pedestrian detection framework. In this framework , a deformable part-based <phrase>model</phrase> is used to obtain the scores of part detectors and the visibilities of parts are mod-eled as hidden variables. Unlike previous occlusion handling approaches that assume <phrase>independence</phrase> among visibility <phrase>probabilities</phrase> of parts or manually define rules for the visibility relationship, a discriminative deep <phrase>model</phrase> is used in this <phrase>paper</phrase> for learning the visibility relationship among overlapping parts at <phrase>multiple layers</phrase>. <phrase>Experimental</phrase> <phrase>results</phrase> on three <phrase>public</phrase> datasets (<phrase>Caltech</phrase>, <phrase>ETH</phrase> and <phrase>Daimler</phrase>) and a new CUHK occlusion dataset 1 specially designed for the evaluation of occlusion handling approaches show the effectiveness of the <phrase>proposed approach</phrase>.
Bill Labs <phrase>Decision Theory</phrase> and Adaptive Systems Is <phrase>Ai</phrase> Going Mainstream at Last? a Look inside <phrase>Microsoft Research</phrase> 21 However, after AI's 15 minutes of fame in the mid-80s, analysts grew tired of waiting for <phrase>AI</phrase> to hit the big time and tucked it into a " promising <phrase>technology</phrase> " niche. Although <phrase>AI</phrase> has continued to make significant strides in theory, <phrase>technology</phrase>, and even deployed applications, its impact has yet to reach its full potential and profitability. There are many feathers in AI's capthe most recent is the <phrase>IBM</phrase> <phrase>Deep Blue</phrase> super-computer's defeat of <phrase>world chess champion</phrase>, <phrase>Garry Kasparov</phrase>. In large corporations, thousands of systems that include <phrase>AI</phrase> save hundreds of millions of dollars annually in <phrase>finance</phrase>, factories, and offices. Household, <phrase>electronic</phrase>, and office appliances, for example , are a little smarter because <phrase>bits</phrase> of <phrase>AI</phrase> help them <phrase>autotune</phrase> their performance and self-diagnose malfunctions. A suite of <phrase>software</phrase> tools that include <phrase>rule-based</phrase> <phrase>AI</phrase> chunks run <phrase>retail</phrase> <phrase>chain stores</phrase> such as Mrs. Fields Cookies. <phrase>Robots</phrase> now serve meals to <phrase>hospital</phrase> patients, clean up toxic dumps, and explore <phrase>Mars</phrase>. The examples of <phrase>AI</phrase>-enabled <phrase>intelligent systems</phrase> are many and varied, including a <phrase>range</phrase> of technologies such as heuristics, <phrase>neural networks</phrase>, <phrase>genetic algorithms</phrase>, <phrase>natural language processing</phrase>, and <phrase>case-based reasoning</phrase>. For the most part, they are limited to solving a particular kind of problemsuch as scheduling or process controlin a particular market segmentmanufacturing, for example, or <phrase>insurance</phrase> <phrase>underwriting</phrase>. <phrase>Microsoft</phrase> is beginning to change all that. <phrase>Steve Ballmer</phrase>, Microsoft's executive <phrase>vice president</phrase> of sales and support, and Bill Gates' buddy from <phrase>Harvard</phrase> days, addressed the National Conference on <phrase>Artificial Intelligence</phrase> in <phrase>Seattle</phrase> in 1994 (AAAI-94), declaring that <phrase>AI</phrase> is very important to <phrase>Microsoft</phrase>. He gave demonstrations of Tip Wizard in <phrase>Excel</phrase> and Liz in the Magic <phrase>School Bus</phrase>, little <phrase>intelligent agents</phrase> that help users understand new programs. Ballmer admitted that these were simple agentsonly the start of the <phrase>intelligence</phrase> <phrase>Microsoft</phrase> hopes to build into future <phrase>products</phrase>. Bill Gates has a vision of the computer of the futurean intelligent assistant that talks, listens, sees, and learns, yet is simple and intuitive to use. To achieve this vision, Gates began assembling a computer <phrase>brain trust</phrase> called <phrase>Microsoft Research</phrase>. First he hired <phrase>Nathan Myhrvold</phrase>, who graduated from <phrase>UCLA</phrase> at 19, and from <phrase>Princeton</phrase> at 23 with a <phrase>PhD</phrase> in <phrase>mathematical</phrase> and <phrase>theoretical physics</phrase>. Myhrvold left his fellowship under <phrase>Stephen Hawking</phrase> at <phrase>Cambridge University</phrase> to start up a <phrase>software</phrase> <phrase>business</phrase> that <phrase>Microsoft</phrase> bought in 1986, and he joined <phrase>Microsoft</phrase>. In 1991, he began building the <phrase>Research</phrase> group now, 
Efficient Learning of <phrase>Deep Boltzmann Machines</phrase> We present a new approximate inference <phrase>algorithm</phrase> for <phrase>Deep Boltzmann Machines</phrase> (DBM's), a <phrase>generative model</phrase> with many layers of hidden variables. The <phrase>algorithm</phrase> learns a separate " recognition " <phrase>model</phrase> that is used to quickly initialize , in a <phrase>single</phrase> bottom-up <phrase>pass</phrase>, the values of the <phrase>latent variables</phrase> in all <phrase>hidden layers</phrase>. We show that using such a recognition <phrase>model</phrase>, followed by a combined top-down and bottom-up <phrase>pass</phrase>, it is possible to efficiently learn a good <phrase>genera</phrase>-tive <phrase>model</phrase> of <phrase>high</phrase>-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down <phrase>feedback</phrase> plays a critical role in the performance of a <phrase>DBM</phrase>, both as a generative and discrimina-tive <phrase>model</phrase>. Moreover, inference is only at most three times slower compared to the approximate inference in a <phrase>Deep Belief</phrase> Network (DBN), making <phrase>large-scale</phrase> learning of DBM's practical. Finally , we demonstrate that the DBM's trained using the proposed approximate inference <phrase>algorithm</phrase> perform well compared to DBN's and SVM's on the MNIST <phrase>handwritten digit</phrase>, <phrase>OCR</phrase> <phrase>English</phrase> letters, and NORB <phrase>visual object</phrase> <phrase>recognition tasks</phrase>.
Beyond <phrase>PBL</phrase>: Preparing Graduates for <phrase>Professional</phrase> Practice An analysis of practitioner studies concludes that a gap exists between <phrase>industry</phrase> expectations of IT graduates and formal <phrase>education</phrase>, in particular in non-technical skills and <phrase>knowledge</phrase>. This <phrase>paper</phrase> reports on the final cycle of an <phrase>Action Research</phrase> project to examine and implement <phrase>alternative</phrase> <phrase>learning environments</phrase> for <phrase>Software Engineering</phrase>. A <phrase>model</phrase> based on <phrase>reflective practice</phrase>, founded on the evaluation of previous cycles applying <phrase>Cognitive</phrase> <phrase>Apprenticeship</phrase> and <phrase>Problem-based Learning</phrase>, was developed and implemented. This study looked at the alignment between <phrase>student</phrase> <phrase>approaches to learning</phrase> and the environment developed, in particular <phrase>student</phrase> disposition towards deep or surface learning. Although evaluation shows the <phrase>student</phrase> cohort achieved significantly higher scores in their assessment that those of previous offerings, it was notable that students who reported themselves as adopting surface approaches were less comfortable with the environment. However, they still exhibited <phrase>deep learning</phrase> characteristics when observed in a subsequent course. While an understanding of <phrase>student</phrase> learning is fundamental in developing <phrase>learning environments</phrase>, alignment between discipline and learning is also critical in educating competent practitioners. <phrase>Results</phrase> of this study show that students placed in an environment that enables them to <phrase>model</phrase> <phrase>professional</phrase> practice, and reflect on that modelling, should be much better prepared for the workplace.
<phrase>Teacher</phrase> Reflection and Theories of Learning Online Many <phrase>universities</phrase> have pursued the development of online offering of their subjects with enthusiasm, with a <phrase>perception</phrase> that ultimately such offerings will increase the availability of the subject and be a cost-effective enterprise. As yet, little attention has been paid to the potential for online subjects to encourage <phrase>deep learning</phrase> in students who undertake them. This <phrase>paper</phrase> considers some current understandings of students' <phrase>approaches to learning</phrase> and examines the potential for online subjects to provide a positive teaching/<phrase>learning environment</phrase>. It concludes that, as in the lecture <phrase>theatre</phrase> or tutorial room, the virtual classroom can succeed through reflective teaching underpinned by the solid application of theories of learning. The proliferation of online subjects offered by <phrase>universities</phrase> seems to have come with a <phrase>rush</phrase>. The promise of offering services to students far removed geographically from the physical <phrase>university</phrase> is attractive and in line with <phrase>government</phrase> policy that encourages flexible <phrase>learning strategies</phrase> for improved access to <phrase>education</phrase> (Anderson et al 2000). Added to this is the intuitive impression that such offerings will cost less than face-to-face teaching, thus meeting the institutional need for competitiveness and <phrase>economy</phrase> (Edwards & Nicoll 2000; Press & <phrase>Washburn</phrase> 2001). Unfortunately, the indications so far are that this optimism is mistaken, with estimates of time taken to produce one hour of online teaching rising to as much as 200 hours (Roach 2001a).Nevertheless, <phrase>university</phrase> departments and schools have embraced the new <phrase>technology</phrase> with something like <phrase>evangelical</phrase> fervour, although its cost effectiveness and pedagogic integrity are still uncertain (Ng 2000). Many issues have arisen for those involved, regarding <phrase>management</phrase> of the <phrase>technology</phrase>, provision of sufficient staff and resources, the day-today running of the subjects and the prevention of cheating (Arnold 1999; Roach 2001b). One <phrase>area</phrase> that has received little attention from researchers is the effect of this type of subject presentation on <phrase>student</phrase> learning.
Recursive Similarity-Based <phrase>Algorithm</phrase> for <phrase>Deep Learning</phrase> Recursive Similarity-Based <phrase>Learning algorithm</phrase> (RSBL) follows the <phrase>deep learning</phrase> idea, exploiting similarity-based methodology to recursively generate new features. Each transformation layer is generated separately, using as inputs <phrase>information</phrase> from all previous layers, and as new features similarity to the k nearest neighbors scaled using Gaussian kernels. In the <phrase>feature space</phrase> created in this way <phrase>results</phrase> of various types of classifiers, including linear <phrase>discrimination</phrase> and distance-based methods, are significantly improved. As an illustrative example a few non-trivial <phrase>benchmark datasets</phrase> from the <phrase>UCI</phrase> <phrase>Machine Learning</phrase> Repository are analyzed.
Offline/realtime <phrase>traffic classification</phrase> using <phrase>semi-supervised</phrase> learning <phrase>traffic classification</phrase>, <phrase>semi-supervised</phrase> learning, clustering Identifying and categorizing <phrase>network traffic</phrase> by application type is challenging because of the continued <phrase>evolution</phrase> of applications, especially of those with a desire to be undetectable. The diminished effectiveness of <phrase>port</phrase>-based identification and the overheads of <phrase>deep packet inspection</phrase> approaches motivate us to classify traffic by exploiting distinctive flow characteristics of applications when they communicate on a network. In this <phrase>paper</phrase>, we explore this latter approach and propose a <phrase>semi-supervised</phrase> classification method that can accommodate both known and unknown applications. To the best of our <phrase>knowledge</phrase>, this is the first work to use <phrase>semi-supervised</phrase> learning techniques for the <phrase>traffic classification</phrase> problem. Our approach allows classifiers to be designed from <phrase>training data</phrase> that consists of only a few labeled and many unlabeled flows. We consider pragmatic classification issues such as <phrase>longevity</phrase> of classifiers and the need for retraining of classifiers. Our performance evaluation using empirical <phrase>Internet traffic</phrase> traces that span a 6-month <phrase>period</phrase> shows that: 1) <phrase>high</phrase> flow and <phrase>byte</phrase> <phrase>classification accuracy</phrase> (i.e., greater than 90%) can be achieved using <phrase>training data</phrase> that consists of a small number of labeled and a large number of unlabeled flows; 2) presence of " mice " and " <phrase>elephant</phrase> " flows in the <phrase>Internet</phrase> complicates the <phrase>design</phrase> of classifiers, especially of those with <phrase>high</phrase> <phrase>byte</phrase> accuracy, and necessities use of weighted sampling techniques to obtain training flows; and 3) retraining of classifiers is necessary only when there are non-transient changes in the network usage characteristics. As a <phrase>proof of concept</phrase>, we implement <phrase>prototype</phrase> offline and realtime classification systems to demonstrate the feasibility of our approach. ABSTRACT Identifying and categorizing <phrase>network traffic</phrase> by application type is challenging because of the continued <phrase>evolution</phrase> of applications, especially of those with a desire to be undetectable. The diminished effectiveness of <phrase>port</phrase>-based identification and the overheads of <phrase>deep packet inspection</phrase> approaches motivate us to classify traffic by exploiting distinctive flow characteristics of applications when they communicate on a network. In this <phrase>paper</phrase>, we explore this latter approach and propose a <phrase>semi-supervised</phrase> classification method that can accommodate both known and unknown applications. To the best of our <phrase>knowledge</phrase>, this is the first work to use <phrase>semi-supervised</phrase> learning techniques for the <phrase>traffic classification</phrase> problem. Our approach allows classifiers to be designed from <phrase>training data</phrase> that consists of only a few labeled and many unlabeled flows. We consider pragmatic classification issues such as <phrase>longevity</phrase> of classifiers and 
<phrase>Deep Learning</phrase> with Hierarchical Convolutional <phrase>Factor Analysis</phrase> Unsupervised multilayered (&#x201C;deep&#x201D;) models are considered for imagery. The <phrase>model</phrase> is represented using a hierarchical convolutional <phrase>factor-analysis</phrase> <phrase>construction</phrase>, with sparse factor loadings and scores. The computation of layer-dependent <phrase>model</phrase> parameters is implemented within a <phrase>Bayesian</phrase> setting, employing a Gibbs sampler and variational <phrase>Bayesian</phrase> (VB) analysis that explicitly exploit the convolutional <phrase>nature</phrase> of the expansion. To address <phrase>large-scale</phrase> and streaming <phrase>data</phrase>, an online version of VB is also developed. The number of <phrase>dictionary</phrase> elements at each layer is inferred from the <phrase>data</phrase>, based on a beta-<phrase>Bernoulli</phrase> implementation of the <phrase>Indian</phrase> <phrase>buffet</phrase> process. Example <phrase>results</phrase> are presented for several <phrase>image-processing</phrase> applications, with comparisons to related models in the <phrase>literature</phrase>.
Adaptive Parallel Tempering for <phrase>Stochastic</phrase> <phrase>Maximum Likelihood</phrase> Learning of RBMs <phrase>Restricted Boltzmann Machines</phrase> (RBM) have attracted a lot of attention of late, as one the principle <phrase>building blocks</phrase> of deep networks. Training RBMs remains problematic however, because of the intractibility of their <phrase>partition</phrase> <phrase>function</phrase>. The <phrase>maximum likelihood</phrase> <phrase>gradient</phrase> requires a very robust sampler which can accurately sample from the <phrase>model</phrase> despite the loss of ergodicity often incurred during learning. While using Parallel Tempering in the negative phase of <phrase>Stochastic</phrase> <phrase>Maximum Likelihood</phrase> (SML-PT) helps address the issue, it imposes a <phrase>trade</phrase>-off between <phrase>computational complexity</phrase> and <phrase>high</phrase> ergodicity, and requires careful hand-tuning of the temperatures. In this <phrase>paper</phrase>, we show that this <phrase>trade</phrase>-off is unnecessary. The choice of optimal temperatures can be automated by minimizing <phrase>average</phrase> return time (a concept first proposed by (Katzgraber et al., 2006)) while chains can be spawned dynamically, as needed, thus minimizing the computational overhead. We show on a synthetic dataset, that this <phrase>results</phrase> in better likelihood scores.
<phrase>Learning Algorithms</phrase> for the Classification <phrase>Restricted Boltzmann Machine</phrase> <phrase>Recent developments</phrase> have demonstrated the capacity of <phrase>restricted Boltzmann machines</phrase> (RBM) to be powerful <phrase>generative models</phrase>, able to extract useful features from <phrase>input data</phrase> or construct deep <phrase>artificial neural networks</phrase>. In such settings, the RBM only yields a preprocessing or an initialization for some other <phrase>model</phrase>, instead of acting as a complete supervised <phrase>model</phrase> in its own right. In this <phrase>paper</phrase>, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable <phrase>gradient</phrase>, we also compare different approaches to estimating this <phrase>gradient</phrase> and address the issue of obtaining such a <phrase>gradient</phrase> for problems with very <phrase>high</phrase> dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely <phrase>semi-supervised</phrase> and multitask learning.
<phrase>Weakly supervised</phrase> scalable audio <phrase>content analysis</phrase> Audio <phrase>Event Detection</phrase> is an important task for <phrase>content analysis</phrase> of <phrase>multimedia</phrase> <phrase>data</phrase>. Most of the current works on detection of audio events is driven through <phrase>supervised learning</phrase> approaches. We propose a <phrase>weakly supervised</phrase> learning framework which can make use of the tremendous amount of web <phrase>multimedia</phrase> <phrase>data</phrase> with significantly reduced annotation effort and expense. Specifically, we use several multiple instance <phrase>learning algorithms</phrase> to show that audio <phrase>event detection</phrase> through weak <phrase>labels</phrase> is feasible. We also propose a novel scal-able multiple instance <phrase>learning algorithm</phrase> and show that its competitive with other multiple instance <phrase>learning algorithms</phrase> for audio <phrase>event detection</phrase> tasks. 1. INTRODUCTION <phrase>Multimedia</phrase> <phrase>content analysis</phrase> is a necessity for meaningful retrieval and indexing of <phrase>multimedia</phrase> <phrase>data</phrase>. It is becoming even more important due to the explosive growth of <phrase>multimedia</phrase> <phrase>data</phrase> over the <phrase>internet</phrase>. The focus of this <phrase>paper</phrase> is the audio component of <phrase>multimedia</phrase> which carries a significant amount of <phrase>information</phrase> about the overall content of <phrase>multimedia</phrase> <phrase>data</phrase>. Audio <phrase>content analysis</phrase> for us means successful detection of different <phrase>acoustic</phrase> events in a given audio <phrase>recording</phrase>. Audio <phrase>content analysis</phrase> or audio <phrase>event detection</phrase> (AED), the term more frequently used in this <phrase>paper</phrase> is important for reasons other than <phrase>multimedia</phrase> retrieval as well. Several applications such as <phrase>surveillance</phrase> [1] and monitoring are much easier to do through audio <phrase>data</phrase> only. Audio signals are not only easier to capture and transmit but can also <phrase>pass</phrase> through obstacles where cameras would be simply useless. An excellent example for audio based monitoring systems are those used in <phrase>wildlife</phrase> monitoring, for example <phrase>bird</phrase> <phrase>species</phrase> recognition using birdsong audio [2] [3]. Moreover, audio <phrase>event detection</phrase> plays an important role for context aware systems [4]. A <phrase>variety</phrase> of methods including those already referred have been proposed for AED tasks. Taking cues from <phrase>automatic speech recognition</phrase> GMM-HMM architectures have been employed for AED as well[5]. Discriminative learning methods in which fixed length representations for audio segments combined with discriminative classifiers such as <phrase>Support Vector Machine</phrase> or <phrase>Random Forest</phrase> classifiers have also been proposed [6] [7] [8][9]. With success of <phrase>Deep neural networks</phrase> for ASR, audio <phrase>event detection</phrase> using DNNs were attempted in [10] [9] [11]. An inherent problem with AED is presence of overlapping events. Successful detection of overlapping events is crucial for <phrase>real world</phrase> applications. Techniques such as matrix factorization and <phrase>deep neural networks</phrase> have been used for this more challenging tasks [12][13]. From review of <phrase>literature</phrase> on 
<phrase>Deep Learning</phrase> <phrase>Design</phrase> for <phrase>Sustainable</phrase> <phrase>Innovation</phrase> within Shifting Learning Landscapes Changes in the underpinning technologies for TEL is occurring at a pace that we have never before experienced, and this is unlikely to slow down. This necessitates a broader and more profound understanding of <phrase>design</phrase> that needs to be more future-proof than relying on the latest or emerging technologies and yet embraces the collaborative, multimodal and ubiquitous <phrase>nature</phrase> of learning in 21C. In addressing this challenge this article develops, exemplifies and <phrase>tests</phrase> the approach of <phrase>Deep Learning</phrase> <phrase>Design</phrase> (DLD), which has <phrase>led</phrase> to relatively <phrase>large-scale</phrase> and <phrase>sustainable</phrase> innovations and also outlined clear directions for near-future developments. Specifically, in this article we: justify why DLD is necessary and describe its key principles; exemplify these principles through four TEL initiatives; and, draw some implications and conclusions from across these projects about DLD and future learning.
Hydroacoustic Signal Classification Using <phrase>Support Vector Machines</phrase> <phrase>Kernel-based</phrase> <phrase>algorithms</phrase> such as <phrase>support vector machines</phrase> (SVMs) are <phrase>state</phrase>-of-the-<phrase>art</phrase> in <phrase>machine learning</phrase> for <phrase>pattern recognition</phrase>. This chapter introduces SVMs and describes a specific application to hydroacoustic signal classification. <phrase>Long</phrase>-<phrase>range</phrase>, passive-<phrase>acoustic</phrase> monitoring in the oceans is facilitated by propagation properties for underwater <phrase>sound</phrase>. In particular, the deep <phrase>sound</phrase> (SOFAR, <phrase>Sound</phrase> Fixing and Ranging) <phrase>channel</phrase> can <phrase>act</phrase> as a <phrase>waveguide</phrase> for underwater signals. In this chapter , SVMs are employed for classifying hydroacoustic signals recorded by the <phrase>sensor</phrase> network for verification of the <phrase>Comprehensive Nuclear-Test-Ban Treaty</phrase>. Constraints in the early <phrase>signal processing</phrase> chain and limited <phrase>data</phrase> require tailored kernel functions and careful <phrase>SVM</phrase> <phrase>model selection</phrase>. We demonstrate how problem-specific kernel functions can increase classifier performance when combined with efficient <phrase>gradient</phrase>-based approaches for optimizing kernel and <phrase>SVM</phrase> regularization parameters.
Automated <phrase>Object Recognition</phrase> through <phrase>Reinforcement Learning</phrase> Approved ACKNOWLEDGEMENTS I would like to express my sincere gratitude to my advisor, Dr. Sunanda Mitra, for her <phrase>academic</phrase> guidance, financial support, and motherlike care for me and my <phrase>family</phrase>. All progress during my course of study and <phrase>thesis</phrase> writing entails her constant encouragement. My deep thanks are owed to Dr. and Dr. Tim <phrase>Dallas</phrase> who kindly agreed to be on my committee and gave me invaluable advice on my <phrase>thesis</phrase>. I am very grateful to all the faculty and staff members in the <phrase>Department</phrase> of Electrical and Computer <phrase>Engineering</phrase> for all their kind help to me during my stay at <phrase>Texas Tech University</phrase>. I tmly appreciate the help from all my colleagues in the <phrase>Computer Vision</phrase> and Great thanks go to the officers in the graduate <phrase>school</phrase>, especially Ms. Barbi Dickensheet for her persistence in correcting my dissertation, and Mrs. Peggy Edmonson for her kind <phrase>academic</phrase> program advice. My <phrase>motivation</phrase> to work hard ultimately comes from my <phrase>family</phrase>. My wife, Huifang, supported my study without reservation by taking good care of me and the children. My three kids, Wenqi, Wencan (Shavm), and Wenlin (Catherine), always make my <phrase>life</phrase> full of hope and joy.
Assessment and <phrase>Learning Outcomes</phrase>: the Evaluation of <phrase>Deep Learning</phrase> in an On-line Course Assessment and <phrase>Learning Outcomes</phrase>: the Evaluation of <phrase>Deep Learning</phrase> in an On-line Course one copy of any article(s) in <phrase>SHURA</phrase> to facilitate their <phrase>private</phrase> study or for non-commercial <phrase>research</phrase>. You may not engage in further distribution of the material or use it for any profit-making activities or any commercial gain. ABSTRACT Using an <phrase>online learning</phrase> environment, students from <phrase>European countries</phrase> collaborated and communicated to carry out <phrase>problem based learning</phrase> in <phrase>occupational therapy</phrase>. The effectiveness of this approach was evaluated by means of the final assessments and published <phrase>learning outcomes</phrase>. In particular, transcripts from <phrase>peer-to-peer</phrase> sessions of synchronous <phrase>communication</phrase> were analysed. The SOLO <phrase>taxonomy</phrase> was used and the development of <phrase>deep learning</phrase> was studied week by week. This allowed the quality of the course to be appraised and showed, to a certain extent, the impact of this online international course on the <phrase>learning strategies</phrase> of the students. <phrase>Results</phrase> indicate that <phrase>deep learning</phrase> can be supported by synchronous <phrase>communication</phrase> and online meetings between course participants. INTRODUCTION Synchronous <phrase>communication</phrase> is little-used in on-line courses at present, owing to real-time communications not being sufficiently reliable and the greater bandwidth requirement compared with <phrase>email</phrase>-based courses such as those based on FirstClass or Blackboard.com. When <phrase>Internet</phrase> Chat facilities are available, they are simple add-ons and are not integrated into the course structure. However, future improvements in communications will make it possible for courses to use synchronous communications systematically. It is recognised that synchronous communications promote <phrase>motivation</phrase> and group cohesion, as well as providing good <phrase>feedback</phrase>, supporting consensus and <phrase>decision making</phrase>, and assisting pacing-encouraging people to keep up to date and providing discipline (Mason, 1998). However, it is more difficult to schedule group meetings, and synchronous tutorials are relatively more expensive as the optimum group size is so much smaller. Synchronous working meetings follow face-to-face groups in typically having less than ten participants, compared with asynchronous groups of twenty or more. Given these disadvantages, it is important to establish what aspects of synchronous courses <phrase>lead</phrase> to the effective development of <phrase>deep learning</phrase>, and the factors that promote a successful outcome. The <phrase>Occupational Therapy</phrase> <phrase>Internet</phrase> <phrase>School</phrase> (OTIS) <phrase>pilot</phrase> course has provided an opportunity to study the development of <phrase>deep learning</phrase> in a ten-week course with a substantial <phrase>degree</phrase> of synchronous <phrase>communication</phrase>.
<phrase>Deep Reinforcement Learning</phrase> Discovers Internal Models <phrase>Deep Reinforcement Learning</phrase> (DRL) is a trending field of <phrase>research</phrase>, showing great promise in challenging problems such as playing <phrase>Atari</phrase>, solving Go and controlling <phrase>robots</phrase>. While DRL agents perform well in practice we are still lacking the tools to analayze their performance. In this work we present the Semi-Aggregated MDP (SAMDP) <phrase>model</phrase>. A <phrase>model</phrase> best suited to describe policies exhibiting both spatial and temporal hierarchies. We describe its advantages for analyzing trained policies over other modeling approaches, and show that under the right <phrase>state</phrase> representation, like that of DQN agents, SAMDP can help to identify skills. We detail the automatic process of creating it from recorded trajectories, up to presenting it on t-<phrase>SNE</phrase> maps. We explain how to evaluate its fitness and show surprising <phrase>results</phrase> indicating <phrase>high</phrase> compatibility with the policy at hand. We conclude by showing how using the SAMDP <phrase>model</phrase>, an extra performance gain can be squeezed from the agent.
The influence of modality on deep-<phrase>reasoning questions</phrase> This study investigated the influence that modality (print versus spoken text) had on learning with deep <phrase>reasoning questions</phrase>. Half the participants were randomly assigned to receive deep-<phrase>reasoning questions</phrase> during the learning session. The other half received the same <phrase>information</phrase> in the absence of deep-<phrase>reasoning questions</phrase>. The participants who received deep <phrase>reasoning questions</phrase> were randomly assigned to one of two different groups. One group received deep <phrase>reasoning questions</phrase> as on-screen printed text while the other group received deep <phrase>reasoning questions</phrase> in a spoken modality via a text to speech <phrase>engine</phrase>. Participants who received deep <phrase>reasoning questions</phrase> had higher post-<phrase>test</phrase> scores than those who did not, a finding that replicated previous <phrase>research</phrase>. Additionally, learning was better for the learners who received printed text than spoken messages, a finding that is not compatible with a number of theoretical and empirical claims in the <phrase>literature</phrase>. (2010) 'The influence of modality on deep-<phrase>reasoning questions</phrase>', Int. interests include <phrase>intelligent tutoring</phrase> environments, <phrase>student</phrase> generated questions, and emotions. Scotty D. Craig is a <phrase>Research</phrase> <phrase>Scientist</phrase> in the Institute for <phrase>Intelligent Systems</phrase> located at the <phrase>University</phrase> of <phrase>Memphis</phrase>. To date, he has worked on projects in such areas as affect and learning, discourse processing, mechanical reasoning, <phrase>multimedia</phrase> learning, <phrase>vicarious learning</phrase> environments and <phrase>intelligent tutoring</phrase> systems.
Authentic Assessment Tasks: Students take a deep approach to learning This <phrase>article presents</phrase> the <phrase>student</phrase>-centered, deep approach to learning. The teaching resources developed for two strategic and services <phrase>marketing</phrase> courses have been integrated with several formative and summative assessment tasks. Authentic assessment tasks allowed for deeper engagement of students with the subject content, <phrase>peers</phrase>, and the course coordinator. A <phrase>student</phrase>-centered deeper approach to learning is evident through the creative thinking and <phrase>problem solving</phrase> demonstrated through the provision of solutions to <phrase>real-life</phrase> <phrase>business</phrase> problems.
<phrase>Web-based</phrase> Personalised System of Instruction: an Effective Approach for Diverse Cohorts with Virtual <phrase>Learning Environments</phrase>? <phrase>Computer-mediated Communication</phrase> Improving Classroom Teaching <phrase>Media</phrase> in <phrase>Education</phrase> Post-<phrase>secondary Education</phrase> Teaching/<phrase>learning Strategies</phrase> The Personalised System of Instruction is a form of mastery learning which, though it has been <phrase>proven</phrase> to be educationally effective, has never seriously challenged the dominant lecture-tutorial teaching method in <phrase>higher education</phrase> and has largely fallen into disuse. An <phrase>information and communications technology</phrase> assisted version of the Personalised System of Instruction using a <phrase>virtual learning environment</phrase> is promoted here based on the authors " longitudinal <phrase>design</phrase> <phrase>research</phrase> into this <phrase>pedagogy</phrase>. The particular elements of the <phrase>virtual learning environment</phrase> which are promoted are <phrase>short</phrase> <phrase>video</phrase> clips, online formative <phrase>tests</phrase> and an assessment <phrase>management</phrase> system. The authors present their experiences of developing and deploying this <phrase>pedagogy</phrase> for the teaching of introductory <phrase>discrete mathematics</phrase> to large classes of <phrase>Computer Science</phrase> students at two <phrase>UK</phrase> <phrase>higher education</phrase> institutions both with whole cohorts and " at <phrase>risk</phrase> " groups of students. In particular, this method is promoted as particularly helpful to students who do not adopt a deep approach to learning as many students fail to do. Moreover " at <phrase>risk</phrase> " students using this method (n = 71) demonstrated an <phrase>average</phrase> <phrase>Glass</phrase> <phrase>effect size</phrase> of 0.83 compared with other " at <phrase>risk</phrase> " students who did not (n = 35). Based on these experiences, this <phrase>pedagogy</phrase> is promoted as an effective approach to teaching in <phrase>higher education</phrase>, especially the teaching of <phrase>cognitive</phrase> skills to diverse cohorts of students on foundation level modules.
Discriminative Sentence Compression with Soft <phrase>Syntactic</phrase> Evidence We present a <phrase>model</phrase> for sentence compression that uses a discriminative <phrase>large-margin</phrase> learning framework coupled with a novel feature set defined on compressed bigrams as well as <phrase>deep syntactic</phrase> representations provided by <phrase>auxiliary</phrase> dependency and phrase-structure parsers. The parsers are trained out-of-domain and contain a significant amount of noise. We argue that the discriminative <phrase>nature</phrase> of the <phrase>learning algorithm</phrase> allows the <phrase>model</phrase> to learn weights relative to any noise in the feature set to optimize compression accuracy directly. This differs from <phrase>current state</phrase>-of-the-<phrase>art</phrase> models (<phrase>Knight</phrase> and Marcu, 2000) that treat noisy parse <phrase>trees</phrase>, for both compressed and uncompressed sentences, as <phrase>gold standard</phrase> when calculating <phrase>model</phrase> parameters.
<phrase>Computational Intelligence</phrase> in <phrase>Astronomy</phrase> - A Win-Win Situation Large archives of <phrase>astronomical</phrase> <phrase>data</phrase> (images, spectra and catalogues of derived parameters) are being assembled worldwide as part of the Virtual <phrase>Observatory</phrase> project. In <phrase>order</phrase> for such massive heterogeneous <phrase>data</phrase> collections to be of use to <phrase>astronomers</phrase>, development of <phrase>Computational Intelligence</phrase> techniques that would combine modern <phrase>machine learning</phrase> with deep <phrase>domain knowledge</phrase> is crucial. Both fields-<phrase>Computer Science</phrase> and <phrase>Astronomy</phrase>-can hugely benefit from such a <phrase>research</phrase> program. <phrase>Astronomers</phrase> can gain new insights into structures buried deeply in the <phrase>data</phrase> collections that would, without the help of <phrase>Computational Intelligence</phrase>, stay masked. On the other hand, computer scientists can get inspiration and <phrase>motivation</phrase> for development of new techniques driven by the specific characteristics of <phrase>astronomical</phrase> <phrase>data</phrase> and the need to include <phrase>domain knowledge</phrase> in a fundamental way. In this review we present three diverse examples of such successful <phrase>symbiosis</phrase>.
Generalized <phrase>Independent Component Analysis</phrase> Over Finite Alphabets <phrase>Independent component analysis</phrase> (ICA) is a statistical method for transforming an <phrase>observable</phrase> multidimensional random <phrase>vector</phrase> into components that are as statistically <phrase>independent</phrase> as possible from each other. Usually the ICA framework assumes a <phrase>model</phrase> according to which the observations are generated (such as a <phrase>linear transformation</phrase> with additive noise). ICA over finite fields is a special case of ICA in which both the observations and the <phrase>independent</phrase> components are over a finite <phrase>alphabet</phrase>. In this work we consider a generalization of this framework in which an observation <phrase>vector</phrase> is decomposed to its <phrase>independent</phrase> components (as much as possible) with no prior assumption on the way it was generated. This generalization is also known as Barlow's minimal redundancy representation problem and is considered an <phrase>open problem</phrase>. We propose several theorems and show that this NP hard problem can be accurately solved with a branch and bound search <phrase>tree</phrase> <phrase>algorithm</phrase>, or tightly approximated with a series of linear problems. Our contribution provides the first efficient and constructive set of solutions to Barlow's problem. The minimal redundancy representation (also known as <phrase>factorial</phrase> code) has many applications, mainly in the fields of <phrase>Neural Networks</phrase> and <phrase>Deep Learning</phrase>. The <phrase>Binary</phrase> ICA (BICA) is also shown to have applications in several domains including <phrase>medical diagnosis</phrase>, multi-cluster assignment, network tomography and <phrase>internet</phrase> resource <phrase>management</phrase>. In this work we show this formulation further applies to multiple disciplines in source coding such as predictive coding, distributed source coding and coding of large <phrase>alphabet</phrase> sources.
Speech enhancement based on deep denoising autoencoder We previously have applied deep autoencoder (DAE) for <phrase>noise reduction</phrase> and speech enhancement. However, the DAE was trained using only clean speech. In this study, we further introduce an explicit denoising process in learning the DAE. In training the DAE, we still adopt greedy layer-wised pretraining plus <phrase>fine tuning</phrase> strategy. In pretraining, each layer is trained as a one <phrase>hidden layer</phrase> neural autoencoder (AE) using noisy-clean speech pairs as input and output (or transformed noisy-clean speech pairs by preceding <phrase>AEs</phrase>). <phrase>Fine tuning</phrase> was done by stacking all <phrase>AEs</phrase> with pretrained parameters for initialization. The trained DAE is used as a filter for speech estimation when noisy speech is given. Speech enhancement experiments were done to examine the performance of the trained denoising DAE. <phrase>Noise reduction</phrase> , speech <phrase>distortion</phrase>, and <phrase>perceptual</phrase> evaluation of speech quality (PESQ) criteria are used in the performance evaluations. <phrase>Experimental</phrase> <phrase>results</phrase> show that adding depth of the DAE consistently increase the performance when a large <phrase>training data</phrase> set is given. In addition, compared with a minimum mean square error based speech enhancement <phrase>algorithm</phrase>, our proposed denois-ing DAE provided <phrase>superior</phrase> performance on the three objective evaluations.
The Use of Goals to Surface Requirements for Evolving Systems This <phrase>paper</phrase> addresses the use of goals to surface requirements for the redesign of existing or legacy systems. Goals are widely recognized as important precursors to system requirements, but the process of identifying and abstracting them has not been researched thoroughly. We present a summary of a goal-based method (GBRAM) for uncovering hidden issues, goals, and requirements and illustrate its application to a commercial system, an <phrase>Intranet</phrase>-based <phrase>electronic commerce</phrase> application, evaluating the method in the process. The core techniques comprising GBRAM are the systematic application of heuristics and inquiry questions for the analysis of goals, scenarios and obstacles. We conclude by discussing the <phrase>lessons learned</phrase> through applying goal refinement in the field and the implications for <phrase>future research</phrase>. 1 INTRODUCTION Obtaining requirements for <phrase>software</phrase> systems is a conceptually and practically complex activity. In particular, the amount of <phrase>communication</phrase> that is required among stakeholders and analysts often leads to misunderstandings. Stakeholders may forget or not be aware of requirements, and analysts, without the deep <phrase>domain knowledge</phrase> of most stakeholders, may not be able to fill in the gaps [CKI88]. In addition, some requirements and constraints may be so obvious to an expert stakeholder that they do not seem worth mentioning, but the thin spread of domain expertise among analyst teams means that these unstated requirements may not be recognized or incorporated into the development process.
<phrase>Bootstrapping</phrase> Deep Lexical Resources: Resources for Courses We propose a <phrase>range</phrase> of deep lexical acquisition methods which make use of morphological , <phrase>syntactic</phrase> and <phrase>ontological</phrase> <phrase>language</phrase> resources to <phrase>model</phrase> word similarity and bootstrap from a <phrase>seed</phrase> <phrase>lexicon</phrase>. The different methods are deployed in learning lexical items for a precision <phrase>grammar</phrase> , and shown to each have strengths and weaknesses over different word classes. A particular focus of this <phrase>paper</phrase> is the relative accessibility of different <phrase>language</phrase> resource types, and predicted " bang for the buck " associated with each in deep lexical acquisition applications.
Redocumentation of a legacy <phrase>banking</phrase> system: an experience <phrase>report</phrase> Successful <phrase>software</phrase> systems need to be maintained. In <phrase>order</phrase> to do that, deep <phrase>knowledge</phrase> about their <phrase>architecture</phrase> and implementation details is required. This <phrase>knowledge</phrase> is often kept implicit (inside the heads of the experts) and sometimes made explicit in documentation. The problem is that systems often lack up-to-date documentation and that system experts are frequently unavailable (as they got another job or retired). Redocumentation addresses that problem by recovering <phrase>knowledge</phrase> about the system and making it explicit in documentation. Automating the redocumentation process can limit the tedious and error-prone manual effort, but it is no '<phrase>silver bullet</phrase>'. In this <phrase>paper</phrase>, we <phrase>report</phrase> on our experience with applying redocumentation techniques in <phrase>industry</phrase>. We provide insights on what (not) to document, what (not) to automate and how to automate it. A <phrase>concrete</phrase> lesson learned during this study is that the "less is more" principle also applies to redocumentation.
<phrase>Knowledge</phrase> <phrase>Labels</phrase> and their Correlates in an Asynchronous <phrase>Text-Based</phrase> <phrase>Computer-Supported</phrase> <phrase>Collaborative Learning</phrase> Environment: Who Uses and Who Benefits? Learning is a complex process involving <phrase>knowledge</phrase> acquisition, transformation, and creation. This <phrase>paper</phrase> focuses on how students' beliefs about and use of learning scaffolds relate to their characteristics, and the specific context in which they are expected to use these <phrase>labels</phrase>. An asynchronous <phrase>text-based</phrase> online environment with built-in learning scaffolds (called " <phrase>knowledge</phrase> <phrase>labels</phrase> ") was the context in this study. Hierarchical <phrase>regression</phrase> revealed a <phrase>range</phrase> of factors <phrase>accounting</phrase> for a significant amount of <phrase>variance</phrase> in the students' beliefs about the usefulness and usage of <phrase>labels</phrase>. Their beliefs about the usefulness of <phrase>labels</phrase> correlated with factors describing a deeper learning approach, more positive course <phrase>learning experiences</phrase>, and deeper engagement with <phrase>online learning</phrase> discourse. Answers about using <phrase>labels</phrase> were mainly related to greater participation in <phrase>online learning</phrase> and the students' deeper engagement in <phrase>online learning</phrase> discourse. This finding suggests a need for deep investigations into the complex interaction between students' personal characteristics and learning processes to understand the value of learning scaffolds.
Peer instruction: a teaching method to foster <phrase>deep understanding</phrase> How the <phrase>computing</phrase> <phrase>education</phrase> <phrase>community</phrase> can learn from <phrase>physics</phrase> <phrase>education</phrase>.
Predicting Moves in <phrase>Chess</phrase> Using <phrase>Convolutional Neural Networks</phrase> We used a three layer <phrase>Convolutional Neural Network</phrase> (<phrase>CNN</phrase>) to make move predictions in <phrase>chess</phrase>. The task was defined as a two-part <phrase>classification problem</phrase>: a piece-selector <phrase>CNN</phrase> is trained to score which <phrase>white</phrase> pieces should be made to move, and move-selector CNNs for each piece produce scores for where it should be moved. This approach reduced the intractable class space in <phrase>chess</phrase> by a <phrase>square root</phrase>. The networks were trained using 20,000 <phrase>games</phrase> consisting of 245,000 moves made by players with an <phrase>ELO rating</phrase> higher than 2000 from the <phrase>Free</phrase> <phrase>Internet</phrase> <phrase>Chess</phrase> Server. The piece-selector network was trained on all of these moves, and the move-selector <phrase>networks trained</phrase> on all moves made by the respective piece. <phrase>Black</phrase> moves were trained on by using a <phrase>data</phrase> augmentation to frame it as a move made by the <phrase>white</phrase> side. The networks were validated against a dataset 20% the size of the <phrase>training data</phrase>. Our best <phrase>model</phrase> for the piece selector network <phrase>produced</phrase> a validation accuracy of 38.3%, and the move-selector networks for the pawn, rook, <phrase>knight</phrase>, <phrase>bishop</phrase>, <phrase>queen</phrase>, and <phrase>king</phrase> performed at 52. The success of the convo-lutions in our <phrase>model</phrase> are reflected in how pieces that move locally perform better than those that move globally. The network was played as an <phrase>AI</phrase> against the <phrase>Sunfish</phrase> <phrase>Chess Engine</phrase>, <phrase>drawing</phrase> with 26 <phrase>games</phrase> out of 100 and losing the rest. We recommend that <phrase>convolution</phrase> layers in <phrase>chess</phrase> <phrase>deep learning</phrase> approaches are useful in <phrase>pattern recognition</phrase> of small, local tactics and that this approach should be trained on and composed with evaluation functions for smarter overall <phrase>play</phrase>.
Eliciting reactive and reflective <phrase>feedback</phrase> for a social <phrase>communication</phrase> tool: a multi-session approach Gaining <phrase>feedback</phrase> from users early in the <phrase>design</phrase> of a complex, novel social system poses unique challenges. We <phrase>report</phrase> on our multi-session, in-context approach to get users to envision how they would use an early <phrase>prototype</phrase> in everyday <phrase>life</phrase>, combined with projections of how their <phrase>friends</phrase> would use it. The <phrase>prototype</phrase> is a novel social <phrase>communication</phrase> <phrase>management</phrase> tool and we required users develop a <phrase>deep understanding</phrase> of the complete system over time. Findings from <phrase>data</phrase> collected across four sessions show that using personalised task scenarios and giving users longer exposure to an early interactive <phrase>prototype</phrase>, combined with <phrase>peer-to-peer</phrase> discussion, enables participants to move beyond initial reactions to develop more reflective opinions. Participants were able to overcome first impressions and learning effects, develop <phrase>deeper understanding</phrase> of new conceptual models underpinning the system, integrate their understanding of piecemeal components and reflect on own use and use by others in deeper ways.
Convolutional-Recursive <phrase>Deep Learning</phrase> for 3D Object Classification <phrase>Recent advances</phrase> in 3D sensing technologies make it possible to easily record color and depth images which together can improve <phrase>object recognition</phrase>. Most current methods rely on very well-designed features for this new 3D modality. We introduce a <phrase>model</phrase> based on a combination of convolutional and recursive <phrase>neural networks</phrase> (<phrase>CNN</phrase> and RNN) for learning features and classifying RGB-D images. The <phrase>CNN</phrase> layer learns <phrase>low-level</phrase> translationally <phrase>invariant features</phrase> which are then given as inputs to multiple, fixed-<phrase>tree</phrase> RNNs in <phrase>order</phrase> to <phrase>compose</phrase> <phrase>higher order</phrase> features. RNNs can be seen as combining <phrase>convolution</phrase> and pooling into one efficient, hierarchical operation. Our main result is that even RNNs with random weights <phrase>compose</phrase> powerful features. Our <phrase>model</phrase> obtains <phrase>state</phrase> of the <phrase>art</phrase> performance on a standard RGB-D object dataset while being more accurate and faster during training and testing than comparable architectures such as two-layer CNNs.
<phrase>Procedural Knowledge</phrase> Extraction on <phrase>MEDLINE</phrase> Abstracts <phrase>Text mining</phrase> is a popular methodology for building <phrase>Technology</phrase> <phrase>Intelligence</phrase> which helps companies or organizations to make better decisions by providing <phrase>knowledge</phrase> about the <phrase>state</phrase>-of-the-<phrase>art</phrase> technologies obtained from the <phrase>Internet</phrase> or inside companies. As a <phrase>matter</phrase> of fact, the objects or events (so-called declarative <phrase>knowledge</phrase>) are the <phrase>target</phrase> <phrase>knowledge</phrase> that text miners want to catch in <phrase>general</phrase>. However, we propose how to extract <phrase>procedural knowledge</phrase> rather than declarative <phrase>knowledge</phrase> utilizing <phrase>machine learning</phrase> method with deep <phrase>language</phrase> processing features, as well as how to <phrase>model</phrase> it. We show the representation of <phrase>procedural knowledge</phrase> in <phrase>MEDLINE</phrase> abstracts and provide experiments that are quite promising in that it shows 82% and 65% performances of extracting purpose/solutions (two components of <phrase>procedural knowledge</phrase> <phrase>model</phrase>) extraction and unit process (<phrase>basic</phrase> unit of purpose/solutions) identification respectively even though we applied strict guidelines in evaluating the performance. 1 Introduction <phrase>Technology</phrase> <phrase>Intelligence</phrase> is an activity helping companies or organizations to make better decisions by gathering and providing <phrase>information</phrase> about the <phrase>state</phrase>-of-the-<phrase>art</phrase> technologies [1]. Recently, the systems supporting <phrase>Technology</phrase> <phrase>Intelligence</phrase> have been actively developed to assist researchers and practitioners to make strategic <phrase>technology</phrase> plans [2]. Usually, these systems import <phrase>text mining</phrase> methodologies to analyze tacit <phrase>information</phrase> inside <phrase>company</phrase> or on the <phrase>Internet</phrase>. However, they focused on extracting declarative <phrase>knowledge</phrase>, which describes objects and events by specifying the properties which characterize them; it does not pay attention to the actions needed to obtain a result, but only on its properties [3]. Therefore, we proposed a methodology that enables to build <phrase>procedural knowledge</phrase> using <phrase>text mining</phrase> technique based on deep <phrase>language</phrase> processing. In <phrase>general</phrase>, <phrase>procedural knowledge</phrase> has been considered as <phrase>knowledge</phrase> of how to do something or <phrase>knowledge</phrase> of skills [4]. It is contrasted with <phrase>propositional</phrase> <phrase>knowledge</phrase> or declarative
<phrase>Deep Belief</phrase> Networks for Real-Time Extraction of <phrase>Tongue</phrase> Contours from <phrase>Ultrasound</phrase> During Speech <phrase>Ultrasound</phrase> has become a useful tool for speech scientists studying mechanisms of <phrase>language</phrase> <phrase>sound</phrase> <phrase>production</phrase>. <phrase>State</phrase>-of-the-<phrase>art</phrase> methods for extracting <phrase>tongue</phrase> contours from <phrase>ultrasound</phrase> images of the <phrase>mouth</phrase>, typically based on active contour <phrase>snakes</phrase> [3], require considerable manual interaction by an expert <phrase>linguist</phrase>. In this <phrase>paper</phrase> we describe a novel method for fully automatic extraction of <phrase>tongue</phrase> contours based on a hierarchy of <phrase>restricted Boltzmann machines</phrase> (RBMs), i.e. <phrase>deep belief</phrase> networks (DBNs) [2]. Usually, DBNs are first trained generatively on <phrase>sensor</phrase> <phrase>data</phrase>, then discriminatively to predict <phrase>human</phrase>-provided <phrase>labels</phrase> of the <phrase>data</phrase>. In this <phrase>paper</phrase> we introduce the translational RBM (tRBM), which allows the DBN to make use of both <phrase>human</phrase> <phrase>labels</phrase> and raw <phrase>sensor</phrase> <phrase>data</phrase> at all stages of learning. This <phrase>method achieves</phrase> performance comparable to <phrase>human</phrase> labelers, without any temporal smoothing or <phrase>human</phrase> intervention required at runtime.
Hierarchical <phrase>Extreme Learning</phrase> Machine for unsupervised representation learning Learning representations from massive unlabelled <phrase>data</phrase> is a topic for <phrase>high</phrase>-level tasks in many applications. The recent great improvements on benchmark <phrase>data</phrase> sets, which are achieved by increasingly complex <phrase>unsupervised learning</phrase> methods and <phrase>deep learning</phrase> models with many parameters, usually requiring many tedious tricks and much expertise to tune. Additionally, the filters learned by these complex architectures are quite similar to standard <phrase>hand-crafted</phrase> <phrase>visual features</phrase>, and training to fine-tune the weights of <phrase>deep architectures</phrase> requires a <phrase>long</phrase> time. In this <phrase>paper</phrase>, the <phrase>Extreme Learning</phrase> Machine-<phrase>Auto Encoder</phrase> (<phrase>ELM</phrase>-AE) is employed as the learning unit to learn <phrase>local receptive</phrase> fields at each layer, and the <phrase>lower</phrase> layer responses are transferred to the last layer (trans-layer) to form a more complete representation to retain more <phrase>information</phrase>. In addition, some beneficial methods in <phrase>deep learning</phrase> architectures such as local contrast normalization and whitening are added to the implemented hierarchical <phrase>Extreme Learning</phrase> Machine networks to further boost the performance. The resulting trans-layer representations are processed into block histograms with <phrase>binary</phrase> <phrase>hashing</phrase> to produce <phrase>translation</phrase> and rotation invariant representations, which are utilized to do <phrase>high</phrase>-level tasks such as recognition and detection. The proposed trans-layer representation method with <phrase>ELM</phrase>-AE based learning of <phrase>local receptive</phrase> filters was tested on the MNIST digit recognition <phrase>data set</phrase>, including MNIST variations, and on the <phrase>Caltech</phrase> 101 <phrase>object recognition</phrase> <phrase>database</phrase>. Compared to traditional <phrase>deep learning</phrase> methods, the proposed <phrase>ELM</phrase>-AE based system has a much faster learning speed and attains 65.97% accuracy on the <phrase>Caltech</phrase> 101 task (15 samples per class) and 99.45% on the standard MNIST <phrase>data set</phrase>.
<phrase>Prefrontal</phrase> pathways <phrase>target</phrase> <phrase>excitatory</phrase> and inhibitory systems in <phrase>memory</phrase>-related medial temporal cortices The <phrase>anterior cingulate cortex</phrase> (<phrase>ACC</phrase>), situated in the caudal part of the <phrase>medial prefrontal cortex</phrase>, is involved in monitoring on-going behavior pertaining to <phrase>memory</phrase> of previously learned outcomes. How <phrase>ACC</phrase> <phrase>information</phrase> interacts with the <phrase>medial temporal lobe</phrase> (MTL) <phrase>memory</phrase> system is not well understood. The present study used a multitiered approach to address two questions on the interactions between the <phrase>ACC</phrase> and the parahippocampal cortices in the <phrase>rhesus monkey</phrase>: (1) What are the <phrase>presynaptic</phrase> characteristics of <phrase>ACC</phrase> projections to the parahippocampal cortices? (2) What are the postsynaptic targets of the pathway and are there <phrase>laminar</phrase> differences in innervation of local <phrase>excitatory</phrase> and inhibitory systems? Labeled <phrase>ACC</phrase> terminations were quantified in parahippocampal areas TH and TF and a <phrase>cluster analysis</phrase> showed that boutons varied in size, with a <phrase>population</phrase> of small (0.97 m) and large (>0.97 m) terminations that were nearly evenly distributed in the <phrase>upper</phrase> and deep layers. Exhaustive sampling as well as unbiased stereological techniques independently showed that small and large boutons were about evenly distributed within <phrase>cortical</phrase> layers in the parahippocampal <phrase>cortex</phrase>. <phrase>Synaptic</phrase> analysis of the pathway, performed at the <phrase>electron microscope</phrase> (EM), showed that while most of the <phrase>ACC</phrase> projections formed <phrase>synapses</phrase> with <phrase>excitatory</phrase> <phrase>neurons</phrase>, a significant proportion (23%) targeted presumed inhibitory classes with a preference for <phrase>parvalbumin</phrase> (PV+) inhibitory <phrase>neurons</phrase>. These findings suggest <phrase>synaptic</phrase> mechanisms that may help integrate signals associated with attention and <phrase>memory</phrase>.
Attribute Recognition from Adaptive Parts Previous part-based attribute recognition approaches perform part detection and attribute recognition in separate steps. The parts are not optimized for attribute recognition and therefore could be sub-optimal. We present an <phrase>end-to-end</phrase> <phrase>deep learning</phrase> approach to overcome the limitation. It generates object parts from key points and perform attribute recognition accordingly, allowing adaptive spatial transform [10] of the parts. Both key point estimation and attribute recognition are learnt jointly in a <phrase>multi-task</phrase> setting. <phrase>Extensive experiments</phrase> on two datasets verify the efficacy of proposed <phrase>end-to-end</phrase> approach.
Extensions of <phrase>Laplacian</phrase> Eigenmaps for <phrase>Manifold</phrase> Learning Extensions of <phrase>Laplacian</phrase> Eigenmaps for <phrase>Manifold</phrase> Learning This <phrase>thesis</phrase> deals with the theory and practice of <phrase>manifold</phrase> learning, especially as they relate to the problem of classification. We begin with a well known <phrase>algorithm</phrase>, <phrase>Laplacian</phrase> Eigenmaps, and then proceed to extend it in two <phrase>independent</phrase> directions. First, we generalize this <phrase>algorithm</phrase> to allow for the use of partially <phrase>labeled data</phrase>, and establish the theoretical foundation of the resulting <phrase>semi-supervised</phrase> learning method. Second, we consider two ways of accelerating the most computationally intensive step of <phrase>Laplacian</phrase> Eigenmaps, the <phrase>construction</phrase> of an adjacency <phrase>graph</phrase>. Both of them produce <phrase>high</phrase> quality approximations, and we conclude by showing that they work well together to achieve a dramatic reduction in computational time. Dedication To my mother and father, always. <phrase>ii</phrase> Acknowledgments It is a pure pleasure to acknowledge the many people who have helped me make it to the end of this <phrase>long</phrase> journey. Most students are lucky to find one decent advisor. For the last three years, I have been incredibly privileged to have three <phrase>mathematical</phrase> powerhouses lighting my way. who stopped me from making what might have been the biggest mistake of my <phrase>life</phrase>, and warmly invited me to join the distinguished <phrase>Norbert Wiener</phrase> <phrase>Center</phrase> <phrase>family</phrase>. Since then, he has extended me nothing but encouraging words (and a rare slap on the wrist when I deserved one). With his broad vision and deep insight, he has inspired me continuously. With his boisterous laughter, he made my many hours in the lab next to his office amusing. And like a godfather and mother hen to all of us, he has seen me through, kindly and elegantly, on to the next stage of my <phrase>life</phrase>. Wojtek has been my <phrase>coach</phrase>, guiding me along one hurdle and, more recently, one comma at a time. With his quick instincts, he has shown me how to move efficiently from theory to applications and back. Along the way he has challenged me with interesting questions and helped me evaluate and improve the <phrase>fruit</phrase> of my <phrase>labor</phrase>. He also taught me the truth about deadlines: They do not exist. And at the end of every meeting, he never forgot to ask how I am, as if to say, don't forget to breathe. Radu has spent countless hours patiently discussing with me a wide <phrase>spectrum</phrase> iii of ideas, from the purest <phrase>results</phrase> of time-<phrase>frequency</phrase> analysis and frame theory, down to the very lines of code bringing 
<phrase>Deep Learning</phrase> with Kernel Regularization for Visual Recognition In this <phrase>paper</phrase> we aim to <phrase>train</phrase> <phrase>deep neural networks</phrase> for rapid visual recognition. The task is highly challenging, largely due to the lack of a meaningful regular-izer on the functions realized by the networks. We propose a novel regularization method that takes advantage of <phrase>kernel methods</phrase>, where a given kernel <phrase>function</phrase> represents <phrase>prior knowledge</phrase> about the recognition task of interest. We derive an efficient <phrase>algorithm</phrase> using <phrase>stochastic gradient descent</phrase>, and demonstrate encouraging <phrase>results</phrase> on a wide <phrase>range</phrase> of <phrase>recognition tasks</phrase>, in terms of both accuracy and speed.
Learning to Assemble Classifiers via <phrase>Genetic Programming</phrase> This article introduces a novel approach for building heterogeneous ensembles based on <phrase>genetic programming</phrase> (GP). Ensemble learning is a <phrase>paradigm</phrase> that aims at combining individual classifiers outputs to improve their performance. Commonly, classifiers outputs are combined by a weighted sum or a voting strategy. However, linear <phrase>fusion</phrase> functions may not effectively exploit individual models' redundancy and diversity. In this <phrase>research</phrase>, a GP <phrase>based approach</phrase> to learn <phrase>fusion</phrase> functions that combine classifiers outputs is proposed. Heterogeneous ensembles are aimed in this study, these models use individual classifiers which are based on different principles (e.g., <phrase>decision trees</phrase> and similarity-based techniques). A detailed empirical assessment is carried out to validate the effectiveness of the <phrase>proposed approach</phrase>. <phrase>Results</phrase> show that the <phrase>proposed method</phrase> is successful at building very effective classification models, outperforming <phrase>alternative</phrase> ensemble methodologies. The proposed ensemble technique is also applied to fuse homogeneous models' outputs with <phrase>results</phrase> also showing its effectiveness. Therefore, an in deep analysis from different perspectives of the proposed strategy to build ensembles is presented with a strong <phrase>experimental</phrase> support.
Experience <phrase>report</phrase>: a multi-classroom <phrase>report</phrase> on the value of peer instruction Peer Instruction (PI) has a significant following in <phrase>physics</phrase>, <phrase>biology</phrase>, and <phrase>chemistry</phrase> <phrase>education</phrase>. Although many CS educators are aware of PI as a <phrase>pedagogy</phrase>, the <phrase>adoption</phrase> rate in CS is low. This <phrase>paper</phrase> reports on four instructors with varying motivations and course contexts and the value they found in adopting PI. Although there are many documented benefits of PI for students (e.g. increased learning), here we describe the experience of the instructor by looking in detail at one particular question they posed in class. Through discussion of the instructors' experiences in their classrooms, we support educators in consideration of whether they would like to have similar classroom experiences. Our primary findings show instructors appreciate that PI assists students in addressing course concepts at a deep level, assists instructors in dynamically adapting their class to address <phrase>student</phrase> misunderstandings and, overall, that PI encourages students to be engaged in conversations which help build <phrase>technical communication</phrase> skills. We propose that using PI to engage students in these activities can effectively support training in analysis and teamwork skills.
Using <phrase>decision trees</phrase> to study the convergence of <phrase>phylogenetic</phrase> analyses In this <phrase>paper</phrase>, we explore the novel use of <phrase>decision trees</phrase> to study the convergence properties of <phrase>phylogenetic</phrase> analyses. A decision learning <phrase>tree</phrase> is constructed from the <phrase>evolutionary</phrase> relationships (or bipartitions) found in the <phrase>evolutionary</phrase> <phrase>trees</phrase> returned from a <phrase>phylogenetic</phrase> analysis. We treat <phrase>evolutionary</phrase> <phrase>trees</phrase> returned from multiple <phrase>runs</phrase> of a <phrase>phylogenetic</phrase> analysis as different classes. Then, we use the depth of a <phrase>decision tree</phrase> as a technique to measure how distinct the <phrase>runs</phrase> are from each other. <phrase>Decision trees</phrase> with shallow depth reflect non-convergence since the <phrase>evolutionary</phrase> <phrase>trees</phrase> can be classified with little <phrase>information</phrase>. Deep <phrase>decision tree</phrase> depths reflect convergence. We study <phrase>Bayesian</phrase> and maximum <phrase>parsimony</phrase> <phrase>phylogenetic</phrase> analyses consisting of thousands of <phrase>trees</phrase>. For some datasets studied here, a <phrase>single</phrase> distinguishing bipartition can classify the entire <phrase>tree</phrase> collection suggesting non-convergence of the underlying <phrase>phylogenetic</phrase> analysis. Thus, we believe that <phrase>decision trees</phrase> <phrase>lead</phrase> to new insights with the potential for helping <phrase>biologists</phrase> reconstruct more robust <phrase>evolutionary</phrase> <phrase>trees</phrase>.
Handwritten <phrase>Character Recognition</phrase> by Alternately Trained Relaxation <phrase>Convolutional Neural Network</phrase> <phrase>Deep learning</phrase> methods have recently achieved impressive performance in the <phrase>area</phrase> of visual recognition and <phrase>speech recognition</phrase>. In this <phrase>paper</phrase>, we propose a <phrase>handwriting recognition</phrase> method based on relaxation <phrase>convolutional neural network</phrase> (R-<phrase>CNN</phrase>) and alternately trained relaxation <phrase>convolutional neural network</phrase> (ATR-<phrase>CNN</phrase>). Previous methods regularize <phrase>CNN</phrase> at full-connected layer or spatial-pooling layer, however, we focus on convolutional layer. The relaxation <phrase>convolution</phrase> layer adopted in our R-<phrase>CNN</phrase>, unlike traditional convolutional layer, does not require <phrase>neurons</phrase> within a feature map to share the same convolutional kernel, endowing the <phrase>neural network</phrase> with more expressive power. As relaxation <phrase>convolution</phrase> sharply increase the total number of parameters, we adopt alternate training in ATR-<phrase>CNN</phrase> to regularize the <phrase>neural network</phrase> during training procedure. Our previous C-NN took the 1st place in ICDAR'13 <phrase>Chinese</phrase> Handwriting <phrase>Character Recognition</phrase> Competition, while our latest ATR-<phrase>CNN</phrase> outperforms our previous one and achieves the <phrase>state</phrase>-of-the-<phrase>art</phrase> accuracy with an <phrase>error rate</phrase> of 3.94%, further narrowing the gap between machine and <phrase>human</phrase> observers (3.87%).
<phrase>Perception</phrase> Processing for <phrase>General</phrase> <phrase>Intelligence</phrase>: Bridging the Symbolic/Subsymbolic Gap Bridging the gap between symbolic and subsymbolic representations is a perhaps the key obstacle along the path from the present <phrase>state</phrase> of <phrase>AI</phrase> achievement to <phrase>human</phrase>-level <phrase>artificial general intelligence</phrase>. One approach to bridging this gap is hybridization for instance, incorporation of a subsymbolic system and a symbolic system into a in-tegrative <phrase>cognitive architecture</phrase>. Here we present a detailed <phrase>design</phrase> for an implementation of this approach, via integrating a version of the <phrase>DeSTIN</phrase> <phrase>deep learning</phrase> system into OpenCog, an integrative <phrase>cognitive architecture</phrase> including rich symbolic capabilities. This is a " tight " integration , in which the symbolic and subsymbolic aspects exert detailed real-time influence on each others' operations. An earlier technical <phrase>report</phrase> has described in detail the revisions to <phrase>DeSTIN</phrase> needed to support this integration, which are mainly along the lines of making it more " representationally transparent, " so that its internal states are easier for OpenCog to understand.
Relevance and ranking in <phrase>online dating</phrase> systems Match-making systems refer to systems where users want to meet other individuals to satisfy some underlying need. Examples of match-making systems include dating services, resume/job bulletin boards, <phrase>community</phrase> based <phrase>question answering</phrase>, and <phrase>consumer</phrase>-to-<phrase>consumer</phrase> marketplaces. One fundamental component of a match-making system is the retrieval and ranking of candidate matches for a given user. We present the first in-depth study of <phrase>information retrieval</phrase> approaches applied to match-making systems. Specifically, we focus on retrieval for a dating service. This domain offers several unique problems not found in traditional <phrase>information retrieval</phrase> tasks. These include two-sided relevance, very subjective relevance, extremely few relevant matches, and structured queries. We propose a machine learned ranking <phrase>function</phrase> that makes use of features extracted from the uniquely rich user profiles that consist of both structured and unstructured attributes. An extensive evaluation carried out using <phrase>data</phrase> gathered from a real <phrase>online dating service</phrase> shows the benefits of our proposed methodology with respect to traditional match-making baseline systems. Our analysis also provides deep insights into the aspects of match-making that are particularly important for producing highly relevant matches.
Seeing Invisible Poses: Estimating 3D Body Pose from Egocentric <phrase>Video</phrase> Understanding the <phrase>camera</phrase> wearer's activity is central to egocentric vision, yet one key facet of that activity is inherently invisible to the <phrase>camera</phrase> the wearer's body pose. Prior work focuses on estimating the pose of hands and <phrase>arms</phrase> when they come into view, but this 1) gives an incomplete view of the full body posture, and 2) prevents any pose estimate at all in many frames, since the hands are only visible in a fraction of <phrase>daily</phrase> <phrase>life</phrase> activities. We propose to infer the " invisible pose " of a person behind the egocentric <phrase>camera</phrase>. Given a <phrase>single</phrase> <phrase>video</phrase>, our efficient learning-<phrase>based approach</phrase> returns the full body 3D joint positions for each frame. Our method exploits cues from the dynamic motion signatures of the surrounding scenewhich changes predictably as a <phrase>function</phrase> of body poseas well as static scene structures that reveal the viewpoint (e.g., sitting vs. standing). We further introduce a novel <phrase>energy</phrase> minimization scheme to infer the pose <phrase>sequence</phrase>. It uses soft predictions of the poses per time instant together with a non-parametric <phrase>model</phrase> of <phrase>human</phrase> pose dynamics over longer <phrase>windows</phrase>. Our method outperforms an array of possible alternatives, including <phrase>deep learning</phrase> approaches for direct pose <phrase>regression</phrase> from images.
Bottleneck features based on gammatone <phrase>frequency</phrase> cepstral coefficients Recent work demonstrates impressive success of the bottleneck (BN) feature in <phrase>speech recognition</phrase>, particularly with deep networks plus appropriate <phrase>pre-training</phrase>. A widely admitted advantage associated with the BN feature is that the network structure can learn multiple environmental conditions with abundant <phrase>training data</phrase>. For tasks with limited <phrase>training data</phrase>, however, this multi-condition training is unavailable, and so the networks tend to be over-fitted and sensitive to <phrase>acoustic</phrase> condition changes. A possible <phrase>solution</phrase> is to base the BN features on a <phrase>channel</phrase>-robust primary feature. In this <phrase>paper</phrase>, we propose to derive the BN feature based on Gammatone <phrase>frequency</phrase> cepstral coefficients (GFCCs). The GFCC feature has shown <phrase>nice</phrase> robustness against <phrase>acoustic</phrase> change, due to its capability of simulating the auditory system of humans. The idea is to integrate the advantage of the GFCC feature in <phrase>acoustic</phrase> robustness and the advantage of the BN feature in signal representation, so that the BN feature can be improved in the condition of mismatched training/<phrase>test</phrase> channels. This is particularly useful for <phrase>small-scale</phrase> tasks for which the <phrase>training data</phrase> are often limited. The experiments are conducted on the WSJCAM0 <phrase>database</phrase>, where the <phrase>test</phrase> utterances are mixed with noises at various SNR levels to simulate the <phrase>channel</phrase> change. The <phrase>results</phrase> confirm that the GFCC-based BN feature is much more robust than the BN features based on the MFCC and the PLP. Furthermore, the primary GFCC feature and the GFCC-based BN feature can be concatenated, leading to a more robust combined feature which provides considerable performance gains in all the tested noise conditions.
Use of kernel deep convex networks and <phrase>end-to-end</phrase> learning for <phrase>spoken language</phrase> understanding We present our recent and ongoing work on applying <phrase>deep learning</phrase> techniques to <phrase>spoken language</phrase> understanding (SLU) problems. The previously developed deep convex network (DCN) is extended to its kernel version (K-DCN) where the number of <phrase>hidden units</phrase> in each DCN layer approaches <phrase>infinity</phrase> using the kernel trick. We <phrase>report</phrase> <phrase>experimental</phrase> <phrase>results</phrase> demonstrating dramatic error reduction achieved by the K-DCN over both the Boosting-based baseline and the DCN on a domain classification task of SLU, especially when a highly correlated set of features extracted from search query click logs are used. Not only can DCN and K-DCN be used as a domain or intent classifier for SLU, they can also be used as local, discriminative feature extractors for the slot filling task of SLU. The interface of K-DCN to slot filling systems via the softmax <phrase>function</phrase> is presented. Finally, we outline an <phrase>end-to-end</phrase> learning strategy for training the softmax parameters (and potentially all DCN and K-DCN parameters) where the learning objective can take any performance measure (e.g. the F-measure) for the full SLU system.
Gated <phrase>Feedback</phrase> <phrase>Recurrent Neural Networks</phrase> In this work, we propose a novel recurrent <phrase>neu</phrase>-ral network (RNN) <phrase>architecture</phrase>. The proposed RNN, gated-<phrase>feedback</phrase> RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from <phrase>upper</phrase> recurrent layers to <phrase>lower</phrase> layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, <phrase>long</phrase> <phrase>short-term memory</phrase> and gated recurrent units, on the tasks of character-level <phrase>language</phrase> modeling and <phrase>Python</phrase> <phrase>program evaluation</phrase>. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.
Optimal Coding in Biological and <phrase>Artificial Neural Networks</phrase> <phrase>Feature representations</phrase> in both, biological <phrase>neural networks</phrase> in the <phrase>primate</phrase> ventral <phrase>stream</phrase> and <phrase>artificial</phrase> <phrase>convolutional neural networks</phrase> trained on <phrase>object recognition</phrase>, incresase in complexity and <phrase>receptive field</phrase> size with layer depth. Somewhat strikingly, <phrase>empirical evidence</phrase> indicates that this analogy extends to the specific representations learned in each layer. This suggests that biological and <phrase>artificial neural networks</phrase> share a fundamental organising principle. We shed <phrase>light</phrase> on this principle in the framework of optimal coding. Specifically, we first investigate which properties of a code render it robust to transmission over noisy channels and formally prove that for equientropic channels an <phrase>upper</phrase> bound on the expected minimum decoding error is attained for codes with maximum marginal <phrase>entropy</phrase>. We then show that the pairwise correlation of units in a deep layer of a <phrase>neural network</phrase>, that has been trained on an <phrase>object recognition</phrase> task, increases when perturbing the distribution of input images, i. <phrase>e</phrase>., that the network exhibits properties of an optimally coding system. By analogy, this suggests that the <phrase>layer-wise</phrase> similarity of <phrase>feature representations</phrase> in biological and <phrase>artificial neural networks</phrase> is a result of optimal coding that enables robust transmission of object <phrase>information</phrase> over noisy channels. Because we find that in equientropic channels the <phrase>upper</phrase> bound on the expected minimum decoding error is <phrase>independent</phrase> of the class-conditional <phrase>entropy</phrase>, our work further provides a plausible explanation why optimal codes can be learned in unsupervised settings.
Optimizing <phrase>search engine</phrase> <phrase>revenue</phrase> in sponsored search Displaying sponsored ads alongside the search <phrase>results</phrase> is a key <phrase>monetization</phrase> strategy for <phrase>search engine</phrase> companies. Since users are more likely to click ads that are relevant to their query, it is crucial for <phrase>search engine</phrase> to deliver the right ads for the query and the <phrase>order</phrase> in which they are displayed. There are several works investigating on how to learn a ranking <phrase>function</phrase> to maximize the number of <phrase>ad</phrase> clicks. In this <phrase>paper</phrase>, we address a new <phrase>revenue</phrase> <phrase>optimization problem</phrase> and aim to answer the question: how to construct a ranking <phrase>model</phrase> that can deliver <phrase>high</phrase> quality ads to the user as well as maximize <phrase>search engine</phrase> <phrase>revenue</phrase>? We introduce two novel methods from di fferent <phrase>machine learning</phrase> perspectives, and both of them take the <phrase>revenue</phrase> component into careful considerations. The <phrase>algorithms</phrase> are built upon the click-through log <phrase>data</phrase> with real <phrase>ad</phrase> clicks and impressions. The extensively <phrase>experimental</phrase> <phrase>results</phrase> verify the <phrase>proposed algorithm</phrase> that can produce more <phrase>revenue</phrase> than other methods as well as avoid losing relevance accuracy. To provide deep insight into the importance of each feature to <phrase>search engine</phrase> <phrase>revenue</phrase>, we extract twelve <phrase>basic</phrase> features from four categories. The <phrase>experimental</phrase> study provides a feature ranking list according to the <phrase>revenue</phrase> benefit of each feature.
Category: <phrase>Learning Algorithms</phrase> Representational Power of <phrase>Restricted Boltzmann Machines</phrase> and <phrase>Deep Belief</phrase> Networks Background on Rbms and Dbns <phrase>Deep Belief</phrase> Networks (DBN) are <phrase>generative models</phrase> with many layers of hidden causal variables, recently introduced by Hinton et al, along with a <phrase>greedy layer-wise</phrase> <phrase>unsupervised learning</phrase> <phrase>algorithm</phrase>. The <phrase>building block</phrase> of a DBN is a <phrase>probabilistic model</phrase> called a <phrase>Restricted Boltzmann Machine</phrase> (RBM), used to represent one layer of the <phrase>model</phrase>. We show that RBMs are <phrase>universal</phrase> approximators of discrete <phrase>distributions</phrase>. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests another criterion for DBNs, obtained by considering that the top layer can perfectly fit its input. Introduction <phrase>Learning algorithms</phrase> that learn to represent functions with many levels of composition are said to have a <phrase>deep architecture</phrase>. Bengio and Le Cun (2007) point to <phrase>results</phrase> in computational theory of circuits to strongly suggest that <phrase>deep architectures</phrase> are much more efficient in terms of representation (number of computational elements, number of parameters) than shallow ones. Hinton, Osindero, and Teh (2006) introduced a <phrase>greedy layer-wise</phrase> <phrase>unsupervised learning</phrase> <phrase>algorithm</phrase> for <phrase>Deep Belief</phrase> Networks (DBN). The training strategy for such networks may hold great promise as a principle to help address the problem of training deep networks. <phrase>Upper</phrase> layers of a DBN are supposed to represent more " abstract " concepts that explain the input observation x, whereas <phrase>lower</phrase> layers extract " <phrase>low-level</phrase> features " from x.
<phrase>Book</phrase> Review: Ensemble Methods: Foundations and <phrase>Algorithms</phrase> 30 REVIEWED BY SURESH SOOD <phrase>Erving Goffman</phrase>, one of the most influential <phrase>sociologists</phrase> of the last century published the Presentation of Self in Everyday <phrase>Life</phrase> (PSEL;1959). This is the most definitive 20th century study of the patterns of <phrase>human</phrase> behavior in mundane social situations. If we look to <phrase>psychology</phrase>, <phrase>sociology</phrase>, <phrase>anthropology</phrase>, <phrase>marketing</phrase> or even organisational studies the representation of <phrase>human</phrase> behavior emerges from rich PSEL type " thick " qualitative descriptions. This approach is prevalent amongst the <phrase>social sciences</phrase> and does not provide a pathway to generalizable or predictive models of behavior. Furthermore, the availability of <phrase>big data</phrase> from <phrase>social networks</phrase> is pushing researchers and practitioners to undertake deep and dynamic behavioral analysis in the converging online and offline worlds. <phrase>Human</phrase> behavior is made up of complex interdependencies not least of all because individuals convey actions using the multiple modes of voice, facial and eye movements, hand gesturing and body to interact on a social basis. The modeling and analysis of <phrase>human</phrase> behavior is giving rise to the new discipline of behavior <phrase>computing</phrase> integrating techniques from both <phrase>computer science</phrase> and <phrase>social sciences</phrase>. In this new field of behavior <phrase>computing</phrase>, a <phrase>major</phrase> distinction over previous behavioral <phrase>research</phrase> is a focus on online <phrase>social networks</phrase> and the <phrase>Internet</phrase> impacting behavior rather than the traditional <phrase>experimental</phrase> analysis of the behavior of animals and organisms. The field of behavior <phrase>computing</phrase> opens up the opportunity for breakthrough advances, discoveries and advanced <phrase>knowledge</phrase> to come from outside of <phrase>social sciences</phrase>. " Behavior <phrase>Computing</phrase> " captures the transformation in the converging study of <phrase>human</phrase> behavior and <phrase>computing</phrase>. This <phrase>book</phrase> is a welcome addition alongside statistical, <phrase>machine learning</phrase> and <phrase>cognitive neuroscience</phrase> books. This <phrase>book</phrase> effectively contextualizes statistical and <phrase>machine learning</phrase> tools in a series of 23 very interesting chapters embracing models, scenarios and <phrase>case studies</phrase> thematically connected with behavior <phrase>computing</phrase>. The end result is a highly presentable <phrase>book</phrase> for a wide-ranging audience inclusive of final year undergraduates or postgraduate students. However, the <phrase>book</phrase> requires familiarity with <phrase>machine learning</phrase> <phrase>algorithms</phrase> and analysis of <phrase>large datasets</phrase> and may just prove to be a <phrase>catalyst</phrase> for social scientists to leave behind existing pastures and embrace the field of behavior <phrase>computing</phrase>. On a more descriptive basis, this <phrase>book</phrase> may aptly hold the title Behavior and Social Informatics <phrase>Computing</phrase> (BSIC) in line with the <phrase>IEEE</phrase> <phrase>Computational Intelligence</phrase> <phrase>Society</phrase> task force of the same name and chaired by Longbing Cao. This <phrase>book</phrase> covers similar 
Scalable stacking and learning for building <phrase>deep architectures</phrase> <phrase>Deep Neural Networks</phrase> (DNNs) have shown remarkable success in <phrase>pattern recognition</phrase> tasks. However, parallelizing DNN training across <phrase>computers</phrase> has been difficult. We present the Deep Stacking Network (DSN), which overcomes the problem of paralleliz-ing <phrase>learning algorithms</phrase> for <phrase>deep architectures</phrase>. The DSN provides a method of stacking simple processing modules in buiding <phrase>deep architectures</phrase>, with a convex learning problem in each module. Additional <phrase>fine tuning</phrase> further improves the DSN, while introducing minor non-convexity. Full learning in the DSN is batch-mode, making it amenable to parallel training over many machines and thus be scal-able over the potentially huge size of the <phrase>training data</phrase>. <phrase>Experimental</phrase> <phrase>results</phrase> on both the MNIST (image) and TIMIT (speech) <phrase>classification tasks</phrase> demonstrate that the DSN <phrase>learning algorithm</phrase> developed in this work is not only parallelizable in implementation but it also attains higher <phrase>classification accuracy</phrase> than the DNN.
Deep multiple instance learning for <phrase>image classification</phrase> and auto-annotation The recent development in learning deep representations has demonstrated its wide applications in traditional vision tasks like classification and detection. However, there has been little investigation on how we could build up a <phrase>deep learning</phrase> framework in a <phrase>weakly supervised</phrase> setting. In this <phrase>paper</phrase>, we attempt to <phrase>model</phrase> <phrase>deep learning</phrase> in a <phrase>weakly supervised</phrase> learning (multiple instance learning) framework. In our setting, each image follows a dual multi-instance assumption , where its object proposals and possible text annotations can be regarded as two instance sets. We thus <phrase>design</phrase> effective systems to exploit the MIL <phrase>property</phrase> with <phrase>deep learning</phrase> strategies from the two ends; we also try to jointly learn the relationship between object and annotation proposals. We conduct <phrase>extensive experiments</phrase> and prove that our <phrase>weakly supervised</phrase> <phrase>deep learning</phrase> framework not only achieves convincing performance in vision tasks including classification and image annotation, but also extracts reasonable <phrase>region</phrase>-keyword pairs with little supervision , on both widely used benchmarks like PASCAL <phrase>VOC</phrase> and <phrase>MIT</phrase> Indoor Scene 67, and also a dataset for image-and patch-level annotations.
Sensory Cue Integration with <phrase>High</phrase>-<phrase>order</phrase> <phrase>Deep Neural Networks</phrase> Humans can easily capture <phrase>real world</phrase> concepts from multi-modal signals by constructing joint representations of these signals. The joint representations may contain abstract <phrase>information</phrase> of multiple modalities and relationships across the modalities. Contrary to humans, it is not easy to obtain joint representations reflecting the structure of multi-modal <phrase>data</phrase> with <phrase>machine learning</phrase> <phrase>algorithms</phrase>, especially with conventional <phrase>neural networks</phrase>. This is because these models only have additive interaction across modalities. To deal with this issue, we propose a novel <phrase>machine learning</phrase> <phrase>algorithm</phrase> which captures multiplicative interaction between multiple modalities by using <phrase>high</phrase>-<phrase>order</phrase> edges. With these edges, the <phrase>proposed method</phrase> is able to learn the highly non-linear correlation among modalities. In the <phrase>experimental</phrase> <phrase>results</phrase>, we demonstrate the effect of this <phrase>high</phrase>-<phrase>order</phrase> interaction by showing improved <phrase>results</phrase> compared to other conventional models with a benchmark dataset, MNIST.
Context <phrase>Fusion</phrase>: The Role of Discourse Structure and Centering Theory Questions are not asked in isolation. Their context, viz. the preceding interactions, might be of help to understand them and retrieve the correct answer. Previous <phrase>research</phrase> in Interactive <phrase>Question Answering</phrase> showed that context <phrase>fusion</phrase> has a big potential to improve the performance of answer retrieval. In this <phrase>paper</phrase>, we study how much context, and what elements of it, should be considered to answer Follow-Up Questions (<phrase>FU Qs</phrase>). Following previous <phrase>research</phrase>, we exploit <phrase>Logistic Regression</phrase> Models to learn aspects of dialogue structure relevant to answering <phrase>FU Qs</phrase>. We enrich existing models based on shallow features with deep features, relying on the theory of discourse structure of (Chai and Jin, 2004), and on Centering Theory, respectively. Using models trained on realistic IQA <phrase>data</phrase>, we show which of the various theoretically motivated features hold up against <phrase>empirical evidence</phrase>. We also show that, while these deep features do not outperform the shallow ones on their own, an IQA system's answer correctness increases if the shallow and deep features are combined.
Intrinsically motivated hierarchical manipulation We present a framework for the <phrase>programming</phrase> of manipulation behavior by means of an intrinsic <phrase>reward function</phrase> that encourages the building of deep control <phrase>knowledge</phrase>. We show how this framework can be used to teach new manipulation skills in a hierarchical and incremental <phrase>fashion</phrase>. We demonstrate the contributions of this <phrase>paper</phrase> on a <phrase>humanoid robot</phrase> through three <phrase>incremental learning</phrase> stages.
Essentializing differences between women and men. People represent many social categories, including <phrase>gender</phrase> categories, in <phrase>essentialist</phrase> terms: They see category members as sharing deep, nonobvious properties that make them the kinds of things they are. The present <phrase>research</phrase> explored the consequences of this mode of representation for social inferences. In two sets of studies, participants learned (a) that they were similar to a <phrase>member</phrase> of the other <phrase>gender</phrase> on a novel attribute, (b) that they were different from a <phrase>member</phrase> of the other <phrase>gender</phrase> on a novel attribute, or (c) just their own standing on a novel attribute. <phrase>Results</phrase> showed that participants made stronger inductive inferences about the attribute in question when they learned that it distinguished them from a <phrase>member</phrase> of the other <phrase>gender</phrase> than in the other conditions. We consider the implications of these <phrase>results</phrase> for the representation of social categories and for everyday social inference processes.
Building <phrase>Machine Learning</phrase> Systems that Understand Over the past five years, <phrase>deep learning</phrase> and <phrase>large-scale</phrase> <phrase>neural networks</phrase> have made significant advances in <phrase>speech recognition</phrase>, <phrase>computer vision</phrase>, <phrase>language</phrase> understanding and <phrase>translation</phrase>, <phrase>robotics</phrase>, and many other fields. <phrase>Deep learning</phrase> allows the use of very raw forms of <phrase>data</phrase> in <phrase>order</phrase> to build <phrase>higher-level</phrase> understanding of <phrase>data</phrase> automatically, and can also be used to learn to accomplish complex tasks. In the next decade, it is likely that a fruitful direction for <phrase>research</phrase> in <phrase>data management</phrase> will be in how to seamlessly integrate these kinds of <phrase>machine learning</phrase> models into systems that store and manage <phrase>data</phrase>. In this <phrase>talk</phrase>, I will highlight some of the advances that have been made in <phrase>deep learning</phrase> and suggest some interesting directions for <phrase>future research</phrase>.
Max-pooling <phrase>convolutional neural networks</phrase> for vision-based hand <phrase>gesture recognition</phrase> Automatic recognition of gestures using <phrase>computer vision</phrase> is important for many <phrase>real-world</phrase> applications such as <phrase>sign language</phrase> recognition and <phrase>human</phrase>-<phrase>robot</phrase> interaction (HRI). Our goal is a real-time hand gesture-based HRI interface for <phrase>mobile</phrase> <phrase>robots</phrase>. We use a <phrase>state</phrase>-of-the-<phrase>art</phrase> big and <phrase>deep neural network</phrase> (NN) combining <phrase>convolution</phrase> and max-pooling (MPCNN) for supervised <phrase>feature learning</phrase> and classification of hand gestures given by humans to <phrase>mobile</phrase> <phrase>robots</phrase> using colored gloves. The hand contour is retrieved by color segmentation, then smoothened by morphological <phrase>image processing</phrase> which eliminates noisy edges. Our big and deep MPCNN classifies 6 gesture classes with 96% accuracy, nearly three times better than the nearest competitor. Experiments with <phrase>mobile</phrase> <phrase>robots</phrase> using an ARM 11 533MHz processor achieve real-time <phrase>gesture recognition</phrase> performance.
Topic: <phrase>Statistical Machine Translation</phrase> Using <phrase>Large-scale</phrase> <phrase>Lexicon</phrase> and <phrase>Deep Syntactic</phrase> Structures, Improve <phrase>Syntax</phrase>-based <phrase>Translation</phrase> Using Deep <phrase>English</phrase>, and <phrase>semi-supervised</phrase> learning approaches for <phrase>data mining</phrase> from the Web.
Virtual <phrase>Practicum</phrase> Experiences to Build <phrase>Professional</phrase> Identity This <phrase>paper</phrase> reports on a how pre-service <phrase>teacher</phrase> use of an online classroom <phrase>simulation</phrase> (ClassSim) supplemented existing <phrase>practicum</phrase> experiences and contributed to the development of their emerging <phrase>professional</phrase> identity. ClassSim was developed to provide pre-service teachers with a safe <phrase>virtual environment</phrase> in which they can explore 'authentic' and practical classroom scenarios. Users were able to assume the role of a <phrase>teacher</phrase> during in the virtual classroom, and during its running time make a number of decisions about the <phrase>management</phrase> of the classroom and the organisation of virtual <phrase>teaching and learning</phrase> experiences. Embedded tools were designed to enhance the development of connections made between educational theory of their pre-service <phrase>teacher</phrase> course and the practicalities of operating in a classroom environment. The <phrase>data</phrase> we present in this <phrase>paper</phrase> shows how the experience with ClassSim contributed to the emerging <phrase>professional</phrase> identity of the users. 1. Background As pre-service teachers progress through their <phrase>degree</phrase>, they are expected to develop their <phrase>professional</phrase> identity via a combination of <phrase>university</phrase> coursework (theory), <phrase>practicum</phrase> experiences (practice). To begin the process of developing a <phrase>professional</phrase> identify pre-service teachers need to begin to make connections between the theory and the practice of their profession. As Allen (2005:1) states " Teaching is a profession that requires skills, dispositions (personal qualities), and <phrase>knowledge</phrase> in four areas: <phrase>subject matter</phrase>, <phrase>child development</phrase>, teaching methodology, and <phrase>self-awareness</phrase>/identity ". Thus, a teacher's <phrase>professional</phrase> identity is shaped by their understanding of the theory of teaching/learning and their unique set of <phrase>life</phrase> experiences both inside and outside the classroom. Moreover, Barty (2004:2) suggests that a teacher's <phrase>professional</phrase> identity " impacts a teacher's <phrase>philosophical</phrase> beliefs, and ultimately, the pedagogical approaches used in their classroom ". However, <phrase>identity formation</phrase> is an ongoing, dynamic process that is open to discussion and modification, and always occurring in a social context (Britzman, 1986). Weber and Mitchell (1996:109) assert that <phrase>identity formation</phrase> " is a constant social negotiation that can never be permanently settled or fixed ". Very little is explicitly taught about how one can develop a <phrase>teacher</phrase> <phrase>professional</phrase> identity via pre-service <phrase>teacher</phrase> <phrase>education</phrase> and this is the <phrase>research</phrase> that this study contributes to. Killian (2003) explains, an effective <phrase>teacher</phrase> needs to have a <phrase>deep understanding</phrase> of the four types of <phrase>knowledge</phrase> including, <phrase>knowledge</phrase> of <phrase>subject matter</phrase>; <phrase>knowledge</phrase> about how <phrase>students learn</phrase>; and <phrase>general</phrase> pedagogical <phrase>knowledge</phrase>. The fourth type of <phrase>knowledge</phrase> is known as '<phrase>pedagogical content knowledge</phrase>' (Shulman, 1987) and this <phrase>knowledge</phrase> develops 
<phrase>Learning By Teaching</phrase>: A New Agent <phrase>Paradigm</phrase> For <phrase>Educational Software</phrase> This <phrase>paper</phrase> discusses Betty's <phrase>Brain</phrase>, a teachable agent in the domain of <phrase>river</phrase> <phrase>ecosystems</phrase> that combines <phrase>learning by teaching</phrase> with self-regulation mentoring to <phrase>promote deep learning</phrase> and understanding. Two studies demonstrate the effectiveness of this system. The first study focused on components that define <phrase>student</phrase>-<phrase>teacher</phrase> interactions in the <phrase>learning by teaching</phrase> task. The second study examined the value of adding meta-<phrase>cognitive</phrase> strategies that governed Betty's behavior and self-regulation hints provided by a mentor agent. The study compared three versions: a system where the <phrase>student</phrase> was tutored by a pedagogical agent (ITS), a <phrase>learning by teaching</phrase> system (LBT) , where students taught a baseline version of Betty, and received tutoring help from the men-<phrase>tor</phrase>, and a <phrase>learning by teaching</phrase> system (SRL), where Betty was enhanced to include self-regulation strategies, and the mentor provided help on domain material plus how to become better learners and better teachers. <phrase>Results</phrase> indicate that the addition of the self-regulated Betty and the self-regulation mentor better prepared students to learn new concepts later, even when they no longer had access to the SRL environment.
Minimum Latency Aggregation Convergecast in <phrase>Wireless Sensor Networks</phrase> Abstract Minimum Latency Aggregation Convergecast in <phrase>Wireless</phrase> <phrase>Sensor</phrase> complies with the regulations of this <phrase>University</phrase> and meets the accepted standards with respect to originality and quality. In <phrase>wireless sensor networks</phrase>, <phrase>sensor</phrase> nodes are used to collect <phrase>data</phrase> from the environment and send it to a <phrase>data</phrase> collection point or a sink node using a convergecast <phrase>tree</phrase>. Considerable savings in <phrase>energy</phrase> can be obtained by aggregating <phrase>data</phrase> at intermediate nodes along the way to the sink. We study the problem of finding a minimum latency aggregation <phrase>tree</phrase> and transmission schedule in <phrase>wireless sensor networks</phrase>. This problem is referred to as Minimum Latency Aggregation Scheduling (MLAS) in the <phrase>literature</phrase> and has been <phrase>proven</phrase> to be <phrase>NP-Complete</phrase> even for <phrase>unit disk</phrase> <phrase>graphs</phrase>. We present a new simpler proof of the <phrase>NP-Completeness</phrase> of the MLAS Problem for arbitrary networks and <phrase>unit disk</phrase> <phrase>graphs</phrase>. We give tight bounds for the latency of aggregation convergecast for grids, tori, and <phrase>trees</phrase>. For regular <phrase>unit interval</phrase> <phrase>graphs</phrase>, we provide an <phrase>algorithm</phrase> which is guaranteed to have a latency that is within one time slot of the optimal latency. Finally, for <phrase>unit interval</phrase> <phrase>graphs</phrase> we give a 2-<phrase>approximation algorithm</phrase> to solve the same problem. For arbitrary <phrase>graphs</phrase>, we introduce a new <phrase>algorithm</phrase> for building an aggregation <phrase>tree</phrase>. Furthermore, we propose two new approaches for building a transmission schedule to perform aggregation on a given <phrase>tree</phrase>. We evaluate the performance of our <phrase>algorithms</phrase> through extensive simulations on randomly generated <phrase>graphs</phrase> and we compare them to the previous <phrase>state</phrase> of the <phrase>art</phrase>. Our <phrase>results</phrase> show that one of our <phrase>algorithms</phrase> has a latency that is 38% less than the latency of the previous best <phrase>algorithm</phrase>. iii Acknowledgments First and foremost, I would like to express my sincere gratitude to my supervisor, Dr. <phrase>Lata</phrase> Narayanan, for her help and guidance throughout this <phrase>thesis</phrase>. Her deep <phrase>knowledge</phrase> of the field and her valuable suggestions helped me achieve the <phrase>results</phrase> presented in this <phrase>thesis</phrase>. I have learned a lot from working with her and she gave me the tools that I needed to complete this work. Her availability outside of normal working hours was also very much appreciated. I also wish to thank my wife and two daughters, who have let me spend the necessary hours to be able to work on this problem. Without their encouragements and support, it would have been very difficult to complete this <phrase>thesis</phrase>.
Surface and <phrase>deep learning</phrase> processes in <phrase>distance education</phrase>: Synchronous versus asynchronous systems <phrase>Distance learning</phrase> is different from regular learning in the classroom. One of the main factors which influence the effectiveness of the learning process is the interaction that exists between the <phrase>teacher</phrase> and the <phrase>student</phrase>. Our <phrase>research</phrase> indicates that different interactions have different effects. There are two methods used for implementing <phrase>distance learning</phrase> systems, i.e. synchronous and asynchronous. Our <phrase>research</phrase> is based on the <phrase>model</phrase> developed by Oliver and McLaughlin. According to this <phrase>model</phrase>, there exist five types of teacherstudent interactions: social, procedural, expository, explanatory and <phrase>cognitive</phrase>. The present study refers to the cog-nitive interaction and differentiates between surface processes and deep processes. The study presents different variables and their influences on the students' achievements and their satisfaction from learning via a synchronous versus an asynchronous <phrase>distance learning</phrase> system. The interaction level between the students and the <phrase>teacher</phrase> and among the students was found to be a significant factor in determining the effectiveness of the teaching method. The observations and interviews which we held with the students helped clarify the <phrase>information</phrase> that was obtained using the quantitative <phrase>research</phrase> tools, and showed that the presence of a teacherstudent interaction which accompanies the learning process is very important for all learners. However, students with <phrase>high</phrase>-level thinking can overcome the <phrase>low-level</phrase> of interactions in asynchronous learning.
<phrase>Deep Learning</phrase> for Detecting Robotic Grasps We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a <phrase>deep learning</phrase> approach to solve this problem, which avoids time-consuming hand-<phrase>design</phrase> of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In <phrase>order</phrase> to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively <phrase>prune</phrase> out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group reg-ularization. We demonstrate that our method outperforms the previous <phrase>state</phrase>-of-the-<phrase>art</phrase> methods in robotic grasp detection, and can be used to successfully execute grasps on a Baxter <phrase>robot</phrase>.
<phrase>Deep learning</phrase> for detection of <phrase>bird</phrase> vocalisations This work focuses on reliable detection of <phrase>bird</phrase> <phrase>sound</phrase> <phrase>emissions</phrase> as recorded in the open field. <phrase>Acoustic</phrase> detection of <phrase>avian</phrase> sounds can be used for the automatized monitoring of multiple <phrase>bird</phrase> <phrase>taxa</phrase> and querying in <phrase>long</phrase>-term recordings for <phrase>species</phrase> of interest for researchers, conservation practitioners, and decision makers. Recordings in the wild can be very noisy due to the exposure of the <phrase>microphones</phrase> to a large number of audio sources originating from all distances and directions, the number and identity of which cannot be known a-priori. The coexistence of the <phrase>target</phrase> vocalizations with abiotic interferences in an unconstrained environment is inefficiently treated by current approaches of <phrase>audio signal</phrase> enhancement. A technique that would spot only <phrase>bird</phrase> vocalization while ignoring other audio sources is of prime importance. These difficulties are tackled in this work, presenting a deep autoencoder that maps the audio <phrase>spectrogram</phrase> of <phrase>bird</phrase> vocalizations to its corresponding <phrase>binary</phrase> mask that encircles the spectral blobs of vocalizations while suppressing other audio sources. The procedure requires minimum <phrase>human</phrase> attendance, it is very fast during execution, thus suitable to scan massive volumes of <phrase>data</phrase>, in <phrase>order</phrase> to analyze them, evaluate insights and hypotheses, identify patterns of <phrase>bird</phrase> activity that, hopefully, finally <phrase>lead</phrase> to <phrase>design</phrase> policies on <phrase>biodiversity</phrase> issues.
<phrase>Natural Language Processing</phrase> in Watson Open domain <phrase>Question Answering</phrase> (QA) is a <phrase>long</phrase> standing <phrase>research</phrase> problem. Recently, <phrase>IBM</phrase> took on this challenge in the context of the <phrase>Jeopardy</phrase>! <phrase>game</phrase>. <phrase>Jeopardy</phrase>! is a well-known <phrase>TV</phrase> quiz show that has been airing on <phrase>television</phrase> in the <phrase>United States</phrase> for more than 25 years. It pits three <phrase>human</phrase> contestants against one another in a competition that requires answering rich <phrase>natural language</phrase> questions over a very broad domain of topics. The development of a system able to compete to grand champions in the <phrase>Jeopardy</phrase>! challenge <phrase>led</phrase> to the <phrase>design</phrase> of the DeepQA <phrase>architecture</phrase> and the implementation of Watson. The DeepQA project shapes a grand challenge in <phrase>Computer Science</phrase> that aims to illustrate how the wide and growing accessibility of <phrase>natural language</phrase> content and the integration and advancement of <phrase>Natural Language Processing</phrase>, <phrase>Information Retrieval</phrase>, <phrase>Machine Learning</phrase>, <phrase>Knowledge Representation and Reasoning</phrase>, and massively parallel computation can drive open-domain automatic <phrase>Question Answering</phrase> <phrase>technology</phrase> to a point where it clearly and consistently rivals the best <phrase>human</phrase> performance. <phrase>Natural Language Processing</phrase> (<phrase>NLP</phrase>) plays a crucial role in the overall Deep QA <phrase>architecture</phrase>. It allows to "make sense" of both question and unstructured <phrase>knowledge</phrase> contained in the large corpora where most of the answers are located. That's why we decided to focus this tutorial on the <phrase>NLP</phrase> <phrase>technology</phrase> adopted by Watson and on how it fits in the <phrase>general</phrase> Deep QA <phrase>architecture</phrase>. OUTLINE The course is structured in two modules (1.5h each), described below. This lesson focuses on the <phrase>NLP</phrase> <phrase>technology</phrase> implemented in Watson, highlighting the advancements with respect to <phrase>state</phrase> of the <phrase>art</phrase> techniques in <phrase>Question Answering</phrase>. In particular:
Representational Power of <phrase>Restricted Boltzmann Machines</phrase> and <phrase>Deep Belief</phrase> Networks <phrase>Deep belief</phrase> networks (DBN) are generative <phrase>neural network</phrase> models with many layers of hidden explanatory factors, recently introduced by Hinton, Osindero, and Teh (2006) along with a <phrase>greedy layer-wise</phrase> <phrase>unsupervised learning</phrase> <phrase>algorithm</phrase>. The <phrase>building block</phrase> of a DBN is a <phrase>probabilistic model</phrase> called a <phrase>restricted Boltzmann machine</phrase> (RBM), used to represent one layer of the <phrase>model</phrase>. <phrase>Restricted Boltzmann machines</phrase> are interesting because inference is easy in them and because they have been successfully used as <phrase>building blocks</phrase> for training deeper models. We first prove that adding <phrase>hidden units</phrase> yields strictly improved modeling power, while a second theorem shows that RBMs are <phrase>universal</phrase> approximators of discrete <phrase>distributions</phrase>. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.
Using Rich Pictures in <phrase>Information</phrase> Systems Teaching The Rich Picture (RP), as described in Peter Checkland's Soft Systems Methodology, is a flexible <phrase>graphical</phrase> technique used to represent a situation, problem or concept. In a <phrase>university</phrase> teaching environment which aims to encourage students to adopt <phrase>deep learning</phrase> (seeking meaning) approaches when learning about <phrase>information</phrase> systems, the RP is proving to be a useful technique. The use of and <phrase>reaction</phrase> to RPs by third year <phrase>university</phrase> students are described and illustrated.
Integrating Learning, <phrase>Problem Solving</phrase>, and Engagement in <phrase>Narrative</phrase>-<phrase>Centered Learning</phrase> Environments A key promise of <phrase>narrative</phrase>-<phrase>centered learning</phrase> environments is the ability to make learning engaging. However , there is concern that learning and engagement may be at odds in these <phrase>game</phrase>-based <phrase>learning environments</phrase>. This view suggests that, on the one hand, students interacting with a <phrase>game-based learning</phrase> environment may be engaged but unlikely to learn, while on the other hand, traditional learning technologies may <phrase>promote deep learning</phrase> but provide limited engagement. This <phrase>paper</phrase> presents findings from a study with <phrase>human</phrase> participants that challenges the view that engagement and learning need be opposed. A study was conducted with 153 <phrase>middle school</phrase> students interacting with a <phrase>narrative</phrase>-<phrase>centered learning</phrase> environment. Rather than finding an oppositional relationship between learning and engagement, the study found a strong positive relationship between <phrase>learning outcomes</phrase>, in-<phrase>game</phrase> <phrase>problem solving</phrase> and increased engagement. Furthermore, the relationship between <phrase>learning outcomes</phrase> and engagement held even when controlling for students' <phrase>background knowledge</phrase> and <phrase>game</phrase>-playing experience. Additional analyses revealed that males tended to <phrase>report</phrase> significantly greater presence in the <phrase>virtual environment</phrase> than girls, and students with more <phrase>game</phrase>-playing experience reported significantly greater presence in the <phrase>virtual environment</phrase> than students with minimal <phrase>game</phrase>-playing experience. Follow up analyses suggested that differences in presence may be more strongly associated with <phrase>game</phrase>-playing experience than <phrase>gender</phrase>.
Interview with Roy <phrase>E</phrase>. <phrase>Disney</phrase> The legendary <phrase>Alan Kay</phrase> and Roy <phrase>E</phrase>. <phrase>Disney</phrase> have graciouslyappeared on <phrase>camera</phrase> for interviews and joined the <phrase>ACM</phrase> <phrase>Computers</phrase> inEntertainment magazine's editorial board. Alan and Roy are two ofthe nicest people to <phrase>talk</phrase> to and work with. Alan talked about softfun versus hard fun, and his <phrase>research</phrase> on <phrase>Squeak</phrase> for enhancing andamplifying learning in children's <phrase>education</phrase>. Roy told us abouteducators versus entertainers, and his views on traditional and CGIanimations. The <phrase>video</phrase> clips of the interviews are available athttp://www.acm.org/<phrase>pubs</phrase>/cie/oct2003/index.html Roy <phrase>E</phrase>. <phrase>Disney</phrase> is Vice <phrase>Chairman</phrase> of the Board of <phrase>Directors</phrase> of TheWalt <phrase>Disney</phrase> <phrase>Company</phrase> and <phrase>chairman</phrase> of <phrase>Walt Disney</phrase> <phrase>Animation</phrase>. He beganhis <phrase>entertainment</phrase> <phrase>industry</phrase> career in 1953, working as an assistantfilm <phrase>editor</phrase> on the "<phrase>Dragnet</phrase>" <phrase>TV</phrase> series. He joined The <phrase>Walt</phrase> DisneyCompany in 1954 and served as assistant <phrase>film editor</phrase> on motionpictures, including "The Living <phrase>Desert</phrase>" and "The Vanishing <phrase>Prairie</phrase>"both <phrase>Academy Award</phrase> winners. As a <phrase>writer</phrase> and <phrase>production</phrase> associate,Roy received an <phrase>Academy Award</phrase> nomination his work on the shortsubject "Mysteries of the Deep" in 1959. More <phrase>information</phrase> isavailable athttp://psc.disney.go.com/corporate/communications/<phrase>bios</phrase>/Disney.html
<phrase>ECONOMIC GEOLOGY</phrase>: <phrase>Lessons Learned</phrase> from <phrase>Deep-Sea</phrase> <phrase>Mining</phrase>. The first attempt to exploit <phrase>deep-sea</phrase> <phrase>manganese</phrase> nodules ended in failure as a result of the collapse of world <phrase>metal</phrase> prices, the onerous provisions imposed by the U. N. Conference on the <phrase>Law</phrase> of the Sea (<phrase>UNCLOS</phrase>), and the overoptimistic assumptions about the viability of nodule <phrase>mining</phrase>. Attention then focused on <phrase>cobalt</phrase>-rich <phrase>manganese</phrase> crusts from seamounts. Since the mid-1980s, a number of new players have committed themselves to <phrase>long</phrase>-term programs to establish the viability of <phrase>mining</phrase> <phrase>deep-sea</phrase> <phrase>manganese</phrase> nodules. These programs require heavy subsidy by host governments. <phrase>Gold</phrase>-rich <phrase>submarine</phrase> hydrothermal deposits located at <phrase>convergent</phrase> plate margins are now emerging as a more promising prospect for <phrase>mining</phrase> than <phrase>deep-sea</phrase> <phrase>manganese</phrase> deposits.
ICML2011 Unsupervised and <phrase>Transfer Learning</phrase> Workshop We organized a <phrase>data mining</phrase> challenge in " unsupervised and <phrase>transfer learning</phrase> " (the UTL challenge) followed by a workshop of the same name at the ICML 2011 conference in <phrase>Bellevue</phrase>, <phrase>Washington</phrase> 1. This introduction presents the highlights of the outstanding contributions that were made, which are regrouped in this issue of JMLR W&CP. Novel methodologies emerged to capitalize on large volumes of <phrase>unlabeled data</phrase> from tasks related (but different) from a <phrase>target</phrase> task, including a method to learn <phrase>data</phrase> kernels (similarity measures) and new <phrase>deep architectures</phrase> for <phrase>feature learning</phrase>.
Modeling Users' <phrase>Powertrain</phrase> Preferences Leslie Pack Kaelbling <phrase>Professor</phrase> <phrase>Thesis</phrase> Supervisor Modeling Users' <phrase>Powertrain</phrase> Preferences Our goal is to construct a system that can determine a drivers preferences and goals and perform appropriate actions to aid the driver achieving his goals and improve the quality of his <phrase>road</phrase> behavior. Because the recommendation problem could be achieved effectively once we know the driver's intention, in this <phrase>thesis</phrase>, we are going to solve the problem to determine the driver's preferences. A <phrase>supervised learning</phrase> approach has already been applied to this problem. However , because the approach locally classify a small interval at a time and is <phrase>memory</phrase>-less, the <phrase>supervised learning</phrase> does not perform well on our goal. Instead, we need to introduce new approach which has following characteristics. First, it should consider the entire <phrase>stream</phrase> of measurements. Second, it should be tolerant to the environment. Third, it should be able to distinguish various intentions. In this <phrase>thesis</phrase>, two different approaches, <phrase>Bayesian</phrase> <phrase>hypothesis testing</phrase> and inverse <phrase>reinforcement learning</phrase>, will be used to classify and estimate the user's preferences. <phrase>Bayesian</phrase> <phrase>hypothesis testing</phrase> classifies the driver as one of several driving types. Assuming that the <phrase>probability distributions</phrase> of the features (i.e. <phrase>average</phrase>, <phrase>standard deviation</phrase>) for a <phrase>short</phrase> <phrase>period</phrase> of measurement are different among the driving types, <phrase>Bayesian</phrase> <phrase>hypothesis testing</phrase> classifies the driver as one of driving types by maintaining a belief distribution for each driving type and updating it online as more measurements are available. On the other hand, inverse <phrase>reinforcement learning</phrase> estimates the users' preferences as a <phrase>linear combination</phrase> of driving types. The inverse <phrase>reinforcement learning</phrase> approach assumes that the driver maximizes a <phrase>reward function</phrase> while driving, and his <phrase>reward function</phrase> is a <phrase>linear combination</phrase> of raw / expert features. Based on the observed tra-jectories of representative drivers, <phrase>apprenticeship</phrase> learning first calculates the <phrase>reward function</phrase> of each driving type with raw features, and these reward functions serve as expert features. After, with observed trajectories of a new driver, the same <phrase>algorithm</phrase> calculates the <phrase>reward function</phrase> of him, not with raw features, but with expert features, and estimates the preferences of any driver in a space of driving types. Acknowledgments I owe my deepest gratitude to <phrase>Professor</phrase> Leslie Pack Kaelbling, my <phrase>academic</phrase>, UROP, and <phrase>thesis</phrase> advisor, for her unending support, guidance, and inspiration, which have enabled me to develop a <phrase>deep understanding</phrase> in the field of <phrase>computer science</phrase> and <phrase>machine learning</phrase>. I would also like to thank <phrase>Professor</phrase> Tomas Lozano-Perez, for his guidance and insightful comments that enabled me to 
Deep nonlinear metric learning with <phrase>independent</phrase> subspace analysis for face verification Face verification is the task of determining by analyzing face images, whether a person is who he/she claims to be. It is a very challenge problem, due to large variations in lighting, background, expression, hairstyle and occlusion. The crucial problem is to compute the similarity of <phrase>two face</phrase> vectors. Metric learning has provides a viable <phrase>solution</phrase> to this problem. Until now, many metric <phrase>learning algorithms</phrase> have been proposed, but they are usually limited to learning a <phrase>linear transformation</phrase> (i.e. finding a global Mahalanobis metric). In this brief, we propose a nonlinear metric learning method, which learns an explicit mapping from the original space to an optimal subspace, using deep <phrase>Independent</phrase> Subspace Analysis network. Compared to <phrase>kernel methods</phrase>, which can also learn nonlinear transformations, our method is a deep and local learning <phrase>architecture</phrase>, and therefore exhibits more powerful ability to learn the <phrase>nature</phrase> of highly <phrase>variable</phrase> dataset. We evaluate our method on the LFW benchmark, and <phrase>results</phrase> show very comparable performance to the <phrase>state</phrase>-of-<phrase>art</phrase> methods (achieving 92.28% accuracy), while maintaining simplicity and good generalization ability.
In Focus Inequity in <phrase>Mathematics Education</phrase>: Questions for Educators Many <phrase>years ago</phrase> I encountered a diagram (Figure 1) that may be familiar to you. It was used to help teachers understand that <phrase>student</phrase> learning depended upon the relationship between the <phrase>teacher</phrase>, the <phrase>student</phrase>, and <phrase>mathematics</phrase>. Although I found it helpful in thinking about my teaching, I eventually realized that there are many more <phrase>triangles</phrase> that affect <phrase>student</phrase> learning. One can draw <phrase>triangles</phrase> with students, teachers, parents, <phrase>school</phrase> board members, legislators, etc. Figure 1. The relationship between the <phrase>teacher</phrase>, the <phrase>student</phrase>, and <phrase>mathematics</phrase>. Figure 2 is another way of acknowledging that there are many factors that affect <phrase>student</phrase> learning and the well-publicized acheivement gap between students from different <phrase>ethnic</phrase> and socioeconomic groups. The <phrase>student</phrase>/<phrase>teacher</phrase>/<phrase>mathematics</phrase> <phrase>triangle</phrase> is located in a classroom, in a <phrase>school</phrase>, in a <phrase>district</phrase>, in a <phrase>community</phrase> that is situated in a larger <phrase>society</phrase>. People in this <phrase>community</phrase> and in the larger <phrase>society</phrase> hold beliefs, attitudes, values, and often deep emotions about a <phrase>variety</phrase> of issuesteaching, learning, assessment, the <phrase>nature</phrase> of <phrase>mathematics</phrase>, the <phrase>nature</phrase> of schools in a <phrase>democratic</phrase> <phrase>society</phrase>, <phrase>race</phrase>, class, <phrase>gender</phrase>, <phrase>sexual orientation</phrase>, <phrase>culture</phrase>, and languageto name a few. In this article I will pose some questions and offer some thoughts about how some of these beliefs, attitudes, values, and emotions affect inequity in <phrase>mathematics education</phrase>. 1 The first question concerns <phrase>mathematics</phrase> and <phrase>culture</phrase>. Is <phrase>mathematics</phrase> <phrase>culture</phrase> <phrase>free</phrase>? Some people say that <phrase>mathematics</phrase> is a set of eternal truths that humans discover. Others maintain that it develops from <phrase>human</phrase> social interactionas all other forms of <phrase>knowledge</phrase> do. For example, Paul Ernest points out, " The basis of <phrase>mathematical</phrase> <phrase>knowledge</phrase> is <phrase>linguistic</phrase> <phrase>knowledge</phrase>, conventions and rules, and <phrase>language</phrase> is a social <phrase>construction</phrase> " (1991, p. 42). It is not my intent to settle this dispute in this article. If <phrase>mathematics</phrase> is not <phrase>culture</phrase> <phrase>free</phrase>, however, then one might wonder: Would <phrase>mathematics</phrase> be different if male <phrase>European</phrase> <phrase>culture</phrase> had not become the dominant force in the world? Since <phrase>mathematics</phrase> develops in part to solve the problems of <phrase>society</phrase>, the <phrase>culture</phrase> of a <phrase>society</phrase> at least influences the course of <phrase>mathematical</phrase> development. Although this influence may seem far removed from the classroom, as soon as we start talking about <phrase>mathematics</phrase> problems, we are close to classroom issues. I think immediately of the distinguished <phrase>mathematician</phrase> George Polya (18871985), a strong <phrase>advocate</phrase> for developing a <phrase>pedagogy</phrase> of <phrase>mathematical</phrase> <phrase>problem solving</phrase>. He wrote " An essential ingredient of the problem is the 
<phrase>Cocktail Party</phrase> Processing via Structured Prediction While <phrase>human</phrase> listeners <phrase>excel</phrase> at selectively attending to a conversation in a <phrase>cocktail party</phrase>, machine performance is still far inferior by comparison. We show that the <phrase>cocktail party</phrase> problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ <phrase>conditional random fields</phrase> (CRFs) to classify speech dominance within each time-<phrase>frequency</phrase> unit for a <phrase>sound</phrase> mixture. To capture complex, nonlinear relationship between input and output, both <phrase>state</phrase> and transition feature functions in CRFs are learned by <phrase>deep neural networks</phrase>. The formulation of the problem as classification allows us to directly optimize a measure that is well correlated with <phrase>human</phrase> speech intelligibility. The proposed system substantially outperforms existing ones in a <phrase>variety</phrase> of noises.
<phrase>Deep Neural Network</phrase> for Real-Time Autonomous Indoor <phrase>Navigation</phrase> Autonomous indoor <phrase>navigation</phrase> of Micro Aerial Vehicles (MAVs) possesses many challenges. One main reason is because <phrase>GPS</phrase> has limited precision in indoor environments. The additional fact that MAVs are not able to carry heavy weight or power consuming sensors, such as <phrase>range</phrase> finders, makes indoor autonomous <phrase>navigation</phrase> a <phrase>challenging task</phrase>. In this <phrase>paper</phrase>, we propose a practical system in which a <phrase>quadcopter</phrase> autonomously navigates indoors and finds a specific <phrase>target</phrase>, i.e. a <phrase>book</phrase> bag, by using a <phrase>single</phrase> <phrase>camera</phrase>. A <phrase>deep learning</phrase> <phrase>model</phrase>, <phrase>Convolutional Neural Network</phrase> (ConvNet), is used to learn a controller strategy that mimics an expert pilot's choice of <phrase>action</phrase>. We show our system's performance through real-time experiments in diverse indoor locations. To understand more about our trained network, we use several visualization techniques.
iCaRL: Incremental Classifier and Representation Learning A <phrase>major</phrase> <phrase>open problem</phrase> on the <phrase>road</phrase> to <phrase>artificial intelligence</phrase> is the development of incrementally learning systems that learn about more and more concepts over time from a <phrase>stream</phrase> of <phrase>data</phrase>. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the <phrase>training data</phrase> for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a <phrase>data</phrase> representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed <phrase>data</phrase> representations and therefore incompatible with <phrase>deep learning</phrase> architec-tures. We show by experiments on the CIFAR-100 and Im-ageNet ILSVRC 2012 datasets that iCaRL can learn many classes incrementally over a <phrase>long</phrase> <phrase>period</phrase> of time where other strategies quickly fail.
Improving the Accuracy of RF Alternate <phrase>Test</phrase> Using Multi-VDD Conditions: Application to Envelope-Based <phrase>Test</phrase> of LNAs This work demonstrates that multi-VDD conditions may be used to improve the accuracy of <phrase>machine learning</phrase> models , significantly decreasing the prediction error. The proposed technique has been successfully applied to a previous alternate <phrase>test</phrase> strategy for LNAs based on response envelope detection. A <phrase>prototype</phrase> has been developed to show its feasibility. The <phrase>prototype</phrase> consists of a <phrase>low-power</phrase> 2.4GHz LNA and a simple envelope detector, integrated in a 90nm <phrase>CMOS</phrase> <phrase>technology</phrase>. Post-layout <phrase>simulation</phrase> <phrase>results</phrase> are provided to verify the functionality of the approach. I. INTRODUCTION Nowadays, advances in RF <phrase>CMOS</phrase> technologies have enabled the integration of complete transceivers in a <phrase>single</phrase> chip, which provides a significant reduction in <phrase>production</phrase> cost. However there is a simultaneous increase in the cost of testing and diagnosing these devices. Their diverse specifications and <phrase>high</phrase> operating <phrase>frequency</phrase>, as well as the large impact of process variations in current <phrase>deep submicron</phrase> technologies, make necessary extensive <phrase>tests</phrase> and dedicated <phrase>high</phrase>-<phrase>frequency</phrase> <phrase>test</phrase> equipment. RF testing exhibits the same difficulties present in analog testing, but adding the problem of handling <phrase>high</phrase>-<phrase>frequency</phrase> signals. That is, RF testing is based on functional characterization, while fault-<phrase>model</phrase>-based <phrase>tests</phrase>, very successful in the <phrase>digital</phrase> <phrase>test</phrase> domain, are difficult to standardize in the RF field since each circuit type demands its own custom fault <phrase>model</phrase>. Reducing RF <phrase>test</phrase> complexity and cost is still an open <phrase>research</phrase> topic that has been addressed in a number of different approaches. Recent work in this <phrase>area</phrase> includes defect modeling and failure diagnosis [1], [2], alternate <phrase>test</phrase> [2][5], <phrase>DfT</phrase> and BIST techniques [6][8], etc. In particular, the combination of BIST techniques with the <phrase>statistical analysis</phrase> of alternate <phrase>test</phrase> seems to be a promising <phrase>solution</phrase> to mitigate most RF <phrase>test</phrase> drawbacks. On one hand, moving some of the testing functions to the device under <phrase>test</phrase> (DUT) would reduce <phrase>test</phrase> equipment cost, and eliminate the problem of transporting <phrase>high</phrase>-<phrase>frequency</phrase> <phrase>test</phrase> signals. On the other hand, alternate <phrase>test</phrase> strategies take advantage of advanced statistical tools to find correlations between a reduced number of <phrase>observables</phrase> (signatures), and the diverse DUT specifications, thus reducing the number of necessary <phrase>test</phrase> measurements and configurations.
Sparse Group <phrase>Restricted Boltzmann Machines</phrase> Since learning in <phrase>Boltzmann</phrase> machines is typically quite slow, there is a need to restrict connections within <phrase>hidden layers</phrase>. However, the resulting states of <phrase>hidden units</phrase> exhibit statistical dependencies. Based on this observation , we propose using l1/l2 regularization upon the activation <phrase>probabilities</phrase> of <phrase>hidden units</phrase> in <phrase>restricted Boltzmann machines</phrase> to capture the local dependencies among <phrase>hidden units</phrase>. This regularization not only encourages <phrase>hidden units</phrase> of many groups to be inactive given observed <phrase>data</phrase> but also makes <phrase>hidden units</phrase> within a group compete with each other for modeling observed <phrase>data</phrase>. Thus, the l1/l2 regularization on RBMs yields sparsity at both the group and the hidden unit levels. We call RBMs trained with the regularizer sparse group RBMs (SGRBMs). The proposed SGRBMs are applied to <phrase>model</phrase> patches of <phrase>natural images</phrase>, <phrase>handwritten digits</phrase> and <phrase>OCR</phrase> <phrase>English</phrase> letters. Then to emphasize that SGRBMs can learn more <phrase>discriminative features</phrase> we applied SGRBMs to pretrain deep networks for <phrase>classification tasks</phrase>. Furthermore, we illustrate the regularizer can also be applied to <phrase>deep Boltzmann machines</phrase>, which <phrase>lead</phrase> to sparse group <phrase>deep Boltzmann machines</phrase>. When adapted to the MNIST <phrase>data set</phrase>, a two-layer sparse group <phrase>Boltzmann</phrase> machine achieves an <phrase>error rate</phrase> of 0.84%, which is, to our <phrase>knowledge</phrase>, the best published result on the <phrase>permutation</phrase>-invariant version of the MNIST task.
Introducing BIS 2008 Workshops on Emerging Web Technologies This volume includes papers presented at the workshops held in conjunction with the 11th <phrase>Business</phrase> <phrase>Information</phrase> Systems Conference, taking place in <phrase>Innsbruck</phrase>, <phrase>Austria</phrase> on 6-7 May 2008. The conference is a well established <phrase>knowledge</phrase> exchange forum, with topics covering development, implementation, application, and improvement of IT systems for <phrase>business</phrase>. It has a <phrase>long</phrase> tradition of organizing special sessions, tracks, and workshops that focus on new and developing <phrase>research</phrase> areas. The common denominator of these diverse workshops is the <phrase>research</phrase> on the application of the emerging technologies (particularly in the Web <phrase>sphere</phrase>). This topic is approached from different directions by each of the workshops: SAW concentrates on the influence that technologies exert on the societies, and on emergence of social <phrase>knowledge</phrase> and structures in <phrase>Web-based</phrase> IT solutions. ADW participants discuss how to use potential that lies in the <phrase>Deep Web</phrase> to enable more thorough analyses, broader <phrase>information</phrase> integration and stimulate outbreak of new <phrase>information</phrase> services. Finally, <phrase>E</phrase>-Learning participants ponder on the best ways to utilize new technologies to speed up <phrase>knowledge</phrase> acquisition and increase its quality for the <phrase>e</phrase>-learning solutions users. The observation that the Web has recently moved from a simple one-way <phrase>channel</phrase>, to a complex social <phrase>communication</phrase> space was a direct <phrase>motivation</phrase> behind SAW 2008. Today, the distinction between the authors and audience is becoming blurred and new ways to create, share and use <phrase>knowledge</phrase> in a social way emerge. SAW papers investigate a <phrase>variety</phrase> of aspects of this change of <phrase>paradigm</phrase>, that transforms our interactions with other people, our relationships, ways of gathering <phrase>information</phrase> and doing <phrase>business</phrase>. Bojrs et al. [1] and Guns [2] focus on bridging <phrase>semantic</phrase> technologies <phrase>research</phrase> with social aspects. <phrase>Massa</phrase> and Souren [3] analyze complex subject of trust measurement in the Web environment. Three papers show how <phrase>Web 2.0</phrase> technologies may be used in <phrase>knowledge management</phrase> (Bibikas et al. [4]), <phrase>customer support</phrase> (Nguyen et al. [5]) and in development of <phrase>mobile</phrase> <phrase>cultural</phrase> services (<phrase>Coppola</phrase> et al. [6]). Jacquemin et al. [7] study how user conflicts may be handled in <phrase>Wikipedia</phrase>. Stocker and Tochtermann [8] analyze usage of <phrase>weblogs</phrase> in <phrase>business</phrase> scenarios and Ettinger et al. [9] demonstrate possible usages of job boards. In parallel to advancement of <phrase>Social Web</phrase>, a significant growth of complexity of Web <phrase>information</phrase> systems can be observed. The growth is giving rise to the <phrase>Deep Web</phrase> phenomenon. While the main way of accessing content on contemporary Web is by means 
<phrase>Radio Resource Management</phrase> in 3g <phrase>Umts</phrase> Networks <phrase>Universal</phrase> <phrase>Mobile</phrase> <phrase>Telecommunication</phrase> System (<phrase>UMTS</phrase>) is a third generation <phrase>mobile</phrase> <phrase>communication</phrase> system, designed to support a wide <phrase>range</phrase> of applications with different quality of service (<phrase>QoS</phrase>) profiles. This 3G system has capability of transporting <phrase>wideband</phrase> and <phrase>high</phrase> <phrase>bit rate</phrase> <phrase>multimedia</phrase> services along with traditional cellular <phrase>telephony</phrase> services e.g voice, <phrase>messaging</phrase> etc. To provide these services with better quality of service and enhance the performance of <phrase>wireless network</phrase>, <phrase>management</phrase> of <phrase>radio</phrase> resources is necessary. To do this, <phrase>UMTS</phrase> offer many <phrase>radio resource management</phrase> (RRM) strategies. These RRM techniques <phrase>play</phrase> important role in providing different services with better quality, keep the end user happy and make the network stable. In our <phrase>thesis</phrase>, our main objective is to explore some RRM strategies and understand their practical importance by simulating some RRM <phrase>algorithms</phrase>. First we start with <phrase>UMTS</phrase> overview and learn some important concept about <phrase>UMTS</phrase> <phrase>architecture</phrase>. Then we go deep into <phrase>physical layer</phrase> of <phrase>UMTS</phrase>. After getting strong concept of <phrase>UMTS</phrase> <phrase>radio</phrase> <phrase>architecture</phrase> and procedures, we worked on different RRM techniques and in the end we analyze two power control <phrase>algorithms</phrase> to understand and get some practical experience of actual RRM strategies, because power control is the important most and critical part of RRM techniques due to interference limited <phrase>nature</phrase> of <phrase>CDMA</phrase> systems. iii iv Acknowledgement
R-<phrase>CNN</phrase> minus R <phrase>Deep convolutional</phrase> <phrase>neural networks</phrase> (CNNs) have had a <phrase>major</phrase> impact in most areas of image understanding , including object category detection. In <phrase>object detection</phrase>, methods such as R-<phrase>CNN</phrase> have obtained excellent <phrase>results</phrase> by integrating CNNs with <phrase>region</phrase> proposal generation <phrase>algorithms</phrase> such as selective search. In this <phrase>paper</phrase>, we investigate the role of proposal generation in <phrase>CNN</phrase>-based detectors in <phrase>order</phrase> to determine whether it is a necessary modelling component, carrying essential geometric <phrase>information</phrase> not contained in the <phrase>CNN</phrase>, or whether it is merely a way of accelerating detection. We do so by designing and evaluating a detector that uses a trivial <phrase>region</phrase> generation scheme, constant for each image. Combined with <phrase>SPP</phrase>, this <phrase>results</phrase> in an excellent and fast detector that does not require to process an image with <phrase>algorithms</phrase> other than the <phrase>CNN</phrase> itself. We also <phrase>streamline</phrase> and simplify the training of <phrase>CNN</phrase>-based detectors by integrating several learning steps in a <phrase>single</phrase> <phrase>algorithm</phrase>, as well as by proposing a number of improvements that accelerate detection.
<phrase>Motivation</phrase> in <phrase>Game</phrase>-Based Learning: It's More than 'Flow' Educational computer <phrase>games</phrase> are sometimes considered to be motivating per se because of the playing activity-and do not require further moti-vational strategies. Therefore the concept of 'flow experience' is commonly used to describe how smooth gaming naturally <phrase>results</phrase> in deep engagement and, consequently, enhanced learning. Learning with an instructional <phrase>game</phrase> without discovering the instructional parts, seems to be a crucial goal considering the presumed needs of today's <phrase>digital</phrase> natives. However, teaching factual <phrase>knowledge</phrase> and the need for educational guidance, assessment and other intrusive components impede the creation of a <phrase>free</phrase> flowing <phrase>educational game</phrase> in contrast to non-educational <phrase>games</phrase>. The present <phrase>paper</phrase> claims, that an exclusively activity -focused view on <phrase>game</phrase>-based learning lacks important aspects of <phrase>motivation</phrase> on the basis of a broad motivational theory and a <phrase>game</phrase> <phrase>model</phrase> of <phrase>motivation</phrase>. Those aspects include personal and <phrase>game</phrase> characteristics, <phrase>usability</phrase> issues as well as anticipated outcomes. An integrative <phrase>model</phrase> of <phrase>game</phrase>-based learning is introduced and the theoretical considerations are transferred to the realization of a demonstrator <phrase>game</phrase> in the context of the <phrase>European</phrase> <phrase>research</phrase> project 80Days.
Introduction to <phrase>special issue</phrase> on <phrase>social computing</phrase>, behavioral modeling, and prediction <phrase>Social computing</phrase> is concerned with the study of social behavior and social context using computational systems. The recent surge of interest in <phrase>social media</phrase> is one exemplar of <phrase>social computing</phrase> <phrase>research</phrase>. Behavioral modeling helps to study and reproduce social behavior and allows for experimenting, <phrase>scenario planning</phrase>, and a <phrase>deep understanding</phrase> of behavior, patterns, and potential outcomes. The pervasive use of <phrase>computing</phrase> and <phrase>Internet</phrase> technologies provides an unprecedented environment for various novel social activities. <phrase>Social computing</phrase> facilitates behavioral modeling in <phrase>model</phrase>-building, pattern-<phrase>mining</phrase>, analysis, and prediction. Numerous interdisciplinary and interdependent systems are created and used to represent the diverse social and physical systems for investigating the interactions between groups, communities, or <phrase>nation-states</phrase>. This presents singular challenges to <phrase>knowledge</phrase> discovery from <phrase>data</phrase> and requires joint efforts from multiple disciplines to study <phrase>social computing</phrase>, and behavioral modeling in <phrase>order</phrase> to document <phrase>lessons learned</phrase> and develop novel theories, experiments , and methodologies. The goal is to enable us to create, experiment, and recreate an operational environment with a better understanding of the contributions from individual disciplines in <phrase>order</phrase> to <phrase>forge</phrase> joint interdisciplinary efforts to advance <phrase>science</phrase> and <phrase>research</phrase>. This <phrase>special issue</phrase> aims to promote an interdisciplinary, fledgling field of <phrase>social computing</phrase>, behavioral modeling, and prediction and to offer an interdisciplinary platform for researchers and practitioners to illustrate pressing needs, demonstrate challenging <phrase>research</phrase> issues, and showcase the <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>research</phrase> and development. The <phrase>special issue</phrase> idea was originated from the successful The workshop attracted 115 attendees from various disciplines such as <phrase>sociology</phrase>, behavioral <phrase>science</phrase>, <phrase>computer science</phrase>, <phrase>psychology</phrase>, <phrase>cultural</phrase> study, <phrase>information</phrase> systems, <phrase>operations research</phrase>, and biomedical infor-matics In <phrase>order</phrase> to reach a wider audience, we solicited submissions through various <phrase>professional</phrase> <phrase>mailing lists</phrase>. In the call for papers, we emphasized <phrase>research</phrase> issues, theories, and applications of <phrase>social computing</phrase>, behavioral modeling, and prediction and included a <phrase>long</phrase> list of topics of interests, such as group formation and <phrase>evolution</phrase>; <phrase>group representation</phrase> and profiling, social conventions and social contexts, intentions, behaviors, and relations; modeling, projection, and forecasting; <phrase>social network analysis</phrase> and <phrase>mining</phrase>, to name some. We received 38 submissions and carried out a two-phase <phrase>peer-review</phrase> process. Four papers were selected to be included
Replication and <phrase>Automation</phrase> of Expert Judgments: <phrase>Information Engineering</phrase> in Legal <phrase>E</phrase>-Discovery The retrieval of <phrase>digital</phrase> evidence responsive to discovery requests in civil litigation, known in the <phrase>United States</phrase> as " <phrase>e</phrase>-discovery, " presents several important and understudied conditions and challenges. Among the most important of these are (i) that the definition of responsiveness that governs the search effort can be learned and made explicit through effective interaction with the responding <phrase>party</phrase>, (<phrase>ii</phrase>) that the governing definition of responsiveness is generally complex, deriving both from considerations of <phrase>subject-matter</phrase> relevance and from considerations of litigation strategy, and (iii) that the result of the search effort is a set (rather than a ranked list) of documents, and sometimes a quite large set, that is turned over to the requesting <phrase>party</phrase> and that the responding <phrase>party</phrase> certifies to be an accurate and complete response to the request. This <phrase>paper</phrase> describes the <phrase>design</phrase> of an " Interactive Task " for the Text Retrieval Conference's Legal <phrase>Track</phrase> that had the evaluation of the effectiveness of <phrase>e</phrase>-discovery applications at the " responsive review " task as its goal. Notable features of the 2008 Interactive Task were <phrase>high-fidelity</phrase> <phrase>human</phrase>-system task modeling, <phrase>authority control</phrase> for the definition of " responsiveness, " and relatively deep sampling for estimation of type 1 and type 2 errors (expressed as " precision " and " recall "). The <phrase>paper</phrase> presents a critical assessment of the strengths and weaknesses of the evaluation <phrase>design</phrase> from the perspectives of reliability, reusability, and cost-benefit tradeoffs.
Learning to Perform <phrase>Physics</phrase> Experiments via <phrase>Deep Reinforcement Learning</phrase> When encountering novel objects, humans are able to infer a wide <phrase>range</phrase> of physical properties such as <phrase>mass</phrase>, <phrase>friction</phrase> and deformability by interacting with them in a goal driven way. This process of active interaction is in the same <phrase>spirit</phrase> as a <phrase>scientist</phrase> performing experiments to discover hidden facts. <phrase>Recent advances</phrase> in <phrase>artificial intelligence</phrase> have yielded machines that can achieve superhuman performance in Go, <phrase>Atari</phrase>, <phrase>natural language processing</phrase>, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a <phrase>basic</phrase> set of tasks that require agents to estimate properties such as <phrase>mass</phrase> and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that <phrase>state</phrase> of <phrase>art</phrase> <phrase>deep reinforcement learning</phrase> methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering <phrase>information</phrase> against the cost of making mistakes in different situations.
Query expansion for mixed-script <phrase>information retrieval</phrase> For many languages that use non-<phrase>Roman</phrase> based <phrase>indigenous</phrase> scripts (e.g., <phrase>Arabic</phrase>, <phrase>Greek</phrase> and <phrase>Indic</phrase> languages) one can often find a large amount of <phrase>user generated</phrase> transliterated content on the Web in the <phrase>Roman</phrase> script. Such content creates a monolingual or multi-lingual space with more than one script which we refer to as the Mixed-Script space. IR in the mixed-script space is challenging because queries written in either the <phrase>native</phrase> or the <phrase>Roman</phrase> script need to be matched to the documents written in both the scripts. Moreover, transliterated content features extensive spelling variations. In this <phrase>paper</phrase>, we formally introduce the concept of Mixed-Script IR, and through analysis of the query logs of Bing <phrase>search engine</phrase>, estimate the prevalence and thereby establish the importance of this problem. We also give a principled <phrase>solution</phrase> to handle the mixed-script term matching and spelling variation where the terms across the scripts are modelled jointly in a <phrase>deep-learning</phrase> <phrase>architecture</phrase> and can be compared in a <phrase>low-dimensional</phrase> abstract space. We present an extensive empirical analysis of the <phrase>proposed method</phrase> along with the evaluation <phrase>results</phrase> in an <phrase>ad-hoc</phrase> retrieval setting of mixed-script IR where the <phrase>proposed method</phrase> achieves significantly better <phrase>results</phrase> (12% increase in MRR and 29% increase in MAP) compared to other <phrase>state</phrase>-of-the-<phrase>art</phrase> baselines.
Recurrent Clustering for Unsupervised <phrase>Feature Extraction</phrase> with Application to <phrase>Sequence</phrase> Detection In many <phrase>unsupervised learning</phrase> applications both spatial and temporal regularities in the <phrase>data</phrase> need to be represented. Traditional clustering <phrase>algorithms</phrase>, which are commonly employed by <phrase>unsupervised learning</phrase> engines, lack the ability to naturally capture temporal dependencies. In <phrase>supervised learning</phrase> methods, temporal features are often learned through the use of a <phrase>feedback</phrase> (or recurrent) signal. <phrase>Drawing</phrase> inspiration from the Elman <phrase>recurrent neural network</phrase>, we introduce a winner-take-all based recurrent clustering <phrase>algorithm</phrase> that is able to identify temporal regularities in an unsupervised manner. We explore the potential pitfalls that result from adding <phrase>feedback</phrase> to an incremental clustering <phrase>algorithm</phrase> and apply the proposed technique to several <phrase>time series</phrase> inference problems in the context of <phrase>semi-supervised</phrase> learning. The <phrase>results</phrase> clearly indicate that the framework can be broadly applied with particular relevance to scalable deep <phrase>machine learning</phrase> architectures.
Practical Detection of Spammers and Content Promoters in Online <phrase>Video</phrase> Sharing Systems A number of online <phrase>video</phrase> sharing systems, out of which <phrase>YouTube</phrase> is the most popular, provide features that allow users to post a <phrase>video</phrase> as a response to a discussion topic. These features open opportunities for users to introduce polluted content, or simply <phrase>pollution</phrase>, into the system. For instance, spammers may post an unrelated <phrase>video</phrase> as response to a popular one, aiming at increasing the likelihood of the response being viewed by a larger number of users. Moreover, content promoters may try to gain visibility to a specific <phrase>video</phrase> by posting a large number of (potentially unrelated) responses to boost the rank of the responded <phrase>video</phrase>, making it appear in the top lists maintained by the system. Content <phrase>pollution</phrase> may jeopardize the trust of users on the system, thus compromising its success in promoting social interactions. In spite of that, the available <phrase>literature</phrase> is very limited in providing a <phrase>deep understanding</phrase> of this problem. In this <phrase>paper</phrase>, we address the issue of detecting <phrase>video</phrase> spammers and promoters. Towards that end, we first manually build a <phrase>test</phrase> collection of real <phrase>YouTube</phrase> users, classifying them as spammers, promoters, and legitimate users. Using our <phrase>test</phrase> collection, we provide a characterization of content, individual, and social attributes that help distinguish each user class. We then investigate the feasibility of using supervised classification <phrase>algorithms</phrase> to automatically detect spammers and promoters, and assess their effectiveness in our <phrase>test</phrase> collection. While our classification approach succeeds at separating spammers and promoters from legitimate users, the <phrase>high</phrase> cost of manually labeling vast amounts of examples compromises its full potential in realistic scenarios. For this reason, we further propose an <phrase>active learning</phrase> approach that automatically chooses a set of examples to <phrase>label</phrase>, which is likely to provide the highest amount of <phrase>information</phrase>, drastically reducing the amount of required <phrase>training data</phrase> while maintaining comparable classification effectiveness.
Deep <phrase>feature learning</phrase> for <phrase>cover song</phrase> identification
Compositional <phrase>Memory</phrase> for Visual <phrase>Question Answering</phrase> Visual <phrase>Question Answering</phrase> (VQA) emerges as one of the most fascinating topics in <phrase>computer vision</phrase> recently. Many <phrase>state</phrase> of the <phrase>art</phrase> methods naively use holistic <phrase>visual features</phrase> with <phrase>language</phrase> features into a <phrase>Long</phrase> <phrase>Short-Term Memory</phrase> (LSTM) module, neglecting the sophisticated interaction between them. This coarse modeling also blocks the possibilities of exploring finer-grained local features that contribute to the <phrase>question answering</phrase> dynamically over time. This <phrase>paper</phrase> addresses this fundamental problem by directly modeling the temporal dynamics between <phrase>language</phrase> and all possible local <phrase>image patches</phrase>. When traversing the question words sequentially, our <phrase>end-to-end</phrase> approach explicitly fuses the features associated to the words and the ones available at multiple local patches in an attention mechanism, and further combines the fused <phrase>information</phrase> to generate dynamic messages, which we call episode. We then feed the episodes to a standard <phrase>question answering</phrase> module together with the contextual visual <phrase>information</phrase> and <phrase>linguistic</phrase> <phrase>information</phrase>. Motivated by recent practices in <phrase>deep learning</phrase>, we use <phrase>auxiliary</phrase> loss functions during training to improve the performance. Our experiments on two latest <phrase>public</phrase> datasets suggest that our method has a <phrase>superior</phrase> performance. Notably, on the DAQUAR dataset we advanced the <phrase>state</phrase> of the <phrase>art</phrase> by 6%, and we also evaluated our approach on the most recent MSCOCO-VQA dataset.
<phrase>Peer Review</phrase> to Improve <phrase>Artificial Intelligence</phrase> Teaching Using a teamwork , project-based methodology is a common approach when teaching <phrase>Artificial Intelligence</phrase>. However, a <phrase>major</phrase> drawback of such approach is that <phrase>AI</phrase> courses comprise a wide syllabus composed of quite <phrase>independent</phrase> topics. In consequence, students focus on one <phrase>single</phrase> topic from the entire course contents: although <phrase>deep learning</phrase> of such topic is probably ensured, learning of the rest of the contents is also probably much more superficial. In this <phrase>paper</phrase>, <phrase>peer review</phrase> is proposed as a <phrase>complement</phrase> to <phrase>project-based learning</phrase>. Students work not only on their project about a chosen topic, but also review <phrase>peers</phrase>' projects on distinct topics, providing them with a wider comprehension of the global syllabus of the course. Empirical <phrase>results</phrase> of the application of this approach to an actual course on <phrase>Artificial Intelligence</phrase> for senior students in <phrase>Telecommunication</phrase> <phrase>Engineering</phrase> are presented too. Analysis focuses on the effects of the reviewing task, as it is the one which broadens students learning. Positive <phrase>results</phrase> have been achieved, thus validating the interest of <phrase>peer review</phrase> as a useful instrument for learning improvement.
Tradeoffs between Convergence Speed and <phrase>Reconstruction</phrase> Accuracy in Inverse Problems Solving inverse problems with iterative <phrase>algorithms</phrase> such as <phrase>stochastic gradient descent</phrase> is a popular technique, especially for large <phrase>data</phrase>. In applications, due to time constraints, the number of iterations one may apply is usually limited, consequently limiting the accuracy achievable by certain methods. Given a <phrase>reconstruction</phrase> error one is willing to tolerate, an important question is whether it is possible to modify the original iterations to obtain a faster convergence to a minimizer with the allowed error. Relying on recent recovery techniques developed for settings in which the desired signal belongs to some <phrase>low-dimensional</phrase> set, we show that using a coarse estimate of this set leads to faster convergence to an error related to the accuracy of the set approximation. Our theory ties to <phrase>recent advances</phrase> in sparse recovery, <phrase>compressed sensing</phrase> and <phrase>deep learning</phrase>. In particular, it provides an explanation for the successful approximation of the ISTA <phrase>solution</phrase> by <phrase>neural networks</phrase> with layers representing iterations.
<phrase>Multi-View</phrase> <phrase>Discriminant</phrase> <phrase>Transfer Learning</phrase> We study to incorporate multiple views of <phrase>data</phrase> in a perceptive <phrase>transfer learning</phrase> framework and propose a <phrase>Multi-view</phrase> <phrase>Discriminant</phrase> Transfer (MDT) learning approach for <phrase>domain adaptation</phrase>. The main idea is to find the optimal <phrase>discriminant</phrase> weight vectors for each view such that the correlation between the two-view projected <phrase>data</phrase> is maximized, while both the domain discrepancy and the view disagreement are minimized simultaneously. Furthermore, we analyze MDT theoretically from <phrase>discriminant</phrase> analysis perspective to explain the condition and reason , under which the <phrase>proposed method</phrase> is not applicable. The analytical <phrase>results</phrase> allow us to investigate whether there exist within-view and/or between-view conflicts, and thus provides a deep insight into whether the <phrase>transfer learning</phrase> <phrase>algorithm</phrase> work properly or not in the view-based problems and the combined learning problem. Experiments show that MDT <phrase>significantly outperforms</phrase> the <phrase>state</phrase>-of-the-<phrase>art</phrase> baselines including some typical <phrase>multi-view</phrase> learning approaches in <phrase>single</phrase>-or cross-domain.
Educating Reflective Learner Centered Designers Learning <phrase>Design</phrase> versus <phrase>Interaction Design</phrase> In this <phrase>paper</phrase> we discuss a multidisciplinary course for educating reflective designers of <phrase>technology</phrase>-enhanced <phrase>learning environments</phrase>. The course supports the coordination of students in two disciplines, <phrase>human</phrase>-computer interaction and instructional <phrase>systems design</phrase>. Through the multidisciplinary teaming and an <phrase>ontology</phrase> of <phrase>design</phrase> processes, students in the course are encouraged to develop <phrase>reflective practice</phrase> as educational designers. Learner-centered <phrase>design</phrase> of <phrase>technology</phrase>-enhanced educational environments (learning <phrase>technology</phrase> <phrase>design</phrase>, or LT <phrase>design</phrase> for <phrase>short</phrase>) requires both a focus on the learning goals for the <phrase>design</phrase>, and the ability to connect learning goals to their implementation in interfaces. While the ends, learning, may be relatively clear, it requires special expertise to develop the means for supporting learning through technologies. Early proponents of learner-centered <phrase>design</phrase> (Soloway et al.) were cross-trained individuals with a background in both <phrase>computer science</phrase> and <phrase>education</phrase>. The TRAILS project (http://www.trails-project.org) has as its aim producing more such individuals who can successfully integrate <phrase>usability</phrase> and <phrase>interface design</phrase> with educational theory and practice to produce effective <phrase>learning environments</phrase> (see DiGiano et. al., 2005.) In this <phrase>paper</phrase>, we first discuss our goals for the <phrase>Penn State</phrase> TRAILS course, including a blend of both learning <phrase>design</phrase> and <phrase>interaction design</phrase>. We then discuss our pedagogical strategy for this goal, namely using <phrase>reflective practice</phrase> of <phrase>design methods</phrase> in an interdisciplinary context. We then describe the course in detail, including our teaming strategy, projects and assignments to promote <phrase>student</phrase> reflection and practice of various <phrase>design methods</phrase>, and an <phrase>ontology</phrase> of <phrase>design methods</phrase> used to help students compare and contrast various learning and <phrase>interaction design</phrase> methodologies. The <phrase>design</phrase> of successful technologies for learning must meet a wide <phrase>variety</phrase> of goals. Learning goals and <phrase>interaction design</phrase> goals may conflict. For instance, <phrase>user-centered design</phrase> emphasizes utility and efficiency for the user in accomplishing tasks. (<phrase>UCD</phrase> <phrase>cites</phrase>) However, designers of user-centered <phrase>products</phrase> might not even consider learning as a goal at all, much less intentionally include affordances for, or minimize impediments to, learningfor instance, an educationally <phrase>sound</phrase> interface might need to introduce " desirable difficulties " (cite) to allow a learner to improve their understanding, even though this might slow down their progress towards the immediate goal of completing an exercise. In <phrase>order</phrase> to create designs that meet this <phrase>variety</phrase> of goals, a designer needs to have a <phrase>deep understanding</phrase> of techniques from many domains. Many <phrase>user-interface</phrase> designers may assume that since they have been learners and since they have been, for instance, children, 
Question Classification using Maximum <phrase>Entropy</phrase> Models User demand for a more powerful search has introduced <phrase>research</phrase> in <phrase>Question Answering</phrase> (QA) systems. QA systems return the exact answer to a users question. This differs to current <phrase>search engines</phrase> such as <phrase>Google</phrase>, which return a list of documents that may be relevant. Components within a QA system require <phrase>deep linguistic</phrase>, <phrase>semantic</phrase> and <phrase>syntactic</phrase> analysis of questions posed by users and potential answers within documents. One such component is Question Classification (<phrase>QC</phrase>). This involves determining what a question is asking and classifying it into its corresponding answer type. Current <phrase>QC</phrase> systems use <phrase>Machine Learning</phrase> methods to perform this task. Maximum <phrase>Entropy</phrase> Modelling, a statistical technique, has been successfully used in many areas of <phrase>Natural Language Processing</phrase> however has not been applied to <phrase>QC</phrase>. This project involves identifying a rich feature set for <phrase>QC</phrase> using Maximum <phrase>Entropy</phrase> Models. <phrase>Results</phrase> will show that this technique achieves a <phrase>state</phrase>-of-the-<phrase>art</phrase> accuracy.
<phrase>Human</phrase> <phrase>Language</phrase> <phrase>Technology</phrase> And <phrase>Knowledge Management</phrase> - Final Roadmap Session <phrase>Ontology</phrase> Creation/<phrase>Population</phrase> Tasks <-> Tools-reusable across domains Understand a process <phrase>model</phrase> (and human's role in this) <phrase>Semantic Web</phrase> User-centered process view Convert the (HCI) disbelievers and keep them practicing "top" or core <phrase>ontology</phrase> (use this to bootstrap new domains), <phrase>Ontology</phrase> integration Rapid customization (to specific domains) Use <phrase>domain specific</phrase> <phrase>ontologies</phrase> to organize massive documents Find, learn, collaboration with domain <phrase>ontology</phrase> creators Integration of shallow/deep methods <phrase>ONTOLOGY</phrase> Problems-<phrase>Ontology</phrase> quality-Access to info, <phrase>knowledge</phrase> visualizations-Understanding-Ambiguity <phrase>ONTOLOGY</phrase> Approaches Relation of HLT to <phrase>ontological</phrase> tasks KR, linguisits, & <phrase>ontologies</phrase> to jointly address Component based methods for <phrase>Life</phrase> cycle Re-use Decomposition Use HLT to support <phrase>knowledge</phrase> audits > Identify IP-> <phrase>innovation</phrase> Context capture Controlled, <phrase>language</phrase> <phrase>management</phrase> <phrase>ONTOLOGY</phrase> Solutions Plug-in (for IE) <phrase>Semantic Web</phrase> Tools to leverage small <phrase>ontologies</phrase>-> large <phrase>ontologies</phrase> <phrase>Interlingua</phrase> approach Statistical-> deeply annotated <phrase>data</phrase> + <phrase>machine learning</phrase> <phrase>Translation</phrase> memories + ML Multimodal/<phrase>multimedia</phrase> sols Multiple <phrase>ontologies</phrase> tailored to users, tasks Content-driven hypertextual authoring Cross-lingual <phrase>news</phrase> linking Advanced <phrase>software</phrase> technologies/platform <phrase>Communication</phrase>/transaction success
Wishart Deep Stacking Network for Fast POLSAR <phrase>Image Classification</phrase> Inspired by the popular <phrase>deep learning</phrase> <phrase>architecture</phrase> - Deep Stacking Network (DSN), a specific deep <phrase>model</phrase> for polarimetric <phrase>synthetic aperture radar</phrase> (POLSAR) <phrase>image classification</phrase> is proposed in this <phrase>paper</phrase>, which is named as Wishart Deep Stacking Network (W-DSN). First of all, a fast implementation of Wishart distance is achieved by a special <phrase>linear transformation</phrase>, which speeds up the classification of POLSAR image and makes it possible to use this polarimetric <phrase>information</phrase> in the following <phrase>Neural Network</phrase> (NN). Then a <phrase>single</phrase>-<phrase>hidden-layer</phrase> <phrase>neural network</phrase> based on the fast Wishart distance is defined for POLSAR <phrase>image classification</phrase>, which is named as Wishart Network (WN) and improves the <phrase>classification accuracy</phrase>. Finally, a <phrase>multi-layer</phrase> <phrase>neural network</phrase> is formed by stacking WNs, which is in fact the proposed <phrase>deep learning</phrase> <phrase>architecture</phrase> W-DSN for POLSAR <phrase>image classification</phrase> and improves the <phrase>classification accuracy</phrase> further. In addition, the structure of WN can be expanded in a straightforward way by adding <phrase>hidden units</phrase> if necessary, as well as the structure of the W-DSN. As a preliminary exploration on formulating specific <phrase>deep learning</phrase> <phrase>architecture</phrase> for POLSAR <phrase>image classification</phrase>, the proposed methods may establish a simple but clever connection between POLSAR image interpretation and <phrase>deep learning</phrase>. The experiment <phrase>results</phrase> tested on real POLSAR image show that the fast implementation of Wishart distance is very efficient (a POLSAR image with 768000 <phrase>pixels</phrase> can be classified in 0.53s), and both the <phrase>single</phrase>-<phrase>hidden-layer</phrase> <phrase>architecture</phrase> WN and the <phrase>deep learning</phrase> <phrase>architecture</phrase> W-DSN for POLSAR <phrase>image classification</phrase> perform well and work efficiently.
<phrase>Language</phrase> Identification from Text Documents The increase in the use of <phrase>microblogging</phrase> came along with the rapid growth on <phrase>short</phrase> <phrase>linguistic</phrase> <phrase>data</phrase>. On the other hand <phrase>deep learning</phrase> is considered to be the new frontier to extract meaningful <phrase>information</phrase> out of large amount of raw <phrase>data</phrase> in an automated manner [1]. In this study, we engaged these two emerging fields to come up with a robust <phrase>language</phrase> identifier on demand, namely <phrase>Stanford</phrase> <phrase>Language</phrase> Identification <phrase>Engine</phrase> (<phrase>SLIDE</phrase>). As a result, we achieved 95.12% accuracy in Discriminating between Similar Languages (<phrase>DSL</phrase>) Shared Task 2015 dataset, beating the maximum reported accuracy achieved so far [2].
<phrase>Data</phrase> Extraction of a Novel Method for Clustering Alignment Based on Combining Tag and Value Similarity Clustering is a <phrase>data mining</phrase> (<phrase>machine learning</phrase>) technique used to place <phrase>data</phrase> elements into related groups without advance <phrase>knowledge</phrase> of the group definitions. <phrase>Data</phrase> Extraction is the process of retrieving <phrase>data</phrase> out of <phrase>data</phrase> sources. Establishment of an updated method called a novel <phrase>data</phrase> extraction and alignment method called CTVS that combines both tag and value similarity enhances the efficiency of the <phrase>data</phrase> extraction and alignment. Record alignment <phrase>algorithm</phrase> has been introduced in <phrase>order</phrase> to perform efficient contemporary alignment method first pair wise and then holistically. Threshold index formula has also been introduced to find the <phrase>data</phrase> regions and to perform clustering methods. The applications of this extraction of <phrase>data records</phrase> also includes the concept of page ranking in <phrase>order</phrase> to speed up the <phrase>search engine</phrase>, clustering the similar <phrase>data</phrase> regions in <phrase>order</phrase> to perform efficient identification of <phrase>web pages</phrase> and similar Query result pages and applicable also in <phrase>data integration</phrase> and comparison shopping. I. INTRODUCTION Web <phrase>databases</phrase> comprises <phrase>deep web</phrase> from these <phrase>web sites</phrase> and <phrase>web pages</phrase> the extraction of the needed <phrase>data</phrase> from a web page is merely a complicated process. If a use user gives a query it automatically extracts the query result pages. Many <phrase>web applications</phrase>, such as Meta querying, <phrase>data integration</phrase> and comparison shopping, need the <phrase>data</phrase> from multiple web <phrase>databases</phrase>. For these applications to further utilize the <phrase>data</phrase> embedded in <phrase>HTML</phrase> pages, automatic <phrase>data</phrase> extraction is necessary. Only when the <phrase>data</phrase> are extracted and organized in a structured manner, such as tables, can they be compared and aggregated. Hence, accurate <phrase>data</phrase> extraction is vital for these applications to perform correctly. This <phrase>paper</phrase> focuses on the problem of automatically extracting <phrase>data records</phrase> that are encoded in the query result pages generated by web <phrase>databases</phrase>. In <phrase>general</phrase>, a query result page contains not only the actual <phrase>data</phrase>, but also other <phrase>information</phrase>, such as navigational panels, advertisements, comments, <phrase>information</phrase> about hosting sites, and so on. We employ the following two-step method, called Combining Tag and Value Similarity (CTVS), to extract the QRRs from a query result page p.
<phrase>Special Issue</phrase> on Control and Optimization in <phrase>Cooperative</phrase> Networks This <phrase>special issue</phrase> of the <phrase>SIAM</phrase> <phrase>Journal</phrase> on Control and Optimization is motivated by the emerging disciplines of <phrase>multi-agent</phrase> networks, distributed motion coordination, and <phrase>cooperative</phrase> control. It is foreseen that, in the near future, large numbers of coordinated devices communicating through <phrase>ad hoc</phrase> networks will perform a <phrase>variety</phrase> of challenging sensing tasks. The potential advantages of employing arrays of sensors are numerous: certain tasks are difficult, if not impossible, when performed by a <phrase>single</phrase> agent; and a group of agents inherently provides robustness to failures of <phrase>single</phrase> agents or <phrase>communication</phrase> links. Although the individual components of these networked systems are increasingly sophisticated, we lack a fundamental understanding of how to assemble and coordinate the individual physical devices into a coherent whole. As a consequence of this limitation, there exists a strong need for the integration of the sensing, <phrase>computing</phrase> and networking aspects of coordinated control of networks. This <phrase>special issue</phrase> gathers <phrase>recent developments</phrase> aimed at providing new sets of control and distributed optimization tools to address the <phrase>research</phrase> challenges posed by <phrase>multi-agent</phrase> networks. This volume presents 16 papers that deal with a wide <phrase>range</phrase> of coordination tasks such as consensus, connectivity maintenance, spatial process estimation, formation stabilization, localization, coverage, and <phrase>target</phrase> detection. The topics considered include the characterization of convergence speeds, the <phrase>design</phrase> of <phrase>algorithms</phrase> that tolerate delays, noisy measurements, and packet drops, and the study of the <phrase>controllability</phrase> of <phrase>graph</phrase> structures and the influence of the interconnection <phrase>topol</phrase>-ogy in the control <phrase>design</phrase>. Collectively, the papers detail deep connections between a wide <phrase>variety</phrase> of scientific disciplines, including <phrase>cooperative</phrase>, distributed and decentralized <phrase>control theory</phrase>, distributed <phrase>algorithms</phrase> and systems, <phrase>communication</phrase> networks, <phrase>graph theory</phrase>, geometric optimization, differential and <phrase>computational geometry</phrase>, <phrase>game</phrase> and learning theory, and process estimation. We believe this <phrase>special issue</phrase> contains an excellent sample of <phrase>current research</phrase> on <phrase>multi-agent</phrase> networks and we envision that it will be of great interest to the broad readership of the <phrase>SIAM</phrase> <phrase>Journal</phrase> on Control and Optimization. A million thanks are due to all the authors who submitted their work to this <phrase>special issue</phrase>. We would also like to thank the proficient and timely editorial support provided by Brian Fauth and Mitch Chernoff and the insightful and gracious advice that we received from the <phrase>editor-in-chief</phrase>, John Baillieul. Benedetto Piccoli Istituto per le Applicazioni del Calcolo " <phrase>Mauro Picone</phrase> " Guest editors vii
Protograph-based Generalized Ldpc Codes: Enumerators, <phrase>Design</phrase>, and Applications As members of the Final Examination Committee, we certify that we have read the dissertation prepared by Shadi <phrase>Ali</phrase> Abu-Surra entitled " Protograph-Based Generalized LDPC Codes: Enumerators, <phrase>Design</phrase> , and Applications " and recommend that it be accepted as fulfilling the dissertation requirement for the <phrase>Degree</phrase> of Doctor of <phrase>Philosophy</phrase>. Final approval and acceptance of this dissertation is contingent upon the candi-date's submission of the final copies of the dissertation to the Graduate <phrase>College</phrase>. I hereby certify that I have read this dissertation prepared under my direction and recommend that it be accepted as fulfilling the dissertation requirement. This dissertation has been submitted in partial fulfillment of requirements for an advanced <phrase>degree</phrase> at The <phrase>University</phrase> of <phrase>Arizona</phrase> and is deposited in the <phrase>University</phrase> <phrase>Library</phrase> to be made available to borrowers under rules of the <phrase>Library</phrase>. Brief quotations from this dissertation are allowable without special permission, provided that accurate acknowledgment of source is made. Requests for permission for extended quotation from or reproduction of this <phrase>manuscript</phrase> in whole or in part may be granted by the head of the <phrase>major</phrase> <phrase>department</phrase> or the <phrase>Dean</phrase> of the Graduate <phrase>College</phrase> when in his or her judgment the proposed use of the material is in the interests of <phrase>scholarship</phrase>. In all other instances, however, permission must be obtained from the <phrase>copyright</phrase> holder. 4 ACKNOWLEDGEMENTS First, I would like to thank my advisor <phrase>Professor</phrase> William Ryan for his continuing support and advice. I have learned so much through him and his experience in <phrase>channel</phrase> coding. I am extremely appreciative to him for all his help and for his abundant proofreadings that greatly improved this work. Without his guidance this dissertation would not have been possible. I would like to give a special thanks to Dr. Dariush Divsalar for teaching me how to have a handle on protograph-based LDPC codes. His numerous insights and strong experience on this topic motivated me to dig deeper and look at the bigger picture, which allowed me to solve many of the challenging problems presented in this work. I would also like to <phrase>express my deep</phrase> gratitude to Dr. Gianluigi Liva for the valuable insight and the interesting discussions, specially, at the start of my work on generalized LDPC codes. Also, I would like to acknowledge my lab partners Fei for helping me keep my sanity and being a great team to work with. I also appreciate the financial support 
The backwash effect on <phrase>SQL</phrase> skills grading This <phrase>paper</phrase> examines the effect of grading approaches for <phrase>SQL</phrase> query formulation on students' <phrase>learning strategies</phrase>. The way that students are graded in a subject has a significant impact on their learning approach, and it is crucial that graded tasks are carefully designed and implemented to inculcate a <phrase>deep learning</phrase> experience. An online examination system is described and evaluated.
Supersizing <phrase>Social Studies</phrase> through the Use of <phrase>Web 2.0</phrase> Technologies This article seeks to demonstrate how <phrase>social studies</phrase> has come to be an all-inclusive subject: it has become supersized. When supported by <phrase>Web 2.0</phrase> <phrase>technology</phrase>, <phrase>social studies</phrase> enables students to address multifaceted problems that require the <phrase>deep understanding</phrase> necessary to arrive at both wise and timely solutions. We discuss how <phrase>curriculum</phrase> integration and emerging <phrase>technology</phrase> applications can support the supersizing of <phrase>social studies</phrase>. Two instructional projects and two instructional tools are presented as examples of how <phrase>social studies</phrase> can be supersized through the use of <phrase>Web 2.0</phrase> technologies. Introduction We are living in a world that is smaller, more connected, and better informed than ever before, yet, in some instances, <phrase>social studies</phrase> instruction looks and feels like it did 50 <phrase>years ago</phrase> (Crocco & Cramer, 2005; Holcomb & Beal, 2010). We believe <phrase>social studies</phrase> today needs innovative ideas and bold approaches to address the new conditions for learning. In this article, we suggest supersizing <phrase>social studies</phrase> by taking advantage of emerging technologies. The use of these tools and related <phrase>21st century</phrase> skills will allow <phrase>social studies</phrase> to position itself in the <phrase>curriculum</phrase> as the " go-to " core subject. In supersized <phrase>social studies</phrase>, multidisciplinary issues come together, enabling students to engage in <phrase>authentic learning</phrase> by using <phrase>real world</phrase> problems and issues. Although the term supersize generally is associated with <phrase>fast food</phrase> and may bring to mind transactions driven by greed and excess, we intend something entirely different. We want to offer a vision of supersized <phrase>social studies</phrase> that reflects how this subject has evolved over time. Today, <phrase>social studies</phrase> teachers should address complex global issues by challenging students to use <phrase>knowledge</phrase> of <phrase>social studies</phrase> and <phrase>21st century</phrase> skills to address and solve the world's problems.
The <phrase>Jordan Curve Theorem</phrase>, Formally and Informally 1. INTRODUCTION. The <phrase>Jordan curve theorem</phrase> states that every simple closed <phrase>pla</phrase>-nar curve separates the plane into a bounded interior <phrase>region</phrase> and an unbounded exterior. One hundred <phrase>years ago</phrase>, <phrase>Oswald Veblen</phrase> declared that this theorem is " justly regarded as a most important step in the direction of a perfectly rigorous <phrase>mathematics</phrase> " [13, p. 83]. Its position as a benchmark of <phrase>mathematical</phrase> rigor has continued to our day. Many vastly underestimate the logical <phrase>gulf</phrase> that separates a typical published proof from a fully formal <phrase>mathematical proof</phrase>, in which every <phrase>single</phrase> logical inference has been generated and checked by computer. A striking example of this logical <phrase>gulf</phrase> can be found in Bourbaki's Elements of Sets. How many primitive symbols of <phrase>logic</phrase> does it require to represent the number " 1 " in fully expanded form? <phrase>Bourbaki</phrase> estimates that the fully expanded representation may require " several tens of thousands of symbols. " The story has it that when A. R. Mathias learned of Bourbaki's estimate, he thought " that must be false, surely only a couple of hundred " are required. This <phrase>led</phrase> him to make a careful calculation and to uncover the astounding fact that Bourbaki's own estimate is wrong by several <phrase>orders of magnitude</phrase>. Over four trillion symbols are needed to express the number " 1 " [7]. Such is the logical <phrase>gulf</phrase>. It is fortunate that not all systems suffer from the same inefficiencies. Who hasn't heard the one about the <phrase>mathematician</phrase> who wakes up at night from the smoke of a fire in his <phrase>hotel</phrase>? Seeing a fire extinguisher, he notes that a <phrase>solution</phrase> to the problem exists and <phrase>falls</phrase> peacefully back into a deep <phrase>sleep</phrase>. Researchers from <phrase>Frege</phrase> to Gdel, who solved the problem of rigor in <phrase>mathematics</phrase>, found theoretical solutions but did not extinguish the fire, because they omitted the practical implementation. Some, such as <phrase>Bourbaki</phrase>, have even gone so far as to claim that " formalized <phrase>mathematics</phrase> cannot in practice be written down in full " and call such a project " absolutely unreal-izable " [1, pp. 10, 11]. While it is true that formal proofs may be too <phrase>long</phrase> to print, computerswhich do not have the same limitations as paperhave become the natural host of formal <phrase>mathematics</phrase>. In recent decades, insomniacs at the same <phrase>hotel</phrase> have reworked the foundations of <phrase>mathematics</phrase>, putting them in an efficient form designed for real use on 
Advancing <phrase>e-governance</phrase>: connecting learning and <phrase>action</phrase> <phrase>Electronic</phrase> Governance is defined in many ways, but all the definitions include an expectation of <phrase>innovation</phrase> and improvement in the <phrase>public sphere</phrase>. <phrase>Innovation</phrase> entails experimentation and <phrase>risk</phrase>. Improvement requires <phrase>deep understanding</phrase> of the context of <phrase>government</phrase>, as well as new tools and new ways of working. Accordingly, advancing <phrase>electronic</phrase> governance demands both learning and <phrase>action</phrase>. This <phrase>talk</phrase> will focus on ways to conceptualize, operationalize, and capitalize on ways to link <phrase>research</phrase> and practice in a shared and mutually rewarding enterprise aimed at better <phrase>public management</phrase> and better governance.
<phrase>Deep Learning</phrase> Approaches to <phrase>Semantic</phrase> Relevance Modeling for <phrase>Chinese</phrase> Question-Answer Pairs The <phrase>human</phrase>-generated question-answer pairs in the Web social communities are of great value for the <phrase>research</phrase> of automatic <phrase>question-answering</phrase> technique. Due to the large amount of noise <phrase>information</phrase> involved in such corpora, it is still a problem to detect the answers even though the questions are exactly located. Quantifying the <phrase>semantic</phrase> relevance between questions and their candidate answers is essential to answer detection in <phrase>social media</phrase> corpora. Since both the questions and their answers usually contain a small number of sentences, the relevance modeling methods have to overcome the problem of word feature sparsity. In this article, the <phrase>deep learning</phrase> principle is introduced to address the <phrase>semantic</phrase> relevance modeling task. Two <phrase>deep belief</phrase> networks with different architectures are proposed by us to <phrase>model</phrase> the <phrase>semantic</phrase> relevance for the question-answer pairs. According to the investigation of the textual similarity between the <phrase>community</phrase>-driven <phrase>question-answering</phrase> (cQA) dataset and the forum dataset, a learning strategy is adopted to promote our models&#8217; performance on the social <phrase>community</phrase> corpora without hand-annotating work. The <phrase>experimental</phrase> <phrase>results</phrase> show that our method outperforms the <phrase>traditional approaches</phrase> on both the cQA and the forum corpora.
Learning Grounded Communicative Intent from <phrase>Human</phrase>-<phrase>Robot</phrase> Dialog Studying how a <phrase>robot</phrase> can learn to communicate with a person provides insight into how <phrase>communication</phrase> might be learned in <phrase>general</phrase>. Deep models of dialog and communicative intent typically rely on modeling the internal <phrase>state</phrase> of the speakers states that are unobservable by a learning <phrase>robot</phrase>. This <phrase>paper</phrase> considers how <phrase>communication</phrase> can be framed to be learnable from experience. In particular, we describe how an agent might learn to communicate by building on three founda-tional capabilities, namely 1) an <phrase>observable</phrase> signal of satisfied intent (a smile), 2) the ability to imitate perceived actions, and 3) <phrase>perceptual</phrase> referents for discourse items. Early <phrase>simulation</phrase> <phrase>results</phrase> show that an agent can learn some <phrase>basic</phrase> <phrase>communication</phrase> skills from these foundations.
Technological <phrase>Pedagogical Content Knowledge</phrase> in <phrase>Teacher</phrase> Preparation: Impact of Coaching <phrase>Professional</phrase> Development and <phrase>Mobile</phrase> Devices This presentation explores the process of change for a <phrase>teacher</phrase> preparation program as it attempted to teach pre-service teachers how to integrate new technologies into instruction in <phrase>elementary</phrase> schools. The aim was to impact the instruction of preservice and inservice teachers in a way that would reach their current students. To become fully literate in today's world students must become proficient in new literacies and <phrase>21 st century</phrase> skills (<phrase>IRA</phrase>, May 2009). <phrase>Elementary</phrase> teachers in the <phrase>21 st century</phrase> need to have a <phrase>deep understanding</phrase> of new technologies and how they can be integrated into learning, however, the fast pace of technological <phrase>innovation</phrase> and <phrase>social change</phrase> makes it hard for educators to stay abreast of new developments and to integrate them into effective classroom instruction. At the same pre-service teachers sometimes lack the pedagogical <phrase>knowledge</phrase> required to integrate <phrase>technology</phrase> effectively into the existing <phrase>curriculum</phrase>. This project attempted to create a <phrase>long</phrase>-term partnership between <phrase>teacher</phrase> <phrase>education</phrase> programs, <phrase>universities</phrase> and <phrase>school districts</phrase> to create a generation of teachers who will be effective and confident using new technologies to prepare their students to participate and <phrase>lead</phrase> in the global <phrase>society</phrase>. <phrase>Teacher</phrase> <phrase>education</phrase> prepares preservice teachers with <phrase>knowledge</phrase> in <phrase>subject matter</phrase> and <phrase>pedagogy</phrase>. <phrase>Knowledge</phrase> of <phrase>subject matter</phrase> content includes facts, concepts, theories and procedures teachers will need to know in <phrase>order</phrase> to convey learning to their students. <phrase>Pedagogy</phrase> <phrase>knowledge</phrase> consists of a <phrase>variety</phrase> of methods to explain concepts, with frameworks to organize and connect ideas to help students apply new learning to their existing <phrase>knowledge</phrase>. Shulman (1987) suggested that teachers must also know how students generally understand their subjects, and areas that they consistently misunderstand. They, then, can anticipate these misunderstandings and know how to deal with them when they arise. This concept he called <phrase>Pedagogical Content Knowledge</phrase>. Teachers who possess <phrase>Pedagogical Content Knowledge</phrase> know the most useful forms of representation for the concepts they teach; the most powerful analogies to help students connect with classroom content. As <phrase>technology</phrase> becomes ubiquitous and integrated into <phrase>teaching and learning</phrase>, <phrase>Koehler</phrase> and Mishra (2009) argue that the most effective teaching takes place at the continuously changing intersection of three areas of <phrase>teacher</phrase> <phrase>knowledge</phrase>: content, <phrase>pedagogy</phrase>, and <phrase>technology</phrase>. They add Technological <phrase>Knowledge</phrase> as a third important component in planning effective <phrase>learning experiences</phrase>. <phrase>Koehler</phrase> et al. (2011) defined Technological <phrase>Knowledge</phrase> as knowing about print and <phrase>digital</phrase> technologies including how to operate, install, remove, create, and archive <phrase>information</phrase>. They called 
Improving <phrase>Markov</phrase>-based <phrase>TCP</phrase> <phrase>Traffic Classification</phrase> This <phrase>paper</phrase> presents an improved variant of our <phrase>Markov</phrase>-based <phrase>TCP</phrase> traffic classifier and demonstrates its performance using traffic captured in a <phrase>university</phrase> network. Payload length, flow direction, and position of the first <phrase>data</phrase> packets of a <phrase>TCP</phrase> connection are reflected in the states of the <phrase>Markov</phrase> models. In addition, we integrate a new " end of connection " <phrase>state</phrase> to further improve the <phrase>classification accuracy</phrase>. Using 10-fold <phrase>cross validation</phrase>, we identify appropriate settings for the payload length intervals and the number of <phrase>data</phrase> packets considered in the models. Finally, we discuss the classification <phrase>results</phrase> for the different applications. 1 Introduction <phrase>Traffic classification</phrase> based on <phrase>port</phrase> numbers has reached its limits since a one-to-one mapping between <phrase>port</phrase> and application does not exist for many modern network services. Examples are <phrase>peer-to-peer</phrase> applications, which often allocate ports dynamically, and <phrase>web applications</phrase>, such as streaming servers, which run on top of HTTP on the same <phrase>port</phrase> 80 or 443. Classification based on <phrase>deep packet inspection</phrase> (DPI) involves <phrase>pattern matching</phrase> on packet payload which is computationally intensive. Moreover, DPI is restricted to unencrypted traffic. Therefore, network operators are interested in <phrase>statistical classification</phrase> methods which consider traffic characteristics beyond packet contents. The common assumption of statistical <phrase>traffic classification</phrase> is that the <phrase>communication</phrase> behavior of an application influences the resulting traffic. Hence, by observing appropriate traffic properties, it should be possible to distinguish applications with different behaviors. In the <phrase>literature</phrase>, the application of <phrase>machine learning</phrase> techniques to the <phrase>traffic classification</phrase> problem is very popular. Examples can be found in the survey <phrase>paper</phrase> of Nguyen and Armitage [8]. Similarly, multiple approaches to <phrase>traffic classification</phrase> are evaluated and compared in the <phrase>paper</phrase> of Kim et al. [6]. Among other approaches, the authors consider various <phrase>machine learning</phrase> <phrase>algorithms</phrase> with <phrase>Support Vector Machines</phrase> being the most accurate. In the article of Este et al. [4], the amount of <phrase>information</phrase> carried by various traffic features is investigated. The authors evaluate the <phrase>mutual information</phrase> of the traffic features and the application type. They conclude that the packet sizes exhibit the highest amount of <phrase>information</phrase> concerning the application type for multiple <phrase>data</phrase> sets. In a previous <phrase>paper</phrase>, we present a novel <phrase>TCP</phrase> <phrase>traffic classification</phrase> approach using <phrase>observable</phrase> left-right <phrase>Markov</phrase> models [7]. This work is motivated by Estevez-Tapiador et al. who deploy <phrase>observable</phrase> <phrase>ergodic</phrase> <phrase>Markov</phrase> models for detecting anomalies in <phrase>TCP</phrase> connections [5].
Tiled <phrase>convolutional neural networks</phrase> <phrase>deep belief</phrase> networks for scalable <phrase>unsupervised learning</phrase> of hierarchical representations. In ICML, 2009. What is the best multi-stage <phrase>architecture</phrase> for <phrase>object recognition</phrase>? In ICCV, 2009. [4] A. Hyvarinen and P. Hoyer. Topographic <phrase>independent component analysis</phrase> as a <phrase>model</phrase> of V1 <phrase>organization</phrase> and <phrase>receptive fields</phrase>. <phrase>Algorithm</phrase> <phrase>Convolutional neural networks</phrase> [1] work well for many <phrase>recognition tasks</phrase>:-<phrase>Local receptive</phrase> fields for computational reasons-Weight sharing gives translational invariance However, weight sharing can be restrictive because it prevents us from learning other kinds of invariances. Abstract <phrase>Convolutional neural networks</phrase> (CNNs) have been successfully applied to many tasks such as digit and <phrase>object recognition</phrase>. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the <phrase>architecture</phrase>. In this <phrase>paper</phrase>, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled <phrase>convolutional neural networks</phrase> (Tiled CNNs), which use a regular " tiled " pattern of tied weights that does not require that adjacent <phrase>hidden units</phrase> share identical weights, but instead requires only that <phrase>hidden units</phrase> k steps away from each other to have tied weights. By pooling over neighboring units, this <phrase>architecture</phrase> is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs' advantage of having a relatively small number of learned parameters (such as ease of learning and greater <phrase>scalability</phrase>). We provide an efficient <phrase>learning algorithm</phrase> for Tiled CNNs based on Topographic ICA, and show that learning complex <phrase>invariant features</phrase> allows us to achieve highly competitive <phrase>results</phrase> for both the NORB and CIFAR-10 datasets. TICA <phrase>network architecture</phrase> Evaluating benefits of convolutional training Training on 8x8 samples and using these weights in a Tiled <phrase>CNN</phrase> obtains only 51.54% on the <phrase>test</phrase> set compared to 58.66% using our <phrase>proposed method</phrase>. Networks learn concepts like edge detectors, corner detectors Invariant to <phrase>translation</phrase>, rotation and scaling <phrase>Algorithms</phrase> for pretraining <phrase>convolutional neural networks</phrase> [2,3] do not use untied weights to learn invariances. TICA can be used to pretrain Tiled CNNs because it can learn invariances even when trained only on <phrase>unlabeled data</phrase> [4, 5]. Tiled CNNs are more flexible and usually better than fully <phrase>convolutional neural networks</phrase>. Pretraining with TICA finds invariant and <phrase>discriminative features</phrase> and works well with finetuning.
Books in Brief echo those of Richards' <phrase>book</phrase>, such as the need to go beyond classic <phrase>epidemiological</phrase> and bio-<phrase>medical</phrase> approaches. I was inspired by the edi-tors' stated ethos of striving to understand and value the work of others. The contributors, however, are largely <phrase>American</phrase>, as are the perspectives on the <phrase>politics</phrase> of <phrase>Ebola</phrase>, the <phrase>media</phrase> response and the absurdity of US <phrase>quarantine</phrase>. Richards, by contrast, shows that local <phrase>quarantine</phrase> was not new to the <phrase>villages</phrase> he studied, which have used it for <phrase>goat</phrase> <phrase>plague</phrase> and historically for <phrase>smallpox</phrase>. In 1976, my colleagues and I saw this in the first known outbreak of <phrase>Ebola</phrase>, in Yambuku in what is now the <phrase>Democratic</phrase> <phrase>Republic</phrase> of the <phrase>Congo</phrase> (<phrase>DR Congo</phrase>). Mdecins Sans Frontires (<phrase>MSF</phrase>) has been the main non-governmental responder in all <phrase>Ebola</phrase> outbreaks in <phrase>DR Congo</phrase> since 1995. In Ebola's Message, <phrase>MSF</phrase> <phrase>public-health</phrase> specialist Armand Sprecher offers a candid, sobering <phrase>essay</phrase> that reflects on the group's failure to partner with locals to control epidemics. He stresses the importance of skilled managers over <phrase>doctors</phrase>, who may have managerial skills only " by chance ". Indeed, <phrase>epidemic</phrase> control is about <phrase>logistics</phrase> and coordination. The final essays deal with challenging <phrase>ethical</phrase> issues. They are particularly relevant for <phrase>clinical research</phrase> in highly lethal epidemics. They provide balanced perspectives for deep disagreements on the <phrase>ethics</phrase> of trial <phrase>design</phrase>; the tension between trials, outbreak control and patient care; and the use of <phrase>experimental</phrase> interventions in <phrase>epidemic</phrase> times. To fight the next <phrase>epidemic</phrase>, as Sprecher writes, it will be crucial to apply these lessons , capitalize on partnerships and hold institutions to their promises of funding and co operation. And, as Richards mentions, it will also be key to consolidate ways of working effectively with local communities, and to recognize their work. This advice demands a <phrase>major</phrase> shift in the mindset of <phrase>epidemiologists</phrase>. A similarly devastating <phrase>Ebola</phrase> <phrase>epidemic</phrase> is unlikely to develop. But others will. The 'big one' will hit, potentially causing a crisis on the scale of the 191819 <phrase>Spanish</phrase> <phrase>influenza pandemic</phrase>, which killed more than 50 million people. Some will argue that <phrase>HIV/AIDS</phrase> is that <phrase>pandemic</phrase>: albeit slowly, it has felled more than 30 million people since the early 1980s and is ongoing. Growing mobility, demographic <phrase>pressure</phrase>, economic and <phrase>agricultural</phrase> develop ment, <phrase>climate change</phrase> and <phrase>environmental degradation</phrase> increase the <phrase>risk</phrase> from such <phrase>zoonotic</phrase> diseases. As these two books demonstrate, we need to learn from <phrase>history</phrase> to avoid repeating our mistakes. 
Unrestricted Quantifier Scope Disambiguation We present the first work on applying statistical techniques to unrestricted Quanti-<phrase>fier</phrase> Scope Disambiguation (QSD), where there is no restriction on the type or the number of quantifiers in the sentence. We formulate unrestricted QSD as learning to build a <phrase>Directed Acyclic Graph</phrase> (DAG) and define evaluation metrics based on the properties of DAGs. Previous work on statistical scope disambiguation is very limited , only considering sentences with two explicitly quantified <phrase>noun</phrase> phrases (<phrase>NPs</phrase>). In addition, they only handle a restricted list of quantifiers. In our system, all <phrase>NPs</phrase>, explicitly quantified or not (e.g. definites, bare singulars/plurals, etc.), are considered for possible scope interactions. We present early <phrase>results</phrase> on applying a simple <phrase>model</phrase> to a small corpus. The preliminary <phrase>results</phrase> are encouraging, and we hope will motivate further <phrase>research</phrase> in this <phrase>area</phrase>. 1 Introduction There are at least two interpretations for the following sentence: (1) Every line ends with a digit. In one <phrase>reading</phrase>, there is a unique digit (say 2) at the end of all lines. This is the case where the quanti-<phrase>fier</phrase> A outscopes (aka having wide-scope over) the quantifier Every. The other case is the one in which Every has wide-scope (or alternatively A has narrow scope), and represents the <phrase>reading</phrase> in which different lines could possibly end with distinct digits. This phenomenon is known as quantifier scope ambiguity. Shortly after the first efforts to build <phrase>natural language understanding</phrase> systems, Quantifier Scope Disambiguation (QSD) was realized to be very difficult. Woods (1978) was one of the first to suggest a way to get around this problem. He presented a framework for scope-underspecified <phrase>semantic</phrase> representation. He suggests representing the <phrase>Logical Form</phrase> (LF) of the above sentence as: (2) <Every x Line> <A y Digit> Ends-with(x, y) in which, the relative scope of the quantifiers is underspecified. Since then scope underspecifica-tion has been the most popular way to deal with quantifier scope ambiguity in deep <phrase>language</phrase> understanding systems (<phrase>e</phrase>. 1). Scope under-specification works in practice, only because many <phrase>NLP</phrase> applications (e.g. <phrase>machine translation</phrase>) could be achieved without quantifier scope disambigua-tion. QSD on the other hand, is critical for many other <phrase>NLP</phrase> tasks such as <phrase>question answering</phrase> systems , dialogue systems and <phrase>computing</phrase> entailment. Almost all efforts in the 80s and 90s on QSD adopt heuristics based on the lexical properties of the quantifiers, <phrase>syntactic</phrase>/<phrase>semantic</phrase> properties of the sentences, and discourse/pragmatic cues (VanLehn
Learning <phrase>Deep Web</phrase> Crawling with Diverse Features The key to <phrase>Deep Web</phrase> crawling is to submit promising keywords to query form and retrieve <phrase>Deep Web</phrase> content efficiently. To select keywords, <phrase>existing methods</phrase> make a decision based on keywords&#8217; statistic <phrase>information</phrase> deriving from TF and <phrase>DF</phrase> in local acquired records, thus work well only in textual <phrase>databases</phrase> providing full text search interfaces, whereas not well in structured <phrase>databases</phrase> of multi-attribute or field-restricted search interfaces. This <phrase>paper</phrase> proposes a novel <phrase>Deep Web</phrase> crawling method. Keywords are encoded as a <phrase>tuple</phrase> by its <phrase>linguistic</phrase>, statistic and <phrase>HTML</phrase> features so that a harvest rate evaluation <phrase>model</phrase> can be learned from the issued keywords for the un-issued in future. The method breaks through the assumption of <phrase>plain-text</phrase> search made by <phrase>existing methods</phrase>. <phrase>Experimental</phrase> <phrase>results</phrase> show that the method outperforms the <phrase>state</phrase> of the <phrase>art</phrase> methods.
A New Multivariate Approach for Prognostics Based on <phrase>Extreme Learning</phrase> Machine and Fuzzy Clustering Prognostics is a core process of prognostics and <phrase>health</phrase> <phrase>management</phrase> (PHM) discipline, that estimates the remaining useful <phrase>life</phrase> (RUL) of a degrading machinery to optimize its service delivery potential. However, machinery operates in a dynamic environment and the acquired condition monitoring <phrase>data</phrase> are usually noisy and subject to a <phrase>high</phrase> level of uncertainty/unpredictability, which complicates prognostics. The complexity further increases, when there is absence of <phrase>prior knowledge</phrase> about <phrase>ground truth</phrase> (or failure definition). For such issues, <phrase>data</phrase>-driven prognostics can be a valuable <phrase>solution</phrase> without <phrase>deep understanding</phrase> of system <phrase>physics</phrase>. This <phrase>paper</phrase> contributes a new <phrase>data</phrase>-driven prognostics approach namely, an "enhanced multivariate degradation modeling," which enables modeling degrading states of machinery without assuming a homogeneous pattern. In brief, a predictability scheme is introduced to reduce the dimensionality of the <phrase>data</phrase>. Following that, the proposed prognostics <phrase>model</phrase> is achieved by integrating two new <phrase>algorithms</phrase> namely, the summation <phrase>wavelet</phrase>-<phrase>extreme learning</phrase> machine and <phrase>subtractive</phrase>-maximum <phrase>entropy</phrase> fuzzy clustering to show <phrase>evolution</phrase> of machine degradation by simultaneous predictions and discrete <phrase>state</phrase> estimation. The prognostics <phrase>model</phrase> is equipped with a dynamic failure threshold assignment procedure to estimate RUL in a realistic manner. To validate the proposition, a <phrase>case study</phrase> is performed on <phrase>turbofan</phrase> engines <phrase>data</phrase> from PHM challenge 2008 (<phrase>NASA</phrase>), and <phrase>results</phrase> are compared with recent publications.
P-2-P <phrase>games</phrase> for <phrase>computer science</phrase> The main idea behind this <phrase>paper</phrase> is to take advantage of <phrase>game design</phrase> and implementation to learn several principles of <phrase>distributed systems</phrase>. This approach represents the other end of the <phrase>spectrum</phrase> with respect to the usual meaning of <phrase>edutainment</phrase>. The educational part is the building of a <phrase>game</phrase>, instead of playing that <phrase>game</phrase>. This <phrase>paper</phrase> shows how several of the main principles of <phrase>distributed systems</phrase> have a " natural " mapping in distributed <phrase>games</phrase>, thus inducing their deep comprehension in the developers of the <phrase>game</phrase>. We discuss an experimentation that lasted several years, involving students of <phrase>computer science</phrase>, at the forth year of their curricula in our <phrase>University</phrase>. The course in <phrase>distributed systems</phrase> requires the <phrase>design</phrase> and implementation of a distributed <phrase>game</phrase>, and the students are requested to perform that task in <phrase>small groups</phrase>. The students are requested to <phrase>design</phrase> and implement a <phrase>Web based</phrase> <phrase>game</phrase> between <phrase>peers</phrase>, avoiding any form of central-ization during the <phrase>game</phrase> playing. Moreover, the <phrase>game</phrase> has to be tolerant to some kind of faults, keeping coherent behavior for the correct players.
Integrating Representation Learning and Skill Learning in a <phrase>Human</phrase>-like <phrase>Intelligent Agent</phrase> Integrating Representation Learning and Skill Learning in a <phrase>Human</phrase>-like <phrase>Intelligent Agent</phrase> Integrating Representation Learning and Skill Learning in a <phrase>Human</phrase>-like <phrase>Intelligent Agent</phrase> Building an <phrase>intelligent agent</phrase> that simulates <phrase>human</phrase> learning of <phrase>math</phrase> and <phrase>science</phrase> could potentially benefit both <phrase>education</phrase>, by contributing to the understanding of <phrase>human</phrase> learning, and <phrase>artificial intelligence</phrase> , by advancing the goal of creating <phrase>human</phrase>-level <phrase>intelligence</phrase>. However, constructing such a learning agent currently requires manual encoding of prior <phrase>domain knowledge</phrase>; in addition to being a poor <phrase>model</phrase> of <phrase>human</phrase> acquisition of <phrase>prior knowledge</phrase>, manual <phrase>knowledge</phrase>-encoding is both time-consuming and error-prone. Previous work showed that one of the key factors that differentiates experts and novices is their different representations of <phrase>knowledge</phrase>. Experts view the world in terms of deep functional features, while novices view it in terms of shallow <phrase>perceptual</phrase> features. Moreover, since the performance of many existing <phrase>learning algorithms</phrase> is sensitive to representation , the deep features are also important in achieving effective learning. In this <phrase>paper</phrase>, we present an efficient <phrase>algorithm</phrase> that acquires representation <phrase>knowledge</phrase> in the form of " deep features " for specific domains, and demonstrate its effectiveness in the domain of <phrase>algebra</phrase> as well as synthetic domains. We integrate this <phrase>algorithm</phrase> into a <phrase>machine-learning</phrase> agent, SimStudent, which learns <phrase>procedural knowledge</phrase> by observing a <phrase>tutor</phrase> solve sample problems, and by getting <phrase>feedback</phrase> while actively <phrase>solving problems</phrase> on its own. We show that learning " deep features " reduces the requirements for <phrase>knowledge engineering</phrase>. Moreover, we propose an approach that automatically discovers <phrase>student</phrase> models using the extended SimStudent. By fitting the discovered <phrase>model</phrase> to real <phrase>student</phrase> <phrase>learning curve</phrase> <phrase>data</phrase>, we show that it is a better <phrase>student</phrase> <phrase>model</phrase> than <phrase>human</phrase>-generated models, and demonstrate how the discovered <phrase>model</phrase> may be used to improve a tutoring system's instructional strategy.
Learning <phrase>Semantics</phrase> with <phrase>Deep Belief</phrase> Network for Cross-<phrase>Language</phrase> <phrase>Information Retrieval</phrase> This <phrase>paper</phrase> introduces a cross-<phrase>language</phrase> <phrase>information retrieval</phrase> (CLIR) framework that combines the <phrase>state</phrase>-of-the-<phrase>art</phrase> keyword-<phrase>based approach</phrase> with a latent <phrase>semantic</phrase>-based retrieval <phrase>model</phrase>. To capture and analyze the hidden <phrase>semantics</phrase> in cross-lingual settings, we construct latent <phrase>semantic</phrase> models that map text in different languages into a shared <phrase>semantic</phrase> space. Our <phrase>proposed framework</phrase> consists of <phrase>deep belief</phrase> networks (DBN) for each <phrase>language</phrase> and we employ canonical correlation analysis (CCA) to construct a shared <phrase>semantic</phrase> space. We evaluated the proposed CLIR approach on a standard <phrase>ad hoc</phrase> CLIR dataset, and we show that the cross-lingual <phrase>semantic</phrase> analysis with DBN and CCA improves the <phrase>state</phrase>-of-the-<phrase>art</phrase> keyword-based CLIR performance.
Boosting Classification Based Speech Separation Using Temporal Dynamics Significant advances in speech separation have been made by formulating it as a <phrase>classification problem</phrase>, where the desired output is the ideal <phrase>binary</phrase> mask (<phrase>IBM</phrase>). Previous work does not explicitly <phrase>model</phrase> the correlation between neighboring time-<phrase>frequency</phrase> units and standard <phrase>binary</phrase> classifiers are used. As one of the most important characteristics of speech signal is its temporal dynamics , the <phrase>IBM</phrase> contains highly structured, instead of, random patterns. In this study, we incorporate temporal dynamics into classification by employing structured output learning. In particular, we use linear-chain structured perceptrons to account for the interactions of neighboring <phrase>labels</phrase> in time. However, the performance of struc-tured perceptrons largely depends on the linear separabil-ity of features. To address this problem, we employ <phrase>pre-trained</phrase> <phrase>deep neural networks</phrase> to automatically learn effective feature functions for structured perceptrons. The experiments show that the proposed system <phrase>significantly outperforms</phrase> previous <phrase>IBM</phrase> estimation systems.
<phrase>MARS</phrase>: A Specialized <phrase>RTE</phrase> System for Parser Evaluation This <phrase>paper</phrase> describes our participation in the the SemEval-2010 Task #12, Parser Evaluation using Textual Entail-ment. Our system incorporated two dependency parsers, one <phrase>semantic</phrase> role labeler, and a deep parser based on <phrase>hand-crafted</phrase> grammars. The <phrase>shortest path</phrase> <phrase>algorithm</phrase> is applied on the <phrase>graph</phrase> representation of the parser outputs. Then, different types of features are extracted and the entail-ment recognition is casted into a <phrase>machine-learning</phrase>-based classification task. The best setting of the system achieves 66.78% of accuracy, which ranks the 3rd place.
<phrase>Transformer</phrase> fault diagnosis using continuous sparse autoencoder. This <phrase>paper</phrase> proposes a novel continuous sparse autoencoder (CSAE) which can be used in <phrase>unsupervised feature learning</phrase>. The CSAE adds Gaussian <phrase>stochastic</phrase> unit into activation <phrase>function</phrase> to extract features of nonlinear <phrase>data</phrase>. In this <phrase>paper</phrase>, CSAE is applied to solve the problem of <phrase>transformer</phrase> fault recognition. Firstly, based on dissolved <phrase>gas</phrase> analysis method, <phrase>IEC</phrase> three ratios are calculated by the concentrations of dissolved gases. Then <phrase>IEC</phrase> three ratios <phrase>data</phrase> is normalized to reduce <phrase>data</phrase> singularity and improve training speed. Secondly, <phrase>deep belief</phrase> network is established by two layers of CSAE and one layer of back propagation (<phrase>BP</phrase>)network. Thirdly, CSAE is adopted to unsupervised training and getting features. Then <phrase>BP</phrase> network is used for supervised training and getting <phrase>transformer</phrase> fault. Finally, the <phrase>experimental</phrase> <phrase>data</phrase> from <phrase>IEC</phrase> TC 10 dataset aims to illustrate the effectiveness of the presented approach. Comparative experiments clearly show that CSAE can extract features from the original <phrase>data</phrase>, and achieve a <phrase>superior</phrase> correct differentiation rate on <phrase>transformer</phrase> fault diagnosis.
Dynamics of <phrase>tongue</phrase> gestures extracted automatically from <phrase>ultrasound</phrase> We describe a system for automatically extracting dynamics of <phrase>tongue</phrase> gestures from <phrase>ultrasound</phrase> images of the <phrase>tongue</phrase> using translational <phrase>deep belief</phrase> networks (tDBNs). In tDBNs, a joint <phrase>model</phrase> of the input and output vectors are learned during a generative pretraining stage, and then a <phrase>translation</phrase> step is used to transform input-only vectors into this joint representation. A final <phrase>fine-tuning</phrase> stage is then used to reconstruct the desired outputs given input vectors. We show that this technique dramatically improves performance on segmenting <phrase>ultrasound</phrase> image sequences of continuous speech into individual <phrase>consonant</phrase> gestures compared with the original DBN method of [1] as well as <phrase>alternative</phrase> methods using <phrase>PCA</phrase> and <phrase>support vector machines</phrase>.
How Teachers Learn <phrase>Technology</phrase> Best.PDF When it comes to teachers learning and valuing the effective use of new technologies, some schools are discovering that the kinds of training programs offered in the past may not represent the most generative method of reaching a full <phrase>range</phrase> of teachers and their students. The key term is "generative"-meaning that behaviors and <phrase>daily</phrase> practice will be changed for the better as a consequence of the <phrase>professional</phrase> development experience. Fortunately, some schools are now identifying approaches more likely to encourage teachers to employ these technologies on a frequent and sustained basis to enhance <phrase>student</phrase> learning. <phrase>Lead</phrase> <phrase>districts</phrase> are finding that adult learning, <phrase>curriculum</phrase> development projects and informal support structures are proving powerful in promoting recurrent use aimed at deep <phrase>curriculum</phrase> integration. After two decades of providing <phrase>software</phrase> classes to teachers, we need to explore different approaches those honoring key principles of adult learning while placing both <phrase>curriculum</phrase> and <phrase>literacy</phrase> ahead of <phrase>software</phrase> and <phrase>technology</phrase>.
Deep Metric Learning for Person Re-identification Various <phrase>hand-crafted</phrase> features and metric learning methods prevail in the field of person re-identification. Compared to these methods, this <phrase>paper</phrase> proposes a more <phrase>general</phrase> way that can learn a similarity metric from image <phrase>pixels</phrase> directly. By using a " <phrase>siamese</phrase> " <phrase>deep neural network</phrase>, the <phrase>proposed method</phrase> can jointly learn the color feature, texture feature and metric in a unified framework. The network has a <phrase>symmetry</phrase> structure with two sub-networks which are connected by a cosine layer. Each sub-network includes two convolutional layers and a full connected layer. To deal with the big variations of person images, binomial deviance is used to evaluate the cost between similarities and <phrase>labels</phrase>, which is proved to be robust to <phrase>outliers</phrase>. Experiments on VIPeR illustrate the <phrase>superior</phrase> performance of our method and a cross <phrase>database</phrase> experiment also shows its good generalization.
Deep networks for predicting <phrase>human</phrase> intent with respect to objects Effective <phrase>human</phrase>-<phrase>robot</phrase> interaction requires systems that can accurately infer and predict <phrase>human</phrase> intentions. In this <phrase>paper</phrase>, we introduce a system that uses stacked <phrase>denoising autoencoders</phrase> to perform intent recognition. We introduce the intent recognition problem, provide an overview of <phrase>deep architectures</phrase> in <phrase>machine learning</phrase>, and outline the components of our system. We also provide preliminary <phrase>results</phrase> for our system's performance.
<phrase>Hypermedia</phrase> and <phrase>Cognition</phrase>: Designing for Comprehension discuss the relationship between <phrase>cognition</phrase> and <phrase>hypermedia</phrase>, it is necessary to distinguish between two kinds of applications: " One encourages those who wish to wander through large clouds of <phrase>information</phrase>, gathering <phrase>knowledge</phrase> along the way. The other is more directly tied to specific <phrase>problem-solving</phrase>, and is quite structured and perhaps even constrained " [20, p. 119]. Applications of the first type appear as brows-able databasesor hyperbasesthat can be freely explored by a reader. In contrast, applications of the second type take the shape of <phrase>electronic</phrase> do-cumentsor hyperdocumentsthat intentionally guide readers through an <phrase>information</phrase> space, controlling their exploration along the lines of a prede-fined structure. Each type has its particular advantages and encourages different <phrase>reading</phrase> strategies. While the first one is better suited to support unconstrained search and <phrase>information retrieval</phrase>, the second one is more adequate for tasks requiring <phrase>deep understanding</phrase> and learning. As <phrase>Hammond</phrase> points out, it " may be fun and perhaps instructive, to open every door and peer inside, but there are many situations where learning is most effective when the freedom of the learner is restricted to a relevant and helpful <phrase>subset</phrase> of activities. " It is this second type of taskreading a hyperdocu-ment for learningthat we address in our discussion of " designing for comprehension. " Comprehension of Hyperdocuments One <phrase>major</phrase> purposeor even the <phrase>major</phrase> purposeof <phrase>reading</phrase> a document is comprehension, and <phrase>reading</phrase> a hyperdocument is no exception. In <phrase>cognitive science</phrase> , comprehension is often characterized as the <phrase>construction</phrase> of a <phrase>mental model</phrase> that represents the objects and <phrase>semantic</phrase> relations described in a text [24]. The readability of a document can be defined as the mental effort spent on the <phrase>construction</phrase> rom the beginning, <phrase>hypermedia</phrase> application <phrase>design</phrase> has been driven primarily by technological innovations and constrained by technical feasibility. For the last few years, however, <phrase>usability</phrase> methods and <phrase>results</phrase> from <phrase>human factors</phrase> <phrase>research</phrase> have been gaining more influence [17]. Despite this trend toward user-oriented development procedures, issues of <phrase>cognition</phrase> and <phrase>human</phrase> <phrase>information processing</phrase> still are widely neglected and barely influence <phrase>hypermedia</phrase> <phrase>design</phrase>.
Are <phrase>computing</phrase> educators and researchers different from the rest? There is evidence to suggest that men and women may approach learning in different ways. Studies have shown that women are more likely to be interested in learning for its own <phrase>sake</phrase> [1,3], are more likely to adopt a deep approach [I], have a greater fear of failure, and are less likely to cheat [3]. Previous work has shown that women seek help in different ways when learning to program [2]; they are much more likely to approach staff directly, while male students generally prefer impersonal mechanisms. This raises the issue of whether the ways in which the genders approach introductory <phrase>programming</phrase> differ in any other ways. One reason why learning to program is difficult is that it is not a <phrase>single</phrase> skill. A <phrase>student</phrase> must <phrase>master</phrase> <phrase>syntax</phrase>, <phrase>semantics</phrase>, structure, and style. A surface approach may be adequate to learn <phrase>syntax</phrase>, so male students might be expected to <phrase>excel</phrase>. However, they would encounter difficulty in the more applied concept of structure, where a deep approach (more likely to be adopted by females) would be beneficial It is not clear where issues of style lit. It is something often neglected by students-if addressed at all it is often seen as something to be considered only after the program works. Given their supposedly different <phrase>approaches to learning</phrase>, it might be expected that male and female students would have different approaches to the style of their programs. <phrase>Gender</phrase> does sometimes appear to have an influence on style-an obvious example is handwriting. Is it possible, then, that <phrase>gender</phrase> has an influence on the style of a <phrase>student</phrase> when <phrase>programming</phrase>? If this is so, there will be implications for our understanding of the way that <phrase>students learn</phrase> to program, and for the way they should be taught. <phrase>Anecdotal evidence</phrase> suggests that it is possible to determine the <phrase>gender</phrase> of the <phrase>author</phrase> of a piece of code simply by looking at the code. This <phrase>poster</phrase> will present four extracts from <phrase>student</phrase> programs. Colleagues will be invited to indicate which program they believe to have been written by which <phrase>gender</phrase>, and to explain how they can tell. This presentation reports preliminary <phrase>results</phrase> from a group of <phrase>computing</phrase> educators in the New <phrase>Zealand</phrase> applied <phrase>computing</phrase> sector, who assessed their approaches to teaching and <phrase>research</phrase>, at a "Getting Started in <phrase>Research</phrase> Workshop". In subsequent workshops, groups of largely non-<phrase>computing</phrase> attendees showed marked differences in their assessments, using the same 
Teaching Patterns and <phrase>Software Design</phrase> In this <phrase>paper</phrase> we describe our experiences with reengi-neering an <phrase>undergraduate</phrase> course in <phrase>software design</phrase>. The course's <phrase>learning outcomes</phrase> require that students can <phrase>model</phrase>, <phrase>design</phrase> and implement <phrase>software</phrase>. These are inherently practical skills and rely on functioning <phrase>knowledge</phrase>. To facilitate a <phrase>learning environment</phrase> in which students can acquire the necessary deep level of understanding, we have designed the course by applying the educational theory of constructive alignment and a number of <phrase>proven</phrase> techniques for teaching, learning, and assessment. Fundamentally, we have embraced the <phrase>active learning</phrase> <phrase>paradigm</phrase> that recog-nises that <phrase>student</phrase> activity is critical to the learning process. In this <phrase>paper</phrase>, we describe several <phrase>active learning</phrase> techniques that we have used including role <phrase>play</phrase>, <phrase>problem solving</phrase> and peer learning. We also describe two novel assessment techniques we have developed , holistic assessment and formative examination. In addition we describe how students work with JU-<phrase>nit</phrase>, a popular <phrase>unit testing</phrase> tool, not as users but as developers by applying <phrase>design patterns</phrase> to extend it with new functionality. Finally, we <phrase>report</phrase> on <phrase>student</phrase> assessment <phrase>results</phrase> and relay <phrase>student</phrase> <phrase>feedback</phrase>.
<phrase>Deep Neural Networks</phrase> for <phrase>Acoustic</phrase> Modeling in <phrase>Speech Recognition</phrase> [ Four <phrase>research</phrase> groups share their views ] <<phrase>AU</phrase>: pleAse check thAt Added sUbtitle is <phrase>Ok</phrase> As given Or pleAse sUpply <phrase>shOrt</phrase> <phrase>AlternAtive</phrase>> M ost current <phrase>speech recognition</phrase> systems use <phrase>hidden Markov models</phrase> (HMMs) to deal with the temporal variability of speech and <phrase>Gaussian mixture</phrase> models (GMMs) to determine how well each <phrase>state</phrase> of each HMM fits a frame or a <phrase>short</phrase> window of frames of coefficients that represents the <phrase>acoustic</phrase> input. An <phrase>alternative</phrase> way to evaluate the fit is to use a <phrase>feed-forward</phrase> <phrase>neural network</phrase> that takes several frames of coefficients as input and produces posterior <phrase>probabilities</phrase> over HMM states as output. <phrase>Deep neural networks</phrase> (DNNs) that have many <phrase>hidden layers</phrase> and are trained using new methods have been shown to outperform GMMs on a <phrase>variety</phrase> of <phrase>speech recognition</phrase> benchmarks, sometimes by a <phrase>large margin</phrase>. This article provides an overview of this progress and represents the shared views of four <phrase>research</phrase> groups that have had recent successes in using DNNs for <phrase>acoustic</phrase> <phrase>model</phrase>-ing in <phrase>speech recognition</phrase>. intrOdUctiOn New <phrase>machine learning</phrase> <phrase>algorithms</phrase> can <phrase>lead</phrase> to significant advances in <phrase>automatic speech recognition</phrase> (ASR). The biggest <phrase>single</phrase> advance occurred nearly four decades ago with the introduction of the expectation-maximization (EM) <phrase>algorithm</phrase> for training HMMs (see [1] and [2] for informative historical reviews of the introduction of HMMs). With the EM <phrase>algorithm</phrase>, it became possible to develop <phrase>speech recognition</phrase> systems for <phrase>real-world</phrase> tasks using the richness of GMMs [3] to represent the relationship between HMM states and the <phrase>acoustic</phrase> input. In these systems the <phrase>acoustic</phrase> input is typically represented by concatenating Mel-<phrase>frequency</phrase> cepstral coefficients (MFCCs) or <phrase>perceptual</phrase> linear predictive coefficients (PLPs) [4] computed from the raw <phrase>waveform</phrase> and their first-and second-<phrase>order</phrase> temporal differences [5]. This nonadaptive but highly engineered preprocessing of the <phrase>waveform</phrase> is designed to discard the large amount of <phrase>information</phrase> in <phrase>waveforms</phrase> that is considered to be irrelevant for <phrase>discrimination</phrase> and to express the remaining <phrase>information</phrase> in a form that facilitates <phrase>discrimination</phrase> with GMM-HMMs.
<phrase>Memory</phrase> and <phrase>information processing</phrase> in neuromorphic systems A striking difference between <phrase>brain</phrase>-inspired neuro-morphic processors and current <phrase>von Neumann</phrase> processors archi-tectures is the way in which <phrase>memory</phrase> and processing is organized. As <phrase>Information</phrase> and <phrase>Communication</phrase> Technologies continue to address the need for increased computational power through the increase of cores within a <phrase>digital</phrase> processor, neuromorphic engineers and scientists can <phrase>complement</phrase> this need by building processor architectures where <phrase>memory</phrase> is distributed with the processing. In this <phrase>paper</phrase> we present a survey of <phrase>brain</phrase>-inspired processor architectures that support models of <phrase>cortical</phrase> networks and <phrase>deep neural networks</phrase>. These architectures <phrase>range</phrase> from serial clocked implementations of multi-<phrase>neuron</phrase> systems to massively parallel asynchronous ones and from purely <phrase>digital</phrase> systems to mixed analog/<phrase>digital</phrase> systems which implement more biological-like models of <phrase>neurons</phrase> and <phrase>synapses</phrase> together with a suite of adaptation and learning mechanisms analogous to the ones found in biological <phrase>nervous</phrase> systems. We describe the advantages of the different approaches being pursued and present the challenges that need to be addressed for building <phrase>artificial</phrase> neural processing systems that can display the richness of behaviors seen in biological systems.
Error-Driven <phrase>Incremental Learning</phrase> in <phrase>Deep Convolutional</phrase> <phrase>Neural Network</phrase> for <phrase>Large-Scale</phrase> <phrase>Image Classification</phrase> <phrase>Supervised learning</phrase> using <phrase>deep convolutional</phrase> <phrase>neural network</phrase> has shown its promise in <phrase>large-scale</phrase> <phrase>image classification</phrase> task. As a <phrase>building block</phrase>, it is now well positioned to be part of a larger system that tackles <phrase>real-life</phrase> <phrase>multimedia</phrase> tasks. An unresolved issue is that such <phrase>model</phrase> is trained on a static snapshot of <phrase>data</phrase>. Instead, this <phrase>paper</phrase> positions the training as a continuous learning process as new classes of <phrase>data</phrase> arrive. A system with such capability is useful in practical scenarios, as it gradually expands its capacity to predict increasing number of new classes. It is also our attempt to address the more fundamental issue: a good learning system must deal with new <phrase>knowledge</phrase> that it is exposed to, much as how <phrase>human</phrase> do. We developed a training <phrase>algorithm</phrase> that grows a network not only incrementally but also hierarchically. Classes are grouped according to similarities, and self-organized into levels. The newly added capacities are divided into component models that predict <phrase>coarse-grained</phrase> superclasses and those return final prediction within a superclass. Importantly, all models are cloned from existing ones and can be trained in parallel. These models inherit features from existing ones and thus further speed up the learning. Our experiment points out advantages of this approach, and also yields a few important open questions.
<phrase>Multi-task</phrase> deep visual-<phrase>semantic</phrase> embedding for <phrase>video</phrase> <phrase>thumbnail</phrase> selection Given the tremendous growth of online videos, <phrase>video</phrase> <phrase>thumbnail</phrase>, as the common visualization form of <phrase>video</phrase> content , is becoming increasingly important to influence user's browsing and searching experience. However, conventional methods for <phrase>video</phrase> <phrase>thumbnail</phrase> selection often fail to produce satisfying <phrase>results</phrase> as they ignore the side <phrase>semantic</phrase> <phrase>information</phrase> (e.g., title, description, and query) associated with the <phrase>video</phrase>. As a result, the selected <phrase>thumbnail</phrase> cannot always represent <phrase>video</phrase> <phrase>semantics</phrase> and the click-through rate is adversely affected even when the retrieved videos are relevant. In this <phrase>paper</phrase>, we have developed a <phrase>multi-task</phrase> deep visual-<phrase>semantic</phrase> embedding <phrase>model</phrase>, which can automatically select query-dependent <phrase>video</phrase> thumbnails according to both visual and side <phrase>information</phrase>. Different from most <phrase>existing methods</phrase>, the <phrase>proposed approach</phrase> employs the deep visual-<phrase>semantic</phrase> embedding <phrase>model</phrase> to directly compute the similarity between the query and <phrase>video</phrase> thumbnails by mapping them into a common latent <phrase>semantic</phrase> space, where even unseen query-<phrase>thumbnail</phrase> pairs can be correctly matched. In particular, we <phrase>train</phrase> the embedding <phrase>model</phrase> by exploring the <phrase>large-scale</phrase> and freely accessible click-through <phrase>video</phrase> and image <phrase>data</phrase>, as well as employing a <phrase>multi-task</phrase> learning strategy to holis-tically exploit the query-<phrase>thumbnail</phrase> relevance from these two highly related datasets. Finally, a <phrase>thumbnail</phrase> is selected by fusing both the representative and query relevance scores. The evaluations on 1,000 query-<phrase>thumbnail</phrase> dataset labeled by 191 workers in <phrase>Amazon Mechanical Turk</phrase> have demonstrated the effectiveness of our <phrase>proposed method</phrase>.
Learning Abstract Behaviors with the Hierarchical Incremental <phrase>Gaussian Mixture</phrase> Network This <phrase>paper</phrase> presents a new probabilistic hierarchical <phrase>model</phrase>, called HIGMN (Hierarchical Incremental <phrase>Gaussian Mixture</phrase> Network), which is based on ideas presented by <phrase>Deep Architectures</phrase>. The proposed <phrase>model</phrase>, composed by layers of IGMNs, is able to extract features from <phrase>data</phrase> input of different domains in the <phrase>low-level</phrase> layers and to correlate these features in a <phrase>high</phrase>-level layer. Experiments show that HIGMN is able to learn an abstract behavior using the features extracted from sensory and motor <phrase>data</phrase> of a <phrase>mobile robot</phrase> and to perform correct actions even in unknown instances of sensory <phrase>perception</phrase>.
"I'm sorry Dave, I'm afraid I can't do that": <phrase>Linguistics</phrase>, <phrase>Statistics</phrase> and <phrase>Natural Language Processing</phrase> <phrase>circa</phrase> 2001 It's the year 2000, but where are the flying cars? I was promised flying cars. <phrase>Avery Brooks</phrase>, <phrase>IBM</phrase> commercial According to many <phrase>pop-culture</phrase> visions of the future, <phrase>technology</phrase> will eventually produce the Machine that Can Speak to Us. Examples <phrase>range</phrase> from the False Maria in Fritz Lang's 1926 <phrase>film</phrase> <phrase>Metropolis</phrase> to <phrase>Knight</phrase> Rider's KITT (a talking <phrase>car</phrase>) to <phrase>Star Wars</phrase>' <phrase>C-3PO</phrase> (said to have been modeled on the False Maria). And, of course, there is the <phrase>HAL 9000</phrase> computer from 2001: A Space <phrase>Odyssey</phrase>; in one of the film's most famous scenes, the <phrase>astronaut</phrase> Dave asks HAL to open a pod <phrase>bay</phrase> door on the <phrase>spacecraft</phrase>, to which HAL responds, " I'm sorry Dave, I'm afraid I can't do that ". <phrase>Natural language processing</phrase>, or <phrase>NLP</phrase>, is the field of <phrase>computer science</phrase> devoted to creating such machines that is, enabling <phrase>computers</phrase> to use <phrase>human</phrase> languages both as input and as output. The <phrase>area</phrase> is quite broad, encompassing problems ranging from simultaneous multi-<phrase>language</phrase> <phrase>translation</phrase> to advanced <phrase>search engine</phrase> development to the <phrase>design</phrase> of computer interfaces capable of combining speech, diagrams, and other modalities simultaneously. A natural consequence of this wide <phrase>range</phrase> of inquiry is the integration of ideas from <phrase>computer science</phrase> with work from many other fields, including <phrase>linguistics</phrase>, which provides models of <phrase>language</phrase>; <phrase>psychology</phrase>, which provides models of <phrase>cognitive processes</phrase>; <phrase>information theory</phrase>, which provides models of <phrase>communication</phrase>; and <phrase>mathematics</phrase> and <phrase>statistics</phrase>, which provide tools for analyzing and acquiring such models. The interaction of these ideas together with advances in <phrase>machine learning</phrase> (see [other chapter]) has resulted in concerted <phrase>research</phrase> activity in statistical <phrase>natural language processing</phrase>: making <phrase>computers</phrase> <phrase>language</phrase>-enabled by having them acquire <phrase>linguistic</phrase> <phrase>information</phrase> directly from samples of <phrase>language</phrase> itself. In this <phrase>essay</phrase>, we describe the <phrase>history</phrase> of statistical <phrase>NLP</phrase>; the twists and turns of the story serve to highlight the sometimes complex interplay between <phrase>computer science</phrase> and other fields. Although currently a <phrase>major</phrase> focus of <phrase>research</phrase>, the <phrase>data</phrase>-driven, computational approach to <phrase>language</phrase> processing was for some time held in deep disregard because it directly conflicts with another commonly-held viewpoint: <phrase>human</phrase> <phrase>language</phrase> is so complex that <phrase>language</phrase> samples alone seemingly cannot yield enough <phrase>information</phrase> to understand it. Indeed, it is often said that <phrase>NLP</phrase> is " <phrase>AI</phrase>-complete " (a <phrase>pun</phrase> on <phrase>NP-completeness</phrase>; see [other chapter]), meaning that the most difficult problems in <phrase>artificial intelligence</phrase> manifest themselves in <phrase>human</phrase> <phrase>language</phrase> phenomena. This belief in <phrase>language</phrase> use as the <phrase>touchstone</phrase> 
Feasibility of <phrase>retrofitting</phrase> centralized <phrase>HVAC</phrase> systems for room-level <phrase>zoning</phrase> Heating, ventilation, and cooling (<phrase>HVAC</phrase>) accounts for 38% of building <phrase>energy</phrase> usage, and over 15% of all US <phrase>energy</phrase> usage, making it one of the nation's largest <phrase>energy</phrase> consumers. Many attempts have been made to optimize the control of <phrase>HVAC</phrase> systems by minimizing the <phrase>energy</phrase> wasted in conditioning buildings that are unoccupied. Systems have been proposed that turn off <phrase>HVAC</phrase> systems when a <phrase>house</phrase> is unoccupied, or put the system into an <phrase>energy</phrase> saving deep-setback mode when the occupants are asleep. An <phrase>area</phrase> that has not been as well explored is the <phrase>retrofitting</phrase> of centralized <phrase>HVAC</phrase> systems to save <phrase>energy</phrase> when the residents are at home and awake. In this <phrase>paper</phrase>, we demonstrate how to use cheap, off-the-shelf sensors and actuators to retrofit a centralized <phrase>HVAC</phrase> system and enable rooms to be heated or cooled individually, in <phrase>order</phrase> to reduce waste caused by conditioning unoccupied rooms. We call this approach room-level <phrase>zoning</phrase>. Sensors are used to detect occupancy in rooms which allows the learning of occupancy patterns and prediction of room occupancy. Unoccupied rooms can be allowed to drift away from a user defined comfortable <phrase>temperature</phrase> if they are less likely to be used in the near future while occupied rooms are maintained at a comfortable <phrase>temperature</phrase>. We implement room-level <phrase>zoning</phrase> in a 1400 square foot <phrase>house</phrase> by <phrase>retrofitting</phrase> an existing centralized <phrase>HVAC</phrase> system with <phrase>wireless</phrase> <phrase>temperature</phrase> sensors to monitor room-level <phrase>temperature</phrase>, motion sensors to monitor occupancy, and wirelessly actuatable dampers to control the flow of conditioned air through the <phrase>house</phrase>. Initial analysis indicates that this method has a 20.5% <phrase>energy</phrase> savings over the existing <phrase>single</phrase>-zoned <phrase>thermostat</phrase>.
A <phrase>deep architecture</phrase> with <phrase>bilinear</phrase> modeling of hidden representations: Applications to phonetic recognition We develop and describe a novel <phrase>deep architecture</phrase>, the <phrase>Tensor</phrase> Deep Stacking Network (T-DSN), where multiple blocks are stacked one on top of another and where a <phrase>bilinear</phrase> mapping from hidden representations to the output in each block is used to incorporate <phrase>higher-order</phrase> <phrase>statistics</phrase> of the input features. A <phrase>learning algorithm</phrase> for the T-DSN is presented, in which the main <phrase>parameter estimation</phrase> burden is shifted to a convex sub-problem with a <phrase>closed-form</phrase> <phrase>solution</phrase>. Using an efficient and scalable parallel implementation, we <phrase>train</phrase> a T-DSN to discriminate standard three-<phrase>state</phrase> monophones in the TIMIT <phrase>database</phrase>. The T-DSN outperforms an <phrase>alternative</phrase> pretrained <phrase>Deep Neural Network</phrase> (DNN) <phrase>architecture</phrase> in frame-level classification (both <phrase>state</phrase> and phone) and in the cross-<phrase>entropy</phrase> measure. For continuous phonetic recognition, T-DSN performs equivalently to a DNN but without the need for a hard-to-scale, sequential <phrase>fine-tuning</phrase> step.
Facilitating <phrase>Cognitive</phrase> Presence in <phrase>Online Learning</phrase>: Interaction Is Not Enough This study assessed the depth of <phrase>online learning</phrase>, with a focus on the <phrase>nature</phrase> of online interaction in four <phrase>distance education</phrase> course designs. The Study Process Questionnaire was used to measure the shift in stu-dents' approach to learning from the beginning to the end of the courses. <phrase>Design</phrase> had a significant impact on the <phrase>nature</phrase> of the interaction and whether students approached learning in a deep and meaningful manner. Structure and <phrase>leadership</phrase> were found to be crucial for on-line learners to take a deep and meaningful approach to learning. Interaction is seen as central to an educational experience and is a primary focus in the study of <phrase>online learning</phrase>. The focus on interaction in <phrase>online learning</phrase> emerges from the potential and properties of new technologies to support sustained educational <phrase>communication</phrase>. <phrase>Communication</phrase> and <phrase>Internet</phrase> technologies provide a <phrase>high</phrase> <phrase>degree</phrase> of communicative potential through asynchronous <phrase>interaction design</phrase> options (<phrase>Garrison</phrase> and Anderson 2003). From an access perspective, participants are able to maintain engagement in a <phrase>community</phrase> of learners when and where they choose. Notwithstanding the widely recognized potential of new and emerging communications <phrase>technology</phrase> to connect learners, until recently much of the <phrase>research</phrase> of collaborative <phrase>online learning</phrase> focused on egalitarian possibilities. Educators were quick to seize the possibility of a more <phrase>democratic</phrase> <phrase>ap</phrase>-133
Online <phrase>collaborative learning</phrase> as a <phrase>catalyst</phrase> for systemic change in the teaching-learning process within a multi-<phrase>campus</phrase> institution of <phrase>higher education</phrase> This presentation describes an innovative and strategic cross-institutional collaborative project to support systemic change in the teaching-learning process at a leading institution of <phrase>higher education</phrase>: <phrase>Monterrey Institute of Technology and Higher Education</phrase> System). The main purpose of the document is to share the scope, framework, content, and strategies for preparing faculty to serve as mentors and trainees of colleagues in the use of online and face-to-face <phrase>collaborative learning</phrase> strategies and tools. The <phrase>paper</phrase> includes three sections. The first section provides an overview of higher education's trends and challenges to frame a meaningful context. Second, a brief background on <phrase>ITESM</phrase> mission, strategies and goals is provided. The remainder of the <phrase>paper</phrase> describes the <phrase>University</phrase> of <phrase>Texas</phrase> at <phrase>Austin</phrase> (UT)-<phrase>ITESM</phrase> Summer Institute on <phrase>Collaborative Learning</phrase> and planned follow-on activities with 50 faculty who will serve as change agents and mentors in the use of online and face-to-face <phrase>collaborative learning</phrase>. OVERVIEW " Within the next century, there can be witnessed an unusual demand for <phrase>higher education</phrase> as well as its great diversification. There is also a deep <phrase>consciousness</phrase> about the importance of <phrase>higher education</phrase> as a platform to sustain the social and <phrase>cultural</phrase> development of our <phrase>society</phrase>... " (World's Declaration on <phrase>Higher Education</phrase>, 1998, p.1) <phrase>Higher Education</phrase> has contributed to the development of societies throughout <phrase>history</phrase>. The concept of the modern <phrase>university</phrase>, as we know it today, has its roots in the <phrase>Middle Ages</phrase> with the development of the first <phrase>universities</phrase> in <phrase>Europe</phrase> during the XIII Century. <phrase>Bologna</phrase>, <phrase>Paris</phrase>, <phrase>Oxford</phrase>, and <phrase>Salamanca</phrase> (Schachner, 1938, p.42) have significantly influenced <phrase>higher education</phrase>. The appearance of Gutenberg's <phrase>printing press</phrase> some 500 <phrase>years ago</phrase> constituted the deepest <phrase>technological change</phrase> within the educational endeavor; at least until now. <phrase>Higher Education</phrase> has shown its <phrase>sustainability</phrase>, adaptability and transformable capability during the last 800 years. But now it has " to develop the most radical transformation and renewal ever made " (World's Declaration on <phrase>Higher Education</phrase>, 1998, p.2). Such transformation is often referred as " The Learning <phrase>Revolution</phrase> " (Oblinger, 1997) and will take place in a new <phrase>era</phrase> of global <phrase>digital</phrase> competition in <phrase>higher education</phrase>. And according to <phrase>Green</phrase> (1997) there are growing forces for change including issues of access to <phrase>higher education</phrase>, funding, economic and social development, accountability, <phrase>autonomy</phrase>, educational models and teaching-learning processes, use of <phrase>technology</phrase>, and <phrase>internationalization</phrase>. In addition <phrase>government</phrase>, the <phrase>public</phrase>, faculty, students, governing boards, and institutional leaders increasingly recognize the need for change and 
Cutting Recursive Autoencoder <phrase>Trees</phrase> <phrase>Deep Learning</phrase> models enjoy considerable success in <phrase>Natural Language Processing</phrase>. While <phrase>deep architectures</phrase> produce useful representations that <phrase>lead</phrase> to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. In this <phrase>paper</phrase>, we rely on empirical <phrase>tests</phrase> to see whether a particular structure makes sense. We present an analysis of the <phrase>Semi-Supervised</phrase> Recursive Autoencoder, a well-known <phrase>model</phrase> that produces structural representations of text. We show that for certain tasks, the structure of the autoencoder can be significantly reduced without loss of <phrase>classification accuracy</phrase> and we evaluate the <phrase>produced</phrase> structures using <phrase>human</phrase> judgment.
<phrase>Data</phrase>-driven <phrase>Web Design</phrase> This <phrase>short</phrase> <phrase>paper</phrase> summarizes challenges and opportunities of applying <phrase>machine learning</phrase> methods to <phrase>Web design</phrase> problems, and describes how structured prediction, <phrase>deep learning</phrase>, and probabilistic program induction can enable useful interactions for designers. We intend for these techniques to foster new work in <phrase>data</phrase>-driven <phrase>Web design</phrase>. The Web provides an enormous repository of <phrase>design</phrase> <phrase>knowledge</phrase>: every page represents a <phrase>concrete</phrase> example of <phrase>human</phrase> <phrase>creativity</phrase> and <phrase>aesthetics</phrase>. Given the ready availability of Web <phrase>data</phrase>, how can we leverage it to help designers? This note outlines three <phrase>machine learning</phrase> applications which enable new interaction mechanisms for <phrase>Web design</phrase>: structured prediction for rapid re-targeting, <phrase>deep learning</phrase> for <phrase>design</phrase>-based search, and probabilistic program induction for operationalizing <phrase>design patterns</phrase>. All of these techniques leverage structure that is intrinsic to Web designs. In <phrase>machine learning</phrase> applications, working with structured representations affords significant advantages over <phrase>unstructured data</phrase> sets, such as images or text (Socher et al., 2011). On the Web, every page is associated with a <phrase>Document Object Model</phrase> (DOM) <phrase>tree</phrase>, which can be used along with render-time <phrase>information</phrase> to bootstrap a visual <phrase>information</phrase> hierarchy for designs. Figure 1. Our example-based retargeting <phrase>algorithm</phrase>, Brico-lage, automatically renders pages in new layouts and styles. To apply <phrase>machine learning</phrase> techniques to <phrase>Web design</phrase>, we need to collect a corpus of <phrase>training examples</phrase>. Although traditional Web crawlers make it easy to scrape content from pages, acquiring and managing all the resources necessary to preserve a page's render-time appearance is much more difficult. Furthermore, with the advent of client-and <phrase>server-side scripting</phrase> and <phrase>dynamic HTML</phrase>, many modern <phrase>Web pages</phrase> are mutable and may change between accesses, frustrating <phrase>algorithms</phrase> that expect consistent <phrase>training data</phrase>. To overcome these challenges, we have constructed a new kind of Web repository. The repository is populated via a <phrase>bespoke</phrase> <phrase>Web crawler</phrase>, which requests pages through a caching proxy backed by an <phrase>SQL</phrase> <phrase>database</phrase>. As a page is crawled, all requested resources are ver-sioned and stored, its DOM <phrase>tree</phrase> is processed to produce a static visual hierarchy of the page's structure, and a set of <phrase>semantic</phrase> and vision-based features are calculated on each constituent page component. These structures are then exposed through a RESTful <phrase>API</phrase>, allowing fast component-wise queries on features. We
Learning attentional policies for tracking and recognition in <phrase>video</phrase> with deep networks We propose a novel attentional <phrase>model</phrase> for simultaneous <phrase>object tracking</phrase> and recognition that is driven by gaze <phrase>data</phrase>. Motivated by theories of the <phrase>human</phrase> <phrase>perceptual</phrase> system, the <phrase>model</phrase> consists of two interacting pathways: ventral and <phrase>dorsal</phrase>. The ventral pathway models object appearance and classification using deep (factored)-<phrase>restricted Boltzmann machines</phrase>. At each point in time, the observations consist of retinal images, with decaying resolution toward the periphery of the gaze. The <phrase>dorsal</phrase> pathway models the location , orientation, scale and speed of the attended object. The posterior distribution of these states is estimated with <phrase>particle</phrase> filtering. Deeper in the <phrase>dorsal</phrase> pathway, we encounter an attentional mechanism that learns to control gazes so as to minimize tracking uncertainty. The approach is modular (with each module easily replaceable with more sophisticated <phrase>algorithms</phrase>), straightforward to implement, practically efficient, and works well in simple <phrase>video</phrase> sequences.
Promoting Reflection and its Effect on Learning in a <phrase>Programming</phrase> <phrase>Tutor</phrase> We studied the effect of post-practice reflection on learning, using <phrase>programming</phrase> tutors, and <phrase>multiple-choice</phrase> format for reflection. We conducted in-vivo controlled studies with introductory <phrase>programming</phrase> students from multiple schools over 3 semesters, and used mixed-factor <phrase>ANOVA</phrase> to analyze the collected <phrase>data</phrase>. We found that reflecting on the concept underlying each problem neither promotes greater learning, measured as pre-post increase in the <phrase>average</phrase> score per problem, nor promotes faster learning, measured as the problems solved per concept learned. We conjecture that the benefits of reflecting on the concept underlying each problem may be limited if a <phrase>tutor</phrase> already promotes <phrase>deep understanding</phrase> of the domain. Problets We have been developing <phrase>software</phrase> tutors, called problets (www.problets.org) to help <phrase>students learn</phrase> C/<phrase>C++</phrase>/<phrase>Java</phrase>/C# <phrase>programming language</phrase> concepts by <phrase>solving problems</phrase>. To date, we have developed, evaluated and deployed problets on expression evaluation (<phrase>arithmetic</phrase>, <phrase>relational</phrase>, logical, assignment), selection statements, loops (while and for) and <phrase>C++</phrase> pointers. The problets present programs to the learner, ask the learner a question about the program, such as predicting its output or identifying <phrase>bugs</phrase> in it, grade the student's answer, and provide delayed <phrase>feedback</phrase>. Figure 1 shows a snapshot of a problet on selection statements, with the program in the left panel and the <phrase>feedback</phrase> in the right panel. Problets generate problems as instances of parameterized problem templates. Each template is associated with a concept in the domain, e.g., some selection statement concepts include executing a selection statement when the condition is true/false, executing nested if statements, executing if-else statements nested in cascaded/classification style, and executing a program with multiple dependent/<phrase>independent</phrase> selection statements. Similarly, some loop concepts include nested dependent and <phrase>independent</phrase> loops, multiple dependent and <phrase>independent</phrase> loops, loops that iterate zero or one time, and loops that update the loop variables multiple times. Problets administer the pre-<phrase>test</phrase>-practice-post-<phrase>test</phrase> protocol: pre-<phrase>test</phrase> to evaluate the learner's <phrase>knowledge</phrase>; an adaptive practice on only the concepts that the learner has not yet mastered [1], followed by post-<phrase>test</phrase> on only the concepts that the learner has practiced. During the pre-and post-<phrase>tests</phrase>, problets do not provide any <phrase>feedback</phrase>. During practice, problets provide delayed <phrase>feedback</phrase>, which includes a <phrase>narrative</phrase> of the step-by-step execution of the program [2]. Problets use the <phrase>concept map</phrase> of the domain, enhanced with learning objectives, as the overlay <phrase>student</phrase> <phrase>model</phrase> [3]. Problets use reified interfaces [4] which promote the use of mental models when <phrase>solving problems</phrase>, e.g., the learner enters the output of the program 
<phrase>Video</phrase> Tracking Using Learned Hierarchical Features In this <phrase>paper</phrase>, we propose an approach to learn hierarchical features for <phrase>visual object</phrase> tracking. First, we offline learn features robust to diverse motion patterns from <phrase>auxiliary</phrase> <phrase>video</phrase> sequences. The hierarchical features are learned via a two-layer <phrase>convolutional neural network</phrase>. Embedding the temporal slowness constraint in the stacked <phrase>architecture</phrase> makes the <phrase>learned features</phrase> robust to complicated motion transformations, which is important for <phrase>visual object</phrase> tracking. Then, given a <phrase>target</phrase> <phrase>video</phrase> <phrase>sequence</phrase>, we propose a <phrase>domain adaptation</phrase> module to online adapt the pre-<phrase>learned features</phrase> according to the specific <phrase>target</phrase> object. The adaptation is conducted in both layers of the deep <phrase>feature learning</phrase> module so as to include appearance <phrase>information</phrase> of the specific <phrase>target</phrase> object. As a result, the learned hierarchical features can be robust to both complicated motion transformations and appearance changes of <phrase>target</phrase> objects. We integrate our <phrase>feature learning</phrase> <phrase>algorithm</phrase> into three tracking methods. <phrase>Experimental</phrase> <phrase>results</phrase> demonstrate that significant improvement can be achieved using our learned hierarchical features, especially on <phrase>video</phrase> sequences with complicated motion transformations.
Evaluating <phrase>Student</phrase> Perceptions of Using <phrase>Blogs</phrase> in an Online Course Evaluating <phrase>Student</phrase> Perceptions of Using <phrase>Blogs</phrase> in an Online Course INTRODUCTION There are many definitions of <phrase>blogs</phrase>, as they have intrigued many in the <phrase>education</phrase> field. A <phrase>blog</phrase> is a read-write web educational application in which a <phrase>target</phrase> audience can freely share insights, experiences, recommendations, or comments on the topic or <phrase>blog</phrase> post of another peer in that audience group. In other words, a <phrase>blog</phrase> needs to be treated as an active participant in the discursive process (Cameron & Anderson, 2006). While there have been studies on <phrase>blogging</phrase> and its use with learning communities, sharing of resources, and ideas (Oravec, 2003; <phrase>Williams</phrase> & Jacobs, 2004), it seems that little focus has been given to how students perceive <phrase>blogging</phrase> in ABSTRACT This article is based on an exploratory study on the use of <phrase>blogs</phrase> to support learning in an online <phrase>MBA</phrase> <phrase>school</phrase>. In this <phrase>paper</phrase>, the authors examine students' perceptions toward <phrase>blogs</phrase> and their effectiveness. The study finds that although students are open to the idea of using <phrase>blogs</phrase> to enhance presentation and reflection of their learning, concerns exist on their suitability for threaded discussions in the presence of other platforms like threaded discussion boards. Therefore, what is less clear is the learning value and potential of <phrase>blogs</phrase>, especially that <phrase>subset</phrase> of learning that is orchestrated (and credentialed) by formal learning organisations. the context of online <phrase>education</phrase>. <phrase>Williams</phrase> (2004) suggested in his <phrase>paper</phrase> that <phrase>blogging</phrase> can be a trans-formational <phrase>technology</phrase> for <phrase>teaching and learning</phrase>, and so <phrase>universities</phrase> should consider setting up <phrase>blog</phrase> facilities within their learning <phrase>management</phrase> system (<phrase>LMS</phrase>). <phrase>Blogs</phrase> are one of the many applications provided by <phrase>Web 2.0</phrase>, a latest <phrase>technology</phrase> being talked about today in the field of <phrase>education</phrase> and learning. Web2.0 allows for vivid networks of participation (O'Reilly, 2005), with value added by user <phrase>action</phrase> (Jones, 2006). Realising that Web2.0 technologies bring wonderful networking, collaborative, and support opportunities to <phrase>Higher Education</phrase> (Instone, 2005; Weller et al., 2005), we felt that using <phrase>Blogs</phrase>, next to Discussion Boards, as an additional pedagogical tool may just be another way to enhance and enrich <phrase>student</phrase> participation, as well as enable deep reflective learning for our students. <phrase>Blogging</phrase> isn't just a part of the recent <phrase>Web 2.0</phrase> <phrase>technology</phrase>, but also an element of <phrase>Enterprise 2.0</phrase>, <phrase>enterprise social software</phrase> that is used in various organisations and includes <phrase>wikis</phrase>, and collabora-tion/<phrase>groupware</phrase> to name a few (Zhang et al., 2009). A <phrase>blog</phrase> does not restrict the reader or <phrase>writer</phrase> to time (Flately, 2005), it is 
MIReNA: finding microRNAs with <phrase>high</phrase> accuracy and no learning at <phrase>genome</phrase> scale and from deep sequencing <phrase>data</phrase> <phrase>MOTIVATION</phrase> MicroRNAs (miRNAs) are a class of endogenes derived from a precursor (pre-<phrase>miRNA</phrase>) and involved in post-<phrase>transcriptional regulation</phrase>. <phrase>Experimental</phrase> identification of novel miRNAs is difficult because they are often transcribed under specific conditions and <phrase>cell</phrase> types. Several computational methods were developed to detect new miRNAs starting from known ones or from deep sequencing <phrase>data</phrase>, and to validate their pre-miRNAs. <phrase>RESULTS</phrase> We present a <phrase>genome</phrase>-wide <phrase>search algorithm</phrase>, called MIReNA, that looks for <phrase>miRNA</phrase> sequences by exploring a multidimensional space defined by only five (physical and <phrase>combinatorial</phrase>) parameters characterizing acceptable pre-miRNAs. MIReNA validates pre-miRNAs with <phrase>high</phrase> sensitivity and specificity, and detects new miRNAs by homology from known miRNAs or from deep sequencing <phrase>data</phrase>. A performance comparison between MIReNA and four available predictive systems has been done. MIReNA approach is strikingly simple but it turns out to be powerful at least as much as more sophisticated algorithmic methods. MIReNA obtains better <phrase>results</phrase> than three known <phrase>algorithms</phrase> that validate pre-miRNAs. It demonstrates that <phrase>machine-learning</phrase> is not a necessary algorithmic approach for pre-miRNAs computational validation. In particular, <phrase>machine learning</phrase> <phrase>algorithms</phrase> can only confirm pre-miRNAs that <phrase>look alike</phrase> known ones, this being a limitation while exploring <phrase>species</phrase> with no known pre-miRNAs. The possibility to adapt the search to specific <phrase>species</phrase>, possibly characterized by specific properties of their miRNAs and pre-miRNAs, is a <phrase>major</phrase> feature of MIReNA. A parameter adjustment calibrates specificity and sensitivity in MIReNA, a key feature for predictive systems, which is not present in <phrase>machine learning</phrase> approaches. Comparison of MIReNA with miRDeep using deep sequencing <phrase>data</phrase> to predict miRNAs highlights a highly specific predictive power of MIReNA. AVAILABILITY At the address http://www.ihes.fr/carbone/data8/.
Compositional Distributional Models of Meaning Distributional models of meaning (see Turney and Pantel (2010) for an overview) are based on the pragmatic <phrase>hypothesis</phrase> that meanings of words are deducible from the contexts in which they are often used. This <phrase>hypothesis</phrase> is formalized using <phrase>vector spaces</phrase>, wherein a word is represented as a <phrase>vector</phrase> of co-occurrence <phrase>statistics</phrase> with a set of context dimensions. With the increasing availability of large corpora of text, these models constitute a well-established <phrase>NLP</phrase> technique for evaluating <phrase>semantic</phrase> similarities. Their methods however do not scale up to larger text constituents (i.e. phrases and sentences), since the uniqueness of multi-word expressions would inevitably <phrase>lead</phrase> to <phrase>data</phrase> sparsity problems, hence to unreliable vectorial representations. The problem is usually addressed by the provision of a compositional <phrase>function</phrase>, the purpose of which is to prepare a <phrase>vector</phrase> for a phrase or sentence by combining the vectors of the words therein. This line of <phrase>research</phrase> has <phrase>led</phrase> to the field of compositional distributional models of meaning (CDMs), where reliable <phrase>semantic</phrase> representations are provided for phrases, sentences, and discourse units such as dialogue utterances and even paragraphs or documents. As a result, these models have found applications in various <phrase>NLP</phrase> tasks, for example paraphrase detection; <phrase>sentiment analysis</phrase>; dialogue <phrase>act</phrase> tagging; <phrase>machine translation</phrase>; textual entailment; and so on, in many cases presenting <phrase>state</phrase>-of-the-<phrase>art</phrase> performance. Being the natural <phrase>evolution</phrase> of the traditional and well-studied distributional models at the word level, CDMs are steadily evolving to a popular and active <phrase>area</phrase> of <phrase>NLP</phrase>. The topic has inspired a number of workshops and tutorials in top CL conferences such as <phrase>ACL</phrase> and EMNLP, special issues at <phrase>high</phrase>-profile <phrase>journals</phrase>, and it attracts a substantial amount of submissions in annual <phrase>NLP</phrase> conferences. The approaches employed by CDMs are as much as diverse as statistical machine leaning (Baroni and Zamparelli, 2010), <phrase>linear algebra</phrase> (Mitchell and Lapata, 2010), simple <phrase>category theory</phrase> (Coecke et al., 2010), or complex <phrase>deep learning</phrase> architectures based on <phrase>neural networks</phrase> and borrowing ideas from <phrase>image processing</phrase> (Socher et al., 2012; Kalchbrenner et al., 2014; Cheng and Kartsaklis, 2015). Furthermore, they create opportunities for interesting novel <phrase>research</phrase>, related for example to efficient methods for creating tensors for <phrase>relational</phrase> words such as <phrase>verbs</phrase> and <phrase>adjectives</phrase> (Grefenstette and Sadrzadeh, 2011), the treatment of logical and functional words in a distributional setting (Sadrzadeh et al., 2013; Sadrzadeh et al., 2014), or the role of <phrase>polysemy</phrase> and the way it affects composition (Kartsaklis and Sadrzadeh, 2013; Cheng 
Deep Learners Benefit More from Out-of-Distribution Examples Recent theoretical and empirical work in <phrase>statistical machine learning</phrase> has demonstrated the potential of <phrase>learning algorithms</phrase> for deep architec-tures, i.e., <phrase>function</phrase> classes obtained by <phrase>composing</phrase> <phrase>multiple levels</phrase> of representation. The <phrase>hypothesis</phrase> evaluated here is that intermediate levels of representation, because they can be shared across tasks and examples from different but related <phrase>distributions</phrase>, can yield even more benefits. Comparative experiments were performed on a <phrase>large-scale</phrase> handwritten <phrase>character recognition</phrase> setting with 62 classes (<phrase>upper</phrase> case, <phrase>lower</phrase> case, digits), using both a <phrase>multi-task</phrase> setting and perturbed examples in <phrase>order</phrase> to obtain out-of-distribution examples. The <phrase>results</phrase> agree with the <phrase>hypothesis</phrase>, and show that a deep learner did beat previously <phrase>published results</phrase> and reached <phrase>human</phrase>-level performance.
<phrase>Spacecraft</phrase> Antenna <phrase>Research</phrase> and Development Activities Aimed at Future Missions Space missions of the <phrase>Jet Propulsion Laboratory</phrase> (<phrase>JPL</phrase>) of the <phrase>National Aeronautics and Space Administration</phrase> (<phrase>NASA</phrase>) can be <phrase>categorized</phrase> into two <phrase>major</phrase> areas: deep-<phrase>space exploration</phrase> and <phrase>Earth</phrase> <phrase>remote sensing</phrase>. As scientists have learned from the previous missions, higher capabilities and more stringent system requirements are being placed on future missions, such as longer distance communications, higher <phrase>data</phrase> rate, and finer <phrase>radar imaging</phrase> resolution. Almost all these stringent requirements call for higher-gain and larger-<phrase>aperture</phrase> <phrase>spacecraft</phrase> antennas. At the same time, however, <phrase>lower</phrase> <phrase>mass</phrase> and smaller stowage volume for the <phrase>spacecraft</phrase> antenna are demanded in <phrase>order</phrase> to reduce payload weight and reduce required shroud space, and thus minimize overall launch cost. To meet these goals, several space-deployable antenna concepts [1] have been investigated over the past several decades. To name a few, there were the Harris Corporation's hoop-column umbrella type, Lockheed's wrapped-rib version, TRW's sunflower antenna, and the more recent Astro mesh. All these deployable antennas are of the <phrase>parabolic reflector</phrase> type with metalized mesh reflecting surfaces. Because they have been parabolic with a relatively small <phrase>focal length</phrase>, they lack wide-angle <phrase>beam</phrase> scanning abilityonly a few beamwidths can be scanned. The mesh surface also limits the <phrase>upper</phrase> <phrase>frequency</phrase> of operation to <phrase>Ku-band</phrase> or <phrase>lower</phrase>. In addition, some of these concepts suffer from higher <phrase>risk</phrase> because of too many mechanical components. One good example of mechanical component failure is the <phrase>Galileo</phrase> <phrase>spacecraft</phrase>,
<phrase>Design</phrase>, use and experience of <phrase>e</phrase>-learning systems The use of computer applications to support learning and assessment is becoming more common, along with a growing body of <phrase>research</phrase> focusing on the pedagogical effectiveness of these applications. However, until recently less <phrase>research</phrase> attention has been given to the <phrase>design</phrase> of learning <phrase>technology</phrase> with regard to their <phrase>usability</phrase>, actual use, and the way they motivate and engage learners. Learner centred <phrase>design</phrase> [7] looks beyond the technological possibilities such as <phrase>distance learning</phrase>, <phrase>virtual reality</phrase>, and computer assisted assessments by focussing on learners in their learning contexts, and how their interaction with these applications can help and stimulate them to apply <phrase>deep learning</phrase> strategies. However, what are the best and most effective ways to accomplish this? Can <phrase>lessons learned</phrase> in the field of HCI be directly applied, or do <phrase>e</phrase>-learning applications have their own set of <phrase>design</phrase> guidelines? The workshop plans to bring together individuals with an interest in the <phrase>design</phrase> and use of <phrase>e</phrase>-learning systems with the aim of improving and understanding the learning experience. The workshop will be a platform to discuss new ideas and to share experiences, but also to identify new <phrase>research</phrase> challenges and potential solutions. K.3 [<phrase>Computers</phrase> and <phrase>education</phrase>]: Computer Uses in <phrase>Education</phrase> <phrase>collaborative learning</phrase>, computer-assisted instruction (CAI), computer-managed instruction (<phrase>CMI</phrase>), and <phrase>Distance learning</phrase>.
<phrase>Neural Network</phrase> Regularization via Robust Weight Factorization Regularization is essential when training large <phrase>neural networks</phrase>. As <phrase>deep neural networks</phrase> can be mathematically interpreted as <phrase>universal</phrase> <phrase>function</phrase> approximators, they are effective at memorizing sampling noise in the <phrase>training data</phrase>. This <phrase>results</phrase> in poor generalization to unseen <phrase>data</phrase>. Therefore, it is no surprise that a new reg-ularization technique, Dropout, was partially responsible for the now-ubiquitous winning entry to ImageNet 2012 by the <phrase>University</phrase> of <phrase>Toronto</phrase>. Currently, Dropout (and related methods such as DropConnect) are the most effective means of regu-larizing large <phrase>neural networks</phrase>. These amount to efficiently visiting a large number of related models at training time, while aggregating them to a <phrase>single</phrase> predictor at <phrase>test</phrase> time. The proposed FaMe <phrase>model</phrase> aims to apply a similar strategy, yet learns a factorization of each weight matrix such that the factors are robust to noise.
Deep Parser for <phrase>Free</phrase> <phrase>English</phrase> Texts Based on <phrase>Machine Learning</phrase> with Limited Resources The <phrase>paper</phrase> presents an attempt at the <phrase>construction</phrase> of a wide scale parser for <phrase>English</phrase> based on Inductive Learning and limited resources. The parser loosely preserves the shift-reduce scheme, enriched with powerful actions, and a <phrase>compound</phrase> <phrase>Decision Tree</phrase> instead of the decision table. The attempt originates directly from Hermjakob's ideas [3], but an important goal was to analyse possible extensions to a wide scale <phrase>solution</phrase>. Several supporting heuristics, as well as the overview of the development process and experiments, are described in the <phrase>paper</phrase>.
SCUT-FBP: A Benchmark Dataset for Facial Beauty <phrase>Perception</phrase> In this <phrase>paper</phrase>, a novel face dataset with attractiveness ratings, namely, the SCUT-FBP dataset, is developed for automatic facial beauty <phrase>perception</phrase>. This dataset provides a benchmark to evaluate the performance of different methods for facial attractiveness prediction, including the <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>deep learning</phrase> method. The SCUT-FBP dataset contains face portraits of 500 <phrase>Asian</phrase> female subjects with attractiveness ratings, all of which have been verified in terms of rating distribution, <phrase>standard deviation</phrase>, consistency, and self-consistency. Benchmark evaluations for facial attractiveness prediction were performed with different combinations of facial geometrical features and texture features using <phrase>classical</phrase> statistical learning methods and the <phrase>deep learning</phrase> method. The best Pearson correlation (0.8187) was achieved by the <phrase>CNN</phrase> <phrase>model</phrase>. Thus, the <phrase>results</phrase> of our experiments indicate that the SCUT-FBP dataset provides a reliable benchmark for facial beauty <phrase>perception</phrase>.
Rethinking analysis of progressive discourse in <phrase>online learning</phrase>: an <phrase>activity theory</phrase> perspective This <phrase>paper</phrase> describes an innovative approach to analyze the progression of dialogue in asynchronous online forums. Although schemes analyzing the content of individual messages exist, they fail to capture the subtle relationships between messages that constitute progressive discourse for <phrase>knowledge</phrase> building. We present a group-level <phrase>discourse analysis</phrase> based on <phrase>cultural</phrase>-historical <phrase>activity theory</phrase> that characterizes the unfolding <phrase>collaborative learning</phrase> and <phrase>knowledge</phrase> <phrase>construction</phrase> processes in context. The application of the mixed-method approach is illustrated in the context of two online graduate courses. The analysis highlights connected sequences of discursive actions that multiple students make to advance shared understanding. The <phrase>mechanics</phrase> of the approach offered in this <phrase>paper</phrase> can be used as an analytic and transformative tool for enhancing <phrase>online learning</phrase>, <phrase>research</phrase>, and instruction. Objectives Historically, researchers analyzing online discourse to improve the depth of learning that occurs in these <phrase>computer-supported</phrase> <phrase>collaborative learning</phrase> (CSCL) environments have focused on the content of individual contributions. Existing <phrase>content analysis</phrase> schemes (<phrase>e</phrase>. thus tend to classify the content of individual student's notes into hierarchical categories of interaction and phases of <phrase>knowledge</phrase> <phrase>construction</phrase> or inquiry. From sociocultural perspectives, however, <phrase>knowledge</phrase> is constructed through social interactions among individuals mediated by tools (<phrase>Vygotsky</phrase>, 1978). It is critical for <phrase>research</phrase> in CSCL to understand the collaborative process of <phrase>knowledge</phrase> <phrase>construction</phrase> that occur as students try to learn and the role that tools may <phrase>play</phrase> in mediating their learning (Hmelo-<phrase>Silver</phrase>, 2003). Different analysis approaches than reductive <phrase>content analysis</phrase> are needed to understand how assessments of individual engagement in the discourse relate to assessments of <phrase>group cognition</phrase> and <phrase>knowledge</phrase> building, where the holistic outcome exceeds the sum of its parts (Bereiter, 2002; Stahl, 2003). As Lee, Chan & van <phrase>Aalst</phrase> (2006) observed, assessing and fostering the complex interactions between individual and group understanding remains a challenge for learning scientists. In the present <phrase>paper</phrase>, we rethink the analysis of <phrase>student</phrase> discourse aimed at <phrase>deep learning</phrase> and <phrase>knowledge</phrase> building based on an <phrase>cultural</phrase>-historical <phrase>activity theory</phrase> (Engestrm, 1987, 1993) framework to highlight the importance of <phrase>cultural</phrase> context in which this activity takes place. <phrase>Knowledge</phrase> building is defined as " the <phrase>production</phrase> and continual improvement of ideas of value to a <phrase>community</phrase> " (Scardamalia & Bereiter, 2003, p. 1370). <phrase>Knowledge</phrase> building <phrase>pedagogy</phrase> thus emphasizes constructivist learning for <phrase>deep understanding</phrase>. A focal activity in <phrase>knowledge</phrase> building communities is to engage in progressive discourse through which participants develop " a new understanding that everyone involved agrees is <phrase>superior</phrase> to 
Spoken <phrase>Emotion</phrase> Recognition Using <phrase>Deep Learning</phrase> Spoken <phrase>emotion</phrase> recognition is a multidisciplinary <phrase>research</phrase> <phrase>area</phrase> that has received increasing attention over the last few years. In this <phrase>paper</phrase>, <phrase>restricted Boltzmann machines</phrase> and <phrase>deep belief</phrase> networks are used to classify emotions in speech. The <phrase>motivation</phrase> lies in the recent success reported using these <phrase>alternative</phrase> techniques in <phrase>speech processing</phrase> and <phrase>speech recognition</phrase>. This classifier is compared with a <phrase>multilayer perceptron</phrase> classifier, using spectral and prosodic characteristics. A well-known <phrase>German</phrase> emotional <phrase>database</phrase> is used in the experiments and two methodologies of <phrase>cross-validation</phrase> are proposed. Our <phrase>experimental</phrase> <phrase>results</phrase> show that the deep <phrase>method achieves</phrase> an improvement of 8.67% over the baseline in a <phrase>speaker</phrase> <phrase>independent</phrase> scheme.
Deep Relative <phrase>Distance Learning</phrase>: Tell the Difference between Similar Vehicles The growing explosion in the use of <phrase>surveillance</phrase> cameras in <phrase>public</phrase> <phrase>security</phrase> highlights the importance of vehicle search from a <phrase>large-scale</phrase> image or <phrase>video</phrase> <phrase>database</phrase>. However, compared with person re-identification or <phrase>face recognition</phrase>, vehicle search problem has <phrase>long</phrase> been neglected by researchers in vision <phrase>community</phrase>. This <phrase>paper</phrase> focuses on an interesting but challenging problem, vehicle re-identification (a.k.a precise vehicle search). We propose a Deep Relative <phrase>Distance Learning</phrase> (DRDL) method which exploits a two-branch <phrase>deep convolutional</phrase> network to project raw vehicle images into an <phrase>Euclidean space</phrase> where distance can be directly used to measure the similarity of arbitrary two vehicles. To further facilitate the <phrase>future research</phrase> on this problem, we also present a carefully-organized <phrase>large-scale</phrase> image <phrase>database</phrase> " VehicleID " , which includes multiple images of the same vehicle captured by different <phrase>real-world</phrase> cameras in a <phrase>city</phrase>. We evaluate our DRDL method on our VehicleID dataset and another recently-released vehicle <phrase>model</phrase> classification dataset " CompCars " in three sets of experiments: vehicle re-identification, vehicle <phrase>model</phrase> verification and vehicle retrieval. <phrase>Experimental</phrase> <phrase>results</phrase> show that our method can achieve <phrase>promising results</phrase> and outper-forms several <phrase>state</phrase>-of-the-<phrase>art</phrase> approaches.
Deep Supervised t-Distributed Embedding <phrase>Deep learning</phrase> has been successfully applied to perform non-linear embedding. In this <phrase>paper</phrase> , we present supervised embedding techniques that use a deep network to collapse classes. The network is <phrase>pre-trained</phrase> using a stack of RBMs, and finetuned using approaches that try to collapse classes. The finetuning is inspired by ideas from NCA, but it uses a <phrase>Student</phrase> t-distribution to <phrase>model</phrase> the similarities of <phrase>data</phrase> points belonging to the same class in the embedding. We investigate two types of objective functions: deep t-distributed MCML (dt-MCML) and deep t-distributed NCA (dt-NCA). Our experiments on two <phrase>handwritten digit</phrase> <phrase>data</phrase> sets reveal the strong performance of dt-MCML in supervised parametric <phrase>data visualization</phrase>, whereas dt-NCA outperforms <phrase>alternative</phrase> techniques when embeddings with more than two or three dimensions are constructed, e.g., to obtain good classification performances. Overall , our <phrase>results</phrase> demonstrate the advantage of using a <phrase>deep architecture</phrase> and a heavy-tailed t-distribution for measuring pairwise similarities in supervised embedding.
Describing <phrase>Multimedia</phrase> Content Using Attention-Based Encoder-Decoder Networks Whereas <phrase>deep neural networks</phrase> were first mostly used for <phrase>classification tasks</phrase>, they are rapidly expanding in the realm of structured output problems, where the observed <phrase>target</phrase> is composed of multiple <phrase>random variables</phrase> that have a rich joint distribution, given the input. We focus in this <phrase>paper</phrase> on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a <phrase>variety</phrase> of tasks: <phrase>machine translation</phrase>, image caption generation, <phrase>video</phrase> clip description and <phrase>speech recognition</phrase>. All these systems are based on a shared set of <phrase>building blocks</phrase>: gated <phrase>recurrent neural networks</phrase> and <phrase>convolutional neural networks</phrase>, along with trained attention mechanisms. We <phrase>report</phrase> on <phrase>experimental</phrase> <phrase>results</phrase> with these systems, showing impressively good performance and the advantage of the attention mechanism.
Ensemble <phrase>deep learning</phrase> for <phrase>speech recognition</phrase> <phrase>Deep learning</phrase> systems have dramatically improved the accuracy of <phrase>speech recognition</phrase>, and various <phrase>deep architectures</phrase> and learning methods have been developed with distinct strengths and weaknesses in recent years. How can ensemble learning be applied to these varying <phrase>deep learning</phrase> systems to achieve greater <phrase>recognition accuracy</phrase> is the focus of this <phrase>paper</phrase>. We develop and <phrase>report</phrase> linear and log-linear stacking methods for ensemble learning with applications specifically to speech-class posterior <phrase>probabilities</phrase> as computed by the convolutional, recurrent, and <phrase>fully-connected</phrase> <phrase>deep neural networks</phrase>. <phrase>Convex optimization</phrase> problems are formulated and solved, with analytical formulas derived for training the ensemble-learning parameters. <phrase>Experimental</phrase> <phrase>results</phrase> demonstrate a significant increase in <phrase>phone recognition</phrase> accuracy after stacking the <phrase>deep learning</phrase> subsystems that use different mechanisms for <phrase>computing</phrase> <phrase>high</phrase>-level, hierarchical features from the raw <phrase>acoustic</phrase> signals in speech.
<phrase>Translation</phrase>-Invariant Representation for Cumulative Foot <phrase>Pressure</phrase> Images <phrase>Human</phrase> can be distinguished by different limb movements and unique ground <phrase>reaction</phrase> force. Cumulative foot <phrase>pressure</phrase> image is a 2-D cumulative ground <phrase>reaction</phrase> force during one <phrase>gait</phrase> cycle. Although it contains <phrase>pressure</phrase> spatial distribution <phrase>information</phrase> and <phrase>pressure</phrase> temporal distribution <phrase>information</phrase>, it suffers from several problems including different shoes and noise, when putting it into practice as a new biometric for pedestrian identification. In this <phrase>paper</phrase>, we propose a hierarchical <phrase>translation</phrase>-invariant representation for cumulative foot <phrase>pressure</phrase> images, inspired by the success of Convolutional <phrase>deep belief</phrase> network for <phrase>digital</phrase> classification. Key contribution in our approach is discriminative hierarchical <phrase>sparse coding</phrase> scheme which helps to learn useful discriminative <phrase>high</phrase>-level <phrase>visual features</phrase>. Based on the <phrase>feature representation</phrase> of cumulative foot <phrase>pressure</phrase> images, we develop a pedestrian recognition system which is invariant to three different shoes and slight local shape change. Experiments are conducted on a proposed open dataset that contains more than 2800 cumulative foot <phrase>pressure</phrase> images from 118 subjects. Evaluations suggest the effectiveness of the <phrase>proposed method</phrase> and the potential of cumulative foot <phrase>pressure</phrase> images as a biometric.
A Readability Checker with <phrase>Supervised Learning</phrase> Using Deep Indicators Checking for readability or simplicity of texts is important for many institutional and individual users. Formulas for approximately measuring text readability have a <phrase>long</phrase> tradition. Usually, they exploit surface-oriented indicators like sentence length, word length, word <phrase>frequency</phrase>, etc. However, in many cases, this <phrase>information</phrase> is not adequate to realistically approximate the <phrase>cognitive</phrase> difficulties a person can have to understand a text. Therefore we use <phrase>deep syntactic</phrase> and <phrase>semantic</phrase> indicators in addition. The <phrase>syntactic</phrase> <phrase>information</phrase> is represented by a dependency <phrase>tree</phrase>, the <phrase>semantic</phrase> <phrase>information</phrase> by a <phrase>semantic network</phrase>. Both representations are automatically generated by a deep syntactico-<phrase>semantic</phrase> analysis. A global readability score is determined by applying a nearest neighbor <phrase>algorithm</phrase> on 3,000 ratings of 300 <phrase>test</phrase> persons. The evaluation showed that the <phrase>deep syntactic</phrase> and <phrase>semantic</phrase> indicators <phrase>lead</phrase> to <phrase>promising results</phrase> comparable to the best surface-based indicators. The combination of deep and shallow indicators leads to an improvement over shallow indicators alone. Finally, a <phrase>graphical user interface</phrase> was developed which highlights difficult passages, depending on the individual indicator values, and displays a global readability score. Povzetek: Strojno uenje z odvisnostnimi drevesi je uporabljeno za ugotavljanje berljivosti besedil.
Wide-Coverage Deep Statistical <phrase>Parsing</phrase> Using Automatic Dependency Structure Annotation 2004) have convincingly argued for the use of dependency (rather than CFG-<phrase>tree</phrase>) representations for parser evaluation. Preiss (2003) and Kaplan et al. (2004) conducted a number of experiments comparing " deep " <phrase>hand-crafted</phrase> wide-coverage with " shallow " treebank-and <phrase>machine-learning</phrase>-based parsers at the level of dependencies, using simple and automatic methods to convert <phrase>tree</phrase> output generated by the shallow parsers into dependencies. In this article, we revisit the experiments in Preiss (2003) and Kaplan et al. (2004), this time using the sophisticated automatic LFG f-structure annotation methodologies of Cahill et al. (2002b, 2004) and Burke (2006), with surprising <phrase>results</phrase>. We compare various PCFG and <phrase>history</phrase>-based parsers (<phrase>Bank</phrase> (Carroll, Briscoe, and Sanfilippo 1998), a statistically significant 3.66% improvement over the 76.57% achieved by the <phrase>hand-crafted</phrase> <phrase>RASP</phrase> <phrase>grammar</phrase> and <phrase>parsing</phrase> system of Carroll and Briscoe (2002).
Feature <phrase>Reinforcement Learning</phrase> using Looping <phrase>Suffix</phrase> <phrase>Trees</phrase> There has recently been much interest in <phrase>history</phrase>-based methods using <phrase>suffix</phrase> <phrase>trees</phrase> to solve POMDPs. However, these <phrase>suffix</phrase> <phrase>trees</phrase> cannot efficiently represent environments that have <phrase>long</phrase>-term dependencies. We extend the recently introduced CTMDP <phrase>algorithm</phrase> to the space of looping <phrase>suffix</phrase> <phrase>trees</phrase> which have previously only been used in solving determinis-tic POMDPs. The resulting <phrase>algorithm</phrase> replicates <phrase>results</phrase> from CTMDP for environments with <phrase>short</phrase> term dependencies, while it outperforms LSTM-based methods on TMaze, a deep <phrase>memory</phrase> environment.
FACE2GPS: Estimating geographic location from facial features The facial appearance of a person is a product of many factors, including their <phrase>gender</phrase>, age, and ethnicity. Methods for estimating these latent factors directly from an image of a face have been extensively studied for decades. We extend this line of work to include estimating the location where the image was taken. We propose a deep <phrase>network architecture</phrase> for making such predictions and demonstrate its superiority to other approaches in an extensive set of quantitative experiments on the GeoFaces dataset. Our experiments show that in 26% of the cases the <phrase>ground truth</phrase> location is the topmost prediction, and if we allow ourselves to consider the top five predictions, the accuracy increases to 47%. In both cases, the <phrase>deep learning</phrase> <phrase>based approach</phrase> <phrase>significantly outperforms</phrase> random chance as well as another baseline method.
Learning Hierarchical Spectral-Spatial Features for <phrase>Hyperspectral</phrase> <phrase>Image Classification</phrase> This <phrase>paper</phrase> proposes a spectral-spatial <phrase>feature learning</phrase> (SSFL) method to obtain robust features of <phrase>hyperspectral</phrase> images (HSIs). It combines the spectral <phrase>feature learning</phrase> and spatial <phrase>feature learning</phrase> in a hierarchical <phrase>fashion</phrase>. Stacking a set of SSFL units, a deep hierarchical <phrase>model</phrase> called the spectral-spatial networks (SSN) is further proposed for HSI classification. SSN can exploit both discriminative spectral and spatial <phrase>information</phrase> simultaneously. Specifically, SSN learns useful <phrase>high-level</phrase> features by alternating between spectral and spatial <phrase>feature learning</phrase> operations. Then, <phrase>kernel-based</phrase> <phrase>extreme learning</phrase> machine (KELM), a shallow <phrase>neural network</phrase>, is embedded in SSN to classify image <phrase>pixels</phrase>. <phrase>Extensive experiments</phrase> are performed on two benchmark HSI datasets to verify the effectiveness of SSN. Compared with <phrase>state</phrase>-of-the-<phrase>art</phrase> methods, SSN with a deep hierarchical <phrase>architecture</phrase> obtains higher <phrase>classification accuracy</phrase> in terms of the overall accuracy, <phrase>average</phrase> accuracy, and kappa ( ) coefficient of agreement, especially when the number of the <phrase>training samples</phrase> is small.
Explanations and <phrase>Case-Based Reasoning</phrase>: Foundational Issues By <phrase>design</phrase>, <phrase>Case-Based Reasoning</phrase> (CBR) systems do not need deep <phrase>general</phrase> <phrase>knowledge</phrase>. In contrast to (<phrase>rule-based</phrase>) <phrase>expert systems</phrase>, CBR systems can already be used with just some initial <phrase>knowledge</phrase>. Further <phrase>knowledge</phrase> can then be added manually or learned over time. CBR systems are not addressing a special group of users. <phrase>Expert systems</phrase>, on the other hand, are intended to <phrase>solve problems</phrase> similar to <phrase>human</phrase> experts. Because of the complexity and difficulty of building and using <phrase>expert systems</phrase>, <phrase>research</phrase> in this <phrase>area</phrase> addressed generating explanations right from the beginning. But for <phrase>knowledge</phrase>-intensive CBR applications, the demand for explanations is also growing. This <phrase>paper</phrase> is a first <phrase>pass</phrase> on examining issues concerning explanations <phrase>produced</phrase> by CBR systems from the <phrase>knowledge</phrase> containers perspective. It discusses what naturally can be explained by each of the four <phrase>knowledge</phrase> containers (<phrase>vocabulary</phrase>, similarity measures, adaptation <phrase>knowledge</phrase>, and case base) in relation to scientific, conceptual, and <phrase>cognitive</phrase> explanations.
The stepping <phrase>stones</phrase> project Widening participation in <phrase>higher education</phrase> institutions is bringing about <phrase>major</phrase> changes to the <phrase>student</phrase> profile. New <phrase>universities</phrase>, especially, have found that a higher proportion of students now come from non-traditional backgrounds. Since there is seen to be a correlation between increasing access to students from non-traditional backgrounds and higher drop-out rates [1] this trend presents problems in terms of achievement, progression and retention.Meeting the diverse needs of students will accelerate changes already underway to enhance <phrase>teaching and learning</phrase> in <phrase>universities</phrase>, but there are obstacles, particularly the reluctance to bring about changes to the <phrase>curriculum</phrase> <phrase>design</phrase> and delivery in case standards should become diluted.In line with <phrase>results</phrase> obtained nationally, statistical examination of our first-year <phrase>database</phrase> showed that students with a GNVQ background had an enhanced <phrase>risk</phrase> of non-completion. The <phrase>learning styles</phrase> and strategies adopted by students and the particular further <phrase>education</phrase> institution where they previously studied are also thought to be influential in determining a student's chance of success. It is therefore vital that changes to <phrase>curriculum</phrase>, delivery, assessment and support are underpinned by detailed <phrase>knowledge</phrase> of the <phrase>approaches to learning</phrase> adopted by our students, and those factors which enable and encourage students to adopt a deep approach to learning, since a student's approach to a given learning activitiy depends upon his or her <phrase>perception</phrase> of the requirements of the task [2].A <phrase>longitudinal study</phrase> is underway to investigate the influence of entrance qualification, feeder institution, <phrase>learning styles</phrase> and achievement at level one. The <phrase>research</phrase> aims to enable early detection of students at <phrase>risk</phrase> of being unsuccessful and to identify the factors within the further <phrase>education</phrase> environment that <phrase>lead</phrase> to successful learning in <phrase>higher education</phrase>. The <phrase>poster</phrase> will outline the underlying issues, the rationale and the approach being taken.
<phrase>Neural Networks</phrase>: What Non-linearity to Choose <phrase>Neural networks</phrase> are now one of the most successful learning formalisms. <phrase>Neurons</phrase> transform inputs <phrase>x 1</phrase> , ..., x n into an output f (w 1 <phrase>x 1</phrase> + ... + w n x n), where f is a non-<phrase>linear function</phrase> and w i are adjustable weights. What f to choose? Usually the <phrase>logistic function</phrase> is chosen, but sometimes the use of different functions improves the practical efficiency of the network. We formulate the problem of choosing f as a <phrase>mathematical optimization</phrase> problem and solve it under different optimality criteria. As a result, we get a list of functions f that are optimal under these criteria. This list includes both the functions that were empirically proved to be the best for some problems, and some new functions that may be worth trying. <phrase>Neural networks</phrase> are now one of the most successful learning formalisms (see, e.g., the recent survey in [Hecht-Nielsen 1991]). After the initial success of linear neural models, in which the output y is equal to the <phrase>linear combination</phrase> of the input signals x it was shown in [<phrase>Minsky</phrase> Papert 1968] that if we only have linear <phrase>neurons</phrase>, then we end up with only linear functions and this severely limits the number of problems that we can solve using the network. The next step, then, is to consider non-linear <phrase>neurons</phrase>, in which the output signal is equal to f (w 1 <phrase>x 1</phrase> + w 2 x 2 + ...), where f (y) is a given non-<phrase>linear function</phrase>. A natural question arises: what <phrase>function</phrase> f (y) do we choose? Why is this problem important? It is a very important problem because although <phrase>neural networks</phrase> help us to <phrase>design</phrase> good learning procedures, these procedures are far from being reliable. Sometimes these procedures do not work; sometimes they work but demand too much time, and too big a sample, to learn. Naturally, we might think that this is because the <phrase>function</phrase> f that we used was not the best one. Sometimes the use of different functions can improve the practical efficiency of the network (see, e.g., [Wasserman 1989, pp. 15-16]). If a simple guess can really improve the learning performance, then it is natural to suppose that deep <phrase>mathematical optimization</phrase> will <phrase>lead</phrase> to even better <phrase>results</phrase>. Why is this problem difficult? We want to find a <phrase>function</phrase> f for which some characteristics J of learning, such as <phrase>average</phrase> learning time or 
Easy Monotonic Policy Iteration A key problem in <phrase>reinforcement learning</phrase> for control with <phrase>general</phrase> <phrase>function</phrase> approximators (such as <phrase>deep neural networks</phrase> and other nonlinear functions) is that, for many <phrase>algorithms</phrase> employed in practice, updates to the policy or Q-<phrase>function</phrase> may fail to improve performanceor worse, actually cause the policy performance to degrade. Prior work has addressed this for policy iteration by deriving tight policy improvement bounds; by optimizing the <phrase>lower</phrase> bound on policy improvement , a better policy is guaranteed. However, existing approaches suffer from bounds that are hard to optimize in practice because they include sup norm terms which cannot be efficiently estimated or differentiated. In this work, we derive a better policy improvement bound where the sup norm of the policy divergence has been replaced with an <phrase>average</phrase> divergence; this leads to an <phrase>algorithm</phrase> , Easy Monotonic Policy Iteration, that generates sequences of policies with guaranteed non-decreasing returns and is easy to implement in a sample-based framework.
Investigating Convergence of <phrase>Restricted Boltzmann Machine</phrase> Learning <phrase>Restricted Boltzmann Machines</phrase> are increasingly popular tools for unsuper-vised learning. They are very <phrase>general</phrase>, can cope with missing <phrase>data</phrase> and are used to pretrain <phrase>deep learning</phrase> machines. RBMs learn a <phrase>generative model</phrase> of the <phrase>data</phrase> distribution. As exact <phrase>gradient</phrase> ascent on the <phrase>data</phrase> likelihood is infeasible, typically <phrase>Markov Chain Monte Carlo</phrase> approximations to the <phrase>gradient</phrase> such as Contrastive Divergence (<phrase>CD</phrase>) are used. Even though there are some theoretical insights into this <phrase>algorithm</phrase>, it is not guaranteed to converge. Recently it has been observed that after an initial increase in likelihood, the training degrades, if no additional regularization is used. The parameters for regularization however cannot be determined even for medium-sized RBMs. In this work, we investigate the learning behavior of training <phrase>algorithms</phrase> by varying minimal set of parameters and show that with relatively simple variants of <phrase>CD</phrase>, it is possible to obtain good <phrase>results</phrase> even without further regularization. Furthermore, we show that it is not necessary to tune many hyperparameters to obtain a good <phrase>model</phrase> finding a suitable learning rate is sufficient. Fast learning, however, comes with a higher <phrase>risk</phrase> of divergence and therefore requires a stopping criterion. For this purpose, we investigate the commonly used Annealed <phrase>Importance Sampling</phrase>, an approximation to the true <phrase>log likelihood</phrase> of the <phrase>data</phrase> and find that it completely fails to discover divergence in certain cases.
An Integrated, Deep-shallow Expert System for )multi-leve~ Diagnosis of <phrase>Dynamic Systems</phrase> Deep-shallow Expert System for Multi-level Diagnosis of <phrase>Dynamic Systems</phrase> ABSI1lACF An integrated e'Q>en system <phrase>architecture</phrase> and strategy for diagnosis of <phrase>dynamic systems</phrase> with <phrase>feedback loops</phrase> a s~chronous or asynchronous <phrase>state</phrase> transitions is presented. The dynamic system under diagnosis is modeled' using structural and behavioral representations in <phrase>multiple levels</phrase> of abstraction I The diagnosis process inte~tes shallow ~d deep expertise. It recursively navigates through the structural hierarchy; and at each level tries the shallow expertise first If it fails it switches to deep, <phrase>simulation</phrase> based expertise. A multilevel simulator assists the diagnosis process in verification and elimination of hypothesized suspects. <phrase>TIle</phrase> simulator shifts on demand thrdugh several levels. from coarse qualitative modeling to detailed quantitative modeling. <phrase>Knowledge</phrase> of pathological behavior (failure modes) of.lower level cOmponents is incorporated in the simulator. Learning is exhibited as deep-to-shallow expertise transfer, as well as up the abstraction levels of the simulator itself, to improve future efficiency. In addition, <phrase>knowledge</phrase> about pathological behavior can be used for off-line training by artificially generating new cases for diagnosis.
Guest Editorial: <phrase>Deep Learning</phrase>
On mismatch in the deep sub-<phrase>micron</phrase> <phrase>era</phrase> - from <phrase>physics</phrase> to circuits Rapid decrease in feature sizes has increasingly accentuated the importance of matching between <phrase>transistors</phrase>. Deep sub-<phrase>micron</phrase> designs will further emphasize the need to focus on the effects of mismatch. Furthermore, increased efforts on <phrase>high</phrase> level analog device modeling will necessitate accompanying mismatch <phrase>simulation</phrase> and measurement methods. The deep sub-<phrase>micron</phrase> <phrase>era</phrase> forces circuit designers to learn more about the <phrase>physics</phrase> and the <phrase>technology</phrase> of <phrase>transistors</phrase>. This study introduces a method and assists circuit designers in including this method in their traditional <phrase>design</phrase> flow of circuits. By proposing a <phrase>solution</phrase> to the problem of building a modeling <phrase>bridge</phrase> between <phrase>transistor</phrase> mismatch and circuit response to it, we hope to enable designers to incorporate <phrase>low level</phrase> mismatch <phrase>information</phrase> in their <phrase>higher level</phrase> <phrase>design</phrase>.
Deep Image Set <phrase>Hashing</phrase> In applications involving matching of image sets, the <phrase>information</phrase> from multiple images must be effectively exploited to represent each set. <phrase>State</phrase>-of-the-<phrase>art</phrase> methods use probabilistic distribution or sub-space to <phrase>model</phrase> a set and use specific distance measure to compare two sets. These methods are slow to compute and not compact to use in a <phrase>large scale</phrase> scenario. Learning-based <phrase>hashing</phrase> is often used in <phrase>large scale</phrase> <phrase>image retrieval</phrase> as they provide a compact representation of each sample and the <phrase>Hamming distance</phrase> can be used to efficiently compare two samples. However, most <phrase>hashing</phrase> methods encode each image separately and discard <phrase>knowledge</phrase> that multiple images in the same set represent the same object or person. We investigate the set <phrase>hashing</phrase> problem by combining both set representation and <phrase>hashing</phrase> in a <phrase>single</phrase> <phrase>deep neural network</phrase>. An image set is first passed to a <phrase>CNN</phrase> module to extract image features, then these features are aggregated using two types of set feature to capture both set specific and <phrase>database</phrase>-wide distribution <phrase>information</phrase>. The computed set feature is then fed into a <phrase>multilayer perceptron</phrase> to learn a compact <phrase>binary</phrase> embedding. Triplet loss is used to <phrase>train</phrase> the network by forming set similarity relations using class <phrase>labels</phrase>. We extensively evaluate our approach on datasets used for image matching and show highly competitive performance compared to <phrase>state</phrase>-of-the-<phrase>art</phrase> methods.
Climbing the <phrase>Tower</phrase> of Babel: Unsupervised Multilingual Learning For centuries, scholars have explored the deep links among <phrase>human</phrase> languages. In this <phrase>paper</phrase> , we present a class of <phrase>probabilistic models</phrase> that use these links as a form of naturally occurring supervision. These models allow us to substantially <phrase>improve performance</phrase> for core text processing tasks, such as morphological segmentation, <phrase>part-of-speech tagging</phrase>, and <phrase>syntactic</phrase> <phrase>parsing</phrase>. Besides these traditional <phrase>NLP</phrase> tasks, we also present a multilingual <phrase>model</phrase> for the computational decipher-ment of <phrase>lost</phrase> languages. <phrase>Electronic</phrase> text is currently being <phrase>produced</phrase> at a vast and unprecedented scale across the languages of the world. <phrase>Natural Language Processing</phrase> (<phrase>NLP</phrase>) holds out the promise of automatically analyzing this growing body of text. However, over the last several decades, <phrase>NLP</phrase> <phrase>research</phrase> efforts have focused on the <phrase>English language</phrase> , often neglecting the thousands of other languages of the world (Bender, 2009). Most of these languages are currently beyond the reach of <phrase>NLP</phrase> <phrase>technology</phrase> due to several factors. One of these is simply the lack of the kinds of hand-annotated <phrase>linguistic</phrase> resources that have helped propel the performance of <phrase>English language</phrase> systems. For complex tasks of <phrase>linguistic</phrase> analysis, hand-annotated corpora can be prohibitively time-consuming and expensive to produce. For example, the most widely used annotated corpus in the <phrase>English language</phrase>, the <phrase>Penn</phrase> Treebank (Marcus et al., 1994), took years for a team of <phrase>professional</phrase> <phrase>linguists</phrase> to produce. It is unrealistic to expect such resources to ever exist for the majority of the world's languages. Another difficulty for multilingual <phrase>NLP</phrase> is that languages exhibit wide variation in their underlying <phrase>linguistic</phrase> structure. A <phrase>model</phrase> that has been developed for one <phrase>language</phrase> may not account for the kinds of structure found in others. In fact, there exists an entire <phrase>academic</phrase> discipline devoted to studying and describing systematic cross-lingual variations in <phrase>language</phrase> structure , known as <phrase>linguistic typology</phrase> (<phrase>Comrie</phrase>, 1989). At first glance, it may seem that <phrase>linguistic</phrase> diversity would make developing intelligent text-processing tools for the world's languages a very daunting task. However, we argue that in fact it is possible to <phrase>harness</phrase> systematic <phrase>linguistic</phrase> diversity and use it to our advantage , utilizing a framework which we call multilingual learning. The goal of this enterprise is twofold: To induce more accurate models of individual <phrase>language</phrase> structure without any <phrase>human</phrase> annotation. To induce accurate models of the relationships between languages. The multilingual learning framework is based on the <phrase>hypothesis</phrase> that cross-lingual variations in <phrase>linguistic</phrase> structure correspond to variations 
<phrase>Student</phrase> activity in seminars: designing multi-functional assessment events In this <phrase>paper</phrase>, we describe assessment in seminars where <phrase>high</phrase> <phrase>student</phrase> activity is encouraged. The aim of our work has been to <phrase>design</phrase> assessment events that result in <phrase>deep learning</phrase> and <phrase>high</phrase> <phrase>student</phrase> activity, but still provide the <phrase>teacher</phrase> with a reliable basis for <phrase>justice</phrase> in examinations. We will discuss a course in <phrase>electronic commerce</phrase> where we have been working with two progressive seminars. Experiences from this attempt are discussed and analysed in the <phrase>paper</phrase>.
Supervised <phrase>Deep Learning</phrase> for <phrase>Multi-class</phrase> <phrase>Image Classification</phrase> <phrase>Multi-Class</phrase> <phrase>Image Classification</phrase> is a big <phrase>research</phrase> topic with broad application prospects in <phrase>Artificial Intelligence</phrase> field nowadays. This course project describes the supervised <phrase>machine learning</phrase> methods, <phrase>Convolutional Neural Networks</phrase> (a.k.a. CNNs) along with Softmax <phrase>logistic regression</phrase>, to perform <phrase>Multi-Class</phrase> <phrase>image classification</phrase> and implement these <phrase>deep learning</phrase> <phrase>algorithms</phrase> on a <phrase>large-scale</phrase> <phrase>Multi-Class</phrase> <phrase>Image Classification</phrase> dataset from ImageNet annual competition task [1]. The implementation categorizes various images by <phrase>visual features</phrase> and shows illustrative examples of the training performance.
<phrase>Digital</phrase> Hardware <phrase>Design</phrase> Teaching: An <phrase>Alternative</phrase> Approach This <phrase>article presents</phrase> the <phrase>design</phrase> and implementation of a <phrase>complete review</phrase> of <phrase>undergraduate</phrase> <phrase>digital</phrase> hardware <phrase>design</phrase> teaching in the <phrase>School</phrase> of <phrase>Engineering</phrase> at the <phrase>University</phrase> of <phrase>Edinburgh</phrase>. Four guiding principles have been used in this exercise: learning-outcome driven teaching, <phrase>deep learning</phrase>, affordability, and flexibility. This has identified discrete <phrase>electronics</phrase> as key components in the early stages of the <phrase>curriculum</phrase> and <phrase>FPGAs</phrase> as an economical platform for the teaching of various <phrase>digital</phrase> hardware <phrase>design</phrase> concepts and techniques in later stages of the <phrase>curriculum</phrase>. In particular, the <phrase>article presents</phrase> the detailed <phrase>design</phrase> and implementation of one <phrase>digital</phrase> hardware <phrase>design</phrase> <phrase>laboratory</phrase>, called Gateway, which introduces students to synchronous <phrase>digital circuit</phrase> development from <phrase>high</phrase> level functional specifications, uses <phrase>Verilog</phrase> for hardware description and <phrase>FPGAs</phrase> as an implementation platform. Biggs&#8217; theory of constructive alignment was applied in the <phrase>design</phrase> of this lab&#8217;s <phrase>learning outcomes</phrase>, lab content, <phrase>teaching and learning</phrase> methods, and assessment methods. The lab makes extensive use of <phrase>multimedia</phrase> in both lab <phrase>content delivery</phrase> and demonstration applications developed by students. <phrase>Student</phrase> <phrase>feedback</phrase> following the deployment of this lab was overwhelmingly positive and an evaluation of the lab <phrase>results</phrase> compared to previous lab offerings&#8217; shows the merit of the approach taken.
Robust <phrase>Face Recognition</phrase> via Multimodal Deep Face Representation Face images appeared in <phrase>multimedia</phrase> applications, e.g., <phrase>social networks</phrase> and <phrase>digital</phrase> <phrase>entertainment</phrase>, usually exhibit dramatic pose, illumination, and expression variations, resulting in considerable performance degradation for traditional <phrase>face recognition</phrase> <phrase>algorithms</phrase>. This <phrase>paper</phrase> proposes a <phrase>comprehensive</phrase> <phrase>deep learning</phrase> framework to jointly learn face representation using multimodal <phrase>information</phrase>. The proposed <phrase>deep learning</phrase> structure is composed of a set of elaborately designed <phrase>convolutional neural networks</phrase> (CNNs) and a three-layer stacked <phrase>auto-encoder</phrase> (SAE). The set of CNNs extracts complementary facial features from multimodal <phrase>data</phrase>. Then, the extracted features are concate-nated to form a <phrase>high</phrase>-dimensional feature <phrase>vector</phrase>, whose <phrase>dimension</phrase> is compressed by SAE. All the CNNs are trained using a <phrase>subset</phrase> of 9,000 subjects from the publicly available CASIA-WebFace <phrase>database</phrase>, which ensures the <phrase>reproducibility</phrase> of this work. Using the proposed <phrase>single</phrase> <phrase>CNN</phrase> <phrase>architecture</phrase> and limited <phrase>training data</phrase>, 98.43% verification rate is achieved on the LFW <phrase>database</phrase>. Benefited from the complementary <phrase>information</phrase> contained in multimodal <phrase>data</phrase>, our small ensemble system achieves higher than 99.0% recognition rate on LFW using publicly available <phrase>training set</phrase>.
<phrase>Deep Learning</phrase> For Sequential <phrase>Pattern Recognition</phrase> Recognition' and the work presented in it are my own. I confirm that: This work was done wholly or mainly while in candidature for a <phrase>research</phrase> <phrase>degree</phrase> at this <phrase>University</phrase>. Where any part of this <phrase>thesis</phrase> has previously been submitted for a <phrase>degree</phrase> or any other qualification at this <phrase>University</phrase> or any other institution, this has been clearly stated. Where I have consulted the published work of others, this is always clearly attributed. Where I have quoted from the work of others, the source is always given. With the exception of such quotations, this <phrase>thesis</phrase> is entirely my own work. I have acknowledged all main sources of help. Where the <phrase>thesis</phrase> is based on work done by myself jointly with others, I have made clear exactly what was done by others and what I have contributed myself. Signed: Date: i " The truth is the whole. The whole, however, is merely the essential <phrase>nature</phrase> reaching its completeness through the process of its own development. Of the Absolute it must be said that it is essentially a result, that only at the end is it what it is in very truth; and just in that consists its <phrase>nature</phrase>, which is to be actual, subject, or self-becoming, self-development. " <phrase>Hegel</phrase> TECHNISCHE UNIVERSITAT M UNCHEN (TUM) In recent years, <phrase>deep learning</phrase> has opened a new <phrase>research</phrase> line in <phrase>pattern recognition</phrase> tasks. It has been hypothesized that this kind of learning would capture more abstract patterns concealed in <phrase>data</phrase>. It is motivated by the new findings both in biological aspects of the <phrase>brain</phrase> and hardware developments which have made the <phrase>parallel processing</phrase> possible. <phrase>Deep learning</phrase> methods come along with the conventional <phrase>algorithms</phrase> for optimization and training make them efficient for <phrase>variety</phrase> of applications in <phrase>signal processing</phrase> and <phrase>pattern recognition</phrase>. This <phrase>thesis</phrase> explores these novel techniques and their related <phrase>algorithms</phrase>. It addresses and compares different attributes of these methods, sketches in their possible advantages and disadvantages. Acknowledgements I would like to <phrase>express my gratitude</phrase> and special thanks to my supervisor, <phrase>Professor</phrase> Martin Kleinsteuber for his enthusiasm, <phrase>patience</phrase> and kindness. I have learned many aspects of carrying out <phrase>research</phrase> in the field of <phrase>pattern recognition</phrase> in the <phrase>department</phrase> of Geometric Optimization and <phrase>Machine Learning</phrase> in the faculty of <phrase>Electrical Engineering</phrase> and <phrase>Information Technology</phrase> of Technische Universitt Mnchen (TUM), from his comments. I would also like to thank Clemens Hage for his advises and suggestions, and 
Pockets of Potential Using <phrase>Mobile</phrase> Technologies to Promote Children's Learning The mission of the <phrase>Joan Ganz Cooney</phrase> <phrase>Center</phrase> at <phrase>Sesame Workshop</phrase> is to <phrase>harness</phrase> <phrase>digital media</phrase> technologies to advance children's learning. The <phrase>Center</phrase> supports <phrase>action research</phrase>, encourages partnerships to connect <phrase>child development</phrase> experts and educators with <phrase>interactive media</phrase> and <phrase>technology</phrase> leaders, and mobilizes <phrase>public</phrase> and <phrase>private</phrase> <phrase>investment</phrase> in promising and <phrase>proven</phrase> new <phrase>media</phrase> technologies for children. The <phrase>Joan Ganz Cooney</phrase> <phrase>Center</phrase> has a deep commitment toward dissemination of useful and timely <phrase>research</phrase>. Working closely with our Cooney Fellows, national advisors, <phrase>media</phrase> scholars, and practitioners, the <phrase>Center</phrase> publishes <phrase>industry</phrase>, policy, and <phrase>research</phrase> briefs examining key issues in the fi eld of <phrase>digital media</phrase> and learning. No part of this publication may be reproduced or transmitted in any form or by any means, <phrase>electronic</phrase> or mechanical, including photocopy, or any <phrase>information</phrase> storage and retrieval system, without permission from the <phrase>Joan Ganz Cooney</phrase> <phrase>Center</phrase> at <phrase>Sesame Workshop</phrase>. A full-text <phrase>PDF</phrase> of this document is available for <phrase>free</phrase> download from www.joanganzcooneycenter.org. Individual print copies of this publication are available for $15 via check, <phrase>money order</phrase>, or <phrase>purchase order</phrase> sent to the address below. Bulk rate prices are available on request. contents foreword executive summary introduction: making the case for <phrase>mobile</phrase> learning the <phrase>current state</phrase> of <phrase>mobile</phrase> learning key opportunities in <phrase>mobile</phrase> learning Encourage " anywhere, anytime " learning Reach underserved children Improve <phrase>21st-century</phrase> social interactions Fit with <phrase>learning environments</phrase> Enable a personalized learning experience key challenges in <phrase>mobile</phrase> learning Negative aspects of <phrase>mobile</phrase> learning <phrase>Cultural</phrase> norms and attitudes No <phrase>mobile</phrase> theory of learning Differentiated access and <phrase>technology</phrase> Limiting physical attributes relevant market trends and innovations Extreme convergence Location, location, location Consolidation at last The <phrase>21st-century</phrase> button goals for <phrase>mobile</phrase> learning Learn: Understand <phrase>mobile</phrase> learning as a unique element of <phrase>education reform</phrase> Develop: Build <phrase>mobile</phrase> learning interventions Promote: Engage the <phrase>public</phrase> and policy-makers in defi ning the potential of <phrase>mobile</phrase> devices for learning Prepare: <phrase>Train</phrase> teachers and learners to effectively incorporate <phrase>mobile</phrase> technologies Stimulate: Generate new <phrase>leadership</phrase> support for <phrase>digital</phrase> learning conclusion: pockets of potential appendix a: <phrase>mobile</phrase> learning examples and <phrase>research</phrase> projects (<phrase>u.s</phrase>.) appendix b: <phrase>mobile</phrase> learning examples and <phrase>research</phrase> projects (non-<phrase>u.s</phrase>.)
Sense Making Alone Doesn't Do It: Fluency Matters Too! ITS Support for Robust Learning with <phrase>Multiple Representations</phrase> Previous <phrase>research</phrase> demonstrates that <phrase>multiple representations</phrase> of learning content can enhance students' learning, but also that <phrase>students learn</phrase> deeply from <phrase>multiple representations</phrase> only if the <phrase>learning environment</phrase> supports them in making connections between the representations. We hypothesized that connection-making support is most effective if it helps students make both in making sense of the content across representations and in becoming fluent in making connections. We tested this <phrase>hypothesis</phrase> in a classroom experiment with 599 4 th-and 5 th-grade students using an ITS for fractions. The experiment further contrasted two forms of support for sense making: auto-linked representations and the use of worked examples involving one representation to guide work with another. <phrase>Results</phrase> confirm our main <phrase>hypothesis</phrase>: A combination of worked examples and fluency support <phrase>lead</phrase> to more robust learning than versions of the ITS without connection-making support. Therefore, combining different types of connection-making support is crucial in promoting students' <phrase>deep learning</phrase> from <phrase>multiple representations</phrase>.
Lexical functions in learning the <phrase>lexicon</phrase> The <phrase>paper</phrase> describes a method of <phrase>computer-aided</phrase> learning of <phrase>idioms</phrase> and collocations. It lists some <phrase>basic</phrase> concepts of Lexical <phrase>function</phrase> theory and describes the <phrase>architecture</phrase> and functionalities of the <phrase>software</phrase> system developed. 1. <phrase>General</phrase> concepts Good command of <phrase>idioms</phrase> and collocations is an important ability of any <phrase>natural language</phrase> <phrase>speaker</phrase>. This ability needs to be developed not only in foreign <phrase>language</phrase> learners but also in people wishing to enhance their <phrase>linguistic competence</phrase> in the <phrase>native</phrase> <phrase>language</phrase>. Since systematic description of idiomaticity is still a very difficult issue and <phrase>linguistic</phrase> resources focused on <phrase>idioms</phrase> and collocations are scarce, tools intended for boosting up this ability are poorly represented in modern <phrase>language</phrase> learning techniques. We developed a <phrase>software</phrase> system which can be used to improve <phrase>human</phrase> <phrase>knowledge</phrase> of the <phrase>combinatorial</phrase> potential of words. In the 1990s, an INTAS-funded project, CALLEX [1], used a combination of <phrase>linguistic</phrase> <phrase>knowledge</phrase> and innovative <phrase>linguistic</phrase> technologies for teaching the <phrase>idiomatic</phrase> <phrase>aspect</phrase> of the <phrase>lexicon</phrase>. It was largely based on two concepts of the MeaningText theory: lexical functions (LF) proposed by I. A. Mel'uk [2] and the theory of lexical decomposition proposed by Ju. D. Apresjan [3,4]. Since then certain aspects of LF theory have progressed, and the material of <phrase>Russian</phrase> and <phrase>English</phrase> dictionaries has been drastically updated. We took over all of the <phrase>general</phrase> ideas of the CALLEX project; also, we use the newest version of the <phrase>dictionary</phrase> developed by its authors as basis for our work. We will now take a closer look at the <phrase>basic</phrase> <phrase>linguistic</phrase> concepts mentioned above. An <phrase>elementary</phrase> lexical <phrase>function</phrase> (LF) is a relation between one word or word combination, which is called <phrase>function</phrase> argument, and another word or word combination, which is called <phrase>function</phrase> value corresponding to this argument. Not every lexical <phrase>function</phrase> value can be predicted with a 100% accuracy by its argument because it is not completely motivated semantically. In a <phrase>general</phrase> case all lexical functions have multiple values. Example: Magn-a large <phrase>degree</phrase> or a <phrase>high</phrase> intensity of X. Magn (CONTRAST (<phrase>NOUN</phrase>)) = sharp / harsh / startling / striking, Magn (IMAGINATION) = lively / warm / heated, Magn (SILENCE) = dead / complete / deep / profound / deadly / mortal. By now the <phrase>dictionary</phrase> of the system contains a description of nearly 120 LFs. According to I. A. Mel'uk's <phrase>hypothesis</phrase>, there are nearly 55 <phrase>elementary</phrase> lexical functions, which may additionally form
Training Deep <phrase>Fourier</phrase> <phrase>Neural Networks</phrase> to Fit <phrase>Time-Series</phrase> <phrase>Data</phrase> We present a method for training a <phrase>deep neural network</phrase> containing <phrase>sinusoidal</phrase> <phrase>activation functions</phrase> to fit to <phrase>time-series</phrase> <phrase>data</phrase>. Weights are initialized using a <phrase>fast Fourier transform</phrase>, then trained with regular-ization to improve generalization. A simple dynamic parameter tuning method is employed to adjust both the learning rate and regularization term, such that stability and efficient training are both achieved. We show how deeper layers can be utilized to <phrase>model</phrase> the observed <phrase>sequence</phrase> using a sparser set of <phrase>sinusoid</phrase> units, and how non-uniform regularization can improve generalization by promoting the shifting of weight toward simpler units. The method is demonstrated with <phrase>time-series</phrase> problems to show that it leads to effective <phrase>extrapolation</phrase> of nonlinear trends.
Deep Edge-Aware Filters There are many edge-aware filters varying in their <phrase>construction</phrase> forms and filtering properties. It seems impossible to uniformly represent and accelerate them in a <phrase>single</phrase> framework. We made the attempt to learn a big and important <phrase>family</phrase> of edge-aware operators from <phrase>data</phrase>. Our method is based on a <phrase>deep convolutional</phrase> <phrase>neural network</phrase> with a <phrase>gradient</phrase> domain training procedure, which gives rise to a powerful tool to approximate various filters without knowing the original models and implementation details. The only difference among these operators in our system becomes merely the learned parameters. Our system enables fast approximation for complex edge-aware filters and achieves up to 200x <phrase>acceleration</phrase>, regardless of their originally very different implementation. Fast speed can also be achieved when creating new effects using spatially varying filter or filter combination, bearing out the effectiveness of our deep edge-aware filters.
<phrase>Grand Challenges</phrase> in <phrase>AI</phrase> Several exciting problems in <phrase>AI</phrase> are seemingly reasonable and yet currently unsolviible. Solutions to these problems will require <phrase>major</phrase> new insights and fundamental advances in <phrase>computer science</phrase> and <phrase>artificial intelligence</phrase>. Such problems include a world-champion <phrase>chess</phrase> machine, the discovery of a <phrase>major</phrase> <phrase>mathematical</phrase> 1 result by a computer, a translating <phrase>telephone</phrase>, and so on. I present here several such <phrase>grand challenges</phrase> that, when met, can be expected to have <phrase>major</phrase> impact on <phrase>AI</phrase>, <phrase>computer science</phrase>, and <phrase>society</phrase>. The scope and size of these problems vary greatly but are on the scale of inter-disciplinary <phrase>grand challenges</phrase> such as decoding the <phrase>human genome</phrase>, give or take an <phrase>order</phrase> of <phrase>magnitude</phrase>. Technical and Conceptual <phrase>Grand challenges</phrase> The technical <phrase>grand challenges</phrase> presented here are primarily conceptual or curiosity-driven and hold out no promise of immediate return on <phrase>investment</phrase>. But the <phrase>deeper understanding</phrase> of the <phrase>nature</phrase> of reasoning, <phrase>problem solving</phrase>, and intelligent behavior resulting from the <phrase>solution</phrase> of these problems is bound to <phrase>lead</phrase> to significant advances in <phrase>technology</phrase>. World-champion <phrase>chess</phrase> machine: SyStems such as Deep Thought already <phrase>play</phrase> at the grand <phrase>master</phrase> level. Hans Berliner says that computer <phrase>chess</phrase> ratings have been increasing steadily over the past 20 years by about 45 points per year. At that rate, we should have a <phrase>chess</phrase> champion computer by about 1998, almost 40 years after Simon's predictions. We didn't quite do it in 10 years. But in the cosmic scale of <phrase>human evolution</phrase>, 40 or even a 100 years is but a fleeting moment! <phrase>Mathematical</phrase> discovery: The second <phrase>major</phrase> challenge for .<phrase>AI</phrase> is the discovery of a <phrase>major</phrase> <phrase>mathematical</phrase> result by a computer. The criteria for success in this case are not as crisp as is the case with <phrase>chess</phrase>. It has been difficult to establish the criteria for what constitutes a <phrase>major</phrase> <phrase>mathematical</phrase> result. For a while, we thought a successor to Doug Lenat's AM program would claim this prize. Some wise men in <phrase>AI</phrase> believe that we will know it when we see it. Self-organizing systems: There has been <phrase>long</phrase> and continuous interest in systems that learn and discover from examples, observations, and books. And there is a lot of current interest in <phrase>neural networks</phrase> that can learn from signals and symbols through an <phrase>evolutionary</phrase> process. Two <phrase>long</phrase>-term <phrase>grand challenges</phrase> for systems that acquire capability through development are to read a chapter in a <phrase>college</phrase> <phrase>freshman</phrase> text (say in <phrase>physics</phrase> or <phrase>accounting</phrase>) and answer the 
Beyond <phrase>curriculum</phrase>: the exploring <phrase>computer science</phrase> program In the past few decades, <phrase>computer science</phrase> has driven <phrase>innovation</phrase> across a <phrase>variety</phrase> of <phrase>academic</phrase> fields and become a robust part of <phrase>democratic</phrase> participation and the <phrase>labor</phrase> <phrase>economy</phrase>. Today's youth are surrounded with applications of these new technologies that impact how they access and produce <phrase>information</phrase> and communicate with <phrase>friends</phrase>, <phrase>family</phrase>, and educators. Yet, though students often gain skills as " users " of these technologies in schools, too many have been denied opportunities to study <phrase>computer science</phrase> and produce new <phrase>knowledge</phrase> required to become " creators " of <phrase>computing</phrase> innovations. The students who do study <phrase>computer science</phrase> courses often represent only a narrow <phrase>band</phrase> of students that excludes significant numbers of girls and students of color. Further, for a field that depends on <phrase>creativity</phrase>, a homogenous workforce fails to take advantage of those with diverse experiences and world viewpoints that likely foster divergent and fresh thinking. This article will provide an overview of Exploring <phrase>Computer Science</phrase> (ECS), a <phrase>curriculum</phrase> and program developed to broaden participation in <phrase>computing</phrase> for <phrase>high school</phrase> students in the <phrase>Los Angeles Unified School District</phrase>. This program is framed around a three-pronged approach to reform: curricular development, <phrase>teacher</phrase> <phrase>professional</phrase> development, and policy work across a <phrase>variety</phrase> of educational institutions. The focus is to provide the necessary structures and support to schools and teachers that leads to <phrase>high</phrase> quality <phrase>teaching and learning</phrase> in <phrase>computer science</phrase> classrooms. In ECS classrooms, <phrase>high</phrase> quality <phrase>teaching and learning</phrase> is viewed within the frame of inquiry-based teaching strategies that <phrase>lead</phrase> to deep <phrase>student</phrase> content learning and engagement. The incorporation of equity-based teaching practices is an essential part of setting up the classroom <phrase>culture</phrase> that facilitates <phrase>inquiry- based learning</phrase>. As the second largest and one of the most diverse <phrase>districts</phrase> in the <phrase>United States</phrase>, the <phrase>Los Angeles Unified School District</phrase> provides an important context to understand opportunities and obstacles encountered while engaging in institutional K-12 <phrase>computer science</phrase> <phrase>education reform</phrase>. This article will begin with an account of the educational <phrase>research</phrase> that provided key <phrase>information</phrase> about the obstacles students encounter in <phrase>computer science</phrase> classrooms. Next, we will describe the key elements of the ECS program. Finally, we will highlight several lessons that we have learned that inform the CS 10K campaign (see Jan Cuny's Critical Perspective " Transforming <phrase>High School</phrase> <phrase>Computing</phrase>: A Call to <phrase>Action</phrase> " , this issue).
Integrating <phrase>perceptual</phrase> representation learning and skill learning in a simulated <phrase>student</phrase> One of the fundamental goals of <phrase>artificial intelligence</phrase> is to understand and develop <phrase>intelligent agents</phrase> that simulate <phrase>human</phrase>-level <phrase>intelligence</phrase>. This fundamental goal complements another essential goal in <phrase>education</phrase>, improving understanding of how humans acquire <phrase>knowledge</phrase> and how students may vary in their abilities to learn. Contributing to both goals, a lot of efforts have been made to develop <phrase>intelligent agents</phrase> that simulate <phrase>human</phrase> learning of <phrase>math</phrase> and <phrase>science</phrase>. However, constructing such a learning agent currently requires manual encoding of prior <phrase>domain knowledge</phrase>, which is both inefficient and less cognitively plausible. Previous <phrase>cognitive science</phrase> <phrase>research</phrase> has shown that one of the key factors that differentiates experts and novices is their different representations of <phrase>knowledge</phrase>. Moreover, for many existing <phrase>learning algorithms</phrase>, " better " representations often <phrase>lead</phrase> to more effective learning. We [1] <phrase>recently proposed</phrase> an efficient <phrase>algorithm</phrase> that acquires representation <phrase>knowledge</phrase> in the form of " deep features ". In this <phrase>paper</phrase>, we integrate this <phrase>algorithm</phrase> into a simulated <phrase>student</phrase>, SimStudent, which learns <phrase>procedural knowledge</phrase> from example solutions and <phrase>problem solving</phrase> experience. We show that with the integration, <phrase>prior knowledge</phrase> <phrase>engineering</phrase> effort is reduced, learning performance is as good or better, and SimStudent becomes a more plausible <phrase>simulation</phrase> of <phrase>human</phrase> learning. I. INTRODUCTION One of the fundamental goals of <phrase>artificial intelligence</phrase> is to understand and develop <phrase>intelligent agents</phrase> that simulate <phrase>human</phrase>-like <phrase>intelligence</phrase>. A large amount of effort (e.g., [2], [3]) has been put toward this <phrase>challenging task</phrase>. Further, <phrase>education</phrase> in the <phrase>21 st century</phrase> will be increasingly about helping students not just to learn content but also to become better learners. Thus, we have a second goal of improving our understanding of how humans acquire <phrase>knowledge</phrase> and how students vary in their abilities to learn. To contribute to both goals, considerable efforts (e.g., [4]) have been made to develop <phrase>intelligent agents</phrase> that <phrase>model</phrase> <phrase>human</phrase> learning of <phrase>math</phrase>, <phrase>science</phrase>, or a second <phrase>language</phrase>. Although such agents produce intelligent behavior with less <phrase>human</phrase> <phrase>knowledge engineering</phrase> than before, there remains a non-trivial element of <phrase>knowledge engineering</phrase> in the encoding of the prior <phrase>domain knowledge</phrase> (e.g., <phrase>programming</phrase> how to extract a coefficient from a term). This requirement increases the difficulty of constructing an <phrase>intelligent agent</phrase>. It also reduces the <phrase>cognitive</phrase> plausibility of the constructed agent, as <phrase>human</phrase> students entering a course do not necessarily have substantial <phrase>domain-specific</phrase> or domain-relevant <phrase>prior knowledge</phrase>. An <phrase>intelligent agent</phrase> that requires only <phrase>domain-independent</phrase> <phrase>prior knowledge</phrase> as given would be a 
Increasing the Effectiveness of This guide is for superintendents, principals, staff developers, teachers and <phrase>professional</phrase> development providers-in <phrase>short</phrase> for anyone who has a stake in ensuring that <phrase>professional</phrase> learning has greater impact on improving practice and increasing <phrase>student</phrase> learning. The purpose of this guide is to help schools, <phrase>districts</phrase>, and SU/LEAs put in place processes and supports needed to make <phrase>professional</phrase> development more effective to meet key <phrase>student</phrase> learning and <phrase>school</phrase> improvement goals. The guide defines effective <phrase>professional</phrase> development and provides tools to self-assess current <phrase>professional</phrase> development programs and processes and plan, implement, and evaluate <phrase>professional</phrase> learning. Work sheets are available in MSWord in a separate document. Effective <phrase>professional</phrase> development depends upon the quality of planning, implementation and evaluation. Within this guide, planning, implementation and evaluation are presented in steps with <phrase>research</phrase>-based <phrase>information</phrase> and tools and worksheets that can be used to develop a <phrase>professional</phrase> development plan, ensure implementation, and evaluate <phrase>results</phrase>. The <phrase>information</phrase> and processes in this guide are based upon <phrase>current research</phrase> about <phrase>educator</phrase> learning, change, and organizational development as well as the National Staff Development Council/Learning <phrase>Forward</phrase> definition and standards for staff development which are in the Appendices. To be user <phrase>friendly</phrase>, <phrase>information</phrase> is condensed. The last section includes excellent references used in creating this guide that will provide much greater detail and depth. In this guide, the term " <phrase>professional</phrase> development " does not refer to an event or events, but rather to ongoing, sustained, <phrase>inquiry-based learning</phrase> that occurs at least weekly throughout the <phrase>school</phrase> year. This type of <phrase>professional</phrase> learning provides focused, <phrase>deep learning</phrase> that builds upon prior learning resulting in positive changes in practice and increased <phrase>student</phrase> learning. <phrase>Research</phrase> shows that effective leaders and teachers increase <phrase>student</phrase> success and that the primary contributor to making educators effective is <phrase>high</phrase>-quality, continuous <phrase>professional</phrase> learning. <phrase>High</phrase>-quality <phrase>professional</phrase> learning is different than <phrase>professional</phrase> development many educators have grown accustomed to. <phrase>Research</phrase> has identified key elements required for <phrase>professional</phrase> learning that will change <phrase>professional</phrase> practice and that can result in increased <phrase>student</phrase> learning. National Staff Development Council/Learning <phrase>Forward</phrase> has divided those elements into Context, Process, and Content. Often, <phrase>professional</phrase> development content is given most consideration but without a context that supports <phrase>professional</phrase> learning and an appropriate <phrase>research</phrase>-based learning process, there will be little change in <phrase>educator</phrase> practice. The National Staff Development Council/Learning <phrase>Forward</phrase> Standards for Staff Development are in the Appendices of this guide. Detailed explanations of each standard and the definition are 
Multimodal learning with <phrase>deep Boltzmann machines</phrase> A Deep <phrase>Boltzmann</phrase> Machine is described for learning a <phrase>generative model</phrase> of <phrase>data</phrase> that consists of multiple and diverse input modalities. The <phrase>model</phrase> can be used to extract a unified representation that fuses modalities together. We find that this representation is useful for classification and <phrase>information retrieval</phrase> tasks. The <phrase>model</phrase> works by learning a <phrase>probability density</phrase> over the space of multimodal inputs. It uses states of <phrase>latent variables</phrase> as representations of the input. The <phrase>model</phrase> can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and filling them in. Our <phrase>experimental</phrase> <phrase>results</phrase> on bi-modal <phrase>data</phrase> consisting of images and text show that the Multimodal <phrase>DBM</phrase> can learn a good <phrase>generative model</phrase> of the joint space of image and text inputs that is useful for <phrase>information retrieval</phrase> from both unimodal and multimodal queries. We further demonstrate that this <phrase>model</phrase> <phrase>significantly outperforms</phrase> SVMs and LDA on discriminative tasks. Finally, we compare our <phrase>model</phrase> to other <phrase>deep learning</phrase> methods, including autoencoders and <phrase>deep belief</phrase> networks, and show that it achieves noticeable gains.
Memristive <phrase>Boltzmann</phrase> machine: A hardware <phrase>accelerator</phrase> for <phrase>combinatorial optimization</phrase> and <phrase>deep learning</phrase> The <phrase>Boltzmann</phrase> machine is a massively parallel computational <phrase>model</phrase> capable of solving a broad class of combinato-rial <phrase>optimization problems</phrase>. In recent years, it has been successfully applied to training deep <phrase>machine learning</phrase> models on massive datasets. <phrase>High</phrase> performance implementations of the <phrase>Boltzmann</phrase> machine using <phrase>GPUs</phrase>, MPI-based HPC clusters , and <phrase>FPGAs</phrase> have been proposed in the <phrase>literature</phrase>. Regrettably , the required all-to-all <phrase>communication</phrase> among the processing units limits the performance of these efforts. This <phrase>paper</phrase> examines a new class of hardware <phrase>accelerators</phrase> for <phrase>large-scale</phrase> <phrase>combinatorial optimization</phrase> and <phrase>deep learning</phrase> based on memristive <phrase>Boltzmann</phrase> machines. A massively parallel, <phrase>memory</phrase>-centric hardware <phrase>accelerator</phrase> is proposed based on recently developed resistive <phrase>RAM</phrase> (RRAM) <phrase>technology</phrase>. The proposed <phrase>accelerator</phrase> exploits the electrical properties of RRAM to realize in situ, <phrase>fine-grained</phrase> parallel computation within <phrase>memory</phrase> arrays, thereby eliminating the need for exchanging <phrase>data</phrase> between the <phrase>memory</phrase> cells and the computational units. Two <phrase>classical</phrase> <phrase>optimization problems</phrase>, <phrase>graph</phrase> partitioning and <phrase>boolean</phrase> <phrase>satisfiability</phrase>, and a <phrase>deep belief</phrase> network application are mapped onto the proposed hardware. As compared to a multicore system, the proposed <phrase>accelerator</phrase> achieves 57 higher performance and 25 <phrase>lower</phrase> <phrase>energy</phrase> with virtually no loss in the quality of the <phrase>solution</phrase> to the <phrase>optimization problems</phrase>. The memristive <phrase>accelerator</phrase> is also compared against an RRAM based processing-in-<phrase>memory</phrase> (<phrase>PIM</phrase>) system, with respective performance and <phrase>energy</phrase> improvements of 6.89 and 5.2.
<phrase>Semi-supervised</phrase> <phrase>Natural Language</phrase> Acquisition Acknowledgments First and foremost, I would like to <phrase>express my deep</phrase> gratitude to my advisor, <phrase>Professor</phrase> Ari Rappoport. When I approached Ari I had no experience in <phrase>research</phrase> and no previous <phrase>knowledge</phrase> of <phrase>natural language processing</phrase> (<phrase>NLP</phrase>). Ari exposed me to the <phrase>NLP</phrase> world on its various aspects but, more importantly , he taught me how to define and prioritize my <phrase>research</phrase> goals from a broad scientific perspective, how to approach a scientific problem, how to build a sophisticated <phrase>model</phrase> that does not involve superfluous technicalities which hide the core phenomenon and how to look on <phrase>data</phrase> and analyze it. Perhaps most importantly, Ari taught me how to write a scientific article and by doing that he probably taught me most of the skills that make one a good <phrase>scientist</phrase>. Besides being a great <phrase>scientist</phrase> and <phrase>teacher</phrase>, Ari is a <phrase>human</phrase> being who deeply cares about the people surrounding him and the <phrase>society</phrase> which he lives in. I have learned a lot from him in this <phrase>aspect</phrase> as well. He constantly gave me the feeling that I have who to rely on and I am grateful for that. <phrase>Omri</phrase> Abend was, and will hopefully remain, my main collaborator. Perhaps the only thing that exceeds Omri'ss brightness is his modesty. In addition to the diverse scientific work we jointly conducted, I was able to consult him in every dilemma I faced while doing my own <phrase>research</phrase>. <phrase>Omri</phrase> significantly contributed to my <phrase>research</phrase> and has also been a close friend to me. I could not have asked for a better partner. During my <phrase>PhD</phrase> years I have worked withlamit Umansky-Pesin. I had enlightening conversations wih <phrase>Yoav</phrase> Seginer, who authored one of the most important <phrase>NLP</phrase> works of the last years, in my opinion. Oren Tsur is a great friend and colleague, I hope we will have the opportunity to write together in the future. Many people have made my <phrase>PhD</phrase> years a wonderful <phrase>period</phrase> which I already miss. To avoid exhaustive lists I will just mention one. Yeal Shor and I spent our days in the same room for about two <phrase>years years</phrase> that were devoted to building the scientific <phrase>infrastructure</phrase> and in which I faced a lot of rejections. I could not have asked for a better partner to spend those years with. It was Eilon Vaadia, the head of the ICNC <phrase>PhD</phrase> program at the time, who insisted that 
Vulnerability of <phrase>Machine Learning</phrase> Models to Adversarial Examples We propose a <phrase>genetic algorithm</phrase> for generating adversarial examples for <phrase>machine learning</phrase> models. Such approach is able to find adversarial examples without the access to model's parameters. Different models are tested, including both deep and shallow <phrase>neural networks</phrase> archi-tectures. We show that RBF networks and SVMs with Gaussian kernels tend to be rather robust and not prone to misclassification of adversarial examples.
Hierarchical Fault Response Modeling of Analog/rf Circuits i ABSTRACT In this <phrase>thesis</phrase> two methodologies have been proposed for evaluating the fault response of analog/RF circuits. These proposed approaches are used to evaluate the response of the faulty circuit in terms of specifications/measurements. Faulty response can be used to evaluate important <phrase>test</phrase> metrics like fail <phrase>probability</phrase>, fault coverage and yield coverage of given measurements under process variations. Once the models for faulty and fault <phrase>free</phrase> circuit are generated, one needs to perform <phrase>Monte Carlo</phrase> sampling (as opposed to <phrase>Monte Carlo</phrase> simulations) to compute these statistical parameters with <phrase>high</phrase> accuracy. The first method is based on adaptively determining the <phrase>order</phrase> of the <phrase>model</phrase> based on the error budget in terms of <phrase>computing</phrase> the statistical metrics and position of the threshold(s) to decide how precisely necessary models need to be extracted. In the second method, using hierarchy in process variations a <phrase>hybrid</phrase> of heuristics and localized linear models have been proposed. Experiments on LNA and Mixer using the adaptive <phrase>model</phrase> <phrase>order</phrase> selection procedure can reduce the number of necessary simulations by 7.54x and 7.03x respectively in the computation of fail <phrase>probability</phrase> for an error budget of 2%. Experiments on LNA using the <phrase>hybrid</phrase> approach can reduce the number of necessary simulations by 21.9x and 17x for four and six output parameters cases for improved accuracy in <phrase>test</phrase> <phrase>statistics</phrase> estimation. <phrase>ii</phrase> ACKNOWLEDGMENTS Although this <phrase>thesis</phrase> <phrase>book</phrase> lists only one <phrase>author</phrase>, in <phrase>reality</phrase> the ideas it molds together were contributed and refined by many extraordinarily insightful colleagues. My first thanks go to the almighty for directing me to Prof. Sule Ozev to conduct this <phrase>masters</phrase> <phrase>research</phrase>. On most days I leave her office after the meeting wondering if I have really got the benefit of my 18 years of <phrase>science</phrase> <phrase>education</phrase>. My interactions with her have proved time and again that learning is a continuous process and this gives me great deal of enthusiasm to delve deep into new and obscure topics. I thank her for all the guidance during this enduring <phrase>period</phrase> of <phrase>research</phrase>. I would also like to thank Christen for promptly agreeing to serve on the defense committee. I am indebted to my parents who have for all the <phrase>motivation</phrase>, emotional and financial support much needed during my educational career. I am thankful to Ender Yilmaz, Afsaneh Nassery, Osman Erol and other members of Prof. Ozev's <phrase>research</phrase> group for all the fruitful discussions not only restricted to <phrase>research</phrase>, but 
Sparsifying <phrase>Neural Network</phrase> Connections for <phrase>Face Recognition</phrase> This <phrase>paper</phrase> proposes to learn <phrase>high</phrase>-performance deep ConvNets with sparse neural connections, referred to as sparse ConvNets, for <phrase>face recognition</phrase>. The sparse Con-vNets are learned in an iterative way, each time one additional layer is sparsified and the entire <phrase>model</phrase> is retrained given the initial weights learned in previous iterations. One important finding is that directly training the sparse ConvNet from scratch failed to find good solutions for <phrase>face recognition</phrase>, while using a previously learned denser <phrase>model</phrase> to properly initialize a sparser <phrase>model</phrase> is critical to continue learning effective features for <phrase>face recognition</phrase>. This <phrase>paper</phrase> also proposes a new neural correlation-based weight selection criterion and empirically verifies its effectiveness in selecting informative connections from previously learned models in each iteration. When taking a moderately sparse structure (26%-76% of weights in the dense <phrase>model</phrase>), the proposed sparse ConvNet <phrase>model</phrase> significantly improves the <phrase>face recognition</phrase> performance of the previous <phrase>state</phrase>-of-the-<phrase>art</phrase> DeepID2+ models given the same <phrase>training data</phrase>, while it keeps the performance of the baseline <phrase>model</phrase> with only 12% of the original parameters.
<phrase>DISCO</phrase> Nets: DISsimilarity COefficient Networks We present a new type of <phrase>probabilistic model</phrase> which we call DISsimilarity COefficient Networks (<phrase>DISCO</phrase> Nets). <phrase>DISCO</phrase> Nets allow us to efficiently sample from a posterior distribution parametrised by a <phrase>neural network</phrase>. During training, <phrase>DISCO</phrase> Nets are learned by minimising the dissimilarity coefficient between the true distribution and the estimated distribution. This allows us to <phrase>tailor</phrase> the training to the loss related to the task at hand. We empirically show that (i) by modeling uncertainty on the output value, <phrase>DISCO</phrase> Nets outperform equivalent non-probabilistic predictive networks and (<phrase>ii</phrase>) <phrase>DISCO</phrase> Nets accurately <phrase>model</phrase> the uncertainty of the output, outperforming existing <phrase>probabilistic models</phrase> based on <phrase>deep neural networks</phrase>.
Trust and Inventory Replenishment Decision Under Continuous Review System Dedication To all those who remind me the worth of <phrase>peace</phrase> and unconditional <phrase>love</phrase> ! <phrase>ii</phrase> ! iii ! ! ! Acknowledgments owe gratitude to many people who contributed to completion of this <phrase>research</phrase> and to my <phrase>personal development</phrase>. Thinking systematically, solving the problem creatively, writing coherently, and <phrase>editing</phrase> patiently were not possible without experts' guidance. I would like to thank Rmy for his endless <phrase>patience</phrase> and wisdom, and Naoufel for his remarkable encouragement. I am deeply grateful for their support, guidance, and friendship. I would like to thank <phrase>Professor</phrase> Max-Olivier Hongler and <phrase>Professor</phrase> Karen Donohue, whose early guidance and generous comments helped me to better shape my <phrase>research</phrase> project. <phrase>Professor</phrase> Andrew <phrase>King</phrase> taught me a lot about <phrase>data analysis</phrase> and provided expert guidance in how to <phrase>design</phrase> the <phrase>pilot</phrase> study. Chapter 3 is highly inspired by valuable lectures and expansive conversations of <phrase>Professor</phrase> Christopher Tucci. It was a privilege to work with him. I also had a unique opportunity to share the <phrase>research</phrase> <phrase>results</phrase> of Chapter 4 with <phrase>Professor</phrase> Rachel Croson during the 7th annual behavioral operations conference in <phrase>Washington</phrase>, <phrase>DC</phrase>. She gave remarkable <phrase>feedback</phrase> and sage advice. I am grateful for her contributions. I would like to sincerely thank Dr. Mehdi Gholam-rezaee for offering his talent and expertise in <phrase>statistics</phrase>. He generously took time away from his busy schedule and helped me to learn <phrase>statistical analysis</phrase> in a practical way. Thanks <phrase>Professor</phrase> Carlos Cordon and <phrase>Professor</phrase> Kamran Kashani of <phrase>IMD</phrase>, and <phrase>Professor</phrase> Farhad Rachidi-Haeri of <phrase>EPFL</phrase> not only for their friendship, but also for their helpful career advice and suggestions in <phrase>general</phrase>. I am also grateful to <phrase>Professor</phrase> Paulo Gonalves for accepting to join my <phrase>PhD</phrase> committee. I ! iv and Edith Brotherton for providing XBeerGame platform, Marc Matthy and Ioanna Paniara for their invaluable help in running the experiments, Ruth Fiaux for her unforgettable help in the early days at <phrase>EPFL</phrase>, and LGPP colleagues for all supports that I have received over these years. The <phrase>Swiss National Science Foundation</phrase> provided institutional support for the first part of this <phrase>research</phrase>. The influence of those whom I worked before joining to <phrase>EPFL</phrase> also continues to be important. <phrase>Professor</phrase> Mojtaba <phrase>Tabari</phrase> with his deep caring for people was and remains my best role <phrase>model</phrase>. <phrase>Professor</phrase> Mirbahador Aryanezhad with his profound <phrase>knowledge</phrase> and his kind <phrase>heart</phrase>. Though, he is no longer of this world, his fond memories cannot <phrase>pass</phrase> away. <phrase>Professor</phrase> 
Deep Multi-scale <phrase>Convolutional Neural Network</phrase> for Dynamic Scene Deblurring Non-uniform blind deblurring for <phrase>general</phrase> dynamic scenes is a challenging <phrase>computer vision</phrase> problem since blurs are caused by <phrase>camera</phrase> shake, scene depth as well as multiple object motions. To remove these complicated motion blurs, conventional <phrase>energy</phrase> optimization based methods rely on simple assumptions such that <phrase>blur</phrase> kernel is partially uniform or locally linear. Moreover, recent <phrase>machine learning</phrase> based methods also depend on synthetic <phrase>blur</phrase> datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where <phrase>blur</phrase> kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale <phrase>convolutional neural network</phrase> that restores blurred images caused by various sources in an <phrase>end-to-end</phrase> manner. Furthermore , we present multi-scale <phrase>loss function</phrase> that mimics conventional coarse-to-fine approaches. Moreover, we propose a new <phrase>large scale</phrase> dataset that provides pairs of realistic blurry image and the corresponding <phrase>ground truth</phrase> sharp image that are obtained by a <phrase>high</phrase>-speed <phrase>camera</phrase>. With the proposed <phrase>model</phrase> trained on this dataset, we demonstrate empirically that our <phrase>method achieves</phrase> the <phrase>state</phrase>-of-the-<phrase>art</phrase> performance in dynamic scene deblurring not only qualitatively , but also quantitatively.
Partial Occlusion Handling in Pedestrian Detection With a Deep <phrase>Model</phrase> Part-based models have demonstrated their merit in <phrase>object detection</phrase>. However, there is a key issue to be solved on how to integrate the inaccurate scores of part detectors when there are occlusions, abnormal deformations, appearances or illuminations. To handle the imperfection of part detectors, this <phrase>paper</phrase> presents a probabilistic pedestrian detection framework. In this framework, a deformable part-based <phrase>model</phrase> is used to obtain the scores of part detectors and the visibilities of parts are modeled as hidden variables. Once the occluded parts are identified, their effects are properly removed from the final detection score. Unlike previous occlusion handling approaches that assumed <phrase>independence</phrase> among the visibility <phrase>probabilities</phrase> of parts or manually defined rules for the visibility relationship, a deep <phrase>model</phrase> is proposed in this <phrase>paper</phrase> for learning the visibility relationship among overlapping parts at <phrase>multiple layers</phrase>. The <phrase>proposed approach</phrase> can be viewed as a <phrase>general</phrase> post-processing of part-detection <phrase>results</phrase> and can take detection scores of existing part-based models as input. <phrase>Experimental</phrase> <phrase>results</phrase> on three <phrase>public</phrase> datasets (<phrase>Caltech</phrase>, <phrase>ETH</phrase> and <phrase>Daimler</phrase>) and a new CUHK occlusion dataset 1 , which is specially designed for the evaluation of occlusion handling approaches, show the effectiveness of the <phrase>proposed approach</phrase>.
SubFlow: Towards practical flow-level <phrase>traffic classification</phrase> Many <phrase>research</phrase> efforts propose the use of flow-<phrase>level features</phrase> (e.g., packet sizes and inter-arrival times) and <phrase>machine learning</phrase> <phrase>algorithms</phrase> to solve the <phrase>traffic classification</phrase> problem. However, these <phrase>statistical methods</phrase> have not made the anticipated impact in the <phrase>real world</phrase>. We attribute this to two main reasons: (a) training the classifiers and <phrase>bootstrapping</phrase> the system is cumbersome, (b) the resulting classifiers have limited ability to adapt gracefully as the traffic behavior changes. In this <phrase>paper</phrase>, we propose an approach that is easy to bootstrap and deploy, as well as robust to changes in the traffic, such as the emergence of new applications. The key novelty of our classifier is that it learns to identify the traffic of each application in isolation, instead of trying to distinguish one application from another. This is a very <phrase>challenging task</phrase> that hides many caveats and subtleties. To make this possible, we adapt and use subspace clustering, a powerful technique that has not been used before in this context. Subspace clustering allows the profiling of applications to be more precise by automatically eliminating irrelevant features. We show that our approach exhibits very <phrase>high</phrase> accuracy in classifying each application on five traces from different <phrase>ISPs</phrase> captured between 2005 and 2011. This new way of looking at application classification could generate powerful and practical solutions in the space of traffic monitoring and <phrase>network management</phrase>. I. INTRODUCTION Identifying the flows generated by different applications is of <phrase>major</phrase> interest for network operators. For <phrase>Internet Service Providers</phrase> (<phrase>ISPs</phrase>), identifying traffic allows them to differentiate the <phrase>QoS</phrase> for different types of applications, such as voice and <phrase>video</phrase>. Moreover, it enables them to control <phrase>high</phrase>-bandwidth and non-interactive applications, such as <phrase>peer-to-peer</phrase> (<phrase>P2P</phrase>). For enterprise networks, it is very important for administrators to know what happens on their network: what services users are running, which application is dominating their traffic, etc. <phrase>Traffic classification</phrase> is also important for securing the network. In fact, even traditional protocols are often used as means to control attacks, such as the use of <phrase>IRC</phrase> as C&C for botnets. Overall, <phrase>traffic classification</phrase> is the first step in building network <phrase>intelligence</phrase>. In this <phrase>paper</phrase>, our goal is to develop a practical <phrase>traffic classification</phrase> approach, which should be: (a) easy to use, and (b) effective, in terms of accuracy. To the best of our <phrase>knowledge</phrase>, no such <phrase>solution</phrase> exists today. First, most deployed solutions rely heavily on payload-based or <phrase>deep packet inspection</phrase> 
Selecting queries from sample to crawl <phrase>deep web</phrase> <phrase>data</phrase> sources This <phrase>paper</phrase> studies the problem of selecting queries to efficiently crawl a <phrase>deep web</phrase> <phrase>data</phrase> source using a set of sample documents. Crawling <phrase>deep web</phrase> is the process of collecting <phrase>data</phrase> from search interfaces by issuing queries. One of the <phrase>major</phrase> challenges in crawling <phrase>deep web</phrase> is the selection of the queries so that most of the <phrase>data</phrase> can be retrieved at a <phrase>low cost</phrase>. We propose to learn a set of queries from a sample of the <phrase>data</phrase> source. To verify that the queries selected from a sample also produce a good result for the entire <phrase>data</phrase> source, we carried out a set of experiments on large corpora including Gov2, newsgroups, <phrase>wikipedia</phrase> and <phrase>Reuters</phrase>. We show that our sampling-based method is effective by empirically proving that 1) The queries selected from samples can harvest most of the <phrase>data</phrase> in the original <phrase>database</phrase>; 2) The queries with low overlapping rate in samples will also result in a low overlapping rate in the original <phrase>database</phrase>; and 3) The size of the sample and the size of the terms from where to select the queries do not need to be very large. Compared with other query selection methods, our method obtains the queries by analyzing a small set of sample documents, instead of learning the next best query incrementally from all the documents matched with previous queries.
Imprecise <phrase>Probability</phrase> as a Linking Mechanism between <phrase>Deep Learning</phrase>, Symbolic <phrase>Cognition</phrase> and Local Feature Detection in Vision Processing A novel approach to <phrase>computer vision</phrase> is outlined, involving the use of imprecise <phrase>probabilities</phrase> to connect a <phrase>deep learning</phrase> based hierarchical vision system with both local feature detection based preprocessing and symbolic <phrase>cognition</phrase> based guidance. The core notion is to cause the <phrase>deep learning</phrase> vision system to utilize imprecise rather than <phrase>single</phrase>-point <phrase>probabilities</phrase>, and use local feature detection and symbolic <phrase>cognition</phrase> to affect the confidence associated with particular imprecise <phrase>probabilities</phrase>, thus modulating the amount of credence the <phrase>deep learning</phrase> system places on various observations and guiding its <phrase>pattern recognition</phrase>/formation activity. The potential application to the hybridization of the <phrase>DeSTIN</phrase>, SIFT and OpenCog systems is described in moderate detail. The underlying ideas are even more broadly applicable, to any <phrase>computer vision</phrase> approach with a significant probabilistic component which satisfies certain broad criteria.
Yet Another Deep Embedding of B:Extending de Bruijn Notations We present BiCoq3, a deep embedding of the B system in Coq, focusing on the technical aspects of the development. The main subjects discussed are related to the representation of sets and maps, the use of induction principles, and the introduction of a new de Bruijn notation providing solutions to various problems related to the <phrase>mecha</phrase>-nisation of languages and logics. Embedding a <phrase>language</phrase> or a <phrase>logic</phrase> is now a well-established practice in the <phrase>academic</phrase> <phrase>community</phrase>, to answer various types of concerns, e.g. normalisation of terms and influence of reduction strategies for a <phrase>programming language</phrase> or consistency for a <phrase>logic</phrase>. It indeed supports such meta-theoretical analyses as well as comparing and promoting interesting concepts and features of other languages, or developing mechanically checked tools to deal with a <phrase>language</phrase>. But a lot of difficulties arise that have to be addressed. First of all, an important <phrase>design</phrase> choice has to be made between shallow and deep approaches, consistently with the objectives of the embedding. Justifying the validity of an embedding its correctness and completeness can also be difficult. Finally, a lot of technical details have to be considered e.g. to manage variables. We address these questions through the presentation of BiCoq and BiCoq3, two versions of a deep embedding of the B <phrase>logic</phrase> in the Coq system. The main objective for these embeddings is to evaluate the correctness of the B method itself, in the context of <phrase>security</phrase> developments; other objectives include the development of <phrase>proven</phrase> tools for the B and the derivation of new <phrase>results</phrase> about the B <phrase>logic</phrase>. Yet we focus in this <phrase>paper</phrase> on the technical aspects of these embed-dings, and explain the need for a full redevelopment between the two versions by describing painfully learned lessons. The presentation includes the definition of an extended de Bruijn notation with interesting potentialities to solve some frequently encountered problems related to the mechanisation of languages. This <phrase>paper</phrase> is divided into 6 sections. Sections 1-3 briefly introduce Coq, the notion of embedding and B. Section 4 presents de Bruijn notations. The technical <phrase>aspect</phrase> of the development of BiCoq and BiCoq3 are described in <phrase>Sec</phrase>. 5, considering in particular de Bruijn context <phrase>management</phrase>, induction principles, techniques to implement maps and new <phrase>results</phrase> obtained through an extension
Learning for <phrase>Semantic</phrase> Interpretation: Scaling Up without Dumbing Down Most recent <phrase>research</phrase> in learning approaches to <phrase>natural language</phrase> have studied fairly <phrase>low-level</phrase>" tasks such as <phrase>morphology</phrase>, <phrase>part-of-speech tagging</phrase>, and <phrase>syntactic</phrase> <phrase>parsing</phrase>. However, I believe that logical approaches may h a ve the most relevance and impact at the level of <phrase>semantic</phrase> interpretation, where a logical representation of sentence meaning is important and useful. We h a ve explored the use of <phrase>inductive logic programming</phrase> for learning parsers that map <phrase>natural-language</phrase> <phrase>database</phrase> queries into <phrase>executable</phrase> <phrase>logical form</phrase>. This work goes against the growing trend in <phrase>computational linguistics</phrase> of focusing on shallow but broad-coverage <phrase>natural language</phrase> tasks scaling up by dumbing down" and instead concerns using <phrase>logic</phrase>-based learning to develop narrower, domain-speciic systems that perform relatively deep processing. I rst present a historical view of the shifting emphasis of <phrase>research</phrase> o n v arious tasks in <phrase>natural language processing</phrase> and then brieey review our own work on learning for <phrase>semantic</phrase> interpretation. I will then attempt to encourage others to study such problems and explain why I believe logical approaches have the most to ooer at the level of producing <phrase>semantic</phrase> interpretations of complete sentences.
Practice in Synonym Extraction <phrase>at Large</phrase> Scale Synonym extraction is an important task in <phrase>natural language processing</phrase> and often used as a submodule in query expansion, <phrase>question answering</phrase> and other applications. Automatic synonym extractor is highly preferred for <phrase>large scale</phrase> applications. Previous studies in synonym extraction are most limited to <phrase>small scale</phrase> datasets. In this <phrase>paper</phrase>, we build a large dataset with 3.4 million synonym/non-synonym pairs to capture the challenges in <phrase>real world</phrase> scenarios. We proposed (1) a new cost <phrase>function</phrase> to accommodate the unbalanced learning problem, and (2) a <phrase>feature learning</phrase> based <phrase>deep neural network</phrase> to <phrase>model</phrase> the complicated relationships in synonym pairs. We compare several different approaches based on SVMs and <phrase>neural networks</phrase>, and find out a novel <phrase>feature learning</phrase> based <phrase>neural network</phrase> outperforms the methods with hand-assigned features. Specifically, the best performance of our <phrase>model</phrase> surpasses the <phrase>SVM</phrase> baseline with a significant 97% relative improvement.
Learning <phrase>Speaker</phrase>-Specific Characteristics With a Deep Neural <phrase>Architecture</phrase> Speech signals convey various yet mixed <phrase>information</phrase> ranging from <phrase>linguistic</phrase> to <phrase>speaker</phrase>-specific <phrase>information</phrase>. However, most of <phrase>acoustic</phrase> representations characterize all different kinds of <phrase>information</phrase> as whole, which could hinder either a speech or a <phrase>speaker recognition</phrase> (SR) system from producing a better performance. In this <phrase>paper</phrase>, we propose a novel deep neural <phrase>architecture</phrase> (<phrase>DNA</phrase>) especially for learning <phrase>speaker</phrase>-specific characteristics from mel-<phrase>frequency</phrase> cepstral coefficients, an <phrase>acoustic</phrase> representation commonly used in both <phrase>speech recognition</phrase> and SR, which <phrase>results</phrase> in a <phrase>speaker</phrase>-specific overcomplete representation. In <phrase>order</phrase> to learn intrinsic <phrase>speaker</phrase>-specific characteristics, we come up with an <phrase>objective function</phrase> consisting of contrastive losses in terms of <phrase>speaker</phrase> similarity/dissimilarity and <phrase>data</phrase> <phrase>reconstruction</phrase> losses used as regularization to normalize the interference of non-<phrase>speaker</phrase>-related <phrase>information</phrase>. Moreover, we employ a <phrase>hybrid</phrase> learning strategy for learning parameters of the <phrase>deep neural networks</phrase>: i.e., local yet greedy layerwise unsupervised pretraining for initialization and global <phrase>supervised learning</phrase> for the ultimate discriminative goal. With four <phrase>Linguistic</phrase> <phrase>Data</phrase> <phrase>Consortium</phrase> (LDC) benchmarks and two non-<phrase>English</phrase> corpora, we demonstrate that our overcomplete representation is robust in characterizing various speakers, no <phrase>matter</phrase> whether their utterances have been used in training our <phrase>DNA</phrase>, and highly insensitive to text and languages spoken. Extensive comparative studies suggest that our approach yields favorite <phrase>results</phrase> in <phrase>speaker</phrase> verification and segmentation. Finally, we discuss several issues concerning our <phrase>proposed approach</phrase>.
Harvesting Useful <phrase>Information</phrase> from Researchers' Home Pages Harvesting Useful <phrase>Information</phrase> from Researchers' Home Pages Inspired by the scale and the usefulness of popular publication-centric <phrase>databases</phrase> like <phrase>Google Scholar</phrase>, <phrase>CiteSeer</phrase>, or <phrase>ACM</phrase> Portal, we would like to build a similar <phrase>database</phrase> but is researcher-centric. In addition, we would like the <phrase>database</phrase> to be built automatically, through <phrase>information</phrase> extracted directly from the targeted researchers' home pages. This dissertation aims at designing and implementing such a system. Acknowledgement I would like to <phrase>express my deep</phrase> gratitude towards my supervisor, Assistant <phrase>Professor</phrase> Kan Min-Yen, who has been guiding me throughout this one year on this project. I would like to thank him for the <phrase>knowledge</phrase>, guidance and <phrase>patience</phrase> that he has generously shown. In addition, I would like to thank everybody in the WING group for a motivational <phrase>learning environment</phrase> and all the support and assistance to this project. Abstract <phrase>ii</phrase> Acknowledgement iii 1 Introduction 1
Real-Time <phrase>Single</phrase> Image and <phrase>Video</phrase> Super-Resolution Using an Efficient Sub-<phrase>Pixel</phrase> <phrase>Convolutional Neural Network</phrase> Recently, several models based on <phrase>deep neural networks</phrase> have achieved <phrase>great success</phrase> in terms of both <phrase>reconstruction</phrase> accuracy and computational performance for <phrase>single</phrase> image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the <phrase>high</phrase> resolution (HR) space using a <phrase>single</phrase> filter, commonly bicubic <phrase>interpolation</phrase>, before <phrase>reconstruction</phrase>. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds <phrase>computational complexity</phrase>. In this <phrase>paper</phrase>, we present the first <phrase>convolutional neural network</phrase> (<phrase>CNN</phrase>) capable of real-time SR of <phrase>1080p</phrase> videos on a <phrase>single</phrase> <phrase>K2</phrase> <phrase>GPU</phrase>. To achieve this, we propose a novel <phrase>CNN</phrase> <phrase>architecture</phrase> where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-<phrase>pixel</phrase> <phrase>convolution</phrase> layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the <phrase>computational complexity</phrase> of the overall SR operation. We evaluate the <phrase>proposed approach</phrase> using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an <phrase>order</phrase> of <phrase>magnitude</phrase> faster than previous <phrase>CNN</phrase>-based methods.
<phrase>Feature Learning</phrase> in <phrase>Deep Neural Networks</phrase> - Studies on <phrase>Speech Recognition</phrase> Tasks Recent studies have shown that <phrase>deep neural networks</phrase> (DNNs) perform significantly better than shallow networks and <phrase>Gaussian mixture</phrase> models (GMMs) on large <phrase>vocabulary</phrase> <phrase>speech recognition</phrase> tasks. In this <phrase>paper</phrase>, we argue that the improved accuracy achieved by the DNNs is the result of their ability to extract dis-criminative <phrase>internal representations</phrase> that are robust to the many sources of variability in speech signals. We show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth, which leads to better <phrase>speech recognition</phrase> performance with deeper networks. We also show that DNNs cannot extrapolate to <phrase>test</phrase> samples that are substantially different from the <phrase>training examples</phrase>. If the <phrase>training data</phrase> are sufficiently representative, however, internal features learned by the DNN are relatively stable with respect to <phrase>speaker</phrase> differences, bandwidth differences, and environment <phrase>distortion</phrase>. This enables DNN-based recognizers to perform as well or better than <phrase>state</phrase>-of-the-<phrase>art</phrase> systems based on GMMs or shallow networks without the need for explicit <phrase>model</phrase> adaptation or feature normalization.
Minimum <phrase>Probability</phrase> Flow Learning Learning in <phrase>probabilistic models</phrase> is often hampered by the <phrase>general</phrase> intractability of the normalization factor and its derivatives. Here we propose a new learning technique that obviates the need to compute an intractable normalization factor or sample from the equilibrium distribution of the <phrase>model</phrase>. This is achieved by establishing dynamics that would transform the observed <phrase>data</phrase> distribution into the <phrase>model</phrase> distribution, and then setting as the objective the minimization of the initial flow of <phrase>probability</phrase> away from the <phrase>data</phrase> distribution. Score matching, minimum <phrase>velocity</phrase> learning, and certain forms of contrastive divergence are shown to be special cases of this learning technique. We demonstrate the application of minimum <phrase>probability</phrase> flow learning to <phrase>parameter estimation</phrase> in Ising models, <phrase>deep belief</phrase> networks , multivariate Gaussian <phrase>distributions</phrase> and a continuous <phrase>model</phrase> with a highly <phrase>general</phrase> <phrase>energy</phrase> <phrase>function</phrase> defined as a <phrase>power series</phrase>. In the <phrase>Ising model</phrase> case, minimum <phrase>probability</phrase> flow learning outperforms <phrase>current state</phrase> of the <phrase>art</phrase> techniques by approximately two <phrase>orders of magnitude</phrase> in learning time, with comparable error in recovered parameters. It is our hope that this technique will alleviate existing restrictions on the classes of <phrase>probabilistic models</phrase> that are practical for use.
Pro3gres Parser in the Conll <phrase>Domain Adaptation</phrase> Shared Task). Pro3gres Parser in the Conll <phrase>Domain Adaptation</phrase> Shared Task. In: <phrase>Acl</phrase> Conference, Workshop on Computational <phrase>Natural Language</phrase> Learning (conll-xi) Pro3gres Parser in the Conll <phrase>Domain Adaptation</phrase> Shared Task We present Pro3Gres, a <phrase>deep-syntactic</phrase>, fast dependency parser that combines a handwritten competence <phrase>grammar</phrase> with proba-bilistic performance disambiguation and that has been used in the biomedical domain. We discuss its performance in the <phrase>domain adaptation</phrase> open submission. We achieve <phrase>average</phrase> <phrase>results</phrase>, which is partly due to difficulties in mapping to the dependency representation used for the shared task.
<phrase>Local Receptive</phrase> Fields Based <phrase>Extreme Learning</phrase> Machine which was originally proposed for " generalized " <phrase>single</phrase>-<phrase>hidden layer</phrase> feedfor-ward <phrase>neural networks</phrase> (SLFNs), provides efficient unified learning solutions for the applications of <phrase>feature learning</phrase>, clustering , <phrase>regression</phrase> and classification. Different from the common understanding and tenet that hidden <phrase>neurons</phrase> of <phrase>neural networks</phrase> need to be iteratively adjusted during training stage, <phrase>ELM</phrase> theories show that hidden <phrase>neurons</phrase> are important but need not be iteratively tuned. In fact, all the parameters of <phrase>hidden nodes</phrase> can be <phrase>independent</phrase> of <phrase>training samples</phrase> and randomly generated according to any continuous <phrase>probability distribution</phrase>. And the obtained <phrase>ELM</phrase> networks satisfy <phrase>universal</phrase> approximation and classification capability. The <phrase>fully connected</phrase> <phrase>ELM</phrase> <phrase>architecture</phrase> has been extensively studied. However, <phrase>ELM</phrase> with local connections has not attracted much <phrase>research</phrase> attention yet. This <phrase>paper</phrase> studies the <phrase>general</phrase> <phrase>architecture</phrase> of locally connected <phrase>ELM</phrase>, showing that: 1) <phrase>ELM</phrase> theories are naturally valid for local connections, thus introducing <phrase>local receptive</phrase> fields to the input layer; 2) each hidden node in <phrase>ELM</phrase> can be a combination of several <phrase>hidden nodes</phrase> (a subnet-work), which is also consistent with <phrase>ELM</phrase> theories. <phrase>ELM</phrase> theories may shed a <phrase>light</phrase> on the <phrase>research</phrase> of different <phrase>local receptive</phrase> fields including true biological <phrase>receptive fields</phrase> of which the exact shapes and formula may be unknown to <phrase>human</phrase> beings. As a specific example of such <phrase>general</phrase> architectures, random convolutional nodes and a pooling structure are implemented in this <phrase>paper</phrase>. <phrase>Experimental</phrase> <phrase>results</phrase> on the NORB dataset, a benchmark for <phrase>object recognition</phrase>, show that compared with conventional <phrase>deep learning</phrase> solutions, the proposed <phrase>local receptive</phrase> fields based <phrase>ELM</phrase> (<phrase>ELM</phrase>-LRF) reduces the <phrase>error rate</phrase> from 6.5% to 2.7% and increases the learning speed up to 200 times.
Efficient <phrase>Deep Web</phrase> Crawling Using <phrase>Reinforcement Learning</phrase> <phrase>Deep web</phrase> refers to the hidden part of the Web that remains unavailable for standard Web crawlers. To obtain content of <phrase>Deep Web</phrase> is challenging and has been acknowledged as a significant gap in the coverage of <phrase>search engines</phrase>. To this end, the <phrase>paper</phrase> proposes a novel <phrase>deep web</phrase> crawling framework based on <phrase>reinforcement learning</phrase>, in which the crawler is regarded as an agent and <phrase>deep web</phrase> <phrase>database</phrase> as the environment. The agent perceives its <phrase>current state</phrase> and selects an <phrase>action</phrase> (query) to submit to the environment according to Q-value. The framework not only enables crawlers to learn a promising crawling strategy from its own experience, but also allows for utilizing diverse features of query keywords. <phrase>Experimental</phrase> <phrase>results</phrase> show that the method outperforms the <phrase>state</phrase> of <phrase>art</phrase> methods in terms of crawling capability and breaks through the assumption of full-text search implied by <phrase>existing methods</phrase>. 1 Introduction <phrase>Deep web</phrase> or <phrase>hidden web</phrase> refers to <phrase>World Wide Web</phrase> content that is not part of the surface Web, which is directly indexed by <phrase>search engines</phrase>. Studies [1] show <phrase>deep web</phrase> content is particularly important. Not only its size is estimated as hundreds of times larger than the so-called surface Web, but also it provides users with <phrase>high</phrase> quality <phrase>information</phrase>. However, to obtain such content of <phrase>deep web</phrase> is challenging and has been acknowledged as a significant gap in the coverage of <phrase>search engines</phrase> [2]. Surfacing is a common <phrase>solution</phrase> to provide users <phrase>deep web</phrase> content search service 1 , in which the crawler pre-computes the submissions for <phrase>deep web</phrase> forms and exhaustively indexes the response <phrase>results</phrase> off-line as other static <phrase>HTML</phrase> pages. The approach enables leveraging the existing <phrase>search engine</phrase> <phrase>infrastructure</phrase> hence adopted by most of crawlers, such as HiWE (<phrase>Hidden Web</phrase> Exposer) [3], <phrase>Hidden Web</phrase> crawler [4] and Google's <phrase>Deep Web</phrase> crawler [2]. One critical challenge in surfacing approach is how a crawler can automatically generate promising queries so that it can carry out efficient surfacing. The challenge 1 We may use crawl and surface interchangeably in the rest of the <phrase>paper</phrase>.
Precision Timed (pret) Computation in Cyber-physical Systems Cyber-Physical Systems (<phrase>CPS</phrase>) are integrations of computation with physical processes. Embedded <phrase>computers</phrase> and networks monitor and control the physical processes, usually with <phrase>feedback loops</phrase> where physical processes affect computations and vice versa. In the physical world, the passage of time is inexorable and concurrency is intrinsic. Neither of these properties is present in today's <phrase>computing</phrase> and networking abstractions. As a consequence, these abstractions require some fundamental rethinking. It is tempting to believe that the <phrase>CPS</phrase> problems can be solved by overlaying <phrase>higher-level</phrase> abstractions on top of existing <phrase>computing</phrase> <phrase>technology</phrase>. Indeed, it would be a scary prospect to suggest that much of the foundation of existing <phrase>technology</phrase> is flawed and must be rebuilt. How could this possibly result in a practical <phrase>research</phrase> program that will see <phrase>results</phrase> in our <phrase>lifetime</phrase>? In this position <phrase>paper</phrase>, we make a case that core <phrase>computing</phrase> abstractions must be and can be effectively and practically rebuilt. The objective is to enable a new generation of cyber-physical systems where computation and physical processes are tightly intertwined. This requires reintroducing properties that were deliberately and systematically abstracted away in the 20-th century view of computation. We approach the problem bottom-up. We must first rebuild the computational engines, and then build revised <phrase>higher level</phrase> abstractions on top of these. 1 The Problem In 1980, Patterson and Ditzel [12] did not invent reduced <phrase>instruction set</phrase> <phrase>computers</phrase> (<phrase>RISC</phrase>). Earlier <phrase>computers</phrase> all had reduced instruction sets. Instead, they argued that trends in computer <phrase>architecture</phrase> had gotten off the sweet spot, and that by dropping back a few years and forking a new version of architectures, leveraging what had been learned, they could get better <phrase>computers</phrase> by employing simpler instruction sets. It is again time for a change in direction in computer <phrase>architecture</phrase>. Architectures currently strive for <phrase>superior</phrase> <phrase>average</phrase>-case performance that regrettably ignores predictability and <phrase>repeatability</phrase> of timing properties. " Correct " execution of the SPECint benchmark suite has nothing to do with how <phrase>long</phrase> it takes to perform any particular <phrase>action</phrase>. C says nothing about timing, so timing is not considered part of correctness. Ar-chitectures have developed deep pipelines with <phrase>speculative execution</phrase> and <phrase>dynamic dispatch</phrase>. <phrase>Memory</phrase> architectures have developed multi-level caches and TLBs. The performance criterion is simple: faster (on <phrase>average</phrase>) is better. The biggest consequences have been in embedded <phrase>computing</phrase>. <phrase>Avionics</phrase> offers an extreme example: in " <phrase>fly by wire</phrase> " <phrase>aircraft</phrase>, where <phrase>software</phrase> interprets <phrase>pilot</phrase> commands and transports them to 
Learning Deep <phrase>Energy</phrase> Models <phrase>Deep generative models</phrase> with multiple <phrase>hidden layers</phrase> have been shown to be able to learn meaningful and compact representations of <phrase>data</phrase>. In this work we propose deep <phrase>energy</phrase> models, which use deep <phrase>feedforward neural</phrase> networks to <phrase>model</phrase> the <phrase>energy</phrase> landscapes that define <phrase>probabilistic models</phrase>. We are able to efficiently <phrase>train</phrase> all layers of our <phrase>model</phrase> simultaneously , allowing the <phrase>lower</phrase> layers of the <phrase>model</phrase> to adapt to the training of the higher layers, and thereby producing better <phrase>genera</phrase>-tive models. We evaluate the generative performance of our models on <phrase>natural images</phrase> and demonstrate that this joint training of <phrase>multiple layers</phrase> yields qualitative and quantitative improvements over greedy layerwise training. We further generalize our models beyond the commonly used sigmoidal <phrase>neural networks</phrase> and show how a deep extension of the product of <phrase>Student</phrase>-t <phrase>distributions</phrase> <phrase>model</phrase> achieves good generative performance. Finally , we introduce a discriminative extension of our <phrase>model</phrase> and demonstrate that it outper-forms other <phrase>fully-connected</phrase> models on <phrase>object recognition</phrase> on the NORB dataset.
<phrase>Data</phrase> <phrase>Programming</phrase>: Creating Large <phrase>Training Sets</phrase>, Quickly Large <phrase>labeled training</phrase> sets are the critical <phrase>building blocks</phrase> of <phrase>supervised learning</phrase> methods and are key enablers of <phrase>deep learning</phrase> techniques. For some applications, creating <phrase>labeled training</phrase> sets is the most time-consuming and expensive part of applying <phrase>machine learning</phrase>. We therefore propose a <phrase>paradigm</phrase> for the programmatic creation of <phrase>training sets</phrase> called <phrase>data</phrase> <phrase>programming</phrase> in which users provide a set of labeling functions, which are programs that heuristically <phrase>label</phrase> subsets of the <phrase>data</phrase>, but that are noisy and may conflict. By viewing these labeling functions as implicitly describing a <phrase>generative model</phrase> for this noise, we show that we can recover the parameters of this <phrase>model</phrase> to " denoise " the generated <phrase>training set</phrase>, and establish theoretically that we can recover the parameters of these <phrase>generative models</phrase> in a handful of settings. We then show how to modify a discriminative <phrase>loss function</phrase> to make it noise-aware, and demonstrate our method over a <phrase>range</phrase> of discriminative models including <phrase>logistic regression</phrase> and LSTMs. Experimentally, on the 2014 <phrase>TAC</phrase>-<phrase>KBP</phrase> Slot Filling challenge, we show that <phrase>data</phrase> <phrase>programming</phrase> would have <phrase>led</phrase> to a new winning score, and also show that applying <phrase>data</phrase> <phrase>programming</phrase> to an LSTM <phrase>model</phrase> leads to a <phrase>TAC</phrase>-<phrase>KBP</phrase> score almost 6 <phrase>F1</phrase> points over a <phrase>state</phrase>-of-the-<phrase>art</phrase> LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that <phrase>data</phrase> <phrase>programming</phrase> may be an easier way for non-experts to create <phrase>machine learning</phrase> models when <phrase>training data</phrase> is limited or unavailable.
Reflective Learning in Large Companies - can it work? A growing number of <phrase>universities</phrase> and companies are now becoming focused on promoting learning that is not merely <phrase>instrumental</phrase>. These aspirations refer to <phrase>deep learning</phrase>, transformational learning, critical learning, intentional learning, reflective learning and <phrase>lifelong learning</phrase>. Our primary goal is to present several ways in which lifelong reflective learning can be effectively and consciously promoted within large companies. The term reflection is used with two meanings. One would be the process by which an experience, in the form of thought, feeling or <phrase>action</phrase> is brought into consideration (while is happening or subsequently) and the other the creation of meaning and conceptualization from experience and the potentiality to look at things from another perspective (critical reflection). We propose the following solutions: modular classes, open engagement lectures, online <phrase>live</phrase> learning, virtual classrooms, <phrase>action</phrase> group learning, immersive <phrase>e</phrase>-learning applications, project-based <phrase>e</phrase>-learning and any valuable combination of these. As <phrase>industrial</phrase> experience shows, integration of these models and methods to create a learning and development program and to incorporate it in the <phrase>daily</phrase> working schedule provide a consistent <phrase>solution</phrase> for <phrase>education</phrase> in the <phrase>long</phrase> run.
Learning Feature Hierarchies with Centered <phrase>Deep Boltzmann Machines</phrase> <phrase>Deep Boltzmann machines</phrase> are in principle powerful models for extracting the hierarchical structure of <phrase>data</phrase>. Unfortunately, attempts to <phrase>train</phrase> layers jointly (without <phrase>greedy layer-wise</phrase> pretraining) have been largely unsuccessful. We propose a modification of the <phrase>learning algorithm</phrase> that initially recenters the output of the <phrase>activation functions</phrase> to zero. This modification leads to a better conditioned <phrase>Hessian</phrase> and thus makes learning easier. We <phrase>test</phrase> the <phrase>algorithm</phrase> on real <phrase>data</phrase> and demonstrate that our suggestion, the centered deep <phrase>Boltzmann</phrase> machine, learns a hierarchy of increasingly abstract representations and a better <phrase>generative model</phrase> of <phrase>data</phrase>.
<phrase>Parsing</phrase> <phrase>German</phrase> <phrase>Topological</phrase> Fields with Probabilistic <phrase>Context-free</phrase> Grammars Abstract <phrase>Parsing</phrase> <phrase>German</phrase> <phrase>Topological</phrase> Fields with Probabilistic <phrase>Context-free</phrase> Grammars A <phrase>research</phrase> <phrase>paper</phrase> submitted in conformity with the requirements for the <phrase>degree</phrase> of M. Sc. 2009 <phrase>Syntactic</phrase> analysis is useful for many <phrase>natural language processing</phrase> applications requiring further <phrase>semantic</phrase> analysis. Recent <phrase>research</phrase> in statistical <phrase>parsing</phrase> has <phrase>produced</phrase> a number of <phrase>high</phrase>-performance parsers using probabilistic <phrase>context-free</phrase> (PCFG) models to parse <phrase>English</phrase> text, such these methods to parse sentences in freer-<phrase>word-order</phrase> languages. Such languages as <phrase>Russian</phrase>, <phrase>Warlpiri</phrase>, and <phrase>German</phrase> feature <phrase>syntactic</phrase> constructions that produce discontinuous constituents, directly violating one of the crucial assumptions of <phrase>context-free</phrase> models of <phrase>syntax</phrase>. While PCFG technologies may thus be inadequate for full <phrase>syntactic</phrase> analysis of all phrasal structure in these languages, clausal structure can still be fruitfully parsed with these methods. In particular, we examine applying PCFG <phrase>parsing</phrase> to parse the <phrase>topological</phrase> field structure of <phrase>German</phrase>. These <phrase>topological</phrase> fields provide a <phrase>high</phrase>-level description of the <phrase>major</phrase> sections of a clause in relation to the clausal main <phrase>verb</phrase> and the subordinating heads and appear in strict linear sequences amenable to PCFG <phrase>parsing</phrase>. They are useful for tasks such as <phrase>deep syntactic</phrase> analysis, <phrase>part-of-speech tagging</phrase> and coreference resolution. In this work, we apply an unlexicalized, <phrase>latent variable</phrase>-based parser (Petrov et al., 2006) to <phrase>topological</phrase> field <phrase>parsing</phrase>, and achieve <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>parsing</phrase> <phrase>results</phrase> on two <phrase>German</phrase> <phrase>newspaper</phrase> corpora without any <phrase>language</phrase>-or <phrase>model</phrase>-dependent adaptation. We perform a qualitative error analysis of the parser output, and identify constructions like ellipses and parentheticals as the chief sources of remaining error. This is confirmed by a 3 further experiment in which <phrase>parsing</phrase> performance improves after restricting the training and <phrase>test</phrase> set to those sentences without these constructions. We also explore techniques for further improving <phrase>parsing</phrase> <phrase>results</phrase>. For example, discrimina-tive reranking of parses made by a generative parser could incorporate <phrase>linguistic</phrase> <phrase>information</phrase> such as those derived by our qualitative analysis. Self-training is another <phrase>semi-supervised</phrase> technique which utilizes additional unannotated <phrase>data</phrase> for training. 4 Acknowledgements Many people have contributed to the making of this document, whom I would like to thank here. First and foremost, I would like to thank my supervisor and mentor, Gerald <phrase>Penn</phrase>. I am continually amazed by the breadth and depth of his <phrase>knowledge</phrase>. Through this, he has given me a much better perspective of <phrase>computational linguistics</phrase>, yet I know that I have much more to learn from him in the future. I would also like to thank Graeme Hirst for his role as the second reader of this <phrase>paper</phrase>. His comments have 
Review of "A Computational Introduction to <phrase>Number Theory</phrase> and <phrase>Algebra</phrase> by Victor Shoup", <phrase>Cambridge University Press</phrase>, 2005 Increasingly, <phrase>number theory</phrase> and <phrase>algebra</phrase> have become useful tools for the well-rounded computer <phrase>scientist</phrase>. Historically, of course, <phrase>number theory</phrase> and <phrase>algebra</phrase> have been indispensable for <phrase>cryptography</phrase>, and finite fields (and, to a more limited extent, other <phrase>algebraic</phrase> structures) have been widely used in <phrase>coding theory</phrase>. More recently, though, these topics have begun to permeate other areas of theoretical <phrase>computer science</phrase>: finite fields and error-correcting codes are now pervasive in <phrase>complexity theory</phrase>; <phrase>quantum computing</phrase> relies heavily on <phrase>abstract algebra</phrase> (especially <phrase>linear algebra</phrase>) and <phrase>coding theory</phrase>; and, somewhat surprisingly , there have been recent explicit constructions of <phrase>combinatorial</phrase> objects such as extractors which rely on deep <phrase>results</phrase> from <phrase>number theory</phrase>. While there are numerous textbooks covering <phrase>number theory</phrase>, <phrase>abstract algebra</phrase>, and/or the basics of finite fields, there have not been many introductory-level books to approach the <phrase>subject matter</phrase> from a " <phrase>computer science</phrase> " perspective. A typical <phrase>algebra</phrase> <phrase>textbook</phrase>, for example, is more interested in proving that a certain <phrase>function</phrase> is well-defined or that a certain <phrase>algebraic structure</phrase> exists than in examining the efficiency of evaluating a particular <phrase>function</phrase> or <phrase>concrete</phrase> <phrase>algorithms</phrase> for generating certain objects. And certainly few books on <phrase>number theory</phrase> go beyond the most <phrase>basic</phrase> applications (if any applications are presented at all). That is not to say that there are no advanced books treating these topics; there certainly are. But the <phrase>student</phrase> looking for an introduction to these areas has traditionally had to learn about them from the <phrase>mathematicians</phrase>' perspective. Victor Shoup, a cryptographer and computational number theorist, has changed that with his publication of A Computational Introduction to <phrase>Number Theory</phrase> and <phrase>Algebra</phrase>. This is an outstanding and well-written <phrase>book</phrase> whose aim is to introduce the reader to a broad <phrase>range</phrase> of material ranging from <phrase>basic</phrase> to relatively advanced without requiring any <phrase>prior knowledge</phrase> on the part of the reader other than <phrase>calculus</phrase> and <phrase>mathematical</phrase> maturity. That the <phrase>book</phrase> succeeds at this goal is quite an accomplishment! Besides focusing on the computational aspects of <phrase>number theory</phrase> and <phrase>algebra</phrase> e.g., presenting <phrase>algorithms</phrase> for various tasks and analyzing their complexity the <phrase>book</phrase> emphasizes important applications of the <phrase>mathematics</phrase> developed. Indeed, the stated purpose of the <phrase>book</phrase> is to <phrase>cover</phrase> enough <phrase>mathematics</phrase> to understand the applications while covering some additional material to whet the reader's appetite; as stated in the preface, the <phrase>author</phrase> has:. .. tried to <phrase>strike</phrase> a reasonable balance between, on the one hand, presenting 
Learning Collaboration Links in a Collaborative Fuzzy Clustering Environment Revealing the common underlying structure of <phrase>data</phrase> spread across multiple <phrase>data</phrase> sites by applying clustering techniques is the aim of collaborative clustering, a recent and innovative idea brought up on the basis of exchanging <phrase>information</phrase> granules instead of <phrase>data</phrase> patterns. The strength of the collaboration between each pair of <phrase>data</phrase> repositories is determined by a user-driven parameter, both in vertical and horizontal collaborative fuzzy clustering. In this study, <phrase>Particle Swarm Optimization</phrase> and Rough <phrase>Set Theory</phrase> are used for setting the most suitable values of the collaboration links between the <phrase>data</phrase> sites. Encouraging empirical <phrase>results</phrase> uncovered the deep impact observed at the individual clusters, allowing us to conclude that the overall effect of the collaboration has been improved.
<phrase>Mining</phrase> <phrase>Pixels</phrase>: <phrase>Weakly Supervised</phrase> <phrase>Semantic</phrase> Segmentation Using Image <phrase>Labels</phrase> We consider the task of learning a classifier for <phrase>semantic</phrase> segmentation using weak supervision, in this case, image <phrase>labels</phrase> specifying the objects within the image. Our method uses <phrase>deep convolutional</phrase> <phrase>neural networks</phrase> (CNNs) and adopts an Expectation-Maximization (EM) <phrase>based approach</phrase> maintaining the uncertainty on <phrase>pixel</phrase> <phrase>labels</phrase>. We focus on the following three crucial aspects of the EM <phrase>based approach</phrase>: (i) initialization; (<phrase>ii</phrase>) latent posterior estimation (<phrase>E</phrase> step) and (iii) the parameter update (M step). We show that saliency and attention maps provide good cues to learn an initialization <phrase>model</phrase> and allows us to skip the bad local maximum to which EM methods are otherwise traditionally prone. In <phrase>order</phrase> to update the parameters, we propose minimizing the combination of the standard softmax loss and the <phrase>KL</phrase> divergence between the true latent posterior and the likelihood given by the <phrase>CNN</phrase>. We argue that this combination is more robust to wrong predictions made by the expectation step of the EM method. We support this argument with empirical and visual <phrase>results</phrase>. We additionally incorporate an approximate intersection-over-<phrase>union</phrase> (<phrase>IoU</phrase>) term into the <phrase>loss function</phrase> for better <phrase>parameter estimation</phrase>. <phrase>Extensive experiments</phrase> and discussions show that: (i) our method is very simple and intuitive; (<phrase>ii</phrase>) requires only image-level <phrase>labels</phrase>; and (iii) consistently outperforms other weakly/<phrase>semi supervised</phrase> <phrase>state</phrase>-of-the-<phrase>art</phrase> methods with a very <phrase>high</phrase> margin on the PASCAL <phrase>VOC</phrase> 2012 dataset.
The use of on-line co-training to reduce the <phrase>training set</phrase> size in <phrase>pattern recognition</phrase> methods: Application to <phrase>left ventricle</phrase> segmentation in <phrase>ultrasound</phrase> The use of statistical <phrase>pattern recognition</phrase> models to segment the <phrase>left ventricle</phrase> of the <phrase>heart</phrase> in <phrase>ultrasound</phrase> images has gained substantial attention over the last few years. The main obstacle for the wider exploration of this methodology lies in the need for large annotated <phrase>training sets</phrase>, which are used for the estimation of the <phrase>statistical model</phrase> parameters. In this <phrase>paper</phrase>, we present a new on-line co-training methodology that reduces the need for large <phrase>training sets</phrase> for such <phrase>parameter estimation</phrase>. Our approach learns the initial parameters of two different models using a small manually annotated <phrase>training set</phrase>. Then, given each frame of a <phrase>test</phrase> <phrase>sequence</phrase>, the methodology not only produces the segmen-tation of the current frame, but it also uses the <phrase>results</phrase> of both classifiers to retrain each other incrementally. This on-line <phrase>aspect</phrase> of our approach has the advantages of producing segmentation <phrase>results</phrase> and retraining the classifiers on the <phrase>fly</phrase> as frames of a <phrase>test</phrase> <phrase>sequence</phrase> are presented, but it introduces a harder learning setting compared to the usual off-line co-training, where the <phrase>algorithm</phrase> has access to the whole set of un-annotated <phrase>training samples</phrase> from the beginning. Moreover, we introduce the use of the following new types of classifiers in the co-training framework: <phrase>deep belief</phrase> network and multiple <phrase>model</phrase> probabilistic <phrase>data</phrase> association. We show that our method leads to a fully automatic <phrase>left ventricle</phrase> segmentation system that achieves <phrase>state</phrase>-of-the-<phrase>art</phrase> accuracy on a <phrase>public</phrase> <phrase>database</phrase> with <phrase>training sets</phrase> containing at least twenty annotated images.
<phrase>Domain-specific</phrase> recommendation based on <phrase>deep understanding</phrase> of text This <phrase>paper</phrase> considers the process of development for adomain-specic recommender system that uses the domain of <phrase>cocktail</phrase> recipes as aexample for experiments. Based on <phrase>ontology</phrase> adeep understanding of text is created recipes are considered. The <phrase>ontology</phrase> is designed by <phrase>basic</phrase> categories to extract features such as ingredients. Ingredients are modeled by avors for comparability. The process of <phrase>data processing</phrase> along with the recommendation extract over2.000 recipes based on aontology with over1.000 ingredients. The keyofthe recommendation is based on domain-specic distance functions. An earest-neighbor approach is used to classify recommendations for ag iven favorite. Va lidation is considered based on the acceptability of <phrase>domain experts</phrase>. 1I ntroduction In <phrase>order</phrase> to understand the recommendation process, aspecic domain is used for experiments that are focused on <phrase>deep understanding</phrase> of text. <phrase>Deep understanding</phrase> [ASdB08] leads to arich <phrase>semantic</phrase> representation of <phrase>data</phrase>, which is necessary for content-based recommendation. As an example of aspecic domain, the domain of <phrase>cocktails</phrase> is chosen because it is denite and documented by bartending manuals and books of <phrase>cocktail</phrase> recipes written by <phrase>domain experts</phrase>. The <phrase>deep understanding</phrase> such as avors of ingredients enriches the recommendation in the perspective of <phrase>perception</phrase>. <phrase>Domain experts</phrase> are interviewed to get <phrase>feedback</phrase> on the recommendation quality. Section 2considers the objectives. In section 3itfollows the related work. To achieve the objectivesfollowing four challenges are considered: In section 4adomain-specic survey with <phrase>domain experts</phrase> is used to understand the eld of <phrase>cocktail</phrase> recipes (challenge one) to process ahuge volume of recipes (challenge two). The aim is to learn howrecipes depends on recommendation. An <phrase>ontology</phrase> is designed to store the features such as ingredients in hierarchy. Forc hallenge three section 5d escribes domain-specic distances between classic recipes. The last experiment in section 6considers an validation of nearest-neighbor recommendation (challenge four). The last section 7considers the conclusion and future work.
Optimal Buffer <phrase>Management</phrase> Policies for Delay Tolerant Networks Delay Tolerant Networks are <phrase>wireless networks</phrase> where disconnections may occur frequently due to propagation phenomena, node mobility, and power outages. Propagation delays may also be <phrase>long</phrase> due to the operational environment (e.g. deep space, underwater). In <phrase>order</phrase> to achieve <phrase>data</phrase> delivery in such challenging networking environments, researchers have proposed the use of store-carry-and-<phrase>forward</phrase> protocols: there, a node may store a message in its buffer and carry it along for <phrase>long</phrase> periods of time, until an appropriate forwarding opportunity arises. Additionally, multiple message replicas are often propagated to increase delivery <phrase>probability</phrase>. This combination of <phrase>long</phrase>-term storage and replication imposes a <phrase>high</phrase> storage overhead on untethered nodes (e.g. handhelds). Thus, efficient buffer <phrase>management</phrase> policies are necessary to decide which messages should be discarded, when node buffers are operated close to their capacity. In this <phrase>paper</phrase>, we propose efficient buffer <phrase>management</phrase> policies for delay tolerant networks. We show that traditional buffer <phrase>management</phrase> policies like drop-tail or drop-front fail to consider all relevant <phrase>information</phrase> in this context and are, thus, sub-optimal. Using the theory of encounter-based message dissemination, we propose an optimal buffer <phrase>management</phrase> policy based on global <phrase>knowledge</phrase> about the network. Our policy can be tuned either to minimize the <phrase>average</phrase> delivery delay or to maximize the <phrase>average</phrase> delivery rate. Finally, we introduce a distributed <phrase>algorithm</phrase> that uses statistical learning to approximate the global <phrase>knowledge</phrase> required by the the optimal <phrase>algorithm</phrase>, in practice. Using simulations based on a synthetic mobility <phrase>model</phrase> and real mobility traces, we show that our buffer <phrase>management</phrase> policy based on statistical learning successfully approximates the performance of the optimal policy in all considered scenarios. At the same time, our policy outperforms existing ones in terms of both <phrase>average</phrase> delivery rate and delivery delay.
Scheduling Policy <phrase>Design</phrase> using <phrase>Stochastic</phrase> <phrase>Dynamic Programming</phrase> Scheduling policies for open soft real-time systems must be able to balance the competing concerns of meeting their objectives under exceptional conditions while achieving good performance in the <phrase>average</phrase> case. Balancing these concerns requires modeling strategies that represent the <phrase>range</phrase> of possible task behaviors, and <phrase>solution</phrase> techniques that are capable of effectively managing uncertainty in <phrase>order</phrase> to discover scheduling policies that are effective across the <phrase>range</phrase> of system modes. We develop methods for solving a particular class of task scheduling problems in an open soft real-time setting involving repeating, non-preemptable tasks that contend for a <phrase>single</phrase> shared resource. We enforce timeliness by optimizing performance with respect to the <phrase>proportional</phrase> progress of tasks in the system. We <phrase>model</phrase> this scheduling problem as an infinite-<phrase>state</phrase> <phrase>Markov decision process</phrase>, and provide guarantees regarding the existence of optimal solutions to this problem. We derive several methods for approximating optimal scheduling policies and provide <phrase>ii</phrase> theoretical justification and <phrase>empirical evidence</phrase> that these solutions are good approximations to the optimal <phrase>solution</phrase>. We consider cases in which task models are known, and adapt <phrase>reinforcement learning</phrase> methods to learn task models when they are not available. iii Acknowledgments I must first <phrase>express my deep</phrase> gratitude to my wife for her <phrase>love</phrase> and understanding. Without her support and <phrase>motivation</phrase>, this dissertation would likely never have been begun, let alone completed. From the years living hundreds of miles apart at the beginning of our graduate careers, through the late nights, stressful days, and the moments of bleak doubt and indecision, she has helped me to keep everything in perspective. Thanks especially to my <phrase>research</phrase> advisor, Bill Smart, for supplying me with ideas and permitting me the freedom to pursue them along their often chaotic trajectories. When it became clear that my initial dissertation topic was prohibitively ambitious, it was his guidance, understanding, and acuity that <phrase>led</phrase> to the fruitful collaboration that resulted in the <phrase>research</phrase> detailed herein. Finally, Bill's efforts in managing to juggle assisting in <phrase>reading</phrase> and <phrase>editing</phrase> this dissertation, along with his other <phrase>professional</phrase> responsibilities, just <phrase>short</phrase> months after his second child was born merits special note. Chris <phrase>Gill</phrase> has been invaluable in his capacities as a collaborator, advisor, instructor, and <phrase>editor</phrase>. His thoughts and ideas helped to inspire this work, and his suggestions have helped to moor this <phrase>research</phrase> in <phrase>reality</phrase>, otherwise those ties might have been tenuous indeed, as the theory at times seems to have 
Automatic Extraction of Outbreak <phrase>Information</phrase> from <phrase>News</phrase> iii This <phrase>thesis</phrase> is dedicated to my wife, <phrase>Wei</phrase>, without whom it would never have been accomplished. iv ACKNOWLEDGMENTS I would like to thank all people who have helped and inspired me during my doctoral study. My deep gratitude to my advisors Prof. Bing Liu and Prof. Peter Nelson for their support and advising. I've learned not just <phrase>data mining</phrase> from them, but also the <phrase>professional</phrase> attitude and methodology in <phrase>conducting</phrase> <phrase>research</phrase>. I would also like to thank my other <phrase>thesis</phrase> committee, Prof.
<phrase>Web-Based</phrase> <phrase>Peer Assessment</phrase> in Learning Computer <phrase>Programming</phrase> <phrase>Peer assessment</phrase> is a method of motivating students in learning computer <phrase>programming</phrase>, involving students marking and providing <phrase>feedback</phrase> on other students' work. This <phrase>paper</phrase> reports on the <phrase>design</phrase> and implementation of a novel <phrase>web-based</phrase> <phrase>peer assessment</phrase> system, and discusses its deployment on a large <phrase>programming</phrase> module. The <phrase>results</phrase> indicate that this <phrase>peer assessment</phrase> system has successfully helped students to develop their understanding of computer <phrase>programming</phrase>. Assessment is a tool for learning, but traditional assessment methods often encourage " surface learning " , characterised by memorisation and comprehension of <phrase>information</phrase>. <phrase>Deep learning</phrase>, such as creating new ideas, and critical judgement of a student's work, can be encouraged by the use of <phrase>peer assessment</phrase> [1,2,10]. When students evaluate each others' work they think more deeply, see how others tackle problems, learn to criticise constructively, and display some important <phrase>cognitive</phrase> skills such as <phrase>critical thinking</phrase> [3,4]. As part of a study investigating the extent that <phrase>peer assessment</phrase> can <phrase>promote deep learning</phrase> in a <phrase>programming</phrase> course, we have developed a novel <phrase>web-based</phrase> <phrase>peer assessment</phrase> tool. In this <phrase>paper</phrase>, we describe the tool and the <phrase>peer assessment</phrase> process it supports, and <phrase>report</phrase> on its deployment on a large computer <phrase>programming</phrase> course. Falchikov [5] defines <phrase>peer assessment</phrase> as " the process whereby groups rate their <phrase>peers</phrase> ". Somervell [6] states that <phrase>peer assessment</phrase> engages students in making judgements on the other students' work. In the <phrase>peer assessment</phrase> process, students are involved both in the learning and in the assessment process. <phrase>Peer assessment</phrase> is primarily a tool for learning rather than for summative assessment [7]. Dochy and McDowell [8] remark that " <phrase>peer assessment</phrase> is not only a tool to provide a peer with constructive <phrase>feedback</phrase> which is understood by the peer. Above all, <phrase>peer assessment</phrase> is a tool for the learner himself. " In addition, <phrase>peer assessment</phrase> focuses on providing and receiving <phrase>feedback</phrase>, which correlates with effective learning. Receiving many and frequent peer feedbacks can prevent some errors and provide hints for making progress in learning [9]. Thus the <phrase>peer assessment</phrase> process provides many benefits to students, including the following: encouragement of students' <phrase>deep learning</phrase> skills in <phrase>programming</phrase> by making judgements and providing <phrase>feedback</phrase> on other student's work [10]; students have opportunities to compare and discuss about what constituted a good or bad piece of work, which help them to improve their <phrase>programming</phrase> style and think more deeply about the quality of work [12]. 
<phrase>Cognitive</phrase> presence in asynchronous <phrase>online learning</phrase>: a comparison of four discussion strategies Some scholars argue that students do not achieve <phrase>higher level</phrase> learning, or <phrase>cognitive</phrase> presence, in online courses. <phrase>Online discussion</phrase> has been proposed to <phrase>bridge</phrase> this gap between online and face-to-face <phrase>learning environments</phrase>. However, the <phrase>literature</phrase> indicates that the conventional approach to <phrase>online discussion</phrase> asking probing questions does not necessarily advance the discussion through the phases of <phrase>cognitive</phrase> presence: triggering events, exploration, integration and resolution, which are crucial for deep <phrase>knowledge</phrase> <phrase>construction</phrase>. Using mixed methods, we examined the contribution of four scenario-based <phrase>online discussion</phrase> strategies structured, scaffolded, debate and role <phrase>play</phrase> to the learners' <phrase>cognitive</phrase> presence, the outcome of the discussion. Learners' discussion postings within each strategy were segmented and <phrase>categorized</phrase> according to the four phases. The discussion strategies, each using the same authentic scenario, were then compared in terms of the number of segments representing these phases. We found that the structured strategy, while highly associated with triggering events, <phrase>produced</phrase> no discussion pertaining to the resolution phase. The scaffolded strategy, on the other hand, showed a strong association with the resolution phase. The debate and role-<phrase>play</phrase> strategies were highly associated with exploration and integration phases. We concluded that discussion strategies requiring learners to take a perspective in an authentic scenario facilitate <phrase>cognitive</phrase> presence, and thus <phrase>critical thinking</phrase> and higher levels of learning. We suggest a <phrase>heuristic</phrase> for sequencing a series of discussion forums and recommend areas for further related <phrase>research</phrase>. <phrase>Online learning</phrase> has been characterized as deficient in providing the social interaction needed for the <phrase>construction</phrase> and development of <phrase>knowledge</phrase>, when compared with face-to-face learning, in complex learning domains (Slagter van Tyron & <phrase>Bishop</phrase> 2009). In response to this deficiency, <phrase>online discussion</phrase> has been used to <phrase>bridge</phrase> the interaction gap between the two <phrase>learning environments</phrase>. It has been presented as a <phrase>substitute</phrase> for face-to-face social interaction, and therefore has become the subject of investigations focusing on its potential for <phrase>construction</phrase> and development of <phrase>knowledge</phrase> (e.g. Weinberger & Fischer 2005). According to Collins et al. (1991), the development of <phrase>knowledge</phrase> is the result of interaction among students, instructor, content and environment or <phrase>culture</phrase> as the essential instructional components. The <phrase>online learning</phrase> system
On <phrase>deep generative models</phrase> with applications to recognition The most popular way to use <phrase>probabilistic models</phrase> in vision is first to extract some descriptors of small <phrase>image patches</phrase> or object parts using well-engineered features, and then to use statistical learning tools to <phrase>model</phrase> the dependencies among these features and eventual <phrase>labels</phrase>. Learning <phrase>probabilistic models</phrase> directly on the raw <phrase>pixel</phrase> values has proved to be much more difficult and is typically only used for regularizing discriminative methods. In this work, we use one of the best, <phrase>pixel</phrase>-level, <phrase>generative models</phrase> of <phrase>natural images</phrase> a gated MRF as the lowest level of a <phrase>deep belief</phrase> network (DBN) that has several <phrase>hidden layers</phrase>. We show that the resulting DBN is very good at coping with occlusion when predicting expression categories from face images, and it can produce features that perform comparably to SIFT descriptors for discriminating different types of scene. The generative ability of the <phrase>model</phrase> also makes it easy to see what <phrase>information</phrase> is captured and what is <phrase>lost</phrase> at each level of representation.
<phrase>Deep belief</phrase> net learning in a <phrase>long</phrase>-<phrase>range</phrase> vision system for autonomous off-<phrase>road</phrase> driving We present a learning-<phrase>based approach</phrase> for <phrase>long</phrase>-<phrase>range</phrase> vision that is able to accurately classify complex terrain at distances up to the horizon, thus allowing <phrase>high</phrase>-level strategic planning. A <phrase>deep belief</phrase> network is trained with unsupervised <phrase>data</phrase> and a <phrase>reconstruction</phrase> criterion to extract features from an input image, and the features are used to <phrase>train</phrase> a realtime classifier to predict traversability. The online supervision is given by a <phrase>stereo</phrase> module that provides robust <phrase>labels</phrase> for nearby areas up to 12 meters distant. The approach was developed and tested on the LAGR <phrase>mobile robot</phrase>.
Soft Weight-sharing for <phrase>Neural Network</phrase> Compression The success of <phrase>deep learning</phrase> in numerous application domains created the desire to run and <phrase>train</phrase> them on <phrase>mobile</phrase> devices. This however, conflicts with their computationally, <phrase>memory</phrase> and <phrase>energy</phrase> intense <phrase>nature</phrase>, leading to a growing interest in compression. Recent work by Han et al. (2015a) propose a pipeline that involves retraining, <phrase>pruning</phrase> and quantization of <phrase>neural network</phrase> weights, obtaining <phrase>state</phrase>-of-the-<phrase>art</phrase> compression rates. In this <phrase>paper</phrase>, we show that competitive compression rates can be achieved by using a version of " soft weight-sharing " (Nowlan & Hinton, 1992). Our <phrase>method achieves</phrase> both quantization and <phrase>pruning</phrase> in one simple (retraining g procedure. This point of view also exposes the relation between compression and the <phrase>minimum description length</phrase> (MDL) principle.
Learning Control of <phrase>Bipedal</phrase> Dynamic Walking <phrase>Robots</phrase> with <phrase>Neural Networks</phrase> Learning Control of <phrase>Bipedal</phrase> Dynamic Walking <phrase>Robots</phrase> with <phrase>Neural Networks</phrase> Stability and robustness are two important performance requirements for a dynamic walking <phrase>robot</phrase>. Learning and adaptation can improve stability and robustness. This <phrase>thesis</phrase> explores such an adaptation capability through the use of <phrase>neural networks</phrase>. Three <phrase>neural network</phrase> models (<phrase>BP</phrase>, CMAC and RBF networks) are studied. The RBF network is chosen as best, despite its weakness at covering <phrase>high</phrase> dimensional input spaces. To overcome this problem, a self-organizing scheme of <phrase>data</phrase> clustering is explored. This system is applied successfully in a <phrase>biped</phrase> walking <phrase>robot</phrase> system with a <phrase>supervised learning</phrase> mode. Generalized Virtual <phrase>Model</phrase> Control (GVMC) is also proposed in this <phrase>thesis</phrase>, which is inspired by a bio-mechanical <phrase>model</phrase> of locomotion, and is an extension of ordinary Virtual <phrase>Model</phrase> Control. Instead of adding virtual <phrase>impedance</phrase> components to the <phrase>biped</phrase> skeletal system in virtual <phrase>Cartesian</phrase> space, GVMC uses adaptation to approximately reconstruct the dynamics of the <phrase>biped</phrase>. The effectiveness of these approaches is proved both theoretically and experimentally (in <phrase>simulation</phrase>). Acknowledgments First of all, I would like to <phrase>express my deep</phrase> appreciation to my <phrase>thesis</phrase> supervisor, <phrase>Professor</phrase> <phrase>Gill</phrase> Pratt for his kind encouragement, support and expert guidance through the course of this <phrase>research</phrase>. His gentle style of advising helped me in the challenging <phrase>research</phrase> <phrase>area</phrase> of <phrase>biped</phrase> locomotion control. I am very grateful to all the members of the leg lab for their kind help during the years. It has been a great experience to stay with such a group of people who have a broad <phrase>spectrum</phrase> of talents in <phrase>robotics</phrase>, mechanical <phrase>design</phrase>, computer, and <phrase>electronics</phrase>. Particularly, I want to thank Jerry Pratt for his unselfish help and support through my <phrase>research</phrase>. The <phrase>simulation</phrase> of <phrase>biped</phrase> locomotion learning control was based on his previous <phrase>simulation</phrase> code (with the virtual <phrase>model</phrase> control scheme). He offered me a lot kind help in solving my puzzles in the creature <phrase>library</phrase>. I also wish to thank Bruce Deffenbaugh for his friendship and encouragement when I was exploring in the darkness. We spent many night hours sitting together and talking about many interesting things. His wisdom and endless stories made me feel fresh before I ran out of <phrase>gas</phrase> in my <phrase>research</phrase> during the night. Thanks to Dr. Hugh Herr for his help in setting up external force experiment in my <phrase>simulation</phrase>, which is an important part in this <phrase>research</phrase>. Also I appreciate the help of <phrase>Professor</phrase> J.J. Slotine for the interesting and constructive discussions on <phrase>neural networks</phrase> 
Towards Lifelong Self-Supervision: A <phrase>Deep Learning</phrase> Direction for <phrase>Robotics</phrase> Despite outstanding success in vision amongst other domains, many of the recent <phrase>deep learning</phrase> approaches have evident drawbacks for <phrase>robots</phrase>. This <phrase>manuscript</phrase> surveys recent work in the <phrase>literature</phrase> that pertain to applying <phrase>deep learning</phrase> systems to the <phrase>robotics</phrase> domain, either as means of estimation or as a tool to resolve motor commands directly from raw percepts. These <phrase>recent advances</phrase> are only a piece to the <phrase>puzzle</phrase>. We suggest that <phrase>deep learning</phrase> as a tool alone is insufficient in building a unified framework to acquire <phrase>general</phrase> <phrase>intelligence</phrase>. For this reason, we <phrase>complement</phrase> our survey with insights from <phrase>cognitive development</phrase> and refer to ideas from <phrase>classical</phrase> <phrase>control theory</phrase>, producing an integrated direction for a <phrase>lifelong learning</phrase> <phrase>architecture</phrase>.
A <phrase>Machine Learning</phrase> Perspective on Predictive Coding with PAQ PAQ8 is an <phrase>open source</phrase> lossless <phrase>data compression</phrase> <phrase>algorithm</phrase> that currently achieves the best compression rates on many benchmarks. This <phrase>report</phrase> presents a detailed description of PAQ8 from a <phrase>statistical machine learning</phrase> perspective. It shows that it is possible to understand some of the modules of PAQ8 and use this understanding to improve the method. However, intuitive statistical explanations of the behavior of other modules remain elusive. We hope the description in this <phrase>report</phrase> will be a <phrase>starting point</phrase> for discussions that will increase our understanding, <phrase>lead</phrase> to improvements to PAQ8, and facilitate a transfer of <phrase>knowledge</phrase> from PAQ8 to other <phrase>machine learning</phrase> methods, such a <phrase>recurrent neural networks</phrase> and <phrase>stochastic</phrase> memoizers. Finally, the <phrase>report</phrase> presents a broad <phrase>range</phrase> of new applications of PAQ to <phrase>machine learning</phrase> tasks including <phrase>language</phrase> modeling and adaptive text prediction, adaptive <phrase>game</phrase> playing, classification, and compression using features from the field of <phrase>deep learning</phrase>.
<phrase>Neural networks</phrase> with a self-refreshing <phrase>memory</phrase>: <phrase>knowledge transfer</phrase> in sequential learning tasks without catastrophic forgetting We explore a dual-<phrase>network architecture</phrase> with self-refreshing <phrase>memory</phrase> (Ans and Rousset 1997) which overcomes catastrophic forgetting in sequential learning tasks. Its principle is that new <phrase>knowledge</phrase> is learned along with an internally generated activity re ecting the network <phrase>history</phrase>. What mainly distinguishes this <phrase>model</phrase> from others using pseudorehearsal in feedforward multilayer networks is a reverberating process used for generating pseudoitems. This process, which tends to go up to network attractors from random activation, is more suitable for capturing optimally the deep structure of previously learned <phrase>knowledge</phrase> than a <phrase>single</phrase> <phrase>feed-forward</phrase> <phrase>pass</phrase> of activity. The proposed mechanism for 'transporting <phrase>memory</phrase>' without loss of <phrase>information</phrase> between two different <phrase>brain</phrase> structures could be viewed as a neurobiologically plausible means for consolidation in <phrase>long-term memory</phrase>. <phrase>Knowledge transfer</phrase> is explored with regard to learning speed, ability to generalize and vulnerability to network <phrase>damages</phrase>. We show that transfer is more ef cient when two related tasks are sequentially learned than when they are learned concurrently. With a self-refreshing <phrase>memory</phrase> network <phrase>knowledge</phrase> can be saved for a <phrase>long</phrase> time and therefore reused in subsequent acquisitions. 1. Introduction Learning in distributed multilayer <phrase>neural networks</phrase> is most often achieved through a <phrase>gradient descent</phrase> adaptive <phrase>algorithm</phrase>, of which the most popular and widely used is the <phrase>backpropagation</phrase> procedure (Rumelhart et al. 1986). It is well known that when <phrase>gradient descent</phrase> learning procedures are used in sequential learning tasks, a <phrase>major</phrase> drawback, termed catastrophic forgetting (or catastrophic interference), generally arises: when a network having previously learned a rst set of items is retrained on a second set of items, the newly learned <phrase>information</phrase> may completely destroy the <phrase>information</phrase> learned about the rst set (McCloskey and Cohen 1989, Ratcliff 1990). Since this behaviour is unacceptable for models of <phrase>human</phrase> learning and <phrase>memory</phrase>, a number of authors have explored several ways of reducing the retroactive interference in sequential learning tasks (Hetherington and Seidenberg
The relationship between Precision-Recall and <phrase>ROC</phrase> curves <phrase>Receiver</phrase> Operator Characteristic (<phrase>ROC</phrase>) curves are commonly used to present <phrase>results</phrase> for <phrase>binary</phrase> decision problems in <phrase>machine learning</phrase>. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between <phrase>ROC</phrase> space and PR space, such that a curve dominates in <phrase>ROC</phrase> space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the <phrase>convex hull</phrase> in <phrase>ROC</phrase> space; we show an efficient <phrase>algorithm</phrase> for <phrase>computing</phrase> this curve. Finally, we also note differences in the two types of curves are significant for <phrase>algorithm</phrase> <phrase>design</phrase>. For example, in PR space it is incorrect to linearly <phrase>interpolate</phrase> between points. Furthermore, <phrase>algorithms</phrase> that optimize the <phrase>area</phrase> under the <phrase>ROC</phrase> curve are not guaranteed to optimize the <phrase>area</phrase> under the PR curve.
Fast Packet Processing on <phrase>High</phrase> Performance Architectures Acknowledgements First of all, I want to say thanks to my <phrase>friends</phrase>. I shared with you many wonderful moments during these years. Gioia and Alfredo. I also want to thank all the people in my <phrase>research</phrase> group. Thanks <phrase>Christian</phrase>, Gregorio, Andrea et al. I spent with you a lot of good moments and a lot of good meals. I would like to acknowledge Fabio and Domenico. Thanks guys. We were an incredible team. Hope to work another time with you in the future. I want to thank my tutors prof. Stefano Giordano and prof. <phrase>Franco</phrase> <phrase>Russo</phrase>. I am grateful to Dr.Andrew Moore for <phrase>giving me the opportunity</phrase> of interning in the Computer Lab at <phrase>University</phrase> of <phrase>Cambridge</phrase> where i met a lot a good people and learn a lot of things. Abstract The rapid growth of <phrase>Internet</phrase> and the fast emergence of new network applications have brought great challenges and complex issues in deploying <phrase>high</phrase>-speed and <phrase>QoS</phrase> guaranteed IP network. For this reason packet classification and network <phrase>intrusion detection</phrase> have assumed a key role in modern <phrase>communication</phrase> networks in <phrase>order</phrase> to provide <phrase>Qos</phrase> and <phrase>security</phrase>. In this <phrase>thesis</phrase> we describe a number of the most advanced solutions to these tasks. We introduce NetFPGA and Network Processors as reference platforms both for the <phrase>design</phrase> and the implementation of the solutions and <phrase>algorithms</phrase> described in this <phrase>thesis</phrase>. The rise in links capacity reduces the time available to network devices for packet processing. For this reason, we show different solutions which, either by <phrase>heuristic</phrase> and <phrase>randomization</phrase> or by smart <phrase>construction</phrase> of <phrase>state machine</phrase>, allow IP lookup, packet classification and <phrase>deep packet inspection</phrase> to be fast in real devices based on <phrase>high</phrase> speed platforms such as NetFPGA or Network Processors.
Speech recognitionwith segmental <phrase>conditional random fields</phrase>: A summary of the JHU CLSP 2010 Summer Workshop This <phrase>paper</phrase> summarizes the 2010 CLSP Summer Workshop on <phrase>speech recognition</phrase> at <phrase>Johns Hopkins University</phrase>. The key theme of the workshop was to improve on <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>speech recognition</phrase> systems by using Segmental <phrase>Conditional Random Fields</phrase> (SCRFs) to integrate multiple types of <phrase>information</phrase>. This approach uses a <phrase>state</phrase>-of-the-<phrase>art</phrase> baseline as a springboard from which to add a suite of novel features including ones derived from <phrase>acoustic</phrase> templates, deep <phrase>neural net</phrase> <phrase>phoneme</phrase> detections, duration models, <phrase>modulation</phrase> features , and whole word point-process models. The SCRF framework is able to appropriately weight these different <phrase>information</phrase> sources to produce significant gains on both the <phrase>Broadcast</phrase> <phrase>News</phrase> and <phrase>Wall Street Journal</phrase> tasks. 1. INTRODUCTION Novel techniques in <phrase>speech recognition</phrase> are often hampered by the <phrase>long</phrase> <phrase>road</phrase> that must be followed to turn them into fully functional systems capable of competing with the <phrase>state</phrase>-of-the-<phrase>art</phrase>. In this work, we explore the use of Segmental <phrase>Conditional Random Fields</phrase> as an integrating <phrase>technology</phrase> which can augment the best conventional systems with <phrase>information</phrase> from novel scientific approaches. The Segmental CRF approach [1] is a modeling technique in which the <phrase>probability</phrase> of a word <phrase>sequence</phrase> w is estimated from observations o as P (w|o) using a log-<phrase>linear model</phrase>. Described in <phrase>Sec</phrase>. 2, the <phrase>model</phrase> determines the <phrase>probability</phrase> of a word <phrase>sequence</phrase> by weight-ing features which each measure some form of consistency between a <phrase>hypothesis</phrase> and the underlying audio. These features are at the word-segment level, for example a feature might be the similarity between observed and expected <phrase>formant</phrase> tracks. The key characteristic of the SCRF approach is that it provides a principled yet flexible way to integrate multiple <phrase>information</phrase> sources: all feature weights are learned jointly, using the conditional <phrase>maximum likelihood</phrase> (<phrase>CML</phrase>) <phrase>objective function</phrase>. In particular, SCRFs can combine <phrase>information</phrase> of different types, for example both <phrase>real valued</phrase> and <phrase>binary</phrase> features; at different granularities, for example at the frame, <phrase>phoneme</phrase> or word level of varying quality, for example from a <phrase>state</phrase>-of-the-<phrase>art</phrase> base-line and from less accurate <phrase>phoneme</phrase> or word detectors of varying degrees of completeness, for example a feature that detects just one word that may be redundant, for example from <phrase>phoneme</phrase> and <phrase>syllable</phrase> detectors
Infusing <phrase>software</phrase> assurance in <phrase>computing</phrase> curricula us. We need to keep up with, preferably in front of, technological and <phrase>social change</phrase>. What should we do when faced with the latest challenge (starting with <phrase>mobile</phrase> devices) to our <phrase>professional</phrase> world? We should embrace change. Each time we are faced with the latest " interference " with our teaching mission, we have an opportunity to think outside the box. Look for a way to use the <phrase>technology</phrase> to your advantage. Each time someone presents us with a truly off the wall <phrase>innovation</phrase> we have an opportunity to show mental flexibility and to experiment with something new and interesting. Who knows what we might learn? Thus it is with <phrase>smart phones</phrase> in the classroom. Annoying and disruptive as they are, they provide us an opportunity to stay on the <phrase>leading edge</phrase> of our field. We must poke, prod, dissect and peer closely at them. We must give a <phrase>fair</phrase> shake to all innovative ideas about addressing the challenge of <phrase>mobile</phrase> devices in the classroom. We have to be willing to try the weird. Often it takes guts. About that deep fried <phrase>Twinkie</phrase>? It was delicious. Ir Each time someone presents us with a truly off the wall <phrase>innovation</phrase> we have an opportunity to show mental flexibility and to experiment with something new and interesting. <phrase>computing</phrase> curricula accordIng to the computer emergency response team (cert) at the <phrase>Software Engineering Institute</phrase> (SEI) at <phrase>Carnegie Mellon University</phrase>, " nearly every facet of modern <phrase>society</phrase> depends heavily on highly complex <phrase>software</phrase> systems. The <phrase>business</phrase>, <phrase>energy</phrase>, transportation , <phrase>education</phrase>, <phrase>communication</phrase>, <phrase>government</phrase>, and defense communities rely on <phrase>software</phrase> to <phrase>function</phrase>, and <phrase>software</phrase> is an intrinsic part of our personal lives. <phrase>Software</phrase> assurance is an important discipline to ensure that <phrase>software</phrase> systems and services <phrase>function</phrase> dependably and are secure ". Toward assured and dependably <phrase>software</phrase> in modern <phrase>society</phrase>, the <phrase>ACM</phrase> Committee for <phrase>Computing</phrase> <phrase>Education</phrase> in <phrase>Community Colleges</phrase> (CCECC) partnered with the SEI to produce <phrase>Software</phrase> Assurance <phrase>Curriculum</phrase> Project Volume IV: <phrase>Community College</phrase> <phrase>Education</phrase> [1]. <phrase>Security</phrase> <phrase>Division</phrase> (NCSD), includes a review of related curricula suitable to <phrase>community colleges</phrase>, outcomes and a body of <phrase>knowledge</phrase> , expected <phrase>academic</phrase> backgrounds of <phrase>target</phrase> audiences, and outlines of six <phrase>undergraduate</phrase> courses. According to the <phrase>American</phrase> Association for <phrase>Community Colleges</phrase> [2], the mission of the <phrase>community college</phrase> sector is diverse: preparing students for transfer into four-year institutions, helping working adults prepare for new careers, as well as offering noncredit programs that offer 
Incorporating <phrase>Vicarious Learning</phrase> Environments with Discourse Scaffolds into <phrase>Physics</phrase> Classrooms The current study investigated the role of discourse scaffolds embedded into <phrase>vicarious learning</phrase> environments in classroom learning. In the current study, one of three <phrase>vicarious learning</phrase> conditions with varying levels of discourse scaffolds preceded standard instruction on seven classroom days. An explanation condition included deep questions, course content and explanations. In a question condition the explanations were deleted. A <phrase>monologue</phrase> condition contained only course content. The explanation condition participants showed significantly greater learning gains for <phrase>daily</phrase> sessions. <phrase>Results</phrase> are discussed in terms of presenting vicarious explanations and their role in classroom instruction. In today's classroom there is an increased demand to <phrase>cover</phrase> more material for more students while at the same time maintaining the same or higher levels of quality. <phrase>Vicarious learning</phrase> procedures in the classroom could provide a method for teachers to produce <phrase>high</phrase> quality material for their students to observe while freeing up the teachers resources to handle more individualized instruction or focusing on <phrase>deeper understanding</phrase> of materials for the overall classroom. The following studies provide the first validation steps for implementing <phrase>vicarious learning</phrase> within the classroom. Computer-based <phrase>vicarious learning</phrase> environments include those in which the learners see and/or hear content for which they are not the addressees and cannot physically interacting with the source of the content they are attempting to <phrase>master</phrase>. <phrase>Vicarious learning</phrase> was observed in humans by <phrase>Bandura</phrase> [2] while modeling aggressive behavior for children. Although Bandura's work on <phrase>vicarious learning</phrase> was associated more within the context of behavioral <phrase>psychology</phrase>, recent work in <phrase>education</phrase> <phrase>research</phrase> has focused on <phrase>cognitive processes</phrase> and <phrase>knowledge</phrase> <phrase>construction</phrase> in computer based environments [7]. These non-interactive <phrase>multimedia</phrase> environments that use auditory narration and images to improve learning have been referred to as a <phrase>vicarious learning</phrase> environment (See [7] for review). A primary mechanism required for learning within these environments is that the learner stay cognitively active during the learning process [3]. When a learner is cognitively active, they presumably are attending to the material and while processing the material are producing inferences about the material as well as linking new material to previous <phrase>knowledge</phrase>. However, it is important to provide
Deep Convex Net: A Scalable <phrase>Architecture</phrase> for Speech Pattern Classification We recently developed <phrase>context-dependent</phrase> DNN-HMM (Deep-<phrase>Neural-Net</phrase>/<phrase>Hidden-Markov-Model</phrase>) for large-<phrase>vocabulary</phrase> <phrase>speech recognition</phrase>. While achieving impressive recognition <phrase>error rate</phrase> reduction, we face the insurmountable problem of <phrase>scalability</phrase> in dealing with virtually unlimited amount of <phrase>training data</phrase> available nowadays. To overcome the <phrase>scalability</phrase> challenge, we have designed the deep convex network (DCN) <phrase>architecture</phrase>. The learning problem in DCN is convex within each module. Additional structure-exploited <phrase>fine tuning</phrase> further improves the quality of DCN. The full learning in DCN is batch-mode based instead of <phrase>stochastic</phrase>, naturally lending it amenable to parallel training that can be distributed over many machines. <phrase>Experimental</phrase> <phrase>results</phrase> on both MNIST and TIMIT tasks evaluated thus far demonstrate <phrase>superior</phrase> performance of DCN over the DBN (<phrase>Deep Belief</phrase> Network) counterpart that forms the basis of the DNN. The superiority is reflected not only in training <phrase>scalability</phrase> and <phrase>CPU</phrase>-only computation, but more importantly in <phrase>classification accuracy</phrase> in both tasks.
Extracting <phrase>Speaker</phrase>-Specific <phrase>Information</phrase> with a Regularized <phrase>Siamese</phrase> Deep Network Speech conveys different yet mixed <phrase>information</phrase> ranging from <phrase>linguistic</phrase> to <phrase>speaker</phrase>-specific components, and each of them should be exclusively used in a specific task. However, it is extremely difficult to extract a specific <phrase>information</phrase> component given the fact that nearly all existing <phrase>acoustic</phrase> representations carry all types of speech <phrase>information</phrase>. Thus, the use of the same representation in both speech and <phrase>speaker recognition</phrase> hinders a system from producing better performance due to interference of irrelevant <phrase>information</phrase>. In this <phrase>paper</phrase>, we present a deep neural <phrase>architecture</phrase> to extract <phrase>speaker</phrase>-specific <phrase>information</phrase> from MFCCs. As a result, a <phrase>multi-objective</phrase> <phrase>loss function</phrase> is proposed for learning <phrase>speaker</phrase>-specific characteristics and regularization via normalizing interference of non-<phrase>speaker</phrase> related <phrase>information</phrase> and avoiding <phrase>information</phrase> loss. With LDC benchmark corpora and a <phrase>Chinese</phrase> speech corpus, we demonstrate that a resultant <phrase>speaker</phrase>-specific representation is insensitive to text/languages spoken and environmental mismatches and hence outperforms MFCCs and other <phrase>state</phrase>-of-the-<phrase>art</phrase> techniques in <phrase>speaker recognition</phrase>. We discuss relevant issues and relate our approach to previous work.
<phrase>Knowledge</phrase> Matters: Importance of Prior <phrase>Information</phrase> for Optimization We explore the effect of introducing prior <phrase>information</phrase> into the intermediate level of deep supervised <phrase>neural networks</phrase> for a learning task on which all the <phrase>black</phrase>-box <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>machine learning</phrase> <phrase>algorithms</phrase> tested have failed to learn. We motivate our work from the <phrase>hypothesis</phrase> that there is an optimization obstacle involved in the <phrase>nature</phrase> of such tasks, and that humans learn useful intermediate concepts from other individuals via a form of supervision or guidance using a <phrase>curriculum</phrase>. The experiments we have conducted provide positive evidence in favor of this <phrase>hypothesis</phrase>. In our experiments, a two-tiered MLP <phrase>architecture</phrase> is trained on a dataset for which each image input contains three sprites, and the <phrase>binary</phrase> <phrase>target</phrase> class is 1 if all three have the same shape. <phrase>Black</phrase>-box <phrase>machine learning</phrase> <phrase>algorithms</phrase> only got chance on this task. Standard deep supervised <phrase>neural networks</phrase> also failed. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allows to nail the task. Much better than chance but imperfect <phrase>results</phrase> are also obtained by exploring <phrase>architecture</phrase> and optimization variants, pointing towards a difficult optimization task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with hypotheses on <phrase>cultural</phrase> learning inspired by the observations of effective <phrase>local minima</phrase> (possibly due to ill-conditioning and the training procedure not being able to escape what appears like a local minimum).
Identifying MicroRNAs and their Targets Abs tract Iw ill summarize what can be learned from predicting and analyzing <phrase>microRNA</phrase> targets. As an example, Iw ill discuss the <phrase>function</phrase> of <phrase>miR</phrase>-150 in the <phrase>immune</phrase> system. Finally, Iwill present anew <phrase>algorithm</phrase> for the identification of microRNAs from deep sequencing <phrase>data</phrase>.
Understanding <phrase>Email</phrase> <phrase>Communication</phrase> Patterns Signature Redacted Understanding <phrase>Email</phrase> <phrase>Communication</phrase> Patterns Understanding <phrase>Email</phrase> <phrase>Communication</phrase> Patterns Signature Redacted Signature Redacted It has been almost two decades since the beginning of the web. This means that the web is no longer just a <phrase>technology</phrase> of the present, but also, a record of our past. <phrase>Email</phrase>, one of the original forms of <phrase>social media</phrase>, is even older than the web and contains a detailed description of our personal and <phrase>professional</phrase> <phrase>history</phrase>. This <phrase>thesis</phrase> explores the world of <phrase>email</phrase> <phrase>communication</phrase> by introducing Immersion, a tool build for the purposes to analyze and visualize the <phrase>information</phrase> hidden behind the <phrase>digital</phrase> traces of <phrase>email</phrase> activity, to help us reflect on our actions, learn something new, quantify it, and hopefully make us react and change our behavior. In closing, I look over the <phrase>email</phrase> overload problem and work-<phrase>life</phrase> balance trends by quantifying <phrase>general</phrase> <phrase>email</phrase> usage using a large <phrase>real-world</phrase> <phrase>email</phrase> dataset. 3 ACKNOWLEDGMENTS I am truly grateful to so many people that it is impossible to acknowledge all of them, and I hope everyone that has helped me knows how extremely thankful I am to have you in my <phrase>life</phrase>. I would like to express a deep appreciation to my advisor Cesar Hidalgo for <phrase>giving me the opportunity</phrase> to come to <phrase>MIT</phrase>, my dream <phrase>school</phrase>. Thank you for continually conveying a <phrase>spirit</phrase> of <phrase>adventure</phrase> and excitement in regard to <phrase>research</phrase> and helping me broaden my horizon. Without your guidance and countless hours of persistent help, and your extreme availability, this <phrase>thesis</phrase> would have not been possible. I would also like to thank my readers Sep Kamvar and Alex Pentland for their valuable <phrase>feedback</phrase>. I'm very grateful to have you both as readers. I am and will always be grateful to my lifelong mentor and role <phrase>model</phrase> Ljupco Kocarev. I hope to collaborate with you again in the future. I would like to thank my <phrase>research</phrase> group Macro Connections. Thank you guys for being great teammates, officemates and <phrase>friends</phrase>. A special thank you to Deepak Jagdish, my equal partner in the project behind this <phrase>thesis</phrase>. Without you, Immersion would have never come to fruition. Thanks also to my best friend Alexandre Sahyoun. I was extremely lucky to have you as a roommate for the last 2 years. We helped each other, and pushed each other, in perfect balance. Thanks to my <phrase>friends</phrase> at <phrase>MIT</phrase>, and outside, for all the weekends and fun times we shared together. We worked hard and played hard. I will miss you all.
-term Project - Dialog <phrase>Act</phrase> Tagging Using <phrase>Memory</phrase>-based Learning We are applying a <phrase>memory</phrase> based learning (MBL) <phrase>algorithm</phrase> to the task of automatic dialog <phrase>act</phrase> (DA) tagging. This work is along the lines of a recent trend that considers MBL as being more appropriate for <phrase>natural language processing</phrase>. We did the experiments on the Switchboard corpus, overcome the problem of <phrase>feature selection</phrase> and yield <phrase>results</phrase> that seem to be better that previous reported <phrase>results</phrase> on the same corpus. A better understanding of the <phrase>semantics</phrase> of user utterances in a spoken dialog system (<phrase>SDS</phrase>) will <phrase>lead</phrase> to more efficient and robust <phrase>SDS</phrase>. Given the fact that deep <phrase>semantic</phrase> analysis of sentences still eludes us, researchers have turned their eyes to a simpler problem: shallow <phrase>semantics</phrase> analysis. Dialog <phrase>act</phrase> (DA) tagging is one type of shallow <phrase>semantics</phrase> analysis of sentences. DAs are a concise abstraction of utterance <phrase>semantics</phrase> and usually express the illocutionary force of the utterance (SUGGEST, ACCEPT, QUESTION etc.). Different DA schemes have been devised in recent years, some relevant to particular problems, other aiming for domain-<phrase>independence</phrase>. Notable is the DAMSL <phrase>architecture</phrase> created by Discourse Resource <phrase>Initiative</phrase> (Core and Allen 1997). This <phrase>architecture</phrase> was extended and used to tag a <phrase>human</phrase>-<phrase>human</phrase> conversation corpus, Switchboard. The resulting DA scheme is known as SWBD-DAMSL and contains approximately 220 different DAs. DAs were obtained by combining <phrase>basic</phrase> DAs with orthogonal functions of the utterance (like <phrase>task-management</phrase> related, <phrase>communication</phrase>-<phrase>management</phrase> related). Due to low <phrase>frequency</phrase> of some DAs, the tag set was further clustered to a set of 42 relevant DAs (Jurafsky et all, 1997). We used this dataset along with clustered SWBD-DAMSL <phrase>architecture</phrase> in our experiments. Even if it does a shallow understanding of the utterances, DA tagging is useful in many applications: detection dialog <phrase>game</phrase> boundaries, detecting the interaction dominance and discussion <phrase>genre</phrase>. Also, a meeting summarizer should be able to detect who spokes, to whom is the conversation addressed and if it asked a question or answered one. Even more, DAs may help the speech recognizer, by restricting its <phrase>grammar</phrase> to the one corresponding to the expected DA.
Anatomical phenotyping in a <phrase>mouse</phrase> <phrase>model</phrase> of fragile X syndrome with <phrase>magnetic resonance imaging</phrase> Fragile X Syndrome (FXS) is the most common <phrase>single</phrase> <phrase>gene</phrase> cause of inherited mental impairment, and <phrase>cognitive</phrase> deficits can <phrase>range</phrase> from simple learning disabilities to <phrase>mental retardation</phrase>. <phrase>Human</phrase> FXS is caused by a loss of the Fragile X <phrase>Mental Retardation</phrase> <phrase>Protein</phrase> (FMRP). The fragile X <phrase>knockout</phrase> (<phrase>FX</phrase> KO) <phrase>mouse</phrase> also shows a loss of FMRP, as well as many of the physical and behavioural characteristics of <phrase>human</phrase> FXS. This work aims to characterize the anatomical changes between the <phrase>FX</phrase> KO and a corresponding <phrase>wild type</phrase> <phrase>mouse</phrase>. Significant volume decreases were found in two regions within the deep <phrase>cerebellar</phrase> nuclei, namely the nucleus interpositus and the fastigial nucleus, which may be caused by a loss of <phrase>neurons</phrase> as indicated by <phrase>histological</phrase> analysis. Well-known links between these nuclei and previously established behavioural and physical characteristics of FXS are discussed. The loss of FMRP has a significant effect on these two nuclei, and future studies of FXS should evaluate the biochemical, <phrase>physiological</phrase>, and behavioral consequences of alterations in these key nuclei.
<phrase>Cooperative</phrase> <phrase>Problem-Based Learning</phrase> (<phrase>CPBL</phrase>): A Practical <phrase>PBL</phrase> <phrase>Model</phrase> for a Typical Course <phrase>Problem-Based Learning</phrase> (<phrase>PBL</phrase>) is an inductive learning approach that uses a realistic problem as the <phrase>starting point</phrase> of learning. Unlike in <phrase>medical</phrase> <phrase>education</phrase>, which is more easily adaptable to <phrase>PBL</phrase>, implementing <phrase>PBL</phrase> in <phrase>engineering</phrase> courses in the traditional semester system setup is challenging. While <phrase>PBL</phrase> is normally implemented in <phrase>small groups</phrase> of up to ten students with a dedicated <phrase>tutor</phrase> during <phrase>PBL</phrase> sessions in <phrase>medical</phrase> <phrase>education</phrase>, this is not plausible in <phrase>engineering</phrase> <phrase>education</phrase> because of the <phrase>high</phrase> enrolment and large class sizes. In a typical course, implementation of <phrase>PBL</phrase> consisting of students in <phrase>small groups</phrase> in medium to large classes is more practical. However, this type of implementation is more difficult to monitor, and thus requires good support and guidance in ensuring commitment and accountability of each <phrase>student</phrase> towards learning in his/her group. To provide the required support, <phrase>Cooperative Learning</phrase> (CL) is identified to have the much needed elements to develop the small <phrase>student</phrase> groups to functional learning teams. Combining both CL and <phrase>PBL</phrase> <phrase>results</phrase> in a <phrase>Cooperative</phrase> <phrase>Problem-Based Learning</phrase> (<phrase>CPBL</phrase>) <phrase>model</phrase> that provides a step by step guide for students to go through the <phrase>PBL</phrase> cycle in their teams, according to CL principles. Suitable for implementation in medium to large classes (approximately 40-60 students for one floating facilitator), with <phrase>small groups</phrase> consisting of 3-5 students, the <phrase>CPBL</phrase> <phrase>model</phrase> is designed to develop the students in the whole class into a learning <phrase>community</phrase>. This <phrase>paper</phrase> provides a detailed description of the <phrase>CPBL</phrase> <phrase>model</phrase>. A sample implementation in a third year <phrase>Chemical Engineering</phrase> course, <phrase>Process Control</phrase> and Dynamics, is also described. I. INTRODUCTION <phrase>Problem Based Learning</phrase> (<phrase>PBL</phrase>) gained worldwide interest as an innovative technique that engage learners for <phrase>deep learning</phrase>, and develop a multitude of crucial <phrase>professional</phrase> skills, especially self-<phrase>directed</phrase> learning and <phrase>problem solving</phrase> [1], which are essential in graduates for the <phrase>21 st Century</phrase> [2,3]. It has been used as in numerous fields, including <phrase>medicine</phrase>, <phrase>science</phrase>, <phrase>engineering</phrase> and <phrase>business</phrase> related fields [4]. In <phrase>engineering</phrase>, <phrase>PBL</phrase> is favoured particularly because it promotes <phrase>deep learning</phrase> and <phrase>problem-solving</phrase> skills [4,5,6]. Other <phrase>engineering</phrase> implementations also noted enhanced generic skills and promotion of positive attitude among students who had gone through <phrase>PBL</phrase> [7,8]. In <phrase>PBL</phrase>, unstructured problems are used as the <phrase>starting point</phrase> of learning, creating deep interests among students to learn new <phrase>knowledge</phrase> and integrate existing ones, and forcing them to think critically and creatively to solve the
<phrase>Deep Learning</phrase>-Based Goal Recognition in <phrase>Open-Ended</phrase> <phrase>Digital</phrase> <phrase>Games</phrase> While many <phrase>open-ended</phrase> <phrase>digital</phrase> <phrase>games</phrase> feature non-linear storylines and multiple <phrase>solution</phrase> paths, it is challenging for <phrase>game</phrase> developers to create effective <phrase>game</phrase> experiences in these settings due to the freedom given to the player. To address these challenges, goal recognition, a computational player-modeling task, has been investigated to enable <phrase>digital</phrase> <phrase>games</phrase> to dynamically predict players' goals. This <phrase>paper</phrase> presents a goal recognition framework based on stacked <phrase>denoising autoencoders</phrase>, a variant of <phrase>deep learning</phrase>. The learned goal recognition models, which are trained from a corpus of player interactions, not only offer improved performance, but also offer the substantial advantage of eliminating the need for <phrase>labor</phrase>-intensive feature <phrase>engineering</phrase>. An evaluation demonstrates that the <phrase>deep learning</phrase>-based goal recognition framework <phrase>significantly outperforms</phrase> the previous <phrase>state</phrase>-of-the-<phrase>art</phrase> goal recognition approach based on <phrase>Markov</phrase> <phrase>logic</phrase> networks.
Learning by <phrase>Blogging</phrase>: Warm-Up and Review Lessons to Facilitate <phrase>Knowledge</phrase> Building in Classrooms In a <phrase>knowledge</phrase>-building classroom, students and their <phrase>teacher</phrase> make a collective inquiry into each topic of the course. All students in the class aim to discover <phrase>knowledge</phrase> and create further <phrase>knowledge</phrase>. However, efficient discussion is hard to initiate in a <phrase>knowledge</phrase>-building classroom where the <phrase>teacher</phrase> is not aware of the <phrase>background knowledge</phrase> of students, or the students have not prepared for deep dialogue. The portfolios of building personal <phrase>knowledge</phrase> are not systematically accumulated for further extension. This study proposes a new strategy to enhance the performance of a <phrase>knowledge</phrase>-building classroom. By <phrase>blogging</phrase> the warm-up of a lesson before class, students are prepared for discussion. The <phrase>teacher</phrase> can preview the students' <phrase>blogs</phrase> to prepare the discussion, thus enhancing the performance of <phrase>knowledge</phrase>-building activities. This <phrase>article presents</phrase> the <phrase>design</phrase> of the learning strategy and the <phrase>blog</phrase> system, and discusses the <phrase>results</phrase> of a preliminary experiment.
<phrase>Design</phrase> of an online global learning <phrase>community</phrase>: international collaboration of grades 7-9 <phrase>science</phrase> students This <phrase>paper</phrase> describes the <phrase>design</phrase> decisions made in the <phrase>construction</phrase> of an online global learning <phrase>community</phrase> for grades 7-9 <phrase>science</phrase> students. The <phrase>collaborative learning</phrase> tools of class profiles, <phrase>student</phrase>-<phrase>scientist</phrase> forums, and <phrase>peer review</phrase> featured in the From Local to Extreme Environments <phrase>curriculum</phrase> are discussed in detail. Initial evaluation of these tools and <phrase>student</phrase> reactions to global collaborations in this ongoing study will be accomplished through <phrase>feedback</phrase> during the unit and embedded surveys. Following is a discussion of <phrase>design</phrase> decisions made in the development of three online tools featured in the From Local to Extreme Environments (FLEXE) project. The three tools (i.e. partner profiles, students-scientists forums, and online <phrase>peer review</phrase>) comprise the primary modes of interaction within the FLEXE online global learning <phrase>community</phrase> (<phrase>GLC</phrase>). The goals of the study are to characterize <phrase>student</phrase> involvement and reactions to online collaborations with diverse <phrase>peers</phrase> from different environments and to understand how <phrase>science</phrase> students use different scientific <phrase>data</phrase> sources as evidence in their written arguments. The project is currently in <phrase>pilot</phrase> use and evaluation. The on-line <phrase>GLC</phrase> presented in this <phrase>paper</phrase> is defined as the <phrase>student</phrase>, <phrase>teacher</phrase>, and <phrase>scientist</phrase> members of the FLEXE <phrase>community</phrase> that participate in FLEXE learning activities. These members represent diverse nationalities, <phrase>cultural</phrase> backgrounds, <phrase>native</phrase> languages, local environments, and <phrase>scientific knowledge</phrase> bases. The term global, in this study, means outside of an individual classroom. The FLEXE <phrase>GLC</phrase> is possible through the use of <phrase>web-based</phrase> <phrase>technology</phrase> that facilitates <phrase>communication</phrase> between schools and between schools and scientists in different parts of the world. Students in the FLEXE project are engaged in activities similar to the <phrase>science</phrase> <phrase>community</phrase>. Students may be challenged by <phrase>cultural</phrase> differences and multiple languages but are aided by the <phrase>universal</phrase> inquiry process of <phrase>science</phrase>. Context FLEXE is an <phrase>Earth</phrase> <phrase>Systems Science</phrase> Project developed in partnership with the Global Learning and Observations to Benefit the Environment program (GLOBE), a worldwide <phrase>web-based</phrase> <phrase>science</phrase> and <phrase>environmental education</phrase> program. The mission of GLOBE is to promote <phrase>teaching and learning</phrase> of <phrase>science</phrase>, enhance environmental <phrase>literacy</phrase> and stewardship, and promote scientific discovery in a worldwide <phrase>community</phrase> of students, teachers and scientists (The GLOBE Program, 2008). GLOBE emphasizes learning through <phrase>student</phrase> collection and analysis of environmental <phrase>data</phrase> using scientific protocols and newly developed <phrase>student</phrase> <phrase>research</phrase> tools. FLEXE is funded by the <phrase>U.S</phrase>. <phrase>National Science Foundation</phrase>. FLEXE expands the boundaries of the <phrase>science</phrase> classroom by directly involving students in a <phrase>GLC</phrase>. In FLEXE, students, teachers, and <phrase>deep-sea</phrase> scientists 
Celebrating AI's Fiftieth Anniversary and Continuing <phrase>Innovation</phrase> at the AAAI/IAAI-06 Conferences today's <phrase>keyboards</phrase>. The introduction of the <phrase>FORTRAN</phrase> <phrase>language</phrase> was still a year away. " The summer conference, convened on the <phrase>campus</phrase> of <phrase>Dartmouth College</phrase> in <phrase>Hanover</phrase>, New <phrase>Hampshire</phrase>, brought together for the first time many of the early pioneers of <phrase>cybernetics</phrase> , <phrase>automata</phrase> and <phrase>information theory</phrase> , <phrase>operations research</phrase>, and <phrase>game theory</phrase>, " according to the <phrase>Software</phrase> <phrase>History</phrase> <phrase>Dictionary</phrase> Project of the <phrase>Charles Babbage Institute</phrase>. " Attendees (of the <phrase>Dartmouth</phrase> Conference) adopted the following slogan as a <phrase>starting point</phrase> for their discussion, 'Every <phrase>aspect</phrase> of learning or any other feature of <phrase>intelligence</phrase> can in principle be so precisely described that a machine can be made to simulate it.' " This statement has essentially remained the <phrase>credo</phrase> of <phrase>artificial intelligence</phrase> <phrase>research</phrase> ever since. 1 first publicly demonstrated their landmark <phrase>Logic</phrase> Theorist at the conference. Considered by many to be the first <phrase>AI</phrase> program, it T he seeds of <phrase>AI</phrase> were sewn at the <phrase>Dartmouth</phrase> Conference in the summer of 1956. John <phrase>Mc</phrase>-Carthy, then an assistant <phrase>mathematics</phrase> <phrase>professor</phrase> at <phrase>Dartmouth</phrase>, organized the conference and coined the name " <phrase>artificial intelligence</phrase> " in his conference proposal. This summer AAAI celebrates the first 50 years of <phrase>AI</phrase>; and continues to foster the fertile fields of <phrase>AI</phrase> at the National <phrase>AI</phrase> conference (AAAI-06) and Innovative Applications of <phrase>AI</phrase> conference (IAAI-06) in <phrase>Boston</phrase>. The computer age was just dawning in 1956. <phrase>MIT</phrase> researchers that year built the <phrase>TX-0</phrase>, the first <phrase>general</phrase>-purpose, programmable computer with <phrase>transistors</phrase>. <phrase>IBM</phrase> shipped the first <phrase>magnetic</phrase> <phrase>disk storage</phrase>, the 305 RAMAC, composed of 50 magnetically coated <phrase>metal</phrase> <phrase>platters</phrase> with 5 million <phrase>bytes</phrase> of <phrase>data</phrase>. The first experiments were underway at <phrase>MIT</phrase> in direct <phrase>keyboard</phrase> input to the Whirlwind, breaking the ground for was the first time a computer proved abstract statements rather than just performing specific calculations. The <phrase>Logic</phrase> Theorist could independently prove 32 of the 58 <phrase>mathematical</phrase> theorems in <phrase>Bertrand Russell</phrase> and Alfred North Whitehead's <phrase>Principia</phrase> Mathe-matica. It ran on the JOHNNIAC computer , created by <phrase>John von Neumann</phrase>. The <phrase>Dartmouth</phrase> Conference atten-dees predicted that much would be achieved in the field by 1970; for instance , that a <phrase>digital</phrase> computer would become a <phrase>chess</phrase> <phrase>grandmaster</phrase>. In fact, it would take seven additional years to achieve this dream for it was not until 1997 that IBM's <phrase>Deep Blue</phrase> beat then reigning <phrase>World Chess Champion</phrase> <phrase>Garry Kasparov</phrase>. Other hypotheses from the conference, like <phrase>computers</phrase> understanding <phrase>spoken language</phrase>, are still only partially realized. 
Minteract: Online Tool for <phrase>Sustainable</phrase> Active Experiential <phrase>Mobile</phrase> Learning The rapid <phrase>evolution</phrase> and ubiquitous use of <phrase>mobile</phrase> devices is an historical opportunity to improve experiential <phrase>interactivity</phrase> in <phrase>education</phrase> practices to support 'deep' learning. A <phrase>major</phrase> barrier to the widespread <phrase>adoption</phrase> of <phrase>mobile</phrase> learning in <phrase>higher education</phrase> is that of cost. Opportunities to overcome this barrier include the <phrase>high</phrase> rate of ownership of <phrase>mobile phones</phrase> by <phrase>university</phrase> students and technological solutions such as packet transmission technologies. mInteract is an online system which uses packet <phrase>technology</phrase> to build no-to-<phrase>low cost</phrase> <phrase>interactivity</phrase> into learning spaces. mInteract supports <phrase>sustainable</phrase> active <phrase>experiential learning</phrase> transactions for both <phrase>student</phrase> and <phrase>teacher</phrase>.
Self learning machines using Deep Networks Self learning machines as defined in this <phrase>paper</phrase> are those learning by observation under limited supervision, and continuously adapt by observing the surrounding environment. The aim is to mimic the behavior of <phrase>human brain</phrase> learning from surroundings with limited supervision, and adapting its learning according to input sensory observations. Recently, <phrase>Deep Belief</phrase> Nets (DBNs) [1] have made good use of <phrase>unsupervised learning</phrase> as <phrase>pre-training</phrase> stage, which is equivalent to the observation stage in humans. However, they still need supervised <phrase>training set</phrase> to adjust the network parameters, as well as being non-adaptive to <phrase>real world</phrase> examples. In this <phrase>paper</phrase>, Self Learning Machine (SLM) is proposed based on <phrase>deep belief</phrase> networks and deep <phrase>auto encoders</phrase>
DHSNet: Deep Hierarchical Saliency Network for Salient <phrase>Object Detection</phrase> Traditional 1 salient <phrase>object detection</phrase> models often use <phrase>hand-crafted</phrase> features to formulate contrast and various <phrase>prior knowledge</phrase>, and then combine them artificially. In this work, we propose a novel <phrase>end-to-end</phrase> deep hierarchical saliency network (DHSNet) based on <phrase>convolutional neural networks</phrase> for detecting salient objects. DHSNet first makes a coarse global prediction by automatically learning various global structured saliency cues, including global contrast, objectness, compactness, and their optimal combination. Then a novel hierarchical recurrent <phrase>convolutional neural network</phrase> (HRCNN) is adopted to further hierarchically and progressively refine the details of saliency maps step by step via integrating local context <phrase>information</phrase>. The whole <phrase>architecture</phrase> works in a global to local and coarse to fine manner. DHSNet is directly trained using whole images and corresponding <phrase>ground truth</phrase> saliency masks. When testing, saliency maps can be generated by directly and efficiently feedforwarding testing images through the network, without relying on any other techniques. Evaluations on four <phrase>benchmark datasets</phrase> and comparisons with other 11 <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>algorithms</phrase> demonstrate that DHSNet not only shows its significant superiority in terms of performance, but also achieves a real-time speed of 23 <phrase>FPS</phrase> on modern <phrase>GPUs</phrase>.
<phrase>Stochastic</phrase>-Based <phrase>Convolutional Networks</phrase> with Reconfigurable <phrase>Logic</phrase> Fabric (Abstract Only) <phrase>Large-scale</phrase> <phrase>convolutional neural network</phrase> (<phrase>CNN</phrase>), well-known to be computationally intensive, is a fundamental algorithmic <phrase>building block</phrase> in many <phrase>computer vision</phrase> and <phrase>artificial intelligence</phrase> applications that follow the <phrase>deep learning</phrase> principle. This work presents a novel <phrase>stochastic</phrase>-based and scalable hardware <phrase>architecture</phrase> and circuit <phrase>design</phrase> that computes a <phrase>convolutional neural network</phrase> with <phrase>FPGA</phrase>. The key idea is to implement a <phrase>multi-dimensional</phrase> <phrase>convolution</phrase> <phrase>accelerator</phrase> that leverages the widely-used <phrase>convolution theorem</phrase>. Our approach has three advantages. First, it can achieve significantly <phrase>lower</phrase> algorithmic complexity for any given accuracy requirement. This <phrase>computing</phrase> complexity, when compared with that of conventional multiplierbased and <phrase>FFT</phrase>-based architectures, represents a significant <phrase>performance improvement</phrase>. Second, this proposed <phrase>stochastic</phrase>-based <phrase>architecture</phrase> is highly <phrase>fault-tolerant</phrase> because the <phrase>information</phrase> to be processed is encoded with a large ensemble of random samples. As such, the local perturbations of its <phrase>computing</phrase> accuracy will be dissipated globally, thus becoming inconsequential to the final overall <phrase>results</phrase>. Overall, being highly scalable and <phrase>energy</phrase> efficient, our <phrase>stochastic</phrase>-based <phrase>convolutional neural network</phrase> <phrase>architecture</phrase> is well-suited for a modular vision <phrase>engine</phrase> with the goal of performing real-time detection, recognition and segmentation of mega-<phrase>pixel</phrase> images, especially those <phrase>perception</phrase>-based <phrase>computing</phrase> tasks that are inherently <phrase>fault-tolerant</phrase>. We also present a performance comparison between <phrase>FPGA</phrase> implementations that use deterministic-based and <phrase>Stochastic</phrase>-based architectures.
New generation <phrase>E</phrase>-Learning <phrase>technology</phrase> by <phrase>Web Services</phrase> This <phrase>paper</phrase> discusses a new approach to build infrastructures for <phrase>E</phrase>-Learning systems for Learning <phrase>Software</phrase> Organizations on the basis of <phrase>Web Services</phrase>. A requirements context is developed to determine which type of <phrase>E</phrase>-Learning applications that can be <phrase>Web Service</phrase> Enabled. This is illustrated with a <phrase>case study</phrase> on an Encapsulated <phrase>Software</phrase> Teaching Environment. Additional facilities, such as didactical agents and deep <phrase>personalization</phrase> to facilitate Learning <phrase>Software</phrase> Organizations are discussed at the end.
An analysis of performance of <phrase>feed forward</phrase> <phrase>neural network</phrase>: using back propagation <phrase>learning algorithm</phrase> In some practical applications <phrase>Neural Network</phrase> (NN), a fast response to external events within extremely <phrase>short</phrase> <phrase>period</phrase> is required. However, using back propagation (<phrase>BP</phrase>) based on <phrase>gradient descent</phrase> optimization method obviously not satisfy many applications because of serious problems with <phrase>BP</phrase> are slow convergence speed of learning and containment low minima. Over the years, many improvements and modifications to the <phrase>learning algorithm</phrase> <phrase>BP</phrase> have been reported. In this <phrase>research</phrase>, we modified existing <phrase>BP</phrase> <phrase>learning algorithm</phrase> with adaptive gain adaptive change the <phrase>momentum</phrase> factor and learning rate. Learning patterns, <phrase>simulation</phrase> <phrase>results</phrase> indicate that the <phrase>proposed algorithm</phrase> can accelerate the convergence behavior and drag the network through deep <phrase>local minima</phrase> compared to the conventional <phrase>BP</phrase> <phrase>algorithm</phrase>. This work focuses upon the training parameters to attain optimal <phrase>solution</phrase> for <phrase>neural network</phrase>. 1. INTRODUCTION A <phrase>neural network</phrase> is called a mapping network if it is able to compute some functional relationship between its input and output. The <phrase>Neural Networks</phrase> are patterned after the <phrase>parallel processing</phrase> methods of the <phrase>human brain</phrase>. The biological <phrase>brain</phrase> is composed of billions of interconnected processing elements called <phrase>neurons</phrase>, which transmit <phrase>information</phrase> and strengthen when the <phrase>brain</phrase> learns. <phrase>Neural Networks</phrase> use interconnected processing elements that allow them to learn from mistakes, learn from example, recognize patterns in noisy <phrase>data</phrase>, and operate with incomplete <phrase>information</phrase>. By evaluating the processing capabilities of the <phrase>human brain</phrase>, <phrase>neural networks</phrase> attempt to overcome the limitations of traditional <phrase>computers</phrase>. An <phrase>Artificial Neural Network</phrase> (ANN) is a <phrase>model</phrase> composed of several highly interconnected computational units called <phrase>neurons</phrase> or nodes. Each node performs a simple operation on an input to generate an output that is forwarded to the next node in the <phrase>sequence</phrase>. This <phrase>parallel processing</phrase> allows for great advantages in <phrase>data analysis</phrase>. <phrase>Artificial Neural Network</phrase> are widely used in various branches of <phrase>engineering</phrase> and <phrase>science</phrase> and their <phrase>property</phrase> to approximate complex and nonlinear equations makes it a useful tools in <phrase>econometric</phrase> analysis. Previous <phrase>research</phrase> has shown that <phrase>artificial neural networks</phrase> are suitable for <phrase>pattern recognition</phrase> and pattern <phrase>classification tasks</phrase> due to their nonlinear nonparametric adaptive-learning properties. <phrase>Neural networks</phrase> are so new that standard <phrase>mathematical notation</phrase> and <phrase>architectural</phrase> representations for them have not yet been firmly established. In addition, papers and works on <phrase>neural networks</phrase> have come from many diverse fields, including <phrase>engineering</phrase>, <phrase>physics</phrase>, <phrase>psychology</phrase> and <phrase>mathematics</phrase>, and many authors tend to use <phrase>vocabulary</phrase> peculiar to their specialty. As a result, many works and papers in this field 
Howard Gardner's <phrase>Theory of Multiple Intelligences</phrase> Many of us are familiar with three <phrase>general</phrase> categories in which people learn: visual learners, auditory learners, and kinesthetic learners. Beyond these three <phrase>general</phrase> categories, many theories of and approaches toward <phrase>human</phrase> potential have been developed. Among them is the <phrase>theory of multiple intelligences</phrase>, developed Gardner's early work in <phrase>psychology</phrase> and later in <phrase>human</phrase> <phrase>cognition</phrase> and <phrase>human</phrase> potential <phrase>led</phrase> to the development of the initial six intelligences. Today there are nine intelligences and the possibility of others may eventually expand the list. These intelligences (or competencies) relate to a person's unique aptitude set of capabilities and ways they might prefer to demonstrate intellectual abilities. 1. Verbal-<phrase>linguistic</phrase> <phrase>intelligence</phrase> (well-developed verbal skills and sensitivity to the sounds, meanings and rhythms of words) 2. Logical-<phrase>mathematical</phrase> <phrase>intelligence</phrase> (ability to think conceptually and abstractly, and capacity to discern logical and numerical patterns) 3. Spatial-visual <phrase>intelligence</phrase> (capacity to think in images and pictures, to visualize accurately and abstractly) 4. Bodily-kinesthetic <phrase>intelligence</phrase> (ability to control one's body movements and to handle objects skillfully) 5. <phrase>Musical</phrase> intelligences (ability to produce and appreciate <phrase>rhythm</phrase>, pitch and <phrase>timber</phrase>) 6. Interpersonal <phrase>intelligence</phrase> (capacity to detect and respond appropriately to the moods, motivations and desires of others) 7. Intrapersonal (capacity to be self-aware and in tune with inner feelings, values, beliefs and thinking processes) 8. <phrase>Naturalist</phrase> <phrase>intelligence</phrase> (ability to recognize and categorize <phrase>plants</phrase>, animals and other objects in <phrase>nature</phrase>) 9. Existential <phrase>intelligence</phrase> (sensitivity and capacity to tackle deep questions about <phrase>human</phrase> existence such as, What is the meaning of <phrase>life</phrase>? Why do we die? How did we get here? (Source: Thirteen ed online, 2004) <phrase>Human</phrase> Potential <phrase>Human</phrase> potential can be tied to one's preferences to learning; thus, Gardner's focus on <phrase>human</phrase> potential lies in the fact that people have a unique blend of capabilities and skills (intelligences). This <phrase>model</phrase> can be used to understand " overall personality, preferences and strengths " (businessballs.com, n.d.). Gardner asserts that people who have an affinity toward one of the intelligences do so in <phrase>concert</phrase> with the other intelligences as " they develop skills and <phrase>solve problems</phrase> " (businessballs.com, 2009). <phrase>Human</phrase> potential can be tied to one's preferences to learning
Learning <phrase>long</phrase>-<phrase>range</phrase> vision for autonomous off-<phrase>road</phrase> driving Most vision-based approaches to <phrase>mobile</phrase> <phrase>robotics</phrase> suffer from the limitations imposed by <phrase>stereo</phrase> obstacle detection, which is <phrase>short</phrase>-<phrase>range</phrase> and prone to failure. We present a self-<phrase>supervised learning</phrase> process for <phrase>long</phrase>-<phrase>range</phrase> vision that is able to accurately classify complex terrain at distances up to the horizon, thus allowing <phrase>superior</phrase> strategic planning. The success of the learning process is due to the self-supervised <phrase>training data</phrase> that is generated on every frame: robust, visually consistent <phrase>labels</phrase> from a <phrase>stereo</phrase> module, normalized wide-context input <phrase>windows</phrase>, and a discrimina-tive and concise <phrase>feature representation</phrase>. A deep hierarchical network is trained to extract informative and meaningful features from an input image, and the features are used to <phrase>train</phrase> a realtime classifier to predict traversability. The trained classifier sees obstacles and paths from 5 to over 100 meters, far beyond the maximum <phrase>stereo</phrase> <phrase>range</phrase> of 12 meters, and adapts very quickly to new environments. The process was developed and tested on the LAGR <phrase>mobile robot</phrase>. <phrase>Results</phrase> from a <phrase>ground truth</phrase> dataset are given as well as field <phrase>test</phrase> <phrase>results</phrase>.
<phrase>General</phrase>-pupose <phrase>technology</phrase> for a <phrase>general</phrase>-purpose <phrase>nervous</phrase> system The <phrase>nervous</phrase> system is a one-<phrase>trick pony</phrase>, using <phrase>general</phrase>-purpose <phrase>neurons</phrase> with the same <phrase>basic</phrase> <phrase>signal transduction</phrase>, transmission and integration mechanisms to handle essentially all <phrase>information processing</phrase> needs in the body: sensation and <phrase>perception</phrase>, posture and movement, <phrase>autonomic</phrase> and visceral <phrase>function</phrase>, <phrase>memory</phrase> and learning. Over the past fifty years, scientists and engineers have developed many different interfaces between <phrase>neurons</phrase> and <phrase>electronic</phrase> instrumentation in <phrase>order</phrase> to study how individual subsystems work and to fix some of them when they malfunction (e.g. pacemakers, <phrase>cochlear implants</phrase>, deep <phrase>brain</phrase> stimulators, etc.). While the various interfaces and their applications may look different, they are all based on strikingly similar, fundamental principles of <phrase>biophysics</phrase>, <phrase>electrochemistry</phrase> and <phrase>information theory</phrase>, and enabled by similar <phrase>microfabrication</phrase> and microelectronic technologies. Neural control is gradually converging on principles of <phrase>design</phrase> and best practices that can and should give rise to <phrase>engineering</phrase> standards and interchangeable components for recurring functions such as bioelectric <phrase>recording</phrase> and stimulation, transmission of power and <phrase>data</phrase>, and physical packaging and <phrase>user interfaces</phrase>. As such <phrase>general</phrase> tools become available, the clinical applications will be limited only by our understanding of the underlying pathologies, which are often best studied by those same tools. This virtuous <phrase>circle</phrase> consists of accessible <phrase>technology</phrase> enabling <phrase>basic</phrase> <phrase>science</phrase> enabling clinical applications generating <phrase>business</phrase> success motivating yet more <phrase>technology</phrase>.
<phrase>Transfer Learning</phrase> by Reusing Structured <phrase>Knowledge Transfer</phrase> learning aims to solve new learning problems by extracting and making use of the common <phrase>knowledge</phrase> found in related domains. A key element of <phrase>transfer learning</phrase> is to identify structured <phrase>knowledge</phrase> to enable the <phrase>knowledge transfer</phrase>. Structured <phrase>knowledge</phrase> comes in different forms, depending on the <phrase>nature</phrase> of the learning problem and characteristics of the domains. In this article, we describe three of our recent works on <phrase>transfer learning</phrase> in a progressively more sophisticated <phrase>order</phrase> of the structured <phrase>knowledge</phrase> being transferred. We show that optimization methods, and techniques inspired by the concerns of <phrase>data</phrase> reuse can be applied to extract and transfer deep structural <phrase>knowledge</phrase> between a <phrase>variety</phrase> of source and <phrase>target</phrase> problems. In our examples, this <phrase>knowledge</phrase> spans explicit <phrase>data</phrase> <phrase>labels</phrase>, <phrase>model</phrase> parameters, relations between <phrase>data</phrase> clusters and <phrase>relational</phrase> <phrase>action</phrase> descriptions.
Finding Opinionated <phrase>Blogs</phrase> Using Statistical Classifiers and Lexical Features This <phrase>paper</phrase> systematically exploited various lexical features for opinion analysis on <phrase>blog</phrase> <phrase>data</phrase> using a statistical learning framework. Our <phrase>experimental</phrase> <phrase>results</phrase> using the TREC <phrase>Blog</phrase> <phrase>track</phrase> <phrase>data</phrase> show that all the features we explored effectively represent opinion expressions, and different classification strategies have a significant impact on opinion classification performance. We also present <phrase>results</phrase> when combining opinion analysis with the retrieval component for the task of retrieving relevant and opinionated <phrase>blogs</phrase>. Compared with the best <phrase>results</phrase> in the TREC evaluation, our system achieves reasonable performance, but does not rely on much <phrase>human</phrase> <phrase>knowledge</phrase> or deep level <phrase>linguistic</phrase> analysis.
<phrase>Parallel Processing</phrase> System <phrase>Design</phrase> with "<phrase>Propeller</phrase>" Processor This is a technical and <phrase>experimental</phrase> <phrase>report</phrase> of <phrase>parallel processing</phrase>, using the "<phrase>Propeller</phrase>" chip. Its eight 32 <phrase>bits</phrase> processors (cogs) can operate simultaneously, either independently or cooperatively, sharing common resources through a central hub. I introduce this unique processor and discuss about the possibility to develop interactive systems and smart interfaces in <phrase>media</phrase> <phrase>arts</phrase>, because we need many kinds of tasks at a same time with NIME-related systems and installations. I will <phrase>report</phrase> about (1) <phrase>Propeller</phrase> chip and its powerful <phrase>IDE</phrase>, (2) external interfaces for analog/<phrase>digital</phrase> inputs/outputs, (3) <phrase>VGA</phrase>/ <phrase>NTSC</phrase>/<phrase>PAL</phrase> <phrase>video</phrase> generation, (4) <phrase>audio signal processing</phrase>, and (5) originally-developed <phrase>MIDI</phrase> <phrase>input/output</phrase> method. I also introduce three <phrase>experimental</phrase> <phrase>prototype</phrase> systems. 1.Introduction <phrase>Propeller</phrase> [1] is supported by <phrase>Parallax</phrase> Inc. With its internal eight processors, we have full control over how and when each cog is employed; there is no <phrase>compiler</phrase>-driven or operating system-driven splitting of tasks among multiple cogs. A shared system <phrase>clock</phrase> keeps each cog on the same time reference, allowing for true deterministic timing and synchronization. We can use two <phrase>programming languages</phrase>: the easy-to-learn <phrase>high</phrase>-level <phrase>Spin</phrase>, and <phrase>Propeller</phrase> <phrase>Assembly</phrase> which can execute at up to 160 MIPS (20 MIPS per cog). There is a popular technique "Interrupt" to realize <phrase>multi-task</phrase> with all <phrase>CPU</phrase>. However, <phrase>Propeller</phrase> doesn't have the "Interrupt" because <phrase>parallel processing</phrase> is controlled by its special hardware. Its resources-common memories (32KB <phrase>RAM</phrase> /ROM) and 32 external I/O pins-are automatically assigned to round-switched cogs. We can easily make <phrase>parallel processing</phrase> <phrase>software</phrase> for <phrase>Propeller</phrase> by the smart and powerful <phrase>IDE</phrase>, without special consideration for synchronization. Because I have no enough space to introduce more both Propeller's languages and Propeller's <phrase>IDE</phrase> here, please refer my analyzing/ experiments <phrase>report</phrase> [2]. This <phrase>website</phrase> is currently only in <phrase>Japanese</phrase>, but the <phrase>English</phrase> version will be available. <phrase>Propeller</phrase> has 32 I/O pins that can be accessed by each cog with double or more monitoring and overwriting. Each cog has special timing circuits for counter/timer modes. 2.1MIDI <phrase>input / output</phrase> <phrase>Propeller</phrase> can deal serial communications like <phrase>MIDI</phrase> only by <phrase>software</phrase>, without special hardware like <phrase>UART</phrase>. There is a sample <phrase>MIDI</phrase>-in object in the <phrase>Parallax</phrase> web <phrase>page [3</phrase>], but I arranged and developed the <phrase>universal</phrase> <phrase>MIDI</phrase>-in/<phrase>MIDI</phrase>-out module [2]. This module deals <phrase>MIDI</phrase> <phrase>information</phrase> with deep Rx/<phrase>Tx</phrase> <phrase>FIFO</phrase> buffers in common <phrase>memory</phrase> in the chip, so it is easy to make intercommunication of each cog. Figure 1 shows the original circuits with <phrase>MIDI</phrase> I/O, audio D/A and <phrase>NTSC</phrase> <phrase>video</phrase> 
Parallel Training for Deep Stacking Networks The Deep Stacking Network (DSN) is a special type of <phrase>deep architecture</phrase> developed to enable and benefit from parallel learning of its <phrase>model</phrase> parameters on large <phrase>CPU</phrase> clusters. As a prospective key component of future speech recognizers, the <phrase>architectural</phrase> <phrase>design</phrase> of the DSN and its parallel training endow the DSN with <phrase>scalability</phrase> over a vast amount of <phrase>training data</phrase>. In this <phrase>paper</phrase>, we present our first parallel implementation of the DSN training <phrase>algorithm</phrase>. Particularly, we show the tradeoff between the time/<phrase>memory</phrase> saving via training parallelism and the associated cost arising from inter-<phrase>CPU</phrase> <phrase>communication</phrase>. Further, in phone classification experiments, we demonstrate a significantly lowered <phrase>error rate</phrase> using parallel full-batch training distributed over a <phrase>CPU</phrase> cluster, compared with sequential <phrase>mini</phrase>-batch training implemented in a <phrase>single</phrase> <phrase>CPU</phrase> machine under otherwise identical <phrase>experimental</phrase> conditions and as exploited prior to the work reported in this <phrase>paper</phrase>.
<phrase>Kernel Methods</phrase> for <phrase>Deep Learning</phrase> We introduce a new <phrase>family</phrase> of <phrase>positive-definite</phrase> kernel functions that mimic the computation in large, multilayer <phrase>neural nets</phrase>. These kernel functions can be used in shallow architectures, such as <phrase>support vector machines</phrase> (SVMs), or in deep <phrase>kernel-based</phrase> architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of <phrase>deep architectures</phrase>. On several problems, we obtain better <phrase>results</phrase> than previous, leading benchmarks from both SVMs with Gaussian kernels as well as <phrase>deep belief</phrase> nets.
<phrase>Shark</phrase>: <phrase>Sql</phrase> and Analytics with Cost-based Query Optimization on <phrase>Coarse-grained</phrase> <phrase>Distributed Memory</phrase> <phrase>Shark</phrase>: <phrase>Sql</phrase> and Analytics with Cost-based Query Optimization on <phrase>Coarse-grained</phrase> <phrase>Distributed Memory</phrase> Permission to make <phrase>digital</phrase> or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or <phrase>commercial advantage and that copies bear</phrase> this notice and the full citation on the first page. To copy otherwise, to republish, to <phrase>post on servers</phrase> or to redistribute to lists, <phrase>requires prior specific permission</phrase>. ABSTRACT <phrase>Shark</phrase> is a <phrase>research</phrase> <phrase>data analysis</phrase> system built on a novel <phrase>coarse-grained</phrase> <phrase>distributed shared-memory</phrase> abstraction. <phrase>Shark</phrase> pairs query processing with deep <phrase>data analysis</phrase>, providing a unified system for easy <phrase>data</phrase> manipulation using <phrase>SQL</phrase> while pushing sophisticated analysis closer to its <phrase>data</phrase>. It scales to thousands of nodes in a <phrase>fault-tolerant</phrase> manner. <phrase>Shark</phrase> can answer queries over 40 times faster than <phrase>Apache</phrase> Hive and run <phrase>machine learning</phrase> programs on <phrase>large datasets</phrase> over 25 times faster than equivalent <phrase>MapReduce</phrase> programs on <phrase>Apache Hadoop</phrase>. Unlike previous systems, <phrase>Shark</phrase> shows that it is possible to achieve these speedups while retaining a <phrase>MapReduce</phrase>-like execution <phrase>engine</phrase>, with the <phrase>fine-grained</phrase> <phrase>fault tolerance</phrase> properties that such an <phrase>engine</phrase> provides. <phrase>Shark</phrase> additionally provides several extensions to its <phrase>engine</phrase>, including table and column-level <phrase>statistics</phrase> collection as well as a cost-based optimizer, both of which we describe in depth in this <phrase>paper</phrase>. Cost-based query optimization in some cases improves the performance of queries with multiple joins by <phrase>orders of magnitude</phrase> over Hive and over 2 compared to previous versions of <phrase>Shark</phrase>. The result is a system that matches the reported speedups of MPP analytic <phrase>databases</phrase> against <phrase>MapReduce</phrase>, while providing more <phrase>comprehensive</phrase> <phrase>fault tolerance</phrase> and complex analytics capabilities.
<phrase>Benchmarking</phrase> <phrase>State</phrase>-of-the-<phrase>Art</phrase> <phrase>Deep Learning</phrase> <phrase>Software</phrase> Tools <phrase>Deep learning</phrase> has been shown as a successful <phrase>machine learning</phrase> method for a <phrase>variety</phrase> of tasks, and its popularity <phrase>results</phrase> in numerous <phrase>open-source</phrase> <phrase>deep learning</phrase> <phrase>software</phrase> tools coming to <phrase>public</phrase>. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in <phrase>deep learning</phrase>, many tools exploit hardware features such as <phrase>multi-core</phrase> CPUs and many-core <phrase>GPUs</phrase> to shorten the training time. However, different tools exhibit different features and running performance when training different types of deep networks on different hardware platforms, which makes it difficult for end users to select an appropriate pair of <phrase>software</phrase> and hardware. In this <phrase>paper</phrase>, we aim to make a comparative study of the <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>GPU</phrase>-accelerated <phrase>deep learning</phrase> <phrase>software</phrase> tools, including Caffe, CNTK, TensorFlow, and Torch. We benchmark the running performance of these tools with three popular types of <phrase>neural networks</phrase> on two <phrase>CPU</phrase> platforms and three <phrase>GPU</phrase> platforms. Our contribution is twofold. First, for end users of <phrase>deep learning</phrase> <phrase>software</phrase> tools, our <phrase>benchmarking</phrase> <phrase>results</phrase> can serve as a guide to selecting appropriate hardware platforms and <phrase>software</phrase> tools. Second, for developers of <phrase>deep learning</phrase> <phrase>software</phrase> tools, our in-depth analysis points out possible future directions to further optimize the running performance.
Notes on <phrase>Discrete Mathematics</phrase> for Computer Scientists 7 <phrase>Strings</phrase> 59 CONTENTS iii iv CONTENTS Preface <phrase>Discrete mathematics</phrase> is a required course in the <phrase>undergraduate</phrase> <phrase>Computer Science</phrase> <phrase>curriculum</phrase>. In a perhaps unsympathetic view, the standard presentations (and there are many)the material in the course is treated as a discrete collection of so many techniques that the students must <phrase>master</phrase> for further studies in <phrase>Computer Science</phrase>. Our <phrase>philosophy</phrase>, and the one embodied in this <phrase>book</phrase> is different. Of course the development of the students abilities to do <phrase>logic</phrase> and proofs, to know about <phrase>naive set theory</phrase>, relations, functions, <phrase>graphs</phrase>, inductively defined structures, definitions by <phrase>recursion</phrase> on inductively defined structures and <phrase>elementary</phrase> <phrase>combinatorics</phrase> is important. But we believe that rather than so many assorted topics and techniques to be learned, the course can flow continuously as a <phrase>single</phrase> <phrase>narrative</phrase>, each topic linked by a formal presentation building on previous topics. We believe that <phrase>Discrete Mathematics</phrase> is perhaps the most intellectually exciting and potentially one of the most interesting courses in the <phrase>computer science</phrase> <phrase>curriculum</phrase>. Rather than a simply viewing the course as a necessary tool for further, and perhaps more interesting developments to come later, we believe it is the place in the <phrase>curriculum</phrase> that an appreciation of the deep ideas of <phrase>computer science</phrase> can be presented; the relation between <phrase>syntax</phrase> and <phrase>semantics</phrase>, how it is that unbounded structures can be defined finitely and how to reason about those structure and how to calculate with them. Most texts, following perhaps standard <phrase>mathematical</phrase> practice, attempt to minimize the formalism, assuming that a students intuition will guide them through to the end, often avoiding proofs in favor of examples 1 <phrase>Mathematical</phrase> intuition is an entirely <phrase>variable</phrase> personal attribute, and even individuals with significant talents can be misguided by intuition. This is shown over and over in the <phrase>history</phrase> of <phrase>mathematics</phrase>; the <phrase>history</phrase> of the characterization of <phrase>infinity</phrase> is a prime example, but many others exist like the Tarski-Banach <phrase>paradox</phrase> [?]. We do not argue that intuition should be banished from teaching <phrase>mathematics</phrase> but instead that the <phrase>discrete mathematics</phrase> course is a place in the <phrase>curriculum</phrase> to cultivate the idea, useful in higher <phrase>mathematics</phrase> and in <phrase>computer science</phrase>, that formalism is trustworthy and can be used to verify intuition. Indeed, we believe, contrary to the common conception, that rather than making the material more opaque, a formal presentation gives the students a way to understand the material in a deeper and more satisfying 
<phrase>Face Recognition</phrase> Based on <phrase>Deep Neural Network</phrase> In modern <phrase>life</phrase>, we see more techniques of biometric features recognition have been used to our surrounding <phrase>life</phrase>, especially the applications in <phrase>telephones</phrase> and <phrase>laptops</phrase>. These biometric recognition techniques contain <phrase>face recognition</phrase>, <phrase>fingerprint recognition</phrase> and <phrase>iris recognition</phrase>. Our work focuses on the <phrase>face recognition</phrase> problem and uses a <phrase>deep learning</phrase> method, <phrase>convolutional neural network</phrase>, to solve it. And we use the Sobel operator to improve our result accuracy. LFW dataset is used for training and testing which gets a considerable result. And we also <phrase>test</phrase> our system on other face dataset, which also has a <phrase>high</phrase> accuracy on the recognition.
A <phrase>Large-Scale</phrase> <phrase>Architecture</phrase> for <phrase>Restricted Boltzmann Machines</phrase> <phrase>Deep Belief</phrase> Nets (DBNs) are an emerging application in the <phrase>machine learning</phrase> domain, which use <phrase>Restricted Boltzmann Machines</phrase> (RBMs) as their <phrase>basic</phrase> <phrase>building block</phrase>. Although <phrase>small scale</phrase> DBNs have shown great potential, the computational cost of RBM training has been a <phrase>major</phrase> challenge in scaling to large networks. In this <phrase>paper</phrase> we present a highly scalable <phrase>architecture</phrase> for <phrase>Deep Belief</phrase> Net processing on hardware systems that can handle hundreds of boards, if not more, of customized <phrase>logic</phrase> with near linear performance increase. We elucidate tradeoffs between flexibility in the <phrase>neu</phrase>-ron connections, and the hardware resources, such as <phrase>memory</phrase> and <phrase>communication</phrase> bandwidth, required to build a custom processor <phrase>design</phrase> that has optimal efficiency. We illustrate how our <phrase>architecture</phrase> can easily support sparse networks with dense regions of connections between neighboring sets of <phrase>neurons</phrase>, which is relevant to applications where there are obvious spatial correlations in the <phrase>data</phrase>, such as in <phrase>image processing</phrase>. We demonstrate the feasibility of our approach by implementing a multi-<phrase>FPGA</phrase> system. We show that a speedup of 46X-112X over an optimized <phrase>single</phrase> core <phrase>CPU</phrase> implementation can be achieved for a four-<phrase>FPGA</phrase> implementation.
<phrase>Deep learning</phrase> via <phrase>semi-supervised</phrase> embedding We show how nonlinear embedding <phrase>algorithms</phrase> popular for use with <i>shallow</i> <phrase>semi-supervised</phrase> learning techniques such as <phrase>kernel methods</phrase> can be applied to deep multilayer architectures, either as a regularizer at the output layer, or on each layer of the <phrase>architecture</phrase>. This provides a simple <phrase>alternative</phrase> to existing approaches to <i>deep</i> learning whilst yielding competitive error rates compared to those methods, and existing <i>shallow</i> <phrase>semi-supervised</phrase> techniques.
Iterative Estimation, Equalization and Decoding Iterative Estimation, Equalization and Decoding Date Approved <phrase>_____</phrase><phrase>_____</phrase><phrase>_____</phrase>__ iii To my beloved wife Tria, with my endless gratitude, <phrase>love</phrase> and admiration. iv Acknowledgments First, I would like to thank Dr. <phrase>John Barry</phrase> for his guidance during my stay at <phrase>Georgia Tech</phrase>. His very clear and objective view of technical matters, as well as his deep insights, have influenced a lot my view of <phrase>research</phrase> in <phrase>telecommunications</phrase>, and hopefully made this work more clear and objective. I would also like to thank Drs. Lanterman and McLaughlin for their constructive and timely <phrase>feedback</phrase> on my <phrase>thesis</phrase>, and Drs. Stber and Wang for being part of my defense committee. to thank the <phrase>School</phrase> of ECE, CRASP and the <phrase>Brazilian</phrase> <phrase>government</phrase>, through CAPES, for their financial support during different stages of this program. I am deeply indebted to my wife, Tria, for her support, <phrase>love</phrase> and encouragement, and for having pushed me when I needed pushing. I consider myself blessed to be married to such a wonderful person. Without her, this work and a lot more would not have been possible. For all she has done and put up with, I dedicate this <phrase>thesis</phrase> to her. v I would also like to thank my parents for their unconditional support and <phrase>love</phrase>. They have always taught me the importance of a positive attitude, and how learning can be fun. These lessons, and their unshakable believe in me, have been very important for the completion of my studies. I am also grateful to my parents-in-<phrase>law</phrase>, who have shown great <phrase>patience</phrase> and <phrase>faith</phrase>. and the list could go on forever. These incredible people showed me how vast the <phrase>area</phrase> of <phrase>telecommunications</phrase> is, and helped me have a <phrase>deeper understanding</phrase> of many <phrase>research</phrase> questions. And the discussions with them about <phrase>music</phrase>, <phrase>cooking</phrase>, <phrase>religion</phrase>, <phrase>politics</phrase>, <phrase>cricket</phrase>, etc., greatly broadened my horizons. They helped make this experience all the more worth it. Finally, there is the <phrase>Brazilian</phrase> crowd of <phrase>Atlanta</phrase>, who provided delightful breaks from the <phrase>daily</phrase> struggles of the Ph.D. program, and from the occasional struggle of <phrase>life</phrase> abroad:
<phrase>Ontology</phrase>-Based Meta-<phrase>Mining</phrase> of <phrase>Knowledge</phrase> Discovery Workflows 1 Abstract This chapter describes a principled approach to meta-learning that has three distinctive features. First, whereas most previous work on meta-learning focused exclusively on the learning task, our approach applies meta-learning to the full <phrase>knowledge</phrase> discovery process and is thus more aptly referred to as meta-<phrase>mining</phrase>. Second, traditional meta-learning regards <phrase>learning algorithms</phrase> as <phrase>black</phrase> boxes and essentially correlates properties of their input (<phrase>data</phrase>) with the performance of their output (learned <phrase>model</phrase>). We propose to tear open the <phrase>black</phrase> box and analyse <phrase>algorithms</phrase> in terms of their core components, their underlying assumptions, the cost functions and optimization strategies they use, and the models and decision boundaries they generate. Third, to ground meta-<phrase>mining</phrase> on a declarative representation of the <phrase>data mining</phrase> (dm) process and its components, we built a DM <phrase>ontology</phrase> and <phrase>knowledge base</phrase> using the <phrase>Web Ontology Language</phrase> (<phrase>owl</phrase>). The <phrase>Data Mining</phrase> Optimization <phrase>Ontology</phrase> (dmop, pronounced dee-mope)) provides a unified <phrase>conceptual framework</phrase> for analysing dm tasks, <phrase>algorithms</phrase>, models, datasets, workflows and performance metrics, as well as their relationships. The dm <phrase>knowledge base</phrase> uses concepts from dmop to describe existing <phrase>data mining</phrase> <phrase>algorithms</phrase> and their implementations in <phrase>major</phrase> dm <phrase>software</phrase> packages. <phrase>Meta-data</phrase> collected from <phrase>data mining</phrase> experiments are also described in terms of concepts from the <phrase>ontology</phrase> and linked to <phrase>algorithm</phrase> and operator descriptions in the <phrase>knowledge base</phrase>; they are then stored in <phrase>data mining</phrase> experiment <phrase>data</phrase> bases to serve as training and evaluation <phrase>data</phrase> for the meta-miner. These three features together lay the groundwork for what we call deep or <phrase>semantic</phrase> meta-<phrase>mining</phrase>, i.e., dm process or <phrase>workflow</phrase> <phrase>mining</phrase> that is driven simultaneously by <phrase>meta-data</phrase> and by the collective expertise of <phrase>data</phrase> miners embodied in the <phrase>data mining</phrase> <phrase>ontology</phrase> and <phrase>knowledge base</phrase>. In Section 2, we review the <phrase>state</phrase> of the <phrase>art</phrase> in the fields of meta-learning and <phrase>data mining</phrase> <phrase>ontologies</phrase>; at the same time, we motivate the need for <phrase>ontology</phrase>-based meta-<phrase>mining</phrase> and distinguish our approach from related work in these two areas. Section 3 gives a detailed description of dmop, while Section 4 introduces a novel method for <phrase>ontology</phrase>-based discovery of generalized patterns from <phrase>data mining</phrase> workflows. Section 5 reports on <phrase>proof-of-concept</phrase> experiments conducted to gauge the efficacy of dmop-based <phrase>workflow</phrase> <phrase>mining</phrase>, and Section 6 concludes.
Deep Narrow <phrase>Sigmoid</phrase> <phrase>Belief Networks</phrase> Are <phrase>Universal</phrase> Approximators Deep Narrow <phrase>Sigmoid</phrase> <phrase>Belief Networks</phrase> Are <phrase>Universal</phrase> Approximators In this <phrase>paper</phrase> we show that exponentially <phrase>deep belief</phrase> networks [3, 7, 4] can approximate any distribution over <phrase>binary</phrase> vectors to arbitrary accuracy, even when the width of each layer is limited to the dimensionality of the <phrase>data</phrase>. This resolves an open the problem in [6]. We further show that such networks can be greedily learned in an easy yet impractical way.
An Application of the Saturated <phrase>Attractor</phrase> Analysis to Three Typical Models A b s t r a c t The saturated <phrase>attractor</phrase> analysis, an approach proposed first in [FP] for a <phrase>comprehensive</phrase> study of the dynamics of the Linsker <phrase>model</phrase> and then successfully applied to the dynamic link <phrase>model</phrase>[FT1], is further developed here. By a unified approach to the Hopfield <phrase>model</phrase>, the Linsker <phrase>model</phrase> and the dynamic link <phrase>model</phrase>, three typical models in the field of the <phrase>neural networks</phrase>, we show a way to choose the parameters of these dynamics in <phrase>order</phrase> to obtain any chosen saturated <phrase>attractor</phrase> which is <phrase>general</phrase> enough in most applications. We generalize our previous <phrase>results</phrase> for the Linsker <phrase>model</phrase> and the dynamic link <phrase>model</phrase> with the clipping <phrase>function</phrase> to the case of the <phrase>sigmoid</phrase> like <phrase>function</phrase>. Our <phrase>results</phrase> allow us for the first time to understand the underlying mechanism among these models and thus to furnish a useful guidance in the further possible applications. w INTRODUCTION The past decade has seen an explosive growth in the studies of <phrase>neural networks</phrase>, the theory underlying learning and <phrase>computing</phrase> in networks has developed into a mature subfield existing somewhere between <phrase>mathematics</phrase>, <phrase>physics</phrase>, <phrase>computer science</phrase> and <phrase>neurobiology</phrase>. In part this was the result of many deep and interesting theoretical exposition in <phrase>physics</phrase> and <phrase>mathematics</phrase>, for example, the application of the <phrase>spin</phrase> glas~ theory to the Hopfield <phrase>model</phrase> allows us to understand clearly the <phrase>phase transition</phrase> from the retrieval to non retrieval <phrase>state</phrase>. Another <phrase>major</phrase> impulse was provided by the successful explanation of some biological phenomena, at least in a primitive level, for example, the Linsker <phrase>model</phrase> mimics the ontogenesis development of the primary visual system[Lin]. Of course, the most important impulse comes from the learning techniques successfully applied to some practical problems which were traditionally thought of as some of the hardest problems in the <phrase>AI</phrase>. One of the recent examples of such an application is the <phrase>face recognition</phrase> using the dynamic link <phrase>model</phrase>, a <phrase>model</phrase> proposed by vonder Malsbnrg first in 1981[KMM]. However, at this moment, the theoretical treatment of these models is obviously far away from being satisfactory, mainly due to the lack of theoretical tools to deal with the nonlinearity exploited in most of the models reported today. In the present <phrase>paper</phrase>, in terms of our previous work on the Linsker <phrase>model</phrase> and the dynamic link <phrase>model</phrase> we develop a unified theoretical framework for tackling the Hopfield <phrase>model</phrase>, the Linsker <phrase>model</phrase> and the dynamic link <phrase>model</phrase>. Our 
Diagrammatic <phrase>Student</phrase> Models: Modeling <phrase>Student</phrase> <phrase>Drawing</phrase> Performance with <phrase>Deep Learning</phrase> Recent years have seen a growing interest in the role that <phrase>student</phrase> <phrase>drawing</phrase> can <phrase>play</phrase> in learning. Because <phrase>drawing</phrase> has been shown to contribute to students' learning and increase their engagement, developing <phrase>student</phrase> models to dynamically support <phrase>drawing</phrase> holds significant promise. To this end, we introduce diagrammatic <phrase>student</phrase> models, which reason about students' <phrase>drawing</phrase> trajectories to generate a series of predictions about their conceptual <phrase>knowledge</phrase> based on their evolving sketches. The diagrammatic <phrase>student</phrase> modeling framework utilizes <phrase>deep learning</phrase>, a <phrase>family</phrase> of <phrase>machine learning</phrase> methods based on a <phrase>deep neural network</phrase> <phrase>architecture</phrase>, to reason about sequences of <phrase>student</phrase> <phrase>drawing</phrase> actions encoded with temporal and <phrase>topological</phrase> features. An evaluation of the <phrase>deep-learning</phrase>-based diagrammatic <phrase>student</phrase> models suggests that it can predict <phrase>student</phrase> performance more accurately and earlier than competitive baseline approaches.
Qualitative <phrase>Case Study</phrase> Guidelines Although widely used, the qualitative <phrase>case study</phrase> method is not well understood. Due to conflicting <phrase>epistemological</phrase> presuppositions and the complexity inherent in qualitative <phrase>case-based</phrase> studies, scientific rigor can be difficult to demonstrate, and any resulting findings can be difficult to justify. For that reason, this <phrase>paper</phrase> discusses methodological problems associated with qualitative <phrase>case-based</phrase> <phrase>research</phrase> and offers guidelines for overcoming them. Due to its nearly <phrase>universal</phrase> acceptance, Yin's six-stage <phrase>case study</phrase> process is adopted and elaborated on. Moreover, additional principles from the wider methodological <phrase>literature</phrase> are integrated and explained. Finally, some modifications to the dependencies between the six <phrase>case study</phrase> stages are suggested. It is expected that following the guidelines presented in this <phrase>paper</phrase> may facilitate the collection of the most relevant <phrase>data</phrase> in the most efficient and effective manner, simplify the subsequent analysis, as well as enhance the validity of the resulting findings. The <phrase>paper</phrase> should be of interest to students (honour, <phrase>masters</phrase>, doctoral), academics, and practitioners involved with <phrase>conducting</phrase> and reviewing qualitative <phrase>case-based</phrase> studies. Where quantitative <phrase>research</phrase> is mainly concerned with the testing of hypotheses and statistical generalisations (Jackson, 2008), <phrase>qualitative research</phrase> does not usually employ statistical procedures or other means of quantification, focusing instead on understanding the <phrase>nature</phrase> of the <phrase>research</phrase> problem rather than on the quantity of observed characteristics (<phrase>Strauss</phrase> & Corbin, 1994). Given that qualitative researchers generally assume that social <phrase>reality</phrase> is a <phrase>human</phrase> creation, they interpret and contextualise meanings from people's beliefs and practices (Denzin & <phrase>Lincoln</phrase>, 2011). <phrase>Case study</phrase> <phrase>research</phrase> involves " intensive study of a <phrase>single</phrase> unit for the purpose of understanding a larger class of (similar) units observed at a <phrase>single</phrase> point in time or over some delimited <phrase>period</phrase> of time " (Gerring, 2004, p. 342). As such, <phrase>case studies</phrase> provide an opportunity for the researcher to gain a deep holistic view of the <phrase>research</phrase> problem, and may facilitate describing, understanding and explaining a <phrase>research</phrase> problem or situation Besides being widely used in <phrase>academia</phrase>, the method is also popular with practitioners as a tool for evaluation and organisational learning. However, although widely used, the qualitative <phrase>case study</phrase> method is not well understood Given the considerable time and resource requirements associated with <phrase>conducting</phrase> such studies (GAO, 1990), any misunderstandings regarding the purpose and implementation of the method as well as the validity of the resulting findings can have significant negative consequences. In the context of <phrase>academic</phrase> studies, significant misunderstandings identified during the 
Format-Transforming <phrase>Encryption</phrase>: More than Meets the DPI <phrase>Nation-states</phrase> and other organizations are increasingly deploying <phrase>deep-packet inspection</phrase> (DPI) technologies to censor <phrase>Internet traffic</phrase> based on <phrase>application-layer</phrase> content. We introduce a new DPI circumvention approach, format-transforming <phrase>encryption</phrase> (<phrase>FTE</phrase>), that cryptographically transforms the format of arbitrary plaintext <phrase>data</phrase> (e.g. packet contents) into specified formats that are designed to bypass DPI <phrase>tests</phrase>. We show how to build a <phrase>general</phrase>-purpose <phrase>FTE</phrase> system, in which these formats are defined compactly by families of <phrase>regular expressions</phrase>. Moreover, we specify and implement a full <phrase>FTE</phrase> record-layer protocol. We exhibit formats that are guaranteed to avoid known filters, and give a framework for learning formats from non-censored HTTP traffic. These formats are put to use in our <phrase>FTE</phrase> record layer, to explore <phrase>trade</phrase>-offs between performance and steganographic capabilities. As one example, we visit the top 100 Alexa webpages through an <phrase>FTE</phrase> <phrase>tunnel</phrase>, incurring an <phrase>average</phrase> overhead of roughly 5%.
<phrase>Blended Learning</phrase>: Uncovering Its Transformative Potential in <phrase>Higher Education</phrase> The purpose of this <phrase>paper</phrase> is to provide a discussion of the transformative potential of <phrase>blended learning</phrase> in the context of the challenges facing <phrase>higher education</phrase>. Based upon a description of <phrase>blended learning</phrase>, its potential to support deep and meaningful learning is discussed. From here, a shift to the need to rethink and restructure the learning experience occurs and its transformative potential is analyzed. Finally, administrative and <phrase>leadership</phrase> issues are addressed and the outline of an <phrase>action</phrase> plan to implement <phrase>blended learning</phrase> approaches is presented. The conclusion is that <phrase>blended learning</phrase> is consistent with the values of traditional <phrase>higher education</phrase> institutions and has the <phrase>proven</phrase> potential to enhance both the effectiveness and efficiency of meaningful <phrase>learning experiences</phrase>.
<phrase>Plant</phrase> Electrical Signal Classification Based on <phrase>Waveform</phrase> Similarity (1) Background: <phrase>Plant</phrase> electrical signals are important <phrase>physiological</phrase> traits which reflect <phrase>plant</phrase> <phrase>physiological</phrase> <phrase>state</phrase>. As a kind of <phrase>phenotypic</phrase> <phrase>data</phrase>, <phrase>plant</phrase> <phrase>action potential</phrase> (<phrase>AP</phrase>) evoked by external stimulie.g., electrical stimulation, environmental stressmay be associated with inhibition of <phrase>gene expression</phrase> related to stress tolerance. However, <phrase>plant</phrase> <phrase>AP</phrase> is a response to environment changes and full of variability. It is an aperiodic signal with <phrase>refractory</phrase> <phrase>period</phrase>, discontinuity, noise, and artifacts. In consequence, there are still challenges to automatically recognize and classify <phrase>plant</phrase> <phrase>AP</phrase>; (2) Methods: Therefore, we proposed an <phrase>AP</phrase> recognition <phrase>algorithm</phrase> based on dynamic difference threshold to extract all <phrase>waveforms</phrase> similar to <phrase>AP</phrase>. Next, an incremental template matching <phrase>algorithm</phrase> was used to classify the <phrase>AP</phrase> and non-<phrase>AP</phrase> <phrase>waveforms</phrase>; (3) <phrase>Results</phrase>: Experiment <phrase>results</phrase> indicated that the template matching <phrase>algorithm</phrase> achieved a classification rate of 96.0%, and it was <phrase>superior</phrase> to <phrase>backpropagation</phrase> <phrase>artificial neural networks</phrase> (<phrase>BP</phrase>-ANNs), supported <phrase>vector</phrase> machine (<phrase>SVM</phrase>) and <phrase>deep learning</phrase> method; (4) Conclusion: These findings imply that the proposed methods are likely to expand possibilities for rapidly recognizing and classifying <phrase>plant</phrase> <phrase>action potentials</phrase> in the <phrase>database</phrase> in the future.
Practical recommendations for <phrase>gradient</phrase>-based training of <phrase>deep architectures</phrase> <phrase>Learning algorithms</phrase> related to <phrase>artificial neural networks</phrase> and in particular for <phrase>Deep Learning</phrase> may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of <phrase>learning algorithms</phrase> based on back-propagated <phrase>gradient</phrase> and <phrase>gradient</phrase>-based optimization. It also discusses how to deal with the fact that more interesting <phrase>results</phrase> can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently <phrase>train</phrase> and debug <phrase>large-scale</phrase> and often deep <phrase>multi-layer</phrase> <phrase>neural networks</phrase>. It closes with open questions about the training difficulties observed with deeper architectures.
A Novel Method on Incremental <phrase>Information</phrase> Acquisition for <phrase>Deep Web</phrase> a Novel Method on Incremental <phrase>Information</phrase> Acquisition for <phrase>Deep Web</phrase> <phrase>Deep Web</phrase> is autonomous, independently updating, and its <phrase>data</phrase> are always in a <phrase>state</phrase> of frequent update. However, the user always hopes to obtain the newest content in the current Web <phrase>database</phrase>. Different from previous <phrase>research</phrase>, this <phrase>paper</phrase> wants to emphasize the importance of updating <phrase>frequency</phrase> in the study of <phrase>Deep Web</phrase> <phrase>information</phrase> acquisition. And, an approach on incremental <phrase>information</phrase> acquisition based on logical <phrase>reinforcement learning</phrase> has been proposed. Then, we find in our <phrase>research</phrase> that under the same condition of constraint resources, the novel approach can improve the freshness of <phrase>data</phrase>, discovery efficiency of new <phrase>data</phrase> and the service quality of <phrase>Deep Web</phrase> <phrase>information</phrase> integration.
DeepBot: a focused crawler for accessing <phrase>hidden web</phrase> content The crawler engines of today cannot reach most of the <phrase>information</phrase> contained in the Web. A great amount of valuable <phrase>information</phrase> is "hidden" behind the query forms of online <phrase>databases</phrase>, and/or is dynamically generated by technologies such as <phrase>Javascript</phrase>. This portion of the web is usually known as the <phrase>Deep Web</phrase> or the <phrase>Hidden Web</phrase>. We have built DeepBot, a <phrase>prototype</phrase> of <phrase>hidden-web</phrase> focused crawler able to access such content. DeepBot receives a set of domain definitions as an input, each one describing a specific <phrase>data</phrase>-collecting task and automatically identifies and learns to execute queries on the forms relevant to them. In this <phrase>paper</phrase> we describe the techniques employed for building DeepBot and <phrase>report</phrase> the <phrase>experimental</phrase> <phrase>results</phrase> obtained when testing it with several <phrase>real world</phrase> <phrase>data</phrase> collection tasks.
<phrase>Technology</phrase> for Performance-based <phrase>Lifelong Learning</phrase> 2. Obstacles to Performance-based <phrase>Lifelong Learning</phrase> Performance-based learning requires that a computer system have deep, exact <phrase>knowledge</phrase> of what a person is doing and what they already know about that task to determine what they should learn now. <phrase>Lifelong learning</phrase> requires that a computer system have broad, <phrase>general</phrase> <phrase>knowledge</phrase> over a number of years of what a person has learned to determine what they should learn now. Separately, these are each difficult goals to accomplish. However, both goals must be met to effectively provide a workforce that meets the evolving needs of <phrase>industry</phrase> and <phrase>government</phrase> for skilled workers. This <phrase>paper</phrase> describes a system that can meet both of these goals and support performance-based <phrase>lifelong learning</phrase>. Recent <phrase>technology</phrase> advancements in the areas of <phrase>e</phrase>-learning and <phrase>knowledge management</phrase> have set the stage for the fulfillment of the vision for <phrase>lifelong learning</phrase> put forth by Wayne Hodgins for the Commission on <phrase>Technology</phrase> and Adult Learning in February, 2000 [1]. According to Hodgins, a key <phrase>aspect</phrase> of this vision is performance-based learning. Performance-based learning is the result of a transition from " teaching by telling " to " learning by doing, " assisted by technological and <phrase>human</phrase> coaches providing the <phrase>low-level</phrase> and <phrase>high</phrase>-level support. Furthermore, his key to the execution of performance-based learning is successful <phrase>information management</phrase>. Successful <phrase>information management</phrase> makes it possible " to deliver just the right <phrase>information</phrase>, in just the right amount, to just the right person in just the right context, at just the right time, and in a form that matches the way that person learns. When this happens, the recipient can <phrase>act</phrase> immediately and effectively. " While the stage has been set for the fulfillment of a vision of performance-based <phrase>lifelong learning</phrase>, there are numerous social and <phrase>technology</phrase> obstacles that may yet stand in the way. These obstacles must first be understood and then proactive steps must be taken to overcome the obstacles or minimize their impact on attaining the vision. There are separate obstacles to implementing a system to support performance-based learning and implementing a system to support <phrase>lifelong learning</phrase>. However, there are even greater obstacles to implementing an integrated system to support performance-based <phrase>lifelong learning</phrase>: Performance-based learning requires that the system have deep, exact <phrase>knowledge</phrase> of what the person is doing and what they already know about that task to determine what they should learn now. <phrase>Lifelong learning</phrase> requires that the system have broad, <phrase>general</phrase> <phrase>knowledge</phrase> over a number 
Learning <phrase>computer graphics</phrase> by <phrase>programming</phrase>: linking theory and practice One of the problems in teaching and studying <phrase>Computer Graphics</phrase> is actual linking between theory and practice: theoretical issues are complex and difficult to visualize while using <phrase>programming</phrase> techniques like <phrase>OpenGL</phrase> without a <phrase>deep understanding</phrase> of the theoretical basis prevents from exploiting the full potential of these powerful aids.Existing <phrase>Computer Graphics</phrase> courses are varying from pure theory, via using ready programs for demonstrations and illustrations up to systematic use of <phrase>programming</phrase> elements as the important component of a course.This presentation describes the implementation of two of these strategies, used by the <phrase>author</phrase> is his <phrase>computer graphics</phrase> courses, in <phrase>order</phrase> to enhance the links between theory and practice
Teaching <phrase>motion planning</phrase> concepts to <phrase>undergraduate</phrase> students <phrase>Motion planning</phrase> is a central problem in <phrase>robotics</phrase>. Although it is an engaging topic for <phrase>undergraduate</phrase> students, it is difficult to teach, and as a result, the material is often only <phrase>covered</phrase> at an abstract level. <phrase>Deep learning</phrase> could be achieved by having students implement and <phrase>test</phrase> different <phrase>algorithms</phrase>. However, there is usually no time within a <phrase>single</phrase> class to have students completely implement several <phrase>motion planning</phrase> <phrase>algorithms</phrase> as they require the development of many <phrase>lower</phrase>-level <phrase>data structures</phrase>. We present an ongoing project to develop a teaching module for robotic <phrase>motion planning</phrase> centered around an integrated <phrase>software</phrase> environment. The module can be taught early in the <phrase>undergraduate</phrase> <phrase>curriculum</phrase>, after students have taken an introductory <phrase>programming</phrase> class.
PeerWise: students sharing their <phrase>multiple choice</phrase> questions PeerWise is a system in which students create <phrase>multiple choice</phrase> questions and answer those created by their <phrase>peers</phrase>. In this <phrase>paper</phrase>, we <phrase>report</phrase> on some quantitative <phrase>results</phrase> which suggest that students who use PeerWise actively perform better in final examinations than students who are not active. We note a significant correlation between performance in written (not just <phrase>multiple choice</phrase>) questions and PeerWise activity, suggesting that active use of the system may contribute to deep (and not just drill-and-practise) learning.
Mapping between Compositional <phrase>Semantic</phrase> Representations and Lexical <phrase>Semantic</phrase> Resources: Towards Accurate Deep <phrase>Semantic</phrase> <phrase>Parsing</phrase> This <phrase>paper</phrase> introduces a <phrase>machine learning</phrase> method based on <phrase>bayesian</phrase> networks which is applied to the mapping between deep <phrase>semantic</phrase> representations and lexical <phrase>semantic</phrase> resources. A <phrase>probabilistic model</phrase> comprising Minimal <phrase>Recursion</phrase> <phrase>Semantics</phrase> (MRS) structures and lexicalist oriented <phrase>semantic</phrase> features is acquired. Lexical <phrase>semantic</phrase> roles enriching the MRS structures are inferred, which are useful to improve the accuracy of deep <phrase>semantic</phrase> <phrase>parsing</phrase>. <phrase>Verb</phrase> classes inference was also investigated, which, together with lexical <phrase>semantic</phrase> <phrase>information</phrase> provided by VerbNet and PropBank resources, can be substantially beneficial to the parse disambiguation task.
<phrase>Book</phrase> Review: 'Learning Through Practice: Models, Traditions, Orientations, and Approaches' edited by Stephen Billett Stephen Billett is the <phrase>editor</phrase> of "Learning Through Practice: Models, Traditions, Orientations, and Approaches," a collection of chapters by academics from around the globe, with a <phrase>variety</phrase> of analytic frameworks and based upon a wide <phrase>variety</phrase> of task domains. This is deep <phrase>reading</phrase>, not a practitioner-oriented review, but does have rewards for the patient and erudite reader.
Describing <phrase>Human</phrase> <phrase>Aesthetic</phrase> <phrase>Perception</phrase> by Deeply-learned Attributes from <phrase>Flickr</phrase> Many <phrase>aesthetic</phrase> models in <phrase>computer vision</phrase> suffer from two shortcomings: 1) the low descriptiveness and inter-pretability of those <phrase>hand-crafted</phrase> <phrase>aesthetic</phrase> criteria (i.e., non-indicative of <phrase>region</phrase>-level <phrase>aesthetics</phrase>), and 2) the difficulty of <phrase>engineering</phrase> <phrase>aesthetic</phrase> features adaptively and automatically toward different image sets. To remedy these problems, we develop a <phrase>deep architecture</phrase> to learn aesthetically-relevant visual attributes from <phrase>Flickr</phrase> 1 , which are localized by multiple textual attributes in a <phrase>weakly-supervised</phrase> setting. More specifically, using a bag-of-words (BoW) representation of the frequent <phrase>Flickr</phrase> image tags, a sparsity-constrained subspace <phrase>algorithm</phrase> discovers a compact set of textual attributes (e.g., <phrase>landscape</phrase> and sunset) for each image. Then, a <phrase>weakly-supervised</phrase> <phrase>learning algorithm</phrase> projects the textual attributes at image-level to the highly-responsive <phrase>image patches</phrase> at <phrase>pixel</phrase>-level. These patches indicate where humans look at appealing regions with respect to each textual attribute, which are employed to learn the visual attributes. <phrase>Psychological</phrase> and anatomical studies have shown that humans perceive <phrase>visual concepts</phrase> hierarchically. Hence, we normalize these patches and feed them into a five-layer <phrase>convolutional neural network</phrase> (<phrase>CNN</phrase>) to mimick the hierarchy of <phrase>human</phrase> perceiving the visual attributes. We apply the learned deep features on image retargeting, <phrase>aesthetics</phrase> ranking, and retrieval. Both subjective and objective <phrase>experimental</phrase> <phrase>results</phrase> thoroughly demonstrate the competitiveness of our approach.
Instance-Based Classification by Emerging Patterns Emerging patterns (<phrase>EPs</phrase>), namely itemsets whose supports change significantly from one class to another, capture discriminating features that sharply contrast instances between the classes. Recently, <phrase>EP</phrase>-based classifiers have been proposed, which first mine as many <phrase>EPs</phrase> as possible (called eager-learning) from the <phrase>training data</phrase> and then aggregate the discriminating power of the mined <phrase>EPs</phrase> for classifying new instances. We propose here a new, instance-based classifier using <phrase>EPs</phrase>, called DeEPs, to achieve much better accuracy and efficiency than the previously proposed <phrase>EP</phrase>-based classifiers. <phrase>High</phrase> accuracy is achieved because the instance-<phrase>based approach</phrase> enables DeEPs to pinpoint all <phrase>EPs</phrase> relevant to a <phrase>test</phrase> instance, some of which are missed by the eager-learning approaches. <phrase>High</phrase> efficiency is obtained using a series of <phrase>data</phrase> reduction and concise <phrase>data</phrase>-representation techniques. Experiments show that DeEPs' decision time is linearly scalable over the number of training instances and nearly linearly over the number of attributes. Experiments on 40 datasets also show that DeEPs is <phrase>superior</phrase> to other classifiers on accuracy.
Exploration for <phrase>Multi-task</phrase> <phrase>Reinforcement Learning</phrase> with <phrase>Deep Generative Models</phrase> Exploration in <phrase>multi-task</phrase> <phrase>reinforcement learning</phrase> is critical in training agents to deduce the underlying MDP. Many of the existing exploration frameworks such as <phrase>E 3</phrase> , R max , Thompson sampling assume a <phrase>single</phrase> stationary MDP and are not suitable for system identification in the <phrase>multi-task</phrase> setting. We present a novel method to facilitate exploration in <phrase>multi-task</phrase> <phrase>reinforcement learning</phrase> using <phrase>deep generative models</phrase>. We supplement our method with a <phrase>low dimensional</phrase> <phrase>energy</phrase> <phrase>model</phrase> to learn the underlying MDP distribution and provide a resilient and adaptive exploration signal to the agent. We evaluate our method on a new set of environments and provide intuitive interpretation of our <phrase>results</phrase>.
<phrase>Statistical classification</phrase> of services tunneled into SSH connections by a K-means based <phrase>learning algorithm</phrase> Secure SHell is a <phrase>TCP</phrase> based protocol designed to enhance with <phrase>security</phrase> features <phrase>telnet</phrase> and other insecure remote <phrase>management</phrase> tools. Due to its versatility, it is often exploited to <phrase>forward</phrase> applications (i.e. HTTP, SCP, etc.) into encoded <phrase>TCP</phrase> traffic flows. The point which makes challenging the identification of the uses of SSH is that packets are enciphered and instruments based on <phrase>deep packet inspection</phrase> (DPI) cannot achieve this task. We approached the problem of early SSH classification with k-means based machine by studying statistical behavior of IP traffic parameters, such as length, arrival time and direction of packets. In this <phrase>paper</phrase> we describe tools and networks designed to collect SSH remote administration traffic as well as relevant <phrase>results</phrase> obtained for its classification. In particular, our tool identifies remote <phrase>management</phrase> traffic out of other SSH encoded applications with accuracy up to 90.34
On learning <phrase>electricity</phrase> with <phrase>multi-agent</phrase> based computational models (NIELS) We present NIELS (NetLogo Investigations in <phrase>Electromagnetism</phrase>, Sengupta & Wilensky, 2005, 2006), a <phrase>multi-agent</phrase> <phrase>based learning environment</phrase> that represents phenomena such as <phrase>Electric Current</phrase>, Resistance, etc in the form of " emergent " (Wilensky & Resnick, 1999) computational models. Based on a <phrase>pilot</phrase> implementation in <phrase>a 7</phrase> th grade classroom, we argue that NIELS models enable learners to " think in levels " (Wilensky & Resnick, 1999) which in turn enables them to bootstrap their existing object-based <phrase>knowledge</phrase> structures to engender a deep, expert-like understanding. Understanding <phrase>electricity</phrase> requires the ability to reason about the relevant phenomena at <phrase>multiple levels</phrase> However, students in traditional <phrase>physics</phrase> classrooms, after instruction, are unable to relate behavior of <phrase>electrons</phrase> and <phrase>atoms</phrase> within the <phrase>wire</phrase> at the microscopic level to the aggregate or macroscopic level behavior such as current, resistance, etc. Eylon & Ganiel (1990) identified this as the " missing macro-micro link " between the domains of <phrase>electrostatics</phrase> and <phrase>electricity</phrase>. To address this issue, we designed NIELS (NetLogo Investigations In <phrase>Electromagnetism</phrase>, Sengupta & Wilensky, 2005), a curricular unit consisting of <phrase>multi-agent</phrase> based models designed in the NetLogo modeling environment (Wilensky, 1999). NIELS models represent phenomena in the domain of <phrase>electricity</phrase> as emergent i.e., aggregate-level phenomena such as <phrase>electric current</phrase> and resistance emerge from simple interactions between thousands of individual agents (<phrase>electrons</phrase> and <phrase>atoms</phrase>). Our <phrase>pilot</phrase> studies in <phrase>a 7</phrase> th grade classroom show that after interacting with NIELS, a) students are able to " think in levels " (Wilensky & Resnick, 1999) about the relevant phenomena, and b) use their existing object-based <phrase>knowledge</phrase> elements (that are also responsible for generating commonly noted " misconceptions " about <phrase>Electricity</phrase>) in <phrase>order</phrase> to understand and explain the relevant phenomena. In their review of this <phrase>literature</phrase> on <phrase>nave</phrase> misconceptions, Reiner, Slotta, Chi and Resnick (2000) argue that <phrase>nave</phrase> reasoning is " incompatible " with that of experts and argue that the <phrase>general</phrase> <phrase>knowledge</phrase> of material substances, their properties, and how they behave is drawn on as a source of conceptual <phrase>information</phrase> whenever the novice encounters a difficult new <phrase>physics</phrase> concept. They also claim that experts use a different <phrase>ontology</phrase> of <phrase>knowledge</phrase> " process " schemas to make sense of phenomena in <phrase>electricity</phrase>. In her more recent work Chi (2005) argued that several misconceptions related to understanding <phrase>complex systems</phrase> cannot be remediated due to the incommensurability between common schemas about processes (for novices) and the " emergent 
Greening, T. <phrase>Scaffolding</phrase> for Success in <phrase>Pbl</phrase> <phrase>Scaffolding</phrase> for Success in <phrase>Problem-based Learning</phrase> <phrase>Problem-Based Learning</phrase> (<phrase>PBL</phrase>) is based on an <phrase>alternative</phrase> pedagogical <phrase>model</phrase> to the conventional , didactic one, and offers benefits to the quality of <phrase>student</phrase> learning. The approach has been adopted by many institutions. The focus of this <phrase>paper</phrase> is on the influence of the learning support structure in an environment (such as the typical <phrase>PBL</phrase> environment) that encourages <phrase>student</phrase> <phrase>independence</phrase> as one of its <phrase>basic</phrase> tenets. The immediate <phrase>reaction</phrase> might be to assume that it takes on a reduced importance in such circumstances. That assumption is challenged in this <phrase>paper</phrase>. The concept of "<phrase>scaffolding</phrase>" may take many forms 1 ; here it is used in a wide context to refer to all forms of learning support, whether or not they be cognitively-based, logistical, etc. Thus, the term is used to broadly refer to the <phrase>range</phrase> of services provided to assist learning. This is of interest as <phrase>Problem Based Learning</phrase> (<phrase>PBL</phrase>) approaches to <phrase>education</phrase> suggest a strong role for factors such as authenticity and <phrase>student</phrase> <phrase>independence</phrase>. With the shift of emphasis that moving towards a <phrase>PBL</phrase> approach implies, a re-examination of the <phrase>nature</phrase> of what it means to offer meaningful support to learners is important. In this <phrase>paper</phrase> the relationship of <phrase>scaffolding</phrase> to the success of <phrase>PBL</phrase> programs is examined. In this context, "success" is measured largely in terms of those benefits with which <phrase>PBL</phrase> is associated, principally in the encouragement of "deep" (or personally meaningful and potentially transformative) <phrase>approaches to learning</phrase>. Elsewhere, deep <phrase>approaches to learning</phrase> have been described in terms of higher levels of integration of <phrase>knowledge</phrase> and greater tolerance to complexity 2. The <phrase>paper</phrase> begins by briefly referring to the defi-nitional dilemma that hinders attempts to compare <phrase>PBL</phrase> programs. A working definition is established. Desirable outcomes of <phrase>PBL</phrase> are then presented, with an emphasis on "deep" learning approaches. This is then countered by raising some concerns about the performance of existing <phrase>PBL</phrase> programs, while hinting that appropriate <phrase>scaffolding</phrase> may offer <phrase>relief</phrase> from some of these concerns. An account of the role of the <phrase>tutor</phrase> in <phrase>PBL</phrase> follows, as an obvious source of <phrase>scaffolding</phrase>. <phrase>Scaffolding</phrase> is then discussed in terms of improving the likelihood of successfully encouraging meaningful <phrase>approaches to learning</phrase> in <phrase>PBL</phrase> students. A review of the <phrase>literature</phrase> reveals an extensive coverage of <phrase>PBL</phrase> <phrase>case studies</phrase>, many based on differing concepts of <phrase>PBL</phrase>. 1 These concepts may directly affect the reported success or failure of the approach; for 
Effects of constructing versus playing an <phrase>educational game</phrase> on <phrase>student</phrase> <phrase>motivation</phrase> and <phrase>deep learning</phrase> strategy use In this study the effects of two different interactive learning tasks, in which simple <phrase>games</phrase> were included were described with respect to <phrase>student</phrase> <phrase>motivation</phrase> and deep strategy use. The <phrase>research</phrase> involved 235 students from four <phrase>elementary</phrase> schools in The <phrase>Netherlands</phrase>. One group of students (N 128) constructed their own <phrase>memory</phrase> 'drag and drop' <phrase>game</phrase>, whereas the other group (N 107) played an existing 'drag and drop' <phrase>memory</phrase> <phrase>game</phrase>. Analyses of <phrase>covariance</phrase> demonstrated a significant difference between the two conditions both on intrinsic <phrase>motivation</phrase> and deep strategy use. The large effect sizes for both <phrase>motivation</phrase> and deep strategy use were in favour of the <phrase>construction</phrase> condition. The <phrase>results</phrase> suggest that constructing a <phrase>game</phrase> might be a better way to enhance <phrase>student</phrase> <phrase>motivation</phrase> and <phrase>deep learning</phrase> than playing an existing <phrase>game</phrase>. Despite the <phrase>promising results</phrase>, the <phrase>low level</phrase> of complexity of the <phrase>games</phrase> used is a study limitation. In the last decades, views on learning and instruction have changed fundamentally. In most contemporary theories of learning, generally referred to as social constructivist learning theories, learning is seen as a process of <phrase>knowledge</phrase> <phrase>construction</phrase> with an emphasis on active and self-regulated learning (Shuell, 2001). Constructivist learning approaches underline the idea of an active, experiencing <phrase>student</phrase> in a situation where <phrase>knowledge</phrase> is not transmitted to the <phrase>student</phrase>, but constructed through activity or social interaction. There are a few <phrase>basic</phrase> assumptions of constructivist learning. First, learning can be seen as an active process of <phrase>knowledge</phrase> achievement (Driscoll, 1994). Students are said to construct their <phrase>knowledge</phrase>, based on their pre-<phrase>knowledge</phrase> and interest. Next to <phrase>knowledge</phrase> <phrase>construction</phrase>, self-regulation is said to be important in learning. This means students can manage their learning process. According to Perkins (1999), <phrase>knowledge</phrase> <phrase>construction</phrase> and self-regulation leads to better understanding, remembering and actively use of <phrase>knowledge</phrase>. This is also shown by <phrase>research</phrase> of Greene and Azevedo (2009), who demonstrated that students who used constructivistic self-regulated <phrase>learning strategies</phrase> were more likely to obtain deep, <phrase>conceptual understanding</phrase> of complex topics than when they would limit their learning to obtaining declarative <phrase>knowledge</phrase>. Constructivist theories of learning have had noticeable impacts on <phrase>education</phrase> practice, for example on <phrase>teacher</phrase> didactics, which has shifted from direct teaching to the promotion of <phrase>active learning</phrase> in classrooms. Additionally constructivist theories ask for different <phrase>learning environments</phrase>. According to Driscoll (1994) these constructivist environments are complex, realistic, and meaningful. A complex <phrase>learning environment</phrase> will engage and challenge students to construct 
<phrase>Algorithms</phrase> for Hyper-Parameter Optimization Several <phrase>recent advances</phrase> to the <phrase>state</phrase> of the <phrase>art</phrase> in <phrase>image classification</phrase> benchmarks have come from better configurations of existing techniques rather than novel approaches to <phrase>feature learning</phrase>. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and <phrase>GPU</phrase> processors make it possible to run more trials and we show that algorithmic approaches can find better <phrase>results</phrase>. We present hyper-parameter optimization <phrase>results</phrase> on tasks of training <phrase>neu</phrase>-ral networks and <phrase>deep belief</phrase> networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning <phrase>neural networks</phrase> for several datasets, but we show it is unreliable for training DBNs. The sequential <phrase>algorithms</phrase> are applied to the most difficult DBN learning problems from [1] and find significantly better <phrase>results</phrase> than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.
Recurrent Online Clustering as a <phrase>Spatio-Temporal</phrase> Feature Extractor in <phrase>DeSTIN</phrase> This <phrase>paper</phrase> presents a <phrase>basic</phrase> enhancement to the <phrase>DeSTIN</phrase> <phrase>deep learning</phrase> <phrase>architecture</phrase> by replacing the explicitly calculated transition tables that are used to capture temporal features with a simpler, more scalable mechanism. This mechanism uses <phrase>feedback</phrase> of <phrase>state</phrase> <phrase>information</phrase> to cluster over a space comprised of both the spatial input and the <phrase>current state</phrase>. The resulting <phrase>architecture</phrase> achieves <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> on the MNIST classification benchmark.
Learning with Recursive <phrase>Perceptual</phrase> Representations Linear <phrase>Support Vector Machines</phrase> (SVMs) have become very popular in vision as part of <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>object recognition</phrase> and other <phrase>classification tasks</phrase> but require <phrase>high</phrase> dimensional feature spaces for good performance. <phrase>Deep learning</phrase> methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-<phrase>convex optimization</phrase> problem. We propose a deep non-linear classifier whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original <phrase>data</phrase> <phrase>manifold</phrase> through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than <phrase>kernel-based</phrase> SVMs. This is especially true when the number of <phrase>training samples</phrase> is smaller than the dimensionality of <phrase>data</phrase>, a common scenario in many <phrase>real-world</phrase> applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous often more complicated methods on several vision and speech benchmarks.
Customizing <phrase>News</phrase> Content for Individuals and Communities As <phrase>media</phrase> organizations embrace the <phrase>Internet</phrase> as a viable <phrase>media</phrase>, new models of <phrase>publishing</phrase> are being investigated. This is due both to the need to find a new <phrase>economics</phrase> of customer-driven <phrase>publishing</phrase> services and to the emergence of a more demanding and technologically empowered customer. Traditional <phrase>media</phrase> channels have not been suitable to efficient customization. <phrase>News</phrase> content has been <phrase>produced</phrase> for global or national audiences, or smaller geographic communities, like local <phrase>newspapers</phrase>. <phrase>Internet</phrase>-based customized <phrase>news</phrase> services can serve the special needs of individual <phrase>news</phrase> customers, as well as small, and often geographically <phrase>dispersed</phrase>, communities. Customized forms of <phrase>information</phrase> selection and presentation increase the perceived relevance of <phrase>media</phrase> content, and provide new and potentially more efficient tools for learning and creation of new <phrase>knowledge</phrase>. <phrase>News</phrase> content can be customized by focusing customer's attention to a selected <phrase>subset</phrase> of <phrase>news</phrase> stories, or by augmenting selected <phrase>news</phrase> with related material or illustrative comparisons. Focusing can include filtering, which means showing only the stories that are assumed to be most relevant to the customer, or prioritization, meaning highlighting and emphasizing these stories. The goal of customized <phrase>news</phrase> augmentation is to relate the <phrase>news</phrase> stories better to what the user already knows. Augmentation combines continuous <phrase>news</phrase> streams with automatically selected personal or communal contextual <phrase>information</phrase> from heterogeneous sources. Financial <phrase>news</phrase> augmentation is an example of a service that combines real-time <phrase>business</phrase> <phrase>news</phrase> with customized background <phrase>information</phrase>, term explanations, and comparisons about numerical <phrase>data</phrase> and organizations appearing in the <phrase>news</phrase>. For the purposes of customized services, rich and dynamically changing user and <phrase>community</phrase> models are gathered about the <phrase>media</phrase> customers. User models can consist of special interests, expertise level, previous activity, and <phrase>community</phrase> context of any individual user. <phrase>Community</phrase> models include <phrase>information</phrase> about <phrase>function</phrase>, identity, discourse, and shared values of the <phrase>community</phrase>. When the <phrase>news</phrase> content is matched with customer profiles, a rich description is needed. This calls for deep <phrase>multi-dimensional</phrase> <phrase>metadata</phrase> to be used as a basis for customization. <phrase>News</phrase> providers have multiple options to position themselves in customized <phrase>media</phrase>. <phrase>Internet</phrase> is driving towards a decentralized and <phrase>community</phrase>-oriented mode of <phrase>publishing</phrase>, and <phrase>news</phrase> organizations are losing their traditional position as the sole gatekeeper of <phrase>information</phrase>. Rich customer profiles and <phrase>high</phrase> quality <phrase>metadata</phrase> about the <phrase>media</phrase> content become increasingly valuable, when <phrase>news</phrase> organizations are facing new competition as new intermediaries are collecting this necessary <phrase>information</phrase> for customization. The <phrase>media</phrase> companies have an advantage, if they can 
Restructuring activity and place: <phrase>augmented reality</phrase> <phrase>games</phrase> on handhelds <phrase>Human</phrase> activities are constrained by interconnected and overlapping factors of: biological abilities, time, space, and social narratives. I focus on how the interplay between two of these factors, space and narratives, can be mediated with <phrase>cultural</phrase> tools of <phrase>locative</phrase> technologies such as <phrase>Augmented Reality</phrase> <phrase>games</phrase> and <phrase>GPS</phrase> units. In <phrase>order</phrase> to understand how place-based pedagogies affect learning and how <phrase>locative</phrase> technologies, like Global Positioning Systems (<phrase>GPS</phrase>) and <phrase>Augmented Reality</phrase> <phrase>Games</phrase> on Handhelds (ARGHs), help connect learners to cultures of place I examine experiences with place-based <phrase>video games</phrase> in a deep woods <phrase>camping</phrase> environment. <phrase>Drawing</phrase> together <phrase>research</phrase> in sociocultural learning, <phrase>design</phrase>, embodiment, <phrase>environmental education</phrase>, <phrase>experiential education</phrase>, <phrase>human geography</phrase>, and <phrase>video games</phrase>, this <phrase>paper</phrase> demonstrates how ARGHs can restructure a learning activity to (1) better connect learners to place, (2) increase and mediate their physical activity and social interactions, and (3) help enculturate them into a <phrase>community</phrase> of practice. Towards the end of each summer, while the older boys were doing manly things on the [distant <phrase>mountains</phrase>], we others took part in the wild pursuit of thieves, kidnappers, and other nefarious individuals. That first summer of mine, quite unexpectedly, as we were about to set out on our regularly scheduled trips one Tuesday morning, we were all called together and the cold facts were put before us. Something terrible had happened; I am sure that I don't remember what. Plans had to be changed at the last moment, and all our energies were to be devoted to helping the local authorities, whoever they were, hunt down the criminals and bring them to <phrase>justice</phrase>. At the same time we would uphold the honor of the camp, and in all <phrase>probability</phrase> bring fame and fortune to ourselves and our counselors. Assignments were quickly made. For the <phrase>sake</phrase> of expediency, the original trip groupings would be maintained, but we would travel unexpected paths. All of this had been well arranged beforehand; and I can visualize the counselors now constructing the complicated plot in the evenings after we had gone to bed. Now they were ready to <phrase>play</phrase> it out. I can't remember much of that first <phrase>Mystery</phrase> Trip except that it rained. It rained all the time. The villains, whoever they were, had left clues and trails as they challenged us to <phrase>track</phrase> them down. Coded messages were found and deciphered. The net was slowly tightening. In tracking those undesirables, we learned more than we 
On optimization methods for <phrase>deep learning</phrase> The predominant methodology in training <phrase>deep learning</phrase> advocates the use of <phrase>stochastic gradient descent</phrase> methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up <phrase>deep learning</phrase> <phrase>algorithms</phrase> with SGDs. In this <phrase>paper</phrase>, we show that more sophisticated off-the-shelf optimization methods such as Limited <phrase>memory</phrase> BFGS (L-BFGS) and Conjugate <phrase>gradient</phrase> (CG) with line search can significantly simplify and speed up the process of pretraining deep <phrase>algorithms</phrase>. In our experiments, the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., <phrase>GPUs</phrase> or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and <phrase>convolutional neural networks</phrase>. Using L-BFGS, our <phrase>convolutional network</phrase> <phrase>model</phrase> achieves 0.69% on the standard MNIST dataset. This is a <phrase>state</phrase>-of-the-<phrase>art</phrase> result on MNIST among <phrase>algorithms</phrase> that do not use distortions or pretraining.
<phrase>Information Retrieval</phrase> as Statistical <phrase>Translation</phrase> We propose a new probabilistic approach to <phrase>information retrieval</phrase> based upon the ideas and methods of <phrase>statistical machine translation</phrase>. The central ingredient in this approach is a <phrase>statistical model</phrase> of how a user might distill or \translate" a given document i n to a query. To assess the relevance of a document to a user's query, we estimate the <phrase>probability</phrase> that the query would have been generated as a <phrase>translation</phrase> of the document, and factor in the user's <phrase>general</phrase> preferences in the form of a prior distribution over documents. We propose a simple, well motivated <phrase>model</phrase> of the document-to-query <phrase>translation</phrase> process, and describe an <phrase>algorithm</phrase> for learning the parameters of this <phrase>model</phrase> in an unsupervised manner from a collection of documents. As we show, one can view this approach as a generalization and justiication of the \lan-guage modeling" strategy <phrase>recently proposed</phrase> by P onte and Croft. On a series of experiments, a <phrase>prototype</phrase> <phrase>translation</phrase>-based retrieval system signiicantly outperforms conventional retrieval techniques. This <phrase>prototype</phrase> system is but a <phrase>skin</phrase>-deep implementation of the full <phrase>translation</phrase>-<phrase>based approach</phrase>, and as such only begins to tap the full potential of <phrase>translation</phrase>-based retrieval.
<phrase>Arabic</phrase> Speech <phrase>Pathology</phrase> Therapy <phrase>Computer Aided</phrase> System This article concerns a <phrase>computer aided</phrase> pathological <phrase>speech therapy</phrase> program, based on speech models such as the <phrase>hidden Markov model</phrase> and <phrase>artificial intelligence</phrase> networks, in <phrase>order</phrase> to help persons, suffering from <phrase>language</phrase> pathologies, follow a correction learning process, with different interactive feedbacks, aiming to evaluate the <phrase>degree</phrase> of <phrase>evolution</phrase> of the illness or the therapy. We dealt with the <phrase>Arabic</phrase> occlusive sigmatism as a prime approach, which is the inability to pronounce the[s] or []. <phrase>Results</phrase> obtained are satisfying and the therapy program is prepared, for autonomous use by patients, for deep analysis and verifications.
Neural <phrase>Universal</phrase> Discrete Denoiser We present a new framework of applying <phrase>deep neural networks</phrase> (DNN) to devise a <phrase>universal</phrase> discrete denoiser. Unlike other approaches that utilize <phrase>supervised learning</phrase> for denoising, we do not require any additional <phrase>training data</phrase>. In such setting, while the <phrase>ground-truth</phrase> <phrase>label</phrase>, i.e., the clean <phrase>data</phrase>, is not available, we devise " pseudo-<phrase>labels</phrase> " and a novel <phrase>objective function</phrase> such that DNN can be trained in a same way as <phrase>supervised learning</phrase> to become a discrete denoiser. We experimentally show that our resulting <phrase>algorithm</phrase>, dubbed as Neural DUDE, <phrase>significantly outperforms</phrase> the previous <phrase>state</phrase>-of-the-<phrase>art</phrase> in several applications with a systematic rule of choosing the hyperparameter, which is an attractive feature in practice.
Defensive <phrase>climate</phrase> in the <phrase>computer science</phrase> classroom As part of an <phrase>NSF</phrase>-funded IT Workforce grant, the authors conducted <phrase>ethnographic</phrase> <phrase>research</phrase> to provide <phrase>deep understanding</phrase> of the <phrase>learning environment</phrase> of <phrase>computer science</phrase> classrooms. Categories emerging from <phrase>data analysis</phrase> included 1) impersonal environment and guarded behavior; and 2) the creation and maintenance of informal hierarchy resulting in competitive behaviors. These <phrase>communication</phrase> patterns <phrase>lead</phrase> to a defensive <phrase>climate</phrase>, characterized by competitiveness rather cooperation, judgments about others, superiority, and neutrality rather than <phrase>empathy</phrase>. The authors identify particular and recognizable types of discourse, which, when prevalent in a classroom, can preclude the development of a collaborative and supportive <phrase>learning environment</phrase>.
Measuring Invariances in Deep Networks For many <phrase>pattern recognition</phrase> tasks, the ideal input feature would be invariant to multiple <phrase>confounding</phrase> properties (such as illumination and viewing angle, in <phrase>computer vision</phrase> applications). Recently, <phrase>deep architectures</phrase> trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, it is difficult to evaluate the <phrase>learned features</phrase> by any means other than using them in a classifier. In this <phrase>paper</phrase>, we propose a number of empirical <phrase>tests</phrase> that directly measure the <phrase>degree</phrase> to which these <phrase>learned features</phrase> are invariant to different input transformations. We find that stacked autoencoders learn modestly increasingly <phrase>invariant features</phrase> with depth when trained on <phrase>natural images</phrase>. We find that convolutional <phrase>deep belief</phrase> networks learn substantially more <phrase>invariant features</phrase> in each layer. These <phrase>results</phrase> further justify the use of " deep " vs. " shallower " representations , but suggest that mechanisms beyond merely stacking one autoencoder on top of another may be important for achieving invariance. Our evaluation met-<phrase>rics</phrase> can also be used to evaluate future work in <phrase>deep learning</phrase>, and thus help the development of future <phrase>algorithms</phrase>.
<phrase>Deep Convolutional</phrase> Features for Image Based Retrieval and Scene Categorization Several recent approaches showed how the representations learned by <phrase>Convolutional Neural Networks</phrase> can be re-purposed for novel tasks. Most commonly it has been shown that the activation features of the last <phrase>fully connected</phrase> layers (fc7 or fc6) of the network, followed by a linear clas-sifier outperform the <phrase>state</phrase>-of-the-<phrase>art</phrase> on several recognition challenge datasets. Instead of recognition, this <phrase>paper</phrase> fo-cuses on the <phrase>image retrieval</phrase> problem and proposes a examines <phrase>alternative</phrase> pooling strategies derived for <phrase>CNN</phrase> features. The presented scheme uses the features maps from an earlier layer 5 of the <phrase>CNN</phrase> <phrase>architecture</phrase>, which has been shown to preserve coarse spatial <phrase>information</phrase> and is semantically meaningful. We examine several pooling strategies and demonstrate <phrase>superior</phrase> performance on the <phrase>image retrieval</phrase> task (<phrase>INRIA</phrase> Holidays) at the fraction of the computational cost, while using a relatively small <phrase>memory</phrase> requirements. In addition to retrieval, we see similar efficiency gains on the SUN397 scene categorization dataset, demonstrating wide applicability of this simple strategy. We also introduce and evaluate a novel GeoPlaces5K dataset from different geographical locations in the world for <phrase>image retrieval</phrase> that stresses more dramatic changes in appearance and viewpoint.
An improved <phrase>deep learning</phrase> <phrase>architecture</phrase> for person re-identification In this work, we propose a method for simultaneously learning features and a corresponding similarity metric for person re-identification. We present a <phrase>deep convolutional</phrase> <phrase>architecture</phrase> with layers specially designed to address the problem of <phrase>e</phrase>-identification. Given a pair of images as input, our network outputs a similarity value indicating whether the two input images depict the same person. novel elements of our <phrase>architecture</phrase> include a layer that computes cross-input <phrase>neighborhood</phrase> differences, which capture local relationships between the two input images based on midlevel features from each input image. A <phrase>high</phrase>-level summary of he outputs of this layer is computed by a layer of patch summary features, which re then spatially integrated in subsequent layers. Our method <phrase>significantly outperforms</phrase> the <phrase>state</phrase> of the <phrase>art</phrase> on both a large <phrase>data set</phrase> (CUHK03) and a medium-sized <phrase>data set</phrase> (CUHK01), and is resistant to over-fitting. We also demonstrate hat by initially training on an unrelated large <phrase>data set</phrase> before <phrase>fine-tuning</phrase> on a <phrase>mall</phrase> <phrase>target</phrase> <phrase>data set</phrase>, our network can achieve <phrase>results</phrase> comparable to the <phrase>state</phrase> of he <phrase>art</phrase> even on a small <phrase>data set</phrase> (VIPeR). This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for <phrase>nonprofit</phrase> educational and <phrase>research</phrase> purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of <phrase>Mitsubishi Electric</phrase> <phrase>Research</phrase> Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the <phrase>copyright</phrase> notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to
Automatic <phrase>Semantic</phrase> Role Labeling The goal of <phrase>semantic</phrase> role labeling is to map sentences to <phrase>domain-independent</phrase> <phrase>semantic</phrase> representations, which abstract away from <phrase>syntactic</phrase> structure and are important for deep <phrase>NLP</phrase> tasks such as <phrase>question answering</phrase> , textual entailment, and complex <phrase>information extraction</phrase>. <phrase>Semantic</phrase> role labeling has recently received significant interest in the <phrase>natural language processing</phrase> <phrase>community</phrase>. In this tutorial, we will first describe the problem and <phrase>history</phrase> of <phrase>semantic</phrase> role labeling, and introduce existing corpora and other related tasks. Next, we will provide a detailed survey of <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>machine learning</phrase> approaches to building a <phrase>semantic</phrase> role labeling system. Finally, we will conclude the tutorial by discussing directions for improving <phrase>semantic</phrase> role labeling systems and their application to other <phrase>natural language</phrase> problems.
A concept-relationship acquisition and inference approach for hierarchical <phrase>taxonomy</phrase> <phrase>construction</phrase> from tags <phrase>Taxonomy</phrase> <phrase>construction</phrase> is a resource-demanding, top down, and time consuming effort. It does not always cater for the prevailing context of the captured <phrase>information</phrase>. This <phrase>paper</phrase> proposes a novel approach to automatically convert tags into a hierarchical <phrase>taxonomy</phrase>. <phrase>Folksonomy</phrase> describes the process by which many users add <phrase>metadata</phrase> in the form of keywords or tags to shared content. Using <phrase>folksonomy</phrase> as a <phrase>knowledge</phrase> source for nominating tags, the <phrase>proposed method</phrase> first converts the tags into a hierarchy. This serves to <phrase>harness</phrase> a core set of <phrase>taxonomy</phrase> terms; the generated hierarchical structure facilitates users " <phrase>information</phrase> <phrase>navigation</phrase> behaviour and permits personalizations. Newly acquired tags are then progressively integrated into a <phrase>taxonomy</phrase> in a largely automated way to complete the <phrase>taxonomy</phrase> creation process. Common <phrase>taxonomy</phrase> <phrase>construction</phrase> techniques are based on 3 main approaches: clustering, lexico-<phrase>syntactic</phrase> <phrase>pattern matching</phrase>, and automatic acquisition from machine-readable dictionaries. In contrast to these prevailing approaches, this <phrase>paper</phrase> proposes a <phrase>taxonomy</phrase> <phrase>construction</phrase> analysis based on <phrase>heuristic</phrase> rules and <phrase>deep syntactic</phrase> analysis. The <phrase>proposed method</phrase> requires only a relatively small corpus to create a preliminary <phrase>taxonomy</phrase>. The approach has been evaluated using an expert-defined <phrase>taxonomy</phrase> in the environmental protection domain and encouraging <phrase>results</phrase> were yielded. 1. Introduction <phrase>Folksonomy</phrase> is the end product of a process by which many users add <phrase>metadata</phrase> in the form of keywords or tags to shared content. <phrase>Folksonomy</phrase> is also known as collaborative tagging, social classification, social indexing, and social tagging (Golder and Huberman, 2005). Recently, <phrase>folksonomy</phrase> has grown in popularity on the web, on sites that allow users to freely tag bookmarks, photographs and other content. <phrase>Folksonomy</phrase> is a common way of organizing content for future <phrase>navigation</phrase>, filtering, visualization, and search. Terms in a <phrase>folksonomy</phrase> can be freely chosen by the user; often there is no restriction or prior assumption imposed on the user to provide a tag. Because of this, <phrase>folksonomy</phrase> has certain advantages and disadvantages. Its advantages include easy-to-create and build up (as there is no prior learning needed), <phrase>free</phrase> of control and completely user-driven. As the tags are often created by the originator (and sometimes by other users as well), the tags often reflect the context of the prevailing <phrase>knowledge</phrase> sources. It is a convenient, <phrase>low cost</phrase> and dynamic framework for indexing <phrase>user-generated content</phrase>. However, <phrase>folksonomy</phrase> also has many disadvantages. Firstly, as it is uncontrolled, redundancies, incompleteness, and possibly inconsistencies commonly found in a <phrase>folksonomy</phrase>. Terms (Tags) can be in different word 
On combined coding and <phrase>modulation</phrase> There are many people who have supported and encouraged me during these years. I would like to take this opportunity to especially acknowledge the following people for their contributions, advice and friendship. First of all, I would like to <phrase>express my deep</phrase> gratitude to my Doktorvater, Prof. Dr.-Ing. A. J. Han Vinck, who made this dissertation come into being. I am indebted for his constant assistance, guidance, encouragement and the tremendous support and opportunities he provided throughout my doctoral studies. He spent countless hours <phrase>proofreading</phrase> my <phrase>research</phrase> papers, discussing my <phrase>research</phrase>, evaluating contemporary topics for me to investigate, and providing me with countless ways to improve upon my dissertation. I thank him for encouraging me to participate and present our <phrase>research</phrase> papers in international conferences, which not only improved my confidence but also gave me an opportunity to get <phrase>feedback</phrase> from the leading researchers in the world. I immensely enjoyed and learned from every meeting I had with him, where, besides <phrase>research</phrase>, we discussed a <phrase>variety</phrase> of things in <phrase>life</phrase>. Next, I am deeply grateful to Prof. Dr. Trung van Tran for his constant support, encouragement and being always there for discussions. A special thank goes to Prof. Dr. John B. Anderson for reviewing the previous version of this dissertation and providing valuable comments. Last but not least, I would like to express my heartfelt gratitude to my <phrase>family</phrase> for their unconditional <phrase>love</phrase>, to Pola Richter for her continuous support and to <phrase>Dj</phrase>. Ahmet Sisman for his close friendship.
On assessment of students' <phrase>academic</phrase> achievement considering <phrase>categorized</phrase> individual differences at <phrase>engineering</phrase> <phrase>education</phrase> (<phrase>Neural Networks</phrase> Approach) This work introduces analysis and evaluation of an interesting, challenging, and interdisciplinary, pedagogical issue. That's originated from categorization of the achievement diversity of students' (individual differences), equivalently students' Structure of the Observed Learning Outcome (SOLO). This students' <phrase>academic</phrase> diversity affected in classrooms by three interactive learning/teaching approaches (orientations) namely: surface, deep, and strategic. Assessment of these approaches has been performed via realistic <phrase>simulation</phrase> adopting <phrase>Artificial Neural Networks</phrase> (ANN s) modeling considering Hebbian rule for coincidence detection learning. That modeling <phrase>results</phrase> in interesting <phrase>mathematical</phrase> analogy of two effective learning performance factors with students' achievement individual differences. Firstly, the effect of two <phrase>brain</phrase> functional phenomena; namely <phrase>long term Potentiation</phrase> (<phrase>LTP</phrase>) and <phrase>depression</phrase> (LTD). That's in accordance with opening time for crossing N-methyl-D-<phrase>aspartate</phrase> <phrase>NMDA</phrase> observed at <phrase>hippocampus</phrase> <phrase>brain</phrase> <phrase>area</phrase>. Secondly, the effect of <phrase>neurons</phrase>' number associated with diverse learning/teaching environments comprise the <phrase>dichotomy</phrase> (extroversion/introversion).This <phrase>dichotomy</phrase> has been investigated as the external and internal environmental learning conditions. The obtained <phrase>simulation</phrase> <phrase>results</phrase> concerned with student's diversity attitudes (extroversion/introversion). They shown to be in well agreement with recently <phrase>published results</phrase> after performing a <phrase>case study</phrase> at an <phrase>engineering</phrase> institution in <phrase>Egypt</phrase>. Finally, introduced study, aims mainly to present interesting analysis of brain's functional development based students' individual differences, and learning abilities.
<phrase>Syntactic</phrase> Kernels for <phrase>Natural Language</phrase> Learning: the <phrase>Semantic</phrase> Role Labeling Case In this <phrase>paper</phrase>, we use <phrase>tree</phrase> kernels to exploit <phrase>deep syntactic</phrase> <phrase>parsing</phrase> <phrase>information</phrase> for <phrase>natural language</phrase> applications. We study the properties of different kernels and we provide <phrase>algorithms</phrase> for their computation in linear <phrase>average</phrase> time. The experiments with SVMs on the task of predicate argument classification provide empirical <phrase>data</phrase> that validates our methods.
<phrase>Multi-objective</phrase> <phrase>Monte-Carlo</phrase> <phrase>Tree</phrase> Search Concerned with <phrase>multi-objective</phrase> <phrase>reinforcement learning</phrase> (MORL), this <phrase>paper</phrase> presents MO-MCTS, an extension of <phrase>Monte-Carlo</phrase> <phrase>Tree</phrase> Search to <phrase>multi-objective</phrase> sequential <phrase>decision making</phrase>. The known <phrase>multi-objective</phrase> indicator referred to as hyper-volume indicator is used to define an <phrase>action</phrase> selection criterion, replacing the UCB criterion in <phrase>order</phrase> to deal with <phrase>multi-dimensional</phrase> rewards. MO-MCTS is firstly compared with an existing MORL <phrase>algorithm</phrase> on the <phrase>artificial</phrase> <phrase>Deep Sea</phrase> Treasure problem. Then a <phrase>scalability</phrase> study of MO-MCTS is made on the NP-hard problem of grid scheduling, showing that the performance of MO-MCTS matches the non <phrase>RL</phrase>-based <phrase>state</phrase> of the <phrase>art</phrase> albeit with a higher computational cost.
<phrase>Deep Belief</phrase> Nets as <phrase>Function</phrase> Approximators for <phrase>Reinforcement Learning</phrase> We describe a continuous <phrase>state</phrase>/<phrase>action</phrase> <phrase>reinforcement learning</phrase> method which uses <phrase>deep belief</phrase> networks (DBNs) in conjunction with a value <phrase>function</phrase>-based <phrase>reinforcement learning</phrase> <phrase>algorithm</phrase> to learn effective control policies. Our approach is to first learn a <phrase>model</phrase> of the <phrase>state</phrase>-<phrase>action</phrase> space from <phrase>data</phrase> in an <phrase>unsupervised pre-training</phrase> phase, and then use neural-fitted Q-iteration (NFQ) to learn an accurate value <phrase>function</phrase> approxima-<phrase>tor</phrase> (analogous to a " <phrase>fine-tuning</phrase> " phase when training DBNs for classification). Our experiments suggest that this approach has the potential to significantly increase the efficiency of the learning process in NFQ, provided care is taken to ensure the initial <phrase>data</phrase> covers interesting areas of the <phrase>state</phrase>-<phrase>action</phrase> space, and may be particularly useful in <phrase>transfer learning</phrase> settings.
Online Assessment of <phrase>SQL</phrase> Query Formulation Skills Being able to formulate useful <phrase>SQL</phrase> queries is a fundamental skill required by many <phrase>software development</phrase> professionals. <phrase>Mastering</phrase> this skill is a difficult process, requiring considerable practice and effort on the part of the <phrase>student</phrase>. In addition, assessment of <phrase>SQL</phrase> query formulation skills is a process that does not appear to have been thoroughly researched, and numerous problems are inherent to the approaches commonly taken in <phrase>universities</phrase> to do this assessment. This <phrase>paper</phrase> examines two of these approaches, identifies problems with them, and then proposes another method of assessment. The way that students are assessed in a subject has a significant impact on their learning approach, and it is crucial that assessment tasks are carefully designed and implemented to inculcate a <phrase>deep learning</phrase> experience. The online assessment method proposed is described, evaluated, and the challenges and benefits of using it are discussed. This is a work-in-progress and the conclusion states that further validation is needed, and there is opportunity for additional <phrase>research</phrase> and development in this particular <phrase>area</phrase> of assessment.
<phrase>Deep Learning</phrase> of <phrase>Invariant Features</phrase> via Simulated Fixations in <phrase>Video</phrase> We apply salient feature detection and tracking in videos to simulate fixations and smooth pursuit in <phrase>human</phrase> vision. With tracked sequences as input, a hierarchical network of modules learns <phrase>invariant features</phrase> using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, <phrase>Caltech</phrase> 101, <phrase>STL</phrase>-10, PubFig), and observe a consistent improvement of 4% to 5% in <phrase>classification accuracy</phrase>. With this approach, we achieve <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>recognition accuracy</phrase> 61% on <phrase>STL</phrase>-10 dataset.
<phrase>Basic</phrase> <phrase>medical</phrase> <phrase>science</phrase> <phrase>education</phrase> must include <phrase>medical informatics</phrase>. <phrase>Medical Informatics</phrase> is the <phrase>science</phrase> and <phrase>art</phrase> of processing <phrase>medical</phrase> <phrase>information</phrase>. In this age of "<phrase>Information</phrase> Explosion" choosing the useful one is rather difficult, and there lies the scope of <phrase>electronic</phrase> <phrase>database management</phrase>. However, still many outstanding personnel related to the <phrase>healthcare</phrase> sector take pride in being "computer illiterate". The onus of the best use lies on the end-user <phrase>health care</phrase> providers only. Another term tele-<phrase>health</phrase> encompasses all the <phrase>e</phrase>-<phrase>health</phrase> and <phrase>telemedicine</phrase> services. <phrase>Computer aided</phrase> or assisted learning (CAL) is a computer based tutorial method that uses the computer to pose questions, provide remedial <phrase>information</phrase> and <phrase>chart</phrase> a <phrase>student</phrase> through a course. Now the emphasis in <phrase>medical</phrase> <phrase>education</phrase>, is on <phrase>problem based learning</phrase> (<phrase>PBL</phrase>) and there CAL could be of utmost help if used judiciously. <phrase>Basic</phrase> <phrase>Medical</phrase> <phrase>Education</phrase> and <phrase>Research</phrase> lays the foundation for advancing and applying proper <phrase>healthcare</phrase> delivery systems. There is no doubt that deep <phrase>knowledge</phrase> of <phrase>anatomy</phrase> is <phrase>mandatory</phrase> for successful <phrase>surgery</phrase>. Also, <phrase>comprehensive</phrase> <phrase>knowledge</phrase> of <phrase>physiology</phrase> is essential for grasping the principles of <phrase>pathology</phrase> and <phrase>pharmacology</phrase> adequately, to avoid incorrect and inadequate practice of <phrase>medicine</phrase>. Similarly, <phrase>medical informatics</phrase> is not just a subject to be learnt and forgotten after the first <phrase>professional</phrase> <phrase>MBBS</phrase> examination. The final aim of every <phrase>student</phrase> should not only be to become a good user but also an expert for advancing <phrase>medical</phrase> <phrase>knowledge base</phrase> through <phrase>medical informatics</phrase>. In view of the fast changing world of <phrase>medical informatics</phrase>, it is of utmost necessity to formulate a flexible syllabus rather than a rigid one.
An Adaptive <phrase>Problem-solving</phrase> <phrase>Solution</phrase> to <phrase>Large-scale</phrase> Scheduling Problems Although the <phrase>general</phrase> class of most scheduling problems is NP-hard in worst-case complexity , in practice, <phrase>domain-specific</phrase> techniques frequently <phrase>solve problems</phrase> in much better than exponential time. Unfortunately, constructing special-purpose systems is a knowledgein-tensive and time-consuming process that requires a <phrase>deep understanding</phrase> of the domain and <phrase>problem-solving</phrase> <phrase>architecture</phrase>. The goal of our work is to develop techniques to allow for automated learning of an effective <phrase>domain-specific</phrase> search strategy given a <phrase>general</phrase> problem solver with a flexible control <phrase>architecture</phrase>. In this approach, a learning system explores a space of possible <phrase>heuristic</phrase> methods a strategy well-suited to the regularities of the given domain and problem distribution. We discuss an application of our approach to scheduling <phrase>satellite communications</phrase>. Using problem <phrase>distributions</phrase> based on actual mission requirements, our approach identifies strategies that not only decrease the amount of <phrase>CPU</phrase> time required to produce schedules, but also increase the percentage of problems that are solvable within computational resource limitations.
Trained <phrase>Ternary</phrase> Quantization <phrase>Deep neural networks</phrase> are widely used in <phrase>machine learning</phrase> applications. However, the deployment of large <phrase>neural networks</phrase> models can be difficult to deploy on <phrase>mobile</phrase> devices with limited power budgets. To solve this problem, we propose Trained <phrase>Ternary</phrase> Quantization (TTQ), a method that can reduce the precision of weights in <phrase>neural networks</phrase> to <phrase>ternary</phrase> values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet <phrase>model</phrase> is trained from scratch, which means it's as easy as to <phrase>train</phrase> normal full precision <phrase>model</phrase>. We highlight our trained quantization method that can learn both <phrase>ternary</phrase> values and <phrase>ternary</phrase> assignment. During inference, only <phrase>ternary</phrase> values (2-<phrase>bit</phrase> weights) and scaling factors are needed, therefore our models are nearly 16 smaller than full-precision models. Our <phrase>ternary</phrase> models can also be viewed as sparse <phrase>binary</phrase> weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the <phrase>ternary</phrase> models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our <phrase>model</phrase> outperforms full-precision AlexNet <phrase>model</phrase> by 0.3% of Top-1 accuracy and outperforms previous <phrase>ternary</phrase> models by 3%.
Latent learning in deep <phrase>neural nets</phrase> <phrase>Psychologists</phrase> define latent learning as learning that occurs without task-specific reinforcement and is not demonstrated until needed. Since this <phrase>knowledge</phrase> is acquired while <phrase>mastering</phrase> some other task(s), it is a form of <phrase>transfer learning</phrase>. We utilize latent learning to enable a deep <phrase>neural net</phrase> to distinguish among a set of handwritten numerals. The accuracies obtained are compared to those achievable with a simplistic 'group-mean' classification technique, which is explained later in this <phrase>paper</phrase>. The deep <phrase>neural net</phrase> <phrase>architecture</phrase> used was a Le-<phrase>Net 5</phrase> [3] convolutional <phrase>neural net</phrase> with only minor differences in the output layer.
Serious <phrase>Games</phrase> for <phrase>Language</phrase> Learning: How Much <phrase>Game</phrase>, How Much <phrase>AI</phrase>? Modern computer <phrase>games</phrase> show potential not just for engaging and entertaining users, but also in promoting learning. <phrase>Game</phrase> designers employ a <phrase>range</phrase> of techniques to promote <phrase>long</phrase>-term user engagement and <phrase>motivation</phrase>. These techniques are increasingly being employed in so-called serious <phrase>games</phrase>, <phrase>games</phrase> that have non-<phrase>entertainment</phrase> purposes such as <phrase>education</phrase> or training. Although such <phrase>games</phrase> share the goal of AIED of promoting deep learner engagement with <phrase>subject matter</phrase>, the techniques employed are very different. Can AIED technologies <phrase>complement</phrase> and enhance serious <phrase>game design</phrase> techniques, or does good serious <phrase>game design</phrase> render AIED techniques superfluous? This <phrase>paper</phrase> explores these questions in the context of the Tactical <phrase>Language</phrase> Training System (TLTS), a program that supports rapid acquisition of foreign <phrase>language</phrase> and <phrase>cultural</phrase> skills. The TLTS combines <phrase>game design</phrase> principles and <phrase>game development</phrase> tools with learner modelling, pedagogical agents, and pedagogical dramas. Learners carry out missions in a simulated <phrase>game</phrase> world, interacting with non-<phrase>player characters</phrase>. A virtual <phrase>aide</phrase> assists the learners if they run into difficulties, and gives performance <phrase>feedback</phrase> in the context of preparatory exercises. <phrase>Artificial intelligence</phrase> plays a key role in controlling the behaviour of the non-<phrase>player characters</phrase> in the <phrase>game</phrase>; <phrase>intelligent tutoring</phrase> provides supplementary <phrase>scaffolding</phrase>.
Loganswer -a Deduction-based <phrase>Question Answering</phrase> System LogAnswer is an open domain <phrase>question answering</phrase> system which employs an automated theorem prover to infer correct replies to <phrase>natural language</phrase> questions. For this purpose LogAnswer operates on a large <phrase>axiom</phrase> set in <phrase>first-order logic</phrase>, representing a formalized <phrase>semantic network</phrase> acquired from extensive textual <phrase>knowledge</phrase> bases. While other work in the field of <phrase>question answering</phrase> focuses on shallow <phrase>linguistic</phrase> methods , LogAnswer emphasizes the use of automatic reasoning. The <phrase>logic</phrase>-<phrase>based approach</phrase> allows the formalization of <phrase>semantics</phrase> and <phrase>background knowledge</phrase>, which <phrase>play</phrase> a vital role in deriving answers. Our approach seeks to combine logical inference with less precise but robust methods from <phrase>machine learning</phrase>. We present the functional LogAnswer <phrase>prototype</phrase>, which consists of automated theorem provers for logical answer derivation as well as an environment for <phrase>deep linguistic</phrase> processing.
<phrase>Information Retrieval</phrase> <phrase>Journal</phrase> Call for Papers <phrase>Special Issue</phrase> on Neural <phrase>Information Retrieval</phrase> Topics of Interest <phrase>Recent advances</phrase> in the application of <phrase>neural network</phrase> models have yielded noteworthy progress in a <phrase>variety</phrase> of <phrase>machine learning</phrase> and <phrase>artificial intelligence</phrase> tasks. In <phrase>speech recognition</phrase> and visual recognition benchmarks, neural models showed significant performance improvements. They also have given impetus to promising new applications such as conversational assistants, automatic scene captioning for the visually-impaired, and speech to speech <phrase>translation</phrase>. Given the impact of neural models, there is clear anticipation that they will also have a <phrase>major</phrase> impact on <phrase>information retrieval</phrase> (IR) tasks. However, despite expectations, the <phrase>area</phrase> of Neural IR has developed more slowly than areas like <phrase>natural language processing</phrase> (<phrase>NLP</phrase>). This could be due to fundamental differences between IR and <phrase>NLP</phrase>, in tasks and datasets. It could also be because the intersection between those working on <phrase>neural network</phrase> models and those working on core IR tasks has been small so far. The goal of this <phrase>special issue</phrase> is to provide an opportunity for researchers working at the intersection of <phrase>information retrieval</phrase> and <phrase>neural networks</phrase> to examine the challenges of applying neural models (both shallow and deep) to IR tasks, present key breakthroughs, and demonstrate improvements over the <phrase>current state</phrase> of the <phrase>art</phrase>. Topics for this issue include the application of <phrase>neural network</phrase> models in IR tasks, including but not limited to:
Assessing Inquiry Learning Inquiry and <phrase>River City</phrase> In this <phrase>paper</phrase>, we provide an overview of the <phrase>design</phrase> of an inquiry-based <phrase>curriculum</phrase> project, and then offer a comparative analysis of the outcomes of two methods for assessing <phrase>student</phrase> understanding of the inquiry process. Our findings indicate that the complex <phrase>nature</phrase> of <phrase>scientific inquiry</phrase> is better captured using an <phrase>alternative</phrase> method of assessment in addition to a more traditional <phrase>multiple-choice</phrase> <phrase>test</phrase>. recently issued a position statement recommending the use of <phrase>science</phrase> inquiry as a method to help students understand the processes and content of <phrase>science</phrase> (<phrase>National Science Teachers Association</phrase>, 2004). However, currently, there is a competing push in <phrase>science</phrase> for coverage of material found on <phrase>state</phrase> and national <phrase>standardized tests</phrase>; in many situations, this competing push forces the emphasis in <phrase>science</phrase> classrooms to change from inquiry-based instruction to <phrase>test-preparation</phrase> (Falk & Drayton, 2004). Could this dilemma of teaching scientific process versus covering <phrase>test</phrase> content be resolved via the inclusion of more inquiry-based questions on these <phrase>standardized tests</phrase>? While this may provide teachers and schools with incentives to <phrase>cover</phrase> inquiry skills as well as factual content, this <phrase>solution</phrase> raises a different concern: Can learning from good inquiry-based projects be adequately assessed using a <phrase>standardized test</phrase> format? What kind of assessments will allow valid inferences about whether a <phrase>student</phrase> has learned how to engage in inquiry, particularly in the " front end " inquiry processes used to derive a strategy for making sense out of complexity: problem finding, <phrase>hypothesis</phrase> formation, <phrase>experimental</phrase> <phrase>design</phrase>? Using an <phrase>NSF</phrase>-funded MultiUser <phrase>Virtual Environment</phrase> (MUVE) as a pedagogical vehicle, our <phrase>research</phrase> team is exploring how a <phrase>technology</phrase>-intensive learning experience that immerses participants in a virtual " world " whose citizens face chronic illnesses can help <phrase>middle school</phrase> <phrase>students learn</phrase> both deep inquiry skills and <phrase>science</phrase> <phrase>knowledge</phrase>. In this <phrase>paper</phrase>, we provide an overview of the <phrase>design</phrase> of this inquiry-based <phrase>curriculum</phrase> project. We then offer a comparative analysis of the outcomes of two methods of assessing <phrase>student</phrase> understanding of the inquiry process in <phrase>order</phrase> to clarify the extent to which typical forms of <phrase>test</phrase> items can validly measure students' inquiry skills. Theoretical Underpinnings Inquiry What is " inquiry? " The <phrase>range</phrase> of possible responses to this question is large. Some refer to inquiry as a set of process skills that include questioning, hypothesizing and testing while others equate it to " hands-on " learning. The National <phrase>Science</phrase> <phrase>Education</phrase> Standards (NSES) define <phrase>scientific inquiry</phrase> as " the diverse ways 
Evaluating EmotiBlog Robustness for <phrase>Sentiment Analysis</phrase> Tasks EmotiBlog is a corpus labelled with the homonymous annotation schema designed for detecting <phrase>subjectivity</phrase> in the new textual genres. Preliminary <phrase>research</phrase> demonstrated its relevance as a <phrase>Machine Learning</phrase> resource to detect opinionated <phrase>data</phrase>. In this <phrase>paper</phrase> we compare EmotiBlog with the JRC corpus in <phrase>order</phrase> to check the EmotiBlog robustness of annotation. For this <phrase>research</phrase> we concentrate on its <phrase>coarse-grained</phrase> <phrase>labels</phrase>. We carry out a deep ML experimentation also with the inclusion of lexical resources. The <phrase>results</phrase> obtained show a similarity with the ones obtained with the JRC demonstrating the EmotiBlog validity as a resource for the SA task.
Identifying Defects in <phrase>Deep-submicron</phrase> <phrase>Cmos</phrase> <phrase>Ics</phrase> Given the oft-cited difficulty of testing modern <phrase>integrated circuits</phrase>, the fact that <phrase>CMOS</phrase> <phrase>ICs</phrase> lend themselves to IDDQ testing is a piece of good fortune. But that valuable advantage is threatened by the <phrase>rush</phrase> of <phrase>semiconductor</phrase> <phrase>technology</phrase> to smaller feature sizes and faster, denser circuits, in line with the <phrase>Semiconductor Industry</phrase> Association's (<phrase>SIA</phrase>) Roadmap-its forecast for the <phrase>CMOS</phrase> IC <phrase>industry</phrase>. With <phrase>safety</phrase> margins for reliability, <phrase>test</phrase>, <phrase>failure analysis</phrase>, and <phrase>design</phrase> verification shrinking, it would be a shame to give up the IDDQ technique-and luckily, we may not have to. Steps can be taken to maintain its applicability as we <phrase>rush</phrase> deeper into the submicron regime. Before discussing them, however, a brief discussion of IDDQ testing seems to be in <phrase>order</phrase>. We will first examine why the IDDQ <phrase>test</phrase> serves several interests, then describe the challenge posed by 0.35-0.07-m <phrase>transistor</phrase> geometries, and finally propose several solutions. <phrase>CMOS</phrase> IC power supply current can be <phrase>amperes</phrase> during <phrase>logic</phrase> <phrase>state</phrase> transitions, but only nanoamperes during the <phrase>steady state</phrase>, or quiescent, portion of the <phrase>clock cycle</phrase>. This low quiescent power supply current, known as IDDQ, is what gives <phrase>CMOS</phrase> its traditional <phrase>low-power</phrase> edge over its <phrase>technology</phrase> competitors. But it does more than that. Engineers in <phrase>design</phrase>, fabrication, and <phrase>test</phrase> have learned to use this low quiescent current as a sensitive <phrase>test</phrase> to identify defects, which often prove to be the reason for customer returns, whether as <phrase>test</phrase> escapes or reliability failures. In fact, <phrase>test</phrase> escape levels below 200 parts per million have recently been attainable only by adding IDDQ testing. The technique has also eliminated the need for burn-in for some mature product lines. More, IDDQ measurements speed <phrase>failure analysis</phrase> by providing current-<phrase>voltage</phrase> signatures and temporal characteristics. Detecting defects with current Current is a more effective parameter than <phrase>voltage</phrase> for defect detection in <phrase>CMOS</phrase> <phrase>ICs</phrase>, although both are necessary for complete testing. The simple <phrase>logic</phrase> circuit with a bridging defect shown in Fig. 1 illustrates how IDDQ increases in the presence of a flaw. Bridging defects and certain open-circuit defects typically elevate the nanoampere levels of a normal circuit by two to seven <phrase>orders of magnitude</phrase>, providing very sensitive defect detection. If <phrase>ICs</phrase> are correctly designed and fabricated for low background current, then IDDQ is a relatively simple measurement with many benefits. But as background IDDQ rises, for whatever the reason, the effectiveness of IDDQ testing diminishes. For optimum detection of <phrase>manufacturing</phrase> defects, the defect-<phrase>free</phrase> 
<phrase>Deep belief</phrase> nets for <phrase>natural language</phrase> call-routing This <phrase>paper</phrase> considers application of <phrase>Deep Belief</phrase> Nets (DBNs) to <phrase>natural language</phrase> call routing. DBNs have been successfully applied to a number of tasks, including image, audio and speech classification , thanks to the recent discovery of an efficient learning technique. DBNs learn a <phrase>multi-layer</phrase> <phrase>generative model</phrase> from <phrase>unlabeled data</phrase> and the features discovered by this <phrase>model</phrase> are then used to initialize a <phrase>feed-forward</phrase> <phrase>neural network</phrase> which is <phrase>fine-tuned</phrase> with <phrase>backpropagation</phrase>. We compare a DBN-initialized <phrase>neural network</phrase> to three widely used text classification <phrase>algorithms</phrase>; <phrase>Support Vector machines</phrase> (<phrase>SVM</phrase>), Boosting and Maximum <phrase>Entropy</phrase> (MaxEnt). The DBN-based <phrase>model</phrase> gives a callrouting <phrase>classification accuracy</phrase> that is equal to the best of the other models even though it currently uses an impoverished representation of the input.
Training in Profiling, Negotiation and <phrase>Crisis Management</phrase> - Using an Immersive and Adaptive Environment The aim of the proposed position <phrase>paper</phrase> is to identify an <phrase>information</phrase> system that can support the lifelong training of various types of demanding learners who have to transact and to do <phrase>business</phrase> with " unknown " people, in unfamiliar environments. Three types of learners are chosen as potential groups that comply with these characteristics: a) diplomatic staff, b) <phrase>security</phrase> staff and c) <phrase>business</phrase> experts (who work for multilateral companies). An environment that will provide engaging and motivating educational experiences to these learner targets is useful for these <phrase>target</phrase> groups, since it (a) utilizes a rich <phrase>knowledge base</phrase> of appropriately coded experience in negotiation methods, <phrase>crisis management</phrase>, <phrase>decision making</phrase> and legal affairs, (b) employs immersive interfaces to provide trainees with a first person learning experience, (c) takes into account the personal profile and background of each trainee in <phrase>order</phrase> to achieve <phrase>deep learning</phrase>, (d) is based on the pedagogical principles of socio-constructivist theories to achieve <phrase>long</phrase>-time <phrase>knowledge</phrase> retention, (<phrase>e</phrase>) incorporates methods of <phrase>information retrieval</phrase> for automatic profile extraction and (g) utilizes <phrase>social networking</phrase> analysis for automatic team creation. The presented system can be built by exploiting recent relevant <phrase>research</phrase> on <phrase>knowledge representation</phrase>, learner modelling, adaptive <phrase>hypermedia</phrase> systems, immersive applications, <phrase>text mining</phrase> and automatic profile extraction, and <phrase>social networking</phrase> <phrase>technology</phrase>.
Integrity as Moral Ideal and <phrase>Business</phrase> Benchmark The increasing <phrase>appeal</phrase> of integrity in <phrase>business</phrase> may not only imply an acknowledgment that integrity has in fact all along been in <phrase>short</phrase> supply, but also suggest that its moral value has been <phrase>short</phrase>-changed into its <phrase>function</phrase> to increase profit. It will therefore be necessary to reconstruct the concept of integrity in <phrase>light</phrase> of an interpretation that takes up the various strands of meaning and ties them together in the <phrase>comprehensive</phrase> vision of the good <phrase>life</phrase> grounded in the <phrase>ethics</phrase> of respect for persons. This will put the moral implications of integrity into sharper profile and set benchmarks for socially attuned and responsible <phrase>business</phrase>. <phrase>Financial Crisis</phrase> as Wake-Up Call In September 2009, <phrase>news</phrase> broke that the <phrase>G20</phrase> <phrase>finance</phrase> ministers had reached a compromise on the reform of the financial markets. They all agreed that lessons must be learned that would prevent another crisis of this <phrase>magnitude</phrase>. While deep divisions remained, the compromise reached has been regarded as a <phrase>milestone</phrase> on several accounts. For one, it acknowledges that something in the regulatory systems of financial markets didn't work as <phrase>economists</phrase> had always maintained it would. They now had to admit that the market left to itself did not exercise the self-healing powers they had all along relied on. Instead, it had to be referred to the care of <phrase>government</phrase>-appointed <phrase>doctors</phrase> prescribing tough <phrase>emergency medicine</phrase> in the form of billions in guarantees and stimulus packages for the collapsing <phrase>economy</phrase> that would cost the taxpayer dearly. Furthermore, the politicians firmly acknowledged in principle that the much-criticized bonus payments did in fact <phrase>play</phrase> a role in triggering the crisis and that in future they should therefore only be paid on companies' <phrase>long</phrase>-term success. Risky decisions on <phrase>short</phrase>-term gains would have to be balanced off against the possibility of losses that could be claimed back from those whose decisions caused them. Finally, they agreed on more transparency in the financial markets and clearer regulations that would give oversight bodies like the <phrase>central banks</phrase> greater monitoring powers that would allow them to step in and to impose restrictions on too risky or shady <phrase>products</phrase> that could destabilize the financial system. In all of this, the term integrity was never mentioned. Yet it may be the <phrase>single</phrase> most important term looming in the background of this debate. The global <phrase>financial crisis</phrase> could even be the exemplary case for a study of integrity or rather the lack 
Designing Deep Networks for Surface Normal Estimation Input Image! Input Image! Input Im Output! Output! Figure 1: Sample <phrase>results</phrase>. Notice our method predicts not only the coarse structure correctly, but also many of the details (e.g., the legs of the <phrase>coffee table</phrase>). On the right, the chair surface and legs and even the top of the shopping bags are captured correctly. <phrase>Convolutional neural networks</phrase> (CNNs) have shown incredible promise in learning representations for visual tasks such as scene classification and <phrase>object detection</phrase>. But their performance tasks such as 3D scene understanding has been not as extensively studied. In this <phrase>paper</phrase>, we want to explore the effectiveness of CNNs on the task of predicting surface orientation, or surface normals, from a <phrase>single</phrase> image. One could treat this as per-<phrase>pixel</phrase> <phrase>regression</phrase> and directly apply a <phrase>CNN</phrase>. However, decades of <phrase>research</phrase> has shown that the output space of this task is governed by powerful physical constraints. In this <phrase>paper</phrase>, we demonstrate how to incorporate insights about 3D representation and reasoning into a <phrase>deep learning</phrase> framework for surface normal prediction. While CNNs have been particularly successful for learning <phrase>image representations</phrase>, we believe their <phrase>design</phrase> can benefit from past <phrase>research</phrase> in 3D scene understanding. Thus, rather use a standard <phrase>feed-forward</phrase> network, our network structure (see Fig. 2), incorporates the following insights: 1. Global and local: we include complementary global and local networks. The global network predicts coarse layout from the whole image and the local network operates in a sliding-window <phrase>fashion</phrase>, considering local evidence. Their competing predictions are fed into a final <phrase>fusion</phrase> network that arbitrates between the two. This <phrase>fusion</phrase> network can be seen as a form of learned reasoning. 2. Man-made constraints: our global network is also trained to predict a box-layout, and we estimate vanishing points in the input images. The box-layout prediction as well as the coarse predictions snapped to the vanishing points are passed on to the <phrase>fusion</phrase> network. 3. Local structure: our local network is trained to predict classic line-labeling categories, and its predictions are fed to the <phrase>fusion</phrase> network. Surface normal prediction is a structured problem; we reduce it to a <phrase>classification problem</phrase> with the coding scheme introduced in [3]. We validate our method on the NYUv2 dataset [4], training with the provided raw <phrase>video</phrase> <phrase>data</phrase> and adopting the per-<phrase>pixel</phrase> evaluation protocol introduced by [1]. We show some qualitative <phrase>results</phrase> in Fig. 1: our method not only captures the coarse structure but also many of the fine details. We show additional <phrase>results</phrase> in 
<phrase>Feedback</phrase> for Metacognitive Support in <phrase>Learning by Teaching</phrase> Environments Past <phrase>research</phrase> on <phrase>feedback</phrase> in computer-based <phrase>learning environments</phrase> has shown that corrective <phrase>feedback</phrase> helps immediate learning, whereas guided and metacognitive <phrase>feedback</phrase> help in gaining <phrase>deep understanding</phrase> and developing the ability to transfer <phrase>knowledge</phrase>. <phrase>Feedback</phrase> becomes important in discovery <phrase>learning environments</phrase>, where novice students are often overwhelmed by the <phrase>cognitive load</phrase> associated with learning and organizing new <phrase>knowledge</phrase> while monitoring their own learning progress. We focus on <phrase>feedback</phrase> mechanisms in teachable agent systems to help improve students' abilities to monitor their agent's <phrase>knowledge</phrase>, and, in the process their own learning and understanding. Our studies demonstrate the effectiveness of guided metacognitive <phrase>feedback</phrase> in preparing students for future learning.
Unsupervised Duplicate Detection Using Sample Non-duplicates Dissertation Von <phrase>Diplom</phrase>-ingenieur The problem of identifying objects in <phrase>databases</phrase> that refer to the same <phrase>real world</phrase> entity, is known, among others, as duplicate detection or record linkage. Objects may be duplicates, even though they are not identical due to errors and missing <phrase>data</phrase>. Traditional scenarios for duplicate detection are <phrase>data</phrase> warehouses, which are populated from several <phrase>data</phrase> sources. Duplicate detection here is part of the <phrase>data</phrase> cleansing process to improve <phrase>data quality</phrase> for the <phrase>data warehouse</phrase>. More recently in application scenarios like web portals, that offer users unified access to several <phrase>data</phrase> sources, or meta <phrase>search engines</phrase>, that distribute a search to several other resources and finally merge the individual <phrase>results</phrase>, the problem of duplicate detection is also present. In such scenarios no <phrase>long</phrase> and expensive <phrase>data</phrase> cleansing process can be carried out, but good duplicate estimations must be available directly. The most common approaches to duplicate detection use either rules or a weighted aggregation of similarity measures between the individual attributes of potential duplicates. However, choosing the appropriate rules, similarity functions, weights, and thresholds requires <phrase>deep understanding</phrase> of the application domain or a good representative <phrase>training set</phrase> for <phrase>supervised learning</phrase> approaches. For this reason, these approaches entail significant costs. This <phrase>thesis</phrase> presents an unsupervised, <phrase>domain independent</phrase> approach to duplicate detection that starts with a broad alignment of potential duplicates , and analyses the distribution of observed similarity values among these potential duplicates and among representative sample non-duplicates to improve the initial alignment. To this end, a refinement of the classic Fellegi-Sunter <phrase>model</phrase> for record linkage is developed, which makes use of these <phrase>distributions</phrase> to iteratively remove clear non-duplicates from the set of potential duplicates. Alternatively also <phrase>machine learning</phrase> methods like <phrase>Support Vector Machines</phrase> are used and compared with the refined Fellegi-Sunter <phrase>model</phrase>. iii iv ABSTRACT Additionally, the presented approach is not only able to align flat records, but makes also use of related objects, which may significantly increase the alignment accuracy, depending on the application. Evaluations show that the approach supersedes other unsupervised approaches and reaches almost the same accuracy as even fully supervised, domain dependent approaches. A very special thank goes to my advisor Peter Fankhauser, who helped me creating many new ideas in innumerable discussions and even more valuable , his empathetic motivations to overcome the naturally occurring crisis during the writing of such a <phrase>thesis</phrase>. I further thank all my colleagues for all the fruitful discussions and creating 
Performing Effective <phrase>Feature Selection</phrase> by Investigating the Deep Structure of the <phrase>Data</phrase> This <phrase>paper</phrase> introduces ADHOC (Automatic Discov-erer of <phrase>Higher-Order</phrase> Correlation), an <phrase>algorithm</phrase> that combines the advantages of both filter and <phrase>feedback</phrase> models to enhance the understanding of the given <phrase>data</phrase> and to increase the efficiency of the <phrase>feature selection</phrase> process. ADHOC <phrase>partitions</phrase> the observed features into a number of groups, called factors, that reflect the <phrase>major</phrase> dimensions of the phenomenon under consideration. The set of learned factors define the <phrase>starting point</phrase> of the search of the best performing feature <phrase>subset</phrase>. A <phrase>genetic algorithm</phrase> is used to explore the <phrase>feature space</phrase> originated by the factors and to determine the set of most informative feature configurations. The feature <phrase>subset</phrase> evaluation <phrase>function</phrase> is the performance of the induction <phrase>algorithm</phrase>. This approach offers three main advantages: (i) the likelihood of selecting good performing features grows; (<phrase>ii</phrase>) the complexity of search diminishes consistently; (iii) the possibility of selecting a bad feature <phrase>subset</phrase> due to over-fitting problems decreases. <phrase>Extensive experiments</phrase> on <phrase>real-world</phrase> <phrase>data</phrase> have been conducted to demonstrate the effectiveness of ADHOC as <phrase>data</phrase> reduction technique as well as <phrase>feature selection</phrase> method.
Learning Detectors from <phrase>Large Datasets</phrase> for Object Retrieval in <phrase>Video</phrase> <phrase>Surveillance</phrase> We address the problem of learning robust and efficient <phrase>multi-view</phrase> object detectors for <phrase>surveillance</phrase> <phrase>video</phrase> indexing and retrieval. Our <phrase>philosophy</phrase> is that effective solutions for this problem can be obtained by learning detectors from huge amounts of <phrase>training data</phrase>. Along this <phrase>research</phrase> direction, we propose a novel approach that consists of strategically partitioning the <phrase>training set</phrase> and learning a large array of complementary, compact, deep <phrase>cascade</phrase> detectors. At <phrase>test</phrase> time, given a <phrase>video</phrase> <phrase>sequence</phrase> captured by a fixed <phrase>camera</phrase>, a small number of detectors is automatically selected per image location. We demonstrate our approach on the problem of vehicle detection in challenging <phrase>surveillance</phrase> scenarios, using a large training dataset composed of around one million images. Our system <phrase>runs</phrase> at an impressive <phrase>average</phrase> rate of 125 frames per second on a conventional <phrase>laptop</phrase> computer.
Seeing Eye-to-eye: Supporting Transdisciplinary Learning What is <phrase>Interactive Art</phrase>? What is <phrase>Programming</phrase>? For the purposes of this <phrase>paper</phrase>, <phrase>interactive art</phrase> is an <phrase>art</phrase> system that changes as a result of the presence of or <phrase>action</phrase> by the audience-participant. Viewing <phrase>interactive art</phrase> as an <phrase>art</phrase> system shows us that it is actually quite a complex field, involving various creators and audiences, not just a set of computational artefacts with an 'optimal' configuration. We can say that <phrase>programming</phrase> is the articulation of statements in a <phrase>programming language</phrase>. Yet any reasonable definition of <phrase>programming</phrase> today (for example, the common one that <phrase>programming</phrase> is a specification of a computation) can describe all uses of a computer. This means that there is no particular <phrase>ontological</phrase> distinction between <phrase>programming</phrase> a computer and using it, so we might as well call all uses of a computer different forms of '<phrase>programming</phrase>'. We can use differences between different levels of <phrase>programming</phrase> to place <phrase>programming languages</phrase> on a continuum between two <phrase>poles</phrase>, termed here 'popular <phrase>programming</phrase>' and 'deep <phrase>programming</phrase>'. 'Popular <phrase>programming</phrase>' <phrase>ranges</phrase> from simple direct manipulation, through developing <phrase>spreadsheet</phrase> formulae, to, at best, hacking another's <phrase>JavaScript</phrase>, Max or <phrase>Director</phrase> code. 'Deep <phrase>programming</phrase>' is characterised by expert usage of a <phrase>general</phrase>-purpose <phrase>language</phrase> , such as C or <phrase>assembly language</phrase>, in combination with an expert <phrase>knowledge</phrase> of computer <phrase>architecture</phrase>. For the <phrase>sake</phrase> of simplicity, the term '<phrase>programming</phrase>' will from now on be used to refer to the entire continuum between popular and deep. So, by 'ability to program', I mean 'ability to deep program'. Why is <phrase>Programming</phrase> Important to <phrase>Art</phrase>? We have known for some time that <phrase>computers</phrase> allow us to think thoughts that are impossible to think with any other tool, but what is it about the tool that allows new thought? An examination of the important developments in <phrase>computing</phrase> <phrase>history</phrase> indicates following four technological strengths, which I call the four <phrase>Ss</phrase>: speed, <phrase>slavery</phrase>, <phrase>synaesthesia</phrase> and structure. Other significant developments (such as the <phrase>Internet</phrase>) exploit one or more of these. Treated briefly here, 'Speed' refers to the computer's ability to do certain things quicker than we can. In <phrase>interactive art</phrase>, the goal is often to generate the response 'in real time'. '<phrase>Slavery</phrase>' is descriptive of both the incredible cheapness and unquestioning obedience of computation, allowing artists and programmers to create massive and wide-ranging programmatic edifices. Such power carries with it the danger of genie-in-the-bottle or sorcerer's apprentice-style mishaps. '<phrase>Synaesthesia</phrase>' is another way of describing that all 
Transfer of conflict and cooperation from experienced <phrase>games</phrase> to new <phrase>games</phrase>: a <phrase>connectionist</phrase> <phrase>model</phrase> of learning The question of whether, and if so how, learning can be transfered from previously experienced <phrase>games</phrase> to novel <phrase>games</phrase> has recently attracted the attention of the <phrase>experimental</phrase> <phrase>game theory</phrase> <phrase>literature</phrase>. Existing <phrase>research</phrase> presumes that learning operates over actions, beliefs or decision rules. This study instead uses a <phrase>connectionist</phrase> approach that learns a direct mapping from <phrase>game</phrase> payoffs to a <phrase>probability distribution</phrase> over own actions. Learning is operationalized as a <phrase>backpropagation</phrase> rule that adjusts the weights of <phrase>feedforward neural</phrase> networks in the direction of increasing the <phrase>probability</phrase> of an agent playing a <phrase>myopic</phrase> best response to the last <phrase>game</phrase> played. One advantage of this approach is that it expands the scope of the <phrase>model</phrase> to any possible n n normal-form <phrase>game</phrase> allowing for a <phrase>comprehensive</phrase> <phrase>model</phrase> of transfer of learning. Agents are exposed to <phrase>games</phrase> drawn from one of seven classes of <phrase>games</phrase> with significantly different strategic characteristics and then forced to <phrase>play</phrase> <phrase>games</phrase> from previously unseen classes. I find significant transfer of learning, i.e., behavior that is path-dependent, or conditional on the previously seen <phrase>games</phrase>. Cooperation is more pronounced in new <phrase>games</phrase> when agents are previously exposed to <phrase>games</phrase> where the incentive to cooperate is stronger than the incentive to compete, i.e., when individual incentives are aligned. Prior exposure to <phrase>Prisoner's dilemma</phrase>, zero-sum and discoordination <phrase>games</phrase> <phrase>led</phrase> to a significant decrease in realized payoffs for all the <phrase>game</phrase> classes under investigation. A distinction is made between superficial and <phrase>deep transfer</phrase> of learning both-the former is driven by superficial payoff similarities between <phrase>games</phrase>, the latter by differences in the incentive structures or strategic implications of the <phrase>games</phrase>. I examine whether agents learn to <phrase>play</phrase> the <phrase>Nash equilibria</phrase> of <phrase>games</phrase>, how they select amongst multiple equilibria, and whether they transfer <phrase>Nash equilibrium</phrase> behavior to unseen <phrase>games</phrase>. Sufficient exposure to a strategically heterogeneous set of <phrase>games</phrase> is found to be a necessary condition for <phrase>deep learning</phrase> (and transfer) across <phrase>game</phrase> classes. Paradoxically, superficial transfer of learning is shown to <phrase>lead</phrase> to better outcomes than <phrase>deep transfer</phrase> for a wide <phrase>range</phrase> of <phrase>game</phrase> classes. The <phrase>simulation</phrase> <phrase>results</phrase> corroborate important <phrase>experimental</phrase> findings with <phrase>human</phrase> subjects, and make several novel predictions that can be tested experimentally.
<phrase>Mass</phrase> detection in <phrase>digital</phrase> <phrase>breast</phrase> tomosynthesis: <phrase>Deep convolutional</phrase> <phrase>neural network</phrase> with <phrase>transfer learning</phrase> from <phrase>mammography</phrase>. PURPOSE Develop a <phrase>computer-aided</phrase> detection (<phrase>CAD</phrase>) system for masses in <phrase>digital</phrase> <phrase>breast</phrase> tomosynthesis (DBT) volume using a <phrase>deep convolutional</phrase> <phrase>neural network</phrase> (DCNN) with <phrase>transfer learning</phrase> from mammograms. METHODS A <phrase>data set</phrase> containing 2282 digitized <phrase>film</phrase> and <phrase>digital</phrase> mammograms and 324 DBT volumes were collected with IRB approval. The <phrase>mass</phrase> of interest on the images was marked by an experienced <phrase>breast</phrase> <phrase>radiologist</phrase> as reference standard. The <phrase>data set</phrase> was partitioned into a <phrase>training set</phrase> (2282 mammograms with 2461 masses and 230 DBT views with 228 masses) and an <phrase>independent</phrase> <phrase>test</phrase> set (94 DBT views with 89 masses). For DCNN training, the <phrase>region</phrase> of interest (ROI) containing the <phrase>mass</phrase> (true positive) was extracted from each image. False positive (FP) ROIs were identified at prescreening by their previously developed <phrase>CAD</phrase> systems. After <phrase>data</phrase> augmentation, a total of 45072 mammographic ROIs and 37450 DBT ROIs were obtained. <phrase>Data</phrase> normalization and reduction of non-uniformity in the ROIs across heterogeneous <phrase>data</phrase> was achieved using a background correction method applied to each ROI. A DCNN with four convolutional layers and three <phrase>fully connected</phrase> (FC) layers was first trained on the <phrase>mammography</phrase> <phrase>data</phrase>. Jittering and dropout techniques were used to reduce <phrase>overfitting</phrase>. After training with the mammographic ROIs, all weights in the first three convolutional layers were frozen, and only the last <phrase>convolution</phrase> layer and the FC layers were randomly initialized again and trained using the DBT training ROIs. The authors compared the performances of two <phrase>CAD</phrase> systems for <phrase>mass</phrase> detection in DBT: one used the DCNN-<phrase>based approach</phrase> and the other used their previously developed feature-<phrase>based approach</phrase> for FP reduction. The prescreening stage was identical in both systems, passing the same set of <phrase>mass</phrase> candidates to the FP reduction stage. For the feature-based <phrase>CAD</phrase> system, 3D clustering and active contour method was used for segmentation; morphological, gray level, and texture features were extracted and merged with a linear <phrase>discriminant</phrase> classifier to score the detected masses. For the DCNN-based <phrase>CAD</phrase> system, ROIs from five consecutive slices centered at each candidate were passed through the trained DCNN and a <phrase>mass</phrase> likelihood score was generated. The performances of the <phrase>CAD</phrase> systems were evaluated using <phrase>free</phrase>-response <phrase>ROC</phrase> curves and the performance difference was analyzed using a non-parametric method. <phrase>RESULTS</phrase> Before <phrase>transfer learning</phrase>, the DCNN trained only on mammograms with an <phrase>AUC</phrase> of 0.99 classified DBT masses with an <phrase>AUC</phrase> of 0.81 in the DBT <phrase>training set</phrase>. After <phrase>transfer learning</phrase> with DBT, the <phrase>AUC</phrase> improved to 0.90. For <phrase>breast</phrase>-based <phrase>CAD</phrase> detection in the <phrase>test</phrase> set, the sensitivity for the feature-based and the DCNN-based <phrase>CAD</phrase> systems was 83% and 91%, respectively, at 1 FP/DBT volume. The difference between the performances for the two systems was statistically significant (<phrase>p-value</phrase> < 0.05). CONCLUSIONS The image patterns learned from the mammograms were transferred to the <phrase>mass</phrase> detection on DBT slices through the DCNN. This study demonstrated that large <phrase>data</phrase> sets collected from <phrase>mammography</phrase> are useful for developing new <phrase>CAD</phrase> systems for DBT, alleviating the problem and effort of collecting entirely new large <phrase>data</phrase> sets for the new modality.
Stacks of convolutional <phrase>Restricted Boltzmann Machines</phrase> for shift-invariant <phrase>feature learning</phrase> In this <phrase>paper</phrase> we present a method for learning class-specific features for recognition. Recently a <phrase>greedy layer-wise</phrase> procedure was proposed to initialize weights of <phrase>deep belief</phrase> networks, by viewing each layer as a separate <phrase>Restricted Boltzmann Machine</phrase> (RBM). We develop the Con-volutional RBM (C-RBM), a variant of the RBM <phrase>model</phrase> in which weights are shared to respect the spatial structure of images. This framework learns a set of features that can generate the images of a specific object class. Our <phrase>feature extraction</phrase> <phrase>model</phrase> is a four layer hierarchy of alternating filtering and maximum subsampling. We learn feature parameters of the first and third layers viewing them as separate C-RBMs. The outputs of our <phrase>feature extraction</phrase> hierarchy are then fed as input to a discriminative classifier. It is experimentally demonstrated that the extracted features are effective for <phrase>object detection</phrase>, using them to obtain performance comparable to the <phrase>state</phrase>-of-the-<phrase>art</phrase> on <phrase>handwritten digit</phrase> recognition and pedestrian detection.
Towards <phrase>deeper understanding</phrase>: Deep convex networks for <phrase>semantic</phrase> utterance classification Following the <phrase>recent advances</phrase> in <phrase>deep learning</phrase> techniques, in this <phrase>paper</phrase>, we present the application of special type of <phrase>deep architecture</phrase> deep convex networks (DCNs) for <phrase>semantic</phrase> utterance classification (SUC). DCNs are shown to have several advantages over <phrase>deep belief</phrase> networks (DBNs) including <phrase>classification accuracy</phrase> and training <phrase>scalability</phrase>. However, <phrase>adoption</phrase> of DCNs for SUC comes with non-trivial issues. Specifically, SUC has an extremely sparse input <phrase>feature space</phrase> encompassing a very large number of lexical and <phrase>semantic</phrase> features. This is about a few thousand times larger than the <phrase>feature space</phrase> for <phrase>acoustic</phrase> modeling, yet with a much smaller number of <phrase>training samples</phrase>. <phrase>Experimental</phrase> <phrase>results</phrase> we obtained on a domain classification task for <phrase>spoken language</phrase> understanding demonstrate the effectiveness of DCNs. The DCN-based method produces higher SUC accuracy than the Boosting-based discriminative classifier with word trigrams.
A <phrase>SVM</phrase> <phrase>Cascade</phrase> for Agreement/Disagreement Classification This article describes a method for classifying dialogue utterances and detecting the interlocutor's agreement or disagreement. This labelling can help improve dialogue <phrase>management</phrase> by providing additional <phrase>information</phrase> on the utterance's content without deep <phrase>parsing</phrase>. The proposed technique improves upon <phrase>state</phrase> of the <phrase>art</phrase> approaches by using a <phrase>Support Vector Machine</phrase> <phrase>cascade</phrase>. A combination of three <phrase>binary</phrase> <phrase>support vector machines</phrase> in a <phrase>cascade</phrase> is employed to filter out utterances that are easy to classify, thus reducing the noise in the learning of <phrase>labels</phrase> for more ambiguous utterances. The approach achieves higher accuracy (by 2.47%) than the <phrase>state</phrase> of the <phrase>art</phrase> while using a simpler approach which relies only on shallow local features of the utterances. RSUM. Dans <phrase>cet</phrase> article, nous dcrivons une mthode de classification d'uttrances destine la detection d'accord/dsaccord dans le dialogue homme-machine. L'tiquetage du dialogue peut tre utilis par le dialogue <phrase>manager</phrase> sans avoir effectuer de parse complexe. Nous propo-sons une technique de classification base d'une hirarchie de classificateurs <phrase>Support Vector Machines</phrase>. Une combinaison de trois classificateurs binaires est utilise pour filtrer les classes pour lesquelles le corpus contient beaucoup d'information et se concentrer sur les classes plus ambiges. <phrase>Cet</phrase> article prsente une analyse dtaille des traits caractristiques de classification et propose une amlioration de 2.47% sur l'tat de l'art tout en utilisant un modle de classification plus performant.
Rule Extraction <phrase>Algorithm</phrase> for <phrase>Deep Neural Networks</phrase>: A Review Despite the highest <phrase>classification accuracy</phrase> in wide varieties of application areas, <phrase>artificial neural network</phrase> has one disadvantage. The way this Network comes to a decision is not easily comprehensible. The lack of explanation ability reduces the acceptability of <phrase>neural network</phrase> in <phrase>data mining</phrase> and decision system. This drawback is the reason why researchers have proposed many rule extraction <phrase>algorithms</phrase> to solve the problem. Recently, <phrase>Deep Neural Network</phrase> (DNN) is achieving a profound result over the standard <phrase>neural network</phrase> for classification and recognition problems. It is a hot <phrase>machine learning</phrase> <phrase>area</phrase> <phrase>proven</phrase> both useful and innovative. This <phrase>paper</phrase> has thoroughly reviewed various rule extraction <phrase>algorithms</phrase>, considering the classification scheme: decompositional, pedagogical, and eclectics. It also presents the evaluation of these <phrase>algorithms</phrase> based on the <phrase>neural network</phrase> structure with which the <phrase>algorithm</phrase> is intended to work. The main contribution of this review is to show that there is a limited study of rule extraction <phrase>algorithm</phrase> from DNN.
Uwb Impulse <phrase>Radio</phrase> <phrase>Waveform</phrase> Shaping Techniques for Narrow-<phrase>band</phrase> Interference Rejection In <phrase>ultra-wideband</phrase> (UWB) impulse <phrase>radio</phrase> (IR) <phrase>technology</phrase>, it is important that the pulse shape be adjusted such that the power spectral characteristics not only satisfy the standard limits, but also be characterized by deep nulls at the locations of existing <phrase>narrowband</phrase> primary users. <phrase>Cognitive Radio</phrase> <phrase>Technology</phrase> (CR) is presented as a viable <phrase>solution</phrase> for UWB systems for narrow-<phrase>band</phrase> interference suppression. In this <phrase>paper</phrase>, we present and compare several <phrase>ultra-wideband</phrase> (UWB) impulse <phrase>radio</phrase> <phrase>waveform</phrase> shaping techniques for narrow-<phrase>band</phrase> interference (NBI) rejection. The generated UWB pulses not only meet the <phrase>Federal Communications Commission</phrase> (<phrase>FCC</phrase>), but also mitigate <phrase>single</phrase> and multiple narrow-<phrase>band</phrase> interference. I. INTRODUCTION <phrase>Ultra-wideband</phrase> <phrase>radio</phrase> is a promising <phrase>technology</phrase> for <phrase>high</phrase> <phrase>data</phrase> rate <phrase>short</phrase>-<phrase>range</phrase> <phrase>wireless communication</phrase>. Compared with the conventional <phrase>narrowband</phrase> (NB) <phrase>communication</phrase> systems, UWB systems have many advantages, e.g. reduced complexity, <phrase>low power</phrase> consumption, immunity to <phrase>multipath</phrase> fading, <phrase>high</phrase> <phrase>security</phrase>, etc. [1]-[3]. Since UWB systems transfer <phrase>information</phrase> <phrase>data</phrase> by using extremely <phrase>short</phrase> duration pulses, they have considerably large bandwidth. <phrase>FCC</phrase> regulates UWB systems can exploit the <phrase>frequencies</phrase> from 3.1 GHz to 10.6 GHz [4]. From Shannon <phrase>channel capacity</phrase> [5], it is evident that UWB systems can achieve higher capacity than any other current <phrase>wireless communication</phrase> systems. However, in <phrase>order</phrase> to reduce the interference between UWB systems and the existing NB systems, <phrase>FCC</phrase> presents a UWB spectral mask to restrict the power <phrase>spectrum</phrase> of UWB systems. <phrase>Cognitive radio</phrase> (CR) aims at a very efficient <phrase>spectrum</phrase> utilization employing smart <phrase>wireless</phrase> devices with awareness, sensing, learning, and adaptation capabilities [6-8]. As a <phrase>solution</phrase> for the <phrase>spectrum</phrase> scarcity problem, <phrase>cognitive radio</phrase> proposes an opportunistic <phrase>spectrum</phrase> usage approach [7], in which <phrase>frequency</phrase> bands that are not being used by their primary (<phrase>licensed</phrase>) users are utilized by <phrase>cognitive</phrase> radios. Thus, CR <phrase>technology</phrase> is presented as a viable <phrase>solution</phrase> for UWB systems for narrow-<phrase>band</phrase> interference suppression. The <phrase>spectrum</phrase> of a transmitted signal is influenced by the <phrase>modulation</phrase> format, the multiple access schemes, and most critically by the spectral shape of the underlying UWB pulse. The choice of the pulse shape is thus a key <phrase>design</phrase> decision in UWB systems. Several pulse <phrase>design methods</phrase> of UWB signals have been proposed to let them match with the <phrase>FCC</phrase> spectral mask. The simple Gaussian monocycle pulses need to be filtered to meet the <phrase>FCC</phrase> spectral mask. This leads that the time duration of the corresponding pulses becomes too <phrase>long</phrase>. On the other hand, Gaussian derivatives pulses [9] have fixed features, 
Diagram Interaction during <phrase>Intelligent Tutoring</phrase> in <phrase>Geometry</phrase>: Support for <phrase>Knowledge</phrase> Retention and <phrase>Deep Understanding</phrase> Prior <phrase>research</phrase> has shown that skilled problem solvers often use features of <phrase>visual representations</phrase> to cue relevant <phrase>knowledge</phrase>, but little is known about how to support learners in developing connections between visual and verbal <phrase>knowledge</phrase> components. In this <phrase>research</phrase>, we investigated two methods to support focus on key <phrase>visual features</phrase> during <phrase>problem solving</phrase> in an intelligent <phrase>tutor</phrase>: 1) <phrase>student</phrase> interaction with diagrams during <phrase>problem solving</phrase>, and 2) <phrase>student</phrase> explanations that connected diagram features to <phrase>geometry</phrase> rules at each <phrase>problem-solving</phrase> step. <phrase>Research</phrase> was conducted in 10 th grade classrooms using an <phrase>experimental</phrase> version of the <phrase>Geometry</phrase> <phrase>Cognitive</phrase> <phrase>Tutor</phrase>. Interaction with diagrams promoted <phrase>long</phrase>-term retention of <phrase>problem-solving</phrase> skills and supported <phrase>deep understanding</phrase> of <phrase>geometry</phrase> rules, as evidenced by items testing transfer and visual-verbal <phrase>knowledge</phrase> integration. Diagram-rule explanations did not significantly influence learning. Findings suggest that <phrase>student</phrase> focus on relevant visual <phrase>information</phrase> should be carefully integrated into <phrase>problem-solving</phrase> practice to support robust learning. <phrase>Visual Representations</phrase> in Skilled Performance Existing <phrase>research</phrase> has found that experts use <phrase>visual representations</phrase> in rich and interconnected ways during skilled <phrase>problem solving</phrase>. Stylianou (2002) studied the <phrase>problem-solving</phrase> processes of <phrase>professional</phrase> <phrase>mathematicians</phrase> and noted that <phrase>mathematicians</phrase> used diagrams extensively to inform their analysis of the problem, their selection of subgoals, and their eventual solutions. During <phrase>problem solving</phrase>, <phrase>mathematicians</phrase> created <phrase>visual representations</phrase> in a step-by-step manner, where visual <phrase>information</phrase> in the representation was analyzed at each step in <phrase>order</phrase> to inform reasoning and to cue relevant approaches. <phrase>Mathematicians</phrase> recognized important features and patterns in their diagrams and revised or annotated their diagrams to reflect the outcome of their analysis at each step. Stylianou's (2002) <phrase>results</phrase> <phrase>complement</phrase> previous <phrase>research</phrase> in expert <phrase>problem solving</phrase> that has demonstrated close connections between <phrase>visual representations</phrase> and existing <phrase>knowledge</phrase>. Koedinger and Anderson (1990) found that experts solving <phrase>geometry</phrase> problems made inferences that were strongly tied to <phrase>geometry</phrase> diagrams, and that features in the problem diagrams cued relevant <phrase>problem-solving</phrase> steps. Koedinger and Anderson found that the <phrase>problem solving</phrase> steps mentioned and skipped by experts could be successfully predicted by a <phrase>model</phrase> (the Diagram Configuration <phrase>Model</phrase>) that parsed diagrams into key geometric configurations and used these configurations to cue relevant schemas. The development of skilled performance in <phrase>geometry</phrase> appears to be correlated with attention to key diagram features, as well as successful association of those features with relevant <phrase>geometry</phrase> rules. Recent <phrase>eye-tracking</phrase> <phrase>research</phrase> suggests that learner focus on key visual <phrase>information</phrase> predicts successful performance even among non-experts. On insight 
Turning on minds with <phrase>computers</phrase> in the kitchen: supporting group reflection in the midst of engaging in hands-on activities How can we promote the kinds of reflection needed for deep and lasting learning and the development of disposition toward scientific reasoning in the context of an <phrase>informal learning</phrase> <phrase>community</phrase>? In our <phrase>research</phrase>, we've discovered that learners have a greater appreciation of what they are learning when we give them the goal of helping others outside their <phrase>community</phrase>. This appreciation is demonstrated by their willingness to jot down notes during activities and later write articles for an online <phrase>cooking</phrase> " <phrase>magazine</phrase>. " The online <phrase>cooking</phrase> <phrase>magazine</phrase> has the potential to support learning and development of disposition toward scientific reasoning in several ways. It provides a place to hang <phrase>scaffolding</phrase> that promotes recognizing what's been learned, what <phrase>led</phrase> to successes, and how <phrase>science</phrase> contributed to those successes. It also provides a context for <phrase>knowledge</phrase> building in which learners create <phrase>concrete</phrase> artifacts they can share outside of the Kitchen <phrase>Science</phrase> Investigators <phrase>community</phrase>. We found that with <phrase>computers</phrase> in the kitchen and an <phrase>online magazine</phrase> to contribute to, participants were stopping and reflecting in ways that we had only seen previously when a facilitator was prompting them.
Intelligent Collaborating Agents to Support <phrase>Teaching and Learning</phrase> This <phrase>paper</phrase> presents the Intelligent Multiagent <phrase>Infrastructure</phrase> for Distributed System in <phrase>Education</phrase> (I-MINDS), an innovative application using <phrase>AI</phrase> and mul-tiagent systems to help teachers teach better and <phrase>students learn</phrase> better. The I-MINDS system consists of a group of <phrase>intelligent agents</phrase> that work cooperatively in a <phrase>distributed computing</phrase> environment. A <phrase>teacher</phrase> agent monitors the <phrase>student</phrase> activities and helps the <phrase>teacher</phrase> manage and better adapt to the class. A <phrase>student</phrase> agent interacts with the <phrase>teacher</phrase> agent and other <phrase>student</phrase> agents to support <phrase>cooperative learning</phrase> activities behind the scene for a <phrase>student</phrase>. This <phrase>paper</phrase> describes two innovations in (a) automated ranking of questions and responses, and (b) agent-supported " buddy group " formation. The <phrase>results</phrase> of the <phrase>proof-of-concept</phrase> <phrase>tests</phrase> have demonstrated encouraging effectiveness of I-MINDS in terms of learning gain and <phrase>deep understanding</phrase> .
Teaching <phrase>data structures</phrase> with BeSocratic (abstract only) <phrase>Data structures</phrase> are one of the fundamental concepts that all computer <phrase>scientist</phrase> students must learn if they are to succeed in their careers. Therefore, it is important to develop and assess questions targeted at improving the teaching of <phrase>data structures</phrase>. Unfortunately, <phrase>research</phrase> suggests that <phrase>multiple choice</phrase> or matching questions cannot be used to properly assess deep <phrase>knowledge</phrase> on a subject [1,2,3,4]. Students can often guess their way to the correct answer. We believe that students must construct these structures instead of simply identifying them. However, analyzing many hand-drawn <phrase>data structures</phrase> is time-consuming for large class sizes. This <phrase>poster</phrase> describes a <phrase>web-based</phrase> <phrase>software</phrase> tool, <i>BeSocratic</i>, designed to facilitate <phrase>interactivity</phrase> in a <phrase>data structures</phrase> course. <i>BeSocratic</i> allows students to build <phrase>data structures</phrase> intuitively using a combination of <phrase>handwriting recognition</phrase> and gestures. Using <i>BeSocratic</i>, instructors can create intelligent tutors that teach students to construct various <phrase>data structures</phrase>. These tutors are able to identify problems and provide multi-tiered <phrase>feedback</phrase> to students. Furthermore, <i>BeSocratic</i> records each <phrase>action</phrase> a <phrase>student</phrase> makes, so it may be replayed and visualized to gain deeper insights into how students construct <phrase>data structures</phrase> and complete <phrase>algorithms</phrase>. We have created and <phrase>pilot</phrase>-tested a <i>BeSocratic</i> activity, which teaches students how to construct splay <phrase>trees</phrase>.
Working Together for Better <phrase>Student</phrase> Learning: A Multi-<phrase>University</phrase>, Multi-<phrase>Federal</phrase> Partner Program for Asynchronous Learning Module Development for <phrase>Radar</phrase>-Based <phrase>Remote Sensing</phrase> Systems Students are not exposed to enough <phrase>real-life</phrase> <phrase>data</phrase>. This <phrase>paper</phrase> describes how a <phrase>community</phrase> of scholars seeks to remedy this deficiency and gives the pedagogical details of an ongoing project that commenced in the Fall 2004 semester. Fostering <phrase>deep learning</phrase>, this multiyear project offers a new <phrase>active-learning</phrase>, hands-on inter-disciplinary <phrase>laboratory</phrase> program in which <phrase>engineering</phrase>, geoscience, and <phrase>meteorology</phrase> students are encouraged to participate actively. Storms, <phrase>tornadoes</phrase>, and hazardous <phrase>weather</phrase> cause damage and loss that could be minimized through enhanced <phrase>radar</phrase> technologies and longer warning <phrase>lead</phrase> times. To study these topics, the program has generated a unique, interdisciplinary <phrase>research</phrase>-oriented <phrase>learning environment</phrase> that will <phrase>train</phrase> future engineers and <phrase>meteorologists</phrase> in the full set of competencies needed to take raw <phrase>radar</phrase> <phrase>data</phrase> and transform them into meaningful interpretations of <phrase>weather</phrase> phenomena .
<phrase>Convolutional Neural Network</phrase> and <phrase>Convex Optimization</phrase> This <phrase>report</phrase> shows that the performance of <phrase>deep convolutional</phrase> <phrase>neural network</phrase> can be improved by incorporating <phrase>convex optimization</phrase> techniques. First, we find that the sub-models learned by dropout can be more effectively combined by solving a convex problem. Also, we generalize this idea to models that are not trained by dropout. Compared to traditional methods, we get an improvement of 0.22% and 0.76% <phrase>test</phrase> accuracy on CIFAR10 dataset. Second, we investigate the performance for different loss functions borrowed from the <phrase>convex optimization</phrase> <phrase>community</phrase> and find that selecting loss functions matters a lot. We also implement a novel loss based on the idea of One-Versus-One <phrase>SVM</phrase>, which has never been explored in the <phrase>literature</phrase>. Experiment shows that it can give performance comparable to the standard cross-<phrase>entropy</phrase> loss, without being fully tuned.
Feature-based <phrase>Image Registration</phrase> <phrase>Master</phrase> of <phrase>Technology</phrase> in <phrase>Telematics</phrase> and <phrase>Signal Processing</phrase> Feature-based <phrase>Image Registration</phrase> <phrase>Master</phrase> of <phrase>Technology</phrase> in <phrase>Telematics</phrase> and <phrase>Signal Processing</phrase> National Institute of <phrase>Technology</phrase> <phrase>Rourkela</phrase> CERTIFICATE This is to certify that the <phrase>thesis</phrase> entitled " FEATURE-BASED <phrase>IMAGE REGISTRATION</phrase> " submitted by Mr. SOMARAJU BODA in partial fulfillment of the requirements for the award of <phrase>Master</phrase> of <phrase>Technology</phrase> <phrase>Degree</phrase> in <phrase>Electronics</phrase> and <phrase>Communication</phrase> <phrase>Engineering</phrase> with specialization in " <phrase>Telematics</phrase> and <phrase>Signal Processing</phrase> " at National Institute of <phrase>Technology</phrase>, <phrase>Rourkela</phrase> (Deemed <phrase>University</phrase>) is an authentic work carried out by him under my supervision and guidance. To the best of my <phrase>knowledge</phrase>, the <phrase>matter</phrase> embodied in the <phrase>thesis</phrase> has not been submitted to any other <phrase>University</phrase> / Institute for the award of any <phrase>Degree</phrase> or <phrase>Diploma</phrase>. <phrase>Rourkela</phrase> ACKNOWLEDGEMENTS First of all, I would like to <phrase>express my deep</phrase> sense of respect and gratitude towards my advisor and guide Prof. U.C. Pati, who has been the guiding force behind this work. I am greatly indebted to him for his constant encouragement, invaluable advice and for propelling me further in every <phrase>aspect</phrase> of my <phrase>academic</phrase> <phrase>life</phrase>. His presence and optimism have provided an invaluable influence on my career and outlook for the future. I consider it my good fortune to have got an opportunity to work with such a wonderful person. for teaching me and also helping me how to learn. They have been great sources of inspiration to me and I thank them from the bottom of my <phrase>heart</phrase>. I would like to thank all <phrase>faculty members</phrase> and staff of the <phrase>Department</phrase> of <phrase>Electronics</phrase> and <phrase>Communication</phrase> <phrase>Engineering</phrase>, N.I.T. <phrase>Rourkela</phrase> for their generous help in various ways for the completion of this <phrase>thesis</phrase>. I would like to thank all my <phrase>friends</phrase> and especially my classmates for all the thoughtful and mind stimulating discussions we had, which prompted us to think beyond the obvious. I " ve enjoyed their companionship so much during my stay at <phrase>NIT</phrase>, <phrase>Rourkela</phrase>. I am especially indebted to my parents for their <phrase>love</phrase>, sacrifice, and support. They are my first teachers after I came to this world and have set great examples for me about how to <phrase>live</phrase>, study, and work.
Defining a Viable <phrase>Multimedia</phrase> Courseware <phrase>Engineering</phrase> Practice <phrase>Multimedia</phrase> courseware that is well designed can facilitate and promote very positive and <phrase>deep learning</phrase> experiences for end users. There are many issues surrounding the development and use of effective <phrase>educational software</phrase> systems that will produce these types of meaningful learning activities. One of the biggest problems encountered is the disjointed relationship between what a client wants or expects and what is in fact realistic. Related to this are factors such as cost and quality. By examining various attributes of common courseware <phrase>products</phrase> and their associated <phrase>engineering</phrase> concepts, tools and practices, we can better understand how to put <phrase>engineering</phrase> systems in place through examples and best practices that will increase efficiency, effectiveness and accuracy, and reduce time to delivery ratios. This <phrase>paper</phrase> discusses the product and process, and various <phrase>management</phrase> strategies. I use the term 'approach' in the title so as not to raise expectations about establishing a rigid framework for the <phrase>industry</phrase> as a whole.
Inference of abduction theories for handling incompleteness in first-<phrase>order</phrase> learning In <phrase>real-life</phrase> domains, learning systems often have to deal with various kinds of imperfections in <phrase>data</phrase> such as noise, incompleteness and inexactness. This problem seriously affects the <phrase>knowledge</phrase> discovery process, specifically in the case of traditional <phrase>Machine Learning</phrase> approaches that exploit simple or constrained <phrase>knowledge</phrase> representations and are based on <phrase>single</phrase> inference mechanisms. Indeed, this limits their capability of discovering fundamental <phrase>knowledge</phrase> in those situations. In <phrase>order</phrase> to broaden the investigation and the applicability of <phrase>machine learning</phrase> schemes in such particular situations, it is necessary to move on to more expressive representations which require more complex inference mechanisms. However, the applicability of such new and complex inference mechanisms, such as <phrase>abductive reasoning</phrase>, strongly relies on a deep <phrase>background knowledge</phrase> about the specific application domain. This work aims at automatically discovering the meta-<phrase>knowledge</phrase> needed to abduction inference strategy to complete the incoming <phrase>information</phrase> in <phrase>order</phrase> to handle cases of missing <phrase>knowledge</phrase>.
<phrase>Complex-valued</phrase> autoencoders Autoencoders are unsupervised <phrase>machine learning</phrase> circuits, with typically one <phrase>hidden layer</phrase>, whose learning goal is to minimize an <phrase>average</phrase> <phrase>distortion</phrase> measure between inputs and outputs. Linear autoencoders correspond to the special case where only linear transformations between visible and hidden variables are used. While linear autoencoders can be defined over any field, only <phrase>real-valued</phrase> linear autoencoders have been studied so far. Here we study <phrase>complex-valued</phrase> linear autoencoders where the components of the training vectors and adjustable matrices are defined over the complex field with the L(2) norm. We provide simpler and more <phrase>general</phrase> proofs that unify the <phrase>real-valued</phrase> and <phrase>complex-valued</phrase> cases, showing that in both cases the <phrase>landscape</phrase> of the <phrase>error function</phrase> is invariant under certain groups of transformations. The <phrase>landscape</phrase> has no <phrase>local minima</phrase>, a <phrase>family</phrase> of global minima associated with <phrase>Principal Component Analysis</phrase>, and many families of saddle points associated with orthogonal projections onto sub-space spanned by sub-optimal subsets of <phrase>eigenvectors</phrase> of the <phrase>covariance matrix</phrase>. The theory yields several iterative, <phrase>convergent</phrase>, <phrase>learning algorithms</phrase>, a clear understanding of the generalization properties of the trained autoencoders, and can equally be applied to the hetero-associative case when external targets are provided. Partial <phrase>results</phrase> on <phrase>deep architecture</phrase> as well as the <phrase>differential geometry</phrase> of autoencoders are also presented. The <phrase>general</phrase> framework described here is useful to classify autoencoders and identify <phrase>general</phrase> properties that ought to be investigated for each class, illuminating some of the connections between autoencoders, <phrase>unsupervised learning</phrase>, clustering, Hebbian learning, and <phrase>information theory</phrase>.
Efficient Cross-Domain Learning of Complex Skills Building an <phrase>intelligent agent</phrase> that simulates <phrase>human</phrase> learning of <phrase>math</phrase> and <phrase>science</phrase> could potentially benefit both <phrase>education</phrase>, by contributing to the understanding of <phrase>human</phrase> learning, and <phrase>artificial intelligence</phrase> , by advancing the goal of creating <phrase>human</phrase>-level <phrase>intelligence</phrase>. However, constructing such a learning agent currently requires significant manual encoding of prior <phrase>domain knowledge</phrase>; in addition to being a poor <phrase>model</phrase> of <phrase>human</phrase> acquisition of <phrase>prior knowledge</phrase>, manual <phrase>knowledge</phrase>-encoding is both time-consuming and error-prone. Recently, we proposed an efficient <phrase>algorithm</phrase> that automatically acquires <phrase>domain-specific</phrase> <phrase>prior knowledge</phrase> in the form of deep features. We integrate this deep feature learner into a <phrase>machine-learning</phrase> agent, SimStudent. To evaluate the generality of the <phrase>proposed approach</phrase> and the effect of integration on <phrase>prior knowledge</phrase>, we carried out a controlled <phrase>simulation</phrase> study in three domains, fraction addition, equation solving, and <phrase>stoichiometry</phrase>, using problems solved by <phrase>human</phrase> students. The <phrase>results</phrase> show that the integration reduces SimStudent's dependence over <phrase>domain-specific</phrase> <phrase>prior knowledge</phrase>, while maintains SimStudent's performance.
Constructive Induction of Features for Planning Constructive induction techniques use <phrase>constructors</phrase> to combine existing features into new features. Usually the goal is to improve the accuracy and/or efficiency of classification. An alternate use of new features is to create representations which allow planning in more efficient <phrase>state</phrase> spaces. An inefficient <phrase>state space</phrase> may be too <phrase>fine grained</phrase>, requiring deep search for plans with many steps, may be too fragmented, requiring separate plans for similar cases, or may be unfocused, resulting in poorly <phrase>directed</phrase> search. Modifying the representation with constructive induction can improve the <phrase>state space</phrase> and overcome these inefficiencies. Additionally, since most learning systems depend on good domain features, constructive induction will compliment the <phrase>action</phrase> of other <phrase>algorithms</phrase>. This abstract describes a system that uses constructive induction to generate new <phrase>state</phrase> features in the Tic-Tat-Toe (TTT) domain. The system generates features like win, block, and fork that are useful for planning in TTT. TTT has been chosen as an initial domain because it is simple, the features are well defined , and it is clear what <phrase>domain knowledge</phrase> has been added. Additionally, previous work on constructive induction in the TTT domain provides a <phrase>starting point</phrase>. The CITRE system (Matheus & Rendell 1989) creates a <phrase>decision tree</phrase> with primitive TTT features (the contents of the board positions) and uses a <phrase>binary</phrase> and constructor to incrementally combine features selected to improve the <phrase>decision tree</phrase>. <phrase>Domain knowledge</phrase> filters out less promising features. CITRE minimally improves classification of board positions but cannot generate some types of planning features. Our system has fewer constraints and includes extensions that expand the space of <phrase>constructible</phrase> features. For example, n-<phrase>ary</phrase> conjunction (not <phrase>binary</phrase>) is used to combine existing features into more complex features and n-<phrase>ary</phrase> disjunc-tion groups symmetrical versions of the same feature. Also, <phrase>constructors</phrase> are applied to all pairs of features, including a new " player to move " feature, without using <phrase>domain knowledge</phrase> as a filter. The perfect " X win " feature is a <phrase>disjunction</phrase> of the 24 ways two X's in a row can appear in TTT. Each such row is represented by a conjunction of primitive features (e.g. and (posll=X) (posl2=X) (to-move=X)). To construct new features, the system calculates the <phrase>information</phrase> gain of each existing feature. Due to the <phrase>symmetry</phrase> of TTT states, symmetrical features have equal <phrase>information</phrase> gains and the same parent primitive features and can be correctly grouped into dis-junctions (e.g. or (posll=X) (poslS=X) (pos31=X) (pos33=X)). Taking 
<phrase>Multiple Kernel</phrase> <phrase>Support Vector</phrase> <phrase>Regression</phrase> for Pricing Nifty Option The goal of present experiments is to investigate the use of <phrase>multiple kernel</phrase> learning as a tool for pricing options in the context of <phrase>Indian</phrase> <phrase>stock market</phrase> for Nifty index options. In this <phrase>paper</phrase>, <phrase>fair</phrase> price of an option is predicted by <phrase>Multiple Kernel</phrase> <phrase>Support Vector</phrase> <phrase>Regression</phrase> (MKLSVR) using linear combinations of kernels and <phrase>Single</phrase> Kernel <phrase>Support Vector</phrase> <phrase>Regression</phrase> (SKSVR). Prices of option highly depend on different <phrase>money market</phrase> conditions like deep-in-the-<phrase>money</phrase>, in-the-<phrase>money</phrase>, at-the-<phrase>money</phrase>, out-of-<phrase>money</phrase> and deep-out-of-<phrase>money</phrase> condition. The <phrase>experimental</phrase> study attempts to identify the forecasting errors with the help of mean square error; <phrase>root</phrase> meant square error, and normalized <phrase>root</phrase> meant square error between the market option prices and the calculated option prices by <phrase>model</phrase> for all market conditions. The <phrase>results</phrase> reflect that <phrase>multiple kernel</phrase> <phrase>support vector</phrase> <phrase>regression</phrase> performed fairly well in comparison to <phrase>support vector</phrase> <phrase>regression</phrase> with <phrase>single</phrase> kernel.
Dependable <phrase>Computing</phrase> in the Context of Mobility, Nomadicity, Ubiquity, and Pervasiveness Why do <phrase>software</phrase> projects fail? : reasons and a <phrase>solution</phrase> using a <phrase>Bayesian</phrase> classifier to predict potential <phrase>risk</phrase> (<phrase>PDF</phrase>) p. 4 Sigma : a <phrase>fault-tolerant</phrase> <phrase>mutual exclusion</phrase> <phrase>algorithm</phrase> in dynamic <phrase>distributed systems</phrase> subject to process crashes and <phrase>memory</phrase> losses p. 7 Intersecting sets : a <phrase>basic</phrase> abstraction for asynchronous agreement problems p. 15 Decision optimal early-stopping k-set agreement in synchronous systems prone to send omission failures p. 23 <phrase>Privacy</phrase>-preserving <phrase>Bayesian network</phrase> structure learning on distributed heterogeneous <phrase>data</phrase> p. 31 Simultaneous <phrase>simulation</phrase> of <phrase>alternative</phrase> system configurations <phrase>Shravan</phrase> Gaonkar p. 41 Availability assessment of <phrase>SunOS</phrase>/<phrase>Solaris</phrase> <phrase>Unix</phrase> systems based on Syslogd and wtmpx log files : a <phrase>case study</phrase> p. 49 On-chip <phrase>debugging</phrase>-based fault emulation for robustness evaluation of <phrase>embedded software</phrase> components p. 57 <phrase>Bayesian</phrase> networks modeling for <phrase>software</phrase> inspection effectiveness p. 65 Application-based metrics for strategic placement of detectors p. 75 A hardware approach to concurrent <phrase>error detection</phrase> capability enhancement in <phrase>COTS</phrase> processors p. 83 Optimal <phrase>fault-tolerant</phrase> routing scheme for generalized <phrase>hypercube</phrase> p. 91 <phrase>High</phrase>-<phrase>order</phrase> syndrome testing for <phrase>VLSI</phrase> circuits p. 101 A new BIST <phrase>solution</phrase> for system-on-chip p. 109 A failure-aware <phrase>model</phrase> for estimating and analyzing the efficiency of <phrase>Web services</phrase> compositions p. 114 Code <phrase>design</phrase> and decoding methods for burst error locating codes p. 125 An evaluation of the virtual <phrase>router</phrase> redundancy protocol extension with <phrase>load balancing</phrase> p. 133 Formal development of <phrase>software</phrase> for tolerating transient faults p. 140 On the fully-informed <phrase>communication</phrase>-induced checkpointing protocol p. 151 Optimal choice of checkpointing interval for <phrase>high availability</phrase> p. 159 An improved scheme of index-based checkpointing p. 167 Compression/scan co-<phrase>design</phrase> for reducing <phrase>test</phrase> <phrase>data</phrase> volume, scan-in power dissipation and <phrase>test</phrase> application time p. 175 Partitioned cache shadowing for deep sub-<phrase>micron</phrase> (<phrase>DSM</phrase>) regime p. 183 Proxy <phrase>cryptography</phrase> for secure inter-domain <phrase>information</phrase> exchanges p. 193 <phrase>Anomaly detection</phrase> with <phrase>high</phrase> deviations for system <phrase>security</phrase> p. 200 A multi-faceted approach towards spam-resistible <phrase>mail</phrase> p. 208 A virtual modeling and a fast <phrase>algorithm</phrase> for grid service reliability p. 219 <phrase>Research</phrase> on <phrase>architecture</phrase> and <phrase>design</phrase> principles of <phrase>COTS</phrase> components based generic <phrase>fault-tolerant</phrase> computer p. 227 Bi-objective <phrase>model</phrase> for <phrase>test</phrase>-suite reduction based on modified condition/decision coverage p. 235 A reliable routing <phrase>algorithm</phrase> based on fuzzy applicability of F sets in <phrase>MANET</phrase> p. 245 An efficient approach to tolerating route errors in <phrase>mobile</phrase> <phrase>ad hoc</phrase> networks p. 250 A novel approach to kernel <phrase>construction</phrase> of <phrase>China</phrase> <phrase>Bridge</phrase> CA p. 258
Tempered <phrase>Markov Chain Monte Carlo</phrase> for training of <phrase>Restricted Boltzmann Machines</phrase> Alternating <phrase>Gibbs sampling</phrase> is the most common scheme used for sampling from <phrase>Restricted Boltzmann Machines</phrase> (RBM), a crucial component in deep architec-tures such as <phrase>Deep Belief</phrase> Networks. However, we find that it often does a very poor job of rendering the diversity of modes captured by the trained <phrase>model</phrase>. We suspect that this hinders the advantage that could in principle be brought by training <phrase>algorithms</phrase> relying on <phrase>Gibbs sampling</phrase> for uncovering spurious modes, such as the Persistent Contrastive Divergence <phrase>algorithm</phrase>. To alleviate this problem, we explore the use of tempered <phrase>Markov Chain Monte-Carlo</phrase> for sampling in RBMs. We find both through visualization of samples and measures of likelihood that it helps both sampling and learning.
Bag of Words Meets Bags of <phrase>Popcorn</phrase> ' This problem is selected from one of the Kaggle's competitions [2]. In this problem , we dig a little " deeper " into <phrase>sentiment analysis</phrase>. Word2Vec is a <phrase>deep-learning</phrase> inspired method that focuses on the meaning of words. Word2Vec [3] attempts to understand meaning and <phrase>semantic</phrase> relationships among words. It works in a way that is similar to deep approaches, such as recurrent <phrase>neural nets</phrase> or deep <phrase>neural nets</phrase>, but is computationally more efficient [3].
Systematically Grounding <phrase>Language</phrase> through Vision in a Deep, <phrase>Recurrent Neural Network</phrase> <phrase>Human intelligence</phrase> consists largely of the ability to recognize and exploit structural systematicity in the world, relating our senses simultaneously to each other and to our <phrase>cognitive</phrase> <phrase>state</phrase>. <phrase>Language</phrase> abilities, in particular, require a learned mapping between the <phrase>linguistic</phrase> input and one's internal <phrase>model</phrase> of the <phrase>real world</phrase>. In <phrase>order</phrase> to demonstrate that con-nectionist methods <phrase>excel</phrase> at this task, we teach a deep, recurrent neural networka variant of the <phrase>long</phrase> <phrase>short-term memory</phrase> (LSTM)to ground <phrase>language</phrase> in a micro-world. The network integrates two inputsa visual scene and an auditory sentenceto produce the meaning of the sentence in the context of the scene. Crucially, the network exhibits strong systematicity, recovering appropriate meanings even for novel objects and descriptions. With its ability to exploit systematic structure across modalities, this network fulfills an important prerequisite of <phrase>general</phrase> machine <phrase>intelligence</phrase>.
<phrase>Special Issue</phrase> on <phrase>User Interfaces</phrase> in <phrase>Theorem Proving</phrase>: Preface <phrase>Theorem proving</phrase> is coming of age. While its foundations predate the first <phrase>computers</phrase>, and systems have been built since the 1950s, dramatic improvements of proving power and expressivity have been made over the last fifteen years. And like other parts of <phrase>computing</phrase>, the complex symbolic manipulations of theorem provers have greatly benefited from the rapid increase in <phrase>computing</phrase> power. This means that applications of <phrase>theorem proving</phrase> and related methods have now become large, diverse and mature, ranging from <phrase>real-world</phrase> hardware and <phrase>software</phrase> verification to the formalisation of complex and deep <phrase>mathematical</phrase> proofs. One <phrase>area</phrase> that is still very much in need of improvement is interfaces for theorem provers. There is no broad agreement about what makes a good <phrase>user interface</phrase> in this <phrase>area</phrase>, and little is known about how to <phrase>bridge</phrase> the gap between imprecise <phrase>human</phrase> interaction and the highly stringent demands of fully formal <phrase>mathematics</phrase>. Yet there is <phrase>universal</phrase> recognition that interfaces must improve if <phrase>theorem proving</phrase> is to become more accessible and productive. Many present systems are unbearably complicated to use and can take months to learn because their interfaces are inadequate for beginning users. Experts are held back too: many interface operations which could significantly enhance <phrase>productivity</phrase> are not supported, although they are commonplace in other modern applications. An example operation is searching: instead of carefully structuring <phrase>data</phrase> in <phrase>order</phrase> to later efficiently retrieve them, nowadays the emphasis has shifted towards flexible and speedy searching in a large shared body of <phrase>knowledge</phrase>. Theorem provers ought to provide such search facilities, for theorems, definitions, and proofs. Second, we have the critical question of how proofs are shown to the user: presentations of proof should be readily comprehensible to both <phrase>author</phrase> and subsequent readers. Finally, there is the issue of the interaction itself, taking place between the user and prover to help direct proof <phrase>construction</phrase>. Here, there is much scope for graphically oriented interfaces with intuitive gestures to supplement or even replace textual interaction. This seems especially possible in restricted problem domains.
Integrating Learning and Engagement in <phrase>Narrative</phrase>-<phrase>Centered Learning</phrase> Environments A key promise of <phrase>narrative</phrase>-<phrase>centered learning</phrase> environments is the ability to make learning engaging. However, there is concern that learning and engagement may be at odds in these <phrase>game</phrase>-based <phrase>learning environments</phrase> and traditional learning systems. This view suggests that, on the one hand, students interacting with a <phrase>game-based learning</phrase> environment may be engaged but unlikely to learn, while on the other hand, traditional learning technologies may <phrase>promote deep learning</phrase> but provide limited engagement. This <phrase>paper</phrase> presents findings from a study with <phrase>human</phrase> participants that challenges the view that engagement and learning need be opposed. A study was conducted with 153 <phrase>middle school</phrase> students interacting with a <phrase>narrative</phrase>-<phrase>centered learning</phrase> environment. Rather than finding an oppositional relationship between learning and engagement, the study found a strong positive relationship between <phrase>learning outcomes</phrase> and increased engagement. Furthermore, the relationship between <phrase>learning outcomes</phrase> and engagement held even when controlling for students' <phrase>background knowledge</phrase> and <phrase>game</phrase>-playing experience.
Topographic Preconditioning of Open <phrase>Ocean</phrase> Deep <phrase>Convection</phrase> Topographic Preconditioning of Open <phrase>Ocean</phrase> Deep <phrase>Convection</phrase> Evidence of enhanced oceanic <phrase>convection</phrase> over Maud Rise in the <phrase>Weddell Sea</phrase> indicates that bottom <phrase>topography</phrase> may <phrase>play</phrase> a role in selecting the location and scale of deep convecting oceanic chimneys below <phrase>large scale</phrase> atmospheric negative <phrase>buoyancy</phrase> forcing. Topographic preconditioning of open <phrase>ocean</phrase> deep <phrase>convection</phrase> is studied using an <phrase>ide</phrase>-alized, three-dimensional, primitive-equation <phrase>model</phrase>. A barotropic mean flow impinges on an isolated Gaussian-shaped <phrase>seamount</phrase> in a stratified domain with uniform negative surface <phrase>buoyancy</phrase> forcing. A <phrase>region</phrase> of topographically trapped flow forms over the <phrase>topography</phrase>. When this "Taylor <phrase>cap</phrase>" is tall enough to interact with the surface mixed-layer, the local isolation from mean horizontal <phrase>advection</phrase> forms a conduit into the deep <phrase>water</phrase>. The <phrase>convective</phrase> penetration depth within this local <phrase>region</phrase> is significantly enhanced relative to <phrase>ambient</phrase> levels away from the <phrase>seamount</phrase> and to similar <phrase>runs</phrase> performed without bottom <phrase>topography</phrase>. The parameter dependencies for these preconditioning processes are investigated. With uniform background <phrase>stratification</phrase>, the doming of isopycnals does not <phrase>play</phrase> a <phrase>major</phrase> role in the preconditioning process. However, when a surface intensified <phrase>strat</phrase>-ification is included, domed isopycnals associated with the Taylor <phrase>cap</phrase> circulation can also <phrase>play</phrase> a preconditioning role. In this case, the pycnocline is first ventilated over the <phrase>seamount</phrase>, leading to rapid <phrase>convective</phrase> deepening into the weakly stratified deep <phrase>water</phrase>. An analytical formula for one-dimensional, non-penetrative <phrase>convection</phrase> into an exponential <phrase>stratification</phrase> profile is derived and compares well with <phrase>results</phrase> from the numerical <phrase>model</phrase>. Previous modeling studies have often parameterized the mehanism by which the horizontal scale of <phrase>oceanographic</phrase> chimneys is set through the use of disk-shaped surface forcing functions. Unlike in such experiments, topographically preconditioned chimneys are not prone to breakup by the growth of baroclinic instabilities. Instead, <phrase>convection</phrase> is generally shut down by horizontal fluxes of <phrase>heat</phrase> due to the mean flow across the <phrase>temperature</phrase> gradients of the <phrase>chimney</phrase> walls. The presence of the mean flow, which is neccessary in <phrase>order</phrase> for the topographic preconditioning to work, causes instabilities to be advected downstream faster than they can grow locally. These <phrase>results</phrase> suggest that the role of baroclinic eddies in shutting down <phrase>oceanographic</phrase> <phrase>convection</phrase> is probably misrepresented in studies which parameterize the preconditioning mechanism, particularly if the preconditioning mechanism being parameterized is a topographic one. Acknowledgements I have enjoyed my time as a graduate <phrase>student</phrase> at <phrase>MIT</phrase> and <phrase>WHOI</phrase> immensely. The years of learning which have culminated in this <phrase>thesis</phrase> (the ultimate sciolist credentials) were enjoyable primarily because of the wonderful people 
A Novel Approach for Generating <phrase>Digital</phrase> <phrase>Chirp</phrase> Signals Using <phrase>FPGA</phrase> <phrase>Technology</phrase> for <phrase>Synthetic Aperture Radar</phrase> Applications Dedication To my loving <phrase>family</phrase> <phrase>ii</phrase> Acknowledgments Acknowledgments I would like to express my heartiest gratitude and deep appreciation to my advisor Prof. Dr. Otmar Loffeld for his consistent help, skillful guidance and attention that he devoted during the course of this work. Even with his eventful schedule, he always found time to counsel me on scientific matters and extended his caring support even while away from his office. I am indebted to Prof. Loffeld for all that I had the advantage to learn from him, and his numerous encouragements that contributed to the accomplishment of this work. I ardently extend my warm thanks to Prof. Dr. Dietmar Ehrhardt for accepting to be my second supervisor, and would like to <phrase>express my deep</phrase> appreciations to him for his support. On a special note I would like to thank Dr. Holger Nies and Dr. Stefan Knedlik for their guidance and advice during the course of this <phrase>research</phrase>. I thank them because they were always willing to help me and answer all of my inquiries. In addition, I will not forget to express my sincere gratitude to the SAR group for their support, reassuring communications and <phrase>friendly</phrase> words of encouragement. I must also thank Mr. Alaa Al Bashar for his assistance during this work. I acknowledge and am thankful for the <phrase>research</phrase> <phrase>scholarship</phrase> for doctoral studies at the <phrase>University</phrase> of <phrase>Siegen</phrase> by the International Postgraduate Program (IPP), and am grateful for the support and facilities provided by ZESS (<phrase>Center</phrase> for <phrase>Sensor</phrase> Systems). I would also like to thank Mrs. Niet-Wunram who was always there to provide any help that I needed during my stay at ZESS.
On the <phrase>evolutionary</phrase> <phrase>design</phrase> of heterogeneous Bagging models Bagging is a popular ensemble <phrase>algorithm</phrase> based on the idea of <phrase>data</phrase> resampling. In this <phrase>paper</phrase>, aiming at increasing the incurred levels of ensemble diversity, we present an <phrase>evolutionary</phrase> approach for optimally designing Bagging models composed of heterogeneous components. To assess its potentials, experiments with well-known <phrase>learning algorithms</phrase> and classification datasets are discussed whereby the accuracy, generalization and diversity levels achieved with heterogeneous Bagging are matched against those delivered by standard Bagging with homogeneous components. Over the last decades, the strategy of combining multiple classifiers into ensembles has received increasing interest due to its potential in bringing about <phrase>significant improvements</phrase> in terms of training accuracy and learning generalization [1,2]. As the key for the success of any ensemble lies in how its components disagree on their predictions [3], several approaches for designing diverse components have been conceived, among which those using different subsets of <phrase>training data</phrase> jointly with a <phrase>single</phrase> learning method [4,5] and those adopting different learning methods associated with different predictors [6,7]. A well-known representative of the first group is Bagging, which is based on the idea of <phrase>data</phrase> resampling [4,5,8,9]. Diversity is promoted in Bagging by using bootstrapped replicas of the training dataset, each replica being generated by randomly <phrase>drawing</phrase>, with replacement, a <phrase>subset</phrase> of the <phrase>training data</phrase>. Typically, each new dataset will have the same number of instances of the original dataset; however, since some instances will appear repeatedly while others will not show up, the effective size will be <phrase>lower</phrase> and the datasets will overlap significantly. Each derived dataset is used to <phrase>train</phrase> a classifier, and then, for any <phrase>test</phrase> instance, the outputs of the individual classifiers are aggregated via the simple <phrase>majority vote</phrase> (MV) rule. Usually, unstable classifiers are adopted as base models, since this type of classifier can generate sufficiently different decision boundaries even for small perturbations in the training parameters [2,4]. In this <phrase>paper</phrase>, aiming at further increasing the diversity levels of the ensemble models <phrase>produced</phrase> by Bagging, we present an <phrase>evolutionary</phrase> approach for optimally designing Bagging models composed of heterogeneous components. Even though the idea of heterogeneous ensembles has been recently advocated [6,7], so far there is no deep investigation on the benefits of adopting different <phrase>learning algorithms</phrase> in the context of Bagging. In fact, this idea seems very reasonable since different classes of <phrase>learning algorithms</phrase> are usually associated with different search/represen-tation biases (and thus <phrase>hypothesis</phrase> spaces) [10], thereby foment-ing ensemble 
Gaussian Processes for Underdetermined Source Separation <phrase>Gaussian process</phrase> (GP) models are very popular for <phrase>machine learning</phrase> and <phrase>regression</phrase> and they are widely used to account for spatial or temporal relationships between multi-variate <phrase>random variables</phrase>. In this <phrase>paper</phrase>, we propose a <phrase>general</phrase> formulation of underdetermined source separation as a problem involving GP <phrase>regression</phrase>. The advantage of the proposed unified view is firstly to describe the different underdetermined source separation problems as particular cases of a more <phrase>general</phrase> framework. Secondly, it provides a flexible means to include a <phrase>variety</phrase> of prior <phrase>information</phrase> concerning the sources such as <phrase>smoothness</phrase>, local stationarity or periodicity through the use of adequate <phrase>covariance</phrase> functions. Thirdly, given the <phrase>model</phrase>, it provides an optimal <phrase>solution</phrase> in the minimum mean squared error (MMSE) sense to the source separation problem. In <phrase>order</phrase> to make the GP models tractable for very large signals, we introduce framing as a GP approximation and we show that computations for regularly sampled and locally stationary <phrase>GPs</phrase> can be done very efficiently in the <phrase>frequency domain</phrase>. These findings establish a deep connection between GP and Nonnegative <phrase>Tensor</phrase> Factorizations with the Itakura-Saito distance and <phrase>lead</phrase> to effective methods to learn GP hyperparameters for very large and regularly sampled signals.
Individual Differences, <phrase>Hypermedia</phrase> <phrase>Navigation</phrase>, and Learning: an <phrase>Empirical Study</phrase> The learning behaviour and performance of 65 postgraduate students using a <phrase>hypermedia</phrase>-based tutorial were measured. <phrase>Data</phrase> were also obtained on <phrase>cognitive</phrase> style, levels of prior experience, <phrase>motivation</phrase>, age, and <phrase>gender</phrase>. A number of statistically significant interactions were found. Field-dependent/<phrase>independent</phrase> <phrase>cognitive</phrase> styles were linked to strategic differences in <phrase>navigation</phrase>. Levels of prior experience were linked to quantitative differences in both <phrase>navigation</phrase> behaviour and learning performance. The implications of these findings are discussed. The rapid rise in the use of the <phrase>World Wide Web</phrase> (WWW or Web) in <phrase>teaching and learning</phrase> has brought <phrase>hypermedia</phrase> into prominence as a mode of <phrase>information</phrase> accessing. The term " <phrase>hypermedia</phrase> " signifies both mode and <phrase>media</phrase> of <phrase>information</phrase> presentation. <phrase>Hypermedia</phrase> may be distinguished from hy-pertext insofar as the former may include <phrase>sound</phrase> and/or moving images in addition to text. However, the <phrase>research</phrase> reported here focuses on the hyper element in that it is concerned not with particular <phrase>media</phrase>, but rather with users' <phrase>navigation</phrase> of the particular type of <phrase>information</phrase> structuring afforded by hyperme-dia. " <phrase>Hypermedia</phrase> " is used throughout this article as the more <phrase>general</phrase> term indicating <phrase>information</phrase> systems offering such structuring, regardless of whether they include <phrase>media</phrase> other than text. <phrase>Hypermedia</phrase> can facilitate relatively nonsequential access patterns as well as the relatively sequential patterns that are characteristic of print <phrase>media</phrase> , allowing a flexible <phrase>range</phrase> of <phrase>design</phrase> options to producers of learning materials. Increasingly, in <phrase>learning environments</phrase> in which students are expected to acquire <phrase>information</phrase> through <phrase>hypermedia</phrase>, students' ability to structure and manage their own <phrase>navigation</phrase> is becoming a required skill. The open and <phrase>free</phrase> browsing <phrase>nature</phrase> of <phrase>hypermedia</phrase>, whilst giving students the freedom to follow nonsequential, and potentially idiosyncratic, paths through a given body of subject content, at the same time places a premium on their ability to effectively exploit this freedom. This <phrase>article presents</phrase> the <phrase>results</phrase> of a <phrase>research</phrase> project that sought to explore the effects of individual differences on learners' <phrase>navigation</phrase> patterns and resultant <phrase>learning outcomes</phrase>. A number of researchers have theorised that <phrase>hypermedia</phrase> offers potential benefits to learning. The types of <phrase>knowledge representation</phrase> <phrase>hypermedia</phrase> affords are arguably closer than <phrase>text-based</phrase> representations to <phrase>human</phrase> <phrase>asso</phrase>-ciative and schema-based <phrase>memory</phrase> structures (Jonassen, 1988, 1992; Mar-chionini, 1988). It has been argued that in supporting such <phrase>knowledge</phrase> structures , <phrase>hypermedia</phrase> is well suited to facilitating learning processes, as proposed particularly by constructivist models suggest that the transformation and <phrase>reconstruction</phrase> of <phrase>information</phrase> characterising <phrase>deep learning</phrase> is facilitated 
Learning <phrase>Typographic</phrase> Style <phrase>Typography</phrase> is a ubiquitous <phrase>art</phrase> form that affects our understanding , <phrase>perception</phrase>, and trust in what we read. Thousands of different <phrase>font</phrase>-faces have been created with enormous variations in the characters. In this <phrase>paper</phrase>, we learn the style of a <phrase>font</phrase> by analyzing a small <phrase>subset</phrase> of only four letters. From these four letters, we learn two tasks. The first is a <phrase>discrimination</phrase> task: given the four letters and a new candidate letter, does the new letter belong to the same <phrase>font</phrase>? Second, given the four basis letters, can we generate all of the other letters with the same characteristics as those in the basis set? We use <phrase>deep neural networks</phrase> to address both tasks, quantitatively and qualitatively measure the <phrase>results</phrase> in a <phrase>variety</phrase> of novel manners, and present a thorough investigation of the weaknesses and strengths of the approach.
Supporting user <phrase>extensibility</phrase> in a networked virtual . . . C-VISions is a networked multiuser 3D <phrase>virtual environment</phrase> that allows users to interact with each other and learn experientially and collaboratively in <phrase>simulation</phrase> and articulation-oriented <phrase>virtual worlds</phrase>. The capabilities of <phrase>virtual worlds</phrase> can be more fully exploited if the freedom to create new worlds is given to the users. This <phrase>paper</phrase> describes the development of a <phrase>graphical user interface</phrase> (<phrase>GUI</phrase>) that allows users to do this. In the context of our <phrase>research</phrase>, the intended users are mainly <phrase>secondary school</phrase> students currently in the process of acquiring new <phrase>scientific knowledge</phrase>. In a <phrase>virtual world</phrase> scenario where the <phrase>virtual environment</phrase> mimics the <phrase>real world</phrase> environment in terms of <phrase>physics</phrase> phenomena, users are given a <phrase>free</phrase> hand in constructing <phrase>science</phrase> experiments in whatever ways they wish. Such a <phrase>learning environment</phrase> allows users to freely construct and <phrase>test</phrase> their personal scientific hypotheses. By relying on <phrase>graphical</phrase> tools that aid the visualization of experiment <phrase>results</phrase>, the learning process is further facilitated and the <phrase>learning environment</phrase> can help students overcome misconceptions and enhance <phrase>deep understanding</phrase> of scientific principles.
<phrase>Pap smear</phrase> <phrase>image classification</phrase> using <phrase>convolutional neural network</phrase> This <phrase>article presents</phrase> the result of a <phrase>comprehensive</phrase> study on <phrase>deep learning</phrase> based <phrase>Computer Aided</phrase> Diagnostic techniques for classification of <phrase>cervical</phrase> <phrase>dysplasia</phrase> using <phrase>Pap smear</phrase> images. All the experiments are performed on a real <phrase>indigenous</phrase> image <phrase>database</phrase> containing 1611 images, generated at two diagnostic centres. Focus is given on constructing an effective feature <phrase>vector</phrase> which can perform multiple level of representation of the features hidden in a <phrase>Pap smear</phrase> image. For this purpose <phrase>Deep Convolutional</phrase> <phrase>Neural Network</phrase> is used, followed by <phrase>feature selection</phrase> using an unsupervised technique with Maximal <phrase>Information</phrase> Compression Index as similarity measure. Finally performance of two classifiers namely Least Square <phrase>Support Vector Machine</phrase> (LSSVM) and Softmax <phrase>Regression</phrase> are monitored and classifier selection is performed based on five measures along with five fold <phrase>cross validation</phrase> technique. Output classes reflects the established <phrase>Bethesda</phrase> system of classification for identifying pre-<phrase>cancerous</phrase> and <phrase>cancerous</phrase> <phrase>lesion</phrase> of <phrase>cervix</phrase>. The proposed system is also compared with two existing conventional systems and also tested on a publicly available <phrase>database</phrase>. <phrase>Experimental</phrase> <phrase>results</phrase> and comparison shows that proposed system performs efficiently in <phrase>Pap smear</phrase> classification.
<phrase>Curriculum</phrase> learning Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful <phrase>order</phrase> which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of <phrase>machine learning</phrase>, and call them "<phrase>curriculum</phrase> learning". In the context of recent <phrase>research</phrase> studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and <phrase>stochastic</phrase> <phrase>neural networks</phrase>), we explore <phrase>curriculum</phrase> learning in various set-ups. The experiments show that <phrase>significant improvements</phrase> in generalization can be achieved. We hypothesize that <phrase>curriculum</phrase> learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the <phrase>local minima</phrase> obtained: <phrase>curriculum</phrase> learning can be seen as a particular form of continuation method (a <phrase>general</phrase> strategy for <phrase>global optimization</phrase> of non-convex functions).
Unconventional <phrase>Monetary Policy</phrase>: Lessons from the Past Three Years Researchers have made great strides in improving our understanding of the effects of unconventional <phrase>monetary policy</phrase>. Although further study is needed, the evidence from the past few years demonstrates that both <phrase>forward</phrase> guidance and <phrase>large-scale</phrase> asset purchases are useful policy tools when <phrase>short</phrase>-term interest rates are constrained by the zero bound. The following is adapted from a presentation made by the <phrase>president</phrase> and <phrase>CEO</phrase> of the <phrase>Federal Reserve Bank</phrase> of Thank you for the kind introduction and for <phrase>giving me the opportunity</phrase> to address this very distinguished group in the beautiful <phrase>city</phrase> of <phrase>Zurich</phrase>. The <phrase>Swiss</phrase> National Bank's annual <phrase>research</phrase> conference has established itself as one of the world's most substantive forums for discussion of <phrase>monetary policy</phrase> issues. The theme selected for this year's conference, " Policy Challenges and Developments in <phrase>Monetary Economics</phrase>, " is particularly timely and relevant. In the past three years, there has certainly been no shortage of policy challenges and developments in the field of <phrase>monetary economics</phrase>. <phrase>Chairman</phrase> Bernanke (2009) said, " Extraordinary times call for <phrase>extraordinary measures</phrase>. " Well, <phrase>extraordinary measures</phrase> have been taken. In the face of severe dislocations in financial markets and deep declines in economic activity, several <phrase>central banks</phrase> have lowered <phrase>short</phrase>-term policy rates essentially to their zero <phrase>lower</phrase> bound. A number of central banksincluding the <phrase>Federal</phrase> Reservehave also used nonstandard or " unconventional " monetary policies. By that I mean efforts to influence interest rates and economic activity using tools other than the <phrase>short</phrase>-term policy rate. Before the <phrase>financial crisis</phrase>, most everything we knew about unconventional <phrase>monetary policy</phrase> came from studies of Japan's <phrase>Lost</phrase> Decade and a few scattered episodes in the <phrase>United States</phrase>. Now, as a result of the events of the past three years, we have numerous examples of unconventional <phrase>monetary policy</phrase> to study. Tonight I'd like to review some of the lessons gleaned from these experiences. I also want to highlight some of the key remaining questions regarding the implementation of such policies and their effectiveness as monetary stimulus. In my remarks, I'll focus on two of these unconventional <phrase>monetary policy</phrase> toolsforward policy guidance and <phrase>large-scale</phrase> asset purchases, or LSAPs in Fedspeak. There are two reasons for this focus. First, these are the policies that the <phrase>Federal Reserve</phrase> and other <phrase>central banks</phrase> have relied on most heavily over the past three years. As a result, they are also the policies that we've learned the most about. And second, 
Visual <phrase>Knowledge</phrase> Discovery Using <phrase>Deep Learning</phrase> Most recent efforts in <phrase>computer vision</phrase> <phrase>harness</phrase> the ever increasing availability of <phrase>information</phrase> on the <phrase>Internet</phrase>. In this work, we are interested in developing a framework that automatically mines images from the <phrase>Internet</phrase> and develop " <phrase>knowledge</phrase> " from those images. We adopt CNNs to discover the <phrase>high</phrase> level concepts in images, and use <phrase>introspection</phrase> with <phrase>NLP</phrase> techniques to describe those concepts in words. Finally, we show that the system can successfully discover some interesting relationships between concepts.
Comparison and Combination of Multilayer Perceptrons and <phrase>Deep Belief</phrase> Networks in <phrase>Hybrid</phrase> <phrase>Automatic Speech Recognition</phrase> Systems To improve the <phrase>speech recognition</phrase> performance, many ways to augment or combine HMMs (<phrase>Hidden Markov Models</phrase>) with other models to build <phrase>hybrid</phrase> architectures have been proposed. The <phrase>hybrid</phrase> HMM/ANN (<phrase>Hidden Markov Model</phrase> / <phrase>Artificial Neural Network</phrase>) <phrase>architecture</phrase> is one of the most successful approaches. In this <phrase>hybrid</phrase> <phrase>model</phrase>, ANNs (which are often <phrase>multilayer perceptron</phrase> <phrase>neural networks</phrase>-MLPs) are used as an HMM-<phrase>state</phrase> posterior <phrase>estimator</phrase>. Recently, <phrase>Deep Belief</phrase> Networks (DBNs) were introduced as a newly powerful <phrase>machine learning</phrase> technique. Generally, DBNs are MLPs with many <phrase>hidden layers</phrase>, however, while weights of MLPs are often initialized randomly, DBNs use a greedy <phrase>layer-by-layer</phrase> <phrase>pre-training</phrase> <phrase>algorithm</phrase> to initialize the network weights. This <phrase>pre-training</phrase> initialization step has resulted in successful realizations of DBNs for various applications such as <phrase>handwriting recognition</phrase>, 3-D <phrase>object recognition</phrase>, <phrase>dimensionality reduction</phrase> and <phrase>automatic speech recognition</phrase> (ASR) tasks. To evaluate the effectiveness of the pre-initialization steps that characterize DBNs from MLPs for ASR tasks, we conduct a comparative evaluation between the two systems on <phrase>phone recognition</phrase> for the TIMIT <phrase>database</phrase>. The effectiveness, advantages and computational cost of each method will be investigated and analyzed. We also show that the <phrase>information</phrase> generated by DBNs and MLPs are complementary, where a consistent improvement is observed when the two systems are combined. In addition, we investigate the ability of the <phrase>hybrid</phrase> HMM/DBN system in the case only a limited amount of <phrase>labeled training</phrase> <phrase>data</phrase> is available.
How Does <phrase>Prior Knowledge</phrase> Impact Students' <phrase>Online Learning</phrase> Behaviors? How Does <phrase>Prior Knowledge</phrase> Impact Students' <phrase>Online Learning</phrase> Behaviors? This study explored the impact of prior <phrase>domain knowledge</phrase> on students' strategies and use of <phrase>digital</phrase> resources during a <phrase>Web-based</phrase> learning task. <phrase>Domain knowledge</phrase> was measured using pre-and posttests of factual <phrase>knowledge</phrase> and <phrase>knowledge</phrase> application. Students utilized an age-and topic-relevant collection of 796 Web resources drawn from an existing educational <phrase>digital library</phrase> to revise essays that they had written prior to the <phrase>online learning</phrase> task. Following <phrase>essay</phrase> revision, participants self-reported their strategies for improving their essays. Screen-capture <phrase>software</phrase> was used to record all <phrase>student</phrase> interactions with <phrase>Web-based</phrase> resources and all modifications to their essays. Analyses examined the relationship between different levels of students' <phrase>prior knowledge</phrase> and <phrase>online learning</phrase> behaviors, self-reported strategies, and <phrase>learning outcomes</phrase>. Findings demonstrated that higher levels of factual <phrase>prior knowledge</phrase> were associated with deeper learning and stronger use of <phrase>digital</phrase> resources, but that higher levels of deep <phrase>prior knowledge</phrase> were associated with less frequent use of online content and fewer deep revisions. These <phrase>results</phrase> suggest that factual <phrase>knowledge</phrase> can serve as a useful <phrase>knowledge base</phrase> during self-<phrase>directed</phrase>, <phrase>online learning</phrase> tasks, but deeper <phrase>prior knowledge</phrase> may <phrase>lead</phrase> novice learners to adopt suboptimal processes and behaviors.
FloTree: a <phrase>multi-touch</phrase> interactive <phrase>simulation</phrase> of <phrase>evolutionary</phrase> processes We present FloTree, a <phrase>multi-user</phrase> <phrase>simulation</phrase> that illustrates key dynamic processes underlying <phrase>evolutionary</phrase> change. Our intention is to create a <phrase>informal learning</phrase> environment that links micro-level <phrase>evolutionary</phrase> processes to macro-level outcomes of <phrase>speciation</phrase> and <phrase>biodiversity</phrase>. On a <phrase>multi-touch</phrase> table, the <phrase>simulation</phrase> represents change from generation to generation in a <phrase>population</phrase> of organisms. By placing hands or <phrase>arms</phrase> on the surface, visitors can add environmental barriers, thus interrupting the <phrase>genetic</phrase> flow between the separated populations. This <phrase>results</phrase> in sub-populations that accumulate <phrase>genetic</phrase> differences independently over time, sometimes leading to the formation of new <phrase>species</phrase>. Learners can morph the result of the <phrase>simulation</phrase> into a corresponding <phrase>phylogenetic tree</phrase>. The <phrase>free</phrase>-form hand and body touch gestures invite creative input from users, encourages social interaction, and provides an opportunity for deep engagement.
Neural Module Networks Visual <phrase>question answering</phrase> is fundamentally composi-tional in naturea question like where is the <phrase>dog</phrase>? shares substructure with questions like what color is the <phrase>dog</phrase>? and where is the <phrase>cat</phrase>? This <phrase>paper</phrase> seeks to simultaneously exploit the representational capacity of deep networks and the com-positional <phrase>linguistic</phrase> structure of questions. We describe a procedure for constructing and learning neural module networks , which <phrase>compose</phrase> collections of jointly-trained neural " modules " into deep networks for <phrase>question answering</phrase>. Our approach decomposes questions into their <phrase>linguistic</phrase> sub-structures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing <phrase>dogs</phrase>, classifying colors, etc.). The resulting <phrase>compound</phrase> networks are jointly trained. We evaluate our approach on two challenging datasets for visual <phrase>question answering</phrase> , achieving <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.
Learning to Align from Scratch Unsupervised joint alignment of images has been demonstrated to <phrase>improve performance</phrase> on <phrase>recognition tasks</phrase> such as face verification. Such alignment reduces undesired variability due to factors such as pose, while only requiring weak supervision in the form of poorly aligned examples. However, prior work on unsu-pervised alignment of complex, <phrase>real-world</phrase> images has required the careful selection of <phrase>feature representation</phrase> based on <phrase>hand-crafted</phrase> image descriptors, in <phrase>order</phrase> to achieve an appropriate, smooth optimization <phrase>landscape</phrase>. In this <phrase>paper</phrase>, we instead propose a novel combination of unsupervised joint alignment with <phrase>unsupervised feature learning</phrase>. Specifically, we incorporate <phrase>deep learning</phrase> into the congealing alignment framework. Through <phrase>deep learning</phrase>, we obtain features that can represent the image at differing resolutions based on network depth, and that are tuned to the <phrase>statistics</phrase> of the specific <phrase>data</phrase> being aligned. In addition, we modify the <phrase>learning algorithm</phrase> for the <phrase>restricted Boltzmann machine</phrase> by incorporating a group sparsity penalty, leading to a topographic <phrase>organization</phrase> of the learned filters and improving subsequent alignment <phrase>results</phrase>. We apply our method to the Labeled Faces in the Wild <phrase>database</phrase> (LFW). Using the aligned images <phrase>produced</phrase> by our proposed unsupervised <phrase>algorithm</phrase>, we achieve higher accuracy in face verification compared to prior work in both unsupervised and supervised alignment. We also match the accuracy for the best available commercial method.
<phrase>Modeling Language</phrase> Vagueness in <phrase>Privacy</phrase> Policies Using <phrase>Deep Neural Networks</phrase> <phrase>Website</phrase> <phrase>privacy</phrase> policies are too <phrase>long</phrase> to read and difficult to understand. The over-sophisticated <phrase>language</phrase> undermines the effectiveness of <phrase>privacy</phrase> notices. People become less willing to share their personal <phrase>information</phrase> when they perceive the <phrase>privacy policy</phrase> as vague. The goal of this <phrase>paper</phrase> is to decode vagueness from a <phrase>natural language processing</phrase> perspective. While thoroughly identifying the vague terms and their <phrase>linguistic</phrase> scope remains an elusive challenge, in this work we seek to learn <phrase>vector</phrase> representations of words in <phrase>privacy</phrase> policies using <phrase>deep neural networks</phrase>. The <phrase>vector</phrase> representations are fed to an interactive visualization tool (LSTMVis) to <phrase>test</phrase> on their ability to discover <phrase>syntactically</phrase> and semantically related terms. The approach holds promise for modeling and understanding <phrase>language</phrase> vagueness.
<phrase>Active Learning</phrase> of Halfspaces 5 This <phrase>thesis</phrase> is dedicated to the <phrase>memory</phrase> of my father. Acknowledgments I would like to thank my advisor Dr. Shai Shalev-Shwartz. Shai has guided me with his deep insights and has had a significant influence on my <phrase>research</phrase>. I have been privileged to have his support and inspiration. Also, I would like to thank the <phrase>machine learning</phrase> HUJI lab for sharing valuable ideas. I would specifically like to thank <phrase>Sivan</phrase> Sabato for her endless <phrase>patience</phrase> and insightful advices. <phrase>Sivan</phrase> has had a significant contribution to this <phrase>thesis</phrase>. Last but not least, I would like to thank my <phrase>family</phrase> for their endless <phrase>love</phrase> and encouragement. Abstract We study pool-based <phrase>active learning</phrase> of half-spaces. In this setting a learner receives a pool of unlabeled examples, and can iteratively query a <phrase>teacher</phrase> for the <phrase>labels</phrase> of examples from the pool. The goal of the learner is to return a low-error prediction rule for the <phrase>labels</phrase> of the examples, using a small number of queries. Most <phrase>active learning</phrase> approaches can be loosely described as more 'aggressive' or more 'mellow'. In recent years a significant advancement has been made for <phrase>active learning</phrase> in the PAC <phrase>model</phrase> using the mellow approach. It has been shown that when the <phrase>data</phrase> is realizable under the assumed <phrase>hypothesis</phrase> class, the mellow approach can guarantee an exponential improvement in <phrase>label</phrase> complexity in comparison with passive learning. An advantage of the mellow approach is its ability to obtain <phrase>label</phrase> complexity improvements in the <phrase>agnostic</phrase> setting. Nonetheless, in the realizable case the mellow approach is not always optimal. In this work we revisit the aggressive approach for the realizable case, and in particular for <phrase>active learning</phrase> of half-spaces in <phrase>Euclidean</phrase> spaces. We show that the aggressive approach can be made efficient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can sometimes be preferable to mellow approaches. We construct an efficient aggressive active learner of half-spaces in <phrase>Euclidean</phrase> spaces, with formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple <phrase>heuristic</phrase> allows using the same <phrase>algorithm</phrase> successfully for pools with low error as well. Our <phrase>algorithm</phrase> for halfspaces is based on a greedy query selection approach as proposed in Tong and Koller (2002); Dasgupta (2005). We obtain improved <phrase>target</phrase>-dependent approximation guarantees for greedy selection 
Learning deep kernels in the space of <phrase>dot product</phrase> <phrase>polynomials</phrase>
Deeply Aggregated Alternating Minimization for Image <phrase>Restoration</phrase> Regularization-based image <phrase>restoration</phrase> has remained an active <phrase>research</phrase> topic in <phrase>computer vision</phrase> and <phrase>image processing</phrase>. It often leverages a guidance signal captured in different fields as an additional cue. In this work, we present a <phrase>general</phrase> framework for image <phrase>restoration</phrase>, called deeply ag-gregated alternating minimization (DeepAM). We propose to <phrase>train</phrase> <phrase>deep neural network</phrase> to advance two of the steps in the conventional AM <phrase>algorithm</phrase>: proximal mapping and -continuation. Both steps are learned from a large dataset in an <phrase>end-to-end</phrase> manner. The <phrase>proposed framework</phrase> enables the <phrase>convolutional neural networks</phrase> (CNNs) to operate as a prior or regularizer in the AM <phrase>algorithm</phrase>. We show that our learned regularizer via deep aggregation outperforms the recent <phrase>data</phrase>-driven approaches as well as the nonlocal-based methods. The flexibility and effectiveness of our framework are demonstrated in several image <phrase>restoration</phrase> tasks, including <phrase>single</phrase> image denoising, RGB-NIR <phrase>restoration</phrase> , and depth super-resolution.
Performance Analysis of <phrase>Information Retrieval</phrase> Systems It has been shown that there is not a best <phrase>information retrieval</phrase> system configuration which would work for any query, but rather that performance can vary from one query to another. It would be interesting if a meta-system could decide which system should process a new query by learning from the context of previously submitted queries. This <phrase>paper</phrase> reports a deep analysis considering more than 80,000 <phrase>search engine</phrase> configurations applied to 100 queries and the corresponding performance. The goal of the analysis is to identify which <phrase>search engine</phrase> configuration responds best to a certain type of query. We considered two approaches to define query types: one is based on query clustering according to the query performance (their difficulty), while the other approach uses various query features (including query difficulty predictors) to cluster queries. We identified two parameters that should be optimized first. An important outcome is that we could not obtain strong conclusive <phrase>results</phrase>; considering the large number of systems and methods we used, this result could <phrase>lead</phrase> to the conclusion that current query features does not fit the optimizing problem.
Learning a <phrase>Generative Model</phrase> of Images by Factoring Appearance and Shape <phrase>Computer vision</phrase> has grown tremendously in the past two decades. Despite all efforts, existing attempts at matching parts of the <phrase>human</phrase> visual system's extraordinary ability to understand visual scenes lack either scope or power. By combining the advantages of <phrase>general</phrase> <phrase>low-level</phrase> <phrase>generative models</phrase> and powerful layer-based and hierarchical models, this work aims at being a first step toward richer, more flexible models of images. After comparing various types of <phrase>restricted Boltzmann machines</phrase> (RBMs) able to <phrase>model</phrase> continuous-valued <phrase>data</phrase>, we introduce our <phrase>basic</phrase> <phrase>model</phrase>, the masked RBM, which explicitly models occlusion boundaries in <phrase>image patches</phrase> by factoring the appearance of any patch <phrase>region</phrase> from its shape. We then propose a <phrase>generative model</phrase> of larger images using a field of such RBMs. Finally, we discuss how masked RBMs could be stacked to form a deep <phrase>model</phrase> able to generate more complicated structures and suitable for various tasks such as segmentation or <phrase>object recognition</phrase>.
Object Classification and Detection in <phrase>High</phrase> Dimensional <phrase>Feature Space</phrase> THIS IS A TEMPORARY TITLE PAGE It will be replaced for the final print by a version provided by the service acadmique. Acknowledgements First and foremost, I would like to thank my <phrase>thesis</phrase> advisor, Dr. Franois Fleuret, for <phrase>giving me the opportunity</phrase> to carry out this very exciting project. Franois has all the qualities a <phrase>PhD</phrase> <phrase>student</phrase> can dream of: he is always there when you need him, ready to discuss new ideas, he has a deep <phrase>knowledge</phrase> not only of the field but also of CS and <phrase>math</phrase> in <phrase>general</phrase>, and he never urges you but instead gives you all the freedom you might need to carry out your work to fruition. He also taught me what it means to be a real <phrase>scientist</phrase>, in addition of an <phrase>engineer</phrase>, which I always wanted to be. I would like also to thank my three <phrase>jury</phrase> members, Prof. as well as my <phrase>jury</phrase> <phrase>president</phrase>, Prof. Mark Pauly, for doing me the honor to supervise my oral exam. Many thanks to Dr. Raghuraman Krishnamoorthi, Dr. Bojan Vrcelj, and all their colleagues for <phrase>giving me the opportunity</phrase> to come to the <phrase>U.S</phrase>. and mentoring me during my <phrase>internship</phrase> at <phrase>Qualcomm</phrase> <phrase>San Diego</phrase>. Experiencing <phrase>life</phrase> in <phrase>California</phrase> and working in a large IT <phrase>company</phrase> was very interesting and something I always wanted to try. Working at Idiap would not have been as fun without all my office mates and colleagues: and probably a few others which I forgot, sorry! I will not soon forget all the <phrase>long</phrase> baby foot <phrase>games</phrase> we played, all the Friday afternoon beers, and all our (sometimes a <phrase>bit</phrase> pointless, Boosting vs. <phrase>SVM</phrase> anyone?) discussions. I would finally like to thank my parents, in-laws, brothers and sisters, and all the rest of my <phrase>family</phrase> for their constant support. Special thanks to my wife Wenqi, always there to share with me the highs and lows of <phrase>PhD</phrase> <phrase>life</phrase> and without who none of it would have been possible. Abstract Object classification and detection aim at recognizing and localizing objects in <phrase>real-world</phrase> images. They are fundamental <phrase>computer vision</phrase> problems and a prerequisite for full scene understanding. Their difficulty lies in the large number of possible object positions and the appearance variations of object classes. This <phrase>thesis</phrase> improves upon several <phrase>classical</phrase> <phrase>machine learning</phrase> <phrase>algorithms</phrase>, enabling large computational gains in <phrase>high</phrase> dimensional <phrase>feature space</phrase>. A common trend in <phrase>machine learning</phrase> and <phrase>computer vision</phrase> <phrase>research</phrase> 
A unified <phrase>architecture</phrase> for <phrase>natural language processing</phrase>: <phrase>deep neural networks</phrase> with multitask learning We describe a <phrase>single</phrase> <phrase>convolutional neural network</phrase> <phrase>architecture</phrase> that, given a sentence, outputs a host of <phrase>language</phrase> processing predictions: part-of-speech tags, chunks, <phrase>named entity</phrase> tags, <phrase>semantic</phrase> roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a <phrase>language</phrase> <phrase>model</phrase>. The entire network is trained <i>jointly</i> on all these tasks using weight-sharing, an instance of <i>multitask learning</i>. All the tasks use <phrase>labeled data</phrase> except the <phrase>language</phrase> <phrase>model</phrase> which is learnt from unlabeled text and represents a novel form of <i><phrase>semi-supervised</phrase> learning</i> for the shared tasks. We show how both <i>multitask learning</i> and <i><phrase>semi-supervised</phrase> learning</i> improve the generalization of the shared tasks, resulting in <phrase>state</phrase>-of-the-<phrase>art</phrase>-performance.
STDP-based spiking <phrase>deep neural networks</phrase> for <phrase>object recognition</phrase> Previous studies have shown that spike-timing-dependent plasticity (STDP) can be used in spik-ing <phrase>neural networks</phrase> (SNN) to extract <phrase>visual features</phrase> of low or intermediate complexity in an un-supervised manner. These studies, however, used relatively shallow architectures, and only one layer was trainable. Another line of <phrase>research</phrase> has demonstrated using rate-based <phrase>neural networks</phrase> trained with back-propagation that having many layers increases the recognition robustness, an approach known as <phrase>deep learning</phrase>. We thus designed a deep SNN, comprising several convolutional (trainable with STDP) and pooling layers. We used a temporal coding scheme where the most strongly activated <phrase>neurons</phrase> fire first, and less activated <phrase>neurons</phrase> fire later or not at all. The network was exposed to <phrase>natural images</phrase>. Thanks to STDP, <phrase>neurons</phrase> progressively <phrase>learned features</phrase> corresponding to prototyp-ical patterns that were both salient and frequent. Only a few tens of examples per category were required and no <phrase>label</phrase> was needed. After learning, the complexity of the extracted features increased along the hierarchy, from edge detectors in the first layer to object <phrase>prototypes</phrase> in the last layer. Coding was very sparse, with only a few thousands spikes per image, and in some cases the object category could be reasonably well inferred from the activity of a <phrase>single</phrase> <phrase>higher-order</phrase> <phrase>neuron</phrase>. More generally , the activity of a few hundreds of such <phrase>neurons</phrase> contained robust category <phrase>information</phrase>, as demonstrated using a classifier on <phrase>Caltech</phrase> 101, <phrase>ETH</phrase>-80, and MNIST <phrase>databases</phrase>. We think that the combination of STDP with latency coding is key to understanding the way that the <phrase>primate</phrase> visual system learns, its remarkable processing speed and its low <phrase>energy</phrase> consumption. These mechanisms are also interesting for <phrase>artificial</phrase> vision systems, particularly for hardware solutions.
<phrase>Deep Learning</phrase> Regularized Fisher Mappings For <phrase>classification tasks</phrase>, it is always desirable to extract features that are most effective for preserving class separability. In this brief, we propose a new <phrase>feature extraction</phrase> method called regularized deep Fisher mapping (RDFM), which learns an explicit mapping from the <phrase>sample space</phrase> to the <phrase>feature space</phrase> using a <phrase>deep neural network</phrase> to enhance the separability of features according to the Fisher criterion. Compared to <phrase>kernel methods</phrase>, the <phrase>deep neural network</phrase> is a deep and nonlocal learning <phrase>architecture</phrase>, and therefore exhibits more powerful ability to learn the <phrase>nature</phrase> of highly <phrase>variable</phrase> datasets from fewer samples. To eliminate the side effects of <phrase>overfitting</phrase> brought about by the large capacity of powerful learners, regularizers are applied in the learning procedure of RDFM. RDFM is evaluated in various types of datasets, and the <phrase>results</phrase> reveal that it is necessary to apply unsupervised regularization in the <phrase>fine-tuning</phrase> phase of <phrase>deep learning</phrase>. Thus, for very flexible models, the optimal Fisher feature extractor may be a balance between discriminative ability and descriptive ability.
<phrase>Semiconducting</phrase> <phrase>bilinear</phrase> <phrase>deep learning</phrase> for incomplete <phrase>image recognition</phrase> <phrase>Image recognition</phrase> with incomplete <phrase>data</phrase> is a well-known hard problem in <phrase>multimedia</phrase> <phrase>content analysis</phrase>. This <phrase>paper</phrase> proposes a novel <phrase>deep learning</phrase> technique called <phrase>semiconducting</phrase> <phrase>bilinear</phrase> <phrase>deep belief</phrase> networks (SBDBN) by referencing human's <phrase>visual cortex</phrase> and intelligent <phrase>perception</phrase>. Inheriting from deep models, SBDBN simulates the <phrase>laminar</phrase> structure of human's <phrase>cerebral cortex</phrase> and the neural loop in human's visual areas. To address the special difficulties of <phrase>image recognition</phrase> with incomplete <phrase>data</phrase>, we <phrase>design</phrase> a novel second-<phrase>order</phrase> <phrase>deep architecture</phrase> with <phrase>semiconducting</phrase> <phrase>restricted boltzmann machines</phrase>. Moreover, two <phrase>peaks</phrase> activation of human's <phrase>perception</phrase> is implemented by three learning stages of <phrase>semiconducting</phrase> <phrase>bilinear</phrase> <phrase>discriminant</phrase> initialization, <phrase>greedy layer-wise</phrase> <phrase>reconstruction</phrase>, and global <phrase>fine-tuning</phrase>. Owing to exploiting the embedding <phrase>information</phrase> according to the reliable features rather than any completion of missing features, the proposed SBDBN has demonstrated outstanding recognition ability on two standard datasets and one constructed dataset, comparing with both incomplete <phrase>image recognition</phrase> techniques and existing <phrase>deep learning</phrase> models.
"What is... <phrase>Dengue Fever</phrase>?" - Modeling and Predicting Pronunciation Errors in a Text-to-Speech System We propose a system to predict baseform-generation errors in a text-to-speech (<phrase>TTS</phrase>) front-end, and aid in the process of cus-tomizing the synthesis <phrase>engine</phrase> to a novel application with a large, <phrase>open-ended</phrase> <phrase>vocabulary</phrase>. We motivate the use of the system by using <phrase>data</phrase> collected during the deployment of the <phrase>IBM</phrase> <phrase>TTS</phrase> <phrase>engine</phrase> in the Watson Deep <phrase>Question-Answering</phrase> system customized to <phrase>play</phrase> a <phrase>game</phrase> of <phrase>Jeopardy</phrase>!. We propose a set of features derived from a lexeme's <phrase>orthography</phrase> and candidate baseform, and use a <phrase>variety</phrase> of learning schemes and <phrase>data</phrase> sampling <phrase>algorithms</phrase> to address the issue of skewed class priors in the <phrase>training data</phrase>. We show that 1) these different approaches provide complementary <phrase>information</phrase> that can then be exploited by <phrase>fusion</phrase> schemes to improve on the baseline performances, and 2) it is possible to use these techniques to retrieve a list of likely incorrect lexemes so as to reduce the number of tokens that must be vetted before finding and fixing an error.
Convex <phrase>Deep Learning</phrase> via Normalized Kernels <phrase>Deep learning</phrase> has been a <phrase>long</phrase> standing pursuit in <phrase>machine learning</phrase>, which until recently was hampered by unreliable training methods before the discovery of improved heuristics for embedded layer training. A complementary <phrase>research</phrase> strategy is to develop <phrase>alternative</phrase> modeling architectures that admit efficient training methods while expanding the <phrase>range</phrase> of representable structures toward deep models. In this <phrase>paper</phrase>, we develop a new <phrase>architecture</phrase> for nested nonlinearities that allows arbitrarily deep compositions to be trained to global optimality. The approach admits both parametric and nonparametric forms through the use of normalized kernels to represent each latent layer. The outcome is a fully convex formulation that is able to capture compositions of trainable nonlinear layers to arbitrary depth.
Optimal <phrase>deep brain stimulation</phrase> of the <phrase>subthalamic nucleus</phrase> - a computational study <phrase>Deep brain stimulation</phrase> (DBS) of the <phrase>subthalamic nucleus</phrase>, typically with periodic, <phrase>high</phrase> <phrase>frequency</phrase> pulse trains, has <phrase>proven</phrase> to be an effective treatment for the motor symptoms of <phrase>Parkinson's disease</phrase> (PD). Here, we use a biophysically-based <phrase>model</phrase> of spiking cells in the <phrase>basal ganglia</phrase> (Terman et al., <phrase>Journal</phrase> of <phrase>Neuroscience</phrase>, 22, 2963-2976, 2002; Rubin and Terman, <phrase>Journal</phrase> of <phrase>Computational Neuroscience</phrase>, 16, 211-235, 2004) to provide computational evidence that <phrase>alternative</phrase> temporal patterns of DBS inputs might be equally effective as the standard <phrase>high</phrase>-<phrase>frequency</phrase> <phrase>waveforms</phrase>, but require <phrase>lower</phrase> amplitudes. Within this <phrase>model</phrase>, DBS performance is assessed in two ways. First, we determine the extent to which DBS causes Gpi (<phrase>globus pallidus</phrase> pars interna) <phrase>synaptic</phrase> outputs, which are burstlike and synchronized in the unstimulated Parkinsonian <phrase>state</phrase>, to cease their pathological <phrase>modulation</phrase> of simulated thalamocortical cells. Second, we evaluate how DBS affects the GPi cells' auto- and cross-correlograms. In both cases, a nonlinear closed-loop <phrase>learning algorithm</phrase> identifies effective DBS inputs that are optimized to have minimal strength. The network dynamics that result differ from the regular, entrained firing which some previous studies have associated with conventional <phrase>high</phrase>-<phrase>frequency</phrase> DBS. This type of optimized <phrase>solution</phrase> is also found with heterogeneity in both the intrinsic network dynamics and the strength of DBS inputs received at various cells. Such <phrase>alternative</phrase> DBS inputs could potentially be identified, guided by the <phrase>model</phrase>-<phrase>free</phrase> <phrase>learning algorithm</phrase>, in <phrase>experimental</phrase> or eventual clinical settings.
Using interactive <phrase>multimedia</phrase> for <phrase>teaching and learning</phrase> <phrase>object oriented</phrase> <phrase>software design</phrase> <phrase>Object Oriented</phrase> (OO) <phrase>design</phrase> and <phrase>programming</phrase> is an abstract and complex domain, and students have problems with understanding the concepts and applying them to the <phrase>design</phrase> of <phrase>Software</phrase> systems. At <phrase>Napier University</phrase>, approximately 400 <phrase>undergraduate</phrase> students per year take <phrase>Object Oriented</phrase> <phrase>Software Design</phrase> (OOSD). There is a growing need to find a way to support students' learning. The question was what we could do to support large number o f students with an abstract domain. The <phrase>solution</phrase> we came up with was using Interactive <phrase>Multimedia</phrase> (IMM) for learning and teaching the subject. Key strengths of IMM are <phrase>interactivity</phrase> and visualisation. IMM can help students develop clear understanding of OO concepts such as objects, classes, and <phrase>message passing</phrase> through <phrase>interactivity</phrase> and visualisation. Learning requires active thinking. Although the IMM materials will be initiated from a lecture or a tutorial, they are aimed to be self-<phrase>directed</phrase> learning materials. The materials should be able to encourage students to think actively in <phrase>order</phrase> to <phrase>promote deep learning</phrase>. <phrase>Hyperlinks</phrase> have been used to prompt internal question and reflection. <phrase>Graphical</phrase> representation is used to visualise OO <phrase>design</phrase> process from <phrase>real world</phrase> physical objects to <phrase>software</phrase> system built. <phrase>Research</phrase> into students' learning using these features is needed in <phrase>order</phrase> to explore new <phrase>design</phrase> aspects with IMM to improve learning in <phrase>higher education</phrase>. Two different types of learning materials have been developed to support this <phrase>research</phrase>. One is a resource-oriented material, which is similar to primary courseware [1] and will be initiated by a <phrase>lecturer</phrase> in a lecture. The other is a task-oriented material with embedded <phrase>hyperlinks</phrase> to the resource-oriented ones, and will be used in a tutorial. To investigate the effectiveness of <phrase>hyperlinks</phrase> in promoting <phrase>cognitive</phrase> <phrase>interactivity</phrase>, we <phrase>test</phrase> three types of <phrase>hyperlinks</phrase>, which are no <phrase>hyperlink</phrase>, static <phrase>hyperlink</phrase> presented as default and dynamic <phrase>hyperlink</phrase> appearing with tips when there is a mistake or incorrect answer made. This <phrase>poster</phrase> will describe trials conducted at Bmnel and Napier <phrase>universities</phrase>. The <phrase>results</phrase> and comparison made from the trials in terms of students' attitudes to IMM assisted learning and their performance will be presented along with findings about <phrase>hyperlinks</phrase> and visualisation in learning. Ada is still a popular <phrase>language</phrase> for introductory <phrase>programming</phrase> courses in many <phrase>university</phrase> <phrase>Computer Science</phrase> departments. It is often used as a 'super Pascal' for teaching <phrase>basic</phrase> algorithmic constructs before moving on to 'big picture' <phrase>object-oriented</phrase> languages. It is a good educational <phrase>language</phrase> because of its clear, 
Resolution Is Not Automatizable Unless W[P] Is Tractable We show that neither Resolution nor <phrase>tree</phrase>-like Resolution is automatizable unless the class W[P] from the hierarchy of parameterized problems is fixed-parameter tractable by <phrase>randomized</phrase> <phrase>algorithms</phrase> with one-sided error. 1. Introduction. Analysis of the usefulness of proof search heuristics and <phrase>automated theorem proving</phrase> procedures based on a proof system P amounts (on the theoretical level) to the following two <phrase>basic</phrase> questions: Question 1. Which theorems in principle possess efficient P-proofs? Question 2. How to find the optimal (or, at least, a nearly optimal) proof of a given theorem in P ? Traditional proof complexity mostly dealt, and still deals with the first question. However , there has been a growing interest in the second one, too. An additional <phrase>motivation</phrase> to study the complexity of finding optimal proofs comes from deep connections with efficient <phrase>interpolation</phrase> theorems; we refer the reader to the surveys [9, 19, 22] for more details. These surveys also serve as a good <phrase>starting point</phrase> for learning more about <phrase>propositional</phrase> proof complexity in <phrase>general</phrase>.
Fact-based question decomposition in DeepQA Factoid questions often contain more than one fact or assertion about their answers. <phrase>Question-answering</phrase> (QA) systems, however, typically do not use such <phrase>fine-grained</phrase> distinctions because of the need for <phrase>deep understanding</phrase> of the question in <phrase>order</phrase> to identify and separate the facts. We argue that decomposing complex factoid questions is beneficial to QA systems, because the more facts that support an answer candidate, the more likely it is to be the correct answer. We broadly categorize decomposable questions into two types: parallel and nested. Parallel decomposable questions contain subquestions that can be evaluated <phrase>independent</phrase> of each other. Nested questions require decompositions to be processed in <phrase>sequence</phrase>, with the answer to an Binner[ subquestion plugged into an Bouter[ subquestion. In this <phrase>paper</phrase>, we present a novel question decomposition framework capable of handling both decomposition types, built on top of the base <phrase>IBM</phrase> Watsoni QA system for <phrase>Jeopardy</phrase>!i. The framework contains a suite of decomposition rules that use predominantly lexico-<phrase>syntactic</phrase> features to identify facts within complex questions. It also contains a question-rewriting component and a candidate re-ranker, which uses <phrase>machine learning</phrase> and <phrase>heuristic</phrase> selection strategies to generate a final ranked answer list, taking into account answer confidences from the base QA system. We apply our decomposition framework to the particularly challenging domain of Final <phrase>Jeopardy</phrase>! questions, which are found to be difficult even for qualified <phrase>Jeopardy</phrase>! players, and we show a statistically significant improvement in the performance of our baseline QA system.
A Connection Between Score Matching and <phrase>Denoising Autoencoders</phrase> <phrase>Denoising autoencoders</phrase> have been previously shown to be competitive alternatives to <phrase>restricted Boltzmann machines</phrase> for unsupervised pretraining of each layer of a <phrase>deep architecture</phrase>. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the <phrase>data</phrase>) of a <phrase>specific energy</phrase>-based <phrase>model</phrase> to that of a nonparametric Parzen <phrase>density</phrase> <phrase>estimator</phrase> of the <phrase>data</phrase>. This yields several useful insights. It defines a proper <phrase>probabilistic model</phrase> for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their <phrase>energy</phrase>. It suggests a different way to apply score matching that is related to learning to denoise and does not require <phrase>computing</phrase> second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of <phrase>denoising autoencoders</phrase> to a larger <phrase>family</phrase> of <phrase>energy</phrase>-based models.
Example from a framework for future <phrase>learning environments</phrase>: <phrase>human factors</phrase> and learner engagement in collaborative workspaces with <phrase>tablet</phrase> <phrase>computing</phrase> This <phrase>research</phrase> centers on a <phrase>design</phrase> to elevate learner engagement or immersion in <phrase>mathematics</phrase> classrooms in <phrase>order</phrase> to optimize students' <phrase>mathematical</phrase> growth. The design's theoretical <phrase>logic</phrase> is related to eight principles for future <phrase>learning environments</phrase>, and to an interpretation of engagement as recruitment of attentional, <phrase>perceptual</phrase> and complex reasoning resources. Introduction Previous <phrase>research</phrase> based on self-reports by <phrase>mathematics</phrase> students at the <phrase>high school</phrase> and <phrase>college</phrase> level suggests a <phrase>perception</phrase> by students that upwards of 80% or more of the time that they are in <phrase>mathematics</phrase> classes does not involve substantial <phrase>mathematical</phrase> <phrase>cognition</phrase>. These figures are <phrase>average</phrase> values; for many students, the percentage of time they self-reported as disconnected from <phrase>mathematical</phrase> activity during class time approaches 95% or more. Although disengagement and its causes are multi-factored and complex, <phrase>student</phrase> explanations <phrase>accounting</phrase> for disengagement include confusing instruction, <phrase>general</phrase> boredom or distraction, and a sense that the class topics are too difficult, too easy, or meaningless. The ascendancy of learning <phrase>science</phrase> <phrase>research</phrase> and its visibility in addressing enduring problems in <phrase>education</phrase> is a promising development in the past decade. How learning <phrase>science</phrase> contributes to the problems of traditional classroom environments is crucial to next generation educational environments. One such problem is the <phrase>productivity</phrase> of class time. The <phrase>research</phrase> discussed in this <phrase>poster</phrase> discusses investigations in promoting routine and productive engagement in <phrase>mathematics</phrase> classrooms. More specifically, it focuses on a <phrase>design</phrase> to recruit learners' attentional, motivational and complex reasoning resources in <phrase>order</phrase> to optimize their <phrase>mathematical</phrase> growth during classroom instruction. The <phrase>design</phrase> involves a blend of <phrase>tablet computers</phrase> with collaborative workspaces to promote deep immersion or engagement by learners in classroom <phrase>mathematics</phrase>.
Vision and <phrase>Language</phrase> Integration Meets <phrase>Multimedia</phrase> <phrase>Fusion</phrase>: Proceedings of <phrase>ACM</phrase> <phrase>Multimedia</phrase> 2016 Workshop Multimodal <phrase>information</phrase> <phrase>fusion</phrase> both at the signal and the <phrase>semantics</phrase> levels is a core part in most <phrase>multimedia</phrase> applications, including <phrase>multimedia</phrase> indexing, retrieval, summarization and others. Early or late <phrase>fusion</phrase> of modality-specific processing <phrase>results</phrase> has been addressed in <phrase>multimedia</phrase> <phrase>prototypes</phrase> since their very early days, through various methodologies including <phrase>rule-based</phrase> approaches, <phrase>information</phrase>-theoretic models and <phrase>machine learning</phrase>. Vision and <phrase>Language</phrase> are two of the predominant modalities that are being fused and which have attracted special attention in international challenges with a <phrase>long</phrase> <phrase>history</phrase> of <phrase>results</phrase>, such as TRECVid, ImageClef and others. During the last decade, vision-<phrase>language</phrase> <phrase>semantic</phrase> integration has attracted attention from traditionally non-interdisciplinary <phrase>research</phrase> communities, such as <phrase>Computer Vision</phrase> and <phrase>Natural Language Processing</phrase>. This is due to the fact that one modality can greatly assist the processing of another providing cues for disambiguation, complementary <phrase>information</phrase> and noise/error filtering. The latest boom of <phrase>deep learning</phrase> methods has opened up new directions in joint modelling of visual and co-occurring verbal <phrase>information</phrase> in <phrase>multimedia</phrase> discourse. The workshop on Vision and <phrase>Language</phrase> Integration Meets <phrase>Multimedia</phrase> <phrase>Fusion</phrase> has been held during the workshop weekend of the <phrase>ACM</phrase> <phrase>Multimedia</phrase> 2016 Conference and the <phrase>European</phrase> Conference on <phrase>Computer Vision</phrase> (ECCV 2016) on October 16, 2016 in <phrase>Amsterdam</phrase>, the <phrase>Netherlands</phrase>. The proceedings contain seven selected <phrase>long</phrase> papers, which have been orally presented at the workshop, and three abstracts of the invited keynote speeches. The papers and abstracts discuss <phrase>data</phrase> collection, representation learning, <phrase>deep learning</phrase> approaches, matrix and <phrase>tensor</phrase> factorization methods and <phrase>graph</phrase> based clustering with regard to the <phrase>fusion</phrase> of <phrase>multimedia</phrase> <phrase>data</phrase>. A <phrase>variety</phrase> of applications is presented including image captioning, summarization of <phrase>news</phrase>, <phrase>video</phrase> hyperlinking, sub-<phrase>shot</phrase> segmentation of <phrase>user generated</phrase> <phrase>video</phrase>, cross-modal classification, cross-modal <phrase>question-answering</phrase>, and the detection of misleading <phrase>metadata</phrase> of <phrase>user generated</phrase> <phrase>video</phrase>. The workshop is organized and supported by the <phrase>EU</phrase> COST <phrase>action</phrase> iV&#38;L Net, the <phrase>European</phrase> Network on Integrating Vision and <phrase>Language</phrase>: Combining <phrase>Computer Vision</phrase> and <phrase>Language</phrase> Processing for Advanced Search, Retrieval, Annotation and Description of Visual <phrase>Data</phrase> (IC 1307--2014-2018).
Co-authoring Personalised Educational Content: Teachers' Perspectives In this <phrase>paper</phrase> we investigate how authoring activities of adaptive educational <phrase>hypermedia</phrase> content may cultivate <phrase>e</phrase>-learning content development skills, and promote reflection on learning <phrase>design</phrase> issues. Developing content for adaptive educational <phrase>hypermedia</phrase> systems is a demanding task that engage authors in self-explanatory activities and deep investigation of resources that correspond to learners' multiple profiles and confront to specific learning <phrase>design</phrase> principles implied by the authoring tool. In this <phrase>paper</phrase> we present an <phrase>empirical study</phrase> where teachers worked in groups and used INSPIREAuth to <phrase>author</phrase> content for INSPIRE and review content developed by <phrase>peers</phrase>. The strategies they used, the benefits they recognised and the difficulties they faced in this process, are described. The possible implications for extending the <phrase>design</phrase> of INSPIREAuth are briefly discussed.
Cross-framework parser stacking for <phrase>data</phrase>-driven dependency <phrase>parsing</phrase> In this article, we present and evaluate an approach to the combination of a <phrase>grammar</phrase>-driven and a <phrase>data</phrase>-driven parser which exploits <phrase>machine learning</phrase> for the acquisition of <phrase>syntactic</phrase> analyses guided by both parsers. We show how conversion of LFG output to dependency representation allows for a technique of parser stacking, whereby the output of the <phrase>grammar</phrase>-driven parser supplies features for a <phrase>data</phrase>-driven dependency parser. We evaluate on <phrase>English</phrase> and Ger-man and show <phrase>significant improvements</phrase> in overall parse <phrase>results</phrase> stemming from the proposed dependency structure as well as other <phrase>linguistic</phrase> features derived from the grammars. Finally, we perform an application-oriented evaluation and explore the use of the stacked parsers as the basis for the projection of dependency annotation to a new <phrase>language</phrase>. KEYWORDS: <phrase>data</phrase>-driven dependency <phrase>parsing</phrase>, <phrase>Lexical Functional Grammar</phrase> (LFG), parser combination , stacking, <phrase>deep linguistic</phrase> features MOTS-CLS : analyse syntaxique en dpendances fond sur des donnes, Grammaires Lexicales Fonctionnelles (LFG), combinaison d'analyseurs, caractristiques linguistiques profondes
The distribution of rewards in sensorimotor maps acquired by <phrase>cognitive</phrase> <phrase>robots</phrase> through exploration Keywords: GNOSYS <phrase>robot</phrase> Internal models Sensorimotor maps Value dependent learning Field <phrase>computing</phrase> a b s t r a c t To exhibit intelligent behavior, <phrase>cognitive</phrase> <phrase>robots</phrase> must have some <phrase>knowledge</phrase> about the consequences of their actions and their value in the context of the goal being realized. We present a neural framework using which explorative sensorimotor experiences of <phrase>cognitive</phrase> <phrase>robots</phrase> can be efficiently 'internalized' using growing sensorimotor maps and planning realized using goal induced quasi-stationary value fields. Further, if there are no predefined reward functions (or the case when they are not good enough in a slightly modified world), the <phrase>robot</phrase> will have to try and realize its goal by exploration after which reward/penalty is given at the end. This <phrase>paper</phrase> proposes three simple rules for distribution of the received end reward among the contributing <phrase>neurons</phrase> in a <phrase>high</phrase> dimensional sensorimotor map. Importantly, reward/penalty distribution over hundreds of <phrase>neurons</phrase> in the sensorimotor map is computed one <phrase>shot</phrase>. This resulting reward distribution can be visualized as an additional value field, representing the new learnt experience and can be combined with other such fields in a <phrase>context dependent</phrase> <phrase>fashion</phrase> to plan/<phrase>compose</phrase> novel emergent behavior. The simplicity and efficiency of the approach is illustrated through the resulting behaviors of the GNOSYS <phrase>robot</phrase> in two different scenarios (a) learning 'when' to optimize 'what constraint' while realizing spatial goals and (b) learning to push a ball intelligently to the corners of a table, while avoiding traps randomly placed by the <phrase>teacher</phrase> (this scenario replicates the famous trap tube <phrase>paradigm</phrase> from <phrase>animal</phrase> reasoning carried out on <phrase>chimpanzees</phrase>, <phrase>capuchins</phrase> and infants). Natural/<phrase>artificial</phrase> systems that are capable of utilizing thoughts at the service of their actions are gifted with the profound opportunity to mentally manipulate the causal structure of their physical interactions with the environment. Complex bodies can in this way decouple behavior from direct control of the environment and react to situations that 'do not really exist' but 'could exist' as a result of their actions on the world. However, the computational basis of such <phrase>cognitive processes</phrase> has still remained elusive. This is a difficult problem, but there are many pressures to provide a solutionfrom the intrinsic viewpoint of better understanding ourselves [3] to creating <phrase>artificial</phrase> agents, <phrase>robots</phrase> and smart devices that can deal autonomously with our needs and with the peculiarities of the environments we inhabit and construct. This quest has <phrase>led</phrase> researchers towards several deep 
The Deep <phrase>Tensor</phrase> <phrase>Neural Network</phrase> With Applications to Large <phrase>Vocabulary</phrase> <phrase>Speech Recognition</phrase> The <phrase>recently proposed</phrase> <phrase>context-dependent</phrase> <phrase>deep neural network</phrase> <phrase>hidden Markov models</phrase> (<phrase>CD</phrase>-DNN-HMMs) have been proved highly promising for large <phrase>vocabulary</phrase> <phrase>speech recognition</phrase>. In this <phrase>paper</phrase>, we develop a more advanced type of DNN, which we call the deep <phrase>tensor</phrase> <phrase>neural network</phrase> (DTNN). The DTNN extends the conventional DNN by replacing one or more of its layers with a double-projection (DP) layer, in which each input <phrase>vector</phrase> is projected into two nonlinear subspaces, and a <phrase>tensor</phrase> layer, in which two subspace projections interact with each other and jointly predict the next layer in the <phrase>deep architecture</phrase>. In addition, we describe an approach to map the <phrase>tensor</phrase> layers to the conventional <phrase>sigmoid</phrase> layers so that the former can be treated and trained in a similar way to the latter. With this mapping we can consider a DTNN as the DNN augmented with DP layers so that not only the <phrase>BP</phrase> <phrase>learning algorithm</phrase> of DTNNs can be cleanly derived but also new types of DTNNs can be more easily developed. Evaluation on Switchboard tasks indicates that DTNNs can outperform the already <phrase>high</phrase>-performing DNNs with 45% and 3% relative word error reduction, respectively, using 30-hr and 309-hr <phrase>training sets</phrase>.
The <phrase>Shallow Grave</phrase> Bernd Heinrich is an <phrase>emeritus professor</phrase> of <phrase>biology</phrase> at the <phrase>University</phrase> of <phrase>Vermont</phrase> and a prolific <phrase>author</phrase> on topics in <phrase>nature</phrase>. He is best known for his <phrase>research</phrase> and writings on <phrase>insect</phrase> and <phrase>bird</phrase> behavior. His most recent <phrase>book</phrase> is advertised as a naturalistic perspective on <phrase>animal</phrase> <phrase>death</phrase>, and it delivers on this promise by <phrase>surveying</phrase> the many fascinating ways that living things have evolved to benefit from the <phrase>death</phrase> that surrounds them. <phrase>Life</phrase> Everlasting: The <phrase>Animal</phrase> Way of <phrase>Death</phrase> moves quickly through engaging anecdotes on scavenging, mating, <phrase>burial</phrase>, and conservation with an inquisitive, almost childlike joy. The <phrase>book</phrase> offers a collection of essays on <phrase>animal behavior</phrase> in the presence of <phrase>death</phrase> but does not represent a concerted effort to understand the role of <phrase>death</phrase> in an <phrase>ecosystem</phrase>, the <phrase>evolution</phrase> of <phrase>death</phrase>, or even a working definition of <phrase>death</phrase>. Heinrich does, however, provide <phrase>firm</phrase> foundations for discussion of these and other theoretical issues. When most of us stumble upon a dead <phrase>deer</phrase> in the woods, our interest and disgust battle to determine the <phrase>radius</phrase> of our hike around the corpse. We give a wide berth. Heinrich never hesitates in his approach of the dead <phrase>animal</phrase>, and without caveat he absorbs the scene to <phrase>report</phrase> what you might have known but never stopped and stooped to consider. He demonstrates how to be boldly curious in the face of <phrase>death</phrase>. This is the gift of <phrase>Life</phrase> Everlasting, for once the emotional veil is lifted, the intellectual heavy lifting can begin. Many of the book's vignettes include drawings, providing visual cues to relevant <phrase>anatomy</phrase>. We learn about the co-<phrase>evolutionary arms race</phrase> between <phrase>beetle</phrase> and <phrase>tree</phrase>, <phrase>salmon</phrase> <phrase>suicide</phrase>, and the role of <phrase>plankton</phrase> in building the <phrase>Roman Empire</phrase>. The web of relationships is dense in the natural grave canines open a hide, <phrase>birds</phrase> call out the dinner bell, <phrase>bacteria</phrase> putrefies while <phrase>insects</phrase> sanitize and <phrase>bury</phrase>. Falling to the <phrase>ocean</phrase> floor with a dead <phrase>whale</phrase>, we <phrase>tour</phrase> the strata of <phrase>deep-sea</phrase> <phrase>life</phrase> until resting on the perfectly dark floor where <phrase>sharks</phrase>, <phrase>eels</phrase>, <phrase>crustaceans</phrase>, and worms take
Clustering and <phrase>Prototype</phrase> Based Classification Acknowledgments This <phrase>thesis</phrase> could not have been written without the support of many people. First of all, I would like to <phrase>express my deep</phrase> gratefulness to my mentor, Prof. Klaus Obermayer. His lecture on " neural <phrase>information processing</phrase> " inspired me to enter the field of <phrase>machine learning</phrase>. I am very grateful to him for letting me join his creative and very international <phrase>research</phrase> group. He has been a driving force behind my <phrase>research</phrase> by inspiring me, providing me with critical advice and always having an open ear for problems. During our almost <phrase>daily</phrase> <phrase>jogging</phrase> through the <phrase>park</phrase> of Schloss <phrase>Charlottenburg</phrase> we had many interesting talks and discussions. I also deeply appreciate the support of Prof. Mathias Bode during my time at the <phrase>company</phrase> Cortologic. His critical and detailed questions guided my way in thinking scientifically. I am very grateful to Prof. Jang Byung-Tak for inviting me to present my <phrase>research</phrase> at <phrase>Seoul National University</phrase> and for interesting discussions during his stay in <phrase>Berlin</phrase>. I also owe thanks to Prof. Joachim Buhmann for providing the <phrase>protein</phrase> <phrase>data set</phrase>. Prof. Thomas Hofmann I wish to thank for letting me use the document <phrase>data</phrase>. I want to thank all my collegues at the NI <phrase>research</phrase> group. Especially, I thank my roommate Roland Vollgraf for his <phrase>company</phrase>. I also thank for their friendship Dr.mer. I appreciate the help of Gabriele Rsler and Camilla Bruns for dealing with all the <phrase>bureaucracy</phrase>. In particular, I am most grateful to my parents for their support and <phrase>love</phrase>. I will never forget they gave me the opportunity for my studies in <phrase>Korea</phrase> and <phrase>Germany</phrase>, their trust and never-ending support. I am also most grateful to my husband, Johannes Mohr, for his support as colleague and as best friend. I thank him for <phrase>reading</phrase> this work critically, correcting my <phrase>English</phrase> and making suggestions for last-minute improvements. I wish to thank him most of all for his <phrase>love</phrase> and understanding in difficult times during my <phrase>thesis</phrase>.
Efficient search methods and <phrase>deep belief</phrase> networks with <phrase>particle</phrase> filtering for non-rigid tracking: Application to lip tracking <phrase>Pattern recognition</phrase> methods have become a powerful tool for segmentation in the sense that they are capable of automatically building a segmentation <phrase>model</phrase> from training images. However, they present several difficulties, such as requirement of a large set of <phrase>training data</phrase>, robustness to imaging conditions not present in the <phrase>training set</phrase>, and complexity of the search process. In this <phrase>paper</phrase> we tackle the second problem by using a <phrase>deep belief</phrase> network learning <phrase>architecture</phrase>, and the third problem by resorting to efficient searching <phrase>algorithms</phrase>. As an example, we illustrate the performance of the <phrase>algorithm</phrase> in lip segmentation and tracking in <phrase>video</phrase> sequences. Quantitative comparison using different strategies for the search process are presented. We also compare our approach to a <phrase>state</phrase>-of-the-<phrase>art</phrase> segmentation and tracking <phrase>algorithm</phrase>. The comparison show that our <phrase>algorithm</phrase> produces competitive segmentation <phrase>results</phrase> and that efficient search strategies reduce ten times the run-complexity.
<phrase>Agnostic</phrase> Learning vs. <phrase>Prior Knowledge</phrase> Challenge " When everything fails, ask for additional <phrase>domain knowledge</phrase> " is the current <phrase>motto</phrase> of <phrase>machine learning</phrase>. Therefore, assessing the real added value of prior/<phrase>domain knowledge</phrase> is a both deep and practical question. Most commercial <phrase>data mining</phrase> programs accept <phrase>data</phrase> pre-formatted as a table, each example being encoded as a fixed set of features. Is it worth spending time <phrase>engineering</phrase> elaborate features incorporating <phrase>domain knowledge</phrase> and/or designing <phrase>ad hoc</phrase> <phrase>algorithms</phrase>? Or else, can off-the-shelf programs working on simple features encoding the raw <phrase>data</phrase> without much <phrase>domain knowledge</phrase> do as well or better than skilled <phrase>data</phrase> analysts? To answer these questions, we organized a challenge for IJCNN 2007. The participants were allowed to compete in two tracks: The " <phrase>prior knowledge</phrase> " <phrase>track</phrase>, for which they had access to the original raw <phrase>data</phrase> representation and as much <phrase>knowledge</phrase> as possible about the <phrase>data</phrase>, and the " <phrase>agnostic</phrase> learning " <phrase>track</phrase> for which they were forced to use <phrase>data</phrase> pre-formatted as a table with dummy features. The challenge <phrase>web site</phrase> remains open for post-challenge
<phrase>Chess</phrase> Q&a : <phrase>Question Answering</phrase> on <phrase>Chess</phrase> <phrase>Games</phrase> We introduce a new dataset 1 for the evaluation of models addressing reasoning tasks. For a position in a <phrase>chess</phrase> <phrase>game</phrase>, we provide question and answer pairs, an image of the board, and the <phrase>sequence</phrase> of moves up to that position. We hope this synthetic task will improve our understanding in <phrase>memory</phrase> based <phrase>Deep Learning</phrase> with posed challenges.
Modeling <phrase>student</phrase> <phrase>programming</phrase> with multimodal <phrase>learning analytics</phrase> (abstract only) Understanding how students solve computational problems is central to <phrase>computer science</phrase> <phrase>education</phrase> <phrase>research</phrase>. This goal is facilitated by <phrase>recent advances</phrase> in the availability and analysis of detailed multimodal <phrase>data</phrase> collected during <phrase>student</phrase> learning. <phrase>Drawing</phrase> on <phrase>research</phrase> into <phrase>student</phrase> <phrase>problem-solving</phrase> processes and findings on <phrase>human</phrase> posture and gesture, this <phrase>poster</phrase> utilizes a multimodal <phrase>learning analytics</phrase> framework that links automatically identified posture and gesture features with <phrase>student</phrase> <phrase>problem-solving</phrase> and dialogue events during one-on-one <phrase>human</phrase> tutoring of introductory <phrase>computer science</phrase>. The findings provide new insight into how bodily movements occur during <phrase>computer science</phrase> tutoring, and lay the foundation for <phrase>programming</phrase> <phrase>feedback</phrase> tools and deep analyses of <phrase>student</phrase> learning processes.
I <phrase>Cognitive</phrase> <phrase>Speech Coding</phrase> <phrase>Cognitive</phrase> <phrase>Speech Coding</phrase> The speech signal coveys both the <phrase>linguistic</phrase>-symbolic and continuous-<phrase>acoustic</phrase> <phrase>information</phrase>. The former is the result of underlying <phrase>cognitive</phrase> speech processes, whereas the latter is the result of <phrase>motor control</phrase> speech processes. The gap between the <phrase>cognitive</phrase> and motor speech processes is narrowing by converging of speech <phrase>engineering</phrase> and <phrase>motor control</phrase>, <phrase>psycholinguistics</phrase>, <phrase>neuropsychology</phrase> and speech <phrase>neuroscience</phrase>, and recent <phrase>deep learning</phrase> approaches. The purpose of this <phrase>paper</phrase> is to propose a novel <phrase>architecture</phrase> of <phrase>speech coding</phrase>. We attribute the novel <phrase>speech coding</phrase> as <phrase>cognitive</phrase>, for compressing the speech signal into code that can be interpreted at the <phrase>linguistic</phrase> level, and manipulated by the computational models of <phrase>speech production</phrase>, such as the Directions Into Velocities of Articulators <phrase>model</phrase> and the Hierarchical <phrase>State</phrase> <phrase>Feedback</phrase> Control <phrase>model</phrase>. Linguistically relevant transmission code brings novel functionality to speech transmission systems, performing tasks such as automatic <phrase>dialect</phrase> correction of the speakers, or intelligibility enhancement of speakers with motor speech disorders. The proposed <phrase>speech coding</phrase> facilitates an integration of speech transmission with <phrase>higher level</phrase> sequential speech applications, such as <phrase>automatic speech recognition</phrase> and synthesis, and <phrase>machine translation</phrase> systems.
Pointing the Unknown Words The problem of rare and unknown words is an important issue that can potentially effect the performance of many <phrase>NLP</phrase> systems, including both traditional <phrase>count</phrase> based models and <phrase>deep learning</phrase> models. We propose a novel way to deal with the rare and unseen words for the <phrase>neural network</phrase> models with attention. Our <phrase>model</phrase> uses two softmax layers in <phrase>order</phrase> to predict the next word in conditional <phrase>language</phrase> models: one of the softmax layers predicts the location of a word in the source sentence, and the other softmax layer predicts a word in the shortlist <phrase>vocabulary</phrase>. The decision of which softmax layer to use at each timestep is adaptively made by an MLP which is conditioned on the context. We motivate this work from a <phrase>psychological</phrase> evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. Using our proposed <phrase>model</phrase>, we observe improvements in two tasks, neural <phrase>machine translation</phrase> on the Europarl <phrase>English</phrase> to <phrase>French</phrase> <phrase>parallel corpora</phrase> and text summarization on the Gigaword dataset.
Learning for <phrase>Semantic</phrase> <phrase>Parsing</phrase> Using <phrase>Statistical Machine Translation</phrase> Techniques <phrase>Semantic</phrase> <phrase>parsing</phrase> is the <phrase>construction</phrase> of a complete, formal, symbolic meaning representation of a sentence. While it is crucial to <phrase>natural language understanding</phrase>, the problem of <phrase>semantic</phrase> <phrase>parsing</phrase> has received relatively little attention from the <phrase>machine learning</phrase> <phrase>community</phrase>. Recent work on <phrase>natural language understanding</phrase> has mainly focused on shallow <phrase>semantic</phrase> analysis, such as <phrase>word-sense disambiguation</phrase> and <phrase>semantic</phrase> role labeling. <phrase>Semantic</phrase> <phrase>parsing</phrase>, on the other hand, involves deep <phrase>semantic</phrase> analysis in which word senses, <phrase>semantic</phrase> roles and other components are combined to produce useful meaning representations for a particular application domain (e.g. <phrase>database</phrase> query). Prior <phrase>research</phrase> in <phrase>machine learning</phrase> for <phrase>semantic</phrase> <phrase>parsing</phrase> is mainly based on <phrase>inductive logic programming</phrase> or deterministic <phrase>parsing</phrase>, which lack some of the robustness that characterizes statistical learning. Existing statistical approaches to <phrase>semantic</phrase> <phrase>parsing</phrase>, however, are mostly concerned with relatively simple application domains in which a meaning representation is no more than a <phrase>single</phrase> <phrase>semantic</phrase> frame. In this proposal, we present a novel statistical approach to <phrase>semantic</phrase> <phrase>parsing</phrase>, <phrase>WASP</phrase>, which can handle meaning representations with a nested structure. The <phrase>WASP</phrase> <phrase>algorithm</phrase> learns a <phrase>semantic</phrase> parser given a set of sentences annotated with their correct meaning representations. The <phrase>parsing</phrase> <phrase>model</phrase> is based on the synchronous <phrase>context-free grammar</phrase>, where each rule maps a <phrase>natural-language</phrase> <phrase>substring</phrase> to its meaning representation. The main <phrase>innovation</phrase> of the <phrase>algorithm</phrase> is its use of <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>statistical machine translation</phrase> techniques. A statistical word alignment <phrase>model</phrase> is used for lexical acquisition, and the <phrase>parsing</phrase> <phrase>model</phrase> itself can be seen as an instance of a <phrase>syntax</phrase>-based <phrase>translation</phrase> <phrase>model</phrase>. In initial evaluation on several <phrase>real-world</phrase> <phrase>data</phrase> sets, we show that <phrase>WASP</phrase> performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and <phrase>word order</phrase>. In future work, we intend to pursue several directions in developing accurate <phrase>semantic</phrase> parsers for a <phrase>variety</phrase> of application domains. This will involve exploiting <phrase>prior knowledge</phrase> about the <phrase>natural-language</phrase> <phrase>syntax</phrase> and the application domain. We also plan to construct a <phrase>syntax</phrase>-aware word-based alignment <phrase>model</phrase> for lexical acquisition. Finally, we will generalize the <phrase>learning algorithm</phrase> to handle <phrase>context-dependent</phrase> sentences and accept noisy <phrase>training data</phrase>.
Forecasting market prices in a <phrase>supply chain</phrase> <phrase>game</phrase> Future market conditions can be a pivotal factor in making <phrase>business</phrase> decisions. We present and evaluate methods used by our agent, Deep <phrase>Maize</phrase>, to forecast market prices in the Trading Agent Competition <phrase>Supply Chain Management</phrase> <phrase>Game</phrase>. As a guiding principle we seek to exploit as many sources of available <phrase>information</phrase> as possible to inform predictions. Since <phrase>information</phrase> comes in several different forms, we integrate well-known methods in a novel way to make predictions. The core of our predictor is a nearest-neighbors <phrase>machine learning</phrase> <phrase>algorithm</phrase> that identifies historical instances with similar economic indicators. We augment this with an <phrase>online learning</phrase> procedure that transforms the predictions by optimizing a scoring rule. This allows us to select more relevant historical contexts using additional <phrase>information</phrase> available during individual <phrase>games</phrase>. We also explore the advantages of two different representations for predicting price <phrase>distributions</phrase>. One uses absolute prices, and the other uses <phrase>statistics</phrase> of price <phrase>time series</phrase> to exploit local stability. We evaluate these methods using both <phrase>data</phrase> from the 2005 <phrase>tournament</phrase> final round and additional simulations. We compare several variations of our predictor to one another and a baseline predictor similar to those used by many other <phrase>tournament</phrase> agents. We show substantial improvements over the baseline predictor, and demonstrate that each element of our predictor contributes to improved performance.
Efficient Methods for <phrase>Unsupervised Learning</phrase> of <phrase>Probabilistic Models</phrase> Efficient Methods for <phrase>Unsupervised Learning</phrase> of <phrase>Probabilistic Models</phrase> <phrase>High</phrase> dimensional <phrase>probabilistic models</phrase> are used for many modern scientific and <phrase>engineering</phrase> <phrase>data analysis</phrase> tasks. Interpreting neural spike trains, compressing <phrase>video</phrase>, identifying features in <phrase>DNA</phrase> microarrays, and recognizing particles in <phrase>high energy physics</phrase> all rely upon the ability to find and <phrase>model</phrase> complex structure in a <phrase>high</phrase> dimensional space. Despite their great promise, <phrase>high</phrase> dimensional <phrase>probabilistic models</phrase> are frequently computationally intractable to work with in practice. In this <phrase>thesis</phrase> I develop solutions to overcome this intractability, primarily in the context of <phrase>energy</phrase> based models. A common cause of intractability is that <phrase>model</phrase> <phrase>distributions</phrase> cannot be analytically normalized. <phrase>Probabilities</phrase> can only be computed up to a constant, making training exceedingly difficult. To solve this problem I propose 'minimum <phrase>probability</phrase> flow learning', a variational technique for <phrase>parameter estimation</phrase> in such models. The utility of this training technique is demonstrated in the case of an <phrase>Ising model</phrase>, a Hopfield auto-<phrase>associative memory</phrase>, an <phrase>independent component analysis</phrase> <phrase>model</phrase> of <phrase>natural images</phrase>, and a <phrase>deep belief</phrase> network. A second common difficulty in training <phrase>probabilistic models</phrase> arises when the <phrase>parameter space</phrase> is ill-conditioned. This makes <phrase>gradient descent</phrase> optimization slow and impractical, but can be alleviated using the natural <phrase>gradient</phrase>. I show here that the natural <phrase>gradient</phrase> can be related to signal whitening, and provide specific prescriptions for applying it to learning problems. It is also difficult to evaluate the performance of models that cannot be analytically normalized, providing a particular challenge to <phrase>hypothesis testing</phrase> and <phrase>model</phrase> comparison. To overcome this, I introduce a method termed '<phrase>Hamiltonian</phrase> annealed <phrase>importance sampling</phrase>,' which more efficiently estimates the normalization constant of non-analytically-normalizable models. This method is then used to calculate and compare the log likelihoods of several <phrase>state</phrase> of the <phrase>art</phrase> <phrase>probabilistic models</phrase> of natural <phrase>image patches</phrase>. 1 Finally, many tasks performed with a trained <phrase>probabilistic model</phrase> (for instance, image denoising or inpainting and <phrase>speech recognition</phrase>) involve generating samples from the <phrase>model</phrase> distribution, which is typically a very computationally expensive process. I introduce a modification to <phrase>Hamiltonian</phrase> <phrase>Monte Carlo</phrase> sampling that reduces the tendency of sampling trajectories to double back on themselves, and enables statistically <phrase>independent</phrase> samples to be generated more rapidly. Taken together, it is my hope that these contributions will help scientists and engineers to build and manipulate <phrase>probabilistic models</phrase>. 2 Acknowledgements
<phrase>Deep Learning</phrase> for Efficient Discriminative <phrase>Parsing</phrase> We propose a new fast purely discrimina-tive <phrase>algorithm</phrase> for <phrase>natural language</phrase> <phrase>parsing</phrase>, based on a " deep " recurrent convolutional <phrase>graph</phrase> <phrase>transformer</phrase> network (GTN). Assuming a decomposition of a <phrase>parse tree</phrase> into a stack of " levels " , the network predicts <phrase>a level</phrase> of the <phrase>tree</phrase> taking into account predictions of previous levels. Using only few <phrase>basic</phrase> text features which leverage word representations from Collobert and Weston (2008), we show similar performance (in F 1 score) to existing pure discriminative parsers and existing " benchmark " parsers (like Collins parser, probabilistic <phrase>context-free</phrase> grammars based), with a huge speed advantage.
<phrase>Domain Adaptation</phrase> and <phrase>Transfer Learning</phrase> in StochasticNets <phrase>Transfer learning</phrase> is a recent field of <phrase>machine learning</phrase> <phrase>research</phrase> that aims to resolve the challenge of dealing with insufficient <phrase>training data</phrase> in the domain of interest. This is a particular issue with traditional <phrase>deep neural networks</phrase> where a large amount of <phrase>training data</phrase> is needed. Recently, StochasticNets was proposed to take advantage of sparse connectivity in <phrase>order</phrase> to decrease the number of parameters that needs to be learned, which in turn may relax <phrase>training data</phrase> size requirements. In this <phrase>paper</phrase>, we study the efficacy of <phrase>transfer learning</phrase> on StochasticNet frameworks. <phrase>Experimental</phrase> <phrase>results</phrase> show 7% improvement on StochasticNet performance when the <phrase>transfer learning</phrase> is applied in training step.
Approximate <phrase>Learning Algorithm</phrase> for <phrase>Restricted Boltzmann Machines</phrase> <phrase>Deep Belief</phrase> Networks (DBNs) are <phrase>Bayesian</phrase> networks with many layers of <phrase>hidden units</phrase> which were recently introduced along with a <phrase>greedy layer-wise</phrase> <phrase>learning algorithm</phrase> by Hinton et al. In the <phrase>learning algorithm</phrase>, the main <phrase>building block</phrase> of a DBN is a <phrase>bipartite</phrase> undirected <phrase>graphical model</phrase> called <phrase>Restricted Boltzmann machine</phrase> (RBM). Hinton et al. used approximate <phrase>learning algorithms</phrase> called contrastive divergences to learn RBMs, but in 2008 Roux and Bengio proposed new <phrase>learning algorithm</phrase> for RBMs, which is more suitable for the <phrase>greedy layer-wise</phrase> learning procedure of DBNs. However, Roux and Bengio's <phrase>learning algorithm</phrase> includes costly calculation whose amount is <phrase>proportional</phrase> to the square of the number of <phrase>training data</phrase>. In this presentation, we propose a new fast <phrase>learning algorithm</phrase> for RBMs by applying an approximate method called Kullback-Leibler Importance Estimation Procedure to Roux and Bengio's <phrase>learning algorithm</phrase>. We also show its validity by comparing our <phrase>proposed algorithm</phrase> with Roux and Bengio's <phrase>learning algorithm</phrase> using numerical experiments based on <phrase>artificial</phrase> <phrase>data</phrase>.
Customer Shopping Pattern Prediction: a <phrase>Recurrent Neural Network</phrase> Approach <phrase>Customer relationship management</phrase> is a popular and strategic topic in <phrase>marketing</phrase> and quality of service. The availability of big transactions <phrase>data</phrase> as well as <phrase>computing</phrase> systems have provided a great opportunity to <phrase>model</phrase> and predict customer behaviour. However, there is a lack of modern modelling and analytical methods to perform analysis on such <phrase>data</phrase>. <phrase>Deep learning</phrase> techniques can assist <phrase>marketing</phrase> decision makers to provide more reliable and practical <phrase>marketing</phrase> strategic plans. In this <phrase>paper</phrase>, we propose a customer behaviour prediction <phrase>model</phrase> using <phrase>recurrent neural networks</phrase> (RNNs) based on the client loyalty number (CLN), recency, <phrase>frequency</phrase>, and monetary (RFM) variables. The experiment <phrase>results</phrase> show that RNNs can predict RFM values of customers efficiently. This <phrase>model</phrase> can be later used in <phrase>recommender systems</phrase> for exclusive promotional offers and loyalty programs <phrase>management</phrase>.
<phrase>Graphical</phrase> Modeling Tools for <phrase>Systems Biology</phrase> Modeling biological systems to understand their mechanistic behavior is an important activity in molecular <phrase>systems biology</phrase>. <phrase>Mathematical</phrase> modeling typically requires deep <phrase>mathematical</phrase> or <phrase>computing</phrase> <phrase>knowledge</phrase>, and this limits the spread of modeling tools among <phrase>biologists</phrase>. <phrase>Graphical</phrase> modeling languages have been introduced to minimize this limit. Here, we survey the main <phrase>graphical</phrase> formalisms (supported by <phrase>software</phrase> tools) available to <phrase>model</phrase> biological systems with a primary focus on their <phrase>usability</phrase>, within the framework of modeling <phrase>reaction</phrase> pathways with two-dimensional (2D) (possibly nested) compartments. Considering the main characteristics of the surveyed formalisms, we synthesise a new proposal (Style) and <phrase>report</phrase> the <phrase>results</phrase> of an online survey conducted among <phrase>biologists</phrase> to assess <phrase>usability</phrase> of available <phrase>graphical</phrase> formalisms. We consider this proposal a guideline developed from what we learned in the survey, which can inform development of <phrase>graphical</phrase> formalisms to <phrase>model</phrase> <phrase>reaction</phrase> pathways in 2D space.
Deep Structure Learning: Beyond <phrase>Connectionist</phrase> Approaches Deep structure learning is a promising new <phrase>area</phrase> of work in the field of <phrase>machine learning</phrase>. Previous work in this <phrase>area</phrase> has shown impressive performance, but all of it has used <phrase>connectionist</phrase> models. We hope to demonstrate that the utility of <phrase>deep architectures</phrase> is not restricted to <phrase>connectionist</phrase> models. Our approach is to use simple, non-<phrase>connectionist</phrase> <phrase>dimensionality reduction</phrase> techniques in conjunction with a <phrase>deep architecture</phrase> to examine more precisely the impact of the <phrase>deep architecture</phrase> itself. To do this, we use standard <phrase>PCA</phrase> as a baseline and compare it with a <phrase>deep architecture</phrase> using <phrase>PCA</phrase>. We perform several <phrase>image classification</phrase> experiments using the features generated by the two techniques, and we conclude that the <phrase>deep architecture</phrase> leads to improved classification performance, supporting the deep structure <phrase>hypothesis</phrase>.
The Development of <phrase>Deep Learning</phrase> during a Synchronous Collaborative On-line Course the Development of <phrase>Deep Learning</phrase> during a Synchronous Collaborative On-line Course one copy of any article(s) in <phrase>SHURA</phrase> to facilitate their <phrase>private</phrase> study or for non-commercial <phrase>research</phrase>. You may not engage in further distribution of the material or use it for any profit-making activities or any commercial gain. ABSTRACT As <phrase>Internet</phrase> bandwidth improves and connections become more reliable, on-line course designers will be encouraged to make more structured use of synchronous communications. Little work has so far been reported on how to make the best use of synchronous communications to support a <phrase>problem solving</phrase> approach. The OTIS <phrase>pilot</phrase> course made extensive use of synchronous <phrase>communication</phrase> to support learning through <phrase>case studies</phrase> in <phrase>occupational therapy</phrase>. The transcripts of <phrase>communication</phrase> sessions have been analysed using the SOLO <phrase>taxonomy</phrase>, to study the development of <phrase>deep learning</phrase> week by week. <phrase>Results</phrase> show that synchronous <phrase>peer-to-peer</phrase> working meetings have an important role to <phrase>play</phrase> in the development of <phrase>deep learning</phrase>.
A <phrase>clash</phrase> of traditions: the <phrase>history</phrase> of comparative and <phrase>experimental</phrase> <phrase>embryology</phrase> in <phrase>Sweden</phrase> as exemplified by the <phrase>research</phrase> of Gsta Jgersten and Sven Hrstadius Until the 1940s <phrase>research</phrase> traditions were often imported from <phrase>Germany</phrase> to <phrase>Sweden</phrase>, and young scientists went to <phrase>German</phrase> <phrase>universities</phrase> to learn new techniques and get in touch with the latest ideas. In <phrase>developmental biology</phrase>, the comparative, <phrase>phylogenetic</phrase> <phrase>embryology</phrase> advocated most forcefully by <phrase>Ernst Haeckel</phrase> co-existed with the "Entwickelungsmechanik" tradition developed by Wilhelm His, Wilheln Roux and others partly as a <phrase>reaction</phrase> to Haeckel's ideas. I use the <phrase>zoology</phrase> <phrase>department</phrase> at <phrase>Uppsala University</phrase> as a microcosmos to reflect the tensions between these traditions: Gsta Jgersten (1903-1993) and Sven Hrstadius (1898-1996) are used as exemples. Jgersten was a <phrase>marine biologist</phrase> who worked on the <phrase>morphology</phrase> and <phrase>evolution</phrase> of <phrase>invertebrates</phrase>, especiallly their <phrase>larval</phrase> forms. He developed a <phrase>comprehensive</phrase> theory describing the <phrase>evolution</phrase> of the <phrase>life</phrase> cycle in early metazoans. <phrase>Recapitulation</phrase> was an important ingredient, and Jgersten explicitly based his reasoning on Ernst Haeckel's "biogenetic <phrase>law</phrase>". Jgersten developed Haeckel's "Gastraea" theory into another hypothetical <phrase>animal</phrase>-Bilaterogastraea-that came into being when the holopelagic Blastaea settled on the ground as an adult and kept a <phrase>pelagic</phrase>, <phrase>planktonic</phrase> <phrase>larval</phrase> form. This was the birth of the pelago-benthic <phrase>life</phrase> cycle, which plays such an important role in Jgersten's speculations on the deep phylogeny of metazoans. Sven Hrstadius was a leading <phrase>experimental</phrase> <phrase>embryologist</phrase> in the mid-twentieth century. His most important work was on the determination and differentiation of the <phrase>sea urchin</phrase> <phrase>embryo</phrase>. Early work inspired by his <phrase>teacher</phrase> John Runnstrm's double <phrase>gradient</phrase> theory showed that gradients of animalness (ectodermal determination) and vegetalness (endodermal determination) existed in the 16- and 32- <phrase>cell</phrase> <phrase>embryos</phrase>. Hrstadius became famous for his elegant extirpation and transplantation experiments using <phrase>glass</phrase> needles, and for his microsugical skills. He also made important contributions to the study of cranial <phrase>neural crest</phrase> development in the <phrase>Mexican</phrase> <phrase>axolotl</phrase>, in collaboration with his <phrase>student</phrase> Sven Sellman. Hrstadius was the great experimentalist, but did not develop <phrase>speculative</phrase> hypotheses the way Jgersten did. The very different styles of scientific <phrase>research</phrase> might have played a role also in the development of the personal difficulties that existed for a <phrase>long</phrase> time between the two professors.
An improved <phrase>algorithm</phrase> for learning <phrase>long</phrase>-term dependency problems in adaptive processing of <phrase>data structures</phrase> Many researchers have explored the use of <phrase>neural-network</phrase> representations for the adaptive processing of <phrase>data structures</phrase>. One of the most popular learning formulations of <phrase>data structure</phrase> processing is <phrase>backpropagation</phrase> through structure (BPTS). The BPTS <phrase>algorithm</phrase> has been successful applied to a number of learning tasks that involve structural patterns such as <phrase>logo</phrase> and natural scene classification. The main limitations of the BPTS <phrase>algorithm</phrase> are attributed to slow convergence speed and the <phrase>long</phrase>-term dependency problem for the adaptive processing of <phrase>data structures</phrase>. In this <phrase>paper</phrase>, an improved <phrase>algorithm</phrase> is proposed to solve these problems. The idea of this <phrase>algorithm</phrase> is to optimize the <phrase>free</phrase> learning parameters of the <phrase>neural network</phrase> in the node representation by using least-squares-based optimization methods in a <phrase>layer-by-layer</phrase> <phrase>fashion</phrase>. Not only can fast convergence speed be achieved, but the <phrase>long</phrase>-term dependency problem can also be overcome since the vanishing of <phrase>gradient</phrase> <phrase>information</phrase> is avoided when our approach is applied to very deep <phrase>tree</phrase> structures.
Visual indexing and retrieval This <phrase>book</phrase> includes a preface, a table of contents, six informative chapters and expansive references. In the preface, the editors of the <phrase>book</phrase> claim that with the advent of <phrase>social networks</phrase>, vast amount of visual <phrase>information</phrase> is available for end-users. Therefore, they need innovative and fruitful methods for content understanding, retrieval and classification. Chapter 1 is written by editors as introduction. They significantly emphasized that " visual indexing and retrieval " is a <phrase>research</phrase> challenge and has become of foremost importance nowadays. The second chapter provides a deep overview of the <phrase>basic</phrase> visual <phrase>feature extraction</phrase> and description methods and approaches in the <phrase>literature</phrase> for images as well as for <phrase>spatio-temporal</phrase> <phrase>data analysis</phrase>. The chapter aims to clarify the process of the main recognition steps of detection and description of image. Interest point detection approaches, point-based and <phrase>region</phrase>-based detectors, the spatiotemporal <phrase>feature extraction</phrase> and some reference feature descriptors are also presented and discussed. On the rest, some well-known <phrase>feature extraction</phrase> schemes, like Harris and LoG detectors and feature description approaches like SIFT, SURF and GIST are presented. In a later chapter, the exploitation of these descriptors in a global <phrase>image processing</phrase> chain is discussed. Feature Coding and pooling as main steps for deriving image representation from visual local descriptors are also detailed. Moreover, Bag-of-Visual-Words (BVW) including recent extensions as <phrase>sparse coding</phrase> and spatial pooling methods are explained. A class of similarity functions, called kernels (Fisher Kernel approach) is deeply presented as a strategy to build similarity measures. Additionally, two <phrase>major</phrase> contributions from the <phrase>Machine learning</phrase> <phrase>community</phrase> as <phrase>learning algorithm</phrase> namely <phrase>Support Vector Machines</phrase> (<phrase>SVM</phrase>) and Boosting are deeply explained. Lastly, new trends (LPBoost, MKL) in <phrase>machine learning</phrase> are expressed as well. Two trends in incorporation of spatial context in visual content indexing and retrieval and multi-resolution/multiscale content description are addressed in chapter 4. Recent works on the border of BoVW and structural <phrase>pattern recognition</phrase> approaches called " <phrase>Graph</phrase> Words " are reviewed. Further, on the basis of scalable, multiresolution/multi-scale visual content representation in modern compression standards is presented. Furthermore, approaches which introduce spatial context during the matching process are explained and Structural pattern are widely represented by <phrase>graphs</phrase>.
Environmental <phrase>Water Pollution</phrase> Based Abrupt Environmental <phrase>Water Pollution</phrase> Based Abrupt Environmental <phrase>Water Pollution</phrase> Based Abrupt Environmental <phrase>Water Pollution</phrase> Based Abrupt Event Event Event Event Monitoring Monitoring Monitoring Monitoring - -- -a a a a Survey Survey Survey Survey The importance of maintaining good <phrase>water</phrase> environment highlights the increasing need for advanced and good technologies. This <phrase>paper</phrase> presents a system that proposes a novel <phrase>design</phrase> of <phrase>water</phrase> environment monitoring system; the functions of remote detection and real-time monitoring of natural <phrase>water</phrase> are implemented through the <phrase>Zigbee</phrase> <phrase>wireless</phrase> <phrase>data transmission</phrase> <phrase>technology</phrase>. <phrase>Water pollution</phrase> monitoring based-the abrupt event monitoring is a challenging and critical issue in <phrase>water</phrase> environment systems. In this system, a novel abrupt event monitoring approach based on kernel <phrase>principal component analysis</phrase> (KPCA) and <phrase>deep learning</phrase> machine (DLM) mechanism is proposed. The system consists of <phrase>wireless</phrase> <phrase>water quality</phrase> monitoring network and remote <phrase>data</phrase> centre. The parameters involved in the <phrase>water quality</phrase> determination such as the <phrase>pH level</phrase>, <phrase>temperature</phrase> and <phrase>turbidity</phrase> is measured in the real time by the sensors that send the <phrase>data</phrase> to the <phrase>base station</phrase> or control/monitoring room. This system proposes how such monitoring system can be setup emphasizing on the aspects of <phrase>low cost</phrase>, easy installation and easy to handling and maintain. form is a necessary condition for its publication, as well as its content. environment system, kernel <phrase>principal component analysis</phrase> (KPCA) and <phrase>deep learning</phrase> machine (DLM), <phrase>ZigBee</phrase>
Latent <phrase>feature learning</phrase> in <phrase>social media</phrase> network The current trend in <phrase>social media</phrase> analysis and application is to use the pre-defined features and devoted to the later <phrase>model</phrase> development modules to meet the end tasks. In this work, we claim that representation is critical to the end tasks and contributes much to the <phrase>model</phrase> development module. We provide evidence that specially learned feature well addresses the diverse, heterogeneous and collective characteristics of <phrase>social media</phrase> <phrase>data</phrase>. Therefore, we propose to transfer the focus from the <phrase>model</phrase> development to latent <phrase>feature learning</phrase>, and present a <phrase>general</phrase> <phrase>feature learning</phrase> framework based on the popular <phrase>deep architecture</phrase>. In particular, following the <phrase>proposed framework</phrase>, we <phrase>design</phrase> a novel <phrase>relational</phrase> generative <phrase>deep learning</phrase> <phrase>model</phrase> to <phrase>test</phrase> the idea on link analysis tasks in the <phrase>social media</phrase> networks. We show that the derived latent features well embed both the <phrase>media</phrase> content and their observed links, leading to improvement in <phrase>social media</phrase> tasks of user recommendation and social image annotation.
<phrase>Game</phrase> Playing: the next Moves <phrase>Game</phrase> Playing as a Domain Search and <phrase>Knowledge</phrase> Computer programs now <phrase>play</phrase> many <phrase>board games</phrase> as well or better than the most expert humans. <phrase>Human</phrase> players, however, learn, plan, allocate resources, and integrate multiple streams of <phrase>knowledge</phrase>. This <phrase>paper</phrase> highlights recent achievements in <phrase>game</phrase> playing, describes some cogni-tively-oriented work, and poses three related challenge problems for the <phrase>AI</phrase> <phrase>community</phrase>. Work on <phrase>games</phrase> has had several traditional justifications. Given unambiguous rules, playing a <phrase>game</phrase> to win is a well-defined problem. A game's rules create <phrase>artificial</phrase> world states whose granularity is explicit. There is an initial <phrase>state</phrase>, a <phrase>state space</phrase> with clear transitions, and a set of readily describable goal states. Without intervening in-strumentation, <phrase>games</phrase> are also noise-<phrase>free</phrase>. For these reasons, as well as for their ability to amuse, <phrase>games</phrase> have often been referred to as " <phrase>toy</phrase> domains. " To <phrase>play</phrase> the most difficult <phrase>games</phrase> well, however, a program must contend with fundamental issues in <phrase>AI</phrase>: <phrase>knowledge representation</phrase>, search, learning, and planning. There are two principal reasons to continue to do <phrase>research</phrase> on <phrase>games</phrase>, despite Deep Blue's triumph (<phrase>Hamilton</phrase> and Hedberg 1997). First, <phrase>human</phrase> fascination with <phrase>game</phrase> playing is <phrase>long</phrase>-standing and pervasive. <phrase>Anthropologists</phrase> have catalogued popular <phrase>games</phrase> in almost every <phrase>culture</phrase>. Indeed, the same <phrase>game</phrase>, under various names, often appears on many continents (Bell 1969; Zaslavsky 1982). <phrase>Games</phrase> intrigue us because they address important cogni-tive functions. In particular, the <phrase>games</phrase> humans like to <phrase>play</phrase> are probably the ones we are good at, the ones that capitalize on our intellectual strengths and forgive our weaknesses. A program that plays many <phrase>games</phrase> well must simulate important <phrase>cognitive</phrase> skills. The second reason to continue <phrase>game</phrase>-playing <phrase>research</phrase> is that some difficult <phrase>games</phrase> remain to be won, <phrase>games</phrase> that people <phrase>play</phrase> very well but <phrase>computers</phrase> do not. These <phrase>games</phrase> clarify what our current approach lacks. They set challenges for us to meet, and they promise ample rewards. This <phrase>paper</phrase> summarizes the role of search and <phrase>knowledge</phrase> in <phrase>game</phrase> playing, the <phrase>state</phrase> of the <phrase>art</phrase>, and recent relevant <phrase>data</phrase> on expert <phrase>human</phrase> <phrase>game</phrase> players. It then shows how <phrase>cognitive</phrase> skills can enhance a <phrase>game</phrase>-playing program, and poses three new challenge problems for the <phrase>AI</phrase> <phrase>community</phrase>. Although rooted in <phrase>game</phrase> playing, these challenges could enhance performance in many domains. In this <phrase>paper</phrase>, a <phrase>game</phrase> is a <phrase>multi-agent</phrase>, noise-<phrase>free</phrase>, <phrase>discrete space</phrase> with a <phrase>finite set</phrase> of objects (the playing pieces) and a finite, static set of rules for <phrase>play</phrase> (agents' serial behavior). The rules delineate where playing 
Greedy <phrase>Algorithms</phrase> for Structurally Constrained <phrase>High</phrase> Dimensional Problems A <phrase>hallmark</phrase> of modern <phrase>machine learning</phrase> is its ability to deal with <phrase>high</phrase> dimensional problems by exploiting structural assumptions that limit the degrees of freedom in the underlying <phrase>model</phrase>. A <phrase>deep understanding</phrase> of the capabilities and limits of <phrase>high</phrase> dimensional learning methods under specific assumptions such as sparsity, group sparsity, and low rank has been attsined. Efforts [1,2] are now underway to distill this valuable experience by proposing <phrase>general</phrase> unified frameworks that can achieve the twio goals of summarizing previous analyses and enabling their application to notions of structure hitherto unexplored. Inspired by these developments, we propose and analyze a <phrase>general</phrase> computational scheme based on a greedy strategy to solve <phrase>convex optimization</phrase> problems that arise when dealing with structurally constrained <phrase>high</phrase>-dimensional problems. Our framework not only unifies existing greedy <phrase>algorithms</phrase> by recovering them as special cases but also yields novel ones. Finally, we extend our <phrase>results</phrase> to infinite dimensional settings by using interesting connections between <phrase>smoothness</phrase> of norms and behavior of martingales in <phrase>Banach spaces</phrase>.
Transformation-Invariant Convolutional Jungles Many <phrase>Computer Vision</phrase> problems arise from <phrase>information processing</phrase> of <phrase>data</phrase> sources with nuisance variances like scale, orientation, contrast, perspective foreshortening or in <phrase>medical imaging</phrase> <phrase>staining</phrase> and local <phrase>warping</phrase>. In most cases these variances can be stated a priori and can be used to improve the generalization of recognition <phrase>algorithms</phrase>. We propose a novel supervised <phrase>feature learning</phrase> approach, which efficiently extracts <phrase>information</phrase> from these constraints to produce interpretable, transformation-<phrase>invariant features</phrase>. The <phrase>proposed method</phrase> can incorporate a large class of transformations , e.g., shifts, rotations, change of scale, morphological operations, non-linear distortions, <phrase>photometric</phrase> transformations, etc. These features boost the <phrase>discrimination</phrase> power of a novel <phrase>image classification</phrase> and segmentation method, which we call Transformation-Invariant Convolu-tional Jungles (TICJ). We <phrase>test</phrase> the <phrase>algorithm</phrase> on two benchmarks in <phrase>face recognition</phrase> and <phrase>medical imaging</phrase>, where it achieves <phrase>state</phrase> of the <phrase>art</phrase> <phrase>results</phrase>, while being computation-ally significantly more efficient than <phrase>Deep Neural Networks</phrase>.
Training <phrase>Restricted Boltzmann Machines</phrase> with Multi-tempering: Harnessing Parallelization <phrase>Restricted Boltzmann Machines</phrase> (RBM's) are unsupervised probabilistic <phrase>neural networks</phrase> that can be stacked to form <phrase>Deep Belief</phrase> Networks. Given the recent popularity of RBM's and the increasing availability of <phrase>parallel computing</phrase> architectures, it becomes interesting to investigate <phrase>learning algorithms</phrase> for RBM's that benefit from parallel computations. In this <phrase>paper</phrase>, we look at two extensions of the parallel tempering <phrase>algorithm</phrase>, which is a <phrase>Markov Chain Monte Carlo</phrase> method to approximate the likelihood <phrase>gradient</phrase>. The first extension is <phrase>directed</phrase> at a more effective exchange of <phrase>information</phrase> among the parallel sampling chains. The second extension estimates gradients by averaging over chains from different temperatures. We investigate the efficiency of the proposed methods and demonstrate their usefulness on the MNIST dataset. Especially the weighted averaging seems to benefit <phrase>Maximum Likelihood</phrase> learning.
<phrase>Emotion</phrase> and <phrase>Computing</phrase> <phrase>Current Research</phrase> and Future Impact <phrase>Emotion</phrase> and <phrase>Computing</phrase> - <phrase>Current Research</phrase> and Future Impact Workshop The workshop focuses on the role of affect and <phrase>emotion</phrase> in computer systems including the three dimensions: <phrase>emotion</phrase> recognition, <phrase>emotion</phrase> generation and <phrase>emotion</phrase> modeling. Both shallow and deep models of <phrase>emotion</phrase> are in the focus of interest. The goal is to provide a forum for the presentation of <phrase>research</phrase> as well as of existing and future applications and for lively discussions among researchers and <phrase>industry</phrase>. In recent years <phrase>computer science</phrase> <phrase>research</phrase> has shown increasing efforts in the field of <phrase>software</phrase> agents which incorporate <phrase>emotion</phrase>. Several approaches have been made concerning <phrase>emotion</phrase> recognition, <phrase>emotion</phrase> modeling, generation of emotional <phrase>user interfaces</phrase> and dialogue systems as well as <phrase>anthropomorphic</phrase> <phrase>communication</phrase> agents. Motivations for emotional <phrase>computing</phrase> are <phrase>manifold</phrase>. From a scientific point of view, emotions <phrase>play</phrase> an essential role in <phrase>decision making</phrase>, as well as in <phrase>perception</phrase> and learning. Furthermore, emotions influence rational thinking and therefore should be part of rational agents as proposed by <phrase>artificial intelligence</phrase> <phrase>research</phrase>. Another focus is on <phrase>human</phrase>-computer interfaces which include believable animations of interface agents. Which commercially interesting applications would incorporate emotional aspects? One of the first interesting applications are dialogue systems which intend to generate <phrase>natural language</phrase> that sounds <phrase>human</phrase>-like on the one hand and which also and especially intend to react on emotional aspects of the utterances of the <phrase>human</phrase> partner adequately. On the other hand an increase of acceptance and quality is the intended result of the effort. Another field of application which is well known to <phrase>AI</phrase> is the field of computer <phrase>games</phrase>. After putting the <phrase>graphical</phrase> quality at first priority for years, the increasing interest in believable and intelligent <phrase>NPCs</phrase> (non <phrase>player characters</phrase>) can be recognized. A market volume of more than a billion <phrase>Euros</phrase> in <phrase>Germany</phrase> makes this application type even more interesting. A third application type often mentioned as an outstanding example is a tutoring system. The success of eLearning depends on the quality of learner and <phrase>teacher</phrase> interaction. Adapting the way of teaching to the learner and potentially to its emotional <phrase>state</phrase> is required to achieve good learning <phrase>results</phrase>. Since more and more teaching will be at least supported by <phrase>electronic</phrase> <phrase>tutoring systems</phrase> and eLearning <phrase>II</phrase> components this field will show increasing demand in techniques discussed in this workshop. Considering <phrase>emotion</phrase> measurement and classification, product <phrase>prototype</phrase> evaluation could benefit from <phrase>emotion</phrase> based <phrase>technology</phrase>. In <phrase>general</phrase> many applications which integrate a user <phrase>model</phrase> often leave <phrase>emotion</phrase> 
Prediction of Diaphragm Wall Deflection in Deep Excavations Using <phrase>Evolutionary</phrase> <phrase>Support Vector Machine</phrase> Inference <phrase>Model</phrase> (esim) Problems in deep excavations are full of uncertain, vague, and incomplete <phrase>information</phrase>. In most instances, successfully solving such problems depends on experts' <phrase>knowledge</phrase> and experience. The primary object of this <phrase>research</phrase> was to propose an " <phrase>Evolutionary</phrase> <phrase>Support Vector Machine</phrase> Inference <phrase>Model</phrase> (ESIM) " to predict wall deformation in deep excavation in <phrase>Taipei</phrase> <phrase>Basin</phrase>. ESIM is developed based on a <phrase>hybrid</phrase> approach that fuses <phrase>support vector machines</phrase> (<phrase>SVM</phrase>) and fast messy <phrase>genetic algorithm</phrase> (fmGA). <phrase>SVM</phrase> is primarily concerned with learning and <phrase>curve fitting</phrase>; and fmGA with optimization. Fifty-seven wall deformation monitoring <phrase>database</phrase> were collected based on monitoring <phrase>data</phrase> and compiled from prior projects. Fifty-two of 57 were selected for training, leaving 5 valid cases available for testing. <phrase>Results</phrase> show that ESIM can successfully predict the deflection and apply to contractors utilizes the <phrase>knowledge</phrase> and experience from past projects to predict wall deformation of new projects. Therefore the <phrase>construction</phrase> and foundation <phrase>construction</phrase> contractors can update wall deflection monitoring <phrase>data</phrase> of different stages during deep excavation process, in <phrase>order</phrase> to predict the wall deformation of the next stage and examine whether the max deflection is within the controlled <phrase>range</phrase>. The <phrase>results</phrase> are used as guidelines on site <phrase>safety</phrase> and <phrase>risk management</phrase>.
Dexter Kozen: An Appreciation I have known Dexter for 30 years, which, of course, means that I was 9 when I first met him. At that time, Dexter was already " the man ". Although he was only a few years ahead of me, he had already made his name by independently defining the notion of alternating <phrase>Turing machines</phrase>, a deep contribution to <phrase>complexity theory</phrase> that made it possible to connect time and space complexity. The <phrase>results</phrase> were viewed as so significant that it was already being taught in a graduate course on <phrase>complexity theory</phrase> that I attended. 1 Of even more interest to me at the time was Dexter's work on <phrase>modal logic</phrase>, since a large part of my <phrase>thesis</phrase> was on dynamic <phrase>logic</phrase>. Finding a <phrase>sound</phrase> and complete axiomatization for dynamic <phrase>logic</phrase> had been an <phrase>open problem</phrase> for many years. Krister Segerberg had suggested an obviously <phrase>sound</phrase> axiomatization, but couldn't prove it complete. Rohit Parikh [7] showed that it was indeed complete, but his proof was rather complicated. Dexter then came up with a much simpler proof, one that got at the essence of Rohit's ideas; this version was published as [6]. The proof is truly beautiful. I have taught it often, and used the ideas in a number of subsequent papers (e.g., [3, 4]). This was my first introduction to Dexter's ability to see into the <phrase>heart</phrase> of a problem. Perhaps the best-known example is Dexter's work on the -<phrase>calculus</phrase>. <phrase>Vaughan</phrase> Pratt [8] had earlier suggested a <phrase>logic</phrase> with <phrase>fixed-point</phrase> operators. Dex-ter's version [5] was more elegant, and is the one that everyone focuses on these days. I also soon learned about Dexter's breadth. Besides <phrase>complexity theory</phrase> and <phrase>modal logic</phrase>, he also <phrase>produced</phrase> <phrase>major</phrase> <phrase>results</phrase> on <phrase>algebra</phrase>, such as the complexity of the theory of real closed <phrase>algebraic</phrase> theories [1]. I remember visiting <phrase>IBM</phrase> <phrase>Yorktown Heights</phrase> in the mid 1980s; Dexter was working there at the time. I mentioned to him some <phrase>graph-theoretic</phrase> problems I was working on. The rest of the day, I could see Dexter <phrase>scratching</phrase> away at a <phrase>notepad</phrase>, thinking about the problems. (He is still <phrase>scratching</phrase> away at notepads, although the <phrase>notepad</phrase> and the problems occupying him are no doubt different.) When I moved to <phrase>Cornell</phrase> in 1996, I saw a different side of Dexter. Dexter is a true <phrase>department</phrase> stalwart. He is well known to be a tremendous <phrase>teacher</phrase>. He can also always be counted on 
Decentralized <phrase>Communication</phrase>-Aware <phrase>Motion Planning</phrase> in <phrase>Mobile</phrase> Networks: An <phrase>Information</phrase>-Gain Approach In this <phrase>paper</phrase> we consider decentralized <phrase>motion-planning</phrase> in <phrase>mobile</phrase> <phrase>cooperative</phrase> networks in the presence of realistic <phrase>stochastic</phrase> <phrase>communication</phrase> links, including <phrase>path-loss</phrase>, fading and shadowing effects. We propose a <phrase>communication</phrase>-aware <phrase>motion-planning</phrase> strategy, where each node considers the <phrase>information</phrase> gained through both its sensing and <phrase>communication</phrase> when deciding on its next move. More specifically, we show how each node can predict the <phrase>information</phrase> gained through its communications, by <phrase>online learning</phrase> of link quality measures such as received <phrase>Signal to Noise Ratio</phrase> (SNR) and correlation characteristics, and combine it with the <phrase>information</phrase> gained through its sensing in <phrase>order</phrase> to build objective functions for <phrase>motion planning</phrase>. We show that in the presence of <phrase>path loss</phrase>, our proposed strategy can improve the performance drastically. We furthermore show that while uncorrelated low-SNR fading channels can ruin the overall performance, the natural <phrase>randomization</phrase> of uncorrelated channels can potentially help the nodes leave deep fade spots with small movements. We finally show that highly correlated deep fades, on the other hand, can degrade the performance drastically for a <phrase>long</phrase> <phrase>period</phrase> of time. We then propose a randomizing <phrase>motion-planning</phrase> strategy that can help the nodes leave highly correlated deep fades.
P -norm <phrase>Multiple Kernel</phrase> Learning Making Learning with Multiple Kernels Effective P -norm <phrase>Multiple Kernel</phrase> Learning To my parents Acknowledgements I am deeply grateful for the opportunity to have worked with so many brilliant and creative minds over the last years. First of all, I would like to thank my <phrase>PhD</phrase> advisors Of course, I owe the utmost gratitude to <phrase>Professor</phrase> Mller, who initially introduced me to the subject of <phrase>machine learning</phrase> in 2007; with his infectious optimism, wit, and curiosity, he created the open and stimulating <phrase>atmosphere</phrase> that characterizes his <phrase>Machine Learning</phrase> <phrase>Laboratory</phrase> at TU <phrase>Berlin</phrase>. It was also <phrase>Professor</phrase> Mller who gave me advice and support in various respects from the very beginning of my <phrase>PhD</phrase> studies, thus turning out to be much more than just a scientific advisor. I am deeply thankful to him. By the same token, I would like to thank all members of his lab for creating such a pleasurable working <phrase>atmosphere</phrase> in the office each day, most notably my former and current office mates Furthermore, I would like to thank <phrase>Professor</phrase> Bartlett very much indeed for kindly inviting me over to visit his learning theory group at <phrase>UC Berkeley</phrase>, a most enjoyable and inspiring experience in many ways, and, most of all, for introducing me to the fascinating world of learning theory; his readiness to share his vast <phrase>knowledge</phrase> with me allowed me to gain a <phrase>deeper understanding</phrase> of the field. I also thank all members of his group, especially Alekh, for our stimulating discussions, not limited to various <phrase>mathematical</phrase> subjects, but also in matters of the everyday <phrase>life</phrase>. I also thank all office mates in Sutardja Dai Hall for the <phrase>nice</phrase> <phrase>atmosphere</phrase>, and the <phrase>UC Berkeley</phrase> for providing such a great office with an absolutely stunning view of the SF <phrase>Bay</phrase>, all of which contributed to making my stay (Oct 2009Sep 2010) such a memorable one. Likewise, I am very grateful to <phrase>Professor</phrase> Blanchard for taking so much of his valuable time for our extensive and fruitful discussions on various <phrase>mathematical</phrase> problems, sharing his immense <phrase>knowledge</phrase> and deep insights with me. I remember us sitting in cafs for hours, trying to solve a <phrase>mathematical</phrase> <phrase>puzzle</phrase> arising from our joint <phrase>research</phrase> III projects. It is rare that a <phrase>professor</phrase> invests so much of his time in mentoring a <phrase>student</phrase> and I owe him a big thanks! My special thanks go to Dr. Ulf Brefeld for his caring mentoring and patient teaching in the initial phase of my <phrase>PhD</phrase>, which 
Learning to Interpret Utterances Using Dialogue <phrase>History</phrase> We describe a methodology for learning a disambiguation <phrase>model</phrase> for deep pragmatic interpretations in the context of situated task-oriented dialogue. The system accumulates <phrase>training examples</phrase> for ambiguity resolution by tracking the fates of <phrase>alternative</phrase> interpretations across dialogue, including subsequent clarificatory episodes initiated by the system itself. We illustrate with a <phrase>case study</phrase> building maximum <phrase>entropy</phrase> models over abductive interpretations in a referential <phrase>communication</phrase> task. The resulting <phrase>model</phrase> correctly resolves 81% of ambiguities left unresolved by an initial handcrafted baseline. A key <phrase>innovation</phrase> is that our method draws exclusively on a system's own skills and experience and requires no <phrase>human</phrase> annotation.
Pre-workshop summary: workshop on iterative, adaptive, and agile processes Broadly, the goals of this workshop are to promote a deep and shared understanding of various iterative, adaptive, and agile processes, including <phrase>Crystal</phrase>, DSDM, <phrase>Extreme Programming</phrase> (<phrase>XP</phrase>), Serum, and the Unified Process (UP) to identify the factors for their successful <phrase>adoption</phrase> to share relevant <phrase>research</phrase> <phrase>results</phrase> discover lines of <phrase>research</phrase> to further this discipline OVERVIEW We suspect very few methodologists created a process with the explicit intent, "Oh boy, this will be heavy and rigid!" And yet it's a common complaint. Why is that? It influences the <phrase>motivation</phrase> for this new generation of process ideas. Will the same forces that have <phrase>led</phrase> to "unhealthy" application of prior generations of development processes also inhibit the new ones? For example, cases are now emerging of organizations just doing occasional <phrase>unit-testing</phrase> or avoiding documentation, and calling it "<phrase>XP</phrase>." What are the skillful means and capabilities required to really help these new process ideas be adopted and succeed? What's working? What isn't? And what are the hard-<phrase>data</phrase> justifications? In this first year of the workshop, there are three <phrase>major</phrase> goals: To provide a supportive and fun forum for interested stakeholders to share their passion, <phrase>research</phrase>, stories, and ideas. Compare and contrast (via a summary of key details, a <phrase>case study</phrase>, and discussion) related processes, ideally including Identify cases, best practices, <phrase>human factors</phrase>, and impediments to successful <phrase>adoption</phrase> of these process as a whole (such as <phrase>XP</phrase>), or specific practices (such as <phrase>daily</phrase> serum or <phrase>pair programming</phrase>). In this workshop, we invite practitioners and researchers interested in the application of iterative, adaptive, and agile processes. And we expect to learn from each other. We include those favoring detailed or <phrase>large-scale</phrase> process descriptions (such as the Rational UP) to those favoring minimalist descriptions (such as Serum), from process <phrase>evangelists</phrase> to process agnostics, from expert developers to <phrase>business</phrase> managers, from domains where speed is the top priority to domains where quality or verifiability are the top priority. Permission to make <phrase>digital</phrase> or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or <phrase>commercial advantage and that copies bear</phrase> this notice and the full citation on the first page. To copy otherwise, or republish, to <phrase>post on servers</phrase> or to redistribute to fists, <phrase>requires prior specific permission</phrase> and/or a fee.
Deep in the <phrase>HEART</phrase> of <phrase>texas</phrase>: celebrating 10 years of <phrase>technology</phrase> discoveries at the regional conference level At a SIGUCCS conference ten <phrase>years ago</phrase>, a small group of individuals from <phrase>Baylor University</phrase>, <phrase>Texas</phrase> A&#38;M <phrase>University</phrase> (<phrase>TAMU</phrase>) and <phrase>University</phrase> of <phrase>Houston</phrase> -- Clear <phrase>Lake</phrase> (UHCL) to discuss creating a biannual regional conference. We were looking for a way within our <phrase>state</phrase> for our colleagues who could not attend the SIGUCCS annual conference to gain the benefits of the <phrase>collegiality</phrase>, networking, sharing of innovations and collaborative <phrase>problem solving</phrase> often experienced by attending the annual conference. <phrase>Higher Education</phrase> And Resource Technologies (<phrase>HEART</phrase>) of <phrase>Texas</phrase> was born.. From our experience learn how you too can create a regional meeting in your <phrase>area</phrase> that is <phrase>low cost</phrase> with many great rewards. We will discuss how we got started and our process for planning and executing our meetings. Learn from our successes and mistakes so you can get started on your own
Biologically Inspired KFLANN Place Fields for <phrase>Robot</phrase> Localization This <phrase>paper</phrase> presents a <phrase>hippocampal</phrase> inspired <phrase>robot</phrase> localization <phrase>model</phrase> that provides a means for a simple robotic platform with <phrase>ultrasonic</phrase> sensors to localize itself. There have been published <phrase>neurobiological</phrase> experiments where rats were found to have <phrase>hippocampal</phrase> <phrase>cell</phrase> activations that positively correlate with the location of the <phrase>animal</phrase> [2, 3, 5]. Such activations found in the hippocamal <phrase>region</phrase> are usually called Place fields (PF) or Place cells (<phrase>PC</phrase>). The Place Field <phrase>model</phrase> presented in this <phrase>paper</phrase> was designed using a unique K-Means Fast Learning <phrase>Artificial Neural Network</phrase> (KFLANN) [13, 14, 15] and establishes a series of localization minima points that <phrase>act</phrase> as references for <phrase>navigation</phrase>. While such evidence of place cells are seen in <phrase>hippocampal</phrase> (CA1) and deep layers of the <phrase>entorhinal cortex</phrase> (EC) [4], from a <phrase>literature</phrase> search, it is uncertain if any applications were ever designed using this biological evidence. The intent of this <phrase>paper</phrase> is to focus on <phrase>experimental</phrase> <phrase>results</phrase> relevant for a <phrase>proof-of-concept</phrase> of <phrase>robot</phrase> localization, rather than illustrating a robustly tested <phrase>navigation</phrase> system. As such, <phrase>basic</phrase> <phrase>ultrasonic</phrase> based experiments will suffice. With some <phrase>experimental</phrase> <phrase>results</phrase>, we show that the KFLANN is suitable for implementing atomic place field vectors (APFV), a <phrase>data structure</phrase> to encapsulate localization <phrase>information</phrase>. Autonomous <phrase>unmanned</phrase> navigational systems have been dependent on localization sensors as a primary means for <phrase>navigation</phrase>. In outdoor autonomous systems such as those seen in the <phrase>DARPA Grand Challenge</phrase>, most vehicles would make use of combinations of <phrase>GPS</phrase>, <phrase>RADAR</phrase>, <phrase>LIDAR</phrase> and INS [6]. There has also been an increasing amount of <phrase>research</phrase> in using only vision to navigate autonomous platforms through spaces such as laboratories and corridors [5]. While these have provided <phrase>a level</phrase> of success, the extent of <phrase>cognitive</phrase> robotic <phrase>navigation</phrase> is far from being solved as a <phrase>single</phrase> failure in localization sensors usually leads to a catastrophic system failure. In the midst of man's quest to create robust autonomous robotic systems that can navigate effortlessly through clutter and uncertain environments, <phrase>laboratory</phrase> mice used in experiments are revealing secrets of their navigational capabilities. Perhaps one of the differences in <phrase>mammalian</phrase> <phrase>navigation</phrase> as compared with robotic <phrase>navigation</phrase> is the dependence in <phrase>cognitive</phrase> capabilities as opposed to the heavy reliance on <phrase>sensor</phrase> <phrase>technology</phrase>. Technological advances need to be complemented with advances in aspects of <phrase>cognitive</phrase> processings. We present a method where localization of a <phrase>robot</phrase> within a structured space is done using <phrase>ultrasound</phrase> <phrase>information</phrase> and a <phrase>compass</phrase> direction. The 
Learning with Models and Learning by Modelling: Exploring the Role of <phrase>Multiple Representations</phrase> Using Computational <phrase>Media</phrase> In this workshop, I will describe some of our ongoing <phrase>research</phrase> activities related to the use of modelling tools, <phrase>construction</phrase> kits and system dynamics simulations to support learning in different educational settings. These activities are related to innovative technological development and educational practice in a number of domains where deep scientific thinking is required. Interactive tools for modelling and <phrase>simulation</phrase> are gaining increasing importance as means to explore, comprehend, learn and communicate complex ideas [1]. In a <phrase>variety</phrase> of learning contexts, ranging from exploratory to task-orientated environments, students benefit from building and using models thus developing both, procedural skills and conceptual competencies [2]. A <phrase>dimension</phrase> of particular interest in the educational use of computer simulations and models is whether and when one learns by building simulations or by using existing simulations [3]. This <phrase>general</phrase> commitment allows for both learning with models and learning by modelling. A promising methodology with an associated <phrase>technology</phrase> to support learning with models and by modelling is system dynamics. System dynamics modelling tools such as Stella and Powersim enable users to experiment with <phrase>complex systems</phrase> and develop better intuitions about the mechanisms that govern dynamic interactions. In accordance with <phrase>recent advances</phrase> in <phrase>research</phrase> on learning and instruction, there are attempts to provide increasingly meaningful <phrase>learning experiences</phrase>. In complex domains such experiences include the ability to construct models in addition to using models for experimentation. Recently, Milrad et al., [4] have suggested an approach called " <phrase>Model</phrase> Facilitated Learning " (MFL) in combination with <phrase>instructional design</phrase> principles. Key aspects of this <phrase>design</phrase> framework include the use of modelling tools, <phrase>construction</phrase> kits and system dynamics simulations to provide <phrase>multiple representations</phrase> to help students in developing an understanding of problems in situations that comprise many interrelated components which are subject to change over time and often involve ill-defined aspects. MFL distinguishes learning by modelling from learning with models and suggests when and why each approach is most likely to be appropriate. In addition, MFL emphasis the notion of socially situated <phrase>learning experiences</phrase> threads throughout elaborated learning sequences. Here, the notion of social situatedness extends to the idea of collaborative modelling. According to the MFL approach, learning with models is generally well suited for the earlier learning stages that often involve simple procedural tasks and simpler conceptual foundations, whereas learning by modelling is generally better suited for more advanced stages of development targeted at causal understanding and mastery of complex 
Gain Control and Linearity Improvement for Low Noise <phrase>Amplifiers</phrase> in 5ghz Direct Conversion <phrase>Receivers</phrase> The members of the Committee appointed to examine the <phrase>thesis</phrase> of MALLESH RAJASHEKHARAIAH find it satisfactory and recommend that it be accepted. <phrase>_____</phrase><phrase>_____</phrase><phrase>_____</phrase><phrase>_____</phrase><phrase>_____</phrase><phrase>______</phrase>____ Chair <phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>_____ _</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase><phrase>______</phrase>____ iii ACKNOWLEDGMENT This project was funded by the <phrase>NSF</phrase> <phrase>Center</phrase> for <phrase>Design</phrase> of Analog-<phrase>Digital</phrase> <phrase>Integrated Circuits</phrase> (www.eecs.wsu.edu/~cdadic) and I would like to express my heartfelt gratitude to CDADIC for the same. I would like to acknowledge the support and guidance received from my advisor Dr Heo, who made all this possible. Thanks are due also to my team <phrase>member</phrase> and friend, Parag Upadhyaya for all the learning and continued support which has thoroughly helped me in my program. I would also like to <phrase>express my deep</phrase> sense of gratitude to Prof. George LaRue for all the support he has provided in several ways, be it some problem in the testing lab, <phrase>software</phrase> tools or the regular dose of laughter the year round! Thanks to all my <phrase>thesis</phrase> committee members, Prof. Also, thanks to Jeff Dykstra of <phrase>Motorola</phrase> and Matt Miller of <phrase>Freescale Semiconductor</phrase> for their valuable suggestions and guidance during the course of the project. Special mention about Alaina McCully and Joanne Buteau for their administrative support. Thanks to Mark Fuller also, for support with the measurements.
<phrase>Complex-Valued</phrase> <phrase>Neural Networks</phrase>: Theories The reviewed <phrase>book</phrase> is devoted to the <phrase>neural networks</phrase> that are based on the <phrase>neurons</phrase> with the <phrase>complex-valued</phrase> weights and <phrase>complex-valued</phrase> <phrase>activation functions</phrase>. In recent years, these <phrase>neural networks</phrase> have become more and more popular. A number of the original solutions in <phrase>pattern recognition</phrase> and classification, in <phrase>artificial</phrase> neural <phrase>information processing</phrase> , in <phrase>image processing</phrase> and in the theory of <phrase>artificial</phrase> <phrase>neurons</phrase> and <phrase>neural networks</phrase> that are based on the use of <phrase>complex-valued</phrase> <phrase>neurons</phrase> have been proposed. It is very important that the use of <phrase>complex-valued</phrase> weights and <phrase>complex-valued</phrase> <phrase>activation functions</phrase> is not a simply theoretical generalization of the <phrase>real-valued</phrase> case, but it makes possible to extend the functionality both of a <phrase>single</phrase> <phrase>neuron</phrase> and a network, to obtain much more stable <phrase>learning algorithms</phrase> and finally to solve many complicated applied problems that either cannot be solved using the <phrase>real-valued</phrase> <phrase>neural networks</phrase> or to solve them in a simpler way and more efficiently. The reviewed <phrase>book</phrase> presents the latest <phrase>results</phrase> in the theory of <phrase>complex valued</phrase> <phrase>neural networks</phrase> and their applications obtained by a wide group of the authors representing a number of leading <phrase>research</phrase> centers. in Chapter 1, the <phrase>book</phrase> <phrase>editor</phrase> A. Hi-<phrase>rose</phrase> provide a reader with a brief description and a brief observation of the latest trends the field. A significant part of the <phrase>book</phrase> (3 chapters from 15) is devoted to the development and different applications of the pioneering idea of Naum Aizenberg. This idea is based on the deep generalization of the principles of the <phrase>Boolean</phrase> threshold <phrase>logic</phrase> to the multiple-valued case (see [1][3]). A key point in this approach is that the neuron's k-valued inputs and output are coded by the complex numbers that are the <phrase>kth</phrase> roots of unity lying on the <phrase>unit circle</phrase>. The activation <phrase>function</phrase> of a <phrase>neuron</phrase> that was initially proposed in [1] is in this case a <phrase>function</phrase> of the argument of the weighted sum: The complex plain is separated onto k equal sectors, and if the weighted sum belongs to the jth sector, the neuron's output is equal to " j , where " = exp(<phrase>i2</phrase>=k) is a primitive <phrase>kth</phrase> <phrase>root</phrase> of unity. This <phrase>neuron</phrase> was later called a multivalued <phrase>neuron</phrase> (MVN) [3]. In Chapter 3, D.L. Lee considers an <phrase>associative memory</phrase> <phrase>model</phrase> based on MVN. A <phrase>neural network</phrase> that is proposed to be used as an <phrase>asso</phrase>-ciative <phrase>memory</phrase> is able to store and recall gray-scale images. The stability 
A <phrase>Personalization</phrase> Recommendation Method Based on <phrase>Deep Web</phrase> <phrase>Data</phrase> Query <phrase>Deep Web</phrase> is becoming a hot <phrase>research</phrase> topic in the <phrase>area</phrase> of <phrase>database</phrase>. Most of the existing researches mainly focus on <phrase>Deep Web</phrase> <phrase>data integration</phrase> <phrase>technology</phrase>. <phrase>Deep Web</phrase> <phrase>data integration</phrase> can partly satisfy people's needs of <phrase>Deep Web</phrase> <phrase>information</phrase> search, but it cannot learn users' interest, and people search the same content online repeatedly would cause much unnecessary waste. According to this kind of demand, this <phrase>paper</phrase> introduced <phrase>personalization</phrase> recommendation to the <phrase>Deep Web</phrase> <phrase>data</phrase> query, proposed a user interest <phrase>model</phrase> based on <phrase>fine-grained</phrase> <phrase>management</phrase> of structured <phrase>data</phrase> and a similarity matching <phrase>algorithm</phrase> based on attribute <phrase>eigenvector</phrase> in allusion to <phrase>personalization</phrase> recommendation. Secondly, As for <phrase>Deep Web</phrase> <phrase>information</phrase> crawl, a crawl <phrase>technology</phrase> based on the <phrase>tree structure</phrase> is presented, with the traversal method of <phrase>tree</phrase> to solve the <phrase>information</phrase> crawl problems in the <phrase>personalization</phrase> service distributed in various web <phrase>databases</phrase>. Finally, developed a <phrase>prototype</phrase> recommendation system based on recruitment <phrase>information</phrase>, verified the efficiency and effectiveness of the <phrase>personalization</phrase> recommendation and the coverage and cost of <phrase>Deep Web</phrase> crawl through the experiment.
Flexible, <phrase>High</phrase> Performance <phrase>Convolutional Neural Networks</phrase> for <phrase>Image Classification</phrase> We present a fast, fully parameterizable <phrase>GPU</phrase> implementation of <phrase>Convolutional Neural Network</phrase> variants. Our feature extractors are neither carefully designed nor pre-<phrase>wired</phrase>, but rather learned in a supervised way. Our deep hierarchical architec-tures achieve the best <phrase>published results</phrase> on benchmarks for object classification (NORB, CIFAR10) and <phrase>handwritten digit</phrase> recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. <phrase>Test</phrase> error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.
<phrase>Eeg</phrase> Based <phrase>Emotion</phrase> Identification Using Unsupervised Deep <phrase>Feature Learning</phrase> Conference Item <phrase>Eeg</phrase> Based <phrase>Emotion</phrase> Identification Using Unsupervised Deep <phrase>Feature Learning</phrase> <phrase>Copyright</phrase> and <phrase>Moral Rights</phrase> for the articles on this site are retained by the individual authors and/or other <phrase>copyright</phrase> owners. For more <phrase>information</phrase> on Open <phrase>Research</phrase> Online's <phrase>data</phrase> policy on reuse of materials please consult the policies page. ABSTRACT Capturing user's emotional <phrase>state</phrase> is an emerging way for implicit relevance <phrase>feedback</phrase> in <phrase>information retrieval</phrase> (IR). Recently, <phrase>EEG</phrase>-based <phrase>emotion</phrase> recognition has drawn increasing attention. However, a key challenge is effective learning of useful features from <phrase>EEG</phrase> signals. In this <phrase>paper</phrase>, we present our ongoing work on using <phrase>Deep Belief</phrase> Network (DBN) to automatically extract <phrase>high-level</phrase> features from raw <phrase>EEG</phrase> signals. Our preliminary experiment on the DEAP dataset shows that the <phrase>learned features</phrase> perform comparably to the use of manually generated features for <phrase>emotion</phrase> recognition.
A Fuzzy Extension for the <phrase>XPath</phrase> <phrase>Query Language</phrase> <phrase>XML</phrase> has become a widespread format for <phrase>data</phrase> exchange over the In-ternet. The <phrase>current state</phrase> of the <phrase>art</phrase> in querying <phrase>XML</phrase> <phrase>data</phrase> is represented by <phrase>XPath</phrase> and <phrase>XQuery</phrase>, both of which define <phrase>binary</phrase> predicates. In this <phrase>paper</phrase>, we <phrase>advocate</phrase> that <phrase>binary</phrase> selection can at times be restrictive due to very <phrase>nature</phrase> of <phrase>XML</phrase>, and to the uses that are made of it. We therefore suggest a querying framework, called FXPath, based on fuzzy logics. In particular, we propose the use of fuzzy predicates for the definition of more " vague " and softer queries. We also introduce a <phrase>function</phrase> called " deep-similar " , which aims at substituting XPath's typical " deep-equal " <phrase>function</phrase>. Its goal is to provide a <phrase>degree</phrase> of similarity between two <phrase>XML</phrase> <phrase>trees</phrase>, assessing whether they are similar both structure-wise and content-wise. The approach is exemplified in the field of <phrase>e</phrase>-learning <phrase>metadata</phrase>.
An Abstract Deep Network for <phrase>Image Classification</phrase> In <phrase>order</phrase> to allow more flexible and <phrase>general</phrase> learning, it is an advantage for <phrase>artificial</phrase> systems to be able to discover re-usable features that capture structure in the environment, known as <phrase>Deep Learning</phrase>. Techniques have been shown based on <phrase>convolutional neural networks</phrase> and stacked <phrase>Restricted Boltzmann Machines</phrase>, which are related to some <phrase>degree</phrase> with neural processes. An <phrase>alternative</phrase> approach using abstract representations, the ARCS Learning Classifier System, has been shown to build feature hierarchies based on reinforcement, providing a different perspective, however with limited classification performance compared to <phrase>Artificial Neural Network</phrase> systems. An Abstract Deep Network is presented that is based on ARCS for building the feature network, and introduces <phrase>gradient descent</phrase> to allow improved <phrase>results</phrase> on an <phrase>image classification</phrase> task. A number of implementations are examined, comparing the use of back-propagation at various depths of the system. The ADN system is able to produce classification error of 1.18% on the MNIST dataset, comparable with the most established <phrase>general</phrase> learning systems on this task. The system shows strong reliability in constructing features, and the abstract representation provides a good platform for studying further effects such as as top-down influences.
Tuning <phrase>Recurrent Neural Networks</phrase> with <phrase>Reinforcement Learning</phrase> <phrase>Sequence</phrase> models can be trained using <phrase>supervised learning</phrase> and a next-step prediction objective. This approach, however, suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. Motivated by the fact that <phrase>reinforcement learning</phrase> (<phrase>RL</phrase>) can be used to impose arbitrary properties on generated <phrase>data</phrase> by choosing appropriate reward functions, in this <phrase>paper</phrase> we propose a novel approach for <phrase>sequence</phrase> training which combines <phrase>Maximum Likelihood</phrase> (ML) and <phrase>RL</phrase> training. We refine a <phrase>sequence</phrase> predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from <phrase>data</phrase>. We propose efficient ways to solve this by augmenting <phrase>deep Q</phrase>-learning with a cross-<phrase>entropy</phrase> reward and deriving novel off-policy methods for RNNs from <phrase>stochastic</phrase> <phrase>optimal control</phrase> (SOC). We explore the usefulness of our approach in the context of <phrase>music</phrase> generation. An LSTM is trained on a large corpus of songs to predict the next note in a <phrase>musical</phrase> <phrase>sequence</phrase>. This Note-RNN is then refined using <phrase>RL</phrase>, where the <phrase>reward function</phrase> is a combination of rewards based on rules of <phrase>music theory</phrase>, as well as the output of another trained Note-RNN. We show that by combining ML and <phrase>RL</phrase>, this <phrase>RL</phrase> Tuner method can not only produce more pleasing melodies, but that it can significantly reduce unwanted behaviors and failure modes of the RNN.
Solving the <phrase>Scalability</phrase> Dilemma with Clouds, Crowds, and <phrase>Algorithms</phrase> The creation, analysis, and dissemination of <phrase>data</phrase> have become profoundly democratized. <phrase>Social networks</phrase> spanning 100/per day of millions of users enable instantaneous discussion, debate, and <phrase>information</phrase> sharing. Streams of <phrase>tweets</phrase>, <phrase>blogs</phrase>, photos, and videos identify breaking events faster and in more detail than ever before. Deep, on-line datasets enable analysis of previously unreachable <phrase>information</phrase>. This sea change is the result of a <phrase>confluence</phrase> of <phrase>Information Technology</phrase> advances such as: intensively networked systems, <phrase>cloud computing</phrase>, <phrase>social computing</phrase> , and pervasive devices and <phrase>communication</phrase>. The key challenge is that the massive scale and diversity of this continuous <phrase>flood</phrase> of <phrase>information</phrase> breaks our existing technologies. <phrase>State</phrase>-of-the-<phrase>art</phrase> <phrase>Machine Learning</phrase> <phrase>algorithms</phrase> do not scale to massive <phrase>data</phrase> sets. Existing <phrase>data</phrase> analytics frameworks cope poorly with incomplete and dirty <phrase>data</phrase> and cannot process heterogeneous multi-format <phrase>information</phrase>. Current <phrase>large-scale</phrase> processing architectures struggle with diversity of <phrase>programming</phrase> models and job types and do not support the rapid marshalling and unmarshalling of resources to solve specific problems. All of these limitations <phrase>lead</phrase> to a <phrase>Scalability</phrase> Dilemma: beyond a point, our current systems tend to perform worse as they are given more <phrase>data</phrase>, more processing resources, and involve more people exactly the opposite of what should happen. The <phrase>Berkeley</phrase> RADLab is a collaborative effort focused on <phrase>cloud computing</phrase> , involving nearly a dozen <phrase>faculty members</phrase> and postdocs, several dozen students and fifteen <phrase>industrial</phrase> sponsors. The lab is in the final year of a five-year effort to develop the <phrase>software</phrase> <phrase>infrastructure</phrase> to enable rapid deployment of robust, scalable, <phrase>data</phrase>-intensive <phrase>internet services</phrase>. In this <phrase>talk</phrase> I will give an overview of the RADLab effort and do a deeper dive on several projects, including: PIQL, a performance insightful <phrase>query language</phrase> for interactive applications, and SCADS, a self-managing, scalable key value store. I will also give an overview of a new effort we are starting on next generation <phrase>cloud computing</phrase> architectures (called the " AM-PLab "-for <phrase>Algorithms</phrase>, Machines, and People) focused on <phrase>large-scale</phrase> <phrase>data</phrase> analytics, <phrase>machine learning</phrase>, and <phrase>hybrid</phrase> <phrase>cloud</phrase>/crowd <phrase>computing</phrase>. In a nutshell, the RADLab approach has been to use <phrase>Statistical Machine Learning</phrase> in the service of building <phrase>large-scale</phrase> systems. The AMPLab is exploring the other side of this relationship, namely, using <phrase>large-scale</phrase> systems to support <phrase>Statistical Machine Learning</phrase> and other analysis techniques for <phrase>data</phrase>-intensive applications. And given the central role of the <phrase>cloud</phrase> in a world of pervasive connectivity, a key part of the <phrase>research</phrase> agenda is to support collaborative efforts of 
Establishing Correspondences between Attribute Spaces and Complex Concept Spaces Using Meta-pgn Classifier In this <phrase>paper</phrase>, we present one approach for extending the learning set of a classification <phrase>algorithm</phrase> with additional <phrase>metadata</phrase>. It is used as a base for giving appropriate names to found regularities. The analysis of correspondence between connections established in the attribute space and existing links between concepts can be used as a <phrase>test</phrase> for creation of an adequate <phrase>model</phrase> of the observed world. Meta-PGN classifier is suggested as a possible tool for establishing these connections. Applying this approach in the field of <phrase>content-based image retrieval</phrase> of <phrase>art</phrase> paintings provides a tool for extracting specific feature combinations, which represent different sides of artists' styles, periods and movements. 1 Introduction The problem of resolving the gaps between computer analysis and <phrase>human</phrase> understanding has a deep <phrase>philosophical</phrase> background even in the problem of understanding between humans. <phrase>Lewis Carroll</phrase> in "Through the Looking-<phrase>Glass</phrase>" gives us an example of absurd use of <phrase>semantics</phrase> and <phrase>pragmatics</phrase> when <phrase>Humpty Dumpty</phrase> talks with Alice: "When I use a word it means just what I choose it to mean neither more nor less". At the base of <phrase>semantics</phrase> lies the definition of concepts as names and corresponding content. Our <phrase>life</phrase> is full with learning concepts (their names and contents) in <phrase>order</phrase> to understand each other. There exist many well-suited theories in the <phrase>area</phrase> of concept formation based on the attribute models. Very close to this understanding is <phrase>formal concept analysis</phrase> [13], that uses the <phrase>philosophical</phrase> view of a concept as a unit consist
<phrase>EEG</phrase>-based <phrase>emotion</phrase> classification using <phrase>deep belief</phrase> networks In recent years, there are many great successes in using <phrase>deep architectures</phrase> for <phrase>unsupervised feature learning</phrase> from <phrase>data</phrase>, especially for images and speech. In this <phrase>paper</phrase>, we introduce recent advanced <phrase>deep learning</phrase> models to classify two emotional categories (positive and negative) from <phrase>EEG</phrase> <phrase>data</phrase>. We <phrase>train</phrase> a <phrase>deep belief</phrase> network (DBN) with <phrase>differential entropy</phrase> features extracted from multichannel <phrase>EEG</phrase> as input. A <phrase>hidden markov model</phrase> (HMM) is integrated to accurately capture a more reliable emotional stage switching. We also compare the performance of the deep models to KNN, <phrase>SVM</phrase> and <phrase>Graph</phrase> reg-ularized <phrase>Extreme Learning</phrase> Machine (GELM). Our <phrase>experimental</phrase> <phrase>results</phrase> show that the DBN and DBN-HMM models improve the accuracy of <phrase>EEG</phrase>-based <phrase>emotion</phrase> classification in comparison with the <phrase>state</phrase>-of-the-<phrase>art</phrase> methods.
<phrase>General</phrase> and specific formalization approach for a <phrase>Balanced Scorecard</phrase>: An expert system with application in <phrase>health care</phrase> The <phrase>Balanced Scorecard</phrase> (<phrase>BSC</phrase>) presents the essentials of strategic and performance <phrase>management</phrase> in clear, straightforward manner which is also usable in <phrase>health care</phrase>. If a <phrase>BSC</phrase> for a clinical <phrase>department</phrase> is an agreement , the first question to consider is the method by which it can be ascertained whether a strategy has been accomplished. There are many different techniques like AHP (analytic hierarchy process) and fuzzy systems to calculate indices. However, how does a formalized <phrase>mathematical</phrase> groundwork looks like that integrates current approaches and is still <phrase>general</phrase> enough to incorporate future <phrase>expert systems</phrase> with applications? The purpose of this <phrase>paper</phrase> is the formalization of <phrase>BSC</phrase> evaluation by respecting <phrase>current research</phrase>. The formalized expert system was implemented in an <phrase>information</phrase> system for <phrase>health care</phrase> <phrase>management</phrase>. Any <phrase>hospital</phrase> is confronted by substantial <phrase>society</phrase> changes due to a <phrase>variety</phrase> of factors such as increasing <phrase>life expectancy</phrase> and <phrase>population ageing</phrase>, economical <phrase>pressure</phrase> and competition, limited resources as well as shrinking and tight budgets, new governmental deregulations and <phrase>liberalization</phrase>. In <phrase>order</phrase> to cope with the changing <phrase>nature</phrase> of this environment, which is also aggravated by deep structural reorganization, it is important to accomplish success goals with a tool of <phrase>strategic management</phrase> such as a <phrase>Balanced Scorecard</phrase>. In <phrase>order</phrase> to support strategy and performance <phrase>management</phrase>, Robert Kaplan and <phrase>David</phrase> Norton's ''<phrase>Balanced Scorecard</phrase> " concept (Kaplan, 1992) may be used. It is a tool of <phrase>strategic management</phrase>, strategic <phrase>communication</phrase> and performance <phrase>management</phrase>, providing frequently measured performance and regular reviewing and refinement strategy with an ongoing evaluation process of clinical indicators. A <phrase>BSC</phrase> <phrase>design</phrase> and implementation process can be separated into four stages: (1) translating the vision and gaining consensus ; (2) communicating the objectives, setting the goals, and linking strategies; (3) setting targets, allocating resources, and establishing milestones; (4) and <phrase>feedback</phrase> and learning (Stewart & Bestor, 2000). Types of performance indicators and factors are termed as ''perspectives ". These perspectives as in the original definition are differentiated into a financial, a customer, and a process and <phrase>innovation</phrase> perspective. The simply monitoring of key financial indicators, which have often been historical in <phrase>nature</phrase> and concentrated almost exclusively on lagging indicators are hiding the key drivers. Examples of Balanced Scorecards in <phrase>health care</phrase> can be found for These articles give a brief overview about BSCs and their development and application in <phrase>healthcare</phrase> <phrase>management</phrase>. <phrase>Strategic management</phrase> is an externally oriented <phrase>philosophy</phrase> of managing an <phrase>organization</phrase> that links strategic thinking and 
An Adaptive <phrase>Signal-Processing</phrase> Approach to Online Adaptive Tutoring Conventional intelligent or adaptive tutoring online systems rely on <phrase>domain-specific</phrase> models of learner behavior based on rules, deep <phrase>domain knowledge</phrase>, and other resource-intensive methods. We have developed and studied a <phrase>domain-independent</phrase> methodology of adaptive tutoring based on <phrase>domain-independent</phrase> <phrase>signal-processing</phrase> approaches that obviate the need for the <phrase>construction</phrase> of explicit expert and <phrase>student</phrase> models. A key advantage of our method over conventional approaches is a <phrase>lower</phrase> barrier to entry for educators who want to develop adaptive <phrase>online learning</phrase> materials.
<phrase>Action Research</phrase> Implemented to Improve <phrase>Zoology</phrase> <phrase>Laboratory</phrase> Activities in a <phrase>Freshman</phrase> <phrase>Biology</phrase> <phrase>Majors</phrase> Course Introduction Traditional <phrase>zoology</phrase> <phrase>laboratory</phrase> activities include observation of slides, observations of living and preserved organisms, and dissections of preserved animals. While these activities are valuable, they have not consistently included higher levels of intellectual challenge. For the <phrase>zoology</phrase> <phrase>laboratory</phrase> course studied, learning was typically evaluated by performance on written <phrase>laboratory</phrase> reports and on an end of the semester <phrase>laboratory</phrase> practical exam, where students essentially had to regurgitate rather than use and synthesize <phrase>information</phrase>. However, Bloom's <phrase>taxonomy</phrase> suggests that the development of <phrase>cognitive</phrase> ability is hierarchical, progressing from simple understanding to application and synthesis of that <phrase>knowledge</phrase>, and that performance tasks undertaken by students should reflect the <phrase>range</phrase> of <phrase>cognitive</phrase> skills (Reed & Bergemann, 2001). This <phrase>led</phrase> us to reconsider how the <phrase>zoology</phrase> <phrase>laboratory</phrase> section was taught, using an <phrase>action research</phrase> approach. Effective teaching involves enabling students to develop a <phrase>deep understanding</phrase> of the materials they are studying. This can be achieved through a <phrase>variety</phrase> of thought-demanding tasks (Levin & Nolan, 2000), including having <phrase>student</phrase> explain concepts in their own words, making predictions, doing drawings, finding exemplars in new contexts and applying concepts to new situations (Brandt, 1992). Zuber-Skerritt (1992a) concurred, stating So far we have arrived at the position that the most appropriate mode of learning and teaching in <phrase>higher education</phrase> is that of the <phrase>alternative</phrase> <phrase>paradigm</phrase> which may be characteri zed by learner-centered, problem-oriented, interdisciplinary, process-centered, and using an open, critical approach. (p. 147)
What do exam <phrase>results</phrase> really measure? Students are evaluated using examinations, but how do we evaluate whether the examination is correctly measuring the students' <phrase>knowledge</phrase> or skill? This <phrase>paper</phrase> presents a methodology which we have used in an experiment: students' exam <phrase>results</phrase> were analysed to reveal which different <phrase>cognitive</phrase> skills were used in answering different questions. The analysis revealed that students were approaching several questions in ways that the instructor had not anticipated. Sometimes questions the instructor considered straightforward actually tested students' <phrase>conceptual understanding</phrase>; on other questions which were intended to require <phrase>problem-solving</phrase>, many students never identified the concepts involved, so that grades measured primarily the ability to avoid distraction. In other cases, we demonstrated that questions did assess <phrase>deep understanding</phrase> of the fundamental concepts, rather than <phrase>rote-learning</phrase> or simple <phrase>pattern-matching</phrase>.
Trace: <phrase>Tennessee</phrase> <phrase>Research</phrase> and Creative Exchange an Analog <phrase>Vlsi</phrase> Deep <phrase>Machine Learning</phrase> Implementation an Analog <phrase>Vlsi</phrase> Deep <phrase>Machine Learning</phrase> Implementation I am submitting herewith a dissertation written by Junjie Lu entitled "An Analog <phrase>VLSI</phrase> Deep <phrase>Machine Learning</phrase> Implementation." I have examined the final <phrase>electronic</phrase> copy of this dissertation for form and content and recommend that it be accepted in partial fulfillment of the requirements for the <phrase>degree</phrase> of Doctor of <phrase>Philosophy</phrase>, with a <phrase>major</phrase> in <phrase>Electrical Engineering</phrase>. We have read this dissertation and recommend its acceptance: (Original signatures are on file with official <phrase>student</phrase> records.)
Combining Deep <phrase>Linguistics</phrase> Analysis and Surface Pattern Learning: A <phrase>Hybrid</phrase> Approach to <phrase>Chinese</phrase> Definitional <phrase>Question Answering</phrase> We explore a <phrase>hybrid</phrase> approach for <phrase>Chinese</phrase> definitional <phrase>question answering</phrase> by combining <phrase>deep linguistic</phrase> analysis with surface pattern learning. We answer four questions in this study: 1) How helpful are <phrase>linguistic</phrase> analysis and pattern learning? 2) What kind of questions can be answered by <phrase>pattern matching</phrase>? 3) How much annotation is required for a pattern-based system to achieve good performance? 4) What <phrase>linguistic</phrase> features are most useful? <phrase>Extensive experiments</phrase> are conducted on biographical questions and other defini-tional questions. <phrase>Major</phrase> findings include: 1) <phrase>linguistic</phrase> analysis and pattern learning are complementary; both are required to make a good definitional QA system; 2) <phrase>pattern matching</phrase> is very effective in answering biographical questions while less effective for other definitional questions; 3) only a small amount of annotation is required for a pattern learning system to achieve good performance on biographical questions; 4) the most useful <phrase>linguistic</phrase> features are copulas and appositives; relations also <phrase>play</phrase> an important role; only some propositions convey vital facts.
<phrase>Machine Learning</phrase> Paradigms for <phrase>Speech Recognition</phrase>: An Overview <phrase>Automatic Speech Recognition</phrase> (ASR) has historically been a driving force behind many <phrase>machine learning</phrase> (ML) techniques, including the ubiquitously used <phrase>hidden Markov model</phrase>, discriminative learning, structured <phrase>sequence</phrase> learning, <phrase>Bayesian</phrase> learning, and adaptive learning. Moreover, ML can and occasionally does use ASR as a <phrase>large-scale</phrase>, realistic application to rigorously <phrase>test</phrase> the effectiveness of a given technique, and to inspire new problems arising from the inherently sequential and dynamic <phrase>nature</phrase> of speech. On the other hand, even though ASR is available commercially for some applications, it is largely an unsolved problemfor almost all applications, the performance of ASR is not on par with <phrase>human</phrase> performance. New insight from modern ML methodology shows great promise to advance the <phrase>state</phrase>-of-the-<phrase>art</phrase> in ASR <phrase>technology</phrase>. This overview article provides readers with an overview of modern ML techniques as utilized in the current and as relevant to future ASR <phrase>research</phrase> and systems. The intent is to foster further cross-<phrase>pollination</phrase> between the ML and ASR communities than has occurred in the past. The article is organized according to the <phrase>major</phrase> ML paradigms that are either popular already or have potential for making significant contributions to ASR <phrase>technology</phrase>. The paradigms presented and elaborated in this overview include: generative and discriminative learning; supervised, unsupervised, <phrase>semi-supervised</phrase>, and <phrase>active learning</phrase>; adaptive and <phrase>multi-task</phrase> learning; and <phrase>Bayesian</phrase> learning. These learning paradigms are motivated and discussed in the context of ASR <phrase>technology</phrase> and applications. We finally present and analyze <phrase>recent developments</phrase> of <phrase>deep learning</phrase> and learning with sparse representations, focusing on their direct relevance to advancing ASR <phrase>technology</phrase>.
Deep versus shallow judgments in learning to rank Much <phrase>research</phrase> in learning to rank has been placed on developing sophisticated learning methods, treating the <phrase>training set</phrase> as a given. However, the number of judgments in the <phrase>training set</phrase> directly aff <phrase>ects</phrase> the quality of the learned system. Given the expense of obtaining relevance judgments for constructing <phrase>training data</phrase>, one often has a limited budget in terms of how many judgments he can get. The <phrase>major</phrase> problem then is how to distribute this judgment <phrase>e</phrase> ffort across diff erent queries. In this <phrase>paper</phrase>, we investigate the tradeo ff between the number of queries and the number of judgments per query when <phrase>training sets</phrase> are constructed. In particular, we show that up to a limit, <phrase>training sets</phrase> with more queries but shallow (less) judgments per query are more cost effective than <phrase>training sets</phrase> with less queries but deep (more) judgments per query.
Training to a Neural Net's Inherent Bias A <phrase>neural net</phrase> with multiple output nodes is capable of distinguishing among a set of related input classes even in the absence of training. It can do so with an accuracy that is markedly better than random guessing. This is because each class will tend to activate a different set of output nodes. We refer to this tendency as the net's 'inherent' bias. Ascertaining a net's inherent bias may be thought of as learning the net. One may learn the net either instead of training it, or prior to training it. Furthermore, one only needs a small number of samples from each input class in <phrase>order</phrase> to reliably learn the net. If a net has been previously trained on a different, related set of classes, then ascertaining the inherent bias is a form of <phrase>knowledge transfer</phrase>. When such a net is trained to respond in accordance with its inherent bias, one may obtain substantially higher accuracies than is provided by nets trained in the standard <phrase>fashion</phrase>. Furthermore, when using a deep net, we were able to obtain such improvements while only allowing the top layer of the net to <phrase>train</phrase>. This layer contained only about 5.7% of the net's <phrase>free</phrase> parameters.
<phrase>Systems biology</phrase> in <phrase>human</phrase> <phrase>health</phrase> and <phrase>disease</phrase> Understanding the factors that influence <phrase>human</phrase> <phrase>health</phrase> and cause diseases has always been one of the <phrase>major</phrase> driving forces of biological <phrase>research</phrase>. With the spectacular progresses in quantitative techniques, <phrase>large-scale</phrase> measurement methods and with the intimate integration between <phrase>experimental</phrase> and computational approaches, <phrase>Biology</phrase> has recently acquired new technological and conceptual tools to investigate, <phrase>model</phrase> and understand <phrase>living organisms</phrase> at the system level. While the still young discipline of <phrase>Systems Biology</phrase> has been most widely devoted to the study of well-characterized <phrase>model</phrase> organisms, it has been clear since the early days of the <phrase>human genome project</phrase> that applications of system-wide approaches to <phrase>human biology</phrase> would open up tremendous opportunities in <phrase>medicine</phrase>. Recent <phrase>lessons learned</phrase> from <phrase>Systems Biology</phrase>, when applied to simple organisms like <phrase>bacteria</phrase> or <phrase>yeast</phrase>, prefigure the kind of insights that will benefit both <phrase>basic</phrase> <phrase>medical</phrase> <phrase>research</phrase> and clinical applications: <phrase>deeper understanding</phrase> of the <phrase>genotype</phrase> <phrase>phenotype</phrase> relationship; impact of the interactions between environmental conditions and <phrase>genotype</phrase>; novel mechanistic and functional insights based on global unbiased approaches; elaboration of powerful predictive models capturing the intricacies of <phrase>physiological</phrase> states. Advances on these various fronts obviously depend on different types of <phrase>research</phrase>, ranging from investigations on fundamental aspects of <phrase>human biology</phrase> to the more clinically oriented applications. Significantly, as techniques and concepts mature, a new discipline is emerging at the interface between <phrase>Medicine</phrase> and <phrase>Systems Biology</phrase>. To provide a snapshot of this evolving field of Systems <phrase>Medicine</phrase> and to illustrate new insights gained by applying <phrase>Systems Biology</phrase> approaches within the context of <phrase>human</phrase> <phrase>health</phrase> and <phrase>disease</phrase>, Molecular <phrase>Systems Biology</phrase> releases this month, in print, a collection of articles recently published in the <phrase>journal</phrase>. In many fields relevant to <phrase>medical</phrase> <phrase>research</phrase>, including <phrase>cancer</phrase> <phrase>biology</phrase>, deciphering the mechanisms of <phrase>disease</phrase> requires a deep <phrase>knowledge</phrase> of how signaling <phrase>transduction</phrase> pathways operate. Quantitative <phrase>proteomics</phrase> has made possible the simultaneous monitoring of the concurrent activity of multiple signaling <phrase>molecules</phrase>, enabling a broader and unbiased view of cellular signaling events. The work by <phrase>White</phrase> and co-workers (<phrase>Wolf</phrase>-Yadlin et al, 2006) illustrates how this type of <phrase>high</phrase>-throughput <phrase>data</phrase> can be correlated to biological response (e.g., proliferation and <phrase>cell migration</phrase>) to further our understanding of one of the <phrase>major</phrase> pathways known to be deregulated in <phrase>cancer</phrase>. These global approaches also reveal the inescapable fact that biological pathways are highly interconnected, which represents one of the <phrase>major</phrase> motivations for adopting a system-level approach in <phrase>biology</phrase>. In the study by Lehr et al 
Deep Teaching and <phrase>Action Research</phrase> Can Inform Practice Deep Teaching and <phrase>Action Research</phrase> Can Inform Practice The purpose of this ongoing <phrase>research</phrase> project was to use <phrase>action research</phrase>, deep teaching, and a <phrase>hybrid</phrase> delivery systems approach to answer two <phrase>research</phrase> questions: How can I set up the classroom experiences to <phrase>mirror</phrase> the <phrase>real world</phrase>? How can I <phrase>lead</phrase> students to be more responsible for their own learning and work? The course selected in which to conduct the <phrase>research</phrase> was the senior-level Training in <phrase>Business</phrase> and <phrase>Industry</phrase> course, which used much of the content in the <phrase>OEIS</phrase> 4: Technical Training and Delivery Systems course. Ethnograph v.5.08 was used to present quantitative findings; other qualitative findings came from students' and instructor's <phrase>journals</phrase>. The findings and conclusions hold promise to inform reflective classroom <phrase>teaching and learning</phrase> and best practices.
Laryngoscopic analysis of <phrase>tibetan</phrase> chanting modes and their relationship to register in <phrase>sino-tibetan</phrase> To investigate how the articulatory structures of the <phrase>larynx</phrase> and <phrase>pharynx</phrase> behave in <phrase>Tibetan</phrase> chanting, we obtained laryngoscopic <phrase>video</phrase> sequences of several <phrase>phonemic</phrase> contrasts in <phrase>Tibetan</phrase> and of two principal chant modes: a " <phrase>high</phrase> " chant which adopts raised-<phrase>larynx</phrase> posture with the <phrase>tongue</phrase> and <phrase>epiglottis</phrase> retracted, and a " deep " chant which adopts lowered-<phrase>larynx</phrase> posture with an open <phrase>pharyngeal</phrase> <phrase>area</phrase> but constricted ventricular ring immediately above the <phrase>glottis</phrase> and ventricular/mucosal channeling of airflow. The two chants reflect different modes of the <phrase>laryngeal</phrase> <phrase>sphincter</phrase> mechanism: one with aryepiglottic approximation but without trilling; the other with only the ventricular component. Aspects of <phrase>phonemic</phrase> patterns found in the three <phrase>Sino-Tibetan languages</phrase> we have observed laryngoscopically (Yi, Bai, and <phrase>Tibetan</phrase>) can be seen in the way the two chant modes are <phrase>produced</phrase> in <phrase>Tibetan</phrase>. The <phrase>high</phrase> chant resembles " tense " register in Yi, while the deep chant resembles " tense/harsh/low-tone " register in Bai. <phrase>Tibetan</phrase> chanting has <phrase>long</phrase> been of interest as a vocal technique. To investigate how the articulatory structures of the <phrase>larynx</phrase> and <phrase>pharynx</phrase> behave during chant modes, we invited <phrase>Tibetan</phrase> chant <phrase>master</phrase>, Gedun Jungney, to visit our facilities in <phrase>Victoria</phrase> to participate in <phrase>video</phrase> <phrase>recording</phrase> using fibreoptic <phrase>laryngoscopy</phrase>. We have used this technique to study other <phrase>Sino-Tibetan languages</phrase> and found a highly developed use of the <phrase>lower</phrase> <phrase>pharynx</phrase> to produce distinctive <phrase>sound</phrase> quality. Without wishing to imply any causal relationship, the sophisticated use of the aryepiglottic and ventricular <phrase>sphincter</phrase> mechanisms in the Yi and Bai languages warrants a phonetic comparison with <phrase>Tibetan</phrase> chant forms. Gedun Jungney, whose <phrase>family</phrase> originates in <phrase>western</phrase> <phrase>Sichuan</phrase> (Dege) and extreme northwest <phrase>Yunnan</phrase>, <phrase>China</phrase>, is the 33-year-old chant <phrase>master</phrase> of the <phrase>Ganden</phrase> Jangtse Norling <phrase>Monastery</phrase> in <phrase>Madras</phrase>, <phrase>India</phrase>. At the time of filming in 1999, he had been learning and perfecting chanting techniques for over 15 years. Laryngoscopic <phrase>video</phrase> images of the <phrase>pharyngeal</phrase>/<phrase>laryngeal</phrase> <phrase>area</phrase> were obtained by means of a Kay 9100 RLS <phrase>light</phrase> source and <phrase>Olympus</phrase> ENF-P3 fibreoptic nasendoscope fitted with a 28mm <phrase>lens</phrase>. An analog <phrase>S-VHS</phrase> <phrase>videotape</phrase> <phrase>master</phrase> was <phrase>produced</phrase> at 30 frames/<phrase>sec</phrase>, and <phrase>video</phrase> images were exported to a <phrase>PC</phrase> PIII-450 for processing using MediaStudio Pro 6.0 and synchronous <phrase>waveform</phrase> and spectrographic analysis using MultiSpeech and WaveSurfer. The <phrase>video</phrase> images show the behaviour of the <phrase>lower</phrase> <phrase>pharynx</phrase> including <phrase>vocal folds</phrase>, ventricular folds, aryepiglottic folds (<phrase>laryngeal</phrase> <phrase>sphincter</phrase>), <phrase>epiglottis</phrase> and <phrase>tongue</phrase> <phrase>root</phrase>. Two principal forms of chanting were elicited in repetitive sequences. A 
Analysis, <phrase>design</phrase> and implementation of personalized recommendation <phrase>algorithms</phrase> supporting self-organized communities <phrase>Hagen</phrase> 2005 <phrase>ii</phrase> iii Acknowledgments A journey is easier when you travel together. This <phrase>thesis</phrase> is the result of four years of work whereby I have been accompanied and supported by many people. It is a pleasant <phrase>aspect</phrase> that I have now the opportunity to <phrase>express my gratitude</phrase> to all of them. The first person I would like to thank is my supervisor Prof. Dr. Bernd J. Krmer. As a bi-national <phrase>PhD</phrase> <phrase>student</phrase>, I have been involved in his <phrase>DAAD</phrase> project IQN since 2002 and obtained the first opportunity to pursue my <phrase>research</phrase> in the Faculty of <phrase>Electrical Engineering</phrase> and <phrase>Information Engineering</phrase> at FernUniversitt in <phrase>Hagen</phrase>. During these days, I have known Prof Krmer as a sympathetic and principle-centered person. His overly enthusiasm and <phrase>integral</phrase> view on <phrase>research</phrase> and his mission for providing " only <phrase>high</phrase>-quality work and not less " has impressed me deeply. I owe him lots of gratitude for having me shown the way to decent <phrase>research</phrase>. He was an excellent supervisor, and I am really glad that I have come to know Prof Kremer in my <phrase>life</phrase>. Special thanks are also due to my second supervisor Prof. Ruimin Shen, who kept an eye on the progress of my work and always was available when I needed his advices. During these years, he gave me many opportunities to be involved in <phrase>research</phrase> and development of many national projects, and to serve as a team leader of the <phrase>E</phrase>-Learner Analysis and Personalized Services <phrase>research</phrase> group of <phrase>Shanghai</phrase> <phrase>E</phrase>-Learning Lab. Through these activities, I have improved my capabilities of <phrase>leadership</phrase>, <phrase>organization</phrase>, and teamwork. He could not even realize how much I have learned from him. I would also like to thank the colleagues who monitored my work and took much effort in <phrase>reading</phrase> and providing me with valuable comments on earlier versions of this <phrase>thesis</phrase>: Dr. thank them all for their kind help, support, interest, and valuable hints. was of great help for my <phrase>life</phrase>, work, and <phrase>research</phrase>. I feel a deep sense of gratitude for my father and mother who formed part of my vision and taught me the good things that really <phrase>matter</phrase> in <phrase>life</phrase>. The happy <phrase>memory</phrase> of my <phrase>family</phrase> always provides a persistent inspiration for my journey in this <phrase>life</phrase>. I am grateful to my kindly grandmother, in-laws, and all relatives, for rendering me the sense and the value of strong <phrase>bond</phrase> with them. Finally, 
<phrase>Deep Linguistic</phrase> Processing with GETARUNS for Spoken Dialogue Understanding In this <phrase>paper</phrase> we will present work carried out to scale up the system for text understanding called GETARUNS, and <phrase>port</phrase> it to be used in dialogue understanding. The current goal is that of extracting automatically argumentative <phrase>information</phrase> in <phrase>order</phrase> to build argumentative structure. The <phrase>long</phrase> term goal is using argumentative structure to produce automatic summarization of spoken dialogues. Very much like other <phrase>deep linguistic</phrase> processing systems, our system is a generic text/dialogue understanding system that can be used in connection with an <phrase>ontology</phrase> <phrase>WordNet</phrase>-and other similar repositories of commonsense <phrase>knowledge</phrase>. We will present the adjustments we made in <phrase>order</phrase> to cope with transcribed spoken dialogues like those <phrase>produced</phrase> in the <phrase>ICSI</phrase> <phrase>Berkeley</phrase> project. In a final section we present preliminary evaluation of the system on two tasks: the task of automatic argumentative labeling and another frequently addressed task: referential vs. non-referential <phrase>pronominal</phrase> detection. <phrase>Results</phrase> obtained <phrase>fair</phrase> much higher than those reported in similar experiments with <phrase>machine learning</phrase> approaches.
Text Generation with <phrase>Language</phrase> Models Abstract Text Generation with <phrase>Language</phrase> <phrase>Models 1</phrase> Acknowledgments I want to wholeheartedly thank the entire faculty in the <phrase>Computer Science</phrase> <phrase>Department</phrase> for the continuous support and assistance provided to me throughout the growth and development of my career at <phrase>RIT</phrase>. I want to specifically thank the following <phrase>faculty members</phrase> who made it possible for me to achieve the completion of my <phrase>Master's Degree</phrase> in Computer Thomas for their contributions. I cannot express in words the gratitude and appreciation I have for <phrase>giving me the opportunity</phrase> to prove myself regardless of the trying circumstances that make <phrase>life</phrase>. Text generation is a task of generating text from a machine representation system. Text generation has many useful applications like <phrase>data</phrase> anonymization, synthetic <phrase>data</phrase> generation for <phrase>data</phrase> sparsity issue, and summarizations. We explore text generation on various thematic topics using statistical <phrase>language</phrase> <phrase>model</phrase> and <phrase>deep learning</phrase> technique (LSTM <phrase>model</phrase>). We explored different LSTM architectures for text generation. We study differences in generated text using different <phrase>language</phrase> models. We used BLEU score and manual inspection to evaluate <phrase>language</phrase> models.
Incremental Slow Feature Analysis: Adaptive and Episodic Learning from <phrase>High</phrase>-dimensional Input Streams Incremental Slow Feature Analysis: Adaptive and Episodic Learning from <phrase>High</phrase>-dimensional Input Streams Istituto Dalle <phrase>Molle</phrase> di studi sull'intelligenza artificiale Galleria 2, 6928 Manno, <phrase>Switzerland</phrase> IDSIA was founded by the Fondazione Dalle <phrase>Molle</phrase> per la Qualit della <phrase>Vita</phrase> and is affiliated with both the Universit della Svizzera italiana (USI) and the Scuola unversitaria professionale della Svizzera italiana (SUPSI). Abstract Slow Feature Analysis (<phrase>SFA</phrase>) extracts features representing the underlying causes of changes within a temporally coherent <phrase>high</phrase>-dimensional raw sensory input signal. Our novel incremental version of <phrase>SFA</phrase> (IncSFA) combines incremental <phrase>Principal Components Analysis</phrase> and Minor Components Analysis. Unlike standard batch-based <phrase>SFA</phrase>, IncSFA adapts along with non-stationary environments, is amenable to episodic training, is not corrupted by <phrase>outliers</phrase>, and is <phrase>covariance</phrase>-<phrase>free</phrase>. These properties make IncSFA a generally useful unsupervised <phrase>preprocessor</phrase> for autonomous learning agents and <phrase>robots</phrase>. In IncSFA, the CCIPCA and <phrase>MCA</phrase> updates take the form of Hebbian and <phrase>anti</phrase>-Hebbian updating, extending the biological plausibility of <phrase>SFA</phrase>. In both <phrase>single</phrase> node and deep network versions, IncSFA learns to encode its input streams (such as <phrase>high</phrase>-dimensional <phrase>video</phrase>) by informative slow features representing meaningful abstract environmental properties. It can handle cases where batch <phrase>SFA</phrase> fails.
What kind of support to they need?: an instructional designer's experience in faculty and <phrase>student</phrase> support for online courses This <phrase>paper</phrase> intends to share two unique <phrase>Lehigh</phrase> experiences in designing and offering online courses to two completely different sets of audience. One of the two courses is offered to students around the world via Lehigh's International Program. The development team consists of a faculty <phrase>member</phrase>, an Instructional <phrase>Technology</phrase> Consultant (IT Consultant), three graduate assistants and other <phrase>media</phrase> support staff. We have about a month to develop an online course from scratch.Lehigh University's <phrase>Clipper</phrase> Project is designed to develop five <phrase>Web-based</phrase> <phrase>freshman</phrase> courses and offer them to <phrase>high school</phrase> seniors early admitted to <phrase>Lehigh</phrase>. Each course is assigned an IT Consultant to help the faculty develop the course. My other project is to work with an <phrase>English</phrase> <phrase>professor</phrase> to redesign his <phrase>English</phrase> course for the <phrase>Clipper</phrase> Project. This <phrase>professor</phrase> is considered a <phrase>catalyst</phrase> on <phrase>campus</phrase> for his continuous interest in exploring new and more effective <phrase>educational technology</phrase>. This team consists of three members, the <phrase>professor</phrase>, an IT Consultant, and a <phrase>graphic</phrase> and <phrase>Web designer</phrase>. Course redesign takes about one year.Serving as the IT consultant for both of these two teams, I had the opportunities to work with two very different <phrase>design</phrase> approaches. One differentiates itself from the other by variables such as the <phrase>target</phrase> audience, the discipline, the learning objectives, the instructor's level of comfort with <phrase>technology</phrase>, students' <phrase>technology</phrase> <phrase>literacy</phrase> and accessibility to various technologies, timeline, and many others. Table A presents a quick comparison of these two projects.In most ideal arrangements, the helpdesk is responsible for supporting students' technological needs and IT Consultants for the training and supporting of the faculty. In <phrase>reality</phrase>, it is fairly difficult to draw a line between the two in an online environment. Owing to their deep and <phrase>long</phrase>-term involvement with all aspects of online courses, IT Consultants are often considered more effective as well as efficient in providing first line support to online students. My experiences echo loud and <phrase>sound</phrase> with this <phrase>perception</phrase>.
Novel <phrase>speech processing</phrase> mechanism derived from auditory neocortical circuit analysis Analysis of the prominent anatomical and <phrase>physiological</phrase> features of auditory <phrase>thalamus</phrase> and <phrase>neocortex</phrase> has enabled <phrase>construction</phrase> of models designed to identify functionality emergent from these biological circuits. These models have recently been shown to provide powerful computational mechanisms for processing of continuous time-varying sequences such as speech; testing on speech <phrase>databases</phrase> has yielded positive initial <phrase>results</phrase> that are reported here. The <phrase>model</phrase> constitutes a novel <phrase>hypothesis</phrase> of underlying functions of auditory <phrase>neocortex</phrase>, and also represents a novel approach to <phrase>speech processing</phrase>. <phrase>Research</phrase> in our <phrase>laboratory</phrase> has been concentrating on the phenomenon of <phrase>long-term potentiation</phrase> (<phrase>LTP</phrase>) [3], which is the most likely candidate for a substrate of neocortical learning and <phrase>memory</phrase>. A set of simple learning rules was formulated based on <phrase>physiological</phrase> properties of <phrase>LTP</phrase>-i) <phrase>synaptic</phrase> weight can only increase, <phrase>ii</phrase>) every increase is small fixed change, and iii) low saturation threshold permits only 5-10 weight increases over the whole <phrase>period</phrase> of training [2] [10]. A series of models were constructed based on the known anatomical <phrase>cortical</phrase> features-sparse-random connec-tivity in the superficial <phrase>cortical</phrase> layers, emergence of the <phrase>cortical</phrase> patches defined by the <phrase>radius</phrase> of the local inhibition, and <phrase>feedback</phrase> inhibition and masking. The above <phrase>variety</phrase> of neocortical features specify a biologically constrained class of microcircuits, which typically perform <phrase>pattern recognition</phrase> or classification via competitive learning and <phrase>lateral inhibition</phrase> [6] [5]. Simulations of those circuits <phrase>lead</phrase> to efficient hardware implementations, with a <phrase>proven</phrase> utility for <phrase>pattern recognition</phrase> via efficient approximation of statistical <phrase>pattern recognition</phrase> methods (e.g. Bayes classifiers) [5]. Key anatomical properties of the auditory <phrase>model</phrase> being reviewed in [16] (see also Table 1) include i) topographic (MGv) versus broadly-tuned (<phrase>MGm</phrase>) <phrase>thalamic</phrase> nuclei, convergently projecting to primary <phrase>auditory cortex</phrase>; local <phrase>cortical</phrase> circuits composed of roughly 100:1 <phrase>excitatory</phrase> to inhibitory cells with <phrase>lateral inhibition</phrase>; and iii) vertical columnar <phrase>organization</phrase> projecting from middle to superficial to deep layers. Key <phrase>physiological</phrase> properties include: i) <phrase>plastic</phrase> (<phrase>NMDA</phrase>-dependent) <phrase>synapses</phrase> from broadly tuned <phrase>MGm</phrase> afferents versus non-<phrase>plastic</phrase> <phrase>synapses</phrase> from topographic MGv affer-ents; <phrase>ii</phrase>) plasticity via <phrase>long-term potentiation</phrase> (<phrase>LTP</phrase>); and iii) time courses for excitation versus inhibition of roughly 1:100. Learning in the <phrase>model</phrase> is based on the <phrase>physiological</phrase> induction and expression rules for <phrase>synaptic</phrase> <phrase>long-term potentiation</phrase> or <phrase>LTP</phrase> [12] [11] which have been shown in previous modeling efforts to give rise to useful computational properties [2] [10] [9] [5] [7] [8]. Superficial <phrase>Cortical</phrase> Layer Broadly tuned afferents from non-specific <phrase>thalamic</phrase> nucleus Sparse connectivity (p ~ .1) <phrase>LTP</phrase> of 
Dynamic Generation of Complex Behavior <phrase>Simulation</phrase> can be an effective training method if the <phrase>simulation</phrase> environment is as realistic as possible. An 'important part of the training for <phrase>Navy</phrase> pilots involves flying against computer-controlled agents in simulated tactical scenarios. In <phrase>order</phrase> for such a situation to be realistic, the computer-controlled agents must be indistinguishable from <phrase>human</phrase>-piloted agents within the simulated environment. The primary goal of the Soar-<phrase>IFOR</phrase> project (Jones et al. 1993; Rosenbloom et al. 1994) is to provide such believable agents for <phrase>flight training</phrase> simulations. To achieve this goal, we have constructed the TACAIR-SOAR system.' Developing this system requires us to address a number of core <phrase>research</phrase> issues within <phrase>artificial intelligence</phrase>, including reasoning about interacting goals, situation interpretation, <phrase>communication</phrase> , explanation, planning, learning, <phrase>natural language understanding</phrase> and generation, temporal reasoning , and plan recognition. This <phrase>report</phrase> focuses on two particular issues required to <phrase>function</phrase> reasonably within the tactical air domain: a system must be able to generate behavior in response to complex goals and situations, and it must be able to do so dynamically, in response to extremely rapid changes in the agent's situation. On the surface, these two capabilities seem to be at odds to each other. Approaches to real-time or reactive behavior have generally not been used within complex domains, and systems that focus on complex goals do not usually do so in real time. Our <phrase>solution</phrase> has been to encode <phrase>knowledge</phrase> within the Soar <phrase>production</phrase> <phrase>architecture</phrase> (Rosen-bloom et al. 1991) in <phrase>order</phrase> to take advantage of <phrase>state</phrase>-have also provided invaluable assistance as <phrase>subject-matter</phrase> experts. of-the-<phrase>art</phrase> matching <phrase>algorithms</phrase> to provide real-time, reactive behavior. In addition, the <phrase>knowledge</phrase> is represented at a fine <phrase>grain size</phrase>, capturing a deep representation of the first principles involved in the tactical flight domain. This representation allows reactive rules to combine in a <phrase>fashion</phrase> that leads to appropriate responses to complex goal and situation combinations. The current version of TAGAIR-SOAR has been flown in simulated exercises against other computer-controlled agents, as well as <phrase>human</phrase>-controlled flight simulators. The agent exhibits a wide <phrase>variety</phrase> of complex behaviors, and it meets the real-time requirements of the task. In addition, the agent provides realistic, <phrase>human</phrase>-like behavior in a number of tactical scenarios.
Unsupervised Discovery of a Statistical <phrase>Verb</phrase> <phrase>Lexicon</phrase> This <phrase>paper</phrase> demonstrates how unsupervised techniques can be used to learn models of <phrase>deep linguistic</phrase> structure. Determining the <phrase>semantic</phrase> roles of a verb's dependents is an important step in <phrase>natural language understanding</phrase>. We present a method for learning models of <phrase>verb</phrase> argument patterns directly from unannotated text. The learned models are similar to existing <phrase>verb</phrase> lexicons such as VerbNet and PropBank, but additionally include <phrase>statistics</phrase> about the linkings used by each <phrase>verb</phrase>. The method is based on a structured <phrase>probabilistic model</phrase> of the domain , and <phrase>unsupervised learning</phrase> is performed with the EM <phrase>algorithm</phrase>. The learned models can also be used discriminatively as <phrase>semantic</phrase> role labelers, and when evaluated relative to the PropBank annotation , the best learned <phrase>model</phrase> reduces 28% of the error between an informed baseline and an <phrase>oracle</phrase> <phrase>upper</phrase> bound.
Rendering + modeling + <phrase>animation</phrase> + postprocessing = <phrase>computer graphics</phrase> Nowadays, students coming into a <phrase>computer graphics</phrase> course have seen movies that have fantastic graphics effects (e.g., <i><phrase>Toy Story</phrase>, A Bug's <phrase>Life</phrase></i> and the <i><phrase>Star</phrase> <phrase>War</phrase></i> series). These students have also acquired a certain level of graphics <phrase>knowledge</phrase> by playing <phrase>games</phrase> and <phrase>reading</phrase> popular <phrase>magazines</phrase>. Their expectations are certainly <phrase>high</phrase> for their first graphics course. Moreover, many deep and powerful theories were developed during the past decade. Either because these topics are too new or because they are too difficult to teach, they are frequently only sketched or even skipped in favor of a "<phrase>programming</phrase>" approach. Hence, what a <phrase>student</phrase> learns in an introductory course might only be some <phrase>programming</phrase> skills using a graphics <phrase>API</phrase>, usually <phrase>OpenGL</phrase>, along with some fundamental <phrase>computer graphics</phrase> <phrase>knowledge</phrase>. Students are only exposed a little to modern developments which are frequently used by the graphics <phrase>industry</phrase> for creating fantastic <phrase>special effects</phrase> or realistic images.At <phrase>Michigan Technological University</phrase>, we have three <phrase>computer graphics</phrase> related courses: an introduction to graphics course, a <phrase>computing</phrase> with <phrase>geometry</phrase> course for junior and senior students and an advanced graphics course for graduate students. We also have graduate courses on geometric modeling and visualization. The minimum prerequisite of the introductory graphics course is only a sophomore <phrase>software development</phrase> course which is usually taken after a <phrase>data structure</phrase> course. This introductory course covers essentially all traditional topics with the help of publicly available tools (e.g., GLUT and GLUI) and other tools developed by the instructor of this course. We believe that our current approach is a successful combination of theory and the current popular <phrase>programming</phrase> approach using tools. However, we still feel that we need to do more to introduce students to modern theories and developments. While many educators have already observed the need of a new graphics course and proposed some approaches [1], our approach is more ambitious and non-traditional.We believe that graphics consists of four <phrase>major</phrase> components: rendering, modeling, <phrase>animation</phrase> and postprocessing. The rendering part has become the <phrase>major</phrase> topic in typical graphics textbooks and the main <phrase>thrust</phrase> of a <phrase>programming</phrase> approach. There is nothing wrong with a focus on rendering methods. The problem is that the <phrase>programming</phrase> approach cannot <phrase>cover</phrase> <phrase>global illumination</phrase> models (e.g., <phrase>ray tracing</phrase> and radiosity) and <phrase>volume rendering</phrase>, because most popular APIs only implement local illumination models. Every scene contains some objects and object building requires the <phrase>knowledge</phrase> of modeling. However, building a good and realistic <phrase>model</phrase> is not part of any <phrase>API</phrase>. <phrase>Animation</phrase> is the skill for creating <phrase>animation</phrase> sequences and simulating physical events. Finally, postprocessing makes created scenes satisfy additional requirements and requires a special set of 2D operations. Obvious topics should include, but not be limited to, filters, <phrase>morphing</phrase>, <phrase>dithering</phrase> and other current techniques. Since the <phrase>programming</phrase> approach usually cannot go this far to <phrase>cover</phrase> all four components, additional course materials and working environments are required. This has become the main <phrase>thrust</phrase> of our project.To address this need, our goal is to <phrase>design</phrase> a <phrase>comprehensive</phrase> introductory <phrase>computer graphics</phrase> course that covers all four <phrase>major</phrase> components to some depth with a breath-first and learning-by-doing approach. To support this goal, a pedagogical environment is required for students to experiment and visualize important concepts, and to perform a semester-<phrase>long</phrase> project of implementing various components. In the following, we discuss a number of problems that prompted us to initiate this project, present a description of our proposed course and its <phrase>software</phrase> tools, address the <phrase>software</phrase> modules that are currently under development and, finally, make our conclusions.
Beyond the <phrase>Short</phrase> Answer Question with <phrase>Research</phrase> Methods <phrase>Tutor</phrase> <phrase>Research</phrase> Methods <phrase>Tutor</phrase> is a new <phrase>intelligent tutoring</phrase> system created by <phrase>porting</phrase> the existing implementation of the AutoTutor system to a new domain, <phrase>Research</phrase> Methods in <phrase>Behavioural Sciences</phrase>, which allows more interactive dialogues. The procedure of <phrase>porting</phrase> allowed for an evaluation of the domain <phrase>independence</phrase> of the AutoTutor framework and for the identification of domain related requirements. Specific recommendations for the development of other dialogue-based tutors were derived from our experience. <phrase>Recent advances</phrase> in <phrase>Intelligent Tutoring</phrase> System <phrase>technology</phrase> focus on developing dialogue-based tutors, which <phrase>act</phrase> as conversational partners in learning. AutoTutor [5, 7], one of the prevalent systems in this field, claims to simulate naturalistic tutoring sessions in the domain of computer <phrase>literacy</phrase>. An innovative characteristic of AutoTutor is the use of a talking head as the primary interface with the user. The system is also claimed to be <phrase>domain independent</phrase> and to be capable of supporting deep reasoning in the tutorial dialogue. One goal of the current project is to <phrase>test</phrase> these claims. Another <phrase>motivation</phrase> was the fact that the domain of AutoTutor, computer <phrase>literacy</phrase>, provides limited potential for activating deep reasoning mechanisms. By <phrase>porting</phrase> the <phrase>tutor</phrase> to a new domain, which requires in-depth qualitative reasoning, we can address issues of domain <phrase>independence</phrase> and framework <phrase>usability</phrase> in a <phrase>concrete</phrase> manner. The new <phrase>tutor</phrase>, based on the AutoTutor framework, is built on the domain of <phrase>Research</phrase> Methods in <phrase>Behavioural Sciences</phrase> and thus was named <phrase>Research</phrase> Methods <phrase>Tutor</phrase> (<phrase>RMT</phrase>).
Analysis of Meaning Making in <phrase>Online Learning</phrase> This <phrase>paper</phrase> reports on our efforts to deepen the analysis of online <phrase>collaborative learning</phrase>. Most studies of <phrase>online learning</phrase> use quantitative methods that assign meaning to contributions in isolation and aggregate over many sessions, obscuring the actual procedures by which participants accomplish learning through the affordances of online <phrase>media</phrase>. Methods for studying the interactional <phrase>construction</phrase> of meaning are available, but have largely been developed for brief episodes of face-to-face <phrase>data</phrase>, and do not scale well to <phrase>online learning</phrase> where <phrase>media</phrase> resources, time scale, and <phrase>synchronicity</phrase> all differ. In <phrase>order</phrase> to resolve this tradeoff, we are developing an analytic method that scales up sequential and interactional analysis to longer term distributed and asynchronous interactions. The <phrase>paper</phrase> describes applications to <phrase>data</phrase> derived from asynchronous interaction of dyads and <phrase>small groups</phrase>. Our <phrase>long</phrase>-term objective is to obtain a <phrase>deep understanding</phrase> of how learning is accomplished in <phrase>technology</phrase>-mediated interactions that take place at multiple time scales in different <phrase>media</phrase>. 1 Introduction <phrase>Recent developments</phrase> have highlighted the ascendancy of <phrase>online learning</phrase> [1, 2]. Online <phrase>collaborative learning</phrase> brings together social processes of learning and representational <phrase>aids</phrase> for this learning, providing a fertile <phrase>area</phrase> for <phrase>research</phrase> and development while serving an important application. An understanding of how participants appropriate and are influenced by the affordances of the medium is needed to adequately inform the <phrase>design</phrase> of the learning experience and the resources that support it [3]. Because learning is largely social, it is also critical to understand the intertwinement of individual and intersubjective trajectories of meaning-making [4]. Yet we do not yet sufficiently understand these areas. Since most <phrase>online learning</phrase> has been mediated through <phrase>text-based</phrase> tools, we lack intensive study of how richer representations mediate <phrase>online learning</phrase>. Moreover, most studies of <phrase>online learning</phrase> use quantitative methods that disaggregate interaction into segments and assign meaning to these segments in isolation through coding, losing the interactionally constructed meaning. These methods aggregate over many sessions, obscuring the actual procedures by which participants accomplish learning through the affordances of online <phrase>media</phrase> [5]. Methods for studying the interactional <phrase>construction</phrase> of meaning are available [6, 7], but have largely been developed for brief episodes of face-to-face <phrase>data</phrase>, and do not scale well to <phrase>online learning</phrase> where <phrase>media</phrase> resources, time scale, and <phrase>synchronicity</phrase> all differ. This analytic tradeoff between <phrase>scalability</phrase> and fidelity must be resolved in <phrase>order</phrase> to inform the <phrase>design</phrase> of improved <phrase>online learning</phrase> environments and participation structures that engage participants more deeply 
A supervised strategy for deep kernel machine This <phrase>paper</phrase> presents an <phrase>alternative</phrase> to the supervised KPCA <phrase>based approach</phrase> for learning a Multilayer Kernel Machine (MKM) [1]. In our proposed procedure, the <phrase>hidden layers</phrase> are learnt in a supervised <phrase>fashion</phrase> based on kernel partial least squares <phrase>regression</phrase>. The main interest resides in a simplified learning scheme as the obtained hidden features are automatically ranked according to their correlation with the <phrase>target</phrase> outputs. The approach is illustrated on <phrase>small scale</phrase> <phrase>real world</phrase> applications and shows compelling evidences.
ISPD 2013 expert designer/user session (eds) We have newly introduced the expert designer/user session (EDS) to ISPD in 2013, which is <phrase>tailor</phrase>-made for designers and users of physical <phrase>design</phrase> (PD) tools. Since ISPD is the premium PD-centric symposium, it is a great opportunity for designers, tools developers and PD researchers to interact and learn from each other. The benefit of including EDS in ISPD's program is twofold. (1) It provides a focus session and <phrase>friendly</phrase> environment, which stimulates technical discussion among <phrase>industry</phrase> and <phrase>academia</phrase>. Experienced PD users would have a chance to get exposed to the detail of the latest <phrase>state</phrase>-ofthe- <phrase>art</phrase> <phrase>research</phrase> in the field of physical <phrase>design</phrase>. (2) The session brings in designers experience of PD tools, which includes opportunities and challenges from users' prospective. This would directly drive the PD <phrase>research</phrase> direction in the future, and tighten the <phrase>bond</phrase> between users, researchers and developers in the PD <phrase>community</phrase>. In turn, it eventually helps improving the PD tools from <phrase>EDA</phrase> vendors. In this Expert Designer Session (EDS), we carefully selected and invited a list of six expert designers/users around the world to present their on-hand <phrase>design</phrase> and tool experience. The topics <phrase>covered</phrase> include <phrase>tree</phrase>-mesh <phrase>clock</phrase> distribution, large block placement, place-and-route for custom designs, layout dependent effects, <phrase>metal</phrase>-based ECO, and <phrase>clock</phrase> aware timing closure flow. The format of EDS consists of an oral presentation session, immediately followed by a <phrase>poster</phrase> session. This encourages deep discussion and <phrase>professional</phrase> networking between PD users, researchers and developers.
3D skeletal <phrase>reconstruction</phrase> from low-resolution <phrase>multi-view</phrase> images This <phrase>paper</phrase> demonstrates how 3D skeletal <phrase>reconstruction</phrase> can be performed by using a pose-sensitive embedding technique applied to <phrase>multi-view</phrase> <phrase>video</phrase> recordings. We apply our approach to challenging low-resolution <phrase>video</phrase> sequences. Usually skeletal <phrase>reconstruction</phrase> can be only achieved with many calibrated <phrase>high</phrase>-resolution cameras, and only blob detection can be achieved with such low-resolution imagery. We show that with this embedding technique (a metric learning technique using a <phrase>deep convolutional</phrase> <phrase>architecture</phrase>), we achieve very good 3D skeletal <phrase>reconstruction</phrase> on low-resolution outdoor scenes with many challenges.
Bands Sensitive <phrase>Convolutional Network</phrase> for <phrase>Hyperspectral</phrase> <phrase>Image Classification</phrase> <phrase>Hyperspectral</phrase> image (HSI) classification deals with the problem of <phrase>pixel</phrase>-wise <phrase>spectrum</phrase> labelling. Traditional HSI classification <phrase>algorithms</phrase> focus on two <phrase>major</phrase> stages: <phrase>feature extraction</phrase> and classifier <phrase>design</phrase>. Though studied for decades, HSI classification hasn't been perfectly solved. One of the main reasons relies on the fact that features extracted by embedding methods can hardly match an <phrase>ad hoc</phrase> classifier. Recently, <phrase>deep learning</phrase> methods achieve an <phrase>end-to-end</phrase> mechanism and can learn features suitable for classification from the raw <phrase>data</phrase>. Inspired by the newly proposed work on <phrase>deep learning</phrase> for HSI classification, in this <phrase>paper</phrase>, we propose to build a <phrase>deep convolutional</phrase> network based on the analysis of spectral <phrase>band</phrase> discriminative characteristics. More specifically, we first <phrase>split</phrase> the <phrase>spectrum</phrase> bands into groups based on their correlation relationships. Then we build a <phrase>band</phrase> variant <phrase>CNN</phrase> submodel, where each group is modelled by one of those submodels. Meanwhile, a conventional <phrase>CNN</phrase> <phrase>model</phrase> is also learned globally on the spatial-spectral space, to maintain robustness of submodel changes. Lastly, we concatenate the global <phrase>CNN</phrase> <phrase>model</phrase> and <phrase>band</phrase>-specific <phrase>CNN</phrase> submodels to one unique <phrase>model</phrase>. In this way, global robustness and <phrase>band</phrase> <phrase>variance</phrase> are mixed together. Experiments on publicly available datasets demonstrate the great performance of the <phrase>proposed method</phrase>.
Occlusion-aware <phrase>multi-view</phrase> <phrase>reconstruction</phrase> of articulated objects for manipulation The goal of this <phrase>research</phrase> is to develop <phrase>algorithms</phrase> using multiple views to automatically recover complete 3D models of articulated objects in unstructured environments and thereby enable a robotic system to facilitate further manipulation of those objects. First, an <phrase>algorithm</phrase> called Procrustes-Lo-RANSAC (PLR) is presented. Structure-from-motion techniques are used to capture 3D <phrase>point cloud</phrase> models of an articulated object in two different configurations. Procrustes analysis, combined with a locally optimized RANSAC sampling strategy, facilitates a straightforward geometric approach to recovering the joint axes, as well as classifying them automatically as either revolute or prismatic. The <phrase>algorithm</phrase> does not require <phrase>prior knowledge</phrase> of the object, nor does it make any assumptions about the planarity of the object or scene. Second, with such a resulting articulated <phrase>model</phrase>, a robotic system is then able to manipulate the object either along its joint axes at a specified grasp point in <phrase>order</phrase> to exercise its degrees of freedom or move its end effector to a particular position even if the point is not visible in the current view. This is one of the main advantages of the occlusion-aware approach, because the models capture all sides of the object meaning that the <phrase>robot</phrase> has <phrase>knowledge</phrase> of parts of the object that are not visible in the current view. Experiments with a <phrase>PUMA</phrase> 500 <phrase>robotic arm</phrase> demonstrate the effectiveness of the approach on a <phrase>variety</phrase> of <phrase>real-world</phrase> objects containing both revolute and prismatic joints. <phrase>ii</phrase> Third, we improve the <phrase>proposed approach</phrase> by using a RGBD <phrase>sensor</phrase> (<phrase>Microsoft</phrase> <phrase>Kinect</phrase>) that yield a depth value for each <phrase>pixel</phrase> immediately by the <phrase>sensor</phrase> itself rather than requiring correspondence to establish depth. KinectFusion <phrase>algorithm</phrase> is applied to produce a <phrase>single</phrase> <phrase>high</phrase>-quality, geometrically accurate 3D <phrase>model</phrase> from which rigid links of the object are segmented and aligned, allowing the joint axes to be estimated using the geometric approach. The improved <phrase>algorithm</phrase> does not require <phrase>artificial</phrase> markers attached to objects, yields much denser 3D models and reduces the computation time. iii Dedication I dedicate this work to my beloved <phrase>family</phrase> who made all of this possible for their endless encouragement and <phrase>faith</phrase>. iv Acknowledgments First, I would like to give my most heartfelt thanks to my adviser, Dr. Stanley Birchfield for his numerous guidance and support at every step of this work. His deep <phrase>love</phrase> and insight of <phrase>science</phrase> inspired me in the right direction and made my journey a truly enjoyable learning 
Behavior and performance of the <phrase>deep belief</phrase> networks on <phrase>image classification</phrase> We apply <phrase>deep belief</phrase> networks of <phrase>restricted Boltzmann machines</phrase> to bags of words of sift features obtained from <phrase>databases</phrase> of 13 Scenes, 15 Scenes and <phrase>Caltech</phrase> 256 and study experimentally their behavior and performance. We find that the final performance in the supervised phase is reached much faster if the system is <phrase>pre-trained</phrase>. <phrase>Pre-training</phrase> the system on a larger dataset keeping the supervised dataset fixed improves the performance (for the 13 Scenes case). After the <phrase>unsupervised pre-training</phrase>, <phrase>neurons</phrase> arise that form approximate explicit representations for several categories (meaning they are mostly active for this category). The last three facts suggest that unsupervised training really discovers structure in these <phrase>data</phrase>. <phrase>Pre-training</phrase> can be done on a completely different dataset (we use <phrase>Corel</phrase> dataset) and we find that the supervised phase performs just as good (on the 15 Scenes dataset). This leads us to conjecture that one can pre-<phrase>train</phrase> the system once (e.g. in a <phrase>factory</phrase>) and subsequently apply it to many supervised problems which then learn much faster. The best performance is obtained with <phrase>single</phrase> <phrase>hidden layer</phrase> system suggesting that the <phrase>histogram</phrase> of sift features doesn't have much <phrase>high</phrase> level structure. The overall performance is almost equal, but slightly worse then that of the <phrase>support vector machine</phrase> and the spatial pyramidal matching.
<phrase>Lessons Learned</phrase> from Authoring for Inquiry Learning: A <phrase>Tale</phrase> of Authoring Tool <phrase>Evolution</phrase> We present an argument for ongoing and deep participation by <phrase>subject matter</phrase> experts (SMEs, i.e. teachers and <phrase>domain experts</phrase>) in advanced <phrase>learning environment</phrase> (LE) development, and thus for the need for highly usable authoring tools. We also argue for the "user <phrase>participatory design</phrase>" of involving SMEs in creating the authoring tools themselves. We describe our experience building authoring tools for the <phrase>Rashi</phrase> LE, and how working with SMEs <phrase>lead</phrase> us through three successive authoring tool designs. We summarize <phrase>lessons learned</phrase> along they way about authoring tool <phrase>usability</phrase>.
Learning to Control a <phrase>Low-Cost</phrase> Manipulator using <phrase>Data</phrase>-Efficient <phrase>Reinforcement Learning</phrase> Over the last years, there has been substantial progress in robust manipulation in unstructured environments. The <phrase>long</phrase>-term goal of our work is to get away from precise, but very expensive robotic systems and to develop affordable, potentially imprecise, self-adaptive manipulator systems that can interactively perform tasks such as playing with children. In this <phrase>paper</phrase>, we demonstrate how a <phrase>low-cost</phrase> off-the-shelf robotic system can learn closed-loop policies for a stacking task in only a handful of trialsfrom scratch. Our manipulator is inaccurate and provides no pose <phrase>feedback</phrase>. For learning a controller in the work space of a <phrase>Kinect</phrase>-style depth <phrase>camera</phrase>, we use a <phrase>model</phrase>-based <phrase>reinforcement learning</phrase> technique. Our learning method is <phrase>data</phrase> efficient, reduces <phrase>model</phrase> bias, and deals with several noise sources in a principled way during <phrase>long</phrase>-term planning. We present a way of incorporating <phrase>state-space</phrase> constraints into the learning process and analyze the learning gain by exploiting the sequential structure of the stacking task. I. INTRODUCTION Over the last years, there has been substantial progress in robust manipulation in unstructured environments. While existing techniques have the potential to solve various household manipulation tasks, they typically rely on extremely expensive <phrase>robot</phrase> hardware [12]. The <phrase>long</phrase>-term goal of our work is to develop affordable, <phrase>lightweight</phrase> manipulator systems that can interactively <phrase>play</phrase> with children. A key problem of cheap manipulators, however, is their inaccuracy and the limited <phrase>sensor</phrase> <phrase>feedback</phrase>, if any. In this <phrase>paper</phrase>, we show how to use a cheap, off-the-shelf robotic manipulator ($370) and a <phrase>Kinect</phrase>-style (http://www.xbox.com/<phrase>kinect</phrase>) depth <phrase>camera</phrase> (<$120) to learn a block stacking task [2, 1] under <phrase>state-space</phrase> constraints. We use <phrase>data</phrase>-efficient <phrase>reinforcement learning</phrase> (<phrase>RL</phrase>) to <phrase>train</phrase> a controller directly in the work space of the depth <phrase>camera</phrase>. Fully autonomous <phrase>RL</phrase> methods typically require many trials to successfully solve a task (e.g., Q-learning), a good ini-tialization (e.g., by imitation [3]), or a <phrase>deep understanding</phrase> of the system. If this <phrase>knowledge</phrase> is unavailable, due to the lack of understanding of complicated dynamics or because a <phrase>solution</phrase> is simply not known, <phrase>data</phrase>-intensive learning methods are required. In a robotic system, however, many physical interactions with the environment are often infeasible and <phrase>lead</phrase> to worn-out <phrase>robots</phrase>. The more fragile a robotic system the more important <phrase>data</phrase>-efficient learning methods are. To sidestep these problems, we build on PILCO (probabilis-tic inference for learning control), a <phrase>data</phrase>-efficient <phrase>model</phrase>-based (indirect) policy search method [7] that reduces <phrase>model</phrase> bias,
A Better Way to Pretrain <phrase>Deep Boltzmann Machines</phrase> We describe how the pretraining <phrase>algorithm</phrase> for <phrase>Deep Boltzmann Machines</phrase> (<phrase>DBMs</phrase>) is related to the pretraining <phrase>algorithm</phrase> for <phrase>Deep Belief</phrase> Networks and we show that under certain conditions, the pretraining procedure improves the variational <phrase>lower</phrase> bound of a two-<phrase>hidden-layer</phrase> <phrase>DBM</phrase>. Based on this analysis, we develop a different method of pretraining <phrase>DBMs</phrase> that distributes the modelling work more evenly over the <phrase>hidden layers</phrase>. Our <phrase>results</phrase> on the MNIST and NORB datasets demonstrate that the new pretraining <phrase>algorithm</phrase> allows us to learn better <phrase>generative models</phrase>.
<phrase>Supervised Learning</phrase> to Detect <phrase>Salt</phrase> Body In this <phrase>paper</phrase> we present a novel approach to detect <phrase>salt</phrase> bodies based on seismic attributes and <phrase>supervised learning</phrase>. We <phrase>report</phrase> on the use of a <phrase>machine learning</phrase> <phrase>algorithm</phrase>, Extremely <phrase>Randomized</phrase> <phrase>Trees</phrase>, to automatically identify and classify <phrase>salt</phrase> regions. We have worked with a complex synthetic seismic dataset from phase I <phrase>model</phrase> of the SEG Advanced Modeling <phrase>Corporation</phrase> (SEAM) that corresponds to deep <phrase>water</phrase> regions of the <phrase>Gulf</phrase> of <phrase>Mexico</phrase>. This dataset has very low <phrase>frequency</phrase> and contains <phrase>sediments</phrase> bearing <phrase>amplitude</phrase> values similar to those of <phrase>salt</phrase> bodies. In the first step of our methodology, where <phrase>machine learning</phrase> is applied directly to the seismic <phrase>data</phrase>, we obtained accuracy values of around 80%. A second (post-processing) smoothing step improved accuracy to around 95%. We conclude that <phrase>machine learning</phrase> is a promising mechanism to identify <phrase>salt</phrase> bodies on seismic <phrase>data</phrase>, especially with models that can produce complex decision boundaries, while being able to control the associated <phrase>variance</phrase> component of error.
Consistency measures for <phrase>feature selection</phrase> The use of <phrase>feature selection</phrase> can improve accuracy, efficiency, applicability and understandability of a learning process. For this reason, many methods of automatic <phrase>feature selection</phrase> have been developed. Some of these methods are based on the search of the features that allows the <phrase>data set</phrase> to be considered consistent. In a search problem we usually evaluate the search states, in the case of <phrase>feature selection</phrase> we measure the possible feature sets. This <phrase>paper</phrase> reviews the <phrase>state</phrase> of the <phrase>art</phrase> of consistency based <phrase>feature selection</phrase> methods, identifying the measures used for feature sets. An in-deep study of these measures is conducted, including the definition of a new measure necessary for completeness. After that, we perform an empirical evaluation of the measures comparing them with the highly reputed wrapper approach. Consistency measures achieve similar <phrase>results</phrase> to those of the wrapper approach with much better efficiency.
New Frontiers in Qlr: Definition, <phrase>Design</phrase> and Display New Frontiers in Qlr: Definition, <phrase>Design</phrase> and Display <phrase>Research</phrase> that is attentive to temporal processes and durational phenomena is an important tradition within the <phrase>social sciences</phrase> internationally with distinct disciplinary trajectories. Qualitative longitudinal <phrase>research</phrase> emerged as a distinct methodological <phrase>paradigm</phrase> around the turn of the <phrase>millennium</phrase>, named within the <phrase>UK</phrase> through <phrase>journal</phrase> special issues, <phrase>literature</phrase> reviews and funding commitments. In 2012-3 the <phrase>ESRC</phrase> National Centre for <phrase>Research</phrase> Methods funded a network for methodological <phrase>innovation</phrase> to map 'New frontiers of QLR', bringing together a group of scholars who have been actively involved in establishing QLR as a methodological field. The network provided an opportunity to consolidate the learning that has developed in QLR over a sustained <phrase>period</phrase> of <phrase>investment</phrase> and to engage critically with what QLR might mean in new times. This <phrase>paper</phrase> documents the series of discussions staged by the network involving the definition of QLR, the kinds of relationships and practices it involves and the consequences of these in a changing <phrase>landscape</phrase> for social <phrase>research</phrase>. The series was deliberately interdisciplinary ensuring that we engaged with the temporal perspectives and norms of different <phrase>academic</phrase> and practice traditions and this has both enriched and complicated the picture that has emerged from our deliberations. In this <phrase>paper</phrase> we argue that QLR is a methodological <phrase>paradigm</phrase> that by definition moves with the times, and is an ongoing site of <phrase>innovation</phrase> and experiment. Key issues identified for future development in QLR include: intervening in debates of '<phrase>big data</phrase>' with visions of deep <phrase>data</phrase> that involve following and connecting cases over time; the potential of longitudinal approaches to reframe the 'sample' exploring new ways of connecting the particular and the <phrase>general</phrase>; new thinking about <phrase>research ethics</phrase> that move us beyond anonymity to better explore the meanings of confidentiality and the co-<phrase>production</phrase> of <phrase>research</phrase> <phrase>knowledge</phrase>; and finally the promotion of a QLR sensibility that involves a heightened awareness of the here and now in the making of <phrase>knowledge</phrase>, yet which also connects <phrase>research</phrase> biographically over a career, enriched by a reflexive understanding of time as a resource in the making of meaning.
<phrase>Internet</phrase> repositories for <phrase>collaborative learning</phrase>: supporting both students and teachers Most efforts to create <phrase>computer-supported</phrase> <phrase>collaborative learning</phrase> environments have been focused on students. However, without providing appropriate integration of collaborative activities into curricula, these efforts will have little widespread impact on educational practices. To improve <phrase>education</phrase> through <phrase>technology</phrase>, <phrase>learning environments</phrase> for students must be integrated with <phrase>curriculum</phrase> development tools for teachers to create an integrated collaboration-oriented classroom. This <phrase>paper</phrase> describes how <phrase>software</phrase> tools for <phrase>Internet</phrase> repositories can aid fundamental collaboration activitieslocating, using, adapting, and sharingat both the <phrase>teacher</phrase> level (with the Teacher's <phrase>Curriculum</phrase> Assistant) and the <phrase>student</phrase> level (with the Remote <phrase>Exploratorium</phrase>). It illustrates how tools for educators and tools for students can be orchestrated into integrated classroom support. 1. Collaborative Activities Require Support The goal of encouraging groups of learners to engage collaboratively in <phrase>problem-solving</phrase> activities has much merit. Social interaction fosters <phrase>deep learning</phrase> in which students develop intellectual structures that allow them to create their own <phrase>knowledge</phrase> [27]. It promotes <phrase>social skills</phrase> that help people participate in the social <phrase>construction</phrase> of their shared <phrase>reality</phrase> [3]. It increases <phrase>student</phrase> engagement and brings out the relevance of learning [16]. It allows the educational process to be more <phrase>student</phrase>-centered, less disciplinary, and more exciting [14, 15]. The use of <phrase>technology</phrase> to foster <phrase>collaborative learning</phrase> is often seen as a key to reforming <phrase>science</phrase> educationon the principle that the best way to learn <phrase>science</phrase> is to engage in the practice of <phrase>science</phrase> [10]. The practices of modern <phrase>science</phrase> involve the use of <phrase>technologic</phrase> tools for: observing and measuring interesting phenomena in the world, generating representations and visualizations of the <phrase>data</phrase>, and creating simulations to understand observed processes and to <phrase>test</phrase> hypotheses. Importantly, the practice of modern <phrase>science</phrase> is highly collaborative. Scientists work together to incrementally <phrase>design</phrase> experiments and simulations, to convergently develop hypotheses and theories, and to <phrase>test</phrase> and evaluate their work [17, 22]. Many projects have successfully combined these elements to foster innovative forms of collaborative <phrase>science</phrase> <phrase>education</phrase> among students [8, 12, 24, 26]. However, <phrase>research</phrase> projects have often been unable to transfer their successful <phrase>results</phrase> to other sites or schools because they did not replicate the initial <phrase>teacher</phrase> learning that occurred implicitly in the <phrase>teacher</phrase>-researcher and <phrase>teacher</phrase>-<phrase>teacher</phrase> collaborations [21]. For educational change to succeed, teachers too must be supported in changing from an isolated teaching <phrase>model</phrase> to one of <phrase>collaborative learning</phrase> with other educators [4]. We believe that for <phrase>collaborative learning</phrase> to succeed in the classroom, collaborative 
<phrase>Higher Order</phrase> <phrase>Conditional Random Fields</phrase> in <phrase>Deep Neural Networks</phrase> We address the problem of <phrase>semantic</phrase> segmentation using <phrase>deep learning</phrase>. Most segmentation systems include a <phrase>Conditional Random Field</phrase> (CRF) to produce a structured output that is consistent with the image's <phrase>visual features</phrase>. Recent <phrase>deep learning</phrase> approaches have incorporated CRFs into <phrase>Convolutional Neural Networks</phrase> (CNNs), with some even training the CRF <phrase>end-to-end</phrase> with the rest of the network. However, these approaches have not employed <phrase>higher order</phrase> potentials, which have previously been shown to significantly improve segmentation performance. In this <phrase>paper</phrase>, we demonstrate that two types of <phrase>higher order</phrase> potential, based on object detections and superpixels, can be included in a CRF embedded within a deep network. We <phrase>design</phrase> these <phrase>higher order</phrase> potentials to allow inference with the <phrase>differentiable</phrase> mean field <phrase>algorithm</phrase>. As a result, all the parameters of our richer CRF <phrase>model</phrase> can be learned <phrase>end-to-end</phrase> with our pixelwise <phrase>CNN</phrase> classifier. We achieve <phrase>state</phrase>-of-the-<phrase>art</phrase> segmentation performance on the PASCAL <phrase>VOC</phrase> benchmark with these trainable <phrase>higher order</phrase> potentials.
<phrase>Social Computing</phrase> for Educational <phrase>Knowledge</phrase> Building <phrase>Research</phrase> and development activities in <phrase>social computing</phrase>, although still in exploratory stages, have already inspired a wide array of undertakings aimed at fostering the collective engagement of <phrase>small groups</phrase> and larger collectivities. <phrase>Social computing</phrase> <phrase>research</phrase> combines theory building and <phrase>design</phrase> work; experiments supporting small social groups or large online communities create new interaction settings that allow testing and expanding theoretical perspectives. We are interested in harnessing <phrase>social computing</phrase> for educational purposes, facilitating the building of <phrase>knowledge</phrase> by distributed <phrase>small groups</phrase> of students working collaboratively. We want to understand how <phrase>knowledge</phrase>-building social activity can be accomplished and how effective <phrase>social computing</phrase> environments can be designed and deployed to promote <phrase>collaborative learning</phrase>. Our project, the Virtual <phrase>Math</phrase> Teams (VMT) project at mathforum.org, is an attempt to develop a <phrase>social computing</phrase> environment that promotes scaffolded discourse about <phrase>mathematics</phrase> among teens and to study the forms of collective interaction that take place there. The VMT project is based upon an evolving theory of <phrase>group cognition</phrase>. This theory hypothesizes that " <phrase>small groups</phrase> are the engines of <phrase>knowledge</phrase> building. The knowing that groups build up in <phrase>manifold</phrase> forms is what becomes internalized by their members as individual learning and externalized in their communities as certifiable <phrase>knowledge</phrase> " (Stahl, 2006, p. 16). The VMT project is an ongoing effort to catalyze and promote a <phrase>math</phrase> discourse <phrase>community</phrase>. Starting very simply in 2003 from a successful online <phrase>math</phrase> problem-of-the-week service (mathforum.org/<phrase>pow</phrase>/) and taking advantage of popular off-the-shelf chat <phrase>software</phrase> to make it collaborative, we have since then gradually evolved a more sophisticated environment involving carefully scripted pedagogical interventions, <phrase>open-ended</phrase> <phrase>math</phrase> issues and custom softwareguided by extensive analysis of <phrase>student</phrase> behaviors through cycles of trials. We now want to strengthen its <phrase>social networking</phrase> supports to facilitate wider <phrase>adoption</phrase> and to further our <phrase>research</phrase> agenda. While the ubiquity of networked <phrase>computers</phrase> connected through the <phrase>Internet</phrase> from homes and schools creates an exciting opportunity for students around the world to explore <phrase>math</phrase> together, the practical difficulties are enormous. We are interested in facilitating the development of <phrase>high</phrase>-level thinking skills and the <phrase>deep understanding</phrase> that comes from engaging in effective dialog and merging personal perspectives, but we find that students are accustomed to using text chat and the <phrase>Internet</phrase> for superficial socializing. Furthermore, their habits of learning are overwhelmingly skewed toward passive acquisition of <phrase>knowledge</phrase> from authority sources like teachers and books, rather than from self-regulated or collaborative inquiry. Finally, attempts 
Segmentation and Interpretation of MR <phrase>Brain</phrase> Images: An Improved Active Shape <phrase>Model</phrase> This <phrase>paper</phrase> reports a novel method for fully automated segmentation that is based on description of shape and its variation using point distribution models (PDM's). An improvement of the active shape procedure introduced by Cootes and Taylor to find new examples of previously learned shapes using PDM's is presented. The new method for segmentation and interpretation of deep neuroanatomic structures such as <phrase>thalamus</phrase>, <phrase>putamen</phrase>, ventricular system, etc. incorporates a priori <phrase>knowledge</phrase> about shapes of the neuroanatomic structures to provide their robust segmentation and labeling in <phrase>magnetic resonance</phrase> (MR) <phrase>brain</phrase> images. The method was trained in eight MR <phrase>brain</phrase> images and tested in 19 <phrase>brain</phrase> images by comparison to <phrase>observer</phrase>-defined <phrase>independent</phrase> standards. Neuroanatomic structures in all testing images were successfully identified. Computer-identified and <phrase>observer</phrase>-defined neuroanatomic structures agreed well. The <phrase>average</phrase> labeling error was 7%+/-3%. Border positioning errors were quite small, with the <phrase>average</phrase> border positioning error of 0.8+/-0.1 <phrase>pixels</phrase> in 256 x 256 MR images. The presented method was specifically developed for segmentation of neuroanatomic structures in MR <phrase>brain</phrase> images. However, it is generally applicable to virtually any task involving deformable shape analysis.
3d <phrase>Virtual Worlds</phrase>: Assessing the Experience and Informing <phrase>Design</phrase> <phrase>Eye Tracking</phrase> <phrase>Technology</phrase> Capturing <phrase>Eye-tracking</phrase> <phrase>Data</phrase> <phrase>Eye-tracking</phrase> <phrase>Data Analysis</phrase> <phrase>Eye Tracking</phrase> and 3d Worlds Experiment <phrase>Eye-tracking</phrase> Apparatus Stimulus Material Protocol <phrase>Design</phrase> INTRODUCTION Over one billion hours of collaborative <phrase>play</phrase> have been logged in the <phrase>game</phrase> Halo (Grossman, 2007). Put into easily understood commercial terms, that's about 526,315 person years of work effort; or the entire <phrase>population</phrase> of <phrase>Ljubljana</phrase>, <phrase>Slovenia</phrase> playing Halo instead of going to work, <phrase>school</phrase> or anywhere else during the day for an entire year. This <phrase>milestone</phrase> underscores the increas-ABSTRACT Teams BLOCKINmeet BLOCKINin BLOCKIN3D BLOCKINvirtual BLOCKINworlds BLOCKINmore BLOCKINfrequently BLOCKINthan BLOCKINever BLOCKINbefore, BLOCKINyet BLOCKINthe BLOCKINtools BLOCKINfor BLOCKINevaluating BLOCKIN3D BLOCKINcollaboration environments BLOCKINare BLOCKINunderdeveloped. ing acceptance of <phrase>virtual worlds</phrase> as legitimate, compelling places for humans to interact with one another, making it likely that 3D interaction technologies will be adopted beyond the realm of <phrase>play</phrase>. In fact, there is evidence that 3D <phrase>virtual worlds</phrase> are becoming a prominent framework for <phrase>human</phrase>-computer interaction (HCI) to support distributed, collaborative work (Kaptelinin & Czerwinski, 2007). Much of what is known about HCI in 3D emerges from the study of <phrase>video games</phrase>. Qualitative studies of how gamers experience <phrase>virtual worlds</phrase> show that new <phrase>games</phrase> are adopted faster if they follow familiar interaction styles (Clarke & Duimering, 2006), and that <phrase>game</phrase> <phrase>play</phrase> sometimes leads to a new category of <phrase>virtual community</phrase> (Nardi & Harris, 2006). These studies provide preliminary guidance for HCI designers searching for new <phrase>metaphors</phrase> that might support deeper engagement in collab-orative work among geographically <phrase>dispersed</phrase> Designing <phrase>software</phrase> for distributed group work is recognized as a <phrase>wicked problem</phrase> with many challenging dimensions (Fitzpatrick, 1998). Grudin (1994) identified eight of the most significant challenges of designing <phrase>software</phrase> for collaborative work. Each of Grudin's eight challenges takes a slightly different form as <phrase>technology</phrase> changes. In this <phrase>paper</phrase>, we specifically address one of Grudin's eight challenges for the 3D generation of collaboration technologies: Evaluation. Before explaining how evaluation of 3D environments might be different than evaluation of other types of collaborative work systems, we need to understand how users experience 3D collaboration differently. Dyck et al.'s (2003) analyses of gaming as an interaction <phrase>metaphor</phrase> reveals the core dimensions of effortless <phrase>community</phrase>, learning by watching, deep customizability and <phrase>fluid</phrase> system-<phrase>human</phrase> interaction. These dimensions of 3D <phrase>games</phrase> present an opportunity for 3D collaboration <phrase>software</phrase> designers to transfer ideas from gaming (Rapeepisarn et al., 2006). Wrapped up with this opportunity are challenges to <phrase>long</phrase> accepted heuristics for <phrase>interface design</phrase>, including simplicity , consistency & ease of use for all users. Dyck et al (2003) describe how these accepted HCI <phrase>design</phrase> heuristics 
<phrase>Siamese</phrase> <phrase>Neural Networks</phrase> for One-<phrase>shot</phrase> <phrase>Image Recognition</phrase> <phrase>Siamese</phrase> <phrase>Neural Networks</phrase> for One-<phrase>shot</phrase> <phrase>Image Recognition</phrase> 2015 The process of learning good features for <phrase>machine learning</phrase> applications can be very computationally expensive and may prove difficult in cases where little <phrase>data</phrase> is available. A prototypical example of this is the one-<phrase>shot</phrase> learning setting, in which we must correctly make predictions given only a <phrase>single</phrase> example of each new class. In this <phrase>paper</phrase>, we explore a method for learning <phrase>siamese</phrase> <phrase>neural networks</phrase> which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful <phrase>discriminative features</phrase> to generalize the predictive power of the network not just to new <phrase>data</phrase>, but to entirely new classes from unknown <phrase>distributions</phrase>. Using a convolutional <phrase>architecture</phrase>, we are able to achieve strong <phrase>results</phrase> which exceed those of other <phrase>deep learning</phrase> models with near <phrase>state</phrase>-of-the-<phrase>art</phrase> performance on one-<phrase>shot</phrase> <phrase>classification tasks</phrase>.
A Specialization in <phrase>Information</phrase> and <phrase>Knowledge Management</phrase> Systems for the <phrase>Undergraduate</phrase> <phrase>Computer Science</phrase> <phrase>Curriculum</phrase> We describe our progress extending the <phrase>undergraduate</phrase> <phrase>Computer Science</phrase> (CS) <phrase>curriculum</phrase> to include a <phrase>deep understanding</phrase> of techniques for <phrase>information</phrase> and <phrase>knowledge management</phrase> systems (IKMS). In a novel five-course <phrase>sequence</phrase>, students build and work with techniques for <phrase>data mining</phrase>, <phrase>information retrieval</phrase> , and text analysis, and develop a <phrase>large-scale</phrase> IKMS project. We teach in a hands-on lab setting where students use tools they have built, performing experiments that could extend the field. Hence undergraduates have firsthand <phrase>knowledge</phrase> of performing CS <phrase>research</phrase> using scientific methods. Second, we utilize a rigorous set of evaluation criteria developed in our <phrase>Psychology</phrase> Institute to evaluate how well <phrase>students learn</phrase> using our approaches. Ultimately , we believe that this specialization warrants inclusion as an option in the standard <phrase>undergraduate</phrase> CS <phrase>curriculum</phrase>. 1 INTRODUCTION Our objective is to increase the <phrase>real-world</phrase> relevance of the <phrase>undergraduate</phrase> CS <phrase>curriculum</phrase> by including a specialization option in <phrase>Information</phrase> and <phrase>Knowledge Management</phrase> Systems (IKMS). The ever-increasing availability of <phrase>data</phrase> of all types in corporate settings and on the web, makes <phrase>information</phrase> and <phrase>knowledge management</phrase> a key need of today's <phrase>information society</phrase>. The core areas of this emerging subdis-cipline are text analysis, <phrase>data mining</phrase>, <phrase>information retrieval</phrase>, and <phrase>database systems</phrase>, all of which are connected through a need for understanding the fundamentals of <phrase>algorithms</phrase>, <phrase>statistics</phrase> , <phrase>machine learning</phrase>, and <phrase>human</phrase>-computer interaction. The CS specialization we have designed consists in total of five <phrase>upper</phrase>-level <phrase>undergraduate</phrase> courses (Fig. 1). Theory and applications of the four core areas required for IKMS are taught in: <phrase>Database</phrase> <phrase>Organization</phrase>, a standard course in <phrase>relational databases</phrase>, <phrase>Information Retrieval</phrase> and <phrase>Data Mining</phrase>, two successful new courses we have developed over the last three years [12, 11], and Intelligent Text Analysis, our newly developed course in applied <phrase>natural language processing</phrase>. The fifth, capstone, course, <phrase>Information</phrase> and <phrase>Knowledge Management</phrase> Systems, is a project course in which <phrase>student</phrase> groups are guided in building systems for realistic <phrase>knowledge management</phrase> applications, combining development of new <phrase>software</phrase> systems with use of existing technologies. This course will integrate what students have learned in previous courses, providing systems development and <phrase>project management</phrase> experience.
Retweet Prediction with Attention-based <phrase>Deep Neural Network</phrase> On <phrase>Twitter</phrase>-like <phrase>social media</phrase> sites, the re-posting statuses or <phrase>tweets</phrase> of other users are usually considered to be the key mechanism for spreading <phrase>information</phrase>. How to predict whether a <phrase>tweet</phrase> will be retweeted by a user has received increasing attention in recent years. Previous methods studied the problem using various <phrase>linguistic</phrase> features, personal <phrase>information</phrase> of users, and many other manually constructed features to achieve the task. Usually, feature <phrase>engineering</phrase> is a laborious task, we require to obtain the external sources and they are difficult or not always available. Recently, <phrase>deep learning</phrase> methods have been used in the <phrase>industry</phrase> and <phrase>research</phrase> <phrase>community</phrase> for their ability to learn optimal features automatically and in many tasks, <phrase>deep learning</phrase> methods can achieve <phrase>state</phrase>-of-the <phrase>art</phrase> performance, such as <phrase>natural language processing</phrase>, <phrase>computer vision</phrase>, <phrase>image classification</phrase> and so on. In this work, we proposed a novel attention-based <phrase>deep neural network</phrase> to incorporate contextual and social <phrase>information</phrase> for this task. We used embeddings to represent the user, the user's attention interests, the <phrase>author</phrase> and <phrase>tweet</phrase> respectively. To <phrase>train</phrase> and evaluate the proposed methods, we also constructed a large dataset collected from <phrase>Twitter</phrase>. <phrase>Experimental</phrase> <phrase>results</phrase> showed that the <phrase>proposed method</phrase> could achieve better <phrase>results</phrase> than the previous <phrase>state</phrase>-of-the-<phrase>art</phrase> methods.
A new way to search <phrase>game</phrase> <phrase>trees</phrase>: technical perspective in 1994, The program <phrase>Chinook</phrase> won the title " Man-Machine World Champi-on'' in checkers from Marion Tinsley. Shortly thereafter, in 1997, the program <phrase>Deep Blue</phrase> bested the <phrase>human</phrase> <phrase>chess</phrase> champion <phrase>Garry Kasparov</phrase>. This past year, <phrase>IBM</phrase> researchers set the stakes even higher: on network <phrase>TV</phrase>, their program Watson defeated two <phrase>human</phrase> champions in the <phrase>game</phrase> show " <phrase>Jeopardy</phrase>!. " Researchers in <phrase>artificial intelligence</phrase> (<phrase>AI</phrase>) are buoyed by these achievements, but they also know that <phrase>computers</phrase> still lag far behind <phrase>human</phrase> levels of <phrase>play</phrase> in many other <phrase>games</phrase> of strategy. One such contest of wits is Go, an ancient two-player <phrase>board game</phrase> from <phrase>China</phrase>. With simple rules but dizzyingly complex strategies, the <phrase>game</phrase> of Go remains a stronghold of <phrase>human</phrase> superiority, thwarting the kinds of approaches that have <phrase>proven</phrase> so successful in checkers and <phrase>chess</phrase>. That may be about to change. After decades of halting progress in computer Go, variations of <phrase>Monte Carlo</phrase> <phrase>tree</phrase> search (MCTS) have shown themselves to be a real, well, <phrase>game</phrase> chang-<phrase>er</phrase>. MCTS is an approach to <phrase>decision making</phrase> that uses random sampling to quickly distinguish strong and weak moves. The following <phrase>paper</phrase> documents the dramatic breakthroughs achieved by applying MCTS to Go. Why do these <phrase>results</phrase> <phrase>matter</phrase> for the broader field of <phrase>AI</phrase>? The <phrase>history</phrase> of <phrase>AI</phrase> has shown that <phrase>algorithms</phrase> and ideas pioneered in the context of <phrase>game</phrase> playing have a wide <phrase>range</phrase> of important applications. Insights from computer <phrase>game</phrase> playing have often constituted the " 1% inspiration " in Edison's formula for creative success. Combined with the " 99% <phrase>perspiration</phrase> " of careful <phrase>engineering</phrase>, they underpin a vast number of systems in common use today. In addition to the potential applications of MCTS, the more immediate achievement is impressive in its own right. The authors describe a line of work that, over five years, has halved the number of rating levels between today's <phrase>computers</phrase> and the world's top <phrase>human</phrase> players. Why has it been so difficult for <phrase>computers</phrase> to <phrase>play</phrase> world-class Go? In a sense, writing a program for <phrase>game</phrase> playing is easy: When it's your turn, make the best move. Of course, picking that move is where the real challenge comes in. In the field of <phrase>reinforcement learning</phrase> that gave rise to the key ideas behind MCTS, a rule for choosing a move as a <phrase>function</phrase> of the <phrase>game</phrase> <phrase>state</phrase> is known as a policy. A rule for evaluating a <phrase>game</phrase> <phrase>state</phrase> to 
<phrase>Deep Learning</phrase> Based <phrase>Semantic</phrase> <phrase>Video</phrase> Indexing and Retrieval We share the implementation details and testing <phrase>results</phrase> for <phrase>video</phrase> retrieval system based exclusively on features extracted by <phrase>convolutional neural networks</phrase>. We show that deep <phrase>learned features</phrase> might serve as <phrase>universal</phrase> signature for <phrase>semantic</phrase> content of <phrase>video</phrase> useful in many search and retrieval tasks. We further show that <phrase>graph</phrase>-based storage structure for <phrase>video</phrase> index allows to efficiently retrieving the content with complicated spatial and temporal search queries.
Time window control: a <phrase>model</phrase> for <phrase>cerebellar</phrase> <phrase>function</phrase> based on synchronization, <phrase>reverberation</phrase>, and time slicing. We present a new <phrase>hypothesis</phrase> of <phrase>cerebellar</phrase> <phrase>function</phrase> that is based on synchronization, delayed <phrase>reverberation</phrase>, and time <phrase>windows</phrase> for triggering spikes. Our <phrase>model</phrase> suggests that granule cells admit mossy fiber activity to the parallel fibers only if the <phrase>Golgi</phrase> cells are firing synchronously and if the mossy-fiber spikes arrive within <phrase>short</phrase> and well-defined time <phrase>windows</phrase>. The concept of time window control organizes <phrase>neuronal</phrase> activity in discrete 'time slices' that can be used to discern meaningful <phrase>information</phrase> from background noise. In particular, <phrase>Purkinje cell</phrase> activity can trigger rebound spikes in deep <phrase>cerebellar</phrase> nuclei cells, which project via <phrase>brain stem</phrase> nuclei and mossy fibers back to the <phrase>cerebellar cortex</phrase>. Using a detailed <phrase>model</phrase> of deep <phrase>cerebellar</phrase> nuclei cells, we demonstrate that the delayed firing of rebound spikes is a robust mechanism so as to ensure that the reverberated activity re-arrives in the mossy fibers just during the <phrase>granule-cell</phrase> time window. Large network simulations reveal that <phrase>synaptic plasticity</phrase> (LTD and <phrase>LTP</phrase>) at the parallel fiber/<phrase>Purkinje cell</phrase> <phrase>synapses</phrase> that relies on the timing of the parallel fiber and climbing fiber activities allows the system to learn, store, and recall spatiotemporal patterns of spike activity. Climbing fiber spikes <phrase>function</phrase> both as <phrase>teacher</phrase> and as synchronization signals. The temporal characteristics of the climbing fiber activity are due to intrinsic oscillatory properties of inferior olivary <phrase>neurons</phrase> and to reverberating projections between deep <phrase>cerebellar</phrase> nuclei, the mesodiencephalic junction, and the inferior <phrase>olive</phrase>. Thus, the reverberating loops of the mossy fiber system and climbing fiber system may interact directly with the time <phrase>windows</phrase> provided by the circuitry of the <phrase>cerebellar cortex</phrase> so as to generate the appropriate <phrase>spatio-temporal</phrase> firing patterns in the deep <phrase>cerebellar</phrase> nuclei <phrase>neurons</phrase> that control premotor systems. In future studies the <phrase>model</phrase> will be extended in that <phrase>high</phrase> <phrase>frequency</phrase> simple spike activities will be included and that their relevance for <phrase>motor control</phrase> will be addressed.
Using a <phrase>Deep Reinforcement Learning</phrase> Agent for <phrase>Traffic Signal</phrase> Control Ensuring transportation systems are efficient is a priority for modern <phrase>society</phrase>. Technological advances have made it possible for transportation systems to collect large volumes of varied <phrase>data</phrase> on an unprecedented scale. We propose a <phrase>traffic signal</phrase> control system which takes advantage of this new, <phrase>high</phrase> quality <phrase>data</phrase>, with minimal abstraction compared to other proposed systems. We apply modern <phrase>deep reinforcement learning</phrase> methods to build a truly adaptive <phrase>traffic signal</phrase> control agent in the traffic microsimulator <phrase>SUMO</phrase>. We propose a new <phrase>state space</phrase>, the discrete traffic <phrase>state</phrase> encoding, which is <phrase>information</phrase> dense. The discrete traffic <phrase>state</phrase> encoding is used as input to a <phrase>deep convolutional</phrase> <phrase>neural network</phrase>, trained using Q-learning with experience replay. Our agent was compared against a one <phrase>hidden layer</phrase> <phrase>neural network</phrase> <phrase>traffic signal</phrase> control agent and reduces <phrase>average</phrase> cumulative delay by 82%, <phrase>average</phrase> queue length by 66% and <phrase>average</phrase> travel time by 20%.
Learning <phrase>Deep Boltzmann Machines</phrase> using Adaptive MCMC When modeling <phrase>high</phrase>-dimensional richly structured <phrase>data</phrase>, it is often the case that the distribution defined by the Deep <phrase>Boltzmann</phrase> Machine (<phrase>DBM</phrase>) has a rough <phrase>energy</phrase> <phrase>landscape</phrase> with many <phrase>local minima</phrase> separated by <phrase>high</phrase> <phrase>energy</phrase> barriers. The commonly used Gibbs sampler tends to get trapped in one local mode, which often <phrase>results</phrase> in unstable learning dynamics and leads to poor parameter estimates. In this <phrase>paper</phrase>, we concentrate on learning DBM's using adaptive MCMC <phrase>algorithms</phrase>. We first show a close connection between Fast PCD and adaptive MCMC. We then develop a Coupled Adaptive Simulated Tempering <phrase>algorithm</phrase> that can be used to better explore a highly multimodal <phrase>energy</phrase> <phrase>landscape</phrase>. Finally, we demonstrate that the <phrase>proposed algorithm</phrase> considerably improves parameter estimates, particularly when learning <phrase>large-scale</phrase> DBM's.
Self-instructional Teaching Module Based on <phrase>Cognitive Load</phrase> Theory Basically an instructional computer module consists of two <phrase>media</phrase> namely text and visuals. These <phrase>multiple representations</phrase> can <phrase>complement</phrase> each other, resulting in a more complete representation of an application domain than a <phrase>single</phrase> source of <phrase>information</phrase> does. According to <phrase>cognitive load</phrase> theory, instruction needs to be designed in a manner that facilitates the acquisition of <phrase>knowledge</phrase> in <phrase>long-term memory</phrase> while reducing unnecessary demands on the <phrase>working memory</phrase>. Presenting <phrase>information</phrase> in a way that <phrase>cognitive load</phrase> <phrase>falls</phrase> within the limitations of <phrase>working memory</phrase> can improve speed and accuracy of understanding, and facilitate <phrase>deep understanding</phrase> of <phrase>information</phrase> content. The aim of this study is to produce a self-instructional teaching module based on <phrase>Cognitive Load</phrase> Theory with principles of <phrase>minimalism</phrase> in an attempt for easier and faster learning and comprehension among <phrase>teacher</phrase> trainees. The module will be compared to the conventional instruction in terms of achievements based on the time of response and <phrase>information</phrase> retention. It is expected that the <phrase>results</phrase> would reveal that the cognitively guided module which physically integrate text and visuals would show to be far <phrase>superior</phrase> a learning tool than the conventional computer module instruction. 1.0 Introduction Malaysia's embarkation on Vision 2020 towards achieving the status of a developed nation has been a move made to meet the challenges of a <phrase>high</phrase>-tech, fast-paced and <phrase>information</phrase>-oriented <phrase>society</phrase>. In line with the nation's drive to fulfil the vision, <phrase>education</phrase> has played an important role, steering systematically from rote-and exam-oriented learning towards <phrase>knowledge</phrase>-and <phrase>technology</phrase>-based learning. The Smart <phrase>School</phrase> Concept <phrase>launched</phrase> in the year 1999 has reinvented the <phrase>teaching and learning</phrase> and <phrase>school</phrase> <phrase>management</phrase> to prepare students for the <phrase>Information Age</phrase>. <phrase>Information</phrase> and <phrase>Communication</phrase> <phrase>Technology</phrase> (ICT) is continually emphasized and seen as a tool to revolutionise learning, producing richer curricula and enhancing pedagogies. All teaching institutions in the <phrase>country</phrase> were <phrase>directed</phrase> to be well-equipped with ICT networks to ensure that future teachers are aware of the <phrase>recent developments</phrase> in ICT and are able to acquire the necessary <phrase>knowledge</phrase> and skills in <phrase>order</phrase> to prepare themselves for the technological advancement at <phrase>school</phrase> levels (Jamalludin and Zaidatun, 2003). Teachers being the most significant and costly resource in schools, are central to <phrase>school</phrase> improvement efforts.
A scalable and <phrase>topology</phrase> configurable protocol for distributed parameter synchronization This <phrase>paper</phrase> addresses the problem of <phrase>model</phrase> synchronization in <phrase>data</phrase>-parallelism of <phrase>deep-learning</phrase> systems. In such systems, workers on different machines continuously update their local copies of the <phrase>model</phrase>, and the updates need to be merged so that the copies are roughly consistent to each other. In modern implementations using <phrase>GPUs</phrase>, workers generate very <phrase>high</phrase> updates, posing significant <phrase>scalability</phrase> challenges. We <phrase>model</phrase> this as a distributed <phrase>state</phrase> <phrase>anti</phrase>-<phrase>entropy</phrase> problem, and propose a fully asynchronous and decentralized parameter sharing protocol that allows machines to reconcile <phrase>model</phrase> differences to their connected <phrase>peers</phrase>. The protocol is provably correct to achieve <phrase>eventual consistency</phrase>, and recovers gracefully from common failures without <phrase>invasive</phrase> maneuver. In addition, it is flexible in that both different consistency requirements and <phrase>topology</phrase> connections can be dynamically reconfigured. We show that such flexibility is important for better <phrase>trade</phrase>-off between system performance and <phrase>model</phrase> convergence rate. For example, under <phrase>high</phrase> update rates, the existing <phrase>master</phrase>-<phrase>slave</phrase> <phrase>architecture</phrase> performs poorly against other configurations with larger <phrase>diameter</phrase>.
Relation Classification via <phrase>Recurrent Neural Network</phrase> <phrase>Deep learning</phrase> has gained much success in sentence-level relation classification. For example, <phrase>convolutional neural networks</phrase> (<phrase>CNN</phrase>) have delivered competitive performance without much effort on feature <phrase>engineering</phrase> as the conventional pattern-based methods. Thus a lot of works have been <phrase>produced</phrase> based on <phrase>CNN</phrase> structures. However, a key issue that has not been well addressed by the <phrase>CNN</phrase>-based method is the lack of capability to learn temporal features, especially <phrase>long</phrase>-distance dependency between nominal pairs. In this <phrase>paper</phrase>, we propose a simple framework based on <phrase>recurrent neural networks</phrase> (RNN) and compare it with <phrase>CNN</phrase>-based <phrase>model</phrase>. To show the limitation of popular used SemEval-2010 Task 8 dataset, we introduce another dataset refined from MIML-RE(Angeli et al., 2014). Experiments on two different datasets strongly indicates that the RNN-based <phrase>model</phrase> can deliver better performance on relation classification , and it is particularly capable of learning <phrase>long</phrase>-distance relation patterns. This makes it suitable for <phrase>real-world</phrase> applications where complicated expressions are often involved.
Quantification via <phrase>Probability</phrase> Estimators Quantification is the name given to a novel <phrase>machine learning</phrase> task which deals with correctly estimating the number of elements of one class in a set of examples. The output of a quantifier is a real value; since training instances are the same as a <phrase>classification problem</phrase>, a natural approach is to <phrase>train</phrase> a classifier and to derive a quantifier from it. Some previous works have shown that just classifying the instances and counting the examples belonging to the class of interest (classify & <phrase>count</phrase>) typically yields bad quantifiers, especially when the class distribution may vary between training and <phrase>test</phrase>. Hence, adjusted versions of classify & <phrase>count</phrase> have been developed by using modified thresholds. However, previous works have explicitly discarded (without a deep analysis) any possible approach based on the <phrase>probability</phrase> estimations of the classifier. In this <phrase>paper</phrase>, we present a method based on averaging the <phrase>probability</phrase> estimations of a classifier with a very simple scaling that does perform reasonably well, showing that <phrase>probability</phrase> estimators for quantification capture a richer view of the problem than methods based on a threshold.
Unsupervised and <phrase>transfer learning</phrase> challenge We organized a <phrase>data mining</phrase> challenge in " unsu-pervised and <phrase>transfer learning</phrase> " (the UTL challenge), in collaboration with the <phrase>DARPA</phrase> <phrase>Deep Learning</phrase> program. The goal of this year's challenge was to learn good <phrase>data</phrase> representations that can be re-used across tasks by building models that capture regularities of the input space. The representations provided by the participants were evaluated by the organizers on <phrase>supervised learning</phrase> " <phrase>target</phrase> tasks " , which were unknown to the participants. In a first phase of the challenge, the competitors were given only <phrase>unlabeled data</phrase> to learn their <phrase>data</phrase> representation. In a second phase of the challenge, the competitors were also provided with a limited amount of <phrase>labeled data</phrase> from " source tasks " , distinct from the " <phrase>target</phrase> tasks ". We made available <phrase>large datasets</phrase> from various application domains: <phrase>handwriting recognition</phrase>, <phrase>image recognition</phrase>, <phrase>video processing</phrase>, text processing, and <phrase>ecology</phrase>. The <phrase>results</phrase> indicate that learned <phrase>data</phrase> representation yield <phrase>results</phrase> significantly better than what can be achieved with raw <phrase>data</phrase> or <phrase>data</phrase> preprocessed with standard normalizations and functional transforms. The UTL challenge is part of the IJCNN 2011 competition program 1. The <phrase>website</phrase> of the challenge remains open for submission of new methods beyond the termination of the challenge as a resource for students and researchers 2 .
Fast Optimization of Non-convex <phrase>Machine Learning</phrase> Objectives In this project we examined the problem of non-<phrase>convex optimization</phrase> in the context of <phrase>Machine Learning</phrase>, <phrase>drawing</phrase> inspiration from the increasing popularity of methods such as <phrase>Deep Belief</phrase> Networks, which involve non-convex objectives. We focused on the task of training the Neural Autoregressive Distribution <phrase>Estimator</phrase>, a <phrase>recently proposed</phrase> variant of the <phrase>Restricted Boltzmann Machine</phrase>, in applications to <phrase>density estimation</phrase>. The aim of the project was to explore the various stages involved in implementing optimization methods and choosing the appropriate one for a given task. We examined a number of optimization methods, ranging from <phrase>derivative</phrase>-<phrase>free</phrase> to second <phrase>order</phrase> and from batch to <phrase>stochastic</phrase>. We experimented with variations of these methods, presenting along the way all the <phrase>major</phrase> steps and decisions involved. The challenges of the problem included the relatively large <phrase>parameter space</phrase> and the non-convexity of the <phrase>objective function</phrase>, the large size of some of the datasets we used, the multitude of hyperparameters and decisions involved in each method, as well as the ever-present danger of <phrase>overfitting</phrase> the <phrase>data</phrase>. Our <phrase>results</phrase> show that second <phrase>order</phrase> Quasi-<phrase>Newton</phrase> batch methods like L-BFGS and variants of <phrase>stochastic</phrase> first <phrase>order</phrase> methods like Averaged <phrase>Stochastic Gradient Descent</phrase> outshine the rest of the methods we examined. Acknowledgements I would like to extend my thanks to my supervisor Dr. Iain Murray for his guidance and <phrase>patience</phrase>. Every piece of <phrase>information</phrase> he provided was to-the-point and saved me many hours of work. From theoretical background, to implementation details, to writing and <phrase>typesetting</phrase> style, the end result would be of far lesser quality, in every <phrase>aspect</phrase>, without his <phrase>feedback</phrase>. I also thank my <phrase>family</phrase> and all my <phrase>friends</phrase> for their <phrase>love</phrase> and support. Special thanks go to my <phrase>friends</phrase> Costas and Stratis who lended me not only strength but also their <phrase>computing</phrase> power for some of the experiments. Declaration I declare that this <phrase>thesis</phrase> was composed by myself, that the work contained herein is my own except where explicitly stated otherwise in the text, and that this work has not been submitted for any other <phrase>degree</phrase> or <phrase>professional</phrase> qualification except as specified.
Peer <phrase>Knowledge</phrase> Modeling in <phrase>Computer Supported</phrase> <phrase>Collaborative Learning</phrase> Learners benefit from collaboration because it triggers effective interaction processes such as externalization, elicitation and negotiation of <phrase>knowledge</phrase>. In <phrase>order</phrase> to communicate effectively, learners need to have a certain representation of their <phrase>peers</phrase>' <phrase>knowledge</phrase>. We refer to the process of building and maintaining a representation of the <phrase>peers</phrase>' <phrase>knowledge</phrase> as peer <phrase>knowledge</phrase> modeling. The present <phrase>thesis</phrase> aim at making contributions to the fields of computer support <phrase>collaborative learning</phrase> and work (CSCL, CSCW) on three main levels. First, as an empirical contribution, we investigate the process of peer <phrase>knowledge</phrase> modeling in the context of CSCL. Our main <phrase>research</phrase> question inquires the effects of a socio-<phrase>cognitive</phrase> support, providing co-learners with cues about their peer's <phrase>prior knowledge</phrase>, on <phrase>collaborative learning</phrase> outcomes and processes. In an <phrase>empirical study</phrase> (the KAT experiment), <phrase>university</phrase> students (N=64) participated in a remote computer-mediated dyadic learning scenario. They were provided (or not) with a visual representation of their partner's <phrase>prior-knowledge</phrase> level through a <phrase>Knowledge</phrase> Awareness Tool (KAT). <phrase>Results</phrase> showed that the KAT enhances co-learners' <phrase>collaborative learning</phrase> gain. This effect appears to be mediated by the positive effect of the KAT on participants' accuracy in estimating their peer's <phrase>knowledge</phrase>. Analyses on the process level showed that participants of the KAT condition produce more elaborated utterances. KAT condition dyads' interactions are more focused on <phrase>knowledge</phrase> negotiation, whereas the control condition dyads are mainly focused on task completion. The KAT seems to provide a sensitizing metacognitive support, structuring and regulating the collaboration by helping co-learners to cope with their <phrase>knowledge</phrase> gaps and discrepancies. Second, as a methodological contribution, we examine the <phrase>affordance</phrase> of dual <phrase>eye tracking</phrase> techniques as an innovative methodology to investigate, on a deeper level, the socio-<phrase>cognitive processes</phrase> underlying collaboration. We introduce <phrase>DUET</phrase> (DUal <phrase>Eye-Tracking</phrase>), a method using a multimodal technique to collect rich <phrase>data</phrase> featuring <phrase>peers</phrase>' synchronized gaze patterns, verbal interaction and potentially activities. We examine the main <phrase>research</phrase> applications of <phrase>DUET</phrase> and exemplify them with analyses conducted in the context of the KAT experiment. <phrase>DUET</phrase> method appears to be a promising technique to investigate collaborative processes on a deep level. Finally, as a third computational contribution, we built upon micro-level analyses of the verbal referencing process to introduce and <phrase>test</phrase> REGARD, a computational <phrase>model</phrase> allowing to automatically detect verbal references and locate the specific object of reference. The <phrase>results</phrase> of the <phrase>test</phrase> show a reasonably good accuracy of the REGARD <phrase>algorithm</phrase> to detect and associate verbal references to 
Learning Stable Group Invariant Representations with <phrase>Convolutional Networks</phrase> Transformation groups, such as translations or rotations, effectively express part of the variability observed in many recognition problems. The group structure enables the <phrase>construction</phrase> of invariant signal representations with appealing <phrase>mathematical</phrase> properties, where convolutions, together with pooling operators, bring stability to additive and geometric perturbations of the input. Whereas physical transformation groups are ubiquitous in image and audio applications, they do not account for all the variability of complex signal classes. We show that the invariance properties built by <phrase>deep convolutional</phrase> networks can be cast as a form of stable group invariance. The network wiring <phrase>architecture</phrase> determines the invariance group, while the trainable filter coefficients characterize the <phrase>group action</phrase>. We give explanatory examples which illustrate how the <phrase>network architecture</phrase> controls the resulting invariance group. We also explore the principle by which additional convolutional layers induce a group factorization enabling more abstract, powerful invariant representations.
HawkEye: a novel process <phrase>automation</phrase> interface Operators in the <phrase>automation</phrase> industries today have difficulties in maintaining their <phrase>situation awareness</phrase> and understanding the impact of events. Massive amounts of <phrase>data</phrase> must be perceived and made sense of in a <phrase>short</phrase> amount of time, and maintaining overview is difficult while digging deep into the details when <phrase>solving problems</phrase>. The HawkEye <phrase>prototype</phrase> described here seeks to overcome these problems by providing a zoomable interface with <phrase>animated</phrase> movement and <phrase>information</phrase> aggregation. The intentions are that the <phrase>information</phrase> layout with zooming can provide a better sense of context, the <phrase>animated</phrase> movement can support continuous learning and the <phrase>information</phrase> aggregation can help operators make sense of the events and their implications as they occur.
<phrase>Future Tense</phrase> Weeding: the Time Is Now On a recent flight from <phrase>Manchester</phrase> to <phrase>Chicago</phrase>, it occurred to me that I must have been the only person in the world who had chosen Stanley J. Slote's 1997 classic Weeding <phrase>Library</phrase> Collections: <phrase>Library</phrase> Weeding Methods for <phrase>airplane</phrase> <phrase>reading</phrase>. I can't imagine why. Who would choose <phrase>Dick Francis</phrase> or even P.J. O'Rourke over a work that begins with this choice 1787 epigraph from the Reverend <phrase>Reginald Heber</phrase>: " A small collection of well chosen books is sufficient for the <phrase>entertainment</phrase> and instruction of any man, and all else are useless <phrase>Lumber</phrase>. " Although the work is somewhat dated (" The <phrase>Book</phrase> <phrase>Card</phrase> Method " occupies an entire chapter) it remains an excellent and practical <phrase>book</phrase> in its articulation of the benefits of weeding. My pleasure in it is heightened by the fact that my copy, purchased through <phrase>abebooks</phrase> (now a <phrase>province</phrase> in Greater Ama-zonia), was actually withdrawn and discarded from Sterling Municipal <phrase>Library</phrase> in <phrase>Baytown</phrase>, <phrase>Texas</phrase>. Every <phrase>book</phrase> its reader indeed. Weeding has been much on our minds lately. In virtually all of the 80+ <phrase>libraries</phrase> with which R2 has worked closely, overcrowded stacks and storage facilities pose a significant problem. They press on the conscience like that extra ten pounds we'd like to shed, or those files we really should back up. Deep down, most <phrase>librarians</phrase> of a certain age recall the 1968 <phrase>Kent</phrase> Study at the <phrase>University</phrase> of <phrase>Pittsburgh</phrase>, which discovered that 40% of the books in <phrase>academic</phrase> <phrase>libraries</phrase> never circulate not even once. We uneasily realize that this number is probably much higher 40 years later, when so much content is available in <phrase>electronic</phrase> form. We cringe slightly at the size of our print reference and <phrase>government</phrase> documents collections, knowing these serve fewer users every year. We begin, with some misgivings, to store or withdraw those bound <phrase>journal</phrase> volumes to which we have purchased <phrase>electronic</phrase> backfile access. And, as we seek to provide the learning <phrase>commons</phrase>, collaborative study spaces, writing centers, and even cafes that please most users, we confront important questions regarding both the current and residual value of our print collections. Consider a few specific scenarios we have encountered in just the past couple of years: Shelves in the <phrase>Davidson College</phrase> <phrase>Library</phrase> are more than 90% full, and books <phrase>loom</phrase> over <phrase>browsers</phrase> in towering stacks that require <phrase>liberal</phrase> distribution of foot stools throughout the <phrase>library</phrase>. At present, the <phrase>library</phrase> has neither compact 
The <phrase>Book</phrase> Review Column 1 the <phrase>Mathematics</phrase> of Voting and Elections: a Hands-on Approach <phrase>Cryptography</phrase> and <phrase>Coding Theory</phrase> Review 2 of Random Curves: Journeys of a <phrase>Mathematician</phrase> 4 a List of Questions for Neal Koblitz In this column we review the following books. The <phrase>mathematics</phrase> behind <phrase>games</phrase> that do not involve any luck is deep. On the other hand you also want to learn how to WIN these <phrase>games</phrase>. These volumes contain several types of articles-<phrase>mathematical</phrase>, practical, natural <phrase>games</phrase>, unnatural <phrase>games</phrase>. 3. <phrase>Mathematical</phrase> Treks: From Surreal Numbers to Magic Circles by Ivars Peterson. Review by William Gasarch. A <phrase>math</phrase> <phrase>book</phrase> for the <phrase>layperson</phrase>. I would certainly give it to my great niece (she's 12). This is a <phrase>textbook</phrase> designed for a course on voting for non-<phrase>majors</phrase>. 6. Branching Programs and <phrase>Binary</phrase> Decision Diagrams: Theory and Applications by Ingo <phrase>Wegener</phrase>. Review by <phrase>Samuel Johnson</phrase>. Branching programs are a <phrase>model</phrase> of computation that we can actually prove things about! <phrase>Binary</phrase> Decision Diagrams are actually used in the <phrase>real world</phrase>. Read about both in this <phrase>book</phrase>. techniques of <phrase>number theory</phrase> and <phrase>algebra</phrase> to analyze problems arising in <phrase>cryptography</phrase> and algorithmic <phrase>number theory</phrase>. Haralambous. How did people find max and min before <phrase>calculus</phrase>? How do people even now find max and min of discrete objects? Read the <phrase>book</phrase> to find out both the <phrase>math</phrase> and the <phrase>history</phrase> of such things. 10. The Space and Motion of Communicating Agents by <phrase>Robin Milner</phrase>. Review by Nick Papanikolaou. This <phrase>book</phrase> is about Robin Milner's way to <phrase>model</phrase> many <phrase>computing</phrase> agents using bigraphs. 7. Origins and Foundations of <phrase>Computing</phrase> by Frederich L. Bauer in cooperation with <phrase>Heinz</phrase> Nixdorf MuseumsForum (that is not a typo, its really one word). Neal Koblitz is a <phrase>mathematician</phrase> who works on <phrase>Number Theory</phrase> and <phrase>Cryptography</phrase>. To quote <phrase>Wikipedia</phrase> The use of <phrase>Elliptic Curves</phrase> in <phrase>Cryptography</phrase> was suggested independently by Neal Koblitz and Victor S. Miller. However, this is not a <phrase>math</phrase> <phrase>book</phrase>. The first two words of the title are <phrase>math</phrase>-words, but the rest of the title gives away the content: Random Curves: Journeys of a <phrase>Mathematician</phrase>. This is Neal Koblitz's <phrase>autobiography</phrase>. Why did Neal Koblitz write an <phrase>autobiography</phrase>? Why did <phrase>Springer</phrase> publish it? Why did I read it (that one is easy I got a <phrase>free</phrase> copy)? Should you read it? This review will give you enough <phrase>information</phrase> to decide for yourself. Neal Koblitz has <phrase>lead</phrase> an interesting <phrase>life</phrase> and he is an excellent <phrase>writer</phrase>. The <phrase>book</phrase> has very little <phrase>mathematics</phrase> in it (that may be a corollary to saying that he has <phrase>lead</phrase> an interesting <phrase>life</phrase>). He has traveled a lot and has many opinions, mostly 
Learning to <phrase>Label</phrase> Aerial Images from Noisy <phrase>Data</phrase> When training a system to <phrase>label</phrase> images, the amount of <phrase>labeled training</phrase> <phrase>data</phrase> tends to be a limiting factor. We consider the task of learning to <phrase>label</phrase> aerial images from existing maps. These provide abundant <phrase>labels</phrase>, but the <phrase>labels</phrase> are often incomplete and sometimes poorly registered. We propose two robust loss functions for dealing with these kinds of <phrase>label</phrase> noise and use the loss functions to <phrase>train</phrase> a <phrase>deep neural network</phrase> on two challenging aerial image datasets. The robust loss functions <phrase>lead</phrase> to big improvements in performance and our best system substantially outperforms the best <phrase>published results</phrase> on the task we consider.
<phrase>Engineering</phrase> Experience <phrase>Amazon</phrase> | <phrase>Software Engineering</phrase> Intern <phrase>Amazon</phrase> | <phrase>Software Engineering</phrase> Intern My <phrase>research</phrase> spans the theoretical and applied aspects of <phrase>statistical machine learning</phrase>, with an emphasis on deep networks and <phrase>latent variable</phrase> models for text and network <phrase>data</phrase>. Past projects have involved topic models for <phrase>bibliometrics</phrase>, <phrase>Bayesian</phrase> <phrase>neural networks</phrase>, and nonparametric models for distributed representations. Investigated word embeddings and their application to <phrase>Information Retrieval</phrase> under the supervision of Nick Craswell (Bing) and Rich Caruana (MSR). Publication in submission. Experimentally evaluated <phrase>neural network</phrase> architectures for <phrase>movie</phrase> and <phrase>book</phrase> recommendations. Autoencoders, supervised deep networks, and <phrase>recurrent neural networks</phrase> were investigated. Developed statistical <phrase>learning algorithms</phrase> for predicting a customer's <phrase>gender</phrase> and age from their purchase <phrase>history</phrase> for the purpose of generating apparel recommendations. <phrase>Patent</phrase> pending. Created a <phrase>Java</phrase>-based service to store and manage the preparation instructions for the millions of <phrase>products</phrase> that <phrase>pass</phrase> through Amazon's fulfillment centers.
<phrase>High</phrase>-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis <phrase>Recent advances</phrase> in <phrase>deep learning</phrase> have shown exciting promise in filling large holes in <phrase>natural images</phrase> with semantically plausible and context aware details, impact-ing fundamental image manipulation tasks such as object removal. While these learning-based methods are significantly more effective in capturing <phrase>high-level</phrase> features than prior techniques, they can only handle very low-resolution inputs due to <phrase>memory</phrase> limitations and difficulty in training. Even for slightly larger images, the inpainted regions would appear blurry and unpleasant boundaries become visible. We propose a multi-scale neural patch synthesis approach based on joint optimization of image content and texture constraints, which not only preserves contextual structures but also produces <phrase>high</phrase>-<phrase>frequency</phrase> details by matching and adapting patches with the most similar mid-layer feature correlations of a deep classification network. We evaluate our method on the ImageNet and <phrase>Paris</phrase> Streetview datasets and achieved <phrase>state</phrase>-of-the-<phrase>art</phrase> inpainting accuracy. We show our approach produces sharper and more coherent <phrase>results</phrase> than prior methods, especially for <phrase>high</phrase>-resolution images.
<phrase>Large-Scale</phrase> <phrase>Deep Belief</phrase> Nets With <phrase>MapReduce</phrase> <phrase>Deep belief</phrase> nets (DBNs) with <phrase>restricted Boltzmann machines</phrase> (RBMs) as the <phrase>building block</phrase> have recently attracted wide attention due to their great performance in various applications. The learning of a DBN starts with pretraining a series of the RBMs followed by <phrase>fine-tuning</phrase> the whole net using <phrase>backpropagation</phrase>. Generally, the sequential implementation of both RBMs and <phrase>backpropagation</phrase> <phrase>algorithm</phrase> takes significant amount of computational time to process massive <phrase>data</phrase> sets. The emerging <phrase>big data</phrase> learning requires <phrase>distributed computing</phrase> for the DBNs. In this <phrase>paper</phrase>, we present a distributed learning <phrase>paradigm</phrase> for the RBMs and the <phrase>backpropagation</phrase> <phrase>algorithm</phrase> using <phrase>MapReduce</phrase>, a popular <phrase>parallel programming</phrase> <phrase>model</phrase>. Thus, the DBNs can be trained in a distributed way by stacking a series of distributed RBMs for pretraining and a distributed <phrase>backpropagation</phrase> for <phrase>fine-tuning</phrase>. Through validation on the benchmark <phrase>data</phrase> sets of various practical problems, the <phrase>experimental</phrase> <phrase>results</phrase> demonstrate that the distributed RBMs and DBNs are amenable to <phrase>large-scale</phrase> <phrase>data</phrase> with a good performance in terms of accuracy and efficiency.
Web-Prospector - An Automatic, Site-Wide <phrase>Wrapper Induction</phrase> Approach for Scientific <phrase>Deep-Web</phrase> <phrase>Databases</phrase> <phrase>Wrapper induction</phrase> techniques traditionally focus on learning wrappers based on examples from one class of <phrase>Web pages</phrase>, i.e. from <phrase>Web pages</phrase> that are all similar in structure and content. Thereby, traditional <phrase>wrapper induction</phrase> targets the understanding of <phrase>Web pages</phrase> generated from a <phrase>database</phrase> using the same generation template as observed in the example set. Applying such techniques to <phrase>Web sites</phrase> generated from biological <phrase>databases</phrase>, however, we found that there is a need for wrapping of structurally diverse <phrase>web pages</phrase> from multiple classes making the problem more challenging. Furthermore, we observed that such scientific <phrase>web sites</phrase> do not just provide mere <phrase>data</phrase>, but they also tend to provide schema <phrase>information</phrase> in terms of <phrase>data</phrase> <phrase>labels</phrase> giving further cues for solving the <phrase>web site</phrase> wrapping task. In this <phrase>paper</phrase> we present a novel approach to automatic <phrase>information extraction</phrase> from whole <phrase>Web sites</phrase> that considers the novel challenge and takes advantage of the additional clues commonly available in scientific <phrase>deep Web</phrase> <phrase>databases</phrase>. The <phrase>solution</phrase> consists of a <phrase>sequence</phrase> of steps: 1. classification of similar <phrase>Web pages</phrase> into classes, 2. discovery of these classes and 3. <phrase>wrapper induction</phrase> for each class. Our approach thus allows us to perform unsupervised <phrase>information retrieval</phrase> from across an entire <phrase>Web site</phrase>. We <phrase>test</phrase> our <phrase>algorithm</phrase> against three <phrase>real-world</phrase> biochemical <phrase>deep Web</phrase> sources and <phrase>report</phrase> our preliminary <phrase>results</phrase>, which are very promising.
Out-of-context <phrase>noun phrase</phrase> <phrase>semantic</phrase> interpretation with cross-<phrase>linguistic</phrase> evidence The acquisition of <phrase>semantic</phrase> <phrase>knowledge</phrase> is <phrase>paramount</phrase> for any application that requires a <phrase>deep understanding</phrase> of <phrase>natural language</phrase> text. Motivated by the problem of building a <phrase>noun phrase</phrase>-level <phrase>semantic</phrase> parser and adapting it to various applications, such as <phrase>machine translation</phrase> and multilingual <phrase>question answering</phrase>, in this <phrase>paper</phrase> we present a <phrase>domain-independent</phrase> <phrase>model</phrase> for <phrase>noun phrase</phrase> <phrase>semantic</phrase> interpretation. We investigate the problem based on cross-<phrase>linguistic</phrase> evidence from a set of four <phrase>Romance languages</phrase>: <phrase>Spanish</phrase>, <phrase>Italian</phrase>, <phrase>French</phrase>, and <phrase>Romanian</phrase>. The focus on <phrase>Romance languages</phrase> is well motivated. It is generally the case that <phrase>English</phrase> <phrase>noun</phrase> phrases translate into constructions of the form "<i>N P N</i>" in <phrase>Romance languages</phrase> where, as we will show, the <i>P</i> (<phrase>preposition</phrase>) varies in ways that correlate with the <phrase>semantics</phrase>. Thus, based on a set of 22 <phrase>semantic</phrase> interpretation categories (such as PART-WHOLE, AGENT, POSSESSION) we present empirical observations regarding the distribution of these <phrase>semantic</phrase> categories in a cross-lingual corpus and their mapping to various <phrase>syntactic</phrase> constructions in <phrase>English</phrase> and <phrase>Romance</phrase>. Furthermore, given a <phrase>training set</phrase> of <phrase>English</phrase> <phrase>noun</phrase> phrases along with their translations in the four <phrase>Romance languages</phrase>, our <phrase>algorithm</phrase> automatically learns classification rules and applies them to unseen <phrase>noun phrase</phrase> instances for <phrase>semantic</phrase> interpretation. <phrase>Experimental</phrase> <phrase>results</phrase> are compared against a <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>model</phrase> reported in the <phrase>literature</phrase>.
Evaluation of Two <phrase>Connectionist</phrase> Approaches to Stack Representation Evaluation of Two <phrase>Connectionist</phrase> Approaches to Stack Representation This study empirically compares two distributed <phrase>connectionist</phrase> learning models trained to represent an arbitrarily deep stack. One is Pol-lack's Recursive Auto-<phrase>Associative Memory</phrase>, a recurrent back propagating <phrase>neural network</phrase> that uses a hidden intermediate representation. The other is the <phrase>Exponential Decay</phrase> <phrase>Model</phrase>, a novel <phrase>architecture</phrase> that we propose here, which tries to learn an explicit represention that models the stack as an exponentially decaying entity. We show that although the concept of a stack is learnable for both approaches, neither <phrase>model</phrase> is able to deliver the arbitrary depth attribute. Ultimately, both suffer from the rapid rate of error propagation inherent in their recursive structures.
<phrase>Short</phrase>-term traffic flow forecasting with spatial-temporal correlation in a <phrase>hybrid</phrase> <phrase>deep learning</phrase> framework <phrase>Deep learning</phrase> approaches have reached a <phrase>celebrity</phrase> status in <phrase>artificial intelligence</phrase> field, its success have mostly relied on <phrase>Convolutional Networks</phrase> (<phrase>CNN</phrase>) and Recurrent Networks. By exploiting fundamental spatial properties of images and videos, the <phrase>CNN</phrase> always achieves dominant performance on visual tasks. And the Recurrent Networks (RNN) especially <phrase>long</phrase> <phrase>short-term memory</phrase> methods (LSTM) can successfully characterize the temporal correlation, thus exhibits <phrase>superior</phrase> capability for <phrase>time series</phrase> tasks. Traffic flow <phrase>data</phrase> have plentiful characteristics on both time and space domain. However, applications of <phrase>CNN</phrase> and LSTM approaches on traffic flow are limited. In this <phrase>paper</phrase>, we propose a novel <phrase>deep architecture</phrase> combined <phrase>CNN</phrase> and LSTM to forecast future traffic flow (CLTFP). An 1-<phrase>dimension</phrase> <phrase>CNN</phrase> is exploited to capture spatial features of traffic flow, and two LSTMs are utilized to mine the <phrase>short</phrase>-term variability and periodicities of traffic flow. Given those meaningful features, the feature-level <phrase>fusion</phrase> is performed to achieve <phrase>short</phrase>-term traffic flow forecasting. The proposed CLTFP is compared with other popular forecasting methods on an open datasets. <phrase>Experimental</phrase> <phrase>results</phrase> indicate that the CLTFP has considerable advantages in traffic flow forecasting. in additional, the proposed CLTFP is analyzed from the view of <phrase>Granger Causality</phrase>, and several interesting properties of traffic flow and CLTFP are discovered and discussed .
Learning <phrase>Deep Architectures</phrase> for <phrase>AI</phrase> Theoretical <phrase>results</phrase> suggest that in <phrase>order</phrase> to learn the kind of complicated functions that can represent <phrase>high</phrase>-level abstractions (e.g. in vision, <phrase>language</phrase>, and other <phrase>AI</phrase>-level tasks), one may need <phrase>deep architectures</phrase>. <phrase>Deep architectures</phrase> are composed of <phrase>multiple levels</phrase> of non-linear operations, such as in <phrase>neural nets</phrase> with many <phrase>hidden layers</phrase> or in complicated <phrase>propositional</phrase> formulae re-using many sub-formulae. Searching the <phrase>parameter space</phrase> of <phrase>deep architectures</phrase> is a difficult task, but <phrase>learning algorithms</phrase> such as those for <phrase>Deep Belief</phrase> Networks have recently been proposed to tackle this problem with notable success, beating the <phrase>state</phrase>-of-the-<phrase>art</phrase> in certain areas. This <phrase>paper</phrase> discusses the motivations and principles regarding <phrase>learning algorithms</phrase> for <phrase>deep architectures</phrase>, in particular those exploiting as <phrase>building blocks</phrase> <phrase>unsupervised learning</phrase> of <phrase>single</phrase>-layer models such as <phrase>Restricted Boltzmann Machines</phrase>, used to construct deeper models such as <phrase>Deep Belief</phrase> Networks.
<phrase>Manifold</phrase> regularized <phrase>deep neural networks</phrase> <phrase>Deep neural networks</phrase> (DNNs) have been successfully applied to a <phrase>variety</phrase> of <phrase>automatic speech recognition</phrase> (ASR) tasks, both in discriminative <phrase>feature extraction</phrase> and <phrase>hybrid</phrase> <phrase>acoustic</phrase> mod-eling scenarios. The development of improved loss functions and regularization approaches have resulted in consistent reductions in ASR word error rates (<phrase>WERs</phrase>). This <phrase>paper</phrase> presents a <phrase>manifold</phrase> learning based regularization framework for DNN training. The associated techniques attempt to preserve the underlying <phrase>low dimensional</phrase> <phrase>manifold</phrase> based relationships amongst speech feature vectors as part of the optimization procedure for estimating network parameters. This is achieved by imposing <phrase>manifold</phrase> based <phrase>locality</phrase> preserving constraints on the outputs of the network. The techniques are presented in the context of a bottleneck DNN <phrase>architecture</phrase> for <phrase>feature extraction</phrase> in a <phrase>tandem</phrase> configuration. The ASR WER obtained using these networks is evaluated on a speech-in-noise task and compared to that obtained using DNN-bottleneck <phrase>networks trained</phrase> without mani-fold constraints.
Towards <phrase>Semantic</phrase> Annotation Supported by Dependency <phrase>Linguistics</phrase> and ILP In this <phrase>paper</phrase> we present a method for <phrase>semantic</phrase> annotation of texts, which is based on a <phrase>deep linguistic</phrase> analysis (DLA) and <phrase>Inductive Logic Programming</phrase> (ILP). The combination of DLA and ILP have following benefits: Manual selection of learning features is not needed. The learning procedure has full available <phrase>linguistic</phrase> <phrase>information</phrase> at its disposal and it is capable to select relevant parts itself. Learned extraction rules can be easily visualized, understood and adapted by <phrase>human</phrase>. A description , implementation and initial evaluation of the method are the main contributions of the <phrase>paper</phrase>.
<phrase>Universal</phrase> <phrase>Anomaly Detection</phrase>: <phrase>Algorithms</phrase> and Applications Modern computer threats are far more complicated than those seen in the past. They are constantly evolving, altering their appearance, perpetually changing disguise. Under such circumstances, detecting known threats, a fortiori zero-day attacks, requires new tools, which are able to capture the essence of their behavior, rather than some fixed signatures. In this work, we propose novel <phrase>universal</phrase> <phrase>anomaly detection</phrase> <phrase>algorithms</phrase>, which are able to learn the normal behavior of systems and alert for abnormalities, without any <phrase>prior knowledge</phrase> on the system <phrase>model</phrase>, nor any <phrase>knowledge</phrase> on the characteristics of the attack. The suggested method utilizes the Lempel-Ziv <phrase>universal</phrase> compression <phrase>algorithm</phrase> in <phrase>order</phrase> to optimally give <phrase>probability</phrase> assignments for normal behavior (during learning), then estimate the likelihood of new <phrase>data</phrase> (during operation) and classify it accordingly. The suggested technique is generic, and can be applied to different scenarios. Indeed, we apply it to key problems in computer <phrase>security</phrase>. The first is detecting Botnets Command and Control (C&C) channels. A <phrase>Botnet</phrase> is a logical network of compromised machines which are remotely controlled by an attacker using a C&C <phrase>infrastructure</phrase>, in <phrase>order</phrase> to perform malicious activities. We derive a detection <phrase>algorithm</phrase> based on timing <phrase>data</phrase>, which can be collected without deep inspection, from open as well as encrypted flows. We evaluate the <phrase>algorithm</phrase> on <phrase>real-world</phrase> network traces, showing how a <phrase>universal</phrase>, low complexity C&C identification system can be built, with <phrase>high</phrase> detection rates and low <phrase>false-alarm</phrase> <phrase>probabilities</phrase>. Further applications include malicious tools detection via system calls monitoring and <phrase>data</phrase> leakage identification.
Complex <phrase>Linguistic</phrase> Annotation - No Easy Way Out! A Case from Bangla and <phrase>Hindi</phrase> POS Labeling Tasks <phrase>Alternative</phrase> paths to <phrase>linguistic</phrase> annotation, such as those utilizing <phrase>games</phrase> or exploiting the web users, are becoming popular in recent times owing to their very <phrase>high</phrase> benefit-to-cost ratios. In this <phrase>paper</phrase>, however, we <phrase>report</phrase> a <phrase>case study</phrase> on POS annotation for Bangla and <phrase>Hindi</phrase>, where we observe that reliable <phrase>linguistic</phrase> annotation requires not only expert anno-tators, but also a great deal of supervision. For our hierarchical POS annotation scheme, we find that close supervision and training is necessary at every level of the hierarchy, or equivalently, complexity of the tagset. Nevertheless , an intelligent annotation tool can significantly accelerate the annotation process and increase the inter-annotator agreement for both expert and non-expert annotators. These findings <phrase>lead</phrase> us to believe that reliable annotation requiring <phrase>deep linguistic</phrase> <phrase>knowledge</phrase> (e.g., POS, chunking, Treebank, <phrase>semantic</phrase> role labeling) requires expertise and supervision. The focus, therefore, should be on <phrase>design</phrase> and development of appropriate annotation tools equipped with <phrase>machine learning</phrase> based predictive modules that can significantly boost the <phrase>productivity</phrase> of the annota-tors.
Towards Biologically Plausible <phrase>Deep Learning</phrase> Neuroscientists have <phrase>long</phrase> criticised <phrase>deep learning</phrase> <phrase>algorithms</phrase> as incompatible with current <phrase>knowledge</phrase> of <phrase>neurobiology</phrase>. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsuper-vised learning but developing a learning mechanism that could account for supervised, unsuper-vised and <phrase>reinforcement learning</phrase>. The <phrase>starting point</phrase> is that the <phrase>basic</phrase> learning rule believed to govern <phrase>synaptic</phrase> weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a <phrase>machine learning</phrase> point of view and can be interpreted as <phrase>gradient descent</phrase> on some <phrase>objective function</phrase> so <phrase>long</phrase> as the <phrase>neuronal</phrase> dynamics push firing rates towards better values of the <phrase>objective function</phrase> (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM <phrase>algorithm</phrase>, i.e., with approximate rather than exact posteriors , implemented by neural dynamics. Another contribution of this <phrase>paper</phrase> is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations <phrase>forward</phrase> and backward, with pairs of layers learning to form a denoising <phrase>auto-encoder</phrase>. Finally, we extend the theory about the proba-bilistic interpretation of <phrase>auto-encoders</phrase> to justify improved sampling schemes based on the gener-ative interpretation of denoising <phrase>auto-encoders</phrase>, and we validate all these ideas on generative learning tasks.
Analysing users' access logs in <phrase>Moodle</phrase> to improve <phrase>e</phrase> learning In this work the UFSC (<phrase>Federal</phrase> <phrase>University</phrase> of <phrase>Santa Catarina</phrase>) and the FGV-RJ (Funda&#231;&#227;o Get&#250;<phrase>lio</phrase> Vargas do <phrase>Rio</phrase> de Janeiro) jointly propose the use of a <phrase>data mining</phrase> tool to support the analysis of trends, students profiles, as well as to estimate or foresee the <phrase>usability</phrase> level of courses being offered, via <phrase>Moodle</phrase>, in the <phrase>Education</phrase> <phrase>area</phrase>. The study carried out by UFSC on the <phrase>Moodle</phrase> <phrase>database</phrase> allowed a <phrase>deep understanding</phrase> of its <phrase>database</phrase>, thus making it easier for the <phrase>Moodle</phrase> <phrase>community</phrase> to execute important tasks, such as the maintenance of the <phrase>Moodle</phrase> <phrase>database</phrase>, its adaptation following an institutional customization, and, also, a <phrase>data</phrase> mart project by the FGV-Online Program to make the necessary analysis possible. In the end of this <phrase>paper</phrase>, an example on its applicability is presented, using the association rules technique. Once a <phrase>data</phrase> mart oriented to the analysis of the system's <phrase>usability</phrase> is developed, various analyses with different objectives can be executed using the <phrase>database</phrase>. Some may use the method proposed here or others, including different <phrase>data mining</phrase> approaches, such as clustering, <phrase>neural networks</phrase> etc. As such, a new contribution is given to the <phrase>Moodle</phrase> <phrase>community</phrase>.
Toward <phrase>Large Scale</phrase> Integration: Building a MetaQuerier over <phrase>Databases</phrase> on the Web The Web has been rapidly " deepened " by myriad searchable <phrase>databases</phrase> online, where <phrase>data</phrase> are hidden behind query interfaces. Toward <phrase>large scale</phrase> integration over this " <phrase>deep Web</phrase>, " we have been building the MetaQuerier system for both exploring (to find) and integrating (to query) <phrase>databases</phrase> on the Web. As an interim <phrase>report</phrase>, first, this <phrase>paper</phrase> proposes our goal of the MetaQuerier for Web-scale integration With its dynamic and <phrase>ad-hoc</phrase> <phrase>nature</phrase>, such <phrase>large scale</phrase> integration mandates both dynamic source discovery and on-the-<phrase>fly</phrase> query <phrase>translation</phrase>. Second, we present the system <phrase>architecture</phrase> and underlying <phrase>technology</phrase> of key subsystems in our ongoing implementation. Third, we discuss " lessons " learned to date, focus-ing on our efforts in system integration, for putting individual subsystems to <phrase>function</phrase> together. On one hand, we observe that, across subsystems, the system integration of an integration system is itself non-trivial which presents both challenges and opportunities beyond subsystems in isolation. On the other hand, we also observe that, across subsystems, there emerge unified insights of " holistic integration " which leverage <phrase>large scale</phrase> itself as a unique opportunity for <phrase>information</phrase> integration.
Incremental Nonmonotonic <phrase>Parsing</phrase> through <phrase>Semantic</phrase> <phrase>Self-organization</phrase> Incremental Nonmonotonic <phrase>Parsing</phrase> through <phrase>Semantic</phrase> <phrase>Self-organization</phrase> Incremental Nonmonotonic <phrase>Parsing</phrase> through <phrase>Semantic</phrase> <phrase>Self-organization</phrase> Incremental Nonmonotonic <phrase>Parsing</phrase> through <phrase>Semantic</phrase> <phrase>Self-organization</phrase> Acknowledgments Many people have contributed in one way or another to the completion of this dissertation and of my graduate studies. I sincerely thank you all. The encouragement, guidance and support I have always received from my adviser Risto Miikkulainen throughout all of these years has been invaluable, as have been the numerous discussions with Raymond Mooney. Also, I have benefitted immeasurably from input from the rest of my committee, who guided me on the objectives of my <phrase>research</phrase>. I am deeply grateful for the friendship and emotional support I found among other colleagues and <phrase>friends</phrase>: up with my never-ending questions about your <phrase>language</phrase>, your <phrase>music</phrase>, and your <phrase>culture</phrase>. It has been fascinating getting to know you all. Deep gratitude also goes to my parents, Sally and Reeves <phrase>Mayberry</phrase>, for always believing in me and their unconditional support in everything I have set out to do. To the rest of my <phrase>family</phrase> thank you for your <phrase>love</phrase> during this <phrase>long</phrase> enterprise. J.R., and Lulu: I immensely enjoyed those summers you all spent with us playing <phrase>games</phrase> and feeding <phrase>squirrels</phrase> around UT. But above all, I want to thank my wife, Coquis, for her unwavering and loving support throughout my dissertation ordeal. Subsymbolic systems have been successfully used to <phrase>model</phrase> several aspects of <phrase>human</phrase> <phrase>language</phrase> processing. Subsymbolic parsers are appealing because they allow combining <phrase>syntactic</phrase>, <phrase>semantic</phrase> , and thematic constraints in sentence interpretation and nonmonotonically revising that interpretation while incrementally processing a sentence. Such parsers are also cognitively plausible: processing is robust and multiple interpretations are simultaneously activated when the input is ambiguous. Yet, it has <phrase>proven</phrase> very difficult to scale them up to realistic <phrase>language</phrase>. They have limited <phrase>memory</phrase> capacity, training takes a <phrase>long</phrase> time, and it is difficult to represent <phrase>linguistic</phrase> structure. A new <phrase>connectionist</phrase> <phrase>model</phrase>, INSOMNet, scales up the subsymbolic approach by utilizing <phrase>semantic</phrase> <phrase>self-organization</phrase>. INSOMNet was trained on <phrase>semantic</phrase> dependency <phrase>graph</phrase> representations from the recently-released LinGO Redwoods HPSG Treebank of sentences from the VerbMobil project. The <phrase>results</phrase> show that INSOMNet accurately learns to represent these <phrase>semantic</phrase> dependencies and generalizes to novel structures. Further evaluation of INSOMNet on the original <phrase>Verb</phrase>-<phrase>Mobil</phrase> sentences transcribed with annotations for <phrase>spoken language</phrase> demonstrates robust <phrase>parsing</phrase> of noisy input, while graceful degradation in performance from adding noise to the network weights underscores INSOMNet's tolerance to damage. Finally, the <phrase>cognitive</phrase> plausibility of the <phrase>model</phrase> is shown on a standard psycholinguistic benchmark, in which INSOMNet 
Image Denoising and Inpainting with <phrase>Deep Neural Networks</phrase> We present a novel approach to <phrase>low-level</phrase> vision problems that combines <phrase>sparse coding</phrase> and deep networks <phrase>pre-trained</phrase> with denoising <phrase>auto-encoder</phrase> (DA). We propose an <phrase>alternative</phrase> training scheme that successfully adapts DA, originally designed for <phrase>unsupervised feature learning</phrase>, to the tasks of image denoising and blind inpainting. Our method's performance in the image denoising task is comparable to that of KSVD which is a widely used <phrase>sparse coding</phrase> technique. More importantly , in blind image inpainting task, the <phrase>proposed method</phrase> provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like <phrase>pixels</phrase> missing at random. Moreover, the <phrase>proposed method</phrase> does not need the <phrase>information</phrase> regarding the <phrase>region</phrase> that requires inpaint-ing to be given a priori. <phrase>Experimental</phrase> <phrase>results</phrase> demonstrate the effectiveness of the <phrase>proposed method</phrase> in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of <phrase>unsupervised feature learning</phrase>.
Maintaining performance on power gating of <phrase>microprocessor</phrase> functional units by using a predictive pre-wakeup strategy Power gating is an effective technique for reducing leakage power in <phrase>deep submicron</phrase> <phrase>CMOS</phrase> <phrase>technology</phrase>. Microarchitectural techniques for power gating of functional units have been developed by detecting suitable idle regions and turning them off to reduce leakage <phrase>energy</phrase> consumption; however, wakeup of functional units is needed when instructions are ready for execution such that wakeup overhead is naturally incurred. This study presents time-based power gating with reference pre-wakeup (PGRP), a novel predictive strategy that detects suitable idle periods for power gating and then enables pre-wakeup of needed functional units for avoiding wakeup overhead. The key insight is that most wakeups are repeated due to program <phrase>locality</phrase>. Thus, the pre-wakeup predictor learns the wakeup events and selects which prior branch instruction can provide early wakeup (wakeup patterns are visible); these <phrase>information</phrase> are then used to adequately prepare available functional units for instruction execution. <phrase>Simulation</phrase> <phrase>results</phrase> with benchmarks from SPEC2000 applications show that substantial leakage <phrase>energy</phrase> reduction with negligible performance degradation (0.38&percnt; on <phrase>average</phrase>) is worthwhile.
<phrase>Parkinson's disease</phrase>: a <phrase>motor control</phrase> study using a wrist <phrase>robot</phrase> <phrase>Deep brain stimulation</phrase> (DBS) is the most common <phrase>surgical procedure</phrase> for patients with <phrase>Parkinson's disease</phrase> (PD). DBS has been shown to have a positive effect on PD symptoms; however, its specific effects on <phrase>motor control</phrase> are not yet understood. We introduce the novel use of a wrist <phrase>robot</phrase> in studying the effects of stimulation on motor performance and learning. We present <phrase>results</phrase> from patients performing reaching movements in a null field and in a force field with and without stimulation. We discuss special cases where robotic testing reveals otherwise undiagnosed impairments, and where clinical scores and <phrase>robot</phrase>-based scores display opposing trends.
On Multiagent Q-Learning in a Semi-Competitive Domain Q-learning is a recent <phrase>reinforcement learning</phrase> (<phrase>RL</phrase>) <phrase>algorithm</phrase> that does not need a <phrase>model</phrase> of its environment and can be used on-line. Therefore it is well-suited for use in repeated <phrase>games</phrase> against an unknown opponent. Most <phrase>RL</phrase> <phrase>research</phrase> has been connned to <phrase>single</phrase> agent settings or to multiagent settings where the agents have totally positively correlated payoos (team problems) or totally negatively correlated pay-oos (zero-sum <phrase>games</phrase>). This <phrase>paper</phrase> is an <phrase>empirical study</phrase> of reinforcement l <phrase>e</phrase> a r n i n g i n t h <phrase>e</phrase> iterated <phrase>prisoner's dilemma</phrase> (IPD), where the agents' payoos are neither totally positively nor totally negatively correlated. <phrase>RL</phrase> is considerably more diicult in such a domain. This <phrase>paper</phrase> investigates the ability o f a v ariety o f Q-learning agents to <phrase>play</phrase> the IPD <phrase>game</phrase> against an unknown opponent. In some experiments, the opponent is the xed strategy Tit-for-Tat, while in others it is another Q-learner. All the Q-learners learned to <phrase>play</phrase> optimally against Tit-for-Tat. Playing against another learner was more diicult because the adaptation of the other learner creates a nonstationary environment i n w ays that are detailed in the <phrase>paper</phrase>. The learners that were studied varied along three dimensions: the length of <phrase>history</phrase> they received as context, the type of <phrase>memory</phrase> they employed (lookup tables based on restricted <phrase>history</phrase> <phrase>windows</phrase> or <phrase>recurrent neural networks</phrase> (RNNs) that can theoretically store features from arbitrarily deep in the past), and the exploration schedule they followed. Although all the learners faced diiculties when playing against other learners, agents with longer <phrase>history</phrase> <phrase>windows</phrase>, <phrase>lookup table</phrase> memories, and longer exploration schedules fared best in the IPD <phrase>games</phrase>.
Distilling <phrase>Knowledge</phrase> from Deep Networks with Applications to <phrase>Healthcare</phrase> Domain <phrase>Exponential growth</phrase> in <phrase>Electronic</phrase> <phrase>Healthcare</phrase> Records (<phrase>EHR</phrase>) has resulted in new opportunities and urgent needs for discovery of meaningful <phrase>data</phrase>-driven representations and patterns of diseases in Computational Phenotyping <phrase>research</phrase>. <phrase>Deep Learning</phrase> models have shown <phrase>superior</phrase> performance for robust prediction in computational phenotyping tasks, but suffer from the issue of <phrase>model</phrase> interpretability which is crucial for clinicians involved in <phrase>decision-making</phrase>. In this <phrase>paper</phrase>, we introduce a novel <phrase>knowledge</phrase>-<phrase>distillation</phrase> approach called Interpretable Mimic Learning, to learn interpretable <phrase>phenotype</phrase> features for making robust prediction while mimicking the performance of <phrase>deep learning</phrase> models. Our framework uses <phrase>Gradient</phrase> Boosting <phrase>Trees</phrase> to learn interpretable features from <phrase>deep learning</phrase> models such as Stacked Denoising Autoencoder and <phrase>Long</phrase> <phrase>Short-Term Memory</phrase>. Exhaustive experiments on a <phrase>real-world</phrase> clinical <phrase>time-series</phrase> dataset show that our method obtains similar or better performance than the <phrase>deep learning</phrase> models, and it provides interpretable phenotypes for clinical <phrase>decision making</phrase>.
From <phrase>Neuron</phrase> to <phrase>Neural Networks</phrase> dynamics This <phrase>paper</phrase> presents an overview of some techniques and concepts coming from dynamical system theory and used for the analysis of dynamical <phrase>neural networks</phrase> models. In a first section, we describe the dynamics of the <phrase>neuron</phrase>, starting from the Hodgkin-<phrase>Huxley</phrase> description, which is somehow the canonical description for the " biological <phrase>neuron</phrase> ". We discuss some models reducing the Hodgkin-<phrase>Huxley</phrase> <phrase>model</phrase> to a two dimensional dynamical system, keeping one of the main feature of the <phrase>neuron</phrase>: its excitability. We present then examples of <phrase>phase diagram</phrase> and bifurcation analysis for the Hodgin-<phrase>Huxley</phrase> equations. Finally, we end this section by a dynamical system analysis for the <phrase>nervous</phrase> <phrase>flux</phrase> propagation along the <phrase>axon</phrase>. We then consider <phrase>neuron</phrase> couplings, with a brief description of <phrase>synapses</phrase>, <phrase>synaptic</phrase> plasticiy and learning, in a second section. We also briefly discuss the delicate issue of causal <phrase>action</phrase> from one <phrase>neuron</phrase> to another when complex <phrase>feedback</phrase> effects and non linear dynamics are involved. The third section presents the limit of weak coupling and the use of normal forms technics to handle this situation. We consider then several examples of recurrent models with different type of <phrase>synaptic</phrase> interactions (symmetric, <phrase>cooperative</phrase>, random). We introduce various techniques coming from <phrase>statistical physics</phrase> and <phrase>dynamical systems theory</phrase>. A last section is devoted to a detailed example of recurrent <phrase>model</phrase> where we go in deep in the analysis of the dynamics and discuss the effect of learning on the <phrase>neuron</phrase> dynamics. We also present recent methods allowing the analysis of the non linear effects of the neural dynamics on signal propagation and causal <phrase>action</phrase>. An appendix, presenting the main notions of <phrase>dynamical systems theory</phrase> useful for the comprehension of the chapter, has been added for the convenience of the reader.
Ten Simple Rules for Aspiring Scientists in a Low-Income <phrase>Country</phrase> Being a <phrase>scientist</phrase> entails a common set of characteristics. Admiring <phrase>nature</phrase> and having concern for social issues; possessing a strong <phrase>academic</phrase> background, team work abilities, honesty, discipline, <phrase>skepticism</phrase>, <phrase>communication</phrase> skills, competitiveness, ability to accept and give criticism, and productive relationships are some of the most obvious traits that scientists should have. To be a <phrase>scientist</phrase> in a low-income <phrase>country</phrase> (LIC), however, requires a complementary set of qualities that are necessary to confront the drawbacks that work against the development of <phrase>science</phrase>. The failure of many young researchers to mature as <phrase>professional</phrase> scientists upon their return to their <phrase>country</phrase> from advanced training elsewhere, motivated us to propose these ten rules. Most LIC scientists want to <phrase>live</phrase> in their home <phrase>country</phrase>. Nevertheless, you must be realistic and prepared to face rudimentary laboratories, power cuts, poor <phrase>water supply</phrase> , deficient <phrase>libraries</phrase>, slow <phrase>Internet</phrase>, and scarce or non-existent national funds for supporting <phrase>research</phrase>, hiring personnel, and providing maintenance or equipment. You must understand that <phrase>science</phrase> is a minor component of the <phrase>cultural</phrase> environment of an LIC and that, for most people and many politicians, <phrase>science</phrase> is a curiosity performed in <phrase>high</phrase>-income countries [1]. Within this adverse scenario, you should establish broad and strong links with your <phrase>community</phrase> and <phrase>country</phrase>. This involves becoming interested in historical, social, and <phrase>political</phrase> issues. LIC researchers have to enjoy the idiosyncrasies of their <phrase>country</phrase>, and cultivate the desire to contribute to the scientific development of their homeland and to the well-being of its people. Do not endorse deep doubts about the possibilities of performing <phrase>research</phrase>. It can be donebut not alone. Try to join efforts with other investigators facing the same problems. Learn how they sidetrack difficulties , and incorporate yourself into a <phrase>research</phrase> team. If you are not able to find a group that fits your specific interest, then procure a group of researchers who, although investigating topics marginal to your own, are capable of understanding the relevance of your work. At the initial phases of your career, belonging to a creative scientific environment in which your <phrase>knowledge</phrase> and skills are appreciated is of <phrase>major</phrase> importance. Be part of a team before trying to <phrase>lead</phrase> one. Your formal <phrase>education</phrase> has finished, but your scientific career is just beginning. <phrase>Research</phrase> should be your main <phrase>professional</phrase> activity. Consider that you may be the country's only specialist in a particular topic, but keep in mind that <phrase>science</phrase> is global. You are 
Simplifying <phrase>deep neural networks</phrase> for neuromorphic architectures <phrase>Deep learning</phrase> using <phrase>deep neural networks</phrase> is taking machine <phrase>intelligence</phrase> to the next level in <phrase>computer vision</phrase>, <phrase>speech recognition</phrase>, <phrase>natural language processing</phrase>, etc. <phrase>Brain</phrase>-like hardware platforms for the <phrase>brain</phrase>-inspired computational models are being studied, but none of such platforms deals with the huge size of practical <phrase>deep neural networks</phrase>. This <phrase>paper</phrase> presents two techniques, factorization and <phrase>pruning</phrase>, that not only compress the models but also maintain the form of the models for the execution on neuromorphic architectures. We also propose a novel method to combine the two techniques. The <phrase>proposed method</phrase> shows <phrase>significant improvements</phrase> in reducing the number of <phrase>model</phrase> parameters over standalone use of each method while maintaining the performance. Our <phrase>experimental</phrase> <phrase>results</phrase> show that the <phrase>proposed method</phrase> can achieve 31&#215; reduction rate without loss of accuracy for the largest layer of AlexNet.
<phrase>Large Margin</phrase> Methods for Structured and Interdependent Output Variables (1985). A <phrase>learning algorithm</phrase> for <phrase>boltzmann</phrase> machines. (2010). Learning the structure of deep sparse <phrase>graphical</phrase> models. In <phrase>AI</phrase>/<phrase>Statistics</phrase>. On tight approximate inference of the logistic-normal topic admixture <phrase>model</phrase>. In <phrase>AI</phrase>/Statistics.ference using message propoga-tion and <phrase>topology</phrase> transformation in <phrase>vector</phrase> Gaussian continuous networks. In UAI. <phrase>Bayesian analysis</phrase> of <phrase>binary</phrase> and polychoto-mous response <phrase>data</phrase>. (2000). Reducing multiclass to <phrase>binary</phrase>: A unifying approach for margin classifiers. <phrase>Partition</phrase>-based logical reasoning for first-<phrase>order</phrase> and propo-sitional theories. A framework for learning predictive structures from multiple tasks and <phrase>unlabeled data</phrase>. (2000). Sequential <phrase>Bayesian</phrase> estimation and <phrase>model selection</phrase> for dynamic kernel machines. (2003). An introduction to MCMC for <phrase>machine learning</phrase>. (2005). Online EM for <phrase>parameter estimation</phrase> in nonlinear-non Gaus-sian <phrase>state-space</phrase> models. In Proc. <phrase>IEEE</phrase> <phrase>CDC</phrase>. A. Proskurowski (1987). Complexity of finding embeddings in a k-<phrase>tree</phrase>. (2007). k-means++: the advantages of careful seeding. In Proc. 18th <phrase>ACM</phrase>-<phrase>SIAM</phrase> symp. on Discrete <phrase>algorithms</phrase>, pp. 1027 A S1035.
<phrase>Machine Learning</phrase> <phrase>Pattern Recognition</phrase> 1.1 <phrase>Machine Learning</phrase> Introduction These lecture notes were written to provide you with a handy reference to the material that was presented in the <phrase>Machine Learning</phrase>: <phrase>Pattern Recognition</phrase> course. It is not meant to replace the <phrase>book</phrase> of the course [<phrase>Bishop</phrase>, 2007], rather, it is meant to illustrate topics that may seem a <phrase>bit</phrase> dry or emphasise subjects that are deemed important to this class. It will often repeat what was said in the class, in the hope that this will help you to understand the material better, but it is not meant to replace the lectures (read: I do hope that you will not try to skip the lectures because you have the lecture notes.. .) There is still no <phrase>universal</phrase> consent as to what <phrase>artificial intelligence</phrase> really is, exactly. But most of the definitions boil down to allowing machines to deal with (and <phrase>excel</phrase> in) their environment in an autonomous way. The variation in the definitions depends very much on what we define the environment to be. Early <phrase>AI</phrase> restricted the environment to, say, a <phrase>chess</phrase> <phrase>game</phrase>. Or to <phrase>theorem proving</phrase>. Those environments are inherently very complex, and we are far from having " solved " those problems. But we have made a lot of progress, and in some cases machines perform better than humans at those tasks. <phrase>Deep blue</phrase> beat Gary <phrase>Kasparov</phrase> in 1997. However, the physical world we <phrase>live</phrase> in is inherently far more complex than such " <phrase>toy</phrase> " worlds; the main inherent difficulty of the physical world is uncertainty. Yet biological machines deal with the physical world quite well. So we know it can be done, and we have quite a challenge to meet. The fact that we can do things ourselves doesn't help us very much in getting machines to do it: there are many problems we can solve very well (e.g., walking, understanding speech, <phrase>reading</phrase> natural text, producing grammatically correct speech, recognising an <phrase>emotion</phrase> on somebody's face,. . .), but we don't really know how we do it. Each of these problems is very complex, and we don't know how to get a machine to do them. So if we ever want to have machines that can do these things, they'll have to learn to do them by themselves. And that's what this course is about. So how can we know what to do with a new observation, from other observations that we've seen before. 
Generalization of Figure-Ground Segmentation from <phrase>Binocular</phrase> to <phrase>Monocular</phrase> Vision in an Embodied Biological <phrase>Brain</phrase> <phrase>Model</phrase> Humans have the remarkable ability to generalize from <phrase>binocular</phrase> to <phrase>monocular</phrase> figure-ground segmentation of complex scenes. This is clearly evident anytime we look at a <phrase>photograph</phrase>, computer monitor or simply close one eye. We hypothesized that this skill is due to of the ability of our brains to use rich embodied signals, such as disparity, to <phrase>train</phrase> up <phrase>depth perception</phrase> when only the <phrase>information</phrase> from one eye is available. In <phrase>order</phrase> to <phrase>test</phrase> this <phrase>hypothesis</phrase> we enhanced our virtual <phrase>robot</phrase>, Emer, who is already capable of performing robust, <phrase>state</phrase>-of-the-<phrase>art</phrase>, invariant 3D <phrase>object recognition</phrase> [1], with the ability to learn figure-ground segmen-tation, allowing him to recognize objects against complex backgrounds. Continued development of this skill holds great promise for efforts, like Emer, that aim to create an <phrase>Artificial General Intelligence</phrase> (AGI). For example, it promises to unlock vast sets of <phrase>training data</phrase>, such as <phrase>Google Images</phrase>, which have previously been inaccessible to AGI models due to their lack of embodied, <phrase>deep learning</phrase>. More immediately practical implications , such as achieving <phrase>human</phrase> performance on the Caltech101 <phrase>object recognition</phrase> dataset [2], are discussed.ment is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any <phrase>copyright</phrase> annotation thereon. <phrase>Disclaimer</phrase>: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements , either expressed or implied,
<phrase>Human</phrase> Pose Estimation Using Deep Consensus Voting In this <phrase>paper</phrase> we consider the problem of <phrase>human</phrase> pose estimation from a <phrase>single</phrase> still image. We propose a novel approach where each location in the image votes for the position of each keypoint using a convolutional <phrase>neural net</phrase>. The voting scheme allows us to utilize <phrase>information</phrase> from the whole image, rather than rely on a sparse set of keypoint locations. Using dense, multi-<phrase>target</phrase> votes, not only produces good keypoint predictions, but also enables us to compute image-dependent joint keypoint <phrase>probabilities</phrase> by looking at consensus voting. This differs from most previous methods where joint <phrase>probabilities</phrase> are learned from relative keypoint locations and are <phrase>independent</phrase> of the image. We finally combine the keypoints votes and joint <phrase>probabilities</phrase> in <phrase>order</phrase> to identify the optimal pose configuration. We show our competitive performance on the MPII <phrase>Human</phrase> Pose and <phrase>Leeds</phrase> <phrase>Sports</phrase> Pose datasets.
Standing on the Shoulders of <phrase>Giants</phrase>: Improving <phrase>Medical</phrase> <phrase>Image Segmentation</phrase> via Bias Correction We propose a simple strategy to improve automatic <phrase>medical</phrase> <phrase>image segmentation</phrase>. The key idea is that without <phrase>deep understanding</phrase> of a segmentation method, we can still improve its performance by directly calibrating its <phrase>results</phrase> with respect to manual segmentation. We formulate the calibration process as a bias correction problem, which is addressed by <phrase>machine learning</phrase> using <phrase>training data</phrase>. We apply this methodology on three segmentation problems/methods and show <phrase>significant improvements</phrase> for all of them.
Can GPGPU <phrase>Programming</phrase> Be Liberated from the <phrase>Data</phrase>-Parallel Bottleneck? <phrase>Cover</phrase> Fe ature platforms. <phrase>GPU</phrase> <phrase>programming</phrase> models, especially, have expanded over recent years to offer higher levels of flexibility. Both <phrase>OpenCL</phrase> (Open <phrase>Computing</phrase> <phrase>Language</phrase>) 1 and <phrase>CUDA</phrase> (Compute Unified Device <phrase>Architecture</phrase>) 2 support heterogeneous platforms to some <phrase>degree</phrase>. To ensure that code can execute on various <phrase>target</phrase> platforms, these models employ a <phrase>data</phrase>-parallel methodology with weak <phrase>communication</phrase> guarantees. This relaxed approach leads to fundamental problems associated with combining SPMD (<phrase>single</phrase> program, multiple <phrase>data</phrase>) <phrase>programming</phrase> with <phrase>SIMD</phrase> (<phrase>single</phrase> instruction, multiple <phrase>data</phrase>) execution, braided parallelism, and composability of operations. To date, most attempts to ease the <phrase>programming</phrase> burden for heterogeneous development have concentrated on <phrase>API</phrase> simplifications, such as those in <phrase>CUDA</phrase> over the graphics-oriented <phrase>programming</phrase> environments that preceded it, those in Microsoft's <phrase>C+ +</phrase> AMP (Accelerated Massive Parallelism) 3 <phrase>design</phrase> that link the benefits of <phrase>C+ +</phrase>-<phrase>type safety</phrase> with <phrase>GPU</phrase> <phrase>programming</phrase>, or those in pragma-based models such as OpenACC. However, these models smooth the <phrase>learning curve</phrase> but do not address the fundamental problems. W ith the growth in <phrase>transistor</phrase> counts in modern hardware, heterogeneous systems are becoming commonplace. Core counts are increasing such that <phrase>GPU</phrase> and <phrase>CPU</phrase> designs are reaching deep into the tens of cores. For performance reasons, different cores in a heterogeneous platform follow different <phrase>design</phrase> choices. Based on throughput <phrase>computing</phrase> goals, <phrase>GPU</phrase> cores tend to support wide vectors and substantial register files. Current designs optimize <phrase>CPU</phrase> cores for latency, dedicating <phrase>logic</phrase> to caches and out-of-<phrase>order</phrase> dependence control. Heterogeneous platforms are clearly here to stay and will soon be ubiquitous. The problems that inevitably arise for hardware developers relate to programmingin particular , making efficient use of such platforms. Existing <phrase>programming</phrase> models attempt to satisfy some of the diverse requirements of heterogeneous Heterogeneous parallel primitives (HPP) addresses two <phrase>major</phrase> shortcomings in current GPGPU <phrase>programming</phrase> models: it supports full composability by defining abstractions and increases flexibility in execution by introducing braided parallelism.
Designing for <phrase>Online Distance Education</phrase>: Putting <phrase>Pedagogy</phrase> before <phrase>Technology</phrase> <phrase>Theological</phrase> schools are increasingly exploring <phrase>online distance education</phrase> as a mode of course delivery. Yet while online course delivery has the potential for effective, <phrase>deep learning</phrase> it can also have a number of pitfalls. This article introduces <phrase>online distance education</phrase> and examines in detail the pedagogical possibilities for <phrase>online learning</phrase> by providing a number of examples drawn from online courses. While championing the use of online course delivery for <phrase>theological</phrase> schools, it also sounds a note of caution by advocating that the use of <phrase>technology</phrase> should be driven by <phrase>sound</phrase> pedagogical principles. Putting <phrase>pedagogy</phrase> before <phrase>technology</phrase> will insure quality <phrase>education</phrase> no <phrase>matter</phrase> what the content or mode of delivery. Linda Harasim has correctly stated``all <phrase>education</phrase> face to face, distance mode, online requires understanding the <phrase>nature</phrase> of the medium in <phrase>order</phrase> to conceptualize and <phrase>design</phrase> it as an educational environment'' (Harasim et al. 1995, 138). That is to say, good <phrase>pedagogy</phrase> requires an awareness of the opportunities and limitations of the mode of <phrase>education</phrase>. In this <phrase>paper</phrase> I will argue that <phrase>online distance education</phrase> provides opportunities for quality <phrase>education</phrase>, although it can <phrase>lead</phrase> to poor pedagogical practices. For <phrase>online distance education</phrase> to be effective one must understand the medium and the pedagogical principles that can <phrase>lead</phrase> to <phrase>deep learning</phrase> in the online environment. At a foundational level, putting <phrase>pedagogy</phrase> before <phrase>technology</phrase> will allow for the effective delivery of <phrase>online distance education</phrase> courses. We will investigate this under seven areas: the parameters of <phrase>online distance education</phrase>; the purposes of <phrase>online distance education</phrase>; the planning of online courses; the pedagogical possibilities in <phrase>online distance education</phrase>; the pitfalls of such use; the institutional, faculty, and <phrase>student</phrase> prerequisites for effective delivery of online courses; and a few predictions about the impact of <phrase>online distance education</phrase>. <phrase>Distance education</phrase> has had a <phrase>long</phrase> <phrase>history</phrase>, extending back to the nineteenth century The development of an extensive, relatively inexpensive <phrase>postal service</phrase> in the late nineteenth century <phrase>led</phrase> to the creation of print-based correspondence courses. Such courses allowed for the distribution of <phrase>information</phrase> and the sustained exchange between learner and instructor via print. Such correspondence did not require physical proximity, and one could reasonably assume that a turnaround time of a few weeks was all that was necessary for each response. Correspondence study continues in many institutions today and is one of the foremost methods of <phrase>distance education</phrase>. With the development of <phrase>telecommunications</phrase>, <phrase>distance education</phrase> was given new 
Improved <phrase>Reconstruction</phrase> of 4D-MR Images by Motion Predictions The <phrase>reconstruction</phrase> of 4D images from 2D <phrase>navigator</phrase> and <phrase>data</phrase> slices requires sufficient observations per motion <phrase>state</phrase> to avoid blurred images and motion artifacts between slices. Especially images from rare motion states, like deep inhalations during <phrase>free</phrase>-breathing, suffer from too few observations. To address this problem, we propose to actively generate more suitable images instead of only selecting from the available images. The method is based on learning the relationship between <phrase>navigator</phrase> and <phrase>data</phrase>-slice motion by <phrase>linear regression</phrase> after <phrase>dimensionality reduction</phrase>. This can then be used to predict new <phrase>data</phrase> slices for a given <phrase>navigator</phrase> by <phrase>warping</phrase> existing <phrase>data</phrase> slices by their predicted displacement field. The method was evaluated for 4D-MRIs of the <phrase>liver</phrase> under <phrase>free</phrase>-breathing, where sliding boundaries pose an additional challenge for <phrase>image registration</phrase>. Leave-one-out <phrase>tests</phrase> for five <phrase>short</phrase> sequences of ten volunteers showed that the proposed prediction method improved on <phrase>average</phrase> the residual mean (95%) motion between the <phrase>ground truth</phrase> and predicted <phrase>data</phrase> slice from 0.9mm (1.9mm) to 0.8mm (1.6mm) in comparison to the best selection method. The approach was particularly suited for unusual motion states, where the mean error was reduced by 40% (2.2mm vs. 1.3mm).
Local Deep Kernel Learning for Efficient Non-linear <phrase>SVM</phrase> Prediction Our objective is to speed up non-linear <phrase>SVM</phrase> prediction while maintaining <phrase>classification accuracy</phrase> above an acceptable limit. We generalize Localized <phrase>Multiple Kernel</phrase> Learning so as to learn a <phrase>tree</phrase>-based primal feature embedding which is <phrase>high</phrase> dimensional and sparse. Primal based classification decouples prediction costs from the number of support vectors and our <phrase>tree</phrase>-structured features efficiently encode non-linearities while speeding up prediction exponentially over the <phrase>state</phrase>-of-the-<phrase>art</phrase>. We develop routines for optimizing over the space of <phrase>tree</phrase>-structured features and efficiently scale to problems with more than half a million training points. Experiments on benchmark <phrase>data</phrase> sets reveal that our formulation can reduce prediction costs by more than three <phrase>orders of magnitude</phrase> in some cases with a moderate sacrifice in <phrase>classification accuracy</phrase> as compared to RBF-SVMs. Furthermore , our formulation leads to better classification accuracies over leading methods.
Learning <phrase>binary</phrase> <phrase>factor analysis</phrase> with automatic <phrase>model selection</phrase> In most cases authors are permitted to post their version of the article (e.g. in Word or Tex form) to their personal <phrase>website</phrase> or <phrase>institutional repository</phrase>. Authors requiring further <phrase>information</phrase> regarding Elsevier's archiving and <phrase>manuscript</phrase> policies are encouraged to visit: a b s t r a c t <phrase>Binary</phrase> <phrase>Factor Analysis</phrase> (BFA) uncovers the <phrase>independent</phrase> <phrase>binary</phrase> <phrase>information</phrase> sources from observations with wide applications. BFA learning hierarchically nests three levels of inverse problems, i.e., inference of <phrase>binary code</phrase> for each observation, <phrase>parameter estimation</phrase> and <phrase>model selection</phrase>. Under <phrase>Bayesian</phrase> Ying-Yang (BYY) framework, the first level becomes an intractable <phrase>Binary</phrase> <phrase>Quadratic Programming</phrase> (BQP) problem, while <phrase>model selection</phrase> can be conducted automatically during parameter learning. We conduct <phrase>extensive experiments</phrase> to reveal that the performance <phrase>order</phrase> of four BQP methods is reversed from making BQP optimization to making BYY automatic <phrase>model selection</phrase>, which implies that learning is not merely optimization. Moreover, the BFA <phrase>learning algorithm</phrase> is further developed with priors over parameters to improve the performance. Finally, based on BFA, we empirically compare BYY with Variational Bayes (VB) and <phrase>Bayesian</phrase> <phrase>information</phrase> criterion (BIC). <phrase>Binary</phrase> <phrase>Factor Analysis</phrase> (BFA) explores latent <phrase>binary</phrase> structures of <phrase>data</phrase>. Unlike the conventional <phrase>factor analysis</phrase> where the latent factor is assumed to be Gaussian, BFA traces the observation to <phrase>independent</phrase> <phrase>Bernoulli</phrase> <phrase>information</phrase> sources. <phrase>Research</phrase> on BFA has been focused on analysis of <phrase>binary data</phrase>, such as social <phrase>research</phrase> questionnaires and market basket <phrase>data</phrase>, with the aid of <phrase>Boolean algebra</phrase> [1], and also on the discovery of <phrase>binary</phrase> factors in continuous <phrase>data</phrase>, [24], taking advantage of the representational capacity of the underlying <phrase>binary</phrase> structure. When considering all the <phrase>random variables</phrase> to be <phrase>binary</phrase>, <phrase>factor analysis</phrase> becomes the <phrase>restricted Boltzmann machine</phrase> which is the <phrase>building block</phrase> of the <phrase>deep belief</phrase> network [5]. This <phrase>paper</phrase> considers the same BFA <phrase>model</phrase> as in [4,2], under <phrase>Bayesian</phrase> Ying-Yang (BYY) <phrase>harmony</phrase> learning [6,7], in a comparison with Variation Bayes (VB) [8] and <phrase>Bayesian</phrase> <phrase>information</phrase> criterion (BIC) [9]. Rissanen's <phrase>Minimum Description Length</phrase> (MDL) stems from another viewpoint but coincides with BIC when it is simplified to a simple <phrase>computable</phrase> criterion [10]. The hierarchy of all unknowns in a learning system makes the learning process not just an optimization but a series of hierarchically nested continuous or discrete optimizations. As summarized in [7], there are three levels of inverse problems, i.e., inverse inference from observation to inner representation, parameter learning, and <phrase>model selection</phrase>. In terms of BFA, the first level 
Training RBMs based on the signs of the <phrase>CD</phrase> approximation of the <phrase>log-likelihood</phrase> derivatives Contrastive Divergence (<phrase>CD</phrase>) learning is frequently applied to <phrase>Restricted Boltzmann Machines</phrase> (RBMs), the <phrase>building blocks</phrase> of deep believe networks. It relies on biased approximations of the <phrase>log-likelihood</phrase> <phrase>gradient</phrase>. This bias can deteriorate the learning process. It was claimed that the signs of most components of the <phrase>CD</phrase> update are equal to the corresponding signs of the <phrase>log-likelihood</phrase> <phrase>gradient</phrase>. This suggests using optimization techniques only depending on the signs. Resilient backprop-agation is such a method and we combine it with <phrase>CD</phrase> learning. However, it does not prevent divergence caused by the approximation bias.
Extracting <phrase>Protein</phrase>-<phrase>Protein</phrase> Interactions with <phrase>Language</phrase> Modelling In this <phrase>paper</phrase>, we <phrase>model</phrase> the corpus-based <phrase>relation extraction</phrase> task, namely <phrase>protein</phrase>-<phrase>protein</phrase> interaction, as a <phrase>classification problem</phrase>. In that framework, we first show that standard <phrase>machine learning</phrase> systems exploiting representations simply based on shallow <phrase>linguistic</phrase> <phrase>information</phrase> can rival <phrase>state</phrase>-of-the-<phrase>art</phrase> systems that rely on <phrase>deep linguistic</phrase> analysis. We also show that it is possible to obtain even more effective systems, still using these easy and reliable pieces of <phrase>information</phrase>, if the specifics of the extraction task and the <phrase>data</phrase> are taken into account. Our original method combining lazy learning and <phrase>language</phrase> modelling out-performs the existing systems when evaluated on the LLL2005 <phrase>protein</phrase>-<phrase>protein</phrase> interaction extraction task <phrase>data</phrase> 1 .
<phrase>Microstructure</phrase> Representation and <phrase>Reconstruction</phrase> of Heterogeneous Materials via <phrase>Deep Belief</phrase> Network for Computational Material <phrase>Design</phrase> Integrated Computational <phrase>Materials Engineering</phrase> (ICME) aims to accelerate <phrase>optimal design</phrase> of complex material systems by integrating <phrase>material science</phrase> and <phrase>design</phrase> <phrase>automation</phrase>. For tractable ICME, it is required that (1) a structural <phrase>feature space</phrase> be identified to allow <phrase>reconstruction</phrase> of new designs, and (2) the <phrase>reconstruction</phrase> process be <phrase>property</phrase>-preserving. The majority of existing structural presentation schemes rely on the designer's understanding of specific material systems to identify geometric and statistical features, which could be biased and insufficient for reconstructing physically meaningful microstructures of complex material systems. In this <phrase>paper</phrase>, we develop a <phrase>feature learning</phrase> mechanism based on convolu-tional <phrase>deep belief</phrase> network to automate a two-way conversion between microstruc-tures and their <phrase>lower</phrase>-dimensional <phrase>feature representations</phrase>, and to achieves a 1000-fold <phrase>dimension</phrase> reduction from the <phrase>microstructure</phrase> space. The proposed <phrase>model</phrase> is applied to a wide <phrase>spectrum</phrase> of heterogeneous material systems with distinct mi-crostructural features including <phrase>Ti</phrase>-6Al-4V <phrase>alloy</phrase>, Pb63-Sn37 <phrase>alloy</phrase>, <phrase>Fontainebleau</phrase> <phrase>sandstone</phrase>, and Spherical <phrase>colloids</phrase>, to produce material reconstructions that are close to the original samples with respect to 2-point correlation functions and mean critical fracture strength. This capability is not achieved by existing synthesis methods that rely on the Markovian assumption of material microstructures.
A C/<phrase>C++</phrase>-Based Functional Verification Framework Using the <phrase>SystemC</phrase> Verification <phrase>Library</phrase> This <phrase>paper</phrase> describes SoCBase-VL, which is a C/<phrase>C++</phrase> based integrated framework for SoC functional verification. It has a layered <phrase>architecture</phrase> which provides easier <phrase>test</phrase>-bench description, automatic verification of <phrase>bus</phrase> interfaces and seamless testbench migration. This framework does not require verification engineers to learn other verification languages as <phrase>long</phrase> as they have sufficient <phrase>knowledge</phrase> on both C/<phrase>C++</phrase> and <phrase>SystemC</phrase>. We have confirmed its usefulness by applying it to a <phrase>TFT-LCD</phrase> Controller verification. 1 Introduction In the dynamic verification, a set of stimuli is applied to a <phrase>design</phrase> and then, its responses are compared to the corresponding correct outputs to check its equivalence or cor-rectness. This verification approach requires a testbench that generates stimuli and checks correct outputs. Thus, the quality of verification depends on the quality of the <phrase>test</phrase>-bench. As the designs are getting more complex, however, the difficulty of authoring the testbenches is continuously growing even more rapidly. The difficulties related to the testbench <phrase>design</phrase> can be summarized as follows: As the number of the <phrase>state</phrase> in a component increases linearly, the number of <phrase>test</phrase> cases increases exponentially. Therefore, manual enumeration of each <phrase>test case</phrase> is not feasible. Several models for a component may be required at different abstraction levels. A testbench for each <phrase>model</phrase> should be redesigned to verify the <phrase>model</phrase>. Describing a testbench often requires a deep and thorough understanding on <phrase>domain-specific</phrase> <phrase>knowledge</phrase>. e.g. <phrase>Bus</phrase> Specification. A quantitative measure of the quality of verification is needed. Otherwise, the quality of verification tends to depend on that of verification engineers. To alleviate those problems, many researchers and <phrase>EDA</phrase> vendors offer tools for testbench authoring [1-5]. The Sys-temC Verification <phrase>Library</phrase> (SCV) is an extension of Sys-temC for easier testbench authoring which provides constrained <phrase>randomization</phrase> and transaction level tracing[1]. SoCBase-VL is another extension of the SCV, which additionally provides a layered <phrase>architecture</phrase> for easier <phrase>test</phrase>-bench description, seamless testbench migration, and an automatic verification of <phrase>bus</phrase> interfaces. It also provides the Coverage Monitor Modeling <phrase>Library</phrase> (<phrase>CML</phrase>) for functional coverage monitoring. In this <phrase>paper</phrase>, we explain our layered testbench <phrase>architecture</phrase> in Section 2 and the <phrase>CML</phrase> in Section 3. In Section 4, we briefly introduce how to use our framework through a practical example. The summary and future works are given in Section 5. A H/W component (or a system) can have several abstraction level models: transaction level <phrase>model</phrase>, RT-level <phrase>model</phrase>, <phrase>FPGA</phrase> <phrase>prototype</phrase> and <phrase>Silicon</phrase>. We propose a layered 
Facilitating Collaborative <phrase>Knowledge</phrase> <phrase>Construction</phrase> This <phrase>paper</phrase> will describe a detailed analysis of a <phrase>problem-based learning</phrase> group to understand how an expert facilitator supports collaborative <phrase>knowledge</phrase> <phrase>construction</phrase>. The study examines the questions and statements that students and the facilitator generated as they traversed a complex conceptual space. The facilitator tended to use <phrase>open-ended</phrase> metacognitive questioning and never offered new ideas. His contributions built on the students thinking. These moves helped support deep <phrase>student</phrase> engagement with conceptual <phrase>knowledge</phrase>. Several specific strategies were identified that supported the goals of helping students construct causal explanations, reason effectively, and become self-<phrase>directed</phrase> learners. Studying facilitation in a face-to-face situation provides some guidance in designing support to use in an online <phrase>problem-based learning</phrase> environment; however, considerable adaptation is necessary as some facilitation can be built into the system but other facilitation may need to be done by a <phrase>human</phrase> <phrase>tutor</phrase>. Implications for CSCL system <phrase>design</phrase> for <phrase>problem-based learning</phrase> as well as preliminary experience with an online <phrase>PBL</phrase> system is discussed.
<phrase>Mental Health</phrase>, Post-<phrase>secondary Education</phrase>, and <phrase>Information</phrase> Communications <phrase>Technology</phrase> The primary aim of this chapter is to explore the use of <phrase>information and communications technology</phrase> (ICT) in post-<phrase>secondary education</phrase> to provide opportunities for students with <phrase>mental health</phrase> difficulties to remain engaged in their studies during times of <phrase>mental illness</phrase>. Higher incompletion rates are particularly concerning amongst this group. The authors discuss how improved outcomes can be achieved through effective use of ICT. This is particularly important from a <phrase>human rights</phrase> perspective so that people diagnosed with <phrase>mental illness</phrase> are afforded the same opportunities as other members of the <phrase>community</phrase>. Strategies afforded by ICT tools that are essential for supporting students with <phrase>mental illness</phrase> to optimise their chances of success in their post-<phrase>secondary education</phrase> outcomes are outlined. The authors combine <phrase>mental health</phrase> and <phrase>human</phrase>-computer interaction (HCI) to argue for the need to <phrase>design</phrase> appropriate instructional ICT strategies to support students experiencing <phrase>mental illness</phrase> to remain engaged with their studies. ICT has evolved with powerful and unique features, offering special applications such as <phrase>educational software</phrase>, <phrase>eCommerce</phrase>, and <phrase>healthcare</phrase>. Yet, very little is being said about how to <phrase>streamline</phrase> these applications as effective HCI environments to enhance <phrase>mental health</phrase> and wellbeing. The chapter explores the positive and negative impact of ICT tools on <phrase>teaching and learning</phrase>. In considering <phrase>mental health</phrase> and post-<phrase>secondary education</phrase>, it focuses on <phrase>human rights</phrase> issues of access and equity, disclosure, and stigma. Authors suggest that ICT can enable students to remain engaged with their learning in <phrase>general</phrase>, while at the same time promote a deep sense of <phrase>community</phrase>.
Application <phrase>Feedback</phrase> in Guiding a Deep-Layered <phrase>Perception</phrase> <phrase>Model</phrase> Deep-layer <phrase>machine learning</phrase> architectures continue to emerge as a promising biologically-inspired framework for achieving scalable <phrase>perception</phrase> in <phrase>artificial</phrase> agents. <phrase>State</phrase> inference is a consequence of robust <phrase>perception</phrase>, allowing the agent to interpret the environment with which it interacts and map such interpretation to desirable actions. However, in existing <phrase>deep learning</phrase> schemes, the <phrase>perception</phrase> process is guided purely by spatial regularities in the observations, with no <phrase>feedback</phrase> provided from the <phrase>target</phrase> application (e.g. classification, control). In this <phrase>paper</phrase>, we propose a simple yet powerful <phrase>feedback</phrase> mechanism, based on adjusting the sample presentation distribution, which guides the <phrase>perception</phrase> <phrase>model</phrase> in allocating resources for patterns observed. As a result, a much more focused <phrase>state</phrase> inference can be achieved leading to greater accuracy and overall performance. The proposed <phrase>paradigm</phrase> is demonstrated on a <phrase>small-scale</phrase> yet complex <phrase>image recognition</phrase> task, clearly illustrating the advantage of incorporating <phrase>feedback</phrase> in a <phrase>deep-learning</phrase> based <phrase>cognitive architecture</phrase>.
Tempering <phrase>Backpropagation</phrase> Networks: Not All Weights are Created Equal <phrase>Backpropagation</phrase> <phrase>learning algorithms</phrase> typically collapse the network's structure into a <phrase>single</phrase> <phrase>vector</phrase> of weight parameters to be optimized. We suggest that their performance may be improved by utilizing the structural <phrase>information</phrase> instead of discarding it, and introduce a framework for " tempering " each weight accordingly. In the tempering <phrase>model</phrase>, activation and error signals are treated as approximately <phrase>independent</phrase> <phrase>random variables</phrase>. The characteristic scale of weight changes is then matched to that of the residuals, allowing structural properties such as a node's fan-in and fan-out to affect the local learning rate and backpropagated error. The <phrase>model</phrase> also permits calculation of an <phrase>upper</phrase> bound on the global learning rate for batch updates, which in turn leads to different update rules for bias vs. non-bias weights. This approach yields hitherto unparalleled performance on the <phrase>family</phrase> relations benchmark, a deep <phrase>multi-layer</phrase> network: for both batch learning with <phrase>momentum</phrase> and the delta-bar-delta <phrase>algorithm</phrase>, convergence at the optimal learning rate is sped up by more than an <phrase>order</phrase> of <phrase>magnitude</phrase>.
<phrase>Deep Learning</phrase> <phrase>Design</phrase> for <phrase>Technology</phrase> Enhanced Learning <phrase>Deep Learning</phrase> <phrase>Design</phrase> for <phrase>Technology</phrase> Enhanced Learning To address the salient challenge of designing <phrase>Technology</phrase> Enhanced Learning (TEL) amidst wide-ranging and profound technological changes, within the <phrase>Web 2.0</phrase> <phrase>landscape</phrase> and beyond, we are developing the notion of <phrase>Deep Learning</phrase> <phrase>Design</phrase> (DLD). This is a <phrase>paradigm</phrase> that we hold is important to both better understanding and realising learning in the <phrase>digital</phrase> age that counters the sort of <phrase>technological determinism</phrase> that is unhealthy for the field of learning. So this article will: consider the current challenges of learning <phrase>design</phrase>; explain why the challenges raised necessitate the introduction of this new approach; and, exemplify and map this new notion of <phrase>design</phrase> to two <phrase>large-scale</phrase> TEL initiatives. These are projects in <phrase>Digital</phrase> Dialogue <phrase>Games</phrase> (DDGs) and Reusable Learning Objects. Finally some implications are considered and some conclusions are drawn. Abstract: To address the salient challenge of designing <phrase>Technology</phrase> Enhanced Learning (TEL) amidst wide-ranging and profound technological changes, within the <phrase>Web 2.0</phrase> <phrase>landscape</phrase> and beyond, we are developing the notion of <phrase>Deep Learning</phrase> <phrase>Design</phrase> (DLD). This is a <phrase>paradigm</phrase> that we hold is important to both better understanding and realising learning in the <phrase>digital</phrase> age that counters the sort of <phrase>technological determinism</phrase> that is unhealthy for the field of learning. So this article will: consider the current challenges of learning <phrase>design</phrase>; explain why the challenges raised necessitate the introduction of this new approach; and, exemplify and map this new notion of <phrase>design</phrase> to two <phrase>large-scale</phrase> TEL initiatives. These are projects in <phrase>Digital</phrase> Dialogue <phrase>Games</phrase> (DDGs) and Reusable Learning Objects. Finally some implications are considered and some conclusions are drawn.
An Application of Nonlinear Resistive Networks in <phrase>Computer Vision</phrase> an Application of Nonlinear Resistive Networks in <phrase>Computer Vision</phrase> In this <phrase>thesis</phrase>, an improved <phrase>optical flow</phrase> <phrase>algorithm</phrase> is presented, as well as a hardware called "Tanh" component. The new approach performs an optimization that reduces the error at spatial discontinuities, and increases the computational speed using analog circuit implementation. Simple <phrase>simulation</phrase> of this <phrase>design</phrase> is tested using HSPICE. We also build a simulator for a complicated circuit using C, which focuses more on speed, and less on transient. For image smoothing and segmentation problems and <phrase>optical flow</phrase> problems, a series of <phrase>test</phrase> images are fed to both the <phrase>resistor</phrase> network and the so-called "TANH" network to determine how effective the Tanh network is in <phrase>image analysis</phrase>. Acknowledgments I would like to express my deepest gratitude to my supervisor, <phrase>Professor</phrase> Berthold K. <phrase>Horn</phrase>, for <phrase>giving me the opportunity</phrase> and support to work on this <phrase>thesis</phrase>, for providing me with crucial guidance and encouragement throughout years, and for serving as a <phrase>model</phrase> of an enthusiastic <phrase>teacher</phrase> and a dedicated <phrase>scientist</phrase>. I am also grateful to Dr. Richard Lanza, my <phrase>thesis</phrase> co-supervisor, for providing invaluable insight, ideas, and support. I have learned much from their expertise and am grateful for the time they have devoted to me. This work has benefited from many discussions with <phrase>Professor</phrase> John L. Wyatt about non-linear networks, <phrase>Professor</phrase> Hae-Seung Lee about analog circuit <phrase>design</phrase>, <phrase>Professor</phrase> Rahul Sarpeshkar about non-linear analog circuit <phrase>design</phrase> and Dr. Christopher Terman about non-linear circuit <phrase>simulation</phrase>. I thank all of them for their enthusiasm, <phrase>patience</phrase>, and advice. <phrase>Love</phrase> and Yajun Fang, for their help and invaluable advice in the past three years. I would also like to thank all my <phrase>friends</phrase> in <phrase>MIT</phrase> and elsewhere for the great moment we have shared in the past years, especially thank Benjamin Walter, for his advice and help in <phrase>thesis</phrase> revising. I would like to <phrase>express my deep</phrase> <phrase>love</phrase> to my <phrase>family</phrase>, especially my parents, who have granted me unconditional <phrase>love</phrase> throughout my <phrase>life</phrase>; my sisters Bizhou and Jin, my brothers Bihui and Juhui, who have given me all kinds of support in both my career and my <phrase>life</phrase>. Finally and most importantly, I would like to thank my girl friend, Xiang Xian, for her <phrase>love</phrase> and encourage.
<phrase>Artificial Intelligence</phrase> as the year 2000 approaches Approaches as a <phrase>human</phrase> being and defying an interrogator to I am aware that I have acquired a reputation for being critical of the claims made for <phrase>artificial intelligence</phrase> (<phrase>AI</phrase>). It is true that I am repelled by some of the hype that I hear and by the lack of <phrase>self-criticism</phrase> that it implies. However, the underlying <phrase>philosophical</phrase> questions have <phrase>long</phrase> been of deep interest to me. I can even claim to be the first <phrase>AI</phrase> <phrase>professional</phrase>; my definition of a <phrase>professional</phrase> being someone who performs a service and accepts a fee in return. I had read Alan Turing's <phrase>paper</phrase> "Calculating Machinery and <phrase>Intelligence</phrase>" when it appeared in Mind in 1950, and was deeply impressed. Without attempting to emulate Turing's erudition-or the wit in which he clothed it-I wrote a <phrase>short</phrase> article on the same subject and offered it to the <phrase>editor</phrase> of The <phrase>Spectator</phrase>, a <phrase>British</phrase> <phrase>journal</phrase> devoted to contemporary affairs. The <phrase>editor</phrase> accepted it and I duly received a fee. Apart from his <phrase>mathematical</phrase> papers, I still consider the <phrase>paper</phrase> in o ~ Mind to be the best thing <phrase>Turing</phrase> o ever wrote. He began with the z question "Can machines think?" In z that form, he found the question to be unsatisfactorily formulated. An attempt to extract the essential o underlying point of interest <phrase>led</phrase> him-to propose the famous <phrase>Turing test</phrase>. p-Instead of asking whether a partic-~ ular machine could think, he suggested that one should instead ask whether it could <phrase>pass</phrase> this <phrase>test</phrase>. The <phrase>test</phrase> involved the machine posing determine whether it was a man or a woman. <phrase>Turing</phrase> admitted that he ]c strong arguments to put form favor of the view that a digita puter would one day be able his <phrase>test</phrase>, although he was incli believe that it would. He <phrase>ma</phrase> interesting suggestion that t/c chine might be equipped learning program and then like a child. The way he put that the machine might <phrase>bq</phrase> grammed to simulate a child'! rather than an adult's <phrase>brain</phrase> brings out the point that th is intimately connected learning. <phrase>Turing</phrase> was quite aware that the <phrase>education</phrase> of a machine would, like that of a child, be a <phrase>long</phrase>, drawn-out process. He also saw practical difficulties. <phrase>TI</phrase> computer would not have 1(and could not be sent to s& like a normal child. Even ifth ciency could be overcome by ' <phrase>engineering</phrase>" he was afrai other children might make sive 
Generalized Autoencoder: A <phrase>Neural Network</phrase> Framework for <phrase>Dimensionality Reduction</phrase> The autoencoder <phrase>algorithm</phrase> and its deep version as traditional <phrase>dimensionality reduction</phrase> methods have achieved <phrase>great success</phrase> via the powerful representability of <phrase>neural networks</phrase>. However, they just use each instance to reconstruct itself and ignore to explicitly <phrase>model</phrase> the <phrase>data</phrase> relation so as to discover the underlying effective <phrase>manifold</phrase> structure. In this <phrase>paper</phrase>, we propose a <phrase>dimensionality reduction</phrase> method by <phrase>manifold</phrase> learning, which iteratively explores <phrase>data</phrase> relation and use the relation to pursue the <phrase>manifold</phrase> structure. The method is realized by a so called " generalized autoen-coder " (GAE), which extends the traditional autoencoder in two aspects: (1) each instance x i is used to reconstruct a set of instances {x j } rather than itself. (2) The <phrase>reconstruction</phrase> error of each instance (||x j x i || 2) is weighted by a <phrase>relational</phrase> <phrase>function</phrase> of x i and x j defined on the learned <phrase>manifold</phrase>. Hence, the GAE captures the structure of the <phrase>data</phrase> space through minimizing the weighted distances between reconstructed instances and the original ones. The generalized autoencoder provides a <phrase>general</phrase> <phrase>neural network</phrase> framework for <phrase>dimensionality reduction</phrase>. In addition, we propose a multilayer <phrase>architecture</phrase> of the generalized autoen-coder called deep generalized autoencoder to handle highly complex datasets. Finally, to evaluate the proposed methods , we perform <phrase>extensive experiments</phrase> on three datasets. The experiments demonstrate that the proposed methods achieve promising performance.
Efficient Training of Very <phrase>Deep Neural Networks</phrase> for Supervised <phrase>Hashing</phrase> In this <phrase>paper</phrase>, we propose training very <phrase>deep neural networks</phrase> (DNNs) for <phrase>supervised learning</phrase> of hash codes. <phrase>Existing methods</phrase> in this context <phrase>train</phrase> relatively " shallow " networks limited by the issues arising in back propagation (e.g. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training <phrase>algorithm</phrase> inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into <phrase>independent</phrase> <phrase>layer-wise</phrase> local updates through <phrase>auxiliary</phrase> variables. Empirically we observe that our training <phrase>algorithm</phrase> always converges and its <phrase>computational complexity</phrase> is linearly <phrase>proportional</phrase> to the number of edges in the networks. Empirically we manage to <phrase>train</phrase> DNNs with 64 <phrase>hidden layers</phrase> and 1024 nodes per layer for supervised <phrase>hashing</phrase> in about 3 hours using a <phrase>single</phrase> <phrase>GPU</phrase>. Our proposed very deep supervised <phrase>hashing</phrase> (VDSH) method <phrase>significantly outperforms</phrase> the <phrase>state</phrase>-of-the-<phrase>art</phrase> on several <phrase>benchmark datasets</phrase>.
A <phrase>Testbed</phrase> for Learning by Demonstration from <phrase>Natural Language</phrase> and RGB-Depth <phrase>Video</phrase> We are developing a <phrase>testbed</phrase> for learning by demonstration combining <phrase>spoken language</phrase> and <phrase>sensor</phrase> <phrase>data</phrase> in a natural <phrase>real-world</phrase> environment. <phrase>Microsoft</phrase> <phrase>Kinect</phrase> RGB-Depth cameras allow us to infer <phrase>high</phrase>-level <phrase>visual features</phrase> , such as the relative position of objects in space, with greater precision and less training than required by traditional systems. Speech is recognized and parsed using a " deep " <phrase>parsing</phrase> system, so that <phrase>language</phrase> features are available at the word, <phrase>syntactic</phrase>, and <phrase>semantic</phrase> levels. We collected an initial <phrase>data set</phrase> of 10 episodes of 7 individuals demonstrating how to " make <phrase>tea</phrase> " , and created a " <phrase>gold standard</phrase> " hand annotation of the actions performed in each. Finally, we are constructing " baseline " HMM-based activity recognition models using the visual and <phrase>language</phrase> features, in <phrase>order</phrase> to be ready to evaluate the performance of our future work on deeper and more structured models. Most <phrase>research</phrase> in <phrase>AI</phrase> has explored problems of <phrase>natural language understanding</phrase>, <phrase>visual perception</phrase>, and learning and reasoning with commonsense <phrase>knowledge</phrase> in isolation. Recently , however, a number of researchers have argued that such a " divide and conquer " approach has reached a point of <phrase>diminishing returns</phrase>, and that significant progress in any of the areas requires a more integrated approach. In work coming out of the <phrase>natural language</phrase> <phrase>community</phrase>, this new direction 1 has been called grounded <phrase>language</phrase> learning (Brana-van, Zettlemoyer, and Barzilay 2010; Kollar et al. 2010; Chen and Mooney 2011), while in the <phrase>machine vision</phrase> <phrase>community</phrase> people speak of <phrase>high</phrase>-level or <phrase>knowledge</phrase>-based scene understanding (Kembhavi, Yeh, and Davis 2010). It is quite challenging, however, to begin this kind of <phrase>research</phrase> on integrated <phrase>intelligence</phrase> for two significant reasons: first, the work would appear to require expertise in (at least) <phrase>natural language processing</phrase>, vision, and <phrase>knowledge representation and reasoning</phrase>; and second, it is difficult to quantitatively measure progress, because there are few if any <phrase>data</phrase> sets and previous approaches against which one can compare. Our project addresses both of these concerns, and also forms a foundation for our own future work on integrated <phrase>intelligent agents</phrase> that can be taught to recognize and assist Figure 1: A <phrase>screenshot</phrase> from the activity recognition dataset, showing the RGB and depth streams. Agent, hand and object locations are marked. with complex <phrase>real-world</phrase> tasks. We are developing a <phrase>testbed</phrase> for learning by demonstration from <phrase>natural language</phrase> and <phrase>sensor</phrase> <phrase>data</phrase>, with the initial domain of kitchen activities. 
Learning Two-Layer Contractive Encodings <phrase>Unsupervised learning</phrase> of feature hierarchies is often a good initialization for supervised training of <phrase>deep architectures</phrase>. In existing <phrase>deep learning</phrase> methods, these feature hierarchies are built <phrase>layer by layer</phrase> in a greedy <phrase>fashion</phrase> using <phrase>auto-encoders</phrase> or <phrase>restricted Boltzmann machines</phrase>. Both yield encoders, which compute linear projections followed by a smooth thresholding <phrase>function</phrase>. In this work, we demonstrate that these encoders fail to find stable features when the required computation is in the exclusive-or class. To overcome this limitation, we propose a two-layer encoder which is not restricted in the type of features it can learn. The proposed encoder can be regularized by an extension of previous work on contractive regularization. We demonstrate the advantages of two-layer encoders qualitatively, as well as on commonly used <phrase>benchmark datasets</phrase>.
Where is your dive buddy: tracking humans underwater using <phrase>spatio-temporal</phrase> features We present an <phrase>algorithm</phrase> for underwater <phrase>robots</phrase> to <phrase>track</phrase> <phrase>mobile</phrase> targets, and specifically <phrase>human</phrase> divers, by detecting periodic motion. Periodic motion is typically associated with propulsion underwater and specifically with the kicking of <phrase>human</phrase> swimmers. By <phrase>computing</phrase> local <phrase>amplitude</phrase> spectra in a <phrase>video</phrase> <phrase>sequence</phrase>, we find the location of a <phrase>diver</phrase> in the robot's field of view. We use the <phrase>Fourier transform</phrase> to extract the responses of varying intensities in the image space over time to detect characteristic low <phrase>frequency</phrase> oscillations to identify an undulating flipper motion associated with typical <phrase>gaits</phrase>. In case of detecting multiple locations that exhibit large low-<phrase>frequency</phrase> <phrase>energy</phrase> reponses, we combine the <phrase>gait</phrase> detector with other methods to eliminate false detections. We present <phrase>results</phrase> of our <phrase>algorithm</phrase> on open-<phrase>ocean</phrase> <phrase>video</phrase> footage of <phrase>swimming</phrase> divers, and also discuss possible extensions and enhancements of the <phrase>proposed approach</phrase> for tracking other objects that exhibit low-<phrase>frequency</phrase> oscillatory motion. I. INTRODUCTION In this <phrase>paper</phrase> propose a technique to allow an underwater <phrase>robot</phrase> to detect specific classes of biological motion using visual sensing. We are specifically interested in tracking humans , which has many applications including servo-control. Development of underwater autonomous vehicles (<phrase>UAV</phrase>) has made rapid progress in recent times. Equipped with a <phrase>variety</phrase> of sensors, these vehicles are becoming an essential part of sea exploration missions, both in deep-and shallow-<phrase>water</phrase> enviornments. In may practical situations the prefered applications of <phrase>UAV</phrase> technologies call for close interactions with humans. The underwater environment poses new challenges and pitfalls that invalidates preassumptions required for many established <phrase>algorithms</phrase> in autonomous <phrase>mobile</phrase> <phrase>robotics</phrase>. While truly autonomous underwater <phrase>navigation</phrase> remains an important goal, having the ability to guide an underwater <phrase>robot</phrase> using sensory inputs also has important benefits; for example, to <phrase>train</phrase> the <phrase>robot</phrase> to perform a repeatative observation or inspection task, it might very well be convenient for a <phrase>scuba diver</phrase> to perform the task as the <phrase>robot</phrase> follows and learns the trajectory. For future executions, the <phrase>robot</phrase> can utilize the the <phrase>information</phrase> collected by following the <phrase>diver</phrase> to carry out the inspection. This approach also has the added advantage of not requiring a second person tele-operating the <phrase>robot</phrase>, which simplifies the operational loop and reduces the associated overhead of <phrase>robot</phrase> deployment. Keeping such semi-autonomous behaviors in mind, we present a novel application of tracking <phrase>scuba divers</phrase> in underwater <phrase>video</phrase> footage and real-time <phrase>streaming video</phrase> for
<phrase>Large-Scale</phrase> <phrase>Deep Learning</phrase> on the YFCC100M Dataset We present a work-in-progress snapshot of learning with a 15 billion parameter <phrase>deep learning</phrase> network on HPC architectures applied to the largest publicly available natural image and <phrase>video</phrase> dataset released to-date. Recent advancements in unsupervised <phrase>deep neural networks</phrase> suggest that scaling up such networks in both <phrase>model</phrase> and training dataset size can yield <phrase>significant improvements</phrase> in the learning of concepts at the highest layers. We <phrase>train</phrase> our three-layer <phrase>deep neural network</phrase> on the <phrase>Yahoo</phrase>! <phrase>Flickr</phrase> <phrase>Creative Commons</phrase> 100M dataset. The dataset comprises approximately 99.2 million images and 800, 000 user-created videos from Yahoo's <phrase>Flickr</phrase> image and <phrase>video</phrase> sharing platform. Training of our network takes eight days on 98 <phrase>GPU</phrase> nodes at the <phrase>High Performance Computing</phrase> <phrase>Center</phrase> at <phrase>Lawrence Livermore National Laboratory</phrase>. Encouraging preliminary <phrase>results</phrase> and <phrase>future research</phrase> directions are presented and discussed.
From <phrase>Neural Networks</phrase> to <phrase>Deep Learning</phrase>: zeroing in on the <phrase>human brain</phrase> Pondering the <phrase>brain</phrase> with the help of <phrase>machine learning</phrase> expert Andrew Ng and researcher-turned-<phrase>author</phrase>-turned-<phrase>entrepreneur</phrase> <phrase>Jeff Hawkins</phrase>.
<phrase>Deep Reinforcement Learning</phrase> with Experience Replay Based on Sarsa SARSA, as one kind of on-policy <phrase>reinforcement learning</phrase> methods, is integrated with <phrase>deep learning</phrase> to solve the <phrase>video games</phrase> control problems in this <phrase>paper</phrase>. We use <phrase>deep convolutional</phrase> <phrase>neural network</phrase> to estimate the <phrase>state</phrase>-<phrase>action</phrase> value, and SARSA learning to update it. Besides, experience replay is introduced to make the training process suitable to scalable <phrase>machine learning</phrase> problems. In this way, a new <phrase>deep reinforcement learning</phrase> method, called deep SARSA is proposed to solve complicated control problems such as imitating <phrase>human</phrase> to <phrase>play</phrase> <phrase>video games</phrase>. From the experiments <phrase>results</phrase>, we can conclude that the deep SARSA learning shows better performances in some aspects than <phrase>deep Q</phrase> learning.
Automated annotation of keywords for <phrase>proteins</phrase> related to mycoplasmataceae using <phrase>machine learning</phrase> techniques <phrase>MOTIVATION</phrase> With the increase in submission of sequences to <phrase>public</phrase> <phrase>databases</phrase>, the curators of these are not able to cope with the amount of <phrase>information</phrase>. The <phrase>motivation</phrase> of this work is to generate a system for automated annotation of <phrase>data</phrase> we are particularly interested in, namely <phrase>proteins</phrase> related to the Mycoplasmataceae <phrase>family</phrase>. Following previous works on automatic annotation using symbolic <phrase>machine learning</phrase> techniques, the present work proposes a method of automatic annotation of keywords (a part of the <phrase>SWISS</phrase>-PROT annotation procedure), and the validation, by an expert, of the annotation rules generated. The aim of this procedure is twofold: to complete the annotation of keywords of those <phrase>proteins</phrase> which is far from adequate, and to produce a <phrase>prototype</phrase> of the validation environment, which is aimed at an expert who does not have a deep <phrase>knowledge</phrase> of the structure of the current <phrase>databases</phrase> containing the necessary <phrase>information</phrase> s/he needs. <phrase>RESULTS</phrase> As for the first objective, a rate of correct keywords annotation of 60% is reported in the <phrase>literature</phrase>. Our preliminary <phrase>results</phrase> show that with a slightly different method, applied this method to <phrase>data</phrase> related to Mycoplasmataceae only, we are able to increase that rate of correct annotation.
IITP: A Supervised Approach for Disorder Mention Detection and Disambiguation In this <phrase>paper</phrase> we briefly describe our supervised <phrase>machine learning</phrase> approach for disorder mention detection system that we submitted as part of our participation in the SemEval-2014 Shared task. The main goal of this task is to build a system that automatically identifies mentions of clinical conditions from the clinical texts. The main challenge lies due in the fact that the same mention of concept may be represented in many surface forms. We develop the system based on the supervised <phrase>machine learning</phrase> <phrase>algorithms</phrase>, namely <phrase>Conditional Random Field</phrase> and <phrase>Support Vector Machine</phrase>. One appealing characteristics of our system is that most of the features for learning are extracted automatically from the given training or <phrase>test</phrase> datasets without using deep <phrase>domain specific</phrase> resources and/or tools. We submitted three <phrase>runs</phrase>, and best performing system is based on <phrase>Conditional Random Field</phrase>. For task A, it shows the precision, recall and F-measure values of 50.00%, 47.90% and 48.90%, respectively under the strict matching criterion. When the matching criterion is relaxed, it shows the precision, recall and F-measure of 81.50%, 79.70% and 80.60%, respectively. For task B, we obtain the accuracies of 33.30% and 69.60% for the relaxed and strict matches, respectively.
<phrase>Large-Scale</phrase> <phrase>Deep Learning</phrase> For Building Intelligent Computer Systems For the past five years, the <phrase>Google</phrase> <phrase>Brain</phrase> team has focused on <phrase>conducting</phrase> <phrase>research</phrase> in difficult problems in <phrase>artificial intelligence</phrase>, on building <phrase>large-scale</phrase> computer systems for <phrase>machine learning</phrase> <phrase>research</phrase>, and, in collaboration with many teams at <phrase>Google</phrase>, on applying our <phrase>research</phrase> and systems to dozens of <phrase>Google</phrase> <phrase>products</phrase>. Our group has recently open-sourced the TensorFlow system (tensorflow.org), a system designed to easily express machine ideas, and to quickly <phrase>train</phrase>, evaluate and deploy <phrase>machine learning</phrase> systems. In this <phrase>talk</phrase>, I'll highlight some of the <phrase>design</phrase> decisions we made in building TensorFlow, discuss <phrase>research</phrase> <phrase>results</phrase> <phrase>produced</phrase> within our group, and describe ways in which these ideas have been applied to a <phrase>variety</phrase> of problems in Google's <phrase>products</phrase>, usually in close collaboration with other teams. This <phrase>talk</phrase> describes joint work with many people at <phrase>Google</phrase>.
DeepSetNet: Predicting Sets with <phrase>Deep Neural Networks</phrase> This <phrase>paper</phrase> addresses the task of set prediction using <phrase>deep learning</phrase>. This is important because the output of many <phrase>computer vision</phrase> tasks, including image tagging and <phrase>object detection</phrase>, are naturally expressed as sets of entities rather than vectors. As opposed to a <phrase>vector</phrase>, the size of a set is not fixed in advance, and it is invariant to the ordering of entities within it. We define a likelihood for a set distribution and learn its parameters using a <phrase>deep neural network</phrase>. We also derive a loss for predicting a discrete distribution corresponding to set <phrase>cardinality</phrase>. Set prediction is demonstrated on the problems of <phrase>multi-class</phrase> <phrase>image classification</phrase> and pedestrian detection. Our approach yields <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> in both cases on standard datasets.
Afterword: <phrase>Political</phrase> <phrase>Ethnography</phrase> as <phrase>Art</phrase> and <phrase>Science</phrase> <phrase>Adam</phrase> Ashforth has written one of the recent <phrase>political</phrase> ethnographies I most admire. His <phrase>Witchcraft</phrase>, <phrase>Violence</phrase>, and <phrase>Democracy</phrase> in <phrase>South Africa</phrase> draws on a total of about three years' residence during the 1990s in <phrase>Soweto</phrase> (<phrase>South</phrase> West <phrase>Township</phrase>), an <phrase>Apartheid</phrase>-built <phrase>black</phrase> <phrase>suburb</phrase> of <phrase>Johannesburg</phrase>, plus subsequent visits to his adopted <phrase>family</phrase> and <phrase>friends</phrase> there. Earlier, Ashforth wrote an impressive historical analysis of the process by which <phrase>Apartheid</phrase> took shape (Ashforth, 1990). But preparation for his <phrase>book</phrase> on <phrase>witchcraft</phrase>, <phrase>violence</phrase>, and <phrase>democracy</phrase> plunged him shoulder-deep into <phrase>ethnography</phrase>. Through first hand observation, personal intervention, and incessant interrogation of his acquaintances, Ashforth built up a powerful picture of coping, strife, and hope amid vicious <phrase>violence</phrase>. Ashforth's <phrase>ethnographic</phrase> involvement forced him to abandon many a preconceived category and explanation of struggle during and after <phrase>Apartheid</phrase>. Ashforth's <phrase>ethnography</phrase> yielded remarkable, even disturbing, <phrase>results</phrase>. His analysis persuades me, at least, of two surprising conclusions I <phrase>long</phrase> resisted when hearing them from <phrase>Adam</phrase>: first, that no one can make sense of local <phrase>South African</phrase> <phrase>politics</phrase> without understanding the enormous part played by fears about, accusations of, and reactions to <phrase>witchcraft</phrase> in Soweto's (and, by extension, <phrase>South</phrase> Africa's) everyday <phrase>politics</phrase>; second, that no one can hope to deal with <phrase>South</phrase> Africa's devastating <phrase>AIDS</phrase> <phrase>epidemic</phrase> or build local-level <phrase>democracy</phrase> without confronting <phrase>witchcraft</phrase> directly. Many a <phrase>political</phrase> <phrase>ethnographer</phrase> will resonate to Ashforth's reflection: Fortunately, from my first day in <phrase>Soweto</phrase> I was blessed with remarkable <phrase>friends</phrase> who guided me through the pleasures and perils of <phrase>life</phrase> in the <phrase>township</phrase>. They steered me toward what little understanding of their world I can now claim, though they do not always agree with the way I have come to understand this place. I have read widely in the years since I began getting to know <phrase>Soweto</phrase>, but the essence of whatever I know about this place I have learned through my <phrase>friends</phrase>: how I know it is by being there as a friend. This is both the strength and the weakness of what follows. For what I came
<phrase>AENEID</phrase>: a generic <phrase>lithography</phrase>-<phrase>friendly</phrase> detailed <phrase>router</phrase> based on post-RET <phrase>data</phrase> learning and hotspot detection In the <phrase>era</phrase> of deep sub-<phrase>wavelength</phrase> <phrase>lithography</phrase> for <phrase>nanometer</phrase> <phrase>VLSI</phrase> designs, manufacturability and yield issues are critical and need to be addressed during the key physical <phrase>design</phrase> implementation stage, in particular detailed routing. However, most existing studies for <phrase>lithography</phrase>-<phrase>friendly</phrase> routing suffer from either huge run-time due to the intensive <phrase>lithographic</phrase> computations involved, or severe loss of quality of <phrase>results</phrase> because of the inaccurate predictive models. In this <phrase>paper</phrase>, we propose <i><phrase>AENEID</phrase></i> - a fast, generic and <phrase>high</phrase> performance <phrase>lithography</phrase>-<phrase>friendly</phrase> detailed <phrase>router</phrase> for enhanced manufacturability. <i><phrase>AENEID</phrase></i> combines novel hotspot detection and routing path prediction techniques through modern <i><phrase>data</phrase> learning</i> methods and applies them at the detailed routing stage to drive <phrase>high fidelity</phrase> <phrase>lithography</phrase>-<phrase>friendly</phrase> routing. Compared with existing litho-<phrase>friendly</phrase> routing works, <i><phrase>AENEID</phrase></i> demonstrates 26% to 66% (avg. 50%) of <phrase>lithography</phrase> hotspot reduction at the cost of only 18%-38% (avg. 30%) of run-time overhead.
<phrase>Collaborative Learning</phrase> with the <phrase>Cognitive</phrase> <phrase>Tutor</phrase> <phrase>Algebra</phrase> 1 <phrase>Collaborative Learning</phrase> with the <phrase>Cognitive</phrase> <phrase>Tutor</phrase> <phrase>Algebra</phrase>. an <phrase>Experimental</phrase> Classroom Study. <phrase>Collaborative Learning</phrase> with the <phrase>Cognitive</phrase> <phrase>Tutor</phrase> <phrase>Algebra</phrase> 2 Interest in developing improved methods for <phrase>mathematics</phrase> instruction has increased since TIMSS and <phrase>PISA</phrase>. In our project, we investigate a new way to promote learning in <phrase>mathematics</phrase>: we enhanced the <phrase>Cognitive</phrase> <phrase>Tutor</phrase> <phrase>Algebra</phrase>, a computer-based <phrase>intelligent tutoring</phrase> system for <phrase>mathematics</phrase> at the <phrase>high school</phrase> level, to a <phrase>collaborative learning</phrase> setting. Although the <phrase>Algebra</phrase> <phrase>Tutor</phrase> has shown to increase learning substantially, there are also several shortcomings. For instance, learning with the <phrase>Algebra</phrase> <phrase>Tutor</phrase> places emphasis on improving students' <phrase>problem solving</phrase> skills, yet a <phrase>deep understanding</phrase> of underlying <phrase>mathematical</phrase> concepts is not necessarily achieved. To reduce these shortcomings, we extended the <phrase>learning environment</phrase> to a dyadic setting, thus adding new learning opportunities such as the possibility to mutually elaborate on the learning content. A script was developed to guide students' interaction and to ensure that students profit from these new learning opportunities. Our script followed a <phrase>general</phrase> <phrase>jigsaw</phrase>-scheme, i.e. it distributed expertise for the problem between partners and allowed them to prepare individually for the following interaction, thus laying the grounds for effective collaboration. During the collaboration, the script provided additional support: it prompted fruitful interaction and adaptively supported students as they encountered difficulties. Following each problem, it guided the dyads to reflect on their interaction in <phrase>order</phrase> to improve subsequent collaboration. In an <phrase>experimental</phrase> classroom study taking place over the course of one week, we compared three conditions to evaluate the impact of collaboration in a <phrase>cognitive</phrase> tutoring setting: individual learning, unscripted <phrase>collaborative learning</phrase>, and scripted <phrase>collaborative learning</phrase>. After a two-day learning phase, several post <phrase>tests</phrase> were administered to assess learning on three levels: reproduction, transfer and future learning. In the collaborative reproduction <phrase>test</phrase>, we found a higher need for <phrase>cognitive</phrase> <phrase>tutor</phrase> assistance in scripted students with low <phrase>prior knowledge</phrase> (note that this was the first post <phrase>test</phrase> after script support had been removed). In the subsequent individual reproduction <phrase>test</phrase>, however, differences between conditions were no longer observed. In fact, the <phrase>learning outcomes</phrase> of students in the two collaborative conditions were comparable to those in the individual condition even though collaborative students had solved fewer problems, i.e. had had less practice, during the learning phase. Analysis of the transfer <phrase>test</phrase> did not reveal differences between conditions. Finally, scripted collaboration better prepared students for future <phrase>collaborative learning</phrase> situations as compared to the unscripted collaboration condition. In addition to presenting the study and its <phrase>results</phrase>, this <phrase>paper</phrase> discusses the advantages and methodological challenges 
Learning to Relate Images A fundamental operation in many vision tasks, including motion understanding, <phrase>stereopsis</phrase>, visual odometry, or invariant recognition, is establishing correspondences between images or between images and <phrase>data</phrase> from other modalities. Recently, there has been increasing interest in learning to infer correspondences from <phrase>data</phrase> using <phrase>relational</phrase>, spatiotemporal, and <phrase>bilinear</phrase> variants of <phrase>deep learning</phrase> methods. These methods use multiplicative interactions between <phrase>pixels</phrase> or between features to represent correlation patterns across multiple images. In this <phrase>paper</phrase>, we review the recent work on <phrase>relational</phrase> <phrase>feature learning</phrase>, and we provide an analysis of the role that multiplicative interactions <phrase>play</phrase> in learning to encode relations. We also discuss how square-pooling and complex <phrase>cell</phrase> models can be viewed as a way to represent multiplicative interactions and thereby as a way to encode relations.
Analyzing <phrase>Human</phrase> Tutorial Dialogues for Cohesion and Coherence during <phrase>Hypermedia</phrase> Learning of a Complex <phrase>Science</phrase> Topic We analyzed cohesion and coherence of 79 think-aloud transcription logs from a <phrase>human</phrase> tutorial dialogue study investigating the effect of tutoring on <phrase>college</phrase> students' learning about the <phrase>circulatory</phrase> system with <phrase>hypermedia</phrase>. The study involves randomly assigning 82 non-<phrase>science</phrase> <phrase>majors</phrase> to either the self-regulated learning (SRL, control condition) or the externally-regulated learning (ERL, <phrase>experimental</phrase> condition with a <phrase>human</phrase> <phrase>tutor</phrase>). The corpus we examined contained a total of 1,445 pages. We used Coh-Metrix, an automated <phrase>web-based</phrase> tool developed to evaluate text and discourse, in <phrase>order</phrase> to assess cohesion and coherence of the tutorial dialogues <phrase>produced</phrase> between a <phrase>human</phrase> <phrase>tutor</phrase> and low-<phrase>domain knowledge</phrase> <phrase>college</phrase> students during 79 tutoring sessions. Our findings showed that there were significant differences in the tutorial dialogues of the ERL tutoring condition versus those of the SRL control condition in the co-referential cohesion, the <phrase>semantic</phrase>/conceptual overlap, the causal ratio, the standard readability formulas, the incidence scores of connectives, the number of words, the number of sentences, the number of turns, the <phrase>average</phrase> sentences per turn, and the <phrase>average</phrase> words per sentence. Our findings have implications for the <phrase>design</phrase> of intelligent tutorial dialogue in <phrase>hypermedia</phrase> systems developed to improve learners' deep <phrase>conceptual understanding</phrase> of complex and challenging <phrase>science</phrase> topics. Cohesion and Coherence in Tutorial Dialogues Cohesion and coherence of text and discourse are critical components affecting text processing and comprehension. According to Graesser et al. (2004), coherence indicates characteristics of mental models that readers establish during comprehension, whereas cohesion refers to features of <phrase>text-based</phrase> <phrase>information</phrase> such as argument overlap, discourse markers, anaphora, and connectives. Coherence, according to Graesser et al. (2003), is the final <phrase>psychological</phrase> construct that readers build in their mind. Readers normally construct a coherent representation in terms of various cohesion and coherence relations during comprehension. For example, they attain coherence of a text by identifying various coherence relations (Sanders, Spooren, & Noordman, 1992) or <phrase>rhetorical</phrase> structures (Mann & Thompson, 1988) in the text, linking <phrase>text-based</phrase> segments, and combining the <phrase>text-based</phrase> <phrase>information</phrase> with their prior Similarly, students can construct coherent mental models for subject matters while learning with the help of a <phrase>human</phrase> <phrase>tutor</phrase> in a classroom or while interacting with a computer-<phrase>based learning environment</phrase> such as a <phrase>hypermedia</phrase> environment (e.g., Azevedo et al., 2004, 2005, in press). Recent <phrase>empirical studies</phrase> have shown that cohesion and coherence are <phrase>cardinal</phrase> to examine whether readers generate inferences to link text units during on-line comprehension to combine the <phrase>text-based</phrase> <phrase>information</phrase> with 
The Deep Versus the Shallow: Effects of Co-Speech Gestures in Learning From Discourse This study concerned the role of gestures that accompany discourse in <phrase>deep learning</phrase> processes. We assumed that co-speech gestures favor the <phrase>construction</phrase> of a complete <phrase>mental representation</phrase> of the discourse content, and we tested the predictions that a discourse accompanied by gestures, as compared with a discourse not accompanied by gestures, should result in better recollection of conceptual <phrase>information</phrase>, a greater number of discourse-based inferences drawn from the <phrase>information</phrase> explicitly stated in the discourse, and poorer recognition of verbatim of the discourse. The <phrase>results</phrase> of three experiments confirmed these predictions.
Learning Rules to Pre-process Web <phrase>Data</phrase> for Automatic Integration <phrase>Web pages</phrase> such as product catalogues and <phrase>web sites</phrase> resulting from querying a <phrase>search engine</phrase> often follow a global layout template which facilitates the retrieval of <phrase>information</phrase> for a user. In this <phrase>paper</phrase> we present a technique which makes such content machine-processable by extracting and transforming it into tabular form. We achieve this goal via ViPER, our fully automatic wrapper system, while localizing and extracting structured <phrase>data records</phrase> from suchlike <phrase>web pages</phrase> following a sophisticated strategy based on the <phrase>visual perception</phrase> of a web page. The first contribution of this <phrase>paper</phrase> is to give deep insight into the post-processing heuristics of ViPER, which become materialized by a set of rules. Once these rules are defined, the regular content of a web page can be abstracted into a <phrase>relational</phrase> view. Second, we show that new, unseen contents rendered with the same layout , only have to be extracted by ViPER, whereas the remaining transformation can be performed by applying the learned rules accordingly.
<phrase>Computational Biology</phrase>, <phrase>Data Mining</phrase>, <phrase>Machine Learning</phrase> <phrase>Bioinformatics</phrase> tools: Experienced in common <phrase>bioinformatics</phrase> packages and tools, especially next generation sequencing <phrase>data</phrase> manipulation and analysis <phrase>software</phrase> (2011 present)-Automated discovery of interesting biological features using multiple <phrase>genomic</phrase> <phrase>time series</phrase> (ongoing)-Quality based merging of draft <phrase>genome</phrase> assemblies (ongoing)-De Novo <phrase>assembly</phrase> of ultra-deep sequencing <phrase>data</phrase> (ongoing)-<phrase>Genome</phrase> wide <phrase>SNP</phrase> discovery in <phrase>Cowpea</phrase>-<phrase>Microarray</phrase> <phrase>data analysis</phrase> to discover <phrase>genes</phrase> related to <phrase>wound healing</phrase> in <phrase>mouse</phrase> and <phrase>human</phrase>-Computational methods to study synteny between <phrase>genomes</phrase>-<phrase>Assembly</phrase> and analysis of <phrase>Malaria</phrase> <phrase>parasite</phrase> <phrase>genome</phrase> to study the <phrase>drug</phrase> resistant strains "<phrase>Wireless</phrase> and <phrase>Mobile</phrase> networks lab.", <phrase>Amirkabir University of Technology</phrase> (2006-2010)-Traffic analysis and localization in <phrase>wireless</phrase> <phrase>ad-hoc</phrase> and <phrase>sensor</phrase> networks
Choosing the Best <phrase>Bayesian</phrase> Classifier: an <phrase>Empirical Study</phrase> It is often difficult for <phrase>data</phrase> miners to know which classifier will perform most effectively in any given dataset. Usually an understanding of <phrase>learning algorithms</phrase> is combined with detailed <phrase>domain knowledge</phrase> of the dataset at hand to <phrase>lead</phrase> to the choice of a classifier. We propose an empirical framework that quantitatively assesses the accuracy of a selection of classifiers on different datasets, resulting in a set of classification rules generated by the J48 <phrase>decision tree</phrase> <phrase>algorithm</phrase>. <phrase>Data</phrase> miners can follow these rules to select the most effective classifier for their work. By optimising the parameters used for learning, a set of rules were learned that select with 78% accuracy (with 0.5% <phrase>classification accuracy</phrase> tolerance), the most effective classifier. The past 20 years have seen a dramatic increase in the amount of <phrase>data</phrase> being stored in <phrase>electronic</phrase> format. The accumulation of this <phrase>data</phrase> has taken place at an explosive rate and it has been estimated that the amount of <phrase>information</phrase> in the world <phrase>doubles</phrase> every two years [1]. Within this <phrase>ocean</phrase> of <phrase>data</phrase>, valuable <phrase>information</phrase> lies dormant. <phrase>Data mining</phrase> uses statistical techniques and advanced <phrase>algorithms</phrase> to search the <phrase>data</phrase> for hidden patterns and relationships. However, as <phrase>data</phrase> expands and the importance of <phrase>data mining</phrase> increases, a problem emerges. There are many different classifiers and many different types of dataset resulting in difficulty in knowing which will perform most effectively in any given case. It is already widely known that some classifiers perform better than others on different datasets. Usually an understanding of <phrase>learning algorithms</phrase> is combined with detailed <phrase>domain knowledge</phrase> of the dataset at hand for the choice of classi-<phrase>fier</phrase>. Experience and deep <phrase>knowledge</phrase> will of course affect the choice of the most effective classifier-but are they always right? It is always possible that another classifier may unknowingly work better. In deciding which classifier will work best for a given dataset there are two options. The first is to put all the trust in an expert's opinion based on <phrase>knowledge</phrase> and experience. The second is to run through every possible classifier that could work on the dataset, identifying rationally the one which performs best. The latter option, while being the most rigorous, would take time and require a significant amount of resources, especially with larger datasets, and as such is impractical. If the expert consistently chooses an ineffective classifier, the most effective classification rules will never be learned, and resources will 
Loading Deep Networks Is Hard 1. the Relevance of the Loading Problem The loading problem formulated by J.S. Judd seems to be a relevant <phrase>model</phrase> for supervised <phrase>connectionist</phrase> learning of the feedforward networks from the complexity point of view. It is known that loading <phrase>general</phrase> network architectures is <phrase>NP-complete</phrase> (intractable) when the (training) tasks are also <phrase>general</phrase>. Many strong restrictions on <phrase>architectural</phrase> <phrase>design</phrase> and/or on the tasks do not help to avoid the intractibility of loading. J.S. Judd concentrated on the width expanding architectures with constant depth and found a <phrase>polynomial</phrase> time <phrase>algorithm</phrase> for loading restricted shallow architectures. He suppressed the eeect of depth on loading complexity and left as an open pro-totypical <phrase>computational problem</phrase> the loading of easy regular triangular architectures that might capture the <phrase>crux</phrase> of depth diiculties. We have <phrase>proven</phrase> this problem to be <phrase>NP-complete</phrase>. This result does not give much hope for the existence of an eecient <phrase>algorithm</phrase> for loading of the deep networks. The <phrase>research</phrase> of the <phrase>neural network</phrase> models for the computational exploitation has been rapidly developed in the last years. This fact is connrmed by the existence of many <phrase>journals</phrase> and conferences concerning this topic. One of the <phrase>basic</phrase> reasons for this exciting activity in this <phrase>area</phrase> is the successful practical use of the <phrase>learning algorithm</phrase> back propagation (Rumelhart, et al. 1986) for the multi-layered <phrase>neural network</phrase>, and it is not by chance that this is the most widely applied <phrase>neural network</phrase> <phrase>architecture</phrase>. The defect of this <phrase>algorithm</phrase>, likewise of many others, is that it is very time consuming and that learning bigger networks can be intractable. The eeort to speed up this <phrase>algorithm</phrase> has brought only limited <phrase>results</phrase>. Diicult problems cannot be 1
<phrase>Visual analytics</phrase> in support of <phrase>education</phrase> The amount of <phrase>data</phrase> about us and our world is increasing rapidly, and the capability to analyze large <phrase>data</phrase> sets---so-called <i><phrase>big data</phrase></i>---becomes a key basis of competition, underpinning new waves of <phrase>productivity</phrase> growth and <phrase>innovation</phrase>. The <phrase>big data</phrase> phenomenon is fueled by cheap sensors and <phrase>high</phrase>-throughput <phrase>simulation</phrase> models, the increasing volume and detail of <phrase>information</phrase> captured by enterprises, the rise of <phrase>multimedia</phrase>, <phrase>social media</phrase>, and the <phrase>Internet</phrase>. It exists from <phrase>social media</phrase> to <phrase>cell biology</phrase> offering unparalleled opportunities to document the inner workings of many <phrase>complex systems</phrase> [1]. <phrase>Research</phrase> by MGI and McKinsey's <phrase>Business</phrase> <phrase>Technology</phrase> Office argues that there will be a shortage of talent necessary for organizations to take advantage of <phrase>big data</phrase>. "By 2018, the <phrase>United States</phrase> alone could face a shortage of 140,000 to 190,000 people with deep analytical skills as well as 1.5 million managers and analysts with the know-how to use the analysis of <phrase>big data</phrase> to make effective decisions" [2]. In everyday <phrase>life</phrase>, people deal with large amounts of <phrase>data</phrase> regularly: online <phrase>search engines</phrase> provide access to millions of <phrase>web sites</phrase> almost instantly; <phrase>consumer</phrase> sites offer literally thousands of purchase options seamlessly; and <phrase>social media</phrase> sites let you create and benefit from extensive <phrase>social networks</phrase>. In bestselling books like <i><phrase>Freakonomics</phrase>, Super Crunchers</i> and <i>The Numerati</i>, authors illuminate how more and more decisions in <phrase>health care</phrase>, <phrase>politics</phrase>, <phrase>education</phrase>, and other sectors utilize <phrase>big data</phrase> and <phrase>data analysis</phrase> [3]. The texts highlight the growing need for specialists <i>and</i> every-day citizens to be able to understand and interpret <phrase>data</phrase>. Whether it is a table of nutritional <phrase>information</phrase>, a <phrase>graph</phrase> of <phrase>stock</phrase> prices, or a <phrase>chart</phrase> comparing <phrase>health care</phrase> plans, the skills of understanding and interpreting <phrase>data</phrase> are necessary to navigate successfully through <phrase>daily</phrase> <phrase>life</phrase>. This <phrase>talk</phrase> starts with a review of <phrase>visual analytics</phrase> projects that aim to increase our understanding of how people learn, increase the efficacy of <phrase>learning environments</phrase>, or support <phrase>decision making</phrase> in <phrase>education</phrase> [4]. The second part of the <phrase>talk</phrase> provides a theoretical framework for the <phrase>design</phrase> of effective <phrase>data analysis</phrase> workflows and insightful visualizations. It also introduces plug-and-<phrase>play</phrase> macroscope tools [5], see also http://cishell.org, that were designed for different <phrase>research</phrase> communities and are used by more than 120,000 users from 40+ countries to <phrase>design</phrase> and benefit from visualizations of complex <phrase>data</phrase>. The <phrase>talk</phrase> concludes with a discussion of challenges that arise when <phrase>visual analytics</phrase> tools are introduced to classrooms and informal <phrase>science</phrase> <phrase>education</phrase>.
A fully automated technique for constructing <phrase>FSM</phrase> abstractions of non-ideal latches in <phrase>communication</phrase> systems The <phrase>design</phrase> of a communications system is typically most effective only when each of its components can be accurately represented by a discrete, symbolic behavioural abstraction. Such abstractions, in addition to providing valuable <phrase>design</phrase> intuition, also enable highly efficient and scalable system-level <phrase>simulation</phrase>. However, given a <phrase>SPICE</phrase>-level description for a subsystem such as a latch, it is a challenge to come up with a discrete, <phrase>symbol</phrase>-level abstraction that accurately captures its continuous-time dynamics. Indeed, the manual <phrase>construction</phrase> of such an abstraction requires deep <phrase>knowledge</phrase> and understanding of the operation of the module in question; moreover, it is very time-consuming, tedious, error-prone and not easily scalable to larger designs. In recent work [1], we adapted methods from <phrase>computational learning theory</phrase> to develop an automated technique, DAE2FSM, that produces <phrase>binary</phrase> <phrase>finite state machine</phrase> (<phrase>FSM</phrase>) abstractions of non-linear analog/mixed-signal (AMS) circuits. In the present <phrase>paper</phrase>, we demonstrate the application of the DAE2FSM technique to automatically derive <phrase>FSM</phrase> abstractions for a mixed-signal communications circuit component, namely a current mode latch (<phrase>CML</phrase>) designed in IBM's 90nm <phrase>LP</phrase> process <phrase>technology</phrase>. We show that the FSMs learned by DAE2FSM not only capture the essence of the latch's behaviour during normal conditions, but also faithfully mimic its behaviour under adverse operating conditions (e.g., under lowered supply voltages). Moreover, in addition to a stand-alone <phrase>CML</phrase>, we also generate FSMs for cascades of two and three latches (such <phrase>topologies</phrase> are used in the <phrase>design</phrase> of power-efficient, <phrase>bit</phrase>-error optimised analog-to-<phrase>digital</phrase> converters). In spite of the inherent non-linearity of such systems, and in spite of the pronounced " analog-ness " of the <phrase>waveforms</phrase> in question, our <phrase>FSM</phrase> abstractions are able to produce discrete-time <phrase>symbol</phrase> sequences that closely match the <phrase>data</phrase> points obtained by sampling from continuous-time <phrase>SPICE</phrase> simulations.
<phrase>Layer-wise</phrase> learning of <phrase>deep generative models</phrase> When using deep, multi-layered architectures to build <phrase>generative models</phrase> of <phrase>data</phrase>, it is difficult to <phrase>train</phrase> all layers at once. We propose a <phrase>layer-wise</phrase> training procedure admitting a performance guarantee compared to the global optimum. It is based on an optimistic proxy of future performance, the best latent marginal. We interpret <phrase>auto-encoders</phrase> in this setting as <phrase>generative models</phrase>, by showing that they <phrase>train</phrase> a <phrase>lower</phrase> bound of this criterion. We <phrase>test</phrase> the new learning procedure against a <phrase>state</phrase> of the <phrase>art</phrase> method (stacked RBMs), and find it to <phrase>improve performance</phrase>. Both theory and experiments highlight the importance, when training <phrase>deep architectures</phrase>, of using an inference <phrase>model</phrase> (from <phrase>data</phrase> to hidden variables) richer than the <phrase>generative model</phrase> (from hidden variables to <phrase>data</phrase>).
TwinSpace: an <phrase>infrastructure</phrase> for cross-<phrase>reality</phrase> team spaces We introduce TwinSpace, a flexible <phrase>software</phrase> <phrase>infrastructure</phrase> for combining interactive workspaces and collaborative <phrase>virtual worlds</phrase>. Its <phrase>design</phrase> is grounded in the need to support deep connectivity and flexible mappings between virtual and real spaces to effectively support collaboration. This is achieved through a robust connectivity layer linking heterogeneous collections of physical and virtual devices and services, and a centralized service to manage and control mappings between physical and virtual. In this <phrase>paper</phrase> we motivate and present the <phrase>architecture</phrase> of TwinSpace, discuss our experiences and <phrase>lessons learned</phrase> in building a generic framework for collaborative cross-<phrase>reality</phrase>, and illustrate the <phrase>architecture</phrase> using two implemented examples that highlight its flexibility and <phrase>range</phrase>, and its support for <phrase>rapid prototyping</phrase>.
Towards <phrase>data</phrase>-driven mastery learning We have developed a novel <phrase>data</phrase>-driven mastery learning system to improve learning in complex procedural <phrase>problem solving</phrase> domains. This new system was integrated into an existing <phrase>logic</phrase> proof tool, and assigned as homework in a <phrase>deductive</phrase> <phrase>logic</phrase> course. <phrase>Student</phrase> performance and dropout were compared across three systems: The Deep Thought <phrase>logic</phrase> <phrase>tutor</phrase>, Deep Thought with integrated hints, and Deep Thought with our <phrase>data</phrase>-driven mastery learning system. <phrase>Results</phrase> show that the <phrase>data</phrase>-driven mastery learning system increases mastery of <phrase>target</phrase> <phrase>tutor</phrase>-actions, improves <phrase>tutor</phrase> scores, and lowers the rate of <phrase>tutor</phrase> dropout over Deep Thought, with or without provided hints.
Focusing <phrase>Formative Assessment</phrase> on the Needs of <phrase>English Language</phrase> Learners <phrase>Formative assessment</phrase> has the potential to enhance <phrase>teaching and learning</phrase>, especially for those students who face particular challenges , such as <phrase>English Language</phrase> Learners (<phrase>ELL students</phrase>). In this <phrase>paper</phrase>, we examine how <phrase>formative assessment</phrase> can enhance the <phrase>teaching and learning</phrase> of <phrase>ELL students</phrase> in particular. We highlight the opportunities and challenges inherent in integrating <phrase>formative assessment</phrase> into instruction for <phrase>ELL students</phrase> in the <phrase>era</phrase> of the <phrase>Common Core</phrase> and other " next generation " standards. We argue that in <phrase>order</phrase> to use <phrase>formative assessment</phrase> effectively with this <phrase>student</phrase> <phrase>population</phrase>, teachers must attend simultaneously to the students' needs both in learning content and skills and in developing the <phrase>English</phrase> required to express their learning. Indeed, it is the extent to which this dual attention to <phrase>language</phrase> and content learning is given that distinguishes <phrase>formative assessment</phrase> strategies to support <phrase>ELL students</phrase> from strategies for non-<phrase>ELL students</phrase>. Much progress has been made over the last decade on understanding how best to teach and assess <phrase>ELL students</phrase>, driven in no small part by the No Child Left Behind <phrase>Act</phrase> of 2001 (<phrase>NCLB</phrase>). Although support for <phrase>NCLB</phrase> has been mixed, there is widespread agreement that the <phrase>Act</phrase> is responsible for shining an important spotlight on <phrase>ELL students</phrase>' <phrase>education</phrase> and the need for <phrase>fair</phrase>, valid, and reliable assessment of <phrase>ELL students</phrase>. <phrase>NCLB</phrase> called for schools and <phrase>districts</phrase> to assess all <phrase>ELL students</phrase> and to be accountable for their achievement in both <phrase>English language</phrase> development and <phrase>academic</phrase> <phrase>knowledge</phrase> and skills, at <phrase>a level</phrase> comparable to that of their non-ELL <phrase>peers</phrase>. This push for accountability persists as the nation ushers in a new <phrase>era</phrase> of <phrase>education reform</phrase>, driven by development of and widespread support for new and more rigorous learning standards nationwide, such as the <phrase>Common Core State Standards</phrase> (CCSS), Next Generation <phrase>Science</phrase> Standards (NGSS), and commensurate standards for <phrase>English language</phrase> proficiency development. The vast majority of states have now committed to <phrase>weaving</phrase> the CCSS and corresponding " next generation " assessments into the fabric of their instruction and assessment systems. The vision embodied by the <phrase>Common Core</phrase> movement is that instruction and assessment will work hand in glove to support deep, <phrase>high</phrase>-quality learning, with career and <phrase>college</phrase> preparedness the ultimate goal for all students , including <phrase>ELL students</phrase> (Darling-<phrase>Hammond</phrase> et al., 2013). In this context, <phrase>formative assessment</phrase> has emerged as a promising <phrase>teaching and learning</phrase> strategy This <phrase>paper</phrase> is one in a series <phrase>produced</phrase> by WestEd on 
Efficient non-greedy optimization of <phrase>decision trees</phrase> <phrase>Decision trees</phrase> and <phrase>randomized</phrase> <phrase>forests</phrase> are widely used in <phrase>computer vision</phrase> and <phrase>machine learning</phrase>. Standard <phrase>algorithms</phrase> for <phrase>decision tree</phrase> induction optimize the <phrase>split</phrase> functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal <phrase>trees</phrase>. In this <phrase>paper</phrase>, we present an <phrase>algorithm</phrase> for optimizing the <phrase>split</phrase> functions at all levels of the <phrase>tree</phrase> jointly with the <phrase>leaf</phrase> parameters , based on a global objective. We show that the problem of finding optimal <phrase>linear-combination</phrase> (oblique) splits for <phrase>decision trees</phrase> is related to structured prediction with <phrase>latent variables</phrase>, and we formulate a convex-concave <phrase>upper</phrase> bound on the tree's empirical loss. The run-time of <phrase>computing</phrase> the <phrase>gradient</phrase> of the proposed surrogate objective with respect to each training exemplar is quadratic in the the <phrase>tree</phrase> depth, and thus training deep <phrase>trees</phrase> is feasible. The use of stochas-tic <phrase>gradient descent</phrase> for optimization enables effective training with <phrase>large datasets</phrase>. Experiments on several classification benchmarks demonstrate that the resulting non-greedy <phrase>decision trees</phrase> outperform greedy <phrase>decision tree</phrase> baselines.
Let the <phrase>Game</phrase> Do the Talking: The Influence of Explicitness and <phrase>Game</phrase> Behavior on Comprehension in an Educational Computer <phrase>Game</phrase> An endogenous <phrase>educational game</phrase> is a <phrase>game</phrase> where the educational content is integrated in the <phrase>game</phrase> <phrase>play</phrase> <phrase>mechanics</phrase> themselves. These <phrase>games</phrase> rely on a constructivist approach to learning, where the learner constructs <phrase>knowledge</phrase> through <phrase>concrete</phrase> experiences. Endogenous educational <phrase>games</phrase> which are specifically developed for educational purposes mostly make this purpose explicit: they make it clear in advance what is about to be learned. This <phrase>research</phrase> tried to find out how such an explicit purpose influences the <phrase>game</phrase> behavior and comprehension by developing two versions of an endogenous <phrase>educational game</phrase> about <phrase>overfishing</phrase>, one with and one without an explicit purpose. It showed that children who played the explicit version got more shallow <phrase>knowledge</phrase> and showed more active <phrase>game</phrase> behavior. The players who showed more explorative <phrase>game</phrase> behavior acquired more deep <phrase>knowledge</phrase> about the <phrase>game</phrase>.
Methodology <phrase>education</phrase> in <phrase>computing</phrase>: towards a congruent <phrase>design</phrase> approach All <phrase>major</phrase> <phrase>computing</phrase> curricula recommendations mention methodological skills and <phrase>knowledge</phrase> as an important learning objective in <phrase>undergraduate</phrase> and graduate <phrase>education</phrase>. None of those curricula recommendations, however, include a methodology course for students. One reason for that lack might be the stunning diversity of <phrase>computing</phrase> fields and the unique methods each branch of <phrase>computing</phrase> uses in their <phrase>research</phrase>. A methodology course in <phrase>computing</phrase> has to make a choice between three options: a narrow but deep specialization in some techniques and methods, a broad but superficial covering of a large number of methods, and a <phrase>higher-level</phrase> view on the principles of methodology and <phrase>research</phrase> <phrase>design</phrase>. This <phrase>paper</phrase> adopts the <phrase>high</phrase>-level approach, and presents a course description for a methodology course that aims at providing students understanding of how the elements of a <phrase>research</phrase> study link together.
Rudeness and Rapport: Insults and Learning Gains in Peer Tutoring For 20 years, researchers have envisioned <phrase>artificially intelligent</phrase> learning companions that evolve with their students as they grow and learn. However, while <phrase>communication</phrase> theory suggests that positivity decreases over time in relationships, most <phrase>tutoring systems</phrase> designed to build rapport with a <phrase>student</phrase> remain adamantly polite, and may therefore inadvertently distance the learner from the agent over time. We present an analysis of <phrase>high school</phrase> <phrase>friends</phrase> interacting in a peer tutoring environment as a step towards designing agents that sustain <phrase>long</phrase>-term pedagogical relationships with learners. We find that tu-tees and tutors use different <phrase>language</phrase> behaviors: tutees express more playful-ness and face-threat, while tutors attend more to the task. This face-threat by the tutee is associated with increased learning gains for their <phrase>tutor</phrase>. Additionally, a small sample of partners who were strangers learned less than <phrase>friends</phrase>, and in these dyads increased face-threat was negatively correlated with learning. Our findings support the idea that learning companions should gradually move towards playful face-threat as they build relationships with their students. 1 Introduction Peer tutoring, a <phrase>paradigm</phrase> in which one <phrase>student</phrase> tutors another of a similar ability, <phrase>results</phrase> in <phrase>deep learning</phrase> gains for the <phrase>tutor</phrase> [1]. Peer tutoring provides a social <phrase>motivation</phrase> for the <phrase>tutor</phrase> to attend more in <phrase>order</phrase> to effectively explain concepts [2]. In addition , the <phrase>tutor</phrase> engages in a series of <phrase>cognitive</phrase> steps that improve learning, such as constructing explanations and reflecting on errors [3]. The tutee plays an active role in this process by challenging, contradicting, and questioning the tutor's moves [3] causing the <phrase>tutor</phrase> to engage in increased reflection and self-explanation [1]. In the ITS <phrase>community</phrase>, an effort has been underway to develop virtual characters that <phrase>act</phrase> as a tutee, or teachable agent, in <phrase>order</phrase> to leverage the benefits of <phrase>human</phrase> peer tutoring [4, 5, 6]. However, most teachable agents focus on the <phrase>cognitive</phrase> elements of the interaction and, to date, none have been designed based on analyses of the social behaviors that emerge as a part of successful peer tutoring. There is therefore great opportunity to expand on the social capabilities of teachable agents in <phrase>order</phrase> to create
Visual Content Structures for <phrase>Wrapper Induction</phrase> in Building Metasearch Systems As there are more and more online sources available on the Web, it becomes very time-consuming, if not impossible, to visit and search all <phrase>web sites</phrase>, one by one. Many <phrase>search engines</phrase> has been developed to help users find <phrase>information</phrase> of their need. However, <phrase>search engines</phrase> work poor for online sources whose <phrase>data</phrase> are often in <phrase>deep web</phrase>, which is not part of surface web indexed by standard <phrase>search engines</phrase>. Metasearch is a very popular mechanism to search <phrase>deep web</phrase>. Metasearch provides the capability for users to search and access all of the <phrase>information</phrase> sources in one query submission. One of the fundamental problems in building metasearch systems is to learn wrappers which extract and integrate <phrase>data records</phrase> from query result pages returned from online sources. In this <phrase>paper</phrase>, develop an unsupervised approach for <phrase>wrapper induction</phrase> that combines visual, content and <phrase>HTML</phrase> tag <phrase>information</phrase>. Our approach first learns a visual content <phrase>model</phrase> that alleviates <phrase>HTML</phrase> tag differences among <phrase>data records</phrase>, and then finds a tag <phrase>model</phrase> from all <phrase>data records</phrase> that match the visual content <phrase>model</phrase>. Experiment shows that our approach works well for <phrase>data</phrase> sets collected from well-known <phrase>search engines</phrase> and shopping <phrase>websites</phrase>.
Dynamic goal coordination in physical agents A <phrase>general</phrase> framework for the problem of coordination of multiple competing goals in dynamic environments for physical agents is presented. This approach to goal coordination is a novel tool to incorporate a deep coordination ability to pure reactive agents. The framework is based on the notion of <phrase>multi-objective</phrase> optimization. We propose a kind of " aggregating functions " formulation with the particularity that the aggregation is weighted by means of a dynamic weighting <phrase>unitary</phrase> <phrase>vector</phrase> (S) which is dependant on the system dynamic <phrase>state</phrase> allowing the agent to dynamically coordinate the priorities of its <phrase>single</phrase> goals. This dynamic weighting <phrase>unitary</phrase> <phrase>vector</phrase> is represented as a set of n 1 angles. The dynamic coordination must be established by means of a mapping between the <phrase>state</phrase> of the agent's environment S to the set of angles i(S) using any sort of <phrase>machine learning</phrase> tool. In this work we investigate the use of <phrase>Reinforcement Learning</phrase> as a first approach to learn that mapping.
Incremental on-line <phrase>semi-supervised</phrase> learning for segmenting the <phrase>left ventricle</phrase> of the <phrase>heart</phrase> from <phrase>ultrasound</phrase> <phrase>data</phrase> Recently, there has been an increasing interest in the investigation of statistical <phrase>pattern recognition</phrase> models for the fully automatic segmentation of the <phrase>left ventricle</phrase> (LV) of the <phrase>heart</phrase> from <phrase>ultrasound</phrase> <phrase>data</phrase>. The main vulnerability of these models resides in the need of large manually annotated <phrase>training sets</phrase> for the <phrase>parameter estimation</phrase> procedure. The issue is that these <phrase>training sets</phrase> need to be annotated by clinicians, which makes this <phrase>training set</phrase> acquisition process quite expensive. Therefore, reducing the dependence on large <phrase>training sets</phrase> is important for a more extensive exploration of <phrase>statistical models</phrase> in the LV segmentation problem. In this <phrase>paper</phrase>, we present a novel incremental on-line <phrase>semi-supervised</phrase> learning <phrase>model</phrase> that reduces the need of large <phrase>training sets</phrase> for estimating the parameters of <phrase>statistical models</phrase>. Compared to other <phrase>semi-supervised</phrase> techniques, our method yields an on-line incremental retraining and segmentation instead of the off-line incremental retraining and segmentation more commonly found in the <phrase>literature</phrase>. Another <phrase>innovation</phrase> of our approach is that we use a <phrase>statistical model</phrase> based on <phrase>deep learning</phrase> architectures, which are easily adapted to this on-line <phrase>incremental learning</phrase> framework. We show that our fully automatic LV segmentation <phrase>method achieves</phrase> <phrase>state</phrase>-of-the-<phrase>art</phrase> accuracy with <phrase>training sets</phrase> containing less than twenty annotated images.
Learning Deep Features for Scene Recognition using Places <phrase>Database</phrase> Scene recognition is one of the <phrase>hallmark</phrase> tasks of <phrase>computer vision</phrase>, allowing definition of a context for <phrase>object recognition</phrase>. Whereas the tremendous recent progress in <phrase>object recognition</phrase> tasks is due to the availability of <phrase>large datasets</phrase> like ImageNet and the rise of <phrase>Convolutional Neural Networks</phrase> (CNNs) for learning <phrase>high-level</phrase> features , performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric <phrase>database</phrase> called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the <phrase>density</phrase> and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using <phrase>CNN</phrase>, we learn deep features for scene <phrase>recognition tasks</phrase>, and establish new <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> on several scene-centric datasets. A visualization of the <phrase>CNN</phrase> layers' responses allows us to show differences in the <phrase>internal representations</phrase> of object-centric and scene-centric networks.
Modelling the effects of <phrase>semantic</phrase> ambiguity in word recognition Most words in <phrase>English</phrase> are ambiguous between different interpretations; words can mean different things in different contexts. We investigate the implications of different types of <phrase>semantic</phrase> ambiguity for <phrase>connectionist</phrase> models of word recognition. We present a <phrase>model</phrase> in which there is competition to activate distributed <phrase>semantic</phrase> representations. The <phrase>model</phrase> performs well on the task of retrieving the different meanings of ambiguous words, and is able to simulate <phrase>data</phrase> reported by Rodd, Gaskell, and Marslen-Wilson [J. Mem. Lang. 46 (2002) 245] on how <phrase>semantic</phrase> ambiguity affects lexical decision performance. In particular, the network shows a disadvantage for words with multiple unrelated meanings (e.g., <phrase>bark</phrase>) that coexists with a benefit for words with multiple related word senses (e.g., twist). The ambiguity disadvantage arises because of interference between the different meanings, while the sense benefit arises because of differences in the structure of the <phrase>attractor</phrase> basins formed during learning. Words with few senses develop deep, narrow <phrase>attractor</phrase> basins, while words with many senses develop shallow, broad basins. We conclude that the mental representations of word meanings can be modelled as stable states within a <phrase>high</phrase>-dimensional <phrase>semantic</phrase> space, and that variations in the meanings of words shape the <phrase>landscape</phrase> of this space.
Auto-<phrase>gopher</phrase> -a <phrase>Wire</phrase>-line Rotary-<phrase>hammer</phrase> <phrase>Ultrasonic</phrase> Drill Developing technologies that would enable <phrase>NASA</phrase> to sample <phrase>rock</phrase>, <phrase>soil</phrase>, and <phrase>ice</phrase> by coring, drilling or abrading at a significant depth is of great importance for a large number of in-situ exploration missions as well as for <phrase>earth</phrase> applications. <phrase>Proven</phrase> techniques to sample <phrase>Mars</phrase> subsurface will be critical for future <phrase>NASA</phrase> <phrase>astrobiology</phrase> missions that will search for records of past and present <phrase>life</phrase> on the <phrase>planet</phrase>, as well as the search of <phrase>water</phrase> and other resources. A deep corer, called Auto-<phrase>Gopher</phrase>, is currently being developed as a joint effort of the JPL's NDEAA <phrase>laboratory</phrase> and <phrase>Honeybee</phrase> <phrase>Robotics</phrase> Corp. The Auto-<phrase>Gopher</phrase> is a <phrase>wire</phrase>-line rotary-<phrase>hammer</phrase> drill that combines <phrase>rock</phrase> breaking by hammering using an <phrase>ultrasonic</phrase> <phrase>actuator</phrase> and cuttings removal by rotating a fluted <phrase>bit</phrase>. The hammering mechanism is based on the <phrase>Ultrasonic</phrase>/Sonic Drill/Corer (USDC) that has been developed as an adaptable tool for many of drilling and coring applications. The USDC uses an intermediate <phrase>free</phrase>-flying <phrase>mass</phrase> to transform the <phrase>high</phrase> <phrase>frequency</phrase> vibrations of the <phrase>horn</phrase> tip into a sonic hammering of a <phrase>drill bit</phrase>. The USDC concept was used in a previous task to develop an <phrase>Ultrasonic</phrase>/Sonic <phrase>Ice</phrase> <phrase>Gopher</phrase>. The <phrase>lessons learned</phrase> from testing the <phrase>ice</phrase> <phrase>gopher</phrase> were implemented into the <phrase>design</phrase> of the Auto-<phrase>Gopher</phrase> by inducing a rotary motion onto the fluted coring <phrase>bit</phrase>. A <phrase>wire</phrase>-line version of such a system would allow penetration of significant depth without a large increase in <phrase>mass</phrase>. A <phrase>laboratory</phrase> version of the corer was developed in the NDEAA lab to determine the <phrase>design</phrase> and drive parameters of the integrated system. The <phrase>design</phrase> configuration lab version of the <phrase>design</phrase> and fabrication and preliminary testing <phrase>results</phrase> are presented in this <phrase>paper</phrase>.
Nonlinear <phrase>low-dimensional</phrase> <phrase>regression</phrase> using <phrase>auxiliary</phrase> coordinates When doing <phrase>regression</phrase> with inputs and outputs that are <phrase>high</phrase>-dimensional, it often makes sense to reduce the dimensionality of the inputs before mapping to the outputs. Much work in <phrase>statistics</phrase> and <phrase>machine learning</phrase>, such as reduced-rank <phrase>regression</phrase>, sliced inverse <phrase>regression</phrase> and their variants, has focused on linear <phrase>dimensionality reduction</phrase>, or on estimating the <phrase>dimensionality reduction</phrase> first and then the mapping. We propose a method where both the <phrase>dimensionality reduction</phrase> and the mapping can be nonlinear and are estimated jointly. Our key idea is to define an <phrase>objective function</phrase> where the <phrase>low-dimensional</phrase> coordinates are <phrase>free</phrase> parameters, in addition to the <phrase>dimensionality reduction</phrase> and the mapping. This has the effect of decoupling many groups of parameters from each other, affording a far more effective optimization than if using a deep network with nested mappings, and to use a good initialization from sliced inverse <phrase>regression</phrase> or spectral methods. Our experiments with image and <phrase>robot</phrase> applications show our approach to improve over direct <phrase>regression</phrase> and various existing approaches. We consider the problem of <phrase>low-dimensional</phrase> <phrase>regression</phrase> , where we want to estimate a mapping between inputs x R Dx and outputs y R Dy that are both continuous and <phrase>high</phrase>-dimensional (such as images, or control commands for a <phrase>robot</phrase>), but going through a <phrase>low-dimensional</phrase>, or latent, space z R Dz : y = g(F(x)), where z = F(x), y = g(z) and D z < D x , D y. In some situations, this can be preferable to a direct (full-dimensional) <phrase>regression</phrase> y = G(x), for example if, in addition to the <phrase>regression</phrase>, we are interested in obtaining a <phrase>low-dimensional</phrase> representation of x for its own <phrase>sake</phrase> (e.g. visualization or <phrase>feature extraction</phrase>). Even when the true mapping G is not <phrase>low-dimensional</phrase>, using a direct <phrase>regression</phrase> requires many parameters (D x D y in <phrase>linear regression</phrase>) and their estimation may be unreliable with small sample sizes. Using a <phrase>low-dimensional</phrase> composite mapping g F with fewer parameters can be seen as a form of regularization and <phrase>lead</phrase> to better generalization with <phrase>test</phrase> <phrase>data</phrase>. Finally, a common practical approach is to reduce the <phrase>dimension</phrase> of x independently of y, say with <phrase>principal component analysis</phrase> (<phrase>PCA</phrase>), and then solve the <phrase>regression</phrase>. However , the latent coordinates z obtained in this way do not necessarily preserve the <phrase>information</phrase> that is needed to predict y. This is the same reason why one would use <phrase>linear discriminant analysis</phrase> 
<phrase>Lithium Niobate</phrase> Microphotonic Modulators Acknowledgements First and foremost I would like to offer my entire <phrase>family</phrase> the gratitude that words alone cannot describe for the constant <phrase>love</phrase>, giving, sharing, support, and encouragement that has defined much of who I am today. They are all truly the backbone of my <phrase>life</phrase>. To my dad whom I owe so much for the years of <phrase>long</phrase> hours and hard work he has put in, to provide me everything he could, and yet ask for nothing more than my happiness in return. To my mom whom by overcoming so many personal challenges in <phrase>life</phrase> taught the persistence and <phrase>patience</phrase> I needed to achieve this goal. My <phrase>debt</phrase> to them is incalculable and I dedicate this work to them. I thank my wife <phrase>Imelda</phrase> for her <phrase>love</phrase> and encouragement over the years, and especially for tolerating my <phrase>long</phrase> work hours, and to my son <phrase>Alexander</phrase> who gave me the greatest incentive of all to obtain this goal. have given me so much confidence to finish. Especially Deanna who has always given me her constant support, while still putting up with me all those years. Also Uncle Vern and Aunt Jean, who I know are always there when I need them. Finally, to my Grandpa and Grandma Marshall and Grandma Cohen who defined the importance of <phrase>love</phrase> and <phrase>family</phrase> to me, and to my Grandad Cohen who expected the best out of me, I thank you. iii With deep appreciation I would like to thank my advisor Dr. Anthony Levi for challenging me to achieve my best. His direction and insights helped me become the <phrase>scientist</phrase> and <phrase>engineer</phrase> that I am today. His inspiring ideas, guidance, and encouragement gave me the confidence to achieve all that I have. Many of the skills used in this work were learned through collaborations with others. serve on my qualification and defense committees, and Dr. Elsa Garmire for the confidence she gave me during my early years at <phrase>USC</phrase>. I owe much of my sanity over the years at <phrase>USC</phrase> to Kim Reid. She is the best I've seen at what she does, and did much of the behind the scenes work that made my iv <phrase>research</phrase> possible. But more importantly she was always an uplifting <phrase>spirit</phrase> in the group, and was always there for me to <phrase>talk</phrase> with during difficult times. To my <phrase>fellow</phrase> <phrase>long</phrase>-term group members Dr. All of them showed me 
Applying System Dynamics to <phrase>Business</phrase>: an Expense <phrase>Management</phrase> Example Is System Dynamics for You? What Is System Dynamics? If you're new to system dynamics, it may be hard to see a pragmatic application to your <phrase>business</phrase> needs. This companion article to " Pipeline Inventory: The Missing Factor in Organizational Expense <phrase>Management</phrase> " [1] describes a seemingly simple problem and how a common approach may make things worse, not better. System dynamics offers an effective, easy <phrase>solution</phrase>. The associated <phrase>model</phrase> can be downloaded for experimentation. Perhaps you've read of system dynamics in books such as The Fifth Discipline [2] and concluded it doesn't apply to you. You may have seen how system dynamics can solve big problems for companies with deep pockets: forecasting demand for a new <phrase>car</phrase> <phrase>design</phrase> [3] or estimating the effects of policy decisions on a regional electric power system [4]. Something applied to such big problems probably won't affect you; you certainly can't afford it. This <phrase>short</phrase> article will demonstrate by example that system dynamics can <phrase>solve problems</phrase> of the <phrase>magnitude</phrase> seen by many middle managers, and such approaches don't need to have prohibitive costs. It provides a <phrase>short</phrase> introduction to system dynamics, describes a real <phrase>management</phrase> problem, presents a <phrase>model</phrase> that can be used to solve the problem, and closes with a summary of the lessons that can be drawn. The <phrase>model</phrase> [5] is also available to examine in closer detail. System dynamics is a method of <phrase>solving problems</phrase> by computer <phrase>simulation</phrase>. Like many <phrase>simulation</phrase> methods, it offers the promise of less expensive learningit's cheaper and faster to experiment with the effect of new policies on a computer <phrase>model</phrase> than on a real system with real people, equipment, and processes.
<phrase>Deep Learning</phrase> Made Easier by Linear Transformations in Perceptrons We transform the outputs of each hidden <phrase>neuron</phrase> in a <phrase>multi-layer</phrase> <phrase>perceptron</phrase> network to have zero output and zero slope on <phrase>average</phrase> , and use separate shortcut connections to <phrase>model</phrase> the linear dependencies instead. This transformation aims at separating the problems of learning the linear and nonlin-ear parts of the whole <phrase>input-output</phrase> mapping, which has many benefits. We study the theoretical properties of the transformation by noting that they make the <phrase>Fisher information</phrase> matrix closer to a <phrase>diagonal matrix</phrase>, and thus standard <phrase>gradient</phrase> closer to the natural <phrase>gradient</phrase>. We experimentally confirm the usefulness of the transformations by noting that they make <phrase>basic</phrase> <phrase>stochastic</phrase> <phrase>gradient</phrase> learning competitive with <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>learning algorithms</phrase> in speed, and that they seem also to help find solutions that generalize better. The experiments include both classification of small images and learning a <phrase>low-dimensional</phrase> representation for images by using a deep unsupervised <phrase>auto-encoder</phrase> network. The transformations were beneficial in all cases, with and without regularization and with networks from two to five <phrase>hidden layers</phrase>.
<phrase>Deep learning</phrase> via <phrase>Hessian</phrase>-<phrase>free</phrase> optimization We develop a 2 nd-<phrase>order</phrase> optimization method based on the " <phrase>Hessian</phrase>-<phrase>free</phrase> " approach, and apply it to training deep <phrase>auto-encoders</phrase>. Without using <phrase>pre-training</phrase>, we obtain <phrase>results</phrase> <phrase>superior</phrase> to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very <phrase>large datasets</phrase>, and isn't limited in applicability to <phrase>auto-encoders</phrase>, or any specific <phrase>model</phrase> class. We also discuss the issue of " pathological <phrase>curvature</phrase> " as a possible explanation for the difficulty of <phrase>deep-learning</phrase> and how 2 nd-<phrase>order</phrase> optimization, and our method in particular, effectively deals with it.
<phrase>Information</phrase> Theoretic Learning for <phrase>Pixel</phrase>-Based Visual Agents In this <phrase>paper</phrase> we promote the idea of using <phrase>pixel</phrase>-based models not only for <phrase>low level</phrase> vision, but also to extract <phrase>high</phrase> level symbolic representations. We use a <phrase>deep architecture</phrase> which has the distinctive <phrase>property</phrase> of relying on computational units that incorporate classic <phrase>computer vision</phrase> invariances and, especially, the <phrase>scale invariance</phrase>. The <phrase>learning algorithm</phrase> that is proposed, which is based on <phrase>information theory</phrase> principles , develops the parameters of the computational units and, at the same time, makes it possible to detect the optimal scale for each <phrase>pixel</phrase>. We give <phrase>experimental</phrase> evidence of the mechanism of <phrase>feature extraction</phrase> at the first level of the hierarchy, which is very much related to SIFT-like features. The comparison shows clearly that, whenever we can rely on the massive availability of <phrase>training data</phrase>, the proposed <phrase>model</phrase> leads to better performances with respect to SIFT.
Enhancing Access and Use of <phrase>Nasa</phrase> <phrase>Satellite</phrase> <phrase>Data</phrase> via Terrafly <phrase>NASA</phrase> <phrase>satellite</phrase> <phrase>data</phrase> form a rich resource that is largely untapped by the applications user <phrase>community</phrase>, in part because of the complexity of using, and the cost of learning how to use, such <phrase>data</phrase>. These users are generally not interested in the <phrase>data</phrase> per se, but rather in one or more specific measurements (e.g., surface <phrase>rain</phrase>) from the <phrase>data</phrase>, which can then be seamlessly infused in their own environment (e.g., <phrase>decision support systems</phrase>). The Goddard <phrase>Earth</phrase> Sciences <phrase>Data</phrase> and <phrase>Information</phrase> Services <phrase>Center</phrase> Distributed Active Archive <phrase>Center</phrase> (GES DISC DAAC) has collaborated with the <phrase>Florida International</phrase> University's <phrase>High</phrase> Performance <phrase>Database</phrase> <phrase>Research</phrase> <phrase>Center</phrase> (<phrase>FIU</phrase> HPDRC) on an initial <phrase>prototype</phrase> effort, which has demonstrated the feasibility of making <phrase>NASA</phrase> <phrase>satellite</phrase> <phrase>data</phrase> more easily and seamlessly accessible, as a <phrase>Web service</phrase>, from the FIU's TerraFly environment. The latter is a Web-enabled system designed to aid in the visualization of spatial and remotely sensed <phrase>data</phrase>, by "flying" over the Earth's surface, via standard <phrase>Web browsers</phrase>. The other part of the collaboration is the GES DISC Giovanni, an online visualization and analysis system, which relieves the users of much of the <phrase>data</phrase> preparation work and provides a tool for easily and quickly obtaining <phrase>information</phrase> from the <phrase>data</phrase>, without having to download and handle large amounts of <phrase>data</phrase>. The <phrase>prototype</phrase> effects the seamless access of <phrase>data</phrase> by <phrase>deep linking</phrase> (i.e., geo-located) TerraFly and Giovanni, to enable a dynamic <phrase>Web service</phrase>, providing on-demand, near-real-time, <phrase>satellite</phrase> <phrase>precipitation</phrase> <phrase>data</phrase>. The Giovanni system is evolving towards a <phrase>service-oriented architecture</phrase>, thus making available to TerraFly users <phrase>data</phrase> not just from <phrase>NASA</phrase> but also potentially from many other sources, as well as making <phrase>NASA</phrase> <phrase>satellite</phrase> <phrase>data</phrase> potentially more widely accessible.
Lecture 24 Constructive <phrase>Type Theory</phrase> 1 Computational Foundation By the <phrase>21st century</phrase>, constructive <phrase>type theory</phrase> emerged as a unifying framework for <phrase>logic</phrase>, <phrase>mathematics</phrase> , and <phrase>computer science</phrase>. One of the prime movers was L.E.J. Brouwer who understood <phrase>mathematics</phrase> in terms of mental constructions and offered a novel account of both the logical operators and the <phrase>basic</phrase> <phrase>mathematical</phrase> entities such as <phrase>natural numbers</phrase>, real numbers, points, and spaces. His deep and original insights enriched our thinking about what these objects are and how we know them. Computer scientists are drawn to his ideas because they are grounded in computation and also provide a particularly rich and natural basis for precisely specifying computational problems and proving that <phrase>algorithms</phrase> solve them. Logicians such as Heyting and <phrase>Kleene</phrase> learned Brouwer's ideas thoroughly and advanced their applications in <phrase>mathematics</phrase> and contributed to a broader understanding of them. By 1967 the <phrase>American</phrase> <phrase>mathematician</phrase> <phrase>Bishop</phrase> wrote his <phrase>book</phrase>, Foundations of Constructive Analysis that systematically developed <phrase>real analysis</phrase> based largely on Brouwer's ideas. In 1970 another famous <phrase>Dutch</phrase> <phrase>mathematician</phrase> N.G. deBruijn used some of Brouwer's ideas to ground a computer system called Automath designed to check all the details of proofs written in a <phrase>classical logic</phrase> inspired by Brouwer's account of the logical operators. By 1984 computer scientists at <phrase>Cornell</phrase> had built the Nuprl (new <phrase>pearl</phrase>) <phrase>proof assistant</phrase> based on a constructive <phrase>logic</phrase> and on experience using this <phrase>logic</phrase> to treat programs as proofs and to explore how to treat constructive proofs as programs. The <phrase>type theory</phrase> of Nuprl was influenced by similar work being done by Per Martin-Lof, initially without any connection to <phrase>programming</phrase>. The first step in understanding constructive <phrase>type theory</phrase> is to know its computation system. At the core it is an untyped <phrase>programming language</phrase> in the manner of <phrase>Lisp</phrase>. We now explain it using it using the notion of canonical term and non-canonical terms introduced by Martin-Lof. The canonical terms are those expressions such, tagged objects such as inl(17), <phrase>function</phrase> constants such as (x.x + 1), recursive <phrase>function</phrase> constants such as fix((f.(x.if x = 0 then 1 elsef (<phrase>x 1</phrase>) + 2))). Non-canonical expressions can be reduced by applying computation rules. For example, (17 x 289) reduces to 4913. The application of a <phrase>function</phrase> reduces by <phrase>computing</phrase> the value of the <phrase>function</phrase> on the given input, as in <phrase>ap</phrase>((x.x + 1); 17) reduces to 18 in one step of computation.
<phrase>Incremental Learning</phrase> by <phrase>Message Passing</phrase> in Hierarchical Temporal <phrase>Memory</phrase> Hierarchical temporal <phrase>memory</phrase> (HTM) is a biologically inspired framework that can be used to learn invariant representations of patterns in a wide <phrase>range</phrase> of applications. <phrase>Classical</phrase> HTM learning is mainly unsupervised, and once training is completed, the network structure is frozen, thus making further training (i.e., <phrase>incremental learning</phrase>) quite critical. In this letter, we develop a novel technique for HTM (incremental) <phrase>supervised learning</phrase> based on <phrase>gradient descent</phrase> error minimization. We prove that error <phrase>backpropagation</phrase> can be naturally and elegantly implemented through <phrase>native</phrase> HTM <phrase>message passing</phrase> based on <phrase>belief propagation</phrase>. Our <phrase>experimental</phrase> <phrase>results</phrase> demonstrate that a two-stage training approach composed of unsupervised pretraining and supervised refinement is very effective (both accurate and efficient). This is in line with recent findings on other <phrase>deep architectures</phrase>.
Multiple dynamic models for tracking the <phrase>left ventricle</phrase> of the <phrase>heart</phrase> from <phrase>ultrasound</phrase> <phrase>data</phrase> using <phrase>particle</phrase> filters and <phrase>deep learning</phrase> architectures The problem of automatic tracking and segmentation of the <phrase>left ventricle</phrase> (LV) of the <phrase>heart</phrase> from <phrase>ultrasound</phrase> images can be formulated with an <phrase>algorithm</phrase> that computes the expected segmentation value in the current time step given all previous and current observations using a filtering distribution. This filtering distribution depends on the observation and transition models, and since it is hard to compute the expected value using the whole <phrase>parameter space</phrase> of segmen-tations, one has to <phrase>resort</phrase> to <phrase>Monte Carlo</phrase> sampling techniques to compute the expected segmentation parameters. Generally, it is straightforward to compute <phrase>probability</phrase> values using the filtering distribution, but it is hard to sample from it, which indicates the need to use a proposal distribution to provide an easier sampling method. In <phrase>order</phrase> to be useful, this proposal distribution must be carefully designed to represent a reasonable approximation for the filtering distribution. In this <phrase>paper</phrase>, we introduce a new LV tracking and segmentation <phrase>algorithm</phrase> based on the method described above, where our contributions are focused on a new transition and observation models, and a new proposal distribution. Our tracking and segmentation <phrase>algorithm</phrase> achieves better overall <phrase>results</phrase> on a previously tested dataset used as a benchmark by the <phrase>current state</phrase>-of-the-<phrase>art</phrase> tracking <phrase>algorithms</phrase> of the <phrase>left ventricle</phrase> of the <phrase>heart</phrase> from <phrase>ultrasound</phrase> images.
Kernel <phrase>Engineering</phrase> for Fast and Easy <phrase>Design</phrase> of <phrase>Natural Language</phrase> Applications Feature <phrase>design</phrase> most difficult <phrase>aspect</phrase> in designing a learning system complex and difficult phase, e.g., structural <phrase>feature representation</phrase>: deep <phrase>knowledge</phrase> and intuitions are required <phrase>design</phrase> problems when the phenomenon is described by many features 3 <phrase>Motivation</phrase> (2)
Solving Deep <phrase>Memory</phrase> POMDPs with Recurrent Policy Gradients This <phrase>paper</phrase> presents Recurrent Policy Gradients, a <phrase>model</phrase>-<phrase>free</phrase> <phrase>reinforcement learning</phrase> (<phrase>RL</phrase>) method creating limited-<phrase>memory</phrase> sto-chastic policies for partially <phrase>observable</phrase> <phrase>Markov</phrase> decision problems (POMDPs) that require <phrase>long</phrase>-term memories of past observations. The approach involves approximating a policy <phrase>gradient</phrase> for a <phrase>Recurrent Neural Network</phrase> (RNN) by backpropagating return-weighted characteristic eligibili-ties through time. Using a " <phrase>Long</phrase> <phrase>Short-Term Memory</phrase> " <phrase>architecture</phrase>, we are able to outperform other <phrase>RL</phrase> methods on two important benchmark tasks. Furthermore, we show <phrase>promising results</phrase> on a complex <phrase>car</phrase> driving <phrase>simulation</phrase> task.
Robust LogitBoost and Adaptive Base Class (<phrase>ABC</phrase>) LogitBoost Logitboost is an influential boosting <phrase>algorithm</phrase> for classification. In this <phrase>paper</phrase>, we develop robust logitboost to provide an explicit formulation of <phrase>tree</phrase>-<phrase>split</phrase> criterion for building weak learners (<phrase>regression</phrase> <phrase>trees</phrase>) for logitboost. This formulation leads to a numerically stable implementation of logitboost. We then propose <phrase>abc</phrase>-logitboost for <phrase>multi-class</phrase> classification, by combining robust logitboost with the prior work of <phrase>abc</phrase>-boost. Previously, <phrase>abc</phrase>-boost was implemented as <phrase>abc</phrase>-mart using the mart <phrase>algorithm</phrase>. Our <phrase>extensive experiments</phrase> on <phrase>multi-class</phrase> classification compare four <phrase>algorithms</phrase>: mart, <phrase>abc</phrase>-mart, (robust) logitboost, and <phrase>abc</phrase>-logitboost, and demonstrate the superiority of <phrase>abc</phrase>-logitboost. Comparisons with other learning methods including <phrase>SVM</phrase> and <phrase>deep learning</phrase> are also available through prior publications.
Joint trajectory tracking and recognition based on bi-directional nonlinear learning Keywords: Visual tracking Trajectory <phrase>generative model</phrase> Autoencoder network Nonlinear <phrase>dimensionality reduction</phrase> <phrase>Particle filter</phrase> Improved <phrase>Hausdorff</phrase> distance a b s t r a c t Motion trajectory is one of the most important cues for tracking and behavior recognition and can be widely applied to numerous fields. However, it is a difficult problem to directly <phrase>model</phrase> the <phrase>spatio-temporal</phrase> variations of trajectories due to their <phrase>high</phrase> dimensionality and nonlinearity. In this <phrase>paper</phrase>, we propose a joint trajectory tracking and recognition <phrase>algorithm</phrase> by combining a <phrase>generative model</phrase> derived from a bi-directional <phrase>deep neural network</phrase> (called ''autoencoder ") into a <phrase>Bayesian</phrase> estimation framework. The ''autoencoder " network embeds <phrase>high</phrase>-dimensional trajectories into a two-dimensional plane based on a peculiar training rule and learns a trajectory <phrase>generative model</phrase> by its inverse mapping. A set of plausible trajectories can be generated by the trajectory <phrase>generative model</phrase>. In the tracking process, the samples from the plausible trajectory set are weighted by a mixed likelihood and are resampled to obtain the <phrase>target</phrase> <phrase>state</phrase> estimation at each time step in <phrase>spirit</phrase> of the <phrase>particle</phrase> filtering. The trajectory identity is inferred by evaluating the improved <phrase>Hausdorff</phrase> distance between the estimated trajectory up to now and the truncated reference trajectories. Moreover, the trajectory recognition <phrase>results</phrase> are also used to guide the trajectory tracking for the next time. The experiments on tracking and recognizing <phrase>handwritten digits</phrase> show that the <phrase>proposed approach</phrase> can achieve both robust tracking and exact recognition in background clutter and partial occlusion. Visual tracking is one of the fundamental problems in <phrase>computer vision</phrase>. Its applications <phrase>range</phrase> from <phrase>video</phrase> <phrase>surveillance</phrase>, humancom-puter interfaces and <phrase>augmented reality</phrase> to <phrase>digital</phrase> <phrase>video editing</phrase>. Most tracking approaches work in a recursive way, i.e., estimating the states at the current time t from the previous estimations up to the time (t 1) given the current observations. In a <phrase>Bayesian</phrase> framework, the tracking problem is commonly formulated as a recursive estimation of a time-evolving posterior distribution p(x t jy 1:t) of <phrase>state</phrase> x t given all the observations y 1:t. Some elegant tools including <phrase>particle</phrase> filters are introduced to solve sequential estimation problem, which can offer the robustness in clutter and occlusion [1]. However, visual tracking still remains a difficult vision problem due to complicated motion and appearance patterns varying with <phrase>target</phrase> types. Recently, researches <phrase>advocate</phrase> joint recognition and tracking where the motion and/or appearance are adaptive to <phrase>target</phrase> identities. Linear type-dependent motion models [2] and linear 
BotFinder: finding bots in <phrase>network traffic</phrase> without <phrase>deep packet inspection</phrase> Bots are the <phrase>root</phrase> cause of many <phrase>security</phrase> problems on the <phrase>Internet</phrase>, as they send spam, steal <phrase>information</phrase> from infected machines, and perform <phrase>distributed denial-of-service</phrase> attacks. Many approaches to bot detection have been proposed, but they either rely on end-host installations, or, if they operate on <phrase>network traffic</phrase>, require <phrase>deep packet inspection</phrase> for signature matching. In this <phrase>paper</phrase>, we present BotFinder, a novel system that detects infected hosts in a network using only <phrase>high</phrase>-level properties of the bot's <phrase>network traffic</phrase>. BotFinder does not rely on <phrase>content analysis</phrase>. Instead, it uses <phrase>machine learning</phrase> to identify the key features of command-and-control <phrase>communication</phrase>, based on observing traffic that bots produce in a controlled environment. Using these features, BotFinder creates models that can be deployed at network egress points to identify infected hosts. We trained our system on a number of representative bot families, and we evaluated BotFinder on <phrase>real-world</phrase> traffic datasets -- most notably, the NetFlow <phrase>information</phrase> of a large <phrase>ISP</phrase> that contains more than 25 billion flows. Our <phrase>results</phrase> show that BotFinder is able to detect bots in <phrase>network traffic</phrase> without the need of <phrase>deep packet inspection</phrase>, while still achieving <phrase>high</phrase> detection rates with very few false positives.
Measure-theoretic <phrase>evolutionary</phrase> annealing There is a deep connection between simulated an-nealing and <phrase>genetic algorithms</phrase> with <phrase>proportional</phrase> selection. <phrase>Evolutionary</phrase> annealing is a novel <phrase>evolutionary algorithm</phrase> that makes this connection explicit, resulting in an <phrase>evolutionary</phrase> optimization method that can be viewed either as <phrase>simulated annealing</phrase> with improved sampling or as a non-Markovian selection mechanism for <phrase>genetic algorithms</phrase> with selection over all prior populations. A martingale-based analysis shows that <phrase>evolutionary</phrase> annealing is asymptotically <phrase>convergent</phrase> and this analysis leads to heuristics for setting learning parameters to optimize the convergence rate. In this work and in parallel work <phrase>evolutionary</phrase> annealing is shown to converge faster than other <phrase>evolutionary algorithms</phrase> on several benchmark problems, establishing a promising foundation for future theoretical and <phrase>experimental</phrase> <phrase>research</phrase> into <phrase>algorithms</phrase> based on <phrase>evolutionary</phrase> annealing.
<phrase>Mobile</phrase> <phrase>Big Data</phrase> Analytics Using <phrase>Deep Learning</phrase> and <phrase>Apache</phrase> Spark The proliferation of <phrase>mobile</phrase> devices, such as <phrase>smart-phones</phrase> and <phrase>Internet</phrase> of Things (<phrase>IoT</phrase>) gadgets, <phrase>results</phrase> in the recent <phrase>mobile</phrase> <phrase>big data</phrase> (MBD) <phrase>era</phrase>. Collecting MBD is unprofitable unless suitable analytics and learning methods are utilized for extracting meaningful <phrase>information</phrase> and hidden patterns from <phrase>data</phrase>. This <phrase>article presents</phrase> an overview and brief tutorial of <phrase>deep learning</phrase> in MBD analytics and discusses a scalable learning framework over <phrase>Apache</phrase> Spark. Specifically, a distributed <phrase>deep learning</phrase> is executed as an iterative <phrase>MapReduce</phrase> <phrase>computing</phrase> on many Spark workers. Each Spark worker learns a partial deep <phrase>model</phrase> on a <phrase>partition</phrase> of the overall MBD, and a <phrase>master</phrase> deep <phrase>model</phrase> is then built by averaging the parameters of all partial models. This Spark-based framework speeds up the learning of deep models consisting of many <phrase>hidden layers</phrase> and millions of parameters. We use a context-aware activity recognition application with a <phrase>real-world</phrase> dataset containing millions of samples to validate our framework and assess its speedup effectiveness.
Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views This <phrase>paper</phrase> presents an <phrase>end-to-end</phrase> <phrase>convolutional neural network</phrase> (<phrase>CNN</phrase>) for 2D-3D exemplar detection. We demonstrate that the ability to adapt the features of <phrase>natural images</phrase> to better align with those of <phrase>CAD</phrase> rendered views is critical to the success of our technique. We show that the adaptation can be learned by <phrase>compositing</phrase> rendered views of textured object models on <phrase>natural images</phrase>. Our approach can be naturally incorporated into a <phrase>CNN</phrase> detection pipeline and extends the accuracy and speed benefits from <phrase>recent advances</phrase> in <phrase>deep learning</phrase> to 2D-3D exemplar detection. We applied our method to two tasks: instance detection, where we evaluated on the <phrase>IKEA</phrase> dataset [36], and object category detection, where we out-perform Aubry et al. [3] for " chair " detection on a <phrase>subset</phrase> of the Pascal <phrase>VOC</phrase> dataset.
Opportunity Analysis for Enterprise Collaboration between Networks of SMEs The competitive environment within the corporate have forced the <phrase>small and medium enterprises</phrase> (SMEs) to be more dynamic in adapting new <phrase>business</phrase> strategies. To achieve this objective, SMEs <phrase>resort</phrase> to enterprise collaboration to ooze out more <phrase>business</phrase> opportunities. Eventually, this culminates into a need for more useful enterprise collaboration to develop the integrated <phrase>products</phrase>. This <phrase>aspect</phrase> has accelerated the SMEs to adopt <phrase>high</phrase> level mechanism to move on from simple analysis to <phrase>deep learning</phrase> in <phrase>order</phrase> to realize more profit. This study is focused towards investigation of exploiting the <phrase>Big Data</phrase> capabilities to find out the potential opportunity lurking in the versatile <phrase>nature</phrase> of ever increasing organizational <phrase>data</phrase>. We have highlighted the issues of <phrase>interoperability</phrase> in the <phrase>paradigm</phrase> of <phrase>avalanche</phrase> of <phrase>data</phrase>, the analysis of potential opportunity as a result of enterprise collaboration leading to an added value. The analysis have been collected at surface level and then integrated into <phrase>deep learning</phrase>. The outcome of the study is <phrase>business</phrase> assets wherein the <phrase>ontological</phrase> modelling has been used at intermediate level.
Convolutional <phrase>Deep Belief</phrase> Networks for <phrase>Single</phrase>-<phrase>Cell</phrase>/<phrase>Object Tracking</phrase> in <phrase>Computational Biology</phrase> and <phrase>Computer Vision</phrase> In this <phrase>paper</phrase>, we propose <phrase>deep architecture</phrase> to dynamically learn the most <phrase>discriminative features</phrase> from <phrase>data</phrase> for both <phrase>single</phrase>-<phrase>cell</phrase> and <phrase>object tracking</phrase> in <phrase>computational biology</phrase> and <phrase>computer vision</phrase>. Firstly, the <phrase>discriminative features</phrase> are automatically learned via a convolutional <phrase>deep belief</phrase> network (CDBN). Secondly, we <phrase>design</phrase> a simple yet effective method to transfer features learned from CDBNs on the source tasks for generic purpose to the <phrase>object tracking</phrase> tasks using only limited amount of <phrase>training data</phrase>. Finally, to alleviate the tracker drifting problem caused by <phrase>model</phrase> updating, we jointly consider three different types of positive samples. <phrase>Extensive experiments</phrase> validate the robustness and effectiveness of the <phrase>proposed method</phrase>.
<phrase>Greedy Layer-Wise</phrase> Training of Deep Networks <phrase>Complexity theory</phrase> of circuits strongly suggests that <phrase>deep architectures</phrase> can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep <phrase>multi-layer</phrase> <phrase>neural networks</phrase> have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to <phrase>train</phrase> such deep networks, since <phrase>gradient</phrase>-based optimization starting from random initialization appears to often get stuck in poor solutions. Hin-ton et al. recently introduced a <phrase>greedy layer-wise</phrase> <phrase>unsupervised learning</phrase> <phrase>algorithm</phrase> for <phrase>Deep Belief</phrase> Networks (DBN), a <phrase>generative model</phrase> with many layers of hidden causal variables. In the context of the above <phrase>optimization problem</phrase>, we study this <phrase>algorithm</phrase> empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the <phrase>variable</phrase> to be predicted in a supervised task. Our experiments also confirm the <phrase>hypothesis</phrase> that the <phrase>greedy layer-wise</phrase> unsu-pervised training strategy mostly helps the optimization, by initializing weights in a <phrase>region</phrase> near a good local minimum, giving rise to internal distributed representations that are <phrase>high</phrase>-level abstractions of the input, bringing better generalization.
The <phrase>Evolution</phrase> of <phrase>Telecommunications</phrase> Policy-making: Comparative Analysis of <phrase>China</phrase> and <phrase>India</phrase> This <phrase>paper</phrase> is a comparative analysis of the <phrase>telecommunications</phrase> policy-making process in <phrase>China</phrase> and <phrase>India</phrase>. As two of the largest <phrase>emerging market</phrase> economies, the <phrase>telecommunications</phrase> systems in the two countries have attracted an enormous amount of <phrase>research</phrase> attention. However, much of the existing <phrase>literature</phrase> focuses on the specifics of <phrase>telecommunications</phrase> policies. Relatively less attention has been paid to the transformation of the <phrase>telecommunications</phrase> policy-making processthe institutions and formal procedures by which policies are framed in the two countries. In this <phrase>paper</phrase>, we adopt an institutionalist perspective to comparatively analyze the formal structures, rule-making procedures and interest groups involved in <phrase>telecommunications</phrase> policy-making in the two countries, in terms of their <phrase>evolution</phrase> over the last two decades. The policy-making system in the two countries began this <phrase>period</phrase> in a somewhat similar situation, being based on ministerial-bureaucratic <phrase>decision-making</phrase>. Since then, both countries have faced similar environmental conditions and organizational challenges as they sought to modernize and develop their <phrase>telecommunications</phrase> infrastructures, with <phrase>technological convergence</phrase> and <phrase>economic liberalization</phrase> as the background. Both countries faced similar problems of assimilating new interest groups and responding to international pressures. Yet the decision systems in the two countries have evolved in significantly different directions. In <phrase>China</phrase>, the <phrase>model</phrase> that evolved has been labeled as inter-ministerial competition, marked by deep-rooted <phrase>political</phrase> involvement, frequent bureaucratic bargaining, and weak legal institutions. China's <phrase>telecommunications</phrase> <phrase>decision-making</phrase> is much more affected by the macro level <phrase>political</phrase> rearrangement. In <phrase>India</phrase>, confronted by an increasingly litigious environment and a more fractious interest group <phrase>culture</phrase>, ministerial <phrase>decision-making</phrase> faced unprecedented challenges and responded by creating new regulatory institutions, and moving towards more transparent and participatory <phrase>decision-making</phrase>. Nevertheless, numerous challenges remain, including institutional capacity and excessive regulatory deference to <phrase>political</phrase> authority. Both models succeed to an extent in broadening the participation in <phrase>telecommunications</phrase> policy-making, but do so using different procedures and by involving different interest groups. We conclude PTC'11 Proceedings Page 2 of 37 by evaluating the experiences of the two countries and suggesting what they can learn from each other.
Efficient Complex Skill Acquisition through Representation Learning One of the fundamental goals of <phrase>artificial intelligence</phrase> is to understand and develop <phrase>intelligent agents</phrase> that simulate <phrase>human</phrase>-level <phrase>intelligence</phrase>. A lot of effort has been made to develop <phrase>intelligent agents</phrase> that simulate <phrase>human</phrase> learning of <phrase>math</phrase> and <phrase>science</phrase>, e.g., for use in <phrase>cognitive</phrase> tutors. However, constructing such a learning agent currently requires manual encoding of prior <phrase>domain knowledge</phrase> for each domain and even for each level of problem difficulty, which hurts the generality of the learning agent and is less cognitively plausible. Li et al. (2012) <phrase>recently proposed</phrase> an efficient <phrase>algorithm</phrase> that acquires representation <phrase>knowledge</phrase> in the form of " deep features, " and use the acquired representation to automatically generate feature predicates to assist future learning. The authors demonstrated the generality of the <phrase>proposed approach</phrase> across multiple domains. The <phrase>results</phrase> showed that by integrating this <phrase>algorithm</phrase> into a simulated <phrase>student</phrase>, SimStudent, the extended agent achieves efficient skill acquisition, while requiring less <phrase>prior knowledge</phrase> <phrase>engineering</phrase> effort, and being a more realistic <phrase>model</phrase> of the <phrase>state</phrase> of <phrase>prior knowledge</phrase> of novice <phrase>algebra</phrase> students. In this work, we further explore the generality of the <phrase>proposed approach</phrase> within one domain, but across multiple difficulty levels. The <phrase>results</phrase> indicates that the new, extended SimStudent is able to acquire skill <phrase>knowledge</phrase> of harder problems using only its learned problem representations, while the original SimStudent requires its <phrase>domain-specific</phrase> <phrase>prior knowledge</phrase> to be engineered explicitly to handle these harder problems. The extended SimStudent's performance is shown to match and even exceed the original as the complexity of problems increases.
Modeling Deep Strategic Reasoning by Humans in Competitive <phrase>Games</phrase> The prior <phrase>literature</phrase> on strategic reasoning by humans of the sort, what do you think that I think that you think, is that humans generally do not reason beyond a <phrase>single</phrase> level. They think about others' strategies but not about others' reasoning about their strategies. When repeatedly faced with another who reasons about their strategies, humans learn to think one level deeper but the learning is generally slow and incomplete. However, recent evidence suggests that if the <phrase>games</phrase> are made competitive and therefore representationally simpler, humans generally exhibited behavior that was more consistent with deeper levels of recursive reasoning. We seek to computationally <phrase>model</phrase> the judgment and be-havioral <phrase>data</phrase> that is consistent with deep recursive reasoning in competitive <phrase>games</phrase>. We present process models built from agent frameworks that not only simulate the observed <phrase>data</phrase> well but also exhibit <phrase>psychological</phrase> intuition. Our goal is to gain insights into the strategic thinking process, ultimately leading to agents which can emulate <phrase>human</phrase> <phrase>decision making</phrase> and effectively interact with humans in mixed settings.
The <phrase>Research</phrase> of <phrase>Mining</phrase> Association Rules Between Personality and Behavior of Learner Under <phrase>Web-Based</phrase> <phrase>Learning Environment</phrase> Discovering the relationship between behavior and personality of learner in the <phrase>web-based</phrase> <phrase>learning environment</phrase> is a key to guide learners in the learning process. This <phrase>paper</phrase> proposes a new concept called personality <phrase>mining</phrase> to find the " deep " personality through the observed <phrase>data</phrase> about the behavior. First, a learner <phrase>model</phrase> which includes personality <phrase>model</phrase> and behavior <phrase>model</phrase> is proposed. Second, we have designed and implemented an improved <phrase>algorithm</phrase>, which is based on Apriori <phrase>algorithm</phrase> widely used in market basket analysis, to identify the relationship. Third, we have discussed various issues like constructing the learner <phrase>model</phrase>, unifying the value domain of heterogeneous <phrase>model</phrase> attributes, and improving Apriori <phrase>algorithm</phrase> with decision domain. Experiment result indicated that this <phrase>algorithm</phrase> for <phrase>mining</phrase> association rules between behavior and personality is feasible and efficient. The <phrase>algorithm</phrase> has been used in a <phrase>web-based</phrase> <phrase>learning environment</phrase> developed at <phrase>Xi'an Jiaotong University</phrase>.
Moving Beyond Feature <phrase>Design</phrase>: <phrase>Deep Architectures</phrase> and Automatic <phrase>Feature Learning</phrase> in <phrase>Music</phrase> Informatics The <phrase>short</phrase> <phrase>history</phrase> of content-based <phrase>music</phrase> informatics <phrase>research</phrase> is dominated by <phrase>hand-crafted</phrase> feature <phrase>design</phrase>, and our <phrase>community</phrase> has grown admittedly complacent with a few de facto standards. Despite commendable progress in many areas, it is increasingly apparent that our efforts are yielding <phrase>diminishing returns</phrase>. This deceleration is largely due to the <phrase>tandem</phrase> of <phrase>heuristic</phrase> feature <phrase>design</phrase> and shallow processing architectures. We systematically discard hopefully irrelevant <phrase>information</phrase> while simultaneously calling upon <phrase>creativity</phrase>, intuition, or sheer luck to craft useful representations , gradually evolving complex, carefully tuned systems to address specific tasks. While other disciplines have seen the benefits of <phrase>deep learning</phrase>, it has only recently started to be explored in our field. By reviewing <phrase>deep architectures</phrase> and <phrase>feature learning</phrase>, we hope to raise awareness in our <phrase>community</phrase> about <phrase>alternative</phrase> approaches to solving <phrase>MIR</phrase> challenges, new and old alike.
Hand Pose Estimation through <phrase>Semi-Supervised</phrase> and <phrase>Weakly-Supervised</phrase> Learning We propose a method for hand pose estimation based on a deep regressor trained on two different kinds of input. Raw depth <phrase>data</phrase> is fused with an intermediate representation in the form of a segmentation of the hand into parts. This intermediate representation contains important topo-logical <phrase>information</phrase> and provides useful cues for reasoning about joint locations. The mapping from raw depth to segmentation maps is learned in a semi/<phrase>weakly-supervised</phrase> way from two different datasets: (i) a synthetic dataset created through a rendering pipeline including densely labeled <phrase>ground truth</phrase> (pixelwise segmentations); and (<phrase>ii</phrase>) a dataset with real images for which <phrase>ground truth</phrase> joint positions are available, but not dense segmentations. Loss for training on real images is generated from a patch-wise <phrase>restoration</phrase> process, which aligns tentative segmentation maps with a large <phrase>dictionary</phrase> of synthetic poses. The underlying premise is that the domain shift between synthetic and real <phrase>data</phrase> is smaller in the intermediate representation, where <phrase>labels</phrase> carry geometric and <phrase>topological</phrase> meaning, than in the raw input domain. Experiments on the <phrase>NYU</phrase> dataset [32] show that the proposed training method decreases error on joints over direct <phrase>regression</phrase> of joints from depth <phrase>data</phrase> by 15.7%.
<phrase>Book</phrase> Review: <phrase>Deep Learning</phrase> learning is a form of <phrase>machine learning</phrase> that enables <phrase>computers</phrase> to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers <phrase>knowledge</phrase> from experience, there is no need for a <phrase>human</phrase> computer operator formally to specify all of the <phrase>knowledge</phrase> needed by the computer. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a <phrase>graph</phrase> of these hierarchies would be many layers deep. This <phrase>book</phrase> introduces a broad <phrase>range</phrase> of topics in relation to <phrase>deep learning</phrase>. The text offers a <phrase>mathematical</phrase> and conceptual background covering relevant concepts in <phrase>linear algebra</phrase>, <phrase>probability theory</phrase> and <phrase>information theory</phrase>, numerical computation, and <phrase>machine learning</phrase>. It describes <phrase>deep learning</phrase> techniques which are used by practitioners in <phrase>industry</phrase>, including deep <phrase>Deep learning</phrase> can be used by <phrase>undergraduate</phrase> or graduate students who are planning careers in either <phrase>industry</phrase> or <phrase>research</phrase> , and by <phrase>software</phrase> engineers who want to begin using <phrase>deep learning</phrase> in their <phrase>products</phrase> or platforms. A <phrase>website</phrase> offers supplementary material for both readers and instructors. This <phrase>book</phrase> can be useful for a <phrase>variety</phrase> of readers, but the <phrase>author</phrase> wrote it with two main <phrase>target</phrase> audiences in mind. One of these <phrase>target</phrase> audiences is <phrase>university</phrase> students (<phrase>undergraduate</phrase> or graduate) who study <phrase>machine learning</phrase>, includ-This is an <phrase>Open Access</phrase> article distributed under the terms of the <phrase>Creative Commons</phrase> Attribution Non-Commercial License (http://creativecommons.org/licenses/by-<phrase>nc</phrase>/4.0/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Mind over Machine: What <phrase>Deep Blue</phrase> Taught Us about <phrase>Chess</phrase>, <phrase>Artificial Intelligence</phrase>, and the <phrase>Human</phrase> <phrase>Spirit</phrase> for Mom and Dad Mind over Machine: What <phrase>Deep Blue</phrase> Taught Us about <phrase>Chess</phrase>, <phrase>Artificial Intelligence</phrase>, and the <phrase>Human</phrase> <phrase>Spirit</phrase> Who taught me never to stop learning and never to give up. But also to ask for help. the world watched as IBM's <phrase>chess</phrase>-playing computer <phrase>Deep Blue</phrase> defeated <phrase>world chess champion</phrase> <phrase>Garry Kasparov</phrase> in a six-<phrase>game</phrase> match. The reverberations of that contest touched people, and <phrase>computers</phrase>, around the world. At the time, it was difficult to assess the historical significance of the moment, but ten years after the fact, we can take a fresh look at the meaning of the computer's victory. With hindsight, we can see how <phrase>Deep Blue</phrase> impacted the <phrase>chess</phrase> <phrase>community</phrase> and influenced the fields of <phrase>philosophy</phrase>, <phrase>artificial intelligence</phrase>, and <phrase>computer science</phrase> in the <phrase>long</phrase> run. For the <phrase>average</phrase> person, <phrase>Deep Blue</phrase> embodied many of our misgivings about <phrase>computers</phrase> becoming our new partners in the <phrase>information age</phrase>. For researchers in the field it was emblematic of the <phrase>growing pains</phrase> experienced by the evolving field of <phrase>AI</phrase> over the previous half century. In the end, what might have seemed like a definitive, <phrase>earth</phrase>-shattering event was really the next step in our ongoing journey toward understanding mind and machine. While <phrase>Deep Blue</phrase> was a <phrase>milestone</phrase>-the end of a <phrase>long</phrase> struggle to build a masterful <phrase>chess</phrase> machine-it was also a jumping off point for other lines of inquiry from new <phrase>supercomputing</phrase> projects to the further development of programs that <phrase>play</phrase> other <phrase>games</phrase>, such as Go. Ultimately, the lesson of Deep Blue's victory is that we will continue to accomplish technological feats we thought impossible just a few decades before. And as we reach each new goalpost, we will acclimate to our new position, recognize the next set of challenges before us, and push on toward the next <phrase>target</phrase>.
Erroneous Examples as a Source of Learning in <phrase>Mathematics</phrase> We analyze why and how errone o u s exam ples can be beneficially employed in learning mathe m a tics. The 'Why' addr esse s reasoning and attitu d <phrase>e</phrase> s that are rarely fostere d in today's mathe m a tic s lessons but which are useful for learning, certainly needed in <phrase>real life</phrase> and in all applications of mathe <phrase>ma</phrase> tic s as well as in maths itself. The 'How' addre sse s pedagogical strategies capitalizing on errors via erroneou s examples and their realizatio n in the <phrase>web-based</phrase> learning environme n t ActiveMath. An analysis of classes of error s infor ms the <phrase>knowledge</phrase> represe n ta tio n, the instructional strategies and the <phrase>feedback</phrase> of ActiveMath. 1. <phrase>MOTIVATION</phrase> New visions for <phrase>school</phrase> mathem atics have been articulated in a number of papers and reports, e.g., [12, 13]. They all suggest a shift from routine and factual <phrase>knowledge</phrase> only to more emphasis on developing competencies such as solving maths-related problem s, reasoning, and comm unicating mathem atically. Consequently, an <phrase>e</phrase>-Learning environmen t should offer opport unities stimulating deep reasoning rather than shallow learning only and encourage students to explore show <phrase>student</phrase> s that many mathem atical questions have more than one correct answer teach the importance of careful reasoning and understa n ding build confidence in all <phrase>student</phrase> s that they can learn mathem atics help <phrase>student</phrase> s verbalize their ideas. While exploiting, verbalizing, looking for alternatives and trying to truly unders tan d the maths mistakes occur naturally and are an essential part of learning. This has been widely neglected in traditional mathe <phrase>ma</phrase> tics <phrase>education</phrase>. The behaviourist view of learning that informs much of traditional schooling is not likely to invite <phrase>student</phrase> s and teachers to see errors in a positive <phrase>light</phrase>. Behaviourism assum es that learning is enhanced, when correct responses are rewarded (positive reinforcement) and incorrect ones are either punished or extinguished through lack of attention (withholding of positive reinforcement) [11]. Within this framework, paying explicit attention to (mathema tical) errors in class is even considered by many as dangerous since it could interfere with fixing the correct result in the student's mind. Hence, traditionally, schools mostly teach 'positive <phrase>knowledge</phrase>' only and 'negative <phrase>knowledge</phrase>' is mostly avoided. Approaches to use errors as learning
Learning the <phrase>Architecture</phrase> of Sum-Product Networks Using Clustering on Variables The sum-product network (SPN) is a <phrase>recently-proposed</phrase> deep <phrase>model</phrase> consisting of a network of sum and product nodes, and has been shown to be competitive with <phrase>state</phrase>-of-the-<phrase>art</phrase> deep models on certain difficult tasks such as image completion. Designing an SPN <phrase>network architecture</phrase> that is suitable for the task at hand is an open question. We propose an <phrase>algorithm</phrase> for learning the SPN <phrase>architecture</phrase> from <phrase>data</phrase>. The idea is to cluster variables (as opposed to <phrase>data</phrase> instances) in <phrase>order</phrase> to identify <phrase>variable</phrase> subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. <phrase>Experimental</phrase> evidence shows that learning the SPN <phrase>architecture</phrase> significantly improves its performance compared to using a previously-proposed static <phrase>architecture</phrase>.
<phrase>Human</phrase> Dimensions of Strategic <phrase>Leadership</phrase> <phrase>Leadership</phrase>: Quotations from the <phrase>Military</phrase> Tradition <phrase>Human</phrase> Dimensions of Strategic <phrase>Leadership</phrase> The <phrase>Army</phrase> has <phrase>long</phrase> recognized that regardless of current doctrine and <phrase>technology</phrase>, wars are fought by men and women operating under conditions of extreme stress and uncertainty. As such, it is critical that strategic leaders have a <phrase>deep understanding</phrase> derived from a study of <phrase>history</phrase> and behavioral sciencesof the complexity of <phrase>human</phrase> behavior under such conditions. Equally important, strategic leaders must get the best ideas and viewpoints from all stakeholders if they are to make <phrase>high</phrase>-quality decisions that achieve <phrase>high</phrase> acceptability among the diverse groups that make up our changing <phrase>Army</phrase> and <phrase>country</phrase>. Lastly, strategic leaders must thoroughly understand the <phrase>culture</phrase> of the organizations they <phrase>lead</phrase>, how to influence that <phrase>culture</phrase>, and how to build healthy, resilient, learning organizations that are equal to the challenges ahead. The successful strategic leader will be one who melds all aspects of the <phrase>human</phrase> <phrase>dimension</phrase> into the practice of the strategic <phrase>art</phrase>. <phrase>Leadership</phrase> is the <phrase>art</phrase> of getting someone else to do something you want done because he wants to do it. <phrase>Leadership</phrase> is a potent combination of strategy and character. But if you must be without one, be without the strategy. i PREFACE The <phrase>Human</phrase> Dimensions of Strategic <phrase>Leadership</phrase> is a recurring theme in the <phrase>curriculum</phrase> of the <phrase>U.S</phrase>. <phrase>Army War College</phrase>. This selected bibliography reflects books, documents, periodical articles, <phrase>multimedia</phrase>, and <phrase>web sites</phrase> relating to this topic. With a few exceptions for important documents, the materials cited in this bibliography are dated 2006 to the present. For older materials, please see previous versions of this bibliography at the USAWC <phrase>Library</phrase> home page. All items are available through the USAWC <phrase>Library</phrase>. For your convenience, at the end of the entries, we have added <phrase>library</phrase> call numbers, <phrase>Internet</phrase> addresses, or <phrase>database</phrase> links. <phrase>Web sites</phrase> were accessed during January 2010.
<phrase>Shark</phrase>: Fast <phrase>Data Analysis</phrase> Using <phrase>Coarse-grained</phrase> <phrase>Distributed Memory</phrase> <phrase>Shark</phrase>: Fast <phrase>Data Analysis</phrase> Using <phrase>Coarse-grained</phrase> <phrase>Distributed Memory</phrase> Permission to make <phrase>digital</phrase> or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or <phrase>commercial advantage and that copies bear</phrase> this notice and the full citation on the first page. To copy otherwise, to republish, to <phrase>post on servers</phrase> or to redistribute to lists, <phrase>requires prior specific permission</phrase>. Abstract <phrase>Shark</phrase> is a <phrase>research</phrase> <phrase>data analysis</phrase> system built on a novel <phrase>coarse-grained</phrase> <phrase>distributed shared-memory</phrase> abstraction. <phrase>Shark</phrase> marries query processing with deep <phrase>data analysis</phrase> , providing a unified system for easy <phrase>data</phrase> manipulation using <phrase>SQL</phrase> and pushing sophisticated analysis closer to <phrase>data</phrase>. It scales to thousands of nodes in a <phrase>fault-tolerant</phrase> manner. <phrase>Shark</phrase> can answer queries 40X faster than <phrase>Apache</phrase> Hive and run <phrase>machine learning</phrase> programs 25X faster than <phrase>MapReduce</phrase> programs in <phrase>Apache Hadoop</phrase> on <phrase>large datasets</phrase>. This is a complete overview of the development of <phrase>Shark</phrase>, including <phrase>design</phrase> decisions, performance details, and comparison with existing <phrase>data warehousing</phrase> solutions. It demonstrates some of Shark's distinguishing features including its in-<phrase>memory</phrase> columnar caching and its unified <phrase>machine learning</phrase> interface.
Coping With Implicit Arguments And Events Coreference In this <phrase>paper</phrase> we present ongoing work for the creation of a linguistically-based system for event coreference. We assume that this task requires <phrase>deep understanding</phrase> of text and that statistically-based methods, both supervised and unsupervised are inadequate. The reason for this choice is due to the fact that event coreference can only take place whenever argumenthood is properly computed. It is a fact that in many cases, arguments of predicates are implicit and thus linguistically unexpressed. This prevents training to produce sensible <phrase>results</phrase>. We also assume that spatiotemporal locations need to be taken into account and this is also very often left implicit. We used GETARUNS system to develop the coreference system which works on the basis of the discourse <phrase>model</phrase> and the automatically annotated markables. We present <phrase>data</phrase> from the analysis, both on unexpressed implicit arguments and the description of the coreference <phrase>algorithm</phrase>. 1 Introduction <phrase>NLP</phrase> processing is more and more oriented towards <phrase>semantic</phrase> processing which in turn requires <phrase>deep understanding</phrase> of texts. We assume that this is only possible if unexpressed implicit <phrase>linguistic</phrase> elements and semantically deficient items are taken into consideration (Delmonte 2009a, 2009b). One of the first problem in the analysis of any text is <phrase>accounting</phrase> for implicit or linguistically unexpressed <phrase>information</phrase>. This kind of <phrase>information</phrase> is not available in dependecy-based current annotated corpora or is only partially available as in <phrase>Penn</phrase> Treebank but it cannot possibly be learnt. The problem of null and <phrase>pronominal</phrase> elements is <phrase>paramount</phrase> in the recovery of Predicate-Argument Structures which constitutes the fundamental element onto which <phrase>propositional</phrase> <phrase>semantics</phrase> is made to work. However, applying <phrase>machine learning</phrase> techniques on available treebanks is of no help. <phrase>State</phrase> of the <phrase>art</phrase> systems are using more and more dependency representations which have lately shown great resiliency, robustness, <phrase>scalability</phrase> and great adaptability for <phrase>semantic</phrase> enrichment and processing. However, by far the majority of systems available off the shelf don't support a fully semantically consistent representation and lack Null Elements or Antecedents for <phrase>pronominal</phrase> ones. If we limit ourselves to Null Elements, and to PennTreebank (hence PT), we may note that Marcus ('94) referred explicitly to Predicate-Argument Structures (hence <phrase>PASs</phrase>) and to the need to address this level of annotation. He mentions explicitly that " we intend to automatically extract a <phrase>bank</phrase> of <phrase>PASs</phrase> intended at the very least for parser evaluation from the resulting annotated corpus " and further on " the notation should 
Collocational <phrase>Knowledge</phrase> versus <phrase>General</phrase> <phrase>Linguistic</phrase> <phrase>Knowledge</phrase> among <phrase>Iranian</phrase> Efl Learners This study has a twofold purpose. The first and foremost is to see whether there exists any correlation between the collocational <phrase>knowledge</phrase> and <phrase>general</phrase> <phrase>linguistic</phrase> <phrase>knowledge</phrase> of EFL learners. The second is to reveal which type(s) of <phrase>collocation</phrase> is or are more difficult for EFL learners. To this end, 35 subjects, screened by a proficiency <phrase>test</phrase>, were given a 90-item <phrase>multiple-choice</phrase> <phrase>test</phrase> including lexical collocations (<phrase>noun</phrase>+<phrase>noun</phrase>, <phrase>noun</phrase>+<phrase>verb</phrase>, <phrase>verb</phrase>+<phrase>noun</phrase>, and <phrase>adjective</phrase>+<phrase>noun</phrase>), and grammatical collocations (<phrase>noun</phrase>+<phrase>preposition</phrase> and <phrase>preposition</phrase>+<phrase>noun</phrase>). A <phrase>native</phrase> <phrase>speaker</phrase> checked the final version of the <phrase>data</phrase> and necessary corrections were made. The <phrase>results</phrase> showed that a) there was no significant correlation between <phrase>general</phrase> <phrase>linguistic</phrase> <phrase>knowledge</phrase> and collocational <phrase>knowledge</phrase> of EFL learners, and b) the grammatical collocations were more difficult than the lexical collocations for learners and from among all subcategories, <phrase>noun</phrase>+<phrase>preposition</phrase> was the most difficult and <phrase>noun</phrase>+<phrase>verb</phrase> was the easiest. Introduction One of the most important aspects of learning a <phrase>language</phrase> is learning the <phrase>vocabulary</phrase> of that <phrase>language</phrase> and its appropriate use. Since traditional techniques of learning vocabularythe learning of individual words or memorizing bilingual <phrase>vocabulary</phrase> listsappeared to be no longer tenable, researchers suggested ways for learning multiword phrases, chunks as well as association between lexical items. Anderson and Nagy (1991), for instance, accentuate the importance of deep meanings including collocational properties in words. Students need to know which words go with which other words, how words go together normally, and how we can manipulate these
The New Face of <phrase>Design</phrase> for Manufacturability 232 <phrase>Deep-submicron</phrase> Beol Yield Challenges <phrase>Infrastructure</phrase> for Successful Beol Yield Ramp, Transfer to <phrase>Manufacturing</phrase>, and <phrase>Dfm</phrase> Characterization at 65 Nm and Below The challenges presented by <phrase>deep-submicron</phrase> interconnect back-end-of-line (BEOL) integration continue to grow in number, complexity, and required resolution at 90 nm and 65 nm. These challenges are causing <phrase>industry</phrase>-wide delays in <phrase>technology</phrase> deployment as well as low and often unstable yields. The historically observed improvements in time to successful yield ramp and final <phrase>manufacturing</phrase> yield as the <phrase>industry</phrase> deploys new <phrase>technology</phrase> nodes disappeared at 90 nm. Such improvements have been significant factors in fueling the <phrase>semiconductor</phrase> industry's growth. In this article, we describe an <phrase>infrastructure</phrase> developed to specifically address BEOL <phrase>deep-submicron</phrase> yield-learning needs. These include the need to reduce the overall time to <phrase>results</phrase> and to provide <phrase>information</phrase> that manufacturers can successfully use in process and yield debug, and in <phrase>higher-level</phrase> <phrase>design</phrase> models. This <phrase>infrastructure</phrase> establishes a needed foundation for <phrase>deep-submicron</phrase> <phrase>technology</phrase> nodes where <phrase>design</phrase> and <phrase>manufacturing</phrase> share yield entitlement. By building on this foundation , manufacturers can accelerate yield issue detection and correction, and realize yield-aware <phrase>design</phrase> flows. The <phrase>International Technology Roadmap for Semiconductors</phrase> 1 provides an excellent summary of the <phrase>confounding</phrase> combination of increased process complexity , reduced yield-learning cycles, and reduced defect visibility that confronts the <phrase>industry</phrase>. Accordingly, a yield ramp <phrase>infrastructure</phrase> must be available that can provide the following characteristics: parts-per-billion sensitivity to key yield-limiting <phrase>topologies</phrase> (not just to purely random defects); identification of nonvisual defects, such as defects in <phrase>high</phrase>-<phrase>aspect-ratio</phrase> features and interfacial defects; direct identification of defect location and layer; ability to generate vastly more <phrase>data</phrase> than has been possible with traditional <phrase>technology</phrase> characterization vehicles, to provide sufficient coverage of ever-expanding <phrase>design</phrase> rule sets, interactions with resolution enhancement technologies (RETs), and so on; Editor's Note: Optimized <phrase>test</phrase> structures are necessary to measure and analyze the causes for systematic yield loss. This article introduces a novel <phrase>test</phrase> structure for BEOLan <phrase>infrastructure</phrase> IP for process monitoring. It also describes a method for characterizing and measuring yield ramp issues and solutions for improving <phrase>silicon</phrase> debug and <phrase>DFM</phrase>.
<phrase>Tuple</phrase> Refinement Method Based on Relationship Keyword Extension Entity <phrase>relation extraction</phrase> is mainly focused on researching extraction approaches and improving precision of the extraction <phrase>results</phrase>. Although many efforts have been made on this field, there still exist some problems. In <phrase>order</phrase> to improve the performance of extracting entity relation, we propose a <phrase>tuple</phrase> refinement method based on relationship keyword extension. Firstly, we utilize the diversity of relationships to extend relationship keywords, and then, use the redundancy of network <phrase>information</phrase> to extract the second entity based on the principle of proximity and the predefined entity type. Under open web environment, we take four relationships in the experiments and adopt <phrase>bootstrapping</phrase> <phrase>algorithm</phrase> to acquire the initial <phrase>tuple</phrase> set. Three <phrase>tuple</phrase> refinement methods are compared: refinement method with threshold set, refinement method with relation extension and refinement method without relation extension. The <phrase>average</phrase> F-scores of the <phrase>experimental</phrase> <phrase>results</phrase> show the <phrase>proposed method</phrase> can effectively improve the performance of entity <phrase>relation extraction</phrase>. 1 Introduction With the rapid development of the <phrase>Internet</phrase>, extracting structured <phrase>data</phrase> from the vast amounts <phrase>unstructured data</phrase> called as <phrase>information extraction</phrase> (IE) has been the present <phrase>research</phrase> hotspot. The <phrase>major</phrase> <phrase>research</phrase> directions of IE [10, 11] are <phrase>named entity</phrase> recognition, anaphora resolution and <phrase>relation extraction</phrase> (RE). Since RE can be used in many applications such as <phrase>semantic web</phrase>, automatic answering and social relationship network, it is very important in the IE. Generally speaking, RE has two methods based on <phrase>knowledge engineering</phrase> and <phrase>machine learning</phrase>. <phrase>Knowledge engineering</phrase> methods require a lot of <phrase>professional</phrase> <phrase>knowledge</phrase>, which waste a lot of time and manpower. Meanwhile they lack the <phrase>area</phrase> portability. <phrase>Machine learning</phrase> methods need to apply a lot of deep <phrase>natural language processing</phrase> (<phrase>NLP</phrase>) technologies which inevitably produce a lot of noise. <phrase>SVM</phrase> methods use <phrase>syntactic</phrase> analysis and <phrase>semantic</phrase> analysis [5, 6, 7] to construct feature vectors,
Review of "<phrase>Eclipse</phrase>: Building Commercial Quality <phrase>Plug-ins</phrase> by Eric Clayberg and Dan Rubel". <phrase>Addison Wesley</phrase>, 2004, 0-321-22847-2 (<phrase>paperback</phrase>) <phrase>Eclipse</phrase> is an <phrase>open-source</phrase> <phrase>IBM</phrase>-sponsored extensible <phrase>IDE</phrase> built using <phrase>Java</phrase>, JFace, and SWT (Standard <phrase>Widget Toolkit</phrase>). It offers a <phrase>comprehensive</phrase> <phrase>API</phrase> for developers to write <phrase>plug-ins</phrase>. This massive tome, one of the books in a series written by experts , reviewed by <phrase>Eclipse</phrase> insiders, and published by <phrase>Addison Wesley</phrase> <phrase>Professional</phrase>, just provides an in-depth description of the process involved in building commercial quality <phrase>plug-ins</phrase> for <phrase>Eclipse</phrase> and IBM's <phrase>WebSphere</phrase> Studio <phrase>Workbench</phrase>. As far as I know, this is the first authoritative, complete <phrase>book</phrase> out there on this topic. Perhaps it is still the best one now. It will fill the gap between common <phrase>Eclipse</phrase> development and <phrase>Eclipse</phrase> customization. The authors start the <phrase>book</phrase> with an introduction to the <phrase>Eclipse</phrase> Environment, then gives an outline about how to build a simple plug-in. Each chapter follows a similar <phrase>order</phrase>, and contains an overview at the very start, a summary at the end, and an in-between detailed description with a wealth of diagrams, screen shots, tips, <phrase>API</phrase> listings, and code snippets. It covers the most important topics in an easy to follow style. Along the way, you will get a <phrase>comprehensive</phrase> understanding of how to write <phrase>Eclipse</phrase> <phrase>plug-ins</phrase> using every facility that <phrase>Eclipse</phrase> provides. Granted, as a reviewer, I must admit that there still leave some flaws in it. For example, some of codes and screenshots are a little out of date. Even a minor of places seem have errors. In a few cases, the authors use deprecated <phrase>Java</phrase> <phrase>API</phrase>. Some codes could not run as written. Although the aforemen-tioned flaws indeed exist in the <phrase>book</phrase>, it is still a must read for <phrase>Eclipse</phrase> <phrase>plug-ins</phrase>' developers. The <phrase>book</phrase> is nicely up-to-date, and keeping up with recently released <phrase>Eclipse</phrase> version 3.0. It is definitely not an on-line documentation or a <phrase>cookbook</phrase> like many others. It is written for <phrase>Eclipse</phrase> developers, engineers, commercial customers, students , and researchers. To different people, it means different things. Anyone who wants to gain a <phrase>deep understanding</phrase> of <phrase>Eclipse</phrase> and <phrase>Eclipse</phrase> <phrase>plug-ins</phrase> will find it indispensable and save you a lot of effort. When you read the <phrase>book</phrase>, you may have a good grasp of the entire process of creating <phrase>Eclipse</phrase> <phrase>plug-ins</phrase>. However, if you are not an experienced <phrase>Java</phrase> developer, you should go elsewhere for some other <phrase>Java</phrase> books to renew your <phrase>knowledge</phrase>. After all, when you are building <phrase>plug-ins</phrase> on <phrase>Eclipse</phrase>, there is a lot to learn. 
Prcis of Learning and the <phrase>Language</phrase> of Thought O ne of the outstanding mysteries of <phrase>language acquisition</phrase> is how children learn the meaning of <phrase>function</phrase> words. <phrase>Function</phrase> words like " and, " " the, " and " when " are <phrase>linguistic</phrase> symbols that do not refer to any <phrase>observable</phrase> object, <phrase>property</phrase>, or <phrase>action</phrase> in the world. Instead, they convey <phrase>semantic</phrase> operations and reflect a deep <phrase>cognitive process</phrase> that allows <phrase>language</phrase> users to expressand potentially thinkcomplex thoughts. For instance: Most politicians who are successful are born near a <phrase>town</phrase> with either one <phrase>factory</phrase> or two <phrase>malls</phrase>. Such utterances are possible because <phrase>language</phrase> users know the meaning of words such as " most, " " who, " " a, " " either, " and " or " that combine with other words to express abstract logical relationships. Because <phrase>function</phrase> words are non-referential, they outside the domain of most contemporary word learning models (<phrase>e</phrase>. typically only <phrase>track</phrase> co-occurrences between words and objects. As such, there are no working theories for how children might acquire the <phrase>semantics</phrase> of <phrase>function</phrase> words. This is unfortunate because the compositionality, abstraction, and structure behind these word meanings makes them some of the most compelling and unique features of <phrase>human</phrase> <phrase>communication</phrase>. Indeed, functions words motivate an entirely different perspective on <phrase>language acquisition</phrase>, where the key <phrase>aspect</phrase> of learning word meanings is inferring the unobserved <phrase>semantic</phrase> operations the word conveys, rather than the observed features of the world it maps to. This <phrase>thesis</phrase>, Learning and the <phrase>language</phrase> of thought, combines computational, developmental, and <phrase>experimental</phrase> approaches in <phrase>order</phrase> to understand the type of inductive learning that supports acquisition of rich conceptual structures like <phrase>function word</phrase> meanings. The <phrase>starting point</phrase> for this approach is <phrase>Fodor</phrase> (1975)'s <phrase>language</phrase> of thought (LOT) <phrase>hypothesis</phrase>. This theory posits that a structured, compositional system supports <phrase>mental representation</phrase> (see also <phrase>Boole</phrase>, 1854; <phrase>Frege</phrase>, 1892). A LOT is much like a computer <phrase>programming language</phrase> for thinking: complex thoughts are represented by <phrase>syntactically</phrase> combining elements from a smaller set of primitive representations. A thought such as " All men are violinists " might be represented by a logical expression such as " x.man(x) <phrase>violinist</phrase>(x) " , where " " , " man " , " <phrase>violinist</phrase> " , and " " are primitive <phrase>cognitive</phrase> symbols. Such a representation system has been argued to explain core properties of <phrase>cognitive processes</phrase> , including most notably their compositionality, systematicity, and <phrase>productivity</phrase> (<phrase>Fodor</phrase> & Pylyshyn, 1988). This <phrase>thesis</phrase> extends 
Simple2Complex: <phrase>Global Optimization</phrase> by <phrase>Gradient Descent</phrase> A method named simple2complex for modeling and training <phrase>deep neural networks</phrase> is proposed. Simple2complex <phrase>train</phrase> <phrase>deep neural networks</phrase> by smoothly adding more and more layers to the shallow networks, as the learning procedure going on, the network is just like growing. Compared with learning by end2end, simple2complex is with less possibility trapping into local minimal, namely, owning ability for <phrase>global optimization</phrase>. Cifar10 is used for verifying the superiority of simple2complex.
Partitioned <phrase>neural networks</phrase> A new method is given for speeding up learning in a <phrase>deep neural network</phrase> with many <phrase>hidden layers</phrase>, by partially partitioning the network rather than fully interconnecting the layers. Empirical <phrase>results</phrase> are shown both for learning a simple <phrase>Boolean function</phrase> on a standard backprop network, and for learning two different, complex, <phrase>real-world</phrase> vision tasks on a more sophisticated <phrase>convolutional network</phrase>. In all cases, the performance of the proposed system was better than traditional systems. The partially-partitioned network outperformed both the fully-partitioned and fully-unpartitioned networks.
Parallel <phrase>Gradient Descent</phrase> for Multilayer <phrase>Feedforward Neural</phrase> Networks We present a parallel approach to classification using <phrase>neural networks</phrase> as the <phrase>hypothesis</phrase> class. <phrase>Neural networks</phrase> can have millions of parameters and learning the optimum value of all parameters from huge datasets in a serial implementation can be a very time consuming task. In this work, we have implemented parallel <phrase>gradient descent</phrase> to <phrase>train</phrase> multilayer <phrase>feedforward neural</phrase> networks. Specifically, we analyze two kinds of par-allelization techniques: (a) <phrase>parallel processing</phrase> of multiple <phrase>training examples</phrase> across several threads and (b) parallelizing matrix operations for a <phrase>single</phrase> training example. We have implemented a serial minibatch <phrase>gradient descent</phrase> <phrase>algorithm</phrase>, its parallel multithreaded version (using Pthread <phrase>library</phrase> in <phrase>C++</phrase>), a BLAS parallelized version and a <phrase>CUDA</phrase> implementation on a <phrase>GPU</phrase>. All implementations have been compared and analyzed for the speedup obtained across various network architectures and increasing problem sizes. We have performed our <phrase>tests</phrase> on the benchmark dataset: MNIST, and finally also compared our implementations with the corresponding implementations in the <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>deep learning</phrase> <phrase>library</phrase>: <phrase>Theano</phrase>.
Analysis of <phrase>E</phrase>. Coli Promoters Using <phrase>Support Vector Machine</phrase> is an authentic record of my own work carried out under the supervision of Mr. R S Salaria. The <phrase>matter</phrase> presented in this <phrase>thesis</phrase> has not been submitted for the award of any other <phrase>degree</phrase> of this or any other <phrase>university</phrase>. This is to certify that the above statement made by the candidate is correct and true to the best of my <phrase>knowledge</phrase>. iii Acknowledgement I wish to <phrase>express my deep</phrase> gratitude to my guide Mr. <phrase>Department</phrase>, for the <phrase>motivation</phrase> and inspiration that triggered me for the <phrase>thesis</phrase> work. I would also like to thank all the staff members and my co-students who were always there at the need of the hour and provided me with all the help and facilities, which I required for the completion of the <phrase>thesis</phrase>. <phrase>Calcutta</phrase>, who spared his valuable time to help me at every moment I needed his expertise. (8043111) iv Abstract <phrase>Support Vector Machines</phrase> (<phrase>SVM</phrase>) is a <phrase>family</phrase> of <phrase>learning algorithms</phrase> which is currently considered as one of the most efficient methods in many <phrase>real-world</phrase> applications. The theory behind <phrase>SVM</phrase> was developed in the sixties and seventies by Vapnik and Chervonenkis, but the first practical implementation of <phrase>SVM</phrase> was only published in the early nineties. Since then the method gained more and more attention among the <phrase>machine learning</phrase> <phrase>community</phrase>, thanks to its ability to outperform most other <phrase>learning algorithms</phrase> (including <phrase>neural networks</phrase>) in many applications. As a result it has been successfully applied to all sorts of classification issues, ranging from handwritten <phrase>character recognition</phrase> to <phrase>speaker</phrase> identification or <phrase>face detection</phrase> in images. <phrase>SVM</phrase> have been applied to many biological issues, including <phrase>gene expression</phrase> <phrase>data analysis</phrase> or <phrase>protein</phrase> classification. Some claim that biological <phrase>data mining</phrase> applications are one of the most promising uses of <phrase>SVM</phrase>, particularly for the <phrase>high</phrase> dimensionality of the <phrase>data</phrase>. As a result, <phrase>research</phrase> about <phrase>SVM</phrase> and <phrase>computational biology</phrase> is the object of much effort today, mainly due to researchers coming from the <phrase>machine learning</phrase> <phrase>community</phrase>. One can expect <phrase>SVM</phrase> to become a standard tool for bioinformaticians in the near future. Recently, the prediction of promoters has attracted many researchers' attention. Unfortunately most previous prediction <phrase>algorithms</phrase> did not provide <phrase>high</phrase> enough sensitivity and specificity. This is where <phrase>SVM</phrase> clearly stands out of the crowd. Our main idea is to use computer power to calculate all possible patterns which are the possible features of promoters (training of <phrase>SVM</phrase>). Once this is 
A Fast and Accurate Unconstrained Face Detector We propose a method to address challenges in unconstrained <phrase>face detection</phrase>, such as arbitrary pose variations and occlusions. First, a new image feature called Normalized <phrase>Pixel</phrase> Difference (<phrase>NPD</phrase>) is proposed. <phrase>NPD</phrase> feature is computed as the difference to sum ratio between two <phrase>pixel</phrase> values, inspired by the Weber Fraction in <phrase>experimental psychology</phrase>. The new feature is scale invariant, bounded, and is able to reconstruct the original image. Second, we propose a deep quadratic <phrase>tree</phrase> to learn the optimal <phrase>subset</phrase> of <phrase>NPD</phrase> features and their combinations, so that complex face <phrase>manifolds</phrase> can be partitioned by the learned rules. This way, only a <phrase>single</phrase> soft-<phrase>cascade</phrase> classifier is needed to handle unconstrained <phrase>face detection</phrase>. Furthermore, we show that the <phrase>NPD</phrase> features can be efficiently obtained from a look up table, and the detection template can be easily scaled, making the proposed face detector very fast. <phrase>Experimental</phrase> <phrase>results</phrase> on three <phrase>public</phrase> face datasets (FDDB, GENKI, and <phrase>CMU</phrase>-<phrase>MIT</phrase>) show that the <phrase>proposed method</phrase> achieves <phrase>state</phrase>-of-the-<phrase>art</phrase> performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes.
<phrase>Concept Maps</phrase> as a Device for Learning <phrase>Database</phrase> Concepts At the <phrase>university</phrase> level, a <phrase>student</phrase> should not just learn to memorise all the complex <phrase>information</phrase> he is taught. Instead, he should aim to learn, what the <phrase>information</phrase> means and what the connections between different pieces of <phrase>information</phrase> are, i.e., aim in <phrase>deep learning</phrase>. In <phrase>order</phrase> to do that, the <phrase>student</phrase> has to be able to organise the <phrase>information</phrase> into structures that are natural and understandable for him. One method for structuring and representing <phrase>information</phrase>, so that <phrase>deep learning</phrase> is enhanced, is <phrase>concept maps</phrase>. <phrase>Concept maps</phrase> have been used in many <phrase>university</phrase> disciplines, e.g., <phrase>biology</phrase>, <phrase>chemistry</phrase>, <phrase>engineering</phrase>, <phrase>history</phrase>, <phrase>medicine</phrase>, <phrase>psychology</phrase>, <phrase>social sciences</phrase> and <phrase>computer science</phrase>. They have even been applied in teaching <phrase>databases</phrase>. In <phrase>order</phrase> to enlighten the suitability of <phrase>concept maps</phrase> for teaching <phrase>database</phrase> concepts, we describe in this <phrase>paper</phrase> how <phrase>concept maps</phrase> were used as a learning and assessment tool in a course of distributed <phrase>databases</phrase>. The learning <phrase>results</phrase> of the course were good, and on the whole, <phrase>concept maps</phrase> showed out to be a promising tool for learning of <phrase>database</phrase> concepts. 1. INTRODUCTION A <phrase>university</phrase> <phrase>student</phrase> has to learn a lot of complex <phrase>information</phrase> [10]. The <phrase>student</phrase> at this level should not, however, just learn to memorise the <phrase>information</phrase>, but instead aim in <phrase>deep learning</phrase>, i.e., aim to learn to understand what the <phrase>information</phrase> means and how the different pieces of <phrase>information</phrase> are connected to each other. Teachers, of course, try to teach the <phrase>information</phrase> in a structured form that is natural for them, but this structure is not necessarily logical for every <phrase>student</phrase>. Therefore, each <phrase>student</phrase> has to be able to organise the <phrase>information</phrase> into structures that are natural and understandable from his own point of view. There are many methods that can be used for structuring and representing <phrase>information</phrase> so that <phrase>deep learning</phrase> can be enhanced. One such method for describing concepts and their connections are <phrase>concept maps</phrase> [17, 18]. <phrase>Concept maps</phrase> are known to develop students' logical thinking and <phrase>study skills</phrase> [1], and they have been used in schools and <phrase>universities</phrase> and as well as in <phrase>business</phrase> and <phrase>industry</phrase>. At <phrase>university</phrase> level, <phrase>concept maps</phrase> have been used successfully for teaching and assessment in many <phrase>science</phrase>. <phrase>Concept maps</phrase> have even been applied in teaching <phrase>databases</phrase>, e.g., in teaching <phrase>database design</phrase> and conceptual level modeling of <phrase>data</phrase> [13, 14, 22]. It would, however, be interesting to study more thoroughly how <phrase>concept maps</phrase> could help students in 
<phrase>Mathematics</phrase> based on <phrase>incremental learning</phrase> - Excluded middle and inductive inference Learning theoretic aspects of <phrase>mathematics</phrase> and <phrase>logic</phrase> have been studied by many authors. They study how <phrase>mathematical</phrase> and logical objects are algorithmically \learned" (inferred) from nite <phrase>data</phrase>. Although they study <phrase>mathematical</phrase> objects, the objective of the studies is learning. In this <phrase>paper</phrase>, a <phrase>mathematics</phrase> of which foundation itself is learning theoretic will be introduced. It is called Limit-<phrase>Computable</phrase> <phrase>Mathematics</phrase>. I t w as originally introduced as a means for \Proof <phrase>Animation</phrase>," which is expected to make i n teractive formal proof development easier. Although the original objective w as not learning theoretic at all, learning theory is indispensable for our <phrase>research</phrase>. It suggests that <phrase>logic</phrase> and learning theory are related in a still unknown but deep new way. <phrase>Mathematical</phrase> or logical concepts seem to be one of the main <phrase>research</phrase> targets of learning theory and its applications. Shapiro 25] investigated how <phrase>axioms</phrase> systems are inductively inferred by ideas of learning theory. W <phrase>e</phrase> m a y s a y t h a t Shapiro studied how logical systems (<phrase>axiom</phrase> systems) are learned. Stephan and Ventsov 27] investigated how <phrase>algebraic</phrase> structures are learned and have given some interesting learning theoretic characterizations of fundamental <phrase>algebraic</phrase> notions. We m a y s a y that they investigated learnability of the <phrase>mathematical</phrase> concepts. Contrary to them, we a r <phrase>e</phrase> n o w d <phrase>e</phrase> v eloping a <phrase>mathematics</phrase> whose <phrase>semantics</phrase> and reasoning system are innuenced by ideas from <phrase>computational learning theory</phrase>. Let us compare these two lines of <phrase>research</phrase>.
<phrase>Artificial Neural Network</phrase> Modeling of Process and Product Indices in Deep Bed Drying of Rough <phrase>Rice</phrase> This study aimed to <phrase>model</phrase> the performance indices of deep bed drying of rough <phrase>rice</phrase> using <phrase>artificial neural networks</phrase> (ANNs), compare the ANN approach to the multivariate <phrase>regression</phrase> method, and determine the sensitivity of the ANN <phrase>model</phrase> to the input variables. The effects of <phrase>air temperature</phrase>, air <phrase>velocity</phrase>, and air <phrase>relative humidity</phrase> on drying <phrase>kinetics</phrase>, product output rate (POR), <phrase>evaporation</phrase> rate (<phrase>ER</phrase>), and percentage of kernel cracking (KC) were investigated. To predict the dependent parameters, 3 well-known networks, namely the <phrase>multilayer perceptron</phrase>, generalized <phrase>feed forward</phrase> (GFF), and modular <phrase>neural network</phrase>, were examined. The GFF networks with the LevenbergMarquardt <phrase>learning algorithm</phrase>, hyperbolic <phrase>tangent</phrase> activation <phrase>function</phrase>, and 4-15-1, 3-4-4-1, 3-7-1, and 3-11-1 <phrase>topologies</phrase> provided <phrase>superior</phrase> <phrase>results</phrase>, respectively, for predicting moisture content, POR, <phrase>ER</phrase>, and CK. The values of all of the drying indices predicted by the ANN were closer to the <phrase>experimental</phrase> <phrase>data</phrase> than linear and <phrase>logarithmic</phrase> <phrase>regression</phrase> models. The output variables were significantly affected by the dependent variables. However, <phrase>air temperature</phrase> and air <phrase>relative humidity</phrase> showed the maximum and the minimum influence on the network outputs, respectively.
<phrase>Deep Learning</phrase> for <phrase>Sentiment Analysis</phrase> - Invited <phrase>Talk</phrase> Richard Socher is the <phrase>CEO</phrase> and founder of Meta-Mind, a startup that seeks to improve <phrase>artificial intelligence</phrase> and make it widely accessible. He obtained his <phrase>PhD</phrase> from <phrase>Stanford</phrase> working on <phrase>deep learning</phrase> with Chris Manning and Andrew Ng and won the best <phrase>Stanford</phrase> CS <phrase>PhD</phrase> <phrase>thesis</phrase> award. He is interested in developing new <phrase>AI</phrase> models that perform well across multiple different tasks in <phrase>natural language processing</phrase> and <phrase>computer vision</phrase>.
The complexity of <phrase>hybrid</phrase> logics over restricted frame classes There are times when all the world's asleep, the questions run too deep for such a simple man. Won't you please, please tell me what we've learned, I know it sounds absurd, but please tell me who I am. Zukunfts-und Vergangenheitsoperatoren drcken aus: " irgendwann in der Zukunft " <phrase>oder</phrase> " immer in der Vergangenheit " Until-und Since-Operatoren drcken zum Beispiel aus: " irgendwann in der Zukunft , und von jetzt an bis dahin " Nominale geben Punkten in einer Struktur feste Namen Sprungoperatoren gestatten Sprnge zu benannten Punkten Binder binden Namen dynamisch an Punkte die globale Modalitt drckt aus: " irgendwann " In Abhngigkeit von der jeweiligen Anwendung ist es angebracht, die Klas-se der Strukturen auf diejenigen einzuschrnken, die die Anforderungen die-ser Anwendung modellieren. Relevante Strukturenklassen f <phrase>ur</phrase> temporale <phrase>oder</phrase> epistemische Anwendungen sind beispielsweise transitive Strukturen, transitive Bume, lineare Ordnungen, die natrlichen Zahlen, Strukturen mitAqui-valenzrelationen ( AR-Strukturen) <phrase>oder</phrase> vollstndige Strukturen. Die Entscheidbarkeit und die Komplexitt von Entscheidungsproblemen f <phrase>ur</phrase> Logiken sind relevant f <phrase>ur</phrase> das automatisierte L <phrase>osen</phrase> dieser Probleme. Wir un-tersuchen systematisch das Erfllbarkeitsproblem und das <phrase>Model-Checking</phrase>-Problem f <phrase>ur</phrase> alle relevanten hybriden Sprachen, die beliebige Kombinationen der oben aufgefhrten Operatoren enthalten, bezglich der genannten Struktu-renklassen. Das schliet ein, eine Hierarchie <phrase>aller</phrase> dieser Sprachen aufzustellen, Ergebnisse aus der Literatur dort einzuordnen und eigene Resultate beizusteu-ern. Im Einzelnen beweisen wir die folgenden Hauptergebnisse unter Zuhilfe-nahme einer breiten Palette von Techniken.
Learning optimal nonlinearities for iterative thresholding <phrase>algorithms</phrase> Iterative shrinkage/thresholding <phrase>algorithm</phrase> (ISTA) is a well-studied method for finding sparse solutions to ill-posed inverse problems. In this letter, we present a <phrase>data</phrase>-driven scheme for learning optimal thresholding functions for ISTA. The proposed scheme is obtained by relating iterations of ISTA to layers of a simple <phrase>deep neural network</phrase> (DNN) and developing a corresponding error <phrase>backpropagation</phrase> <phrase>algorithm</phrase> that allows to fine-tune the thresholding functions. Simulations on sparse statistical signals illustrate potential gains in estimation quality due to the proposed <phrase>data</phrase> adaptive ISTA.
Parallel tempering is efficient for learning <phrase>restricted Boltzmann machines</phrase> A new interest towards <phrase>restricted Boltzmann machines</phrase> (RBMs) has risen due to their usefulness in greedy learning of <phrase>deep neural networks</phrase>. While contrastive divergence learning has been considered an efficient way to learn an RBM, it has a drawback due to a biased approximation in the learning <phrase>gradient</phrase>. We propose to use an advanced <phrase>Monte Carlo method</phrase> called parallel tempering instead, and show experimentally that it works efficiently.
A <phrase>Multi-Agent</phrase> <phrase>Architecture</phrase> Implementation of <phrase>Learning by Teaching</phrase> Systems Our group has been designing and implementing <phrase>learning environments</phrase> that promote <phrase>deep understanding</phrase> and transfer in complex domains. We have adopted the <phrase>learning by teaching</phrase> <phrase>paradigm</phrase>, and developed computer-based agents that students teach, and learn from this experience. The success of teachable agents has <phrase>led</phrase> us to develop a <phrase>multi-agent</phrase> <phrase>architecture</phrase> that will be used to develop extended instructional systems based on gaming environments. 1. Introduction Advances in computer <phrase>technology</phrase> have facilitated the development of sophisticated computer-based <phrase>Intelligent Tutoring</phrase> Systems (ITS) [1]. The ITS <phrase>paradigm</phrase> is problem-based, with the system selecting problems for users to solve, and providing them <phrase>feedback</phrase> on their solutions. The tutoring <phrase>paradigm</phrase> suffers from its emphasis on localized <phrase>feedback</phrase>, and its inability to help students practice <phrase>higher-order</phrase> <phrase>cognitive</phrase> skills in complex domains, where <phrase>problem solving</phrase> requires active <phrase>decision-making</phrase> by learners in terms of setting learning goals and applying strategies for achieving these goals. Studies of expertise [2] have shown that <phrase>knowledge</phrase> needs to be connected and organized around important concepts, and these structures should support transfer to other contexts. Other studies have established that improved learning happens when the students take control of their own learning, develop metacognitive strategies to assess what they know, and acquire more <phrase>knowledge</phrase> when needed. Thus the learning process must help students build new <phrase>knowledge</phrase> from existing <phrase>knowledge</phrase> (constructivist learning), guide students to discover learning opportunities while <phrase>problem solving</phrase> (exploratory learning), and help them to define learning goals and monitor their progress in achieving them (metacognitive strategies). To advance the <phrase>state</phrase> of the <phrase>art</phrase> in the <phrase>design</phrase> and implementation of <phrase>learning environments</phrase>, we have adopted the <phrase>learning by teaching</phrase> <phrase>paradigm</phrase>. Teaching
Effects of Self-explanation in an <phrase>Open- Ended</phrase> Domain Self-explanation is used in several <phrase>intelligent tutoring</phrase> systems in the domains of <phrase>Mathematics</phrase> and <phrase>Physics</phrase> to facilitate <phrase>deep learning</phrase>. Since these domains are well structured, instructional material to self-explain can be clearly defined. We are interested in investigating whether self-explanation can be used in an <phrase>open-ended</phrase> domain. For this purpose, we enhanced <phrase>KERMIT</phrase>, an <phrase>intelligent tutoring</phrase> system that teaches conceptual <phrase>database design</phrase>. The resulting system, <phrase>KERMIT</phrase>-SE, supports self-explanation by engaging students in tutorial dialogues when their solutions are erroneous. An evaluation study was conducted in July 2002, to investigate whether students will learn better when self-explaining. The <phrase>results</phrase> indicate that self-explanation leads to improved performance in both conceptual and <phrase>procedural knowledge</phrase>.
A One-<phrase>Bit</phrase>-Matching <phrase>Learning Algorithm</phrase> for <phrase>Independent Component Analysis</phrase> <phrase>Independent component analysis</phrase> (ICA) has many practical applications in the fields of signal and <phrase>image processing</phrase> and several ICA <phrase>learning algorithms</phrase> have been constructed via the selection of <phrase>model</phrase> <phrase>probability density</phrase> functions. However, there is still a lack of deep <phrase>mathematical</phrase> theory to validate these ICA <phrase>algorithms</phrase>, especially for the <phrase>general</phrase> case that super-and sub-Gaussian sources coexist. In this <phrase>paper</phrase>, according to the one-<phrase>bit</phrase>-matching principle and by turning the de-<phrase>mixing</phrase> matrix into an <phrase>orthogonal matrix</phrase> via certain normalization, we propose a one-<phrase>bit</phrase>-matching ICA <phrase>learning algorithm</phrase> on the Stiefel <phrase>manifold</phrase>. It is shown by the simulated and audio experiments that our proposed <phrase>learning algorithm</phrase> works efficiently on the ICA problem with both super-and sub-Gaussian sources and outperforms the extended Infomax and Fast-ICA <phrase>algorithms</phrase>.
Multi-Entity Polarity Analysis in Financial Documents The amount of <phrase>information</phrase> available in the <phrase>Internet</phrase> does not allow performing manual <phrase>content analysis</phrase> to identify <phrase>information</phrase> of interest. Thus automated analyses are used to identify <phrase>information</phrase> of interest, and one increasingly important approach is the polarity analysis. Polarity analysis is the classification of a text document in positive, negative, and neutral, according to a certain topic. This classification of <phrase>information</phrase> is particularly useful in the <phrase>finance</phrase> domain, where <phrase>news</phrase> about a <phrase>company</phrase> can affect the performance of its stocks. Although most of the methods in financial domain consider that the whole document is associated with a particular entity, this is not always the case. In fact, it is common that authors cite several entities in a <phrase>single</phrase> document and these entities are cited with different polarity. Accordingly, the objective of this <phrase>paper</phrase> was to study strategies for polarity detection in financial documents with multiple entities. Specifically, we studied methods based on learning of multiple models, one for each observed entity, using <phrase>SVM</phrase> classifiers. We evaluated models based on the <phrase>partition</phrase> of documents into fragments according to the entities they cite. We used several heuristics to segment documents based on shallow and deep <phrase>natural language processing</phrase> (<phrase>NLP</phrase>). We found that entity-specific models created by partitioning the document collection into segments outperformed the strategy based on the use of entire documents. We also observed that more complex segmentation using anaphora resolution was not able to outperform a <phrase>low-cost</phrase> approach, based on simple string matching.
<phrase>Large-scale</phrase> <phrase>Artificial Neural Network</phrase>: <phrase>MapReduce</phrase>-based <phrase>Deep Learning</phrase> Faced with continuously increasing scale of <phrase>data</phrase>, original back-propagation <phrase>neural network</phrase> based <phrase>machine learning</phrase> <phrase>algorithm</phrase> presents two non-trivial challenges: huge amount of <phrase>data</phrase> makes it difficult to maintain both efficiency and accuracy; redundant <phrase>data</phrase> aggravates the system workload. This project is mainly focused on the <phrase>solution</phrase> to the issues above, combining <phrase>deep learning</phrase> <phrase>algorithm</phrase> with <phrase>cloud computing</phrase> platform to deal with <phrase>large-scale</phrase> <phrase>data</phrase>. A <phrase>MapReduce</phrase>-based handwriting character recognizer will be designed in this project to verify the efficiency improvement this mechanism will achieve on training and practical <phrase>large-scale</phrase> <phrase>data</phrase>. Careful discussion and experiment will be developed to illustrate how <phrase>deep learning</phrase> <phrase>algorithm</phrase> works to <phrase>train</phrase> <phrase>handwritten digits</phrase> <phrase>data</phrase>, how <phrase>MapReduce</phrase> is implemented on <phrase>deep learning</phrase> <phrase>neural network</phrase>, and why this combination accelerates computation. Besides performance, the <phrase>scalability</phrase> and robustness will be mentioned in this <phrase>report</phrase> as well. Our system comes with two demonstration <phrase>software</phrase> that visually illustrates our <phrase>handwritten digit</phrase> recognition/encoding application.
Enhancing Learning through Self-Explanation Self-explanation is an effective teaching/learning strategy that has been used in several <phrase>intelligent tutoring</phrase> systems in the domains of <phrase>Mathematics</phrase> and <phrase>Physics</phrase> to facilitate <phrase>deep learning</phrase>. Since all these domains are well structured, the instructional material to self-explain can be clearly defined. We are interested in investigating whether self-explanation can be used in an <phrase>open-ended</phrase> domain. For this purpose, we enhanced <phrase>KERMIT</phrase>, an <phrase>intelligent tutoring</phrase> system that teaches conceptual <phrase>database design</phrase>. The resulting system, <phrase>KERMIT</phrase>-SE, supports self-explanation by engaging students in tutorial dialogues when their solutions are erroneous. We plan to conduct an evaluation in July 2002, to <phrase>test</phrase> the <phrase>hypothesis</phrase> that students will learn better with <phrase>KERMIT</phrase>-SE than without self-explanation.
<phrase>Model</phrase>-driven Optimization of Multihop <phrase>Wireless Networks</phrase> <phrase>Model</phrase>-driven Optimization of Multihop <phrase>Wireless Networks</phrase> <phrase>Model</phrase>-driven Optimization of Multihop <phrase>Wireless Networks</phrase> Acknowledgments First and foremost, my deep appreciation goes to Prof. work is under their supervision and support. From them, I have learned how excellent networking <phrase>research</phrase> is done-rigorous, focused, motivated, open to new ideas, and involved into <phrase>research</phrase>. Prof. Gustavo de Veciana for serving on my committee. I owe a special gratitude to my <phrase>family</phrase>. My parents and sister always give me selfless care, support, and <phrase>love</phrase>. Abstract Interference is fundamental to <phrase>wireless networks</phrase>. It is hard to achieve good performance when <phrase>design</phrase> routing metrics or <phrase>algorithms</phrase> without taking it into account. We study interference in <phrase>wireless networks</phrase> through empirical experiments and simulations. We find out that current routing protocols face difficulties in effectively managing it, which can <phrase>lead</phrase> to severe problems. For instance, a simple network of two links with one flow is vulnerable to severe performance degradation if interference is not properly accounted for. Motivated by these observations , we develop a simple and effective <phrase>model</phrase> to capture effects of interference in a <phrase>wireless network</phrase>. Different from the existing interference models, our <phrase>model</phrase> captures <phrase>IEEE 802.11</phrase> DCF under both homogeneous and heterogeneous traffic and link characteristics, and is simple enough to be directly used as a <phrase>basic</phrase> <phrase>building block</phrase> for <phrase>wireless</phrase> performance optimization. Based on this <phrase>model</phrase>, we develop optimization <phrase>algorithms</phrase> for several objectives, such as network throughput and fairness. Given traffic demands as input, these <phrase>algorithms</phrase> compute rates at which individual vi flows must send to achieve these objectives. We implement these <phrase>algorithms</phrase> in Qualnet simulations and 19-node <phrase>testbed</phrase>. Our experiment and <phrase>simulation</phrase> <phrase>results</phrase> show that our methods can systematically account for and control interference to achieve good performance. More specifically, when optimizing fairness, our methods can achieve almost perfect fairness; when optimizing network throughput, they can <phrase>lead</phrase> to 100-200% improvement for <phrase>UDP</phrase> traffic and 10-50% for <phrase>TCP</phrase> traffic.
<phrase>Action</phrase> Learning for Continuous Improvement and Enhanced <phrase>Innovation</phrase> in <phrase>Construction</phrase> <phrase>Action</phrase> Learning for Continuous Improvement and Enhanced <phrase>Innovation</phrase> in <phrase>Construction</phrase> <phrase>Action</phrase> Learning (AL) for Continuous Improvement and Enhanced <phrase>Innovation</phrase> in <phrase>Construction</phrase> is a <phrase>research</phrase> programme of <phrase>Academic</phrase> Enterprise, <phrase>University</phrase> of Salford. The aim of the project was to find out how effective <phrase>action</phrase> learning might be in developing more innovative working methods in small and medium-sized building businesses in the <phrase>construction</phrase> <phrase>industry</phrase>. Four <phrase>construction</phrase> based AL SETs have been set up in different parts of the <phrase>UK</phrase>; a SET is a group of people, normally between 6-8 people with complementary problems, who <phrase>band</phrase> together as " partners in adversity " to discuss how they might learn from their own actions as they attempt to resolve key and complex issues. Each of our <phrase>Construction</phrase> AL SETs is in a different phase of its development, but the progress made by them all is exciting. Members of such SETs first seem to gain a systemic confidence of the new <phrase>construction</phrase> tasks in front of them and then become more innovative and creative in their resulting every day actions. They begin to explore new possibilities for change and seem to be able drive improvement from the " ashes " of their own site problems, issues, and failures. The innovations they come up with are typically fairly <phrase>small scale</phrase> to begin with, but soon grow, and often combine with others to have a fairly large impact on the <phrase>productivity</phrase> of their organisation. Our evidence suggests AL does seem to be able to create the sort of deep-seated <phrase>cultural</phrase> change needed by the <phrase>construction</phrase>. The AL process is initially heavily dependent upon the support of skilled SET Advisors who facilitate necessary changes in attitude and behaviour. This early facilitation is essential if it is to work successfully, however, SETs soon learn to look after themselves, growing from strength to strength in confidence and then develop creative responses to their own real site problems and issues. The AL process is initially extremely intensive in facilitator support costs, however, <phrase>video conferencing</phrase> has been shown to help make this form of <phrase>construction</phrase>. 5553. <phrase>Academic</phrase> Enterprise is the third <phrase>major</phrase> arm of a <phrase>University</phrase> activity that is endeavouring to reach out and support <phrase>industry</phrase> and <phrase>commerce</phrase> to enable it to flourish.
Towards Interactive Tools for Constructing Articulate Simulations towards Interactive Tools for Constructing Articulate Simulations Having learners construct computer-based simulations is becoming increasingly important as an approach to induce learning. Qualitative simulations incorporate a rich <phrase>vocabulary</phrase> for articulating insights about systems and their behaviours, including notions such as structural constituents, identifying qualitatively distinct behaviours, and making explicit the causal dependencies that govern a system's behaviour. When building a qualitative <phrase>model</phrase> all these details have to be made explicit (they have to be represented in the <phrase>model</phrase>). As a result, building a qualitative <phrase>model</phrase> induces a <phrase>deep understanding</phrase> of the system and its behaviour. In our <phrase>research</phrase> we want to exploit this phenomenon, i.e. have learners learn by building qualitative simulations. However, building qualitative models is generally seen as a difficult task, and no easy-to-use tools exist to support a learner in performing such a task. In this <phrase>paper</phrase>, we present MOBUM, a <phrase>domain independent</phrase> <phrase>model</phrase> building environment aimed at supporting learners in building qualitative simulations. MOBUM is a fully implemented <phrase>prototype</phrase> in <phrase>JAVA</phrase>. The main goal of this <phrase>prototype</phrase> is to generate and clarify ideas with respect to how such a <phrase>model</phrase> building tool should be constructed.
<phrase>Artificial</phrase> <phrase>Blue</phrase> and Deep <phrase>Intelligence</phrase> This <phrase>paper</phrase> explores the implications of the <phrase>Kasparov</phrase> v <phrase>Deep Blue</phrase> <phrase>Chess</phrase> Match for work on legal <phrase>knowledge based systems</phrase>. It suggests that it is necessary toaccept the specificity of computer andhuman legal cultures and not simply adopt legal theories which appear most easily palatable for <phrase>AI</phrase> workers. A second suggestion is that <phrase>AI</phrase> work is better conceived in terms of a <phrase>cybernetic</phrase> partnership between <phrase>computers</phrase> and <phrase>human</phrase> programmers, <phrase>domain experts</phrase> and users. A third suggestion, that of tendency for humans to make mistakes suggests a proper role for <phrase>AI</phrase> precisely in those areas where <phrase>human</phrase> weakness is a significant factor. The final observation is that there may already be in <phrase>train</phrase> a process of changing the rules or at least the conditions of <phrase>law</phrase> to suit <phrase>AI</phrase> in <phrase>law</phrase>. This superficially attractive idea raises the deepest <phrase>political</phrase> issues. " Should we try to make <phrase>artificial intelligence</phrase> by duplicating how humans do it or instead try to exploit the particular strength of machines? " <phrase>David</phrase> <phrase>Stork</phrase> (1 997) " (I)n most <phrase>knowledge based systems</phrase> ... it is not intended to imitate the <phrase>problem-solving</phrase> approach and reasoning methods of the expert. Most LKBSs are based on <phrase>artificial</phrase> models only meant to produce reliable <phrase>results</phrase>. " (De Vries et al 1991.) The victory of <phrase>Deep Blue</phrase> against <phrase>Kasparov</phrase> could, paradoxically, give comfort to those who believe in <phrase>AI</phrase> as well as those who believe in the uniqueness of <phrase>human intelligence</phrase>. For the <phrase>AI</phrase> <phrase>community</phrase>, there is tangible proof of success. 1 <phrase>Deep Blue</phrase> came close to fulfilling the promise of the original <phrase>human</phrase> v computer <phrase>chess</phrase> <phrase>game</phrase> with HAL in 2001. For believers in the 'uniqueness of <phrase>human intelligence</phrase>', there is satisfaction that <phrase>Deep Blue</phrase> can do well in <phrase>chess</phrase> only because it is ultimately a quantifiable <phrase>game</phrase>, and that even the <phrase>Japanese</phrase> Go is as yet not amenable to the number crunching power of current machines. Thus it is suggested on the <phrase>Kasparov</phrase> vs <phrase>Deep Blue</phrase> <phrase>website</phrase> (1997a). <phrase>Deep Blue</phrase>, as it stands today, is not a 'learning system'. It is therefore not capable of utilizing <phrase>artificial</phrase> intelli gence to either learn from its opponent or 'think' about the current position of the <phrase>chessboard</phrase>. <phrase>Stork</phrase> (1997) provides an apparently comfortable compromise reasoning that computer <phrase>intelligence</phrase> and <phrase>human intelligence</phrase> have their own strengths and domains. According to him, current <phrase>computers</phrase> have their strengths in quantification and <phrase>research</phrase> whereas humans do well those 
<phrase>Deep learning</phrase> code fragments for code clone detection Code clone detection is an important problem for <phrase>software maintenance</phrase> and <phrase>evolution</phrase>. Many approaches consider either structure or identifiers, but none of the existing detection techniques <phrase>model</phrase> both sources of <phrase>information</phrase>. These techniques also depend on generic, handcrafted features to represent code fragments. We introduce learning-based detection techniques where everything for representing terms and fragments in <phrase>source code</phrase> is mined from the repository. Our code analysis supports a framework, which relies on <phrase>deep learning</phrase>, for automatically linking patterns mined at the lexical level with patterns mined at the <phrase>syntactic</phrase> level. We evaluated our novel learning-<phrase>based approach</phrase> for code clone detection with respect to feasibility from the point of view of <phrase>software</phrase> maintainers. We sampled and manually evaluated 398 file- and 480 method-level pairs across eight <phrase>real-world</phrase> <phrase>Java</phrase> systems; 93% of the file- and method-level samples were evaluated to be true positives. Among the true positives, we found pairs mapping to all four clone types. We compared our approach to a traditional structure-oriented technique and found that our learning-<phrase>based approach</phrase> detected clones that were either undetected or suboptimally reported by the prominent tool Deckard. Our <phrase>results</phrase> affirm that our learning-<phrase>based approach</phrase> is suitable for clone detection and a tenable technique for researchers.
<phrase>AutoSOME</phrase>: a clustering method for identifying <phrase>gene expression</phrase> modules without <phrase>prior knowledge</phrase> of cluster number BACKGROUND Clustering the <phrase>information</phrase> content of large <phrase>high</phrase>-dimensional <phrase>gene expression</phrase> datasets has widespread application in "<phrase>omics</phrase>" <phrase>biology</phrase>. Unfortunately, the underlying structure of these natural datasets is often fuzzy, and the computational identification of <phrase>data</phrase> clusters generally requires <phrase>knowledge</phrase> about cluster number and <phrase>geometry</phrase>. <phrase>RESULTS</phrase> We integrated strategies from <phrase>machine learning</phrase>, <phrase>cartography</phrase>, and <phrase>graph theory</phrase> into a new informatics method for automatically clustering <phrase>self-organizing map</phrase> ensembles of <phrase>high</phrase>-dimensional <phrase>data</phrase>. Our new method, called <phrase>AutoSOME</phrase>, readily identifies discrete and fuzzy <phrase>data</phrase> clusters without <phrase>prior knowledge</phrase> of cluster number or structure in diverse datasets including whole <phrase>genome</phrase> <phrase>microarray</phrase> <phrase>data</phrase>. Visualization of <phrase>AutoSOME</phrase> output using network diagrams and differential <phrase>heat</phrase> maps reveals unexpected variation among well-characterized <phrase>cancer</phrase> <phrase>cell</phrase> lines. Co-expression analysis of <phrase>data</phrase> from <phrase>human</phrase> <phrase>embryonic</phrase> and induced pluripotent <phrase>stem cells</phrase> using <phrase>AutoSOME</phrase> identifies >3400 up-regulated <phrase>genes</phrase> associated with pluripotency, and indicates that a recently identified <phrase>protein</phrase>-<phrase>protein</phrase> interaction network characterizing pluripotency was underestimated by a factor of four. CONCLUSIONS By effectively extracting important <phrase>information</phrase> from <phrase>high</phrase>-dimensional <phrase>microarray</phrase> <phrase>data</phrase> without <phrase>prior knowledge</phrase> or the need for <phrase>data</phrase> <phrase>filtration</phrase>, <phrase>AutoSOME</phrase> can yield systems-level insights from whole <phrase>genome</phrase> <phrase>microarray</phrase> expression studies. Due to its generality, this new method should also have practical utility for a <phrase>variety</phrase> of <phrase>data</phrase>-intensive applications, including the <phrase>results</phrase> of deep sequencing experiments. <phrase>AutoSOME</phrase> is available for download at http://jimcooperlab.mcdb.ucsb.edu/<phrase>autosome</phrase> <phrase>webcite</phrase>.
<phrase>Authentic learning</phrase> interactions: myth or <phrase>reality</phrase>? This <phrase>paper</phrase> examines a project which utilises a virtual dental clinic and a fictitious eight year old patient-"<phrase>Matthew</phrase>" to situate students within an <phrase>authentic learning</phrase> environment. A case is presented which requires the <phrase>student</phrase> to examine clinical <phrase>information</phrase> about <phrase>Matthew</phrase> in the form of patient <phrase>history</phrase>, clinical slides, <phrase>radiographs</phrase> and expert <phrase>information</phrase> from teachers, <phrase>psychologists</phrase> and an <phrase>endocrinologist</phrase>. The dental <phrase>student</phrase> creates a legitimate treatment plan for <phrase>Matthew</phrase> by analysing <phrase>information</phrase> typically obtained in a real patient encounter. A situated-learning <phrase>design</phrase> was adopted as it provided a means of engaging the dental <phrase>student</phrase> with a legitimate case of a paediatric patient with <phrase>diabetes</phrase>. The <phrase>student</phrase> is required to make a treatment plan based on the <phrase>information</phrase> gleaned from the resources and then compare their reasoning with that of an expert paediatric <phrase>dentist</phrase>. An initial evaluation of the virtual dental clinic was undertaken in <phrase>order</phrase> to determine the perceptions of practicing <phrase>dentists</phrase> in relation to the case. <phrase>Authentic Learning</phrase> Environments <phrase>Authentic learning</phrase> experiences are "those which are problem-or <phrase>case-based</phrase>, that immerse the learner in the situation requiring him or her to acquire skills or <phrase>knowledge</phrase> in <phrase>order</phrase> to solve the problem or manipulate the situation" (Jonassen, Mayes, & McAleese, 1992, p. 235). "Authentic tasks enable students to immerse themselves in the <phrase>culture</phrase> of the <phrase>academic</phrase> domain, much like an apprentice" (Young, 1993, p. 43). Interest in environments that immerse students in <phrase>authentic learning</phrase> experiences, where the meaning of <phrase>knowledge</phrase> and skills are realistically embedded, has been longstanding (<phrase>Dewey</phrase>, 1938; <phrase>Piaget</phrase>, 1952). More recently, attempts to enhance <phrase>cognition</phrase> in <phrase>authentic learning</phrase>-performing tasks have become widespread (<phrase>e</phrase>. The need for realistic experience and <phrase>real-world</phrase> <phrase>problem-solving</phrase> is also advocated. Ramsden (1987) suggests that <phrase>university</phrase> graduates are not obtaining deep conceptual <phrase>knowledge</phrase> that will allow them to think like experts in their discipline. We contend that a key role of a <phrase>university</phrase> <phrase>education</phrase> is to produce <phrase>health</phrase> professionals who are capable of solving complex and ill-structured problems and have the ability to reflect on their <phrase>professional</phrase> practice. This practitioner is capable of reflecting on the
Machine <phrase>Lifelong Learning</phrase>: Challenges and Benefits for <phrase>Artificial General Intelligence</phrase> We propose that it is appropriate to more seriously consider the <phrase>nature</phrase> of systems that are capable of learning over a <phrase>lifetime</phrase>. There are three reasons for taking this position. First, there exists a body of related work for this <phrase>research</phrase> under names such as constructive induction, continual learning, sequential task learning and most recently learning with <phrase>deep architectures</phrase>. Second, the computational and <phrase>data</phrase> storage power of modern <phrase>computers</phrase> are capable of implementing and testing machine <phrase>lifelong learning</phrase> systems. Third, there are significant challenges and benefits to pursuing programs of <phrase>research</phrase> in the <phrase>area</phrase> to AGI and <phrase>brain</phrase> sciences. This <phrase>paper</phrase> discusses each of the above in the context of a <phrase>general</phrase> framework for machine <phrase>lifelong learning</phrase>.
Predictive <phrase>Behavior Analysis</phrase> for Smart Environments Predictive <phrase>behavior analysis</phrase> allows prediction of the (<phrase>human</phrase>) behavior based on the analysis of historical <phrase>data</phrase>. Efficient approaches for predictive <phrase>behavior analysis</phrase> are available for scenarios with structured processes (e.g., based on <phrase>ERP</phrase> systems). The prediction of behavior becomes an obstacle when unstructured (<phrase>decision making</phrase>) processes underlie the scenario. Scenarios with unstructured processes can be found in smart environments <phrase>logging</phrase> <phrase>sensor</phrase> (event) streams such as e.g., Smart Home or Connected Cars. No efficient solutions exist to identify abnormal behavior (anomalies) in such smart environments. To provide a <phrase>solution</phrase> for <phrase>anomaly detection</phrase> in unstructured processes we suggest crossing <phrase>process engineering</phrase> with <phrase>deep learning</phrase>. Methods from <phrase>process engineering</phrase> allow identifying deviations while <phrase>deep learning</phrase> improves the robustness of anomalie detection and prediction. This conjunction is a promising approach in <phrase>order</phrase> to find an efficient <phrase>solution</phrase>. 1 Introduction Predictive <phrase>behavior analysis</phrase> allows prediction of the (<phrase>human</phrase>) behavior based on the analysis of historical <phrase>data</phrase> [Bi55]. Particularly, predictive <phrase>behavior analysis</phrase> intends finding patterns, which allow to identify deviations of <phrase>human</phrase> behavior and to give predictions how likely it is that activities occur in the future. Efficient approaches for predictive behavior are available for scenarios with structured processes. <phrase>Blue</phrase> Yonder 3 is the leading predictive application for structured processes (i.e., they evaluate operating SCM, <phrase>ERP</phrase> or HR systems). The forecasts of <phrase>Blue</phrase> Yonder are highly robust because it is assumed that <phrase>human</phrase> behavior only rarely changes. The prediction of behavior becomes an obstacle when unstructured (<phrase>decision making</phrase>) processes underlie the scenario. Unstructured processes are characterized by activities that take place spontaneously and thus the prediction of an appropriate <phrase>order</phrase> of activities is hampered. Scenarios with unstructured processes can be found in smart environments <phrase>logging</phrase> <phrase>sensor</phrase> (event) streams such as e.g., Smart Home or Connected Cars. No efficient solutions exist to
A <phrase>Hybrid</phrase> Neural <phrase>Learning Algorithm</phrase> Using <phrase>Evolutionary</phrase> Learning and <phrase>Derivative</phrase> <phrase>Free</phrase> <phrase>Local Search</phrase> Method In this <phrase>paper</phrase> we investigate a <phrase>hybrid</phrase> <phrase>model</phrase> based on the Discrete <phrase>Gradient</phrase> method and an <phrase>evolutionary</phrase> strategy for determining the weights in a <phrase>feed forward</phrase> <phrase>artificial neural network</phrase>. Also we discuss different variants for <phrase>hybrid</phrase> models using the Discrete <phrase>Gradient</phrase> method and an <phrase>evolutionary</phrase> strategy for determining the weights in a <phrase>feed forward</phrase> <phrase>artificial neural network</phrase>. The Discrete <phrase>Gradient</phrase> method has the advantage of being able to jump over many <phrase>local minima</phrase> and find very deep <phrase>local minima</phrase>. However, earlier <phrase>research</phrase> has shown that a good <phrase>starting point</phrase> for the discrete <phrase>gradient</phrase> method can improve the quality of the <phrase>solution</phrase> point. <phrase>Evolutionary algorithms</phrase> are best suited for global optimisation problems. Nevertheless they are cursed with longer training times and often unsuitable for <phrase>real world</phrase> application. For optimisation problems such as weight optimisation for ANNs in <phrase>real world</phrase> applications the dimensions are large and time complexity is critical. Hence the idea of a <phrase>hybrid</phrase> <phrase>model</phrase> can be a suitable option. In this <phrase>paper</phrase> we propose different <phrase>fusion</phrase> strategies for <phrase>hybrid</phrase> models combining the <phrase>evolutionary</phrase> strategy with the discrete <phrase>gradient</phrase> method to obtain an optimal <phrase>solution</phrase> much quicker. Three different <phrase>fusion</phrase> strategies are discussed: a linear <phrase>hybrid</phrase> <phrase>model</phrase>, an iterative <phrase>hybrid</phrase> <phrase>model</phrase> and a restricted <phrase>local search</phrase> <phrase>hybrid</phrase> <phrase>model</phrase>. Comparative <phrase>results</phrase> on a <phrase>range</phrase> of standard datasets are provided for different <phrase>fusion</phrase> <phrase>hybrid</phrase> models.
Restricted <phrase>Deep Belief</phrase> Net- Works for <phrase>Multi-view</phrase> Learn- Ing A <phrase>Deep belief</phrase> network (DBN) is a probabilistic <phrase>generative model</phrase> with <phrase>multiple layers</phrase> of <phrase>hidden nodes</phrase> and a layer of visible nodes, where parameterizations between layers obey <phrase>harmonium</phrase> or <phrase>restricted Boltzmann machines</phrase> (RBMs). In this <phrase>paper</phrase> we present restricted <phrase>deep belief</phrase> network (RDBN) for <phrase>multi-view</phrase> learning, where each layer of <phrase>hidden nodes</phrase> is composed of view-specific and shared <phrase>hidden nodes</phrase>, in <phrase>order</phrase> to learn individual and shared hidden spaces from multiple views of <phrase>data</phrase>. View-specific <phrase>hidden nodes</phrase> are connected to corresponding view-specific <phrase>hidden nodes</phrase> in the <phrase>lower</phrase>-layer or visible nodes involving a specific view, whereas shared <phrase>hidden nodes</phrase> follow inter-layer connections without restrictions as in standard DBNs. RDBN is trained using <phrase>layer-wise</phrase> contrastive divergence learning. Numerical experiments on synthetic and <phrase>real-world</phrase> datasets demonstrate the useful behavior of the RDBN, compared to the multi-wing <phrase>harmonium</phrase> (MWH) which is a two-layer undirected <phrase>model</phrase>.
Using a Network <phrase>Simulation</phrase> Tool to engage students in <phrase>Active Learning</phrase> enhances their understanding of complex <phrase>data</phrase> communications concepts Computer networking concepts can be difficult to understand and teach as they frequently relate to complex and dynamic processes which are not readily visible or intuitive and are therefore problematic to conceptualise. Consequently teachers often incorporate <phrase>simulation</phrase> or visualisation tools to support the learning process, but often in a superficial way and without evaluating their effectiveness. To tackle this issue we designed the practical sessions in a 2 nd year <phrase>undergraduate</phrase> networking unit to use a network <phrase>simulation</phrase> tool, Packet Tracer , to facilitate <phrase>active learning</phrase> by providing an analytical, <phrase>problem solving</phrase> and evaluation framework. To then evaluate the effectiveness of using Packet Tracer in this way, students were assessed before and after participating in one specific practical session. Measured <phrase>results</phrase> showed a marked improvement in <phrase>student</phrase> understanding of the topic presented (VLANs). We show that the use of the <phrase>simulation</phrase> tool, not merely to demonstrate concepts, but to also provide <phrase>feedback</phrase> and guidance enhanced <phrase>deep learning</phrase>.
Learning <phrase>Deep Belief</phrase> Networks from Non-stationary Streams <phrase>Deep learning</phrase> has <phrase>proven</phrase> to be beneficial for complex tasks such as classifying images. However, this approach has been mostly applied to static datasets. The analysis of non-stationary (e.g., concept drift) streams of <phrase>data</phrase> involves specific issues connected with the temporal and changing <phrase>nature</phrase> of the <phrase>data</phrase>. In this <phrase>paper</phrase>, we propose a <phrase>proof-of-concept</phrase> method, called Adaptive <phrase>Deep Belief</phrase> Networks, of how <phrase>deep learning</phrase> can be generalized to learn online from changing streams of <phrase>data</phrase>. We do so by exploiting the generative properties of the <phrase>model</phrase> to incrementally retrain the <phrase>Deep Belief</phrase> Network whenever new <phrase>data</phrase> are collected. This approach eliminates the need to store past observations and, therefore, requires only constant <phrase>memory</phrase> consumption. Hence, our approach can be valuable for <phrase>lifelong learning</phrase> from non-stationary <phrase>data</phrase> streams.
Kodu <phrase>Game</phrase> Lab: a <phrase>Programming</phrase> Environment Kodu <phrase>Game</phrase> Lab is a <phrase>tile</phrase>-based <phrase>visual programming</phrase> tool that enables users to learn <phrase>programming</phrase> concepts through making and playing computer <phrase>games</phrase>. Kodu is a relatively new <phrase>programming language</phrase> designed specifically for young children to learn through <phrase>independent</phrase> exploration. It is integrated in a real-time isometric 3D gaming environment that is designed to compete with modern <phrase>console</phrase> <phrase>games</phrase> in terms of intuitive <phrase>user interface</phrase> and <phrase>graphical</phrase> <phrase>production</phrase> values. Kodu <phrase>Game</phrase> Lab (KGL) is a <phrase>tile</phrase>-based <phrase>visual programming</phrase> environment that enables users to create and <phrase>play</phrase> <phrase>video games</phrase> and <phrase>animated</phrase> stories that include feature rich <phrase>multimedia</phrase> and multisensory content. The <phrase>software</phrase> was developed to enable young children to create <phrase>video games</phrase> with <phrase>graphical</phrase> content that can compete with the types of <phrase>games</phrase> that most users would consider as standard. The visual (Figure 1), auditory and kinesthetic attributes of the <phrase>software</phrase> have the potential to make it attractive and distinct for the students and provide deep and engaging <phrase>learning experiences</phrase>.
<phrase>Domain Adaptation</phrase> for <phrase>Large-Scale</phrase> Sentiment Classification: A <phrase>Deep Learning</phrase> Approach The exponential increase in the availability of online reviews and recommendations makes sentiment classification an interesting topic in <phrase>academic</phrase> and <phrase>industrial</phrase> <phrase>research</phrase>. Reviews can span so many different domains that it is difficult to gather annotated <phrase>training data</phrase> for all of them. Hence, this <phrase>paper</phrase> studies the problem of <phrase>domain adaptation</phrase> for sentiment classifiers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a <phrase>deep learning</phrase> approach which learns to extract a meaningful representation for each review in an unsuper-vised <phrase>fashion</phrase>. Sentiment classifiers trained with this <phrase>high</phrase>-level <phrase>feature representation</phrase> clearly outperform <phrase>state</phrase>-of-the-<phrase>art</phrase> methods on a benchmark composed of reviews of 4 types of <phrase>Amazon</phrase> <phrase>products</phrase>. Furthermore, this method scales well and allowed us to successfully perform <phrase>domain adaptation</phrase> on a larger <phrase>industrial</phrase>-strength dataset of 22 domains.
Analysis and Synthesis of Collaborative Opportunistic <phrase>Navigation</phrase> <phrase>Systems Analysis</phrase> and Synthesis of Collaborative Opportunistic <phrase>Navigation</phrase> Systems Dedicated to anyone who got <phrase>lost</phrase> and could not find the way home Acknowledgments I would like to <phrase>express my gratitude</phrase> to a number of people who played a role in my <phrase>academic</phrase> journey. First, I thank my advisor Todd Humphreys for his advice and support throughout my doctoral studies. I still remember vividly our first meeting in which he discussed his <phrase>research</phrase> vision. I thank him for introducing me to such a captivating field of <phrase>research</phrase> and for giving me the freedom to pursue the various intriguing problems of my dissertation. I also thank my co-advisor Ari Arapostathis for his advice during my M.S.E. and Ph.D. studies. I enjoyed each and every <phrase>research</phrase> meeting with him, which have often branched to discussing topics in <phrase>science</phrase>, <phrase>philosophy</phrase>, and <phrase>politics</phrase>. I have learned a great deal from his deep <phrase>mathematical</phrase> insight and his approach in formulating fundamental <phrase>research</phrase> questions. I thank my Ph.D. committee members for taking time to serve on my committee. I thank Brian Evans for being an outstanding mentor throughout my <phrase>academic</phrase> journey. I thank Maruthi <phrase>Akella</phrase> who taught me the Nonlinear Dynamics and <phrase>Adaptive Control</phrase> course, which was the most engaging controls course I have taken in my graduate studies. I thank <phrase>Constantine</phrase> Caramanis from whom I learned how to tackle any problem through <phrase>mathematical</phrase> fundamentals exclusively. I thank Ahmed Tewfik, Gustavo de Veciana, and Behcet Acikmese for v their advice and support during my faculty search.
Shadow systems: the good, the bad and the ugly We know them as shadow systems, workaround systems, and even feral systems. Operating at the fringes of an <phrase>organization</phrase>, they covertly replicate the <phrase>data</phrase> and functionality of formally sanctioned systems. Because of their duplicative acts, they are often said to have negative consequences for their hosts: undermining official systems, 12 sapping valuable resources and corrupting organizational <phrase>data</phrase> and processes. 7 But not all shadow systems <phrase>live</phrase> up to their bad reputations. Some shadow systems offer an effective and efficient way for users to cope with the deficiencies of formal systems. 1, 4 This article reports on an <phrase>ethnographic</phrase> investigation of a shadow system used in a higher educational institution, <phrase>Central Queensland University</phrase> (CQU), whose findings challenge conventional views of these organizational outlaws. The study explored how the system was built, implemented, and applied with a specific interest in people's lived experiences the good, the bad, and the ugly. During the course of the investigation I observed a number of important lessons that impact on successful organizational use of these systems (see Table 3). These lessons should encourage <phrase>technology</phrase> practitioners and senior managers alike to reexamine typical prejudices surrounding shadow systems, and see them for what they are. In some cases, shadow systems may be bad to the core, but in others they can be just what an <phrase>organization</phrase> needs a powerful source of <phrase>creativity</phrase> and <phrase>innovation</phrase>. Because of their informality, shadow systems are rarely obvious in organizations and this can make it difficult for investigators to get access to them. In this investigation, though, I was able to avoid this problem by concentrating on a narrow but deep investigation of a shadow system popular at my own <phrase>organization</phrase>. This shadow system, called Webfuse, was developed to support the <phrase>teaching and learning</phrase> activities of academics and <phrase>general</phrase> staff in a <phrase>single</phrase> faculty at CQU. Background details of how Webfuse, and the corresponding formal systems, developed over time in response to significant milestones in CQU's <phrase>history</phrase> are documented in Table 1. The <phrase>basic</phrase> functionality provided by Webfuse, a home-grown system, is similar in many ways to commercially available <phrase>learning management systems</phrase>. Unlike commercial variants, however, it provides many additional functions tailored to CQU's specific circumstances. One of its more popular functions (one that shadows the functionality of the central system) is the uploading of students' end of term <phrase>results</phrase> into the Enterprise System. This process is referred to as '<phrase>results</phrase> 
Learning <phrase>Video</phrase> Object Segmentation from Static Images Inspired by <phrase>recent advances</phrase> of <phrase>deep learning</phrase> in instance segmentation and <phrase>object tracking</phrase>, we introduce <phrase>video</phrase> object segmentation problem as a concept of guided instance segmentation. Our <phrase>model</phrase> proceeds on a per-frame basis, guided by the output of the previous frame towards the object of interest in the next frame. We demonstrate that highly accurate object segmentation in videos can be enabled by using a convnet trained with static images only. The key ingredient of our approach is a combination of offline and <phrase>online learning</phrase> strategies, where the former serves to produce a refined mask from the previous' frame estimate and the latter allows to capture the appearance of the specific object instance. Our method can handle different types of input annotations: bounding boxes and segments, as well as incorporate multiple annotated frames, making the system suitable for diverse applications. We obtain competitive <phrase>results</phrase> on three different datasets, independently from the type of input annotation.
<phrase>Constraint-satisfaction</phrase> Inference for Entity Recognition One approach to QA answering is to match a question to candidate answers in a background corpus based on <phrase>semantic</phrase> overlap, possibly in combination with other levels of matching, such as lexical <phrase>vector space</phrase> similarity and <phrase>syntactic</phrase> similarity. While the computation of deep <phrase>semantic</phrase> similarity is as yet generally infeasible, <phrase>semantic</phrase> analysis in a specific domain is feasible, if the analysis is constrained to finding <phrase>domain-specific</phrase> entities and <phrase>basic</phrase> relations. Finding <phrase>domain-specific</phrase> entities, the focus of this chapter, is still not a trivial task due to ambiguities of terms. This problem, like many others in <phrase>Natural Language Processing</phrase>, is a <phrase>sequence</phrase> labelling task. We describe the development of a new approach to <phrase>sequence</phrase> labelling in <phrase>general</phrase>, based on the <phrase>constraint satisfaction</phrase> inference. The output of the <phrase>machine-learning</phrase>-based classifiers that solve aspects of the task (such as subsequently predicting the output of the <phrase>label</phrase> <phrase>sequence</phrase>) are considered as constraints on the global structured output analysis. The <phrase>constraint-satisfaction</phrase> inference method is compared to other <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>sequence</phrase> labelling approaches, showing competitive performance.
Performance Bounds for Lambda Policy Iteration and Application to the <phrase>Game</phrase> of <phrase>Tetris</phrase> HAL is a multidisciplinary <phrase>open access</phrase> archive for the deposit and dissemination of scientific <phrase>research</phrase> documents, whether they are published or not. The documents may come from teaching and <phrase>research</phrase> institutions in <phrase>France</phrase> or abroad, or from <phrase>public</phrase> or <phrase>private</phrase> <phrase>research</phrase> centers. L'archive ouverte pluridisciplinaire HAL, est destine <phrase>au</phrase> <phrase>dpt</phrase> et la <phrase>diffusion</phrase> de documents scientifiques de niveau recherche, publis ou non, emanant destablissements d'enseignement et de recherche franais o etrangers, des laboratoires publics ou privs. Abstract We consider the discrete-time infinite-horizon <phrase>optimal control</phrase> problem formalized by that introduced Policy Iteration, a <phrase>family</phrase> of <phrase>algorithms</phrase> parameterized by that generalizes the standard <phrase>algorithms</phrase> Value Iteration and Policy Iteration, and has some deep connections with the Temporal Differences <phrase>algorithm</phrase> <phrase>TD</phrase>() described by Sutton and Barto (1998). We deepen the original theory developped by the authors by providing convergence rate bounds which generalize standard bounds for Value Iteration described for instance by Puterman (1994). Then, the main contribution of this <phrase>paper</phrase> is to develop the theory of this <phrase>algorithm</phrase> when it is used in an approximate form and show that this is <phrase>sound</phrase>. Doing so, we extend and unify the separate analyses devel-opped by Munos for Approximate Value Iteration (Munos, 2007) and Approximate Policy Iteration (Munos, 2003). Eventually, we revisit the use of this <phrase>algorithm</phrase> in the training of a <phrase>Tetris</phrase> playing controller as originally done by Bertsekas and Ioffe (1996). We provide an original performance bound that can be applied to such an undiscounted control problem. Our empirical <phrase>results</phrase> are different from those of Bertsekas and Ioffe (which were originally qualified as " paradoxical " and " intriguing "), and much more conform to what one would expect from a learning experiment. We discuss the possible reason for such a difference.
Coverage of course topics in a <phrase>student</phrase> generated <phrase>MCQ</phrase> repository A recent approach to engaging students in <phrase>deep learning</phrase> involves an online tool, PeerWise, through which students contribute <phrase>multiple-choice</phrase> questions to a shared question <phrase>bank</phrase>. Earlier work demonstrated a strong correlation between the use of PeerWise and <phrase>student</phrase> performance. In this study we investigate the quality of the <phrase>MCQ</phrase> repository created by students in an introductory <phrase>programming</phrase> course by analysing the <phrase>range</phrase> of topics on which students chose to write questions (i.e.~the repository coverage) without guidance from an instructor. We assess the repository coverage by comparing it with a common list of typical introductory <phrase>programming</phrase> topics, and by looking at its extent. We find that, despite having freedom to choose any topic, students created a repository that <phrase>covered</phrase> all the <phrase>major</phrase> topics in the <phrase>curriculum</phrase>.
<phrase>Kernel-based</phrase> <phrase>Semantic</phrase> Role Labeling <phrase>Kernel-based</phrase> <phrase>Semantic</phrase> Role Labeling Automatic <phrase>semantic</phrase> <phrase>parsing</phrase> has always been one of the main goals of <phrase>natural language understanding</phrase>. Through deep <phrase>semantic</phrase> <phrase>parsing</phrase>, the <phrase>natural language</phrase> can be translated into the form <phrase>language</phrase>, so that <phrase>computers</phrase> can communicate with <phrase>human</phrase> beings freely. To that end, it has been going on for years of effort. However, because this issue is too complex, the <phrase>results</phrase> are not very idea now. Shallow <phrase>semantic</phrase> <phrase>parsing</phrase> is a simplified deep <phrase>semantic</phrase> <phrase>parsing</phrase>. It only <phrase>labels</phrase> predicate related constituents with <phrase>semantic</phrase> roles in a sentence, such as agent, patient, time, place, and so on. The technique can promote many applications, such as question and answering, <phrase>information extraction</phrase>, and <phrase>machine translation</phrase>. <phrase>Semantic</phrase> role labeling is a way to achieve shallow <phrase>semantic</phrase> <phrase>parsing</phrase>. It has many advantages, such as clear definition and easy to evaluate. More and more researchers have paid much attention on it in recent years. At present, the mainstream studies of <phrase>semantic</phrase> role labeling focus on the use of a <phrase>variety</phrase> of <phrase>statistical machine learning</phrase> techniques, the use of all kinds of <phrase>linguistics</phrase> features, and to identify and classify <phrase>semantic</phrase> roles. In recent years, studies have shown that <phrase>machine learning</phrase> models is not the primary factor to effect the <phrase>semantic</phrase> role labeling performance, but the use of the features. Therefore, in <phrase>order</phrase> to improve the system performance, detailed features <phrase>engineering</phrase> work is essential. However, as more and more features have been added, the interaction among features has become more and more serious. It makes the growth trend of system performance gradually slowing down and reaching an <phrase>upper</phrase> bound. So we must find new ways to solve this problem. Through the combination or decomposition of features, <phrase>kernel-based</phrase> methods can map <phrase>low-dimensional</phrase> <phrase>feature space</phrase> into higher-dimensional <phrase>feature space</phrase>. Thereby, it makes the problem which is not easy to distinguish in <phrase>low-dimensional</phrase> <phrase>feature space</phrase> becoming addressed in <phrase>high</phrase>-dimensional <phrase>feature space</phrase>. We make use of the advantages of this method and apply to the <phrase>semantic</phrase> role labeling task. In addition to using existing <phrase>kernel-based</phrase> methods, we propose a <phrase>variety</phrase> of new methods. At first, we construct a baseline <phrase>semantic</phrase> role labeling system, which uses a fea- III ture <phrase>vector</phrase> to represent a classification object and uses a <phrase>polynomial</phrase> kernel to combine features automatically. The evaluation <phrase>results</phrase> show that the system is one of the <phrase>state</phrase>-of-the-<phrase>art</phrase> systems, which base on <phrase>single</phrase> <phrase>syntactic</phrase> parser. Then, for our baseline system, it has the problem of 
Understanding <phrase>Complex Systems</phrase> 1 Promoting Transfer by Grounding <phrase>Complex Systems</phrase> Principles Promoting Transfer by Grounding <phrase>Complex Systems</phrase> Principles Connecting <phrase>Science</phrase> with <phrase>Complex Systems</phrase> Principles Understanding <phrase>Complex Systems</phrase> Understanding scientific phenomena in terms of <phrase>complex systems</phrase> principles is both scientifically and pedagogically important. Situations from different disciplines of <phrase>science</phrase> are often governed by the same principle, and so promoting <phrase>knowledge transfer</phrase> across disciplines makes valuable cross-fertilization and scientific unification possible. Although evidence for this kind of transfer has been historically controversial, experiments and observations of students suggest pedagogical methods to promote transfer of <phrase>complex systems</phrase> principles. One powerful strategy is for students to actively interpret the elements and interactions of perceptually grounded scenarios. Such interpretation can be facilitated through the presentation of a situation alongside a description of how the agents in the situation are behaving, and by students exploring and constructing computational models of the situation. The resulting <phrase>knowledge</phrase> can be both concretely grounded yet highly perspective-dependent and generalizeable. We discuss methods for coordinating computational and mental models of <phrase>complex systems</phrase>, the roles of idealization and concreteness in fostering understanding and generalization, and other complementary theoretical approaches to achieving transfer. When and how do students transfer what they have learned to new situations? This is one of the most important questions confronting <phrase>education</phrase> and <phrase>cognitive science</phrase>. Addressing it has crucial practical consequence, while also touching on deep <phrase>basic research</phrase> issues related to learning, analogical reasoning, and conceptual representation. Considerable <phrase>research</phrase> suggests that students do not spontaneously transfer what they have learned, at least not to superficially dissimilar domains 1983). This is disturbing because teachers choose content with the hope that students will apply what they have learned to relevant new situations. We believe that students can transfer scientific principles across superficially dissimilar domains, and we are not alone in this belief To present our case, we will describe kinds of transfer that are worth " fighting for. " Identifying these turns out not to be only an educational question, but a scientific question as well. Accordingly, we will describe an approach toward <phrase>science</phrase> that seeks to unite phenomena from disparate domains according to <phrase>general</phrase> principles that govern <phrase>complex systems</phrase>. This <phrase>complex systems</phrase> approach to <phrase>science</phrase> offers unique educational opportunities for imparting scientific understanding that is both concretely grounded yet widely applicable across many domains. The notion of a grounded generalization may <phrase>sound</phrase> like an <phrase>oxymoron</phrase>, but it is key to our account of transfer. The time-honored method for conveying generalizations has been to use symbolic formalisms such as <phrase>predicate logic</phrase> or <phrase>algebra</phrase>. These formalisms can enable a 
Investigation of full-<phrase>sequence</phrase> training of <phrase>deep belief</phrase> networks for <phrase>speech recognition</phrase> Recently, <phrase>Deep Belief</phrase> Networks (DBNs) have been proposed for <phrase>phone recognition</phrase> and were found to achieve highly competitive performance. In the original DBNs, only frame-level <phrase>information</phrase> was used for training DBN weights while it has been known for <phrase>long</phrase> that sequential or full-<phrase>sequence</phrase> <phrase>information</phrase> can be helpful in improving <phrase>speech recognition</phrase> accuracy. In this <phrase>paper</phrase> we investigate approaches to optimizing the DBN weights, <phrase>state</phrase>-to-<phrase>state</phrase> transition parameters, and <phrase>language</phrase> <phrase>model</phrase> scores using the sequential discriminative training criterion. We describe and analyze the proposed training <phrase>algorithm</phrase> and strategy, and discuss practical issues and how they affect the final <phrase>results</phrase>. We show that the DBNs learned using the <phrase>sequence</phrase>-based training criterion outperform those with frame-based criterion using both three-layer and six-layer models, but the optimization procedure for the deeper DBN is more difficult for the former criterion.
Searching the <phrase>internet</phrase> for learning materials through didactic indicators <phrase>Internet</phrase> offers a huge amount of didactic materials that can be used in creating new online courses. However, those materials need a deep analysis to understand their context and contents before their potential use. As a consequence, the search of didactic material in <phrase>internet</phrase> is often quite tedious and time consuming so the searcher usually limits his/her analysis to the first found <phrase>web pages</phrase>. To help users in finding efficiently and timely the most appropriate online materials, we have developed a system, called SAXEF (System for Automatic eXtraction of lEearning object Features), that is capable to automatically extract the didactic indicators (a sort of <phrase>DNA</phrase>) of any web page (or group of pages) found on <phrase>internet</phrase>. Moreover, we have developed an <phrase>e</phrase>-learning <phrase>search engine</phrase>, SaxSearch, around SAXEF. It allows the user to make requests in terms of didactic indicators and automatically browses the <phrase>internet</phrase> to find the <phrase>web pages</phrase> that best match the user requirements.
<phrase>Risk</phrase> and <phrase>Risk Management</phrase> Practices within <phrase>Information</phrase> System <phrase>Outsourcing</phrase> <phrase>Information</phrase> systems (IS) <phrase>outsourcing</phrase> is one <phrase>aspect</phrase> of <phrase>outsourcing</phrase>, where service or activities of IS are contracted out to the third <phrase>party</phrase> <phrase>management</phrase> to obtain a required result. The most outsourced <phrase>information</phrase> systems (IS) <phrase>function</phrase> of higher learning institutions is <phrase>information technology</phrase> (IT) <phrase>infrastructure</phrase>, application <phrase>management</phrase>, and <phrase>E</phrase>-learning. Beside the advantages IS <phrase>outsourcing</phrase> brings , it faces risks and requires effective <phrase>management</phrase> from the outset of the <phrase>outsourcing</phrase> evaluation through the <phrase>life</phrase> of the contractual relationship. The aim of the <phrase>thesis</phrase> was to study the risks associated with IS <phrase>outsourcing</phrase> and the <phrase>management</phrase> of these risks within the context of <phrase>private</phrase> higher learning institutions in <phrase>Ethiopia</phrase>. The strategy of inquiry used is a <phrase>case study</phrase> strategy. For <phrase>data</phrase> collection, I interviewed ten individuals having important roles in the IS <phrase>outsourcing</phrase> process such as <phrase>chief information officer</phrase>, Chief Network <phrase>Officer</phrase> Chief <phrase>Software Development</phrase> <phrase>Officer</phrase> and also users and <phrase>domain experts</phrase>. The <phrase>research</phrase> employed a <phrase>qualitative research</phrase> method. For this reason, the <phrase>research</phrase> result largely drawn from the analysis of the interviews. After analysing the interviews, an inductive <phrase>qualitative research</phrase> approach was used to draw conclusions. A <phrase>single</phrase> case <phrase>design</phrase> and <phrase>narrative</phrase> approach was used for analysing the interview. Finally findings revealed that even though the participants of the <phrase>university</phrase> understood what <phrase>outsourcing</phrase> IS is all about, the <phrase>university</phrase> doesnot have documented and structured <phrase>outsourcing</phrase> strategy program. The study help also discovering the fact that there are no <phrase>risk management</phrase> frameworks for IS <phrase>outsourcing</phrase> projects. IS <phrase>outsourcing</phrase> project is managed like any other system projects. In the <phrase>private</phrase> higher learning institution the achievements of <phrase>service level agreement</phrase> (SLA) requirements have a great importance in the performance monitoring of the outsourced IS functions. The participation of users with any type of IS <phrase>outsourcing</phrase> project is very <phrase>high</phrase> but the involvement of the top level <phrase>management</phrase> is unsatisfactory. Acknowledgment First, special thanks to <phrase>GOD</phrase> for all His mercy, grace, and support through my entire <phrase>life</phrase>. With his infinite help and grace everything is possible. I would like to <phrase>express my deep</phrase> sense of gratitude to my supervisors Dr. Pivi Jokela (Mr. Hkan Sterner, Ph.D. Candidate), from the faculty of Informatics, for his guidance and encouragement throughout the <phrase>research</phrase>. Without there valuable comments and inspiration, the feasibility of the <phrase>masters</phrase> <phrase>thesis</phrase> could be more difficult. I also would like to thank Prof. Anita Mirijamdotter for the support throughout the whole work, and also for the 
<phrase>Computer Aided</phrase> Tracing of Children?s <phrase>Physics</phrase> Learning: a <phrase>Teacher</phrase> Oriented View For an effective <phrase>Teacher</phrase>-<phrase>Student</phrase> interaction, the <phrase>Teacher</phrase> has to maintain a constant understanding of "what is going on" in the Student's mind. When coming to <phrase>Physics</phrase>, the Teacher's ability to propose and to relate explanations at different levels of abstraction-as a chains of causal interactions (deep) or as a set of <phrase>observable</phrase> phenomena (shallow)-may determine a successful and lasting learning in the <phrase>Student</phrase>. Here, we describe a <phrase>knowledge representation</phrase> to be used by the <phrase>teacher</phrase> to depict to herself the student's <phrase>mental model</phrase> and to tune her future lessons according to the current <phrase>student</phrase> comprehension. Supported by a <phrase>cognitive</phrase> theory of children <phrase>physics</phrase> learning, we used the system WHY for modeling the <phrase>evolution</phrase> of a student's learning as it appeared at the teacher's eyes. Two of WHY's features turned out to be essential: (a) to deal with explanations having different levels of abstraction, and (b) the possibility to continuously evaluate the coherence of the hypothesized learner's <phrase>model</phrase> with respect to her explanation. In the <phrase>long</phrase> term, the work's outcome might contribute to the development of <phrase>teaching assistant</phrase> systems that support the <phrase>teacher</phrase> in identifying "what has to be explained next".
Sparse <phrase>deep belief</phrase> net <phrase>model</phrase> for visual <phrase>area</phrase> <phrase>V2</phrase> Motivated in part by the <phrase>hierarchical organization</phrase> of the <phrase>cortex</phrase>, a number of <phrase>algorithms</phrase> have recently been proposed that try to learn hierarchical, or " deep, " structure from <phrase>unlabeled data</phrase>. While several authors have formally or informally compared their <phrase>algorithms</phrase> to computations performed in visual <phrase>area</phrase> V1 (and the <phrase>cochlea</phrase>), little attempt has been made thus far to evaluate these <phrase>algorithms</phrase> in terms of their fidelity for mimicking computations at deeper levels in the <phrase>cortical</phrase> hierarchy. This <phrase>paper</phrase> presents an <phrase>unsupervised learning</phrase> <phrase>model</phrase> that faithfully mimics certain properties of visual <phrase>area</phrase> <phrase>V2</phrase>. Specifically, we develop a sparse variant of the <phrase>deep belief</phrase> networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the first layer, similar to prior work on <phrase>sparse coding</phrase> and ICA, <phrase>results</phrase> in localized, oriented, edge filters, similar to the Gabor functions known to <phrase>model</phrase> V1 <phrase>cell</phrase> <phrase>receptive fields</phrase>. Further, the second layer in our <phrase>model</phrase> encodes correlations of the first layer responses in the <phrase>data</phrase>. Specifically, it picks up both colinear (" contour ") features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex " corner " features matches well with the <phrase>results</phrase> from the Ito & Komatsu's study of biological <phrase>V2</phrase> responses. This suggests that our sparse variant of <phrase>deep belief</phrase> networks holds promise for modeling more <phrase>higher-order</phrase> features.
<phrase>Semantic</phrase> Role Labeling Using Lexical Statistical <phrase>Information</phrase> Our system for <phrase>semantic</phrase> role labeling is multi-stage in <phrase>nature</phrase>, being based on <phrase>tree</phrase> <phrase>pruning</phrase> techniques, <phrase>statistical methods</phrase> for lexicalised feature encoding, and a C4.5 <phrase>decision tree</phrase> classifier. We use both shallow and <phrase>deep syntactic</phrase> <phrase>information</phrase> from automatically generated chunks and parse <phrase>trees</phrase>, and develop a <phrase>model</phrase> for learning the <phrase>semantic</phrase> arguments of predicates as a <phrase>multi-class</phrase> <phrase>decision problem</phrase>. We evaluate the performance on a set of relatively 'cheap' features and <phrase>report</phrase> an F 1 score of 68.13% on the overall <phrase>test</phrase> set.
3D <phrase>Object Recognition</phrase> with <phrase>Deep Belief</phrase> Nets We introduce a new type of top-level <phrase>model</phrase> for <phrase>Deep Belief</phrase> Nets and evaluate it on a 3D <phrase>object recognition</phrase> task. The top-level <phrase>model</phrase> is a third-<phrase>order</phrase> <phrase>Boltzmann</phrase> machine, trained using a <phrase>hybrid</phrase> <phrase>algorithm</phrase> that combines both generative and discriminative gradients. Performance is evaluated on the NORB <phrase>database</phrase> (normalized-uniform version), which contains <phrase>stereo</phrase>-pair images of objects under different lighting conditions and viewpoints. Our <phrase>model</phrase> achieves 6.5% error on the <phrase>test</phrase> set, which is close to the best published result for NORB (5.9%) using a convolutional <phrase>neural net</phrase> that has built-in <phrase>knowledge</phrase> of <phrase>translation</phrase> invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for <phrase>semi-supervised</phrase> learning, and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the <phrase>database</phrase>. With the extra <phrase>unlabeled data</phrase> (and the same amount of <phrase>labeled data</phrase> as before), our <phrase>model</phrase> achieves 5.2% error.
Reset-<phrase>Free</phrase> Guided Policy Search: Efficient <phrase>Deep Reinforcement Learning</phrase> with <phrase>Stochastic</phrase> Initial States Autonomous learning of robotic skills can allow <phrase>general</phrase>-purpose <phrase>robots</phrase> to learn wide behavioral repertoires without extensive manual <phrase>engineering</phrase>. However, robotic skill learning must typically make <phrase>trade</phrase>-offs to enable practical <phrase>real-world</phrase> learning, such as requiring manually designed policy or value <phrase>function</phrase> representations, initialization from <phrase>human</phrase> demonstrations, instrumentation of the training environment, or extremely <phrase>long</phrase> training times. We propose a new <phrase>reinforcement learning</phrase> <phrase>algorithm</phrase> that can <phrase>train</phrase> <phrase>general</phrase>-purpose <phrase>neural network</phrase> policies with minimal <phrase>human</phrase> <phrase>engineering</phrase>, while still allowing for fast, efficient learning in <phrase>stochastic</phrase> environments. We build on the guided policy search (<phrase>GPS</phrase>) <phrase>algorithm</phrase>, which transforms the <phrase>reinforcement learning</phrase> problem into <phrase>supervised learning</phrase> from a computational <phrase>teacher</phrase> (without <phrase>human</phrase> demonstrations). In contrast to prior <phrase>GPS</phrase> methods, which require a consistent set of initial states to which the system must be reset after each episode, our approach can handle random initial states, allowing it to be used even when deterministic resets are impossible. We compare our method to existing policy search <phrase>algorithms</phrase> in <phrase>simulation</phrase>, showing that it can <phrase>train</phrase> <phrase>high</phrase>-dimensional <phrase>neural network</phrase> policies with the same sample efficiency as prior <phrase>GPS</phrase> methods, and can learn policies directly from image <phrase>pixels</phrase>. We also present <phrase>real-world</phrase> <phrase>robot</phrase> <phrase>results</phrase> that show that our method can learn manipulation policies with <phrase>visual features</phrase> and random initial states.
Volatility and Option Pricing Acknowledgements It is a pleasure to thank those who made this <phrase>thesis</phrase> possible. It is difficult to overstate my gratitude to my <phrase>Ph</phrase>. <phrase>Avellaneda</phrase>. His great enthusiasm, deep inspiration, and insightful understanding of <phrase>financial mathematics</phrase> influenced me tremendously ever since the beginning of my <phrase>PhD</phrase> study at Courant. I deeply appreciate his <phrase>patience</phrase>, efforts, and <phrase>creativity</phrase> in guiding me. This <phrase>thesis</phrase> would not have been possible without him. I would like to thank Prof. Jonathan <phrase>Goodman</phrase> for his passionate teaching and his time answering my questions. I am also grateful to Prof. Robert Kohn for encouraging and personal guidance. I wish to express my sincere thanks to Prof. Jim Gatheral for <phrase>reading</phrase> my <phrase>thesis</phrase> and his constructive comments. I thank Prof. Petter Kolm for being in my <phrase>thesis</phrase> committee. My warm thanks are due to Mike Lipkin for his extensive discussions around my work. I wish to extend my thanks to all those who have helped me with my <phrase>thesis</phrase> at Courant. The financial support of the MacCracken Fellowship Program is gratefully acknowledged. I am indebted to my many <phrase>student</phrase> colleagues for providing a stimulating and fun environment in which to learn and grow. I am lucky to meet these intellectual and caring people, who have helped me through the difficult times. I owe my special loving thanks to Stella Chen for supporting me spiritually throughout my study. Abstract This <phrase>thesis</phrase> studies leveraged exchange-traded funds (<phrase>ETF</phrase>) and options written on them. In the first part, we give an exact formula linking the price <phrase>evolution</phrase> of a leveraged <phrase>ETF</phrase> (LETF) with the price of its underlying <phrase>ETF</phrase>. We <phrase>test</phrase> the formula empirically on historical <phrase>data</phrase> for 56 leveraged funds (44 double-leveraged, 12 triple-leveraged) using <phrase>daily</phrase> closing prices. The <phrase>results</phrase> indicate excellent agreement between the formula and the empirical <phrase>data</phrase>. The formula shows that the <phrase>evolution</phrase> of the price of an LETF is sensitive to the realized volatility of the underlying product. The relationship between an LETF and its underlying asset is " path-dependent. " The second part of the study focuses on the relations between options on LETFs and options on the underlying <phrase>ETFs</phrase>. The main result shows that an option on an LETF can be replicated by a basket of options on the underlying <phrase>ETF</phrase> after a suitable choice of strikes and notionals. In particular, we obtain a new, relative-value, <phrase>model</phrase> for pricing LETF options. The derivation makes strong use 
<phrase>Design</phrase> and Learning of Output Representation S for <phrase>Speech Recognition</phrase> 2 In <phrase>deep learning</phrase> <phrase>research</phrase>, often the focus has been on the input 3 <phrase>feature representation</phrase> while the output representation tends to 4 receive much less attention. In this <phrase>paper</phrase>, three largely separate case 5 studies are provided to argue for the importance of learning output 6 representations. In these studies, three ways of designing and/or 7 learning output representations for the <phrase>deep-learning</phrase> approach to 8 <phrase>speech recognition</phrase> are discussed and analyzed. First, the very large 9 number of output units in the current <phrase>context-dependent</phrase> (<phrase>CD</phrase>) deep 10 <phrase>neural net</phrase> (DNN) based speech recognizers can be effectively 11 reduced, without lowering <phrase>recognition accuracy</phrase> while improving 12 decoding efficiency, by performing <phrase>dimensionality reduction</phrase> using 13 low-rank approximation to large DNN output matrices. Second, the 14 currently popular <phrase>CD</phrase>-DNN that uses " beads-on-a-string " or linear-15 <phrase>sequence</phrase> representations for <phrase>linguistic</phrase> speech units in the DNN 16 output layer can be generalized to structured multi-linear or <phrase>graph</phrase> 17 representations. Temporally overlapping <phrase>linguistic</phrase> " features " or 18 symbols are used as a basis for such <phrase>phonological</phrase> <phrase>design</phrase>. Third, when a 19 special type of deep networks, the deep convex network (DCN), is 20 used as a representational <phrase>model</phrase> for speech <phrase>acoustic</phrase> patterns, the 21 output units in each of the DCN modules are designed to be linear, 22 enabling drastic simplification in learning the parameters of the 23 entire network.
Ruler: <phrase>high</phrase>-speed packet matching and rewriting on NPUs <phrase>Programming</phrase> specialized network processors (NPU) is inherently difficult. Unlike mainstream processors where <phrase>architectural</phrase> features such as <phrase>out-of-order execution</phrase> and caches hide most of the complexities of efficient program execution, programmers of NPUs face a 'bare-<phrase>metal</phrase>' view of the <phrase>architecture</phrase>. They have to deal with a multithreaded environment with a <phrase>high</phrase> <phrase>degree</phrase> of parallelism, pipelining and multiple, heterogeneous, execution units and <phrase>memory</phrase> <phrase>banks</phrase>. <phrase>Software development</phrase> on such architectures is expensive. Moreover, different NPUs, even within the same <phrase>family</phrase>, differ considerably in their <phrase>architecture</phrase>, making portability of the <phrase>software</phrase> a <phrase>major</phrase> concern. At the same time expensive network processing applications based on <phrase>deep packet inspection</phrase> are both in-creasingly important and increasingly difficult to realize due to <phrase>high</phrase> link rates. They could potentially benefit greatly from the hardware features offered by NPUs, provided they were easy to use. We therefore propose to use more abstract <phrase>programming</phrase> models that hide much of the complexity of 'bare-<phrase>metal</phrase>' architectures from the <phrase>programmer</phrase>. In this <phrase>paper</phrase>, we present one such <phrase>programming</phrase> <phrase>model</phrase>: Ruler, a flexible <phrase>high</phrase>-level <phrase>language</phrase> for deep packet in-spection (DPI) and packet rewriting that is easy to learn, platform <phrase>independent</phrase> and lets the <phrase>programmer</phrase> concentrate on the functionality of the application. Ruler provides packet matching and rewrit-ing based on <phrase>regular expressions</phrase>. We describe our implementa-tion on the <phrase>Intel</phrase> IXP2xxx NPU and show how it provides versatile packet processing at <phrase>gigabit</phrase> line rates.
Learning the Structure of Deep Sparse <phrase>Graphical</phrase> Models <phrase>Deep belief</phrase> networks are a powerful way to <phrase>model</phrase> complex <phrase>probability distributions</phrase>. However, it is difficult to learn the structure of a <phrase>belief network</phrase>, particularly one with <phrase>hidden units</phrase>. The <phrase>Indian</phrase> <phrase>buffet</phrase> process has been used as a nonparametric <phrase>Bayesian</phrase> prior on the structure of a <phrase>directed</phrase> <phrase>belief network</phrase> with a <phrase>single</phrase> infinitely wide <phrase>hidden layer</phrase>. Here, we introduce the cascading <phrase>Indian</phrase> <phrase>buffet</phrase> process (CIBP), which provides a prior on the structure of a layered, <phrase>directed</phrase> <phrase>belief network</phrase> that is unbounded in both depth and width, yet allows tractable inference. We use the CIBP prior with the nonlinear Gaussian <phrase>belief network</phrase> framework to allow each unit to vary its behavior between discrete and continuous representations. We use <phrase>Markov chain Monte Carlo</phrase> for inference in this <phrase>model</phrase> and explore the structures learned on image <phrase>data</phrase>.
An Extensible <phrase>Language</phrase> Interfacefor <phrase>Robot</phrase> Manipulation This <phrase>paper</phrase> describes our Extensible <phrase>Language</phrase> Interface (ELI) for <phrase>robots</phrase>. The system is intended to interpret far-field speech commands in <phrase>order</phrase> to perform fetch-and-carry tasks, potentially for use in an eldercare context. By " extensible " we mean that the <phrase>robot</phrase> is able to learn new <phrase>nouns</phrase> and <phrase>verbs</phrase> by simple interaction with its user. An associated <phrase>video</phrase> [1] illustrates the <phrase>range</phrase> of phenomena handled by our implemented real-time system. 1 Introduction As argued in [2] with an eye toward <phrase>Vygotsky</phrase>, much of <phrase>intelligence</phrase> is actually illusory since the bulk of what we consider <phrase>knowledge</phrase> or competence is transmitted culturally. No one figures out how to cook <phrase>macaroni</phrase> and <phrase>cheese</phrase> by experimentation some other person tells you how to do it. While part of the feeling of aliveness comes from the responsiveness of a creature with a reasonably deep <phrase>perception</phrase> of its environment , even humans from a different <phrase>society</phrase> can be successfully demonized as " subhuman " if you cannot understand what they say. If <phrase>robots</phrase> are ever to be perceived as sentient it seems crucial that they also be able to learn in this manner and thus partake of the rich prevailing <phrase>culture</phrase> which underpins much of " <phrase>human</phrase>-ness ". <phrase>Language</phrase> understanding and learning also has pragmatic value. For instance, a <phrase>robot</phrase> that could perform simple fetch-and-carry tasks would be a boon to eldercare. However the <phrase>robot</phrase> must be told what to do somehow. The current generation of senior citizens is not comfortable with tablets, <phrase>keyboards</phrase>, styli, <phrase>PDAs</phrase>, or <phrase>Bluetooth</phrase> headsets these are just one more thing to drop or misplace. The most <phrase>human</phrase>-<phrase>friendly</phrase> interface is direct speech using an audio <phrase>pickup</phrase> on the <phrase>robot</phrase> itself. The trick then is interpreting the spoken commands robustly. In addition, a particular home may have locations, like the " solarium " , or objects, like " my favorite cup " , which cannot be known a priori and hence cannot be preprogrammed into the <phrase>robot</phrase>. Thus it would be convenient if the <phrase>robot</phrase> could just be shown such places and objects and learn whatever models it needed automatically. In addition there may be activities such as " tidy
Deep <phrase>Spatio-Temporal</phrase> Architectures and Learning for <phrase>Protein Structure Prediction</phrase> Residue-residue contact prediction is a fundamental problem in <phrase>protein structure prediction</phrase>. Hower, despite considerable <phrase>research</phrase> efforts, contact prediction methods are still largely unreliable. Here we introduce a novel deep <phrase>machine-learning</phrase> <phrase>architecture</phrase> which consists of a multidimensional stack of learning modules. For contact prediction, the idea is implemented as a three-dimensional stack of <phrase>Neu</phrase>-ral Networks NN k ij , where i and j index the spatial coordinates of the contact map and k indexes " time ". The temporal <phrase>dimension</phrase> is introduced to capture the fact that <phrase>protein folding</phrase> is not an instantaneous process, but rather a progressive refinement. Networks at level k in the stack can be trained in supervised <phrase>fashion</phrase> to refine the predictions <phrase>produced</phrase> by the previous level, hence addressing the problem of vanishing gradients, typical of <phrase>deep architectures</phrase>. Increased accuracy and generalization capabilities of this approach are established by rigorous comparison with other <phrase>classical</phrase> <phrase>machine learning</phrase> approaches for contact prediction. The deep approach leads to an accuracy for difficult <phrase>long</phrase>-<phrase>range</phrase> contacts of about 30%, roughly 10% above the <phrase>state</phrase>-of-the-<phrase>art</phrase>. Many variations in the architectures and the training <phrase>algorithms</phrase> are possible, leaving room for further improvements. Furthermore, the approach is applicable to other problems with strong underlying spatial and temporal components.
Deep issues: <phrase>personalization</phrase> and <phrase>privacy</phrase> Many people don't like to have others learn <phrase>information</phrase> about them. Ironically, those same people are usually quite happy to learn <phrase>information</phrase> about others, and sometimes resent it when legal barriers block them from learning such <phrase>information</phrase>. Nonetheless, when it comes to <phrase>information</phrase> about us, many believe we should have (in the words of various <phrase>privacy</phrase> advocates) a legal " right to control <phrase>information</phrase> about ourselves. " Do we currently have such a legal right to control the flow of <phrase>information</phrase> about ourselves by stopping others from speaking about us? The answer, as is typical in <phrase>law</phrase>, is " sometimes. " 1 First, the one thing that's not helpful here is to <phrase>talk</phrase> in <phrase>general</phrase> terms about our right to <phrase>privacy</phrase>. The <phrase>Personalization</phrase> and <phrase>PRIVACY</phrase> <phrase>Eugene Volokh</phrase> P eople are constantly learning <phrase>information</phrase> about us. They see what we do, what we buy, what we look at, and the like. If they know who we are, and if they have enough financial incentive, they can record this <phrase>information</phrase> under our name. If we engage in computerized transactions with them, such <phrase>recording</phrase> becomes very easy, as does combining this <phrase>information</phrase> with still other <phrase>information</phrase> tied to our names. If the transactions are personalizedif we voluntarily turn over <phrase>information</phrase> about ourselves that facilitates our <phrase>business</phrase> arrangementthen they will have even more <phrase>information</phrase> to record. And once they've recorded this <phrase>information</phrase>, they can easily communicate it to others (usually for <phrase>money</phrase>).
<phrase>Social Security</phrase> and Social Welfare <phrase>Data Mining</phrase>: An Overview The importance of <phrase>social security</phrase> and social welfare <phrase>business</phrase> has been increasingly recognized in more and more countries. It impinges on a large proportion of the <phrase>population</phrase> and affects <phrase>government</phrase> service policies and people's <phrase>life</phrase> quality. Typical welfare countries, such as <phrase>Australia</phrase> and <phrase>Canada</phrase>, have accumulated a huge amount of <phrase>social security</phrase> and social welfare <phrase>data</phrase>. Emerging <phrase>business</phrase> issues such as fraudulent outlays, and customer service and performance improvements challenge existing policies, as well as techniques and systems including <phrase>data</phrase> matching and <phrase>business intelligence</phrase> reporting systems. The need for a <phrase>deep understanding</phrase> of customers and customergovernment interactions through advanced <phrase>data</phrase> analytics has been increasingly recognized by the <phrase>community</phrase> <phrase>at large</phrase>. So far, however, no substantial work on the <phrase>mining</phrase> of <phrase>social security</phrase> and social welfare <phrase>data</phrase> has been reported. For the first time in <phrase>data mining</phrase> and <phrase>machine learning</phrase>, and to the best of our <phrase>knowledge</phrase>, this <phrase>paper</phrase> draws a <phrase>comprehensive</phrase> overall picture and summarizes the corresponding techniques and illustrations to analyze <phrase>social security</phrase>/welfare <phrase>data</phrase>, namely, <phrase>social security</phrase> <phrase>data mining</phrase> (SSDM), based on a thorough review of a large number of related references from the past half century. In particular, we introduce an SSDM framework, including <phrase>business</phrase> and <phrase>research</phrase> issues, <phrase>social security</phrase>/welfare services and <phrase>data</phrase>, as well as challenges , goals, and tasks in <phrase>mining</phrase> <phrase>social security</phrase>/welfare <phrase>data</phrase>. A summary of SSDM <phrase>case studies</phrase> is also presented with substantial citations that direct readers to more specific techniques and practices about SSDM.
Acquisition of <phrase>OWL</phrase> <phrase>DL</phrase> <phrase>Axioms</phrase> from Lexical Resources <phrase>State</phrase>-of-the-<phrase>art</phrase> <phrase>research</phrase> on automated learning of <phrase>ontologies</phrase> from text currently focuses on inexpressive <phrase>ontologies</phrase>. The acquisition of complex <phrase>axioms</phrase> involving logical connectives, role restrictions, and other expressive features of the <phrase>Web Ontology Language</phrase> <phrase>OWL</phrase> remains largely unexplored. In this <phrase>paper</phrase>, we present a method and implementation for enriching inexpressive <phrase>OWL</phrase> ontolo-gies with expressive <phrase>axioms</phrase> which is based on a <phrase>deep syntactic</phrase> analysis of <phrase>natural language</phrase> definitions. We argue that it can serve as a core for a <phrase>semi-automatic</phrase> <phrase>ontology engineering</phrase> process supported by a methodology that integrates methods for both <phrase>ontology</phrase> learning and evaluation. The feasibility of our approach is demonstrated by generating complex class descriptions from <phrase>Wikipedia</phrase> definitions and from a <phrase>fishery</phrase> glossary provided by the <phrase>Food and Agriculture Organization</phrase> of the <phrase>United Nations</phrase>.
Add one <phrase>Egg</phrase>, a Cup of <phrase>Milk</phrase>, and Stir: <phrase>Single</phrase> source Documentation for Today What happens when the <phrase>software</phrase> <phrase>firm</phrase> you work for decides it will not deliver large printed manuals any more? Then the request comes to put everything online. Six months later, user profiles shift to the <phrase>World Wide Web</phrase> and you're asked to deliver <phrase>HTML</phrase>. In the future, a <phrase>database</phrase> of <phrase>SGML</phrase> <phrase>information</phrase> chunks may let us deliver anything, any which way, Today, we must devise a system that allows us to " <phrase>author</phrase> once, publish many ". Such a system is crucial for <phrase>software</phrase> and hardware documentation. The method I chose was to go from FrameMaker to Acrobat <phrase>PDF</phrase> files to <phrase>HTML</phrase>. I wrote the source document in <phrase>Adobe</phrase> FrameMaker. Then I converted to <phrase>PDF</phrase> files with <phrase>Adobe Acrobat</phrase>, and again, converted the Frame files to <phrase>HTML</phrase> files using Quadralay WebWorks <phrase>Publisher</phrase>. <phrase>HTML</phrase> and <phrase>PDF</phrase> files aren't full-bodied <phrase>publishing</phrase> formats, and the future will probably be written in <phrase>SGML</phrase>. But while we're waiting for the future, just learning <phrase>SGML</phrase> and <phrase>diving</phrase> deep into DTDs alone could be a mistake. <phrase>SGML</phrase> is a <phrase>language</phrase> which sets out structure, and most of us are concerned with content. One way to handle content is to use <phrase>Information</phrase> Mapping, or <phrase>information</phrase> types of your own devising. hh%hn lo mnke di&Uhnrd copies of all or <phrase>port</phrase> ofthis material for <phrase>pc</phrase>%onol or classroom use is gmnted witbout fee provided tbot the cop& arc not mnde or distributed for profit or commercial ndvaor;lge, <phrase>dlc</phrase> copy-ridlt notice, lbe title of tbe publication nod its date nppnlr, ,<phrase>md</phrase> notice is @en tlut <phrase>copyright</phrase> is by permission of the <phrase>ACM</phrase>. How do you change to <phrase>single</phrase> source? Many writers think there are only two ways to change to <phrase>single</phrase> source: 1. Use a <phrase>software</phrase>-specific conversion. 2. Convert to <phrase>SGML</phrase>. The problem with option 1 is that you may be able to convert to a specific format, but the soft\-vare may not be able to handle another format. You might be able to convert to Acrobat <phrase>PDF</phrase> just fine, but what about <phrase>HTML</phrase>? The problem with option 2 is that <phrase>SGML</phrase> is a complex <phrase>technology</phrase>. There are not only complex tags and rules to learn, but a <phrase>Document Type Definition</phrase> (<phrase>DTD</phrase>) to <phrase>design</phrase>. But a <phrase>DTD</phrase> handles only logical or structural formatting: the <phrase>SGML</phrase> <phrase>software</phrase> companies haven't really agreed how to handle the physical formatting. Do they use DSSLs (Document Style <phrase>Semantics</phrase> and <phrase>Specification Language</phrase> instances) or the <phrase>U.S</phrase>. Navy's FOSIs 
<phrase>Seachange</phrase>: <phrase>design</phrase> of online quiz questions to foster <phrase>deep learning</phrase> The <phrase>design</phrase> of different types of quiz question will influence the extent to which formative and summative <phrase>feedback</phrase> is presented to students. Typically, quiz questions are considered limited in their capacity to assess <phrase>higher order</phrase> <phrase>cognitive</phrase> skills. This <phrase>paper</phrase> extends the notion of online quiz <phrase>design</phrase> by presenting examples in a <phrase>WebCT</phrase> <phrase>learning environment</phrase> in <phrase>order</phrase> to demonstrate a formative approach to assessment, closely integrated with learning processes. A matrix of questions is presented using Bloom's <phrase>taxonomy</phrase> showing the type of question, pedagogical underpinnings and <phrase>cognitive</phrase> skills required. The implications of this type of question <phrase>design</phrase> is that automated quiz type questions do not necessarily imply a narrow focus on recall, but may assess a <phrase>range</phrase> of learning processes. Limits of traditional assessment Educators can be in no doubt of the demands of <phrase>society</phrase> for lifelong capable learners who are able to perform <phrase>cognitive</phrase>, metacognitive and metacognitive tasks and demonstrate competencies such as <phrase>problem solving</phrase>, <phrase>critical thinking</phrase>, questioning, searching for <phrase>information</phrase>, making judgments and evaluating <phrase>information</phrase> (Reeves, 2000). Assessment processes are now in the <phrase>limelight</phrase>, with increasing emphasis placed not on testing discrete skills or on measuring what people know, but on fostering learning and transfer of <phrase>knowledge</phrase>. The traditional approach to assessment is largely a form of objective testing which tends to value students' capacity to memorize facts and then recall them during a <phrase>test</phrase> situation. Magone et al (1994) calls this the one right answer mentality. A second form of assessment is the measurement of competencies, or what we call 'sequestered <phrase>problem solving</phrase>' (Schwartz et al, 2000). In these contexts students are asked to <phrase>solve problems</phrase> in isolation and without the resources that are typically available in the <phrase>real world</phrase> such as texts, Web-resources and <phrase>peers</phrase>. Often these <phrase>tests</phrase> of aptitude are <phrase>single</phrase> <phrase>shot</phrase>, and summative rather than formative. In contrast, assessment that supports learning and <phrase>knowledge transfer</phrase> provides the basis for future learning, and continuing <phrase>motivation</phrase> to learn. This approach is sometimes called the <phrase>alternative</phrase> assessment movement, as it is concerned with assessing performance (Cumming & Maxwell, 1999). Both testing and measuring competence as forms of assessment have been critiqued as being controlling, limiting and contrary to <phrase>student</phrase>-centered <phrase>teaching and learning</phrase>. Other indicators of the need to rethink online and off-line assessment have come from Bull & McKenna (2000) who argue that "the development and integration of <phrase>computer-aided</phrase> assessment has been done in an <phrase>ad hoc</phrase> manner". 
<phrase>Deep learning</phrase> using partitioned <phrase>data</phrase> vectors <phrase>Deep learning</phrase> is a popular field that encompasses a <phrase>range</phrase> of <phrase>multi-layer</phrase> <phrase>connectionist</phrase> techniques. While these techniques have achieved <phrase>great success</phrase> on a number of difficult <phrase>computer vision</phrase> problems, the representation biases that allow this success have not been thoroughly explored. In this <phrase>paper</phrase>, we examine the <phrase>hypothesis</phrase> that one strength of many <phrase>deep learning</phrase> <phrase>algorithms</phrase> is their ability to exploit spatially local statistical <phrase>information</phrase>. We present a formal description of how <phrase>data</phrase> vectors can be partitioned into sub-vectors that preserve spatially local <phrase>information</phrase>. As a <phrase>test case</phrase>, we then use <phrase>statistical models</phrase> to examine how much of such structure exists in the MNIST dataset. Finally, we present <phrase>experimental</phrase> <phrase>results</phrase> from training RBMs using partitioned <phrase>data</phrase>, and demonstrate the advantages they have over non-partitioned RBMs. Through these <phrase>results</phrase>, we show how the performance advantage is reliant on spatially local structure, by demonstrating the performance impact of randomly permuting the <phrase>input data</phrase> to destroy local structure. Overall, our <phrase>results</phrase> support the <phrase>hypothesis</phrase> that a representation bias reliant upon spatially local statistical <phrase>information</phrase> can <phrase>improve performance</phrase>, so <phrase>long</phrase> as this bias is a good match for the <phrase>data</phrase>. We also suggest statistical tools for determining a priori whether a dataset is a good match for this bias or not. I. INTRODUCTION In the past few years, the <phrase>area</phrase> of exploration known as <phrase>Deep Learning</phrase> has demonstrated the ability of multilayer con-nectionist networks to achieve good performance on a <phrase>range</phrase> of difficult <phrase>machine learning</phrase> problems, and has been gaining increasing prominence and attention in the <phrase>machine learning</phrase> and <phrase>neural network</phrase> communities. The theoretical basis for understanding why these techniques perform well on particular problems, however, has only begun to be explored. A <phrase>deeper understanding</phrase> of how and why <phrase>deep learning</phrase> <phrase>algorithms</phrase> work will not only help us to decide what problems are good candidates for their application, but may also suggest ways of improving existing techniques or harnessing their strengths in novel contexts. The <phrase>hypothesis</phrase> we seek to examine is the <phrase>hypothesis</phrase> that many <phrase>deep learning</phrase> <phrase>algorithms</phrase> make use of spatially local structure in their <phrase>input data</phrase> to help them achieve good performance. There has <phrase>long</phrase> been an intuition in the <phrase>deep learning</phrase> <phrase>community</phrase> that this <phrase>hypothesis</phrase> is likely to hold, based partly on everyday experience, and partly on the structure of the <phrase>human</phrase> <phrase>visual cortex</phrase>. To our <phrase>knowledge</phrase>, however, there has been no previous attempt to formalize and <phrase>test</phrase> this <phrase>hypothesis</phrase> specifically. 
Representational <phrase>Distance Learning</phrase> for <phrase>Deep Neural Networks</phrase> <phrase>Deep neural networks</phrase> (DNNs) provide useful models of visual representational transformations. We present a method that enables a DNN (<phrase>student</phrase>) to learn from the internal representational spaces of a reference <phrase>model</phrase> (<phrase>teacher</phrase>), which could be another DNN or, in the future, a biological <phrase>brain</phrase>. Representational spaces of the <phrase>student</phrase> and the <phrase>teacher</phrase> are characterized by representational distance matrices (RDMs). We propose representational <phrase>distance learning</phrase> (RDL), a <phrase>stochastic gradient descent</phrase> method that drives the RDMs of the <phrase>student</phrase> to approximate the RDMs of the <phrase>teacher</phrase>. We demonstrate that RDL is competitive with other <phrase>transfer learning</phrase> techniques for two publicly available benchmark <phrase>computer vision</phrase> datasets (MNIST and CIFAR-100), while allowing for <phrase>architectural</phrase> differences between <phrase>student</phrase> and <phrase>teacher</phrase>. By pulling the student's RDMs toward those of the <phrase>teacher</phrase>, RDL significantly improved visual classification performance when compared to baseline networks that did not use <phrase>transfer learning</phrase>. In the future, RDL may enable combined supervised training of <phrase>deep neural networks</phrase> using task constraints (e.g., images and category <phrase>labels</phrase>) and constraints from <phrase>brain</phrase>-activity measurements, so as to build models that replicate the internal representational spaces of biological brains.
Influencing Individual Perceptions of Deep Level Diversity in Virtual Learning Teams (<phrase>VLT</phrase>) Two emerging trends are impacting both <phrase>universities</phrase> and corporate training programs: virtual learning and diversity. Virtual Learning Teams (<phrase>VLT</phrase>) learn by solving intellectual and <phrase>cognitive</phrase> tasks that require the sharing and utilization of <phrase>information</phrase> to achieve learning objectives. <phrase>VLT</phrase> members are separated by spatial distance and do not have the same opportunities to communicate as do traditional learning teams. Compounding the operational challenge of establishing and maintaining <phrase>VLT</phrase> <phrase>member</phrase> relationships is the fact that their members are often non-traditional students who tend to be diverse in terms of demographics, work experience, and beliefs compared to traditional teams. Naturally, this creates challenges in facilitating <phrase>social integration</phrase>. One way in which <phrase>social integration</phrase> may be enhanced is through leveraging the very <phrase>communication</phrase> <phrase>technology</phrase> that VLTs are reliant upon. <phrase>Results</phrase> of our study provide insight into how <phrase>communication</phrase> <phrase>technology</phrase> can support learning.
Visual Closed-Loop Control for Pouring Liquids Pouring a specific amount of <phrase>liquid</phrase> is a <phrase>challenging task</phrase>. In this <phrase>paper</phrase> we develop methods for <phrase>robots</phrase> to use visual <phrase>feedback</phrase> to perform closed-loop control for pouring liquids. We propose both a <phrase>model</phrase>-based and a <phrase>model</phrase>-<phrase>free</phrase> method utilizing <phrase>deep learning</phrase> for estimating the volume of <phrase>liquid</phrase> in a container. Our <phrase>results</phrase> show that the <phrase>model</phrase>-<phrase>free</phrase> method is better able to estimate the volume. We combine this with a simple <phrase>PID controller</phrase> to pour specific amounts of <phrase>liquid</phrase>, and show that the <phrase>robot</phrase> is able to achieve an <phrase>average</phrase> 38ml <phrase>deviation</phrase> from the <phrase>target</phrase> amount. To our <phrase>knowledge</phrase>, this is the first use of raw visual <phrase>feedback</phrase> to pour liquids in <phrase>robotics</phrase>. I. INTRODUCTION The last years have seen dramatic improvements in robotic capabilities relevant to household tasks such as putting items into a <phrase>dishwasher</phrase> [1], folding and ironing <phrase>clothing</phrase> [2], [3], and cleaning surfaces [4]. So far, however, <phrase>robots</phrase> have not been able to robustly perform household tasks involving liquids, such as pouring a <phrase>glass</phrase> of <phrase>water</phrase>. Solving such tasks requires both <phrase>robust control</phrase> and detection of <phrase>liquid</phrase> during the pouring operation. Humans often are not very accurate at this, requiring specialized containers to measure a specific amount of <phrase>liquid</phrase>. Instead people often use vague, relative terms such as " Pour me a half cup of <phrase>coffee</phrase> " or " Just a little, please. " While there has been recent success in <phrase>robotics</phrase> on controlling a manipulator to pour liquids simulated by small balls [5] and on detecting liquids using <phrase>optical flow</phrase> or <phrase>deep learning</phrase> [6], [7], the task of pouring certain amounts of actual liquids has not been addressed. In this <phrase>paper</phrase>, we introduce a framework that enables <phrase>robots</phrase> to robustly pour specific amounts of a <phrase>liquid</phrase> into containers typically found in a home environment, such as <phrase>coffee</phrase> mugs, cups, glasses, or <phrase>bowls</phrase>. We achieve this in the most <phrase>general</phrase> setting, without requiring specialized hardware, such as highly accurate force sensors for measuring the amount of <phrase>liquid</phrase> held by a <phrase>robot</phrase> manipulator, scales placed under the <phrase>target</phrase> container, or sensors designed for detecting liquids. However, while we avoid requiring specialized environmental augmentation, our investigation is on how accurate a <phrase>robot</phrase> could pour under relatively controlled conditions, such as having been able to <phrase>train</phrase> on the <phrase>target</phrase> containers. The intuition behind our approach is based on the insight that people strongly rely on visual cues when pouring liquids. 
<phrase>Problem-based Learning</phrase> of Theoretical <phrase>Computer Science</phrase> In this <phrase>paper</phrase>, we <phrase>report</phrase> our first experiment in teaching the theory of <phrase>computability</phrase> in the problem-based way. As far as we know, this is the first experiment of applying the problem-based method to a purely theoretical course of <phrase>computer science</phrase>. Performing the course consisted of three parts: First, the new subjects were learnt according to the <phrase>classical</phrase> seven step method, which contains both individual and group work, and problem reports were written. Second, the students participated in a traditional exercise session, in which the new techniques were practised in details. And third, the students kept a learning diary, in which they processed the subjects further, tried to construct an overall schema of things learnt, and supervised their own learning. The <phrase>results</phrase> were really successful: the students committed themselves well and the drop out percentage was very small; they achieved very <phrase>deep understanding</phrase> of the subjects measured by their grades and quality of learning diaries; the experience was enjoyable for both the students and the teachers; and finally, the method supported different kinds of learners very well.
<phrase>Book</phrase> Review Handbook for <phrase>Language</phrase> Engineers The reader learns from this book's introduction (written by <phrase>Ali</phrase> Farghaly) that the objective of the <phrase>book</phrase> is to " equip <phrase>linguists</phrase> embarking on <phrase>NLP</phrase> assignments. " The introduction also explains why <phrase>language</phrase> engineers are needed and summarizes the contents of each chapter. " Domain Analysis and Representation " by Farghaly and Bruce Hedin offers a good discussion of the importance of domain analysis in <phrase>natural language processing</phrase> (<phrase>NLP</phrase>). It provides a helpful background on the notion of sublanguages and correctly notes that distinctions between the different domains can be blurred, as domains often overlap. The chapter would have been more convincing if there were further examples of <phrase>NLP</phrase> applications that benefit from narrowing down the domain of their operation. The chapter also discusses the analysis of domain into topics. Section 2.4.4 covers statistical approaches to classification, but no references or further <phrase>reading</phrase> pointers are given. " The <phrase>Language</phrase> of the <phrase>Internet</phrase> " by Naomi <phrase>Baron</phrase> gives an easy-to-read and useful chronological overview of the developments on the <phrase>Internet</phrase>. It provides <phrase>information</phrase> about a number of technologies that are used on the <phrase>Internet</phrase>, comments on the changing styles of <phrase>natural language</phrase> use, and briefly overviews Web <phrase>markup</phrase> and <phrase>programming languages</phrase> and the <phrase>Semantic Web</phrase>. At the same time, I found this chapter too <phrase>general</phrase>. It would have been useful if the chapter had <phrase>covered</phrase> specific new <phrase>NLP</phrase> applications that are relevant to the <phrase>Internet</phrase>, such as <phrase>question answering</phrase>, and had <phrase>covered</phrase> in greater detail low-quality <phrase>machine translation</phrase> for <phrase>e</phrase>-<phrase>mail</phrase> and chat or text categorization of <phrase>Web pages</phrase>. " <phrase>Grammar</phrase> Writing, Testing and Evaluation " by Miriam Butt and Tracy Holloway <phrase>King</phrase> provides a very good and accessible historical and <phrase>linguistic</phrase> account of grammars and <phrase>parsing</phrase> in <phrase>NLP</phrase>. It covers deep and shallow <phrase>parsing</phrase> and associated techniques as well as testing and evaluation of grammars and parsers. Morphological analyzers and part-of-speech taggers are briefly outlined too. A good practical point is the section on documentation of <phrase>grammar</phrase> writing. " <phrase>Ontologies</phrase> " by Natalya Noy is a concise and plainly written introduction to the topic of <phrase>ontologies</phrase> and their development. It clarifies key terms in the <phrase>ontology</phrase> development <phrase>jargon</phrase> and includes an overview of <phrase>major</phrase> <phrase>ontologies</phrase>, <phrase>ontology</phrase> <phrase>libraries</phrase>, and
Supporting <phrase>Privacy</phrase> in <phrase>E</phrase>-Learning with <phrase>Semantic</phrase> Streams The goal of the <phrase>semantic web</phrase> is to facilitate the exchange of meaningful <phrase>information</phrase> in a form that is easy for machines to process. The goal of an <phrase>e</phrase>-learning system is to support the learning process by providing meaningful <phrase>information</phrase> for students in a form that best suits them. Natural synergies between these two areas have <phrase>led</phrase> to a surge in the feasibility of user modelling and <phrase>intelligent tutoring</phrase> systems, where computer systems can understand a learner, and help guide them through the learning process. The question arises then; with deep meaningful <phrase>information</phrase> being shared about learners, who is protecting the <phrase>privacy rights</phrase> and desires of the learner? This <phrase>paper</phrase> provides an investigation of this question, and outlines approaches that are being undertaken to integrate <phrase>privacy</phrase> into our current <phrase>semantic web</phrase> <phrase>e</phrase>-learning framework.
Input Convex <phrase>Neural Networks</phrase> This <phrase>paper</phrase> presents the input convex <phrase>neural network</phrase> <phrase>architecture</phrase>. These are scalar-valued (potentially deep) <phrase>neural networks</phrase> with constraints on the network parameters such that the output of the network is a <phrase>convex function</phrase> of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, <phrase>data</phrase> imputation, <phrase>reinforcement learning</phrase>, and others. In this <phrase>paper</phrase> we lay the <phrase>basic</phrase> groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing <phrase>neural network</phrase> architectures can be made input-convex with only minor modification, and develop specialized optimization <phrase>algorithms</phrase> tailored to this setting. Finally, we highlight the performance of the methods on multi-<phrase>label</phrase> prediction, image completion, and <phrase>reinforcement learning</phrase> problems, where we show improvement over the existing <phrase>state</phrase> of the <phrase>art</phrase> in many cases.
Assessment for Learning: Planning for <phrase>Professional</phrase> Development Assessment has the power to transform both learning and teaching because it is so integrally linked to <phrase>motivation</phrase> and learning. The increased attention being paid to standards and their assessment has created a new opportunity to support learning and improve classroom practices. Providing <phrase>long</phrase>-term, differentiated, <phrase>multi-dimensional</phrase> and flexible approaches to adult learning for all members of the <phrase>school</phrase> <phrase>community</phrase> is essential and challenging. For many of us seizing this opportunity to support <phrase>student</phrase> learning requires rethinking adult learning and using assessment to guide our <phrase>professional</phrase> development work. In an assessment for <phrase>learning environment</phrase>, rather than something that happens at the end of the learning, assessment is used to support and inform learning, build self-confidence, and capacity for success. (Stiggins, 2001). Assessment for learning is ongoing, and requires deep involvement on the part of the learner in clarifying outcomes, monitoring ongoing learning, collecting evidence and presenting evidence of learning to others. Understanding the difference between assessment of learning and assessment for learning is key. Assessment that directly supports learning has five key characteristics: learners are involved so a shared <phrase>language</phrase> and understanding of learning is developed, learners self-assess and receive specific, descriptive <phrase>feedback</phrase> about the learning during the learning, learners collect, organize, and communicate evidence of their learning with others, instruction is adjusted in response to ongoing assessment <phrase>information</phrase>, and a safe <phrase>learning environment</phrase> invites <phrase>risk</phrase> taking, encourages learning from mistakes, enables focused <phrase>goal setting</phrase>, and supports thoughtful learning. Why use assessment for learning as the organizing core of adult learning? Experience and <phrase>research</phrase> tells us that adults (like <phrase>student</phrase> learners) have more ownership, are more motivated 1 Authors note: The authors would like to thank Kathy Busick for her thoughtful comments and suggestions on earlier drafts.
Events are Not Simple: Identity, Non-Identity, and Quasi-Identity 1 Despite considerable theoretical and computational work on coreference, deciding when two entities or events are identical is very difficult. In a project to build corpora containing corefer-ence links between events, we have identified three levels of event identity (full, partial, and none). Event coreference annotation on two corpora was performed to validate the findings. 1 The Problem of Identity Last year we had HLT in <phrase>Montreal</phrase>, and this year we did it in <phrase>Atlanta</phrase>. Does the " did it " refer to the same conference or a different one? The two conferences are not identical , of course, but they are also not totally unre-latedelse the " did it " would not be interpretable. When creating text, we treat instances of entities and events as if they are fixed, well-described, and well-understood. When we say " that <phrase>boat</phrase> over there " or " Mary's <phrase>wedding</phrase> next month " , we assume the reader creates a <phrase>mental representation</phrase> of the <phrase>referent</phrase>, and we proceed to refer to it without further thought. However, as has been often noted in theoretical studies of <phrase>semantics</phrase>, this assumption is very prob-Entities and (even more so) events are complex composite phenomena in the world, and they undergo change. Since nobody has complete <phrase>knowledge</phrase>, the <phrase>au</phrase>-thor's <phrase>mental image</phrase> of the entity or event in question might differ from the reader's, and from the truth. Specifically, the properties the <phrase>author</phrase> assumes for the event or entity might not be the ones the reader assumes. This difference has deep consequences for the treatment of the <phrase>semantic</phrase> meaning of a text. In particular, it fundamentally affects how one must perform coreference among entities or events. Humphreys et al., 1997). Determining when two event mentions in text corefer is, however, an unsolved problem 2. Past work in <phrase>NLP</phrase> has avoided some of the more complex problems by considering only certain types of coreference, or by simply ignoring the <phrase>major</phrase> problems. The <phrase>results</phrase> have been partial, or inconsistent, annotations. In this <phrase>paper</phrase> we describe our approach to the problem of coreference among events. In <phrase>order</phrase> to build a corpus containing event coreference links that is annotated with <phrase>high</phrase> enough inter-annotator agreement to be useful for <phrase>machine learning</phrase>, it has <phrase>proven</phrase> necessary to create a <phrase>model</phrase> of event identity that is more elaborate than is usually assumed in the <phrase>NLP</phrase> <phrase>literature</phrase>, and to formulate quite specific definitions for its central concepts. 2 
<phrase>Benchmarking</phrase> User Performance by Using <phrase>Virtual Reality</phrase> for Task-based Training <phrase>Conveyor belts</phrase> have a <phrase>high</phrase> accident and fatality rate associated with them because of the dangerous environment their constantly moving parts create. Because of this <phrase>high</phrase> fatality rate, different methods are being considered to improve current <phrase>safety</phrase> training methods. By looking at the principles of <phrase>cognitive</phrase> learning and what makes a computer-based training program successful, a <phrase>safety</phrase> training program using <phrase>virtual reality</phrase> (VR) is being proposed. The training program structure includes four steps in creating a <phrase>comprehensive</phrase> two phase training program that will <phrase>train</phrase> personnel on the required operational processes in an instructional-based phase and then <phrase>test</phrase> their abilities through an interactive task-based session that tracks the user's progress and choices, tallies points based on corrective actions taken, and gives immediate <phrase>feedback</phrase> and consequences to the user's actions. This <phrase>paper</phrase> focus on the task-based phase of the <phrase>prototype</phrase> development and what steps are taken in creating an individual exercise. One example exercise is described in detail from choosing the material that is tested, implementing the animations and coding using Right Hemisphere's Deep Creator and <phrase>LISP</phrase> coding, implementing the tracking methods, and the output file that has been designed to keep <phrase>track</phrase> of the user's performance.
<phrase>Deep Learning</phrase> and Continuous Representations for <phrase>Natural Language Processing</phrase> Introduction <phrase>Deep learning</phrase> techniques have demonstrated tremendous success in the speech and <phrase>language</phrase> processing <phrase>community</phrase> in recent years, establishing new <phrase>state</phrase>-of-the-<phrase>art</phrase> performance in <phrase>speech recognition</phrase>, <phrase>language</phrase> modeling, and have shown great potential for many other <phrase>natural language processing</phrase> tasks. The focus of this tutorial is to provide an extensive overview on recent <phrase>deep learning</phrase> approaches to problems in <phrase>language</phrase> or text processing, with particular emphasis on important <phrase>real-world</phrase> applications including <phrase>language</phrase> understanding, <phrase>semantic</phrase> representation modeling, <phrase>question answering</phrase> and <phrase>semantic</phrase> <phrase>parsing</phrase>, etc. In this tutorial, we will first survey the latest <phrase>deep learning</phrase> <phrase>technology</phrase>, presenting both theoretical and practical perspectives that are most relevant to our topic. We plan to <phrase>cover</phrase> common methods of <phrase>deep neural networks</phrase> and more advanced methods of recurrent, recursive, stacking and <phrase>convolutional networks</phrase>. In addition, we will introduce <phrase>recently proposed</phrase> continuous-space representations for both <phrase>semantic</phrase> word embedding and <phrase>knowledge base</phrase> embedding, which are modeled by either matrix/<phrase>tensor</phrase> decomposition or <phrase>neural networks</phrase>. Next, we will review <phrase>general</phrase> problems and tasks in text/<phrase>language</phrase> processing, and underline the distinct properties that differentiate <phrase>language</phrase> processing from other tasks such as speech and image <phrase>object recognition</phrase>. More importantly, we highlight the <phrase>general</phrase> issues of <phrase>natural language processing</phrase>, and elaborate on how new <phrase>deep learning</phrase> technologies are proposed and fundamentally address these issues. We then place particular emphasis on several important applications, including (1) <phrase>machine translation</phrase>, (2) <phrase>semantic</phrase> <phrase>information retrieval</phrase> and (3) <phrase>semantic</phrase> <phrase>parsing</phrase> and <phrase>question answering</phrase>. For each task, we will discuss what particular architectures of <phrase>deep learning</phrase> models are suitable given the <phrase>nature</phrase> of the task, and how learning can be performed efficiently and effectively using <phrase>end-to-end</phrase> optimization strategies. Background: A review of <phrase>deep learning</phrase> theory and applications in relevant fields Advanced architectures for <phrase>modeling language</phrase> structure Common problems and concepts in <phrase>language</phrase> processing: Why <phrase>deep learning</phrase> is needed
Percolations: interdisciplinary <phrase>innovation</phrase> may invoke carnivorous colleagues <phrase>Passions</phrase> run deep in the most unexpected places. A <phrase>Praying Mantis</phrase> bites its partner's head off after sex, and the <phrase>Portia</phrase> <phrase>spider</phrase> lulls a <phrase>fellow</phrase> <phrase>spider</phrase> into complacency, then lunges at it and eats it. Whether appalled or fascinated, we understand intellectually that these bizarre behaviors must contribute to <phrase>species</phrase> survival. On the other hand, the intended victims sometimes learn to deflect their aggressor and yet have as much "fun" as they want. That too contributes to <phrase>species</phrase> survival because the resilient <phrase>spider</phrase> or <phrase>mantis</phrase> achieves far more satisfaction and future <phrase>productivity</phrase> than either their aggressor or the deceased. Believe it or not, after attending ITiCSE, I see the connection between these behaviors and innovative <phrase>computing</phrase> <phrase>education</phrase>.
Simulating the shaping of the fastigial deep nuclear <phrase>saccade</phrase> command by <phrase>cerebellar</phrase> Purkinje cells Early <phrase>lesion</phrase> and <phrase>physiological</phrase> studies established the key contributions of the <phrase>cerebellar cortex</phrase> and fastigial deep nuclei in maintaining the accuracy of saccades. Recent evidence has demonstrated that fastigial oculomotor <phrase>region</phrase> cells (FORCs) provide commands that are critical both for driving and braking saccades. Modeling studies have largely ignored the mechanisms by which the FORC activity patterns, and those of the Purkinje cells (PCs) that inhibit them, are <phrase>produced</phrase> by the mossy fiber (<phrase>MF</phrase>) inputs common to both. We have created a <phrase>hybrid</phrase> network of integrate-and-fire and summation units to <phrase>model</phrase> the circuitry between PCs, FORCs, and MFs that can account for all observed <phrase>PC</phrase> and FORC activity patterns. The <phrase>model</phrase> demonstrates that a crucial component of FORC activity may be due to the rebound <phrase>depolarization</phrase> intrinsic to FORC <phrase>neurons</phrase> that, like the <phrase>MF</phrase>-driven activity of FORCs, is also shaped by <phrase>PC</phrase> inhibition and <phrase>disinhibition</phrase>. The <phrase>model</phrase> further demonstrates that the shaping of the FORC <phrase>saccade</phrase> command by PCs can be adaptively modified through plausible learning rules based on <phrase>cerebellar</phrase> <phrase>long-term depression</phrase> (LTD) and <phrase>long-term potentiation</phrase> (<phrase>LTP</phrase>), which are guided by climbing fiber (CF) input to PCs that realistically indicates only the direction (but not the <phrase>magnitude</phrase>) of <phrase>saccade</phrase> error. These modeling <phrase>results</phrase> provide new insights into the <phrase>adaptive control</phrase> by the <phrase>cerebellum</phrase> of the deep nuclear <phrase>saccade</phrase> command.
<phrase>Large-scale</phrase> deep <phrase>unsupervised learning</phrase> using graphics processors The promise of <phrase>unsupervised learning</phrase> methods lies in their potential to use vast amounts of <phrase>unlabeled data</phrase> to learn complex, highly nonlinear models with millions of <phrase>free</phrase> parameters. We consider two well-known <phrase>unsupervised learning</phrase> models, <phrase>deep belief</phrase> networks (DBNs) and <phrase>sparse coding</phrase>, that have recently been applied to a flurry of <phrase>machine learning</phrase> applications (Hinton &amp; Salakhutdinov, 2006; <phrase>Raina</phrase> et al., 2007). Unfortunately, current <phrase>learning algorithms</phrase> for both models are too slow for <phrase>large-scale</phrase> applications, forcing researchers to focus on smaller-scale models, or to use fewer <phrase>training examples</phrase>. In this <phrase>paper</phrase>, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep <phrase>unsupervised learning</phrase> methods. We develop <phrase>general</phrase> principles for massively parallelizing <phrase>unsupervised learning</phrase> tasks using graphics processors. We show that these principles can be applied to successfully scaling up <phrase>learning algorithms</phrase> for both DBNs and <phrase>sparse coding</phrase>. Our implementation of DBN learning is up to 70 times faster than a dual-core <phrase>CPU</phrase> implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million <phrase>free</phrase> parameters from several weeks to around a <phrase>single</phrase> day. For <phrase>sparse coding</phrase>, we develop a simple, inherently <phrase>parallel algorithm</phrase>, that leads to a 5 to 15-fold speedup over previous methods.
Learning Grimaces by Watching <phrase>TV</phrase> Differently from <phrase>computer vision</phrase> systems which require explicit supervision, humans can learn facial expressions by observing people in their environment. In this <phrase>paper</phrase>, we look at how similar capabilities could be developed in <phrase>machine vision</phrase>. As a <phrase>starting point</phrase>, we consider the problem of relating facial expressions to objectively-measurable events occurring in videos. In particular, we consider a <phrase>gameshow</phrase> in which contestants <phrase>play</phrase> to win significant sums of <phrase>money</phrase>. We extract events affecting the <phrase>game</phrase> and corresponding facial expressions objectively and automatically from the videos, obtaining large quantities of labelled <phrase>data</phrase> for our study. We also develop, using benchmarks such as FER and SFEW 2.0, <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>deep neural networks</phrase> for <phrase>facial expression</phrase> recognition , showing that <phrase>pre-training</phrase> on face verification <phrase>data</phrase> can be highly beneficial for this task. Then, we extend these models to use facial expressions to predict events in videos and learn nameable expressions from them. The dataset and <phrase>emotion</phrase> recognition models are available at
Automatic <phrase>knowledge base</phrase> refinement: learning from examples and deep <phrase>knowledge</phrase> in <phrase>rheumatology</phrase> MESICAR is a second generation expert system which contains very <phrase>general</phrase> descriptions of rheumatological disorders in the primary <phrase>medical</phrase> care field. With the help of a detailed hierarchical description of the <phrase>human</phrase> <phrase>anatomy</phrase> the system is able to support diagnostic decisions. The <phrase>paper</phrase> describes how <phrase>machine learning</phrase> techniques are used to automatically construct more specific <phrase>disease</phrase> descriptions for common, frequently occurring cases. The system MESICAR-LEARN implements a learning method which integrates analytical and empirical learning techniques. Cases diagnosed by MESICAR form the <phrase>training examples</phrase>, and MESICAR's <phrase>knowledge base</phrase> is used as <phrase>domain theory</phrase>. The learned concepts are integrated into a hierarchy of <phrase>disease</phrase> descriptions. They support efficient and fast reasoning on common cases in addition to the <phrase>general</phrase> diagnostic support afforded by MESICAR's deep <phrase>knowledge</phrase>.
Generalization of TORCS <phrase>car</phrase> racing controllers with <phrase>artificial neural networks</phrase> and <phrase>linear regression</phrase> analysis Designing controllers for simulated cars is a <phrase>challenging task</phrase> because there are numerous <phrase>sensor</phrase> inputs and a lot of actuators to be controlled. Although it is possible to use domain-expert <phrase>knowledge</phrase> on the <phrase>car</phrase> racing, it is not trivial to represent the <phrase>knowledge</phrase> into controllers and tune the parameters for them. Therefore, it is natural to adopt <phrase>machine learning</phrase> approach into the <phrase>knowledge</phrase>-oriented controllers to enhance their performance and minimize tedious parameter tunings. In this <phrase>paper</phrase>, we try to enhance our own <phrase>heuristic</phrase> controllers using <phrase>machine learning</phrase> models which decide the appropriate parameters of the <phrase>heuristic</phrase> controllers given the current sensory inputs. At first, we predict the desired speed only using the equations derived by the <phrase>linear regression</phrase> analysis for the both curved and straight <phrase>track</phrase> segments. Because the decision on reducing speed before the corner is more complex than the one in the straight line and the corners, it is necessary to use a non-<phrase>linear model</phrase> such as <phrase>artificial neural networks</phrase>. Secondly, the <phrase>linear regression</phrase> and <phrase>artificial neural networks</phrase> are specialized to predict desired speed in different situations. <phrase>Experimental</phrase> <phrase>results</phrase> on TORCS-based <phrase>car</phrase> racing simulations show that the combination of the two <phrase>machine learning</phrase> <phrase>algorithms</phrase> with the <phrase>heuristic</phrase> outperforms other alternatives (<phrase>heuristic</phrase> only, <phrase>heuristic</phrase> <phrase>linear regression</phrase>, and <phrase>heuristic</phrase> <phrase>artificial neural networks</phrase>). <phrase>Game</phrase> is one of the most important problems used in <phrase>artificial intelligence</phrase> <phrase>research</phrase> because they provide with simple and abstract models of <phrase>real world</phrase>. For example, iterated <phrase>prisoner's dilemma</phrase> (IPD) <phrase>games</phrase> have been widely used to find good decision strategies in <phrase>economy</phrase>, <phrase>politics</phrase>, and foreign relations. In the <phrase>game</phrase>, there are only two choices (cooperation or defection) but it successfully simulates a lot of realistic dilemma situations. Since the beginning of the <phrase>artificial intelligence</phrase>, several <phrase>board games</phrase> have been widely used as a <phrase>research</phrase> platform and finally, some of them are nearly solved. For example, <phrase>Deep Blue</phrase> team by <phrase>IBM</phrase> defeated the <phrase>world chess champion</phrase> with a specially designed super computer. In the <phrase>board games</phrase>, each player has finite number of decision choices and there is no hidden <phrase>information</phrase> between players. Recently, the <phrase>game</phrase> <phrase>AI</phrase> <phrase>research</phrase> has expanded their territories to other <phrase>game</phrase> genres like real-time simulations, <phrase>video games</phrase>, and <phrase>role-playing games</phrase> [1]. They're quite different with the traditional <phrase>board games</phrase>. It is not a <phrase>turn-based</phrase> <phrase>game</phrase> and each player has to respond to the <phrase>game</phrase> as quickly as possible. Unlike the <phrase>board games</phrase>, players have to decide 
<phrase>Deep Generative Models</phrase> for Modeling Animate Motion Introduction <phrase>Recent advances</phrase> in <phrase>motion capture</phrase> <phrase>technology</phrase> have fueled interest in the synthesis and analysis of complex animate motion for <phrase>animation</phrase> and tracking. In this work we focus on <phrase>model</phrase> driven analysis and synthesis but avoid the complexities involved in imposing <phrase>physics</phrase>-based constraints [1], and the storage requirements involved in concatenating parts of training sequences [2], relying instead on a " pure " learning approach in which all the <phrase>knowledge</phrase> in the <phrase>model</phrase> comes from the <phrase>data</phrase>. <phrase>Data</phrase> from modern <phrase>motion capture</phrase> systems is <phrase>high</phrase>-dimensional and contains complex non-linear relationships between the components of the observation <phrase>vector</phrase>, which usually represent joint angles with respect to some skeletal structure. <phrase>Hidden Markov models</phrase> cannot <phrase>model</phrase> such <phrase>data</phrase> efficiently because they rely on a <phrase>single</phrase>, discrete K-<phrase>state</phrase> multinomial to represent the <phrase>history</phrase> of the <phrase>time series</phrase>. To <phrase>model</phrase> N <phrase>bits</phrase> of <phrase>information</phrase> about the past <phrase>history</phrase> they require 2 N hidden states. To avoid this exponential explosion, we need a <phrase>model</phrase> with distributed (i.e. componential) hidden <phrase>state</phrase> that has a representational capacity which is linear in the number of components. Linear <phrase>dynamical systems</phrase> satisfy this requirement, but they cannot <phrase>model</phrase> the complex non-linear dynamics created by the non-linear properties of muscles, contact forces of the foot on the ground and myriad other factors.
Building Machines That Learn and Think like People Building Machines That Learn and Think like People Recent progress in <phrase>artificial intelligence</phrase> (<phrase>AI</phrase>) has renewed interest in building systems that learn and think like people. Many advances have come from using <phrase>deep neural networks</phrase> trained <phrase>end-to-end</phrase> in tasks such as <phrase>object recognition</phrase>, <phrase>video games</phrase>, and <phrase>board games</phrase>, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from <phrase>human intelligence</phrase> in crucial ways. We review progress in <phrase>cognitive science</phrase> suggesting that truly <phrase>human</phrase>-like learning and <phrase>thinking machines</phrase> will have to reach beyond current <phrase>engineering</phrase> trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving <phrase>pattern recognition</phrase> problems; (b) ground learning in intuitive theories of <phrase>physics</phrase> and <phrase>psychology</phrase>, to support and enrich the <phrase>knowledge</phrase> that is learned; and (c) <phrase>harness</phrase> compositionality and learning-to-learn to rapidly acquire and generalize <phrase>knowledge</phrase> to new tasks and situations. We suggest <phrase>concrete</phrase> challenges and promising routes towards these goals that can combine the strengths of recent <phrase>neural network</phrase> advances with more structured <phrase>cognitive</phrase> models. Abstract Recent progress in <phrase>artificial intelligence</phrase> (<phrase>AI</phrase>) has renewed interest in building systems that learn and think like people. Many advances have come from using <phrase>deep neural networks</phrase> trained <phrase>end-to-end</phrase> in tasks such as <phrase>object recognition</phrase>, <phrase>video games</phrase>, and <phrase>board games</phrase>, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from <phrase>human intelligence</phrase> in crucial ways. We review progress in <phrase>cognitive science</phrase> suggesting that truly <phrase>human</phrase>-like learning and <phrase>thinking machines</phrase> will have to reach beyond current <phrase>engineering</phrase> trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving <phrase>pattern recognition</phrase> problems; (b) ground learning in intuitive theories of <phrase>physics</phrase> and <phrase>psychology</phrase>, to support and enrich the <phrase>knowledge</phrase> that is learned; and (c) <phrase>harness</phrase> compositionality and learning-to-learn to rapidly acquire and generalize <phrase>knowledge</phrase> to new tasks and situations. We suggest <phrase>concrete</phrase> challenges and promising routes towards these goals that can combine the strengths of recent <phrase>neural network</phrase> advances with more structured <phrase>cognitive</phrase> models.
Facilitating <phrase>deep learning</phrase> through self-explanation in an <phrase>open-ended</phrase> domain Self-explanation has been used successfully in teaching <phrase>Mathematics</phrase> and <phrase>Physics</phrase> to facilitate <phrase>deep learning</phrase>. We are interested in investigating whether self-explanation can be used in an <phrase>open-ended</phrase>, ill-structured domain. For this purpose, we enhanced <phrase>KERMIT</phrase>, an <phrase>intelligent tutoring</phrase> system that teaches conceptual <phrase>database design</phrase>. The resulting system, <phrase>KERMIT</phrase>-SE, supports self-explanation by engaging students in tutorial dialogues when their solutions are erroneous. The <phrase>results</phrase> of an evaluation study indicate that self-explanation leads to improved performance in both conceptual and <phrase>procedural knowledge</phrase>.
Latent <phrase>Regression</phrase> <phrase>Bayesian Network</phrase> for <phrase>Data</phrase> Representation Deep <phrase>directed</phrase> <phrase>generative models</phrase> have attracted much attention recently due to their expressive representation power and the ability of ancestral sampling. One <phrase>major</phrase> difficulty of learning <phrase>directed</phrase> models with many <phrase>latent variables</phrase> is the intractable inference. To address this problem, most existing <phrase>algorithms</phrase> make assumptions to render the <phrase>latent variables</phrase> <phrase>independent</phrase> of each other, either by designing specific priors, or by approximating the true posterior using a factorized distribution. We believe the correlations among <phrase>latent variables</phrase> are crucial for faithful <phrase>data</phrase> representation. Driven by this idea, we propose an inference method based on the conditional pseudo-likelihood that preserves the dependencies among the <phrase>latent variables</phrase>. For learning, we propose to employ the hard Expectation Max-imization (EM) <phrase>algorithm</phrase>, which avoids the intractability of the traditional EM by max-out instead of sum-out to compute the <phrase>data</phrase> likelihood. Qualitative and quantitative evaluations of our <phrase>model</phrase> against <phrase>state</phrase> of the <phrase>art</phrase> deep models on <phrase>benchmark datasets</phrase> demonstrate the effectiveness of the <phrase>proposed algorithm</phrase> in <phrase>data</phrase> representation and <phrase>reconstruction</phrase>.
Wyner-ziv <phrase>Video</phrase> Coding Using Hadamard Transform and <phrase>Deep Learning</phrase> Access to the published version may require subscription. AbstractPredictive schemes are current standards of <phrase>video</phrase> coding. Unfortunately they do not apply well for <phrase>lightweight</phrase> devices such as <phrase>mobile phones</phrase>. The <phrase>high</phrase> encoding complexity is the bottleneck of the Quality of Experience (QoE) of a <phrase>video</phrase> conversation between <phrase>mobile phones</phrase>. A considerable amount of <phrase>research</phrase> has been conducted towards tackling that bottleneck. Most of the schemes use the so-called Wyner-Ziv <phrase>Video</phrase> Coding <phrase>Paradigm</phrase>, with <phrase>results</phrase> still not comparable to those of predictive coding. This <phrase>paper</phrase> shows a novel approach for Wyner-Ziv <phrase>video</phrase> compression. It is based on the <phrase>Reinforcement Learning</phrase> and Hadamard Transform. Our Scheme shows very <phrase>promising results</phrase>.
Microsyntactic Phenomena as a <phrase>Computational Linguistics</phrase> Issue Microsyntactic <phrase>linguistic</phrase> units, such as <phrase>syntactic</phrase> <phrase>idioms</phrase> and non-standard <phrase>syntactic</phrase> constructions, are poorly represented in <phrase>linguistic</phrase> resources, mostly because the former are elements occupying an intermediate position between the <phrase>lexicon</phrase> and the <phrase>grammar</phrase> and the latter are too specific to be routinely tackled by <phrase>general</phrase> grammars. Consequently, many such units produce substantial gaps in systems intended to solve sophisticated <phrase>computational linguistics</phrase> tasks, such as <phrase>parsing</phrase>, deep <phrase>semantic</phrase> analysis, <phrase>question answering</phrase>, <phrase>machine translation</phrase>, or text generation. They also present obstacles for applying advanced techniques to these tasks, such as <phrase>machine learning</phrase>. The <phrase>paper</phrase> discusses an approach aimed at bridging such gaps, focusing on the development of monolingual and multilingual corpora where microsyntactic units are to be tagged.
Recognising Textual Entailment with Robust Logical Inference We use logical inference techniques for recognising textual entailment, with <phrase>theorem proving</phrase> operating on deep <phrase>semantic</phrase> interpretations as the backbone of our system. However, the performance of <phrase>theorem proving</phrase> on its own turns out to be highly dependent on a wide <phrase>range</phrase> of <phrase>background knowledge</phrase>, which is not necessarily included in <phrase>pub</phrase>-lically available <phrase>knowledge</phrase> sources. Therefore, we achieve robustness via two extensions. Firstly, we incorporate <phrase>model</phrase> building, a technique borrowed from <phrase>automated reasoning</phrase>, and show that it is a useful robust method to approximate entailment. Secondly, we use <phrase>machine learning</phrase> to combine these deep <phrase>semantic</phrase> analysis techniques with simple shallow word overlap. The resulting <phrase>hybrid</phrase> <phrase>model</phrase> achieves <phrase>high</phrase> accuracy on the <phrase>RTE</phrase> testset, given the <phrase>state</phrase> of the <phrase>art</phrase>. Our <phrase>results</phrase> also show that the various techniques that we employ perform very differently on some of the subsets of the <phrase>RTE</phrase> corpus and as a result, it is useful to use the <phrase>nature</phrase> of the dataset as a feature.
<phrase>Problem-based Learning</phrase> Meets <phrase>Case-based Reasoning</phrase> The modern <phrase>education</phrase> <phrase>community</phrase> agrees that deep and effective learning is best promoted by situating learning in authentic activity. Many in the <phrase>education</phrase> <phrase>community</phrase> have put in place constructivist classroom practices that put students into situations where they must make hypotheses, collect <phrase>data</phrase>, and determine which <phrase>data</phrase> to use in the process of solving a problem or participating in some kind of realistic analysis or investigation. <phrase>Research</phrase> in <phrase>case-based reasoning</phrase> (CBR), which provides a plausible <phrase>model</phrase> of learning from <phrase>problem solving</phrase> situations, makes suggestions about <phrase>education</phrase> that are consistent with these educational theories and methodologies and which can provide added concreteness and detail. In this <phrase>paper</phrase>, we show how CBR's suggestions can enhance <phrase>problem-based learning</phrase> (<phrase>PBL</phrase>), which is already a well-worked-out and successful approach to <phrase>education</phrase>. The computational accounts CBR provides of reasoning activities, especially of <phrase>knowledge</phrase> access, access to old experiences (cases), and use of old experiences in reasoning, suggest guidelines about materials that should be made available as resources, the kinds of reflection that will promote transfer, qualities of good problems, qualities of the environment in which problems are solved (e.g., affordances for <phrase>feedback</phrase>), and sequencing a <phrase>curriculum</phrase>. The two approaches <phrase>complement</phrase> each other well, and together, we believe they provide a powerful foundation for educational practice in the constructivist tradition, one that at once combines <phrase>lessons learned</phrase> from classroom practice with <phrase>sound</phrase> <phrase>cognitive</phrase> theory.
Integrated <phrase>perception</phrase> with recurrent <phrase>multi-task</phrase> <phrase>neural networks</phrase> Modern discriminative predictors have been shown to match natural intelligences in specific <phrase>perceptual</phrase> tasks in <phrase>image classification</phrase>, object and part detection, boundary extraction, etc. However, a <phrase>major</phrase> advantage that natural intelligences still have is that they work well for all <phrase>perceptual</phrase> problems together, solving them efficiently and coherently in an integrated manner. In <phrase>order</phrase> to capture some of these advantages in machine <phrase>perception</phrase>, we ask two questions: whether <phrase>deep neural networks</phrase> can learn <phrase>universal</phrase> <phrase>image representations</phrase>, useful not only for a <phrase>single</phrase> task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new <phrase>architecture</phrase>, which we call multinet, in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the <phrase>results</phrase> of their analysis in a common shared representation of the <phrase>data</phrase>. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation.
Learning Dynamic Classes of Events using Stacked <phrase>Multilayer Perceptron</phrase> Networks People often use a <phrase>web search engine</phrase> to find <phrase>information</phrase> about events of interest, for example, <phrase>sport</phrase> competitions, <phrase>political</phrase> elections, festivals and <phrase>entertainment</phrase> <phrase>news</phrase>. In this <phrase>paper</phrase>, we study a problem of detecting event-related queries, which is the first step before selecting a suitable time-aware retrieval <phrase>model</phrase>. In <phrase>general</phrase>, event-related <phrase>information</phrase> needs can be observed in query streams through various temporal patterns of user search behavior, e.g., spiky <phrase>peaks</phrase> for popular events, and periodicities for repetitive events. However, it is also common that users search for non-popular events, which may not exhibit temporal variations in query streams, e.g., past events recently occurred, historical events triggered by anniversaries or similar events, and future events anticipated to happen. To address the challenge of detecting dynamic classes of events, we propose a novel <phrase>deep learning</phrase> <phrase>model</phrase> to classify a given query into a predetermined set of multiple event types. Our proposed <phrase>model</phrase>, a Stacked <phrase>Multilayer Perceptron</phrase> (S-MLP) network, consists of <phrase>multi-layer</phrase> <phrase>perceptron</phrase> used as a <phrase>basic</phrase> learning unit. We assemble stacked units to further learn complex relationships between <phrase>neutrons</phrase> in successive layers. To evaluate our proposed <phrase>model</phrase>, we conduct experiments using <phrase>real-world</phrase> queries and a set of manually created <phrase>ground truth</phrase>. Preliminary <phrase>results</phrase> have shown that our proposed <phrase>deep learning</phrase> <phrase>model</phrase> outper-forms the <phrase>state</phrase>-of-the-<phrase>art</phrase> classification models significantly.
On-the-<phrase>Fly</phrase> Learning in a Perpetual Learning Machine Despite the promise of <phrase>brain</phrase>-inspired <phrase>machine learning</phrase>, <phrase>deep neural networks</phrase> (DNN) have frustratingly failed to <phrase>bridge</phrase> the deceptively large gap between learning and <phrase>memory</phrase>. Here, we introduce a Perpetual Learning Machine; a new type of DNN that is capable of <phrase>brain</phrase>-like dynamic 'on the <phrase>fly</phrase>' learning because it exists in a self-supervised <phrase>state</phrase> of Perpetual <phrase>Stochastic Gradient Descent</phrase>. Thus, we provide the means to unify learning and <phrase>memory</phrase> within a <phrase>machine learning</phrase> framework. We also explore the elegant duality of abstraction and synthesis: the Yin and Yang of <phrase>deep learning</phrase>. I. INTRODUCTION It is an embarassing fact that while <phrase>deep neural networks</phrase> (DNN) are frequently compared to the <phrase>brain</phrase>, and even their performance found to be similar in specific static tasks, there remains a critical difference; DNN do not exhibit the <phrase>fluid</phrase> and dynamic learning of the <phrase>brain</phrase> but are static once trained. For example, to add a new class of <phrase>data</phrase> to a trained DNN it is necessary to add the respective new <phrase>training data</phrase> to the pre-existing <phrase>training data</phrase> and retrain (probably from scratch) to account for the new class. By contrast, learning is essentially additive in the <phrase>brain</phrase> if we want to learn a new thing, we do. Thus, whilst there is little doubt that the learning of the <phrase>brain</phrase> and <phrase>machine learning</phrase> are essentially the same, the learning of the <phrase>brain</phrase> involves the emergent phenomenon of <phrase>memory</phrase> which has failed to emerge from <phrase>machine learning</phrase>. Indeed, recent machine-inspired approaches to '<phrase>memory</phrase>' have involved explicit add-on storage facilities [e.g., 1] which explicitly discriminate between learning (training i.e., of weights) and <phrase>memory</phrase> (storage i.e., of <phrase>data</phrase>). Thus, the problem has been brushed under the carpet. In this article, we describe a novel form of <phrase>supervised learning</phrase> <phrase>model</phrase>, which we call a Perpetual Learning Machine, which gives rise to the <phrase>basic</phrase> properties of <phrase>memory</phrase>. Our <phrase>model</phrase> involves two DNNs, one for storage and the other for recall. The storage DNN learns the classes of some training images. The recall DNN learns to synthesise the same images from the same classes. Together, the two networks hold, encoded, the <phrase>training set</phrase>. We then place these pair of DNNs in a self-supervised and <phrase>homeostatic</phrase> <phrase>state</phrase> of Perpetual <phrase>Stochastic Gradient Descent</phrase> (PSGD). During each step of PSGD, a random class is chosen and an image synthesised from the recall DNN. This randomly synthesised image is then used in combination with 
Learning from the <phrase>Mars Rover</phrase> Mission: Scientific Discovery, Learning and <phrase>Memory</phrase> Recognizing and Preserving Learning Purpose <phrase>Knowledge management</phrase> for <phrase>space exploration</phrase> is part of a multi-generational effort. Each mission builds on <phrase>knowledge</phrase> from prior missions, and learning is the first step in <phrase>knowledge</phrase> <phrase>production</phrase>. This <phrase>paper</phrase> uses the <phrase>Mars Exploration Rover</phrase> mission as a site to explore this process. Approach <phrase>Observational study</phrase> and analysis of the work of the MER <phrase>science</phrase> and <phrase>engineering</phrase> team during rover operations, to investigate how learning occurs, how it is recorded, and how these representations might be made available for subsequent missions. Findings Learning occurred in many areas: planning <phrase>science</phrase> strategy, using instruments within the constraints of the <phrase>martian</phrase> environment, the <phrase>Deep Space Network</phrase>, and the mission requirements; using <phrase>software</phrase> tools effectively; and running two teams on <phrase>Mars</phrase> time for three months. This learning is preserved in many ways. Primarily it resides in individual's memories. It is also encoded in stories, procedures, <phrase>programming</phrase> sequences, published reports, and <phrase>lessons learned</phrase> <phrase>databases</phrase>. <phrase>Research</phrase> implications Shows the earliest stages of <phrase>knowledge</phrase> creation in a scientific mission, and demonstrates that <phrase>knowledge management</phrase> must begin with an understanding of <phrase>knowledge</phrase> creation. Practical implications Shows that studying learning and <phrase>knowledge</phrase> creation suggests proactive ways to capture and use <phrase>knowledge</phrase> across multiple missions and generations. Value This <phrase>paper</phrase> provides a unique analysis of the learning process of a scientific space mission, relevant for <phrase>knowledge management</phrase> researchers and designers, as well as demonstrating in detail how new learning occurs in a learning <phrase>organization</phrase>. This <phrase>paper</phrase> describes an <phrase>aspect</phrase> of <phrase>knowledge management</phrase> that is rarely discussed: the creation of new scientific, technical and organizational <phrase>knowledge</phrase> as part of an exploration mission, and the distribution of that <phrase>knowledge</phrase> within and across institutional boundaries. <phrase>Knowledge management</phrase> for space projects and institutions shares many of the same challenges as <phrase>knowledge management</phrase> for any other complex endeavor. Personnel must be trained, policies must be implemented and documented, records and documents must be created and stored. However, a scientific mission into space has several properties that add additional complexity to <phrase>knowledge management</phrase> requirements. First, the entire <phrase>motivation</phrase> for the mission is to develop new <phrase>scientific knowledge</phrase>. This <phrase>knowledge</phrase> <phrase>results</phrase> from learning by individuals, groups, and institutions. Much of the <phrase>literature</phrase> on institutional learning has focused on learning in the service of fundamental corporate goals, rather than on situations where the
<phrase>Group Cognition</phrase> in Online Teams <phrase>Group Cognition</phrase> in Online Teams This chapter represents a disciplinary perspective from <phrase>Computer-Supported</phrase> <phrase>Collaborative Learning</phrase> (CSCL), an interdisciplinary field concerned with leveraging <phrase>technology</phrase> for <phrase>education</phrase> and with analyzing <phrase>cognitive processes</phrase> like learning and meaning making in <phrase>small groups</phrase> of students (Stahl, Koschmann & Suthers, 2006). <phrase>Group cognition</phrase> is a theory developed to support CSCL <phrase>research</phrase> by describing how collaborative groups of students could achieve <phrase>cognitive</phrase> accomplishments together and how that could benefit the individual learning of the participants (Stahl, 2006). It is important to note that while it may very well be the case that a group of students working together manage to <phrase>solve problems</phrase> faster than any of them may have been able to do alone, the most important benefits to <phrase>group cognition</phrase> are the potential for genuinely innovative solutions that go beyond the expertise of any individual in the group, the <phrase>deeper understanding</phrase> that is achieved through the interaction as part of that creative process, and the lasting impact of that <phrase>deep understanding</phrase> that the students take with them when they move on from that interaction, which they may then carry with them as new resources into subsequent group <phrase>problem-solving</phrase> scenarios. <phrase>Group cognition</phrase> can then be seen as what transforms groups into factories for the creation of new <phrase>knowledge</phrase>. The types of problems that have been the focus of exploration within the <phrase>group cognition</phrase> <phrase>paradigm</phrase> have thus not been routine, well-structured problems where every participant can know exactly what their piece of the <phrase>puzzle</phrase> is up front in such a way that the team can <phrase>function</phrase> as a well oiled machine. Many critical group tasks do not fit into well-known and practiced protocolsfor example, low-resource circumstances that may occur in disaster situations, where standard solutions are not an option. In acknowledgement of this, the focus within the <phrase>group-cognition</phrase> <phrase>research</phrase> has been on problems that offer groups the opportunity to explore creatively how those problems can be approached from a <phrase>variety</phrase> of perspectives, where the groups are encouraged to explore unique perspectives. The processes that are the concern of <phrase>group-cognition</phrase> <phrase>research</phrase> have not primarily been those that are related to efficiency of <phrase>problem solving</phrase> (as in some other chapters of this volume). Rather, the focus has been on the pivotal moments where a creative spark or a process of collaborative <phrase>knowledge</phrase> building occurs through interaction. Our fascination has been with identifying the conditions under which these moments of inspiration are triggered, with the goal 
Analyzing <phrase>Drum</phrase> Patterns Using Conditional <phrase>Deep Belief</phrase> Networks We present a system for the <phrase>high</phrase>-level analysis of beat-synchronous <phrase>drum</phrase> patterns to be used as part of a <phrase>comprehensive</phrase> rhythmic understanding system. We use a <phrase>multi-layer</phrase> <phrase>neural network</phrase>, which is greedily <phrase>pre-trained</phrase> <phrase>layer-by-layer</phrase> using restriced <phrase>Boltzmann</phrase> machines (RBMs), in <phrase>order</phrase> to <phrase>model</phrase> the contextual time-<phrase>sequence</phrase> <phrase>information</phrase> of a <phrase>drum</phrase> pattern. For the input layer of the network, we use a conditional RBM, which has been shown to be an effective <phrase>generative model</phrase> of <phrase>multi-dimensional</phrase> sequences. Subsequent layers of the <phrase>neural network</phrase> can be <phrase>pre-trained</phrase> as conditional or standard RBMs in <phrase>order</phrase> to learn <phrase>higher-level</phrase> rhythmic features. We show that this <phrase>model</phrase> can be <phrase>fine-tuned</phrase> in a discriminative manner to make accurate predictions about beat-measure alignment. The <phrase>model</phrase> generalizes well to multiple rhythmic styles due to the distributed <phrase>state-space</phrase> of the <phrase>multi-layer</phrase> <phrase>neural network</phrase>. In addition, the outputs of the discriminative network can serve as posterior <phrase>probabilities</phrase> over beat-alignment <phrase>labels</phrase>. These posterior <phrase>probabilities</phrase> can be used for Viterbi decoding in a <phrase>hidden Markov model</phrase> in <phrase>order</phrase> to maintain temporal continuity of the predicted <phrase>information</phrase>.
Facial <phrase>Action</phrase> Recognition Combining Heterogeneous Features via Multikernel Learning This <phrase>paper</phrase> presents our response to the first international challenge on facial <phrase>emotion</phrase> recognition and analysis. We propose to combine different types of features to automatically detect <phrase>action</phrase> units (AUs) in facial images. We use one multikernel <phrase>support vector machine</phrase> (<phrase>SVM</phrase>) for each <phrase>AU</phrase> we want to detect. The first kernel matrix is computed using local Gabor <phrase>binary</phrase> pattern histograms and a <phrase>histogram</phrase> intersection kernel. The second kernel matrix is computed from active appearance <phrase>model</phrase> coefficients and a <phrase>radial basis function</phrase> kernel. During the training step, we combine these two types of features using the <phrase>recently proposed</phrase> SimpleMKL <phrase>algorithm</phrase>. <phrase>SVM</phrase> outputs are then averaged to exploit temporal <phrase>information</phrase> in the <phrase>sequence</phrase>. To evaluate our system, we perform deep experimentation on several key issues: influence of features and kernel <phrase>function</phrase> in <phrase>histogram</phrase>-based <phrase>SVM</phrase> approaches, influence of spatially <phrase>independent</phrase> <phrase>information</phrase> versus geometric local appearance <phrase>information</phrase> and benefits of combining both, sensitivity to <phrase>training data</phrase>, and interest of temporal context adaptation. We also compare our <phrase>results</phrase> with those of the other participants and try to explain why our method had the best performance during the <phrase>facial expression</phrase> recognition and analysis challenge.
Scene <phrase>character recognition</phrase> using PCANet Scene <phrase>character recognition</phrase> is capturing increasing interests due to the renewed interests in scene text recognition. For scene <phrase>character recognition</phrase>, <phrase>feature representation</phrase> is an important issue, which has been pursued in recent years. In this <phrase>paper</phrase>, we propose to use PCANet (<phrase>principal component analysis</phrase> network) to learn character features for scene <phrase>character recognition</phrase>. PCANet is proposed in [1], and it is a kind of <phrase>deep learning</phrase> framework by cascading <phrase>PCA</phrase> as in <phrase>convolution</phrase> <phrase>neural network</phrase>. In this <phrase>paper</phrase>, we apply PCANet to scene <phrase>character recognition</phrase> by customizing the architectures of PCANet to characters. The <phrase>proposed method</phrase> achieves promising performance on the Chars74K-15 dataset (achieving an accuracy of 64%) and the ICDAR03-CH dataset (achieving an accuracy of 75%), demonstrating the effectiveness of PCANet in scene <phrase>character recognition</phrase>.
The Utility of <phrase>Knowledge Transfer</phrase> for Noisy <phrase>Data</phrase> <phrase>Knowledge transfer</phrase> <phrase>research</phrase> has traditionally focused on features that are relevant for a class of problems. In contrast, our <phrase>research</phrase> focuses on features that are irrelevant. When attempting to acquire a new concept from sensory <phrase>data</phrase>, a learner is exposed to significant volumes of extraneous <phrase>data</phrase>. In <phrase>order</phrase> to use <phrase>knowledge transfer</phrase> for quickly acquiring new concepts within a given class (e.g. learning a new character from the set of characters, a new face from the set of faces, a new vehicle from the set of vehicles, etc.), a learner must know which features are ignorable or it will repeatedly be forced to re-learn them. We have previously demonstrated <phrase>knowledge transfer</phrase> in <phrase>deep convolutional</phrase> <phrase>neural nets</phrase> (DCNN's) (Gutstein, Fuentes, & Freudenthal 2007). In this <phrase>paper</phrase>, we give <phrase>experimental</phrase> <phrase>results</phrase> that demonstrate the increased importance of <phrase>knowledge transfer</phrase> when learning new concepts from noisy <phrase>data</phrase>. Additionally, we exploit the layered <phrase>nature</phrase> of DCNN's to discover more efficient and targeted methods of transfer. We observe that most of the transfer occurs within the 3.2% of weights that are closest to the input.
Applying the <phrase>Model</phrase>-view-controller <phrase>Paradigm</phrase> to <phrase>Adaptive Test</phrase> Putting It All Together Yield Learning Processes and Methods <phrase>Mvc</phrase> Implementation <phrase>Experimental</phrase> Demonstration Yield Learning Processes and Methods h DESPITE THE GROWING complexity of <phrase>semiconductor</phrase> devices, <phrase>adaptive test</phrase> deployments for such devices remain elusive. The <phrase>high</phrase> cost and difficulties of developing a custom <phrase>adaptive test</phrase> <phrase>solution</phrase> are likely causes for this sparsity. Indeed, there are many requirements that an <phrase>adaptive test</phrase> <phrase>solution</phrase> should meet. A good <phrase>adaptive test</phrase> system should dynamically adjust to process variation <phrase>statistics</phrase> that may not be stationary. It should modulate the <phrase>test</phrase> program at <phrase>high</phrase> granularities (die level or even sub-die level), have low adaptation latency to achieve real-time adaptivity, and comfortably handle learning from <phrase>terabyte</phrase>-scale historical <phrase>test</phrase> <phrase>data</phrase>. Moreover, it should achieve these goals without violating stringent <phrase>industrial</phrase> requirements on permissible <phrase>test</phrase> error. Finally, many of the specific details of these requirements are likely to differ across product lines, e.g., <phrase>consumer</phrase> <phrase>products</phrase> certainly have very different comfort levels for permissible <phrase>test</phrase> error than automotive <phrase>products</phrase>. However, the advantages of a successful <phrase>adaptive test</phrase> deployment are clear: <phrase>test</phrase> time reduction, <phrase>test</phrase> quality improvement, improved <phrase>data analysis</phrase> ability, and <phrase>acceleration</phrase> of yield learning. Therefore, substantial <phrase>industrial</phrase> interest exists for making <phrase>adaptive test</phrase> a <phrase>reality</phrase> in the near term. This interest is driving groups from a broad swath of <phrase>industry</phrase> to collaborate on <phrase>adaptive test</phrase> development. The <phrase>Test</phrase> & <phrase>Test</phrase> Equipment team within the <phrase>International Technology Roadmap for Semiconductors</phrase> (ITRS) has created a <phrase>subgroup</phrase> specifically targeted at the <phrase>adaptive test</phrase> problem [1]. The requirements that an <phrase>adaptive test</phrase> system must meet can be broken down to three discrete sets of challenges. First, there is the problem of <phrase>data</phrase> handling: device, wafer, and lot measurements should be linked and traceable throughout the fabrication process, <phrase>data</phrase> should be standardized and easy to exchange between <phrase>adaptive test</phrase> programs and throughout the <phrase>organization</phrase>, and large <phrase>data</phrase> sets should be readily accessible. Second, <phrase>innovation</phrase> within the <phrase>adaptive test</phrase> <phrase>community</phrase> is rapidly increasing the number of <phrase>adaptive test</phrase> techniques that have been demonstrated in <phrase>literature</phrase>. Thus, an <phrase>adaptive test</phrase> system should simplify integration of new <phrase>adaptive test</phrase> techniques as they become available. Third, <phrase>test</phrase> engineers should be provided with a convenient means of obtaining deep visibility into the inner workings of an <phrase>adaptive test</phrase> system. For Editor's note: Adaptive Testing has been a focus <phrase>area</phrase> for IC testing in the last few years. The ''<phrase>Model</phrase>-View-Controller'' (<phrase>MVC</phrase>) <phrase>architecture</phrase> has the potential to improve <phrase>engineering</phrase> <phrase>productivity</phrase> for analysis and application of Adaptive Testing. The vision of <phrase>MVC</phrase> is that it will be used to 
Learning Parametric Designing Learning Parametric Designing INTRODUCTION Parametric <phrase>design</phrase> techniques offer obvious advantages for <phrase>engineering</phrase> and <phrase>manufacturing</phrase> processes, now <phrase>architects</phrase> have emerged to apply these methods in their working environment suggesting solutions and novel designs at an earlier stage of the process. Through the coupling of <phrase>architectural</phrase> <phrase>design</phrase> with parametric modelling methods, the chapter presents techniques that enhance students' learning and <phrase>knowledge</phrase> about designing and <phrase>architectural</phrase> building processes. This allows a deeper comprehension of the <phrase>design</phrase> objectives and <phrase>aids</phrase> <phrase>architectural</phrase> designers in their decisions to find solutions. A dilemma of semester-based teaching is that students reach their highest level of skills and experience at the end of a term, after which they leave for their break and are therefore unable to apply their freshly gained <phrase>knowledge</phrase> immediately. At the beginning of the next following term, however, the <phrase>knowledge</phrase> and skills they had gained earlier are likely to be either inactive or not employed, and learning foci may have shifted to other aims. ABSTRACT Parametric designing, its instruments, and techniques move <phrase>architectural</phrase> <phrase>design</phrase> <phrase>education</phrase> towards novel avenues of <phrase>deep learning</phrase>. Akin to learning and working environments of <phrase>engineering</phrase> and <phrase>manufacturing</phrase> , it offers similar advantages for <phrase>architects</phrase>. Yet it is not as simple as using another tool; parametric designing fundamentally shifts the engagement with the <phrase>design</phrase> problem. Parametric designing allows <phrase>architects</phrase> to be substantially deeper involved in the overall <phrase>design</phrase> and development process extending it effectively beyond <phrase>production</phrase> and lifecycle. Leaning parametric <phrase>design</phrase> strategies enhance <phrase>architects</phrase>' critical engagement with their designs and their <phrase>communication</phrase>. Subsequently, the computational aid of parametric modelling alters substantially how and what <phrase>students learn</phrase> and <phrase>architects</phrase> practice. The <phrase>architectural</phrase> <phrase>design</phrase> studio presented here addressed these issues by integrating the learning experience from the beginning by focusing on parameters that create or inform about the <phrase>design</phrase>. The objective of this 'parametric designing' was to allow students to understand the impact each step and <phrase>variable</phrase> has on the <phrase>design</phrase> and follow the impact it has onto the project. Students developed and communicated their <phrase>design</phrase> parameters by utilizing their <phrase>knowledge</phrase> throughout the <phrase>design</phrase>-studio environment. Because of this, students began to think about <phrase>design</phrase> problems in different ways. The studio explored <phrase>design</phrase> by basing it on parameters and their connecting rules. In <phrase>order</phrase> to build up a <phrase>philosophy</phrase> around parametric dependencies and relationships, the participants used <phrase>digital</phrase> instruments that aided them to create and express their designs. With these instruments, they could develop expertise to engage creatively 
<phrase>Verb</phrase> Temporality Analysis using Reichenbach's Tense System This <phrase>paper</phrase> presents the analysis process of <phrase>verb</phrase> temporality using Reichenbach's tense system, a <phrase>language</phrase>-<phrase>independent</phrase> system which describes tense as relations among <phrase>linguistic</phrase> and extra-<phrase>linguistic</phrase> temporal entities. Several difficulties arise from the deep analysis required for classification into Reichenbach's categories. They regard establishing the logical <phrase>sequence</phrase> of clauses in the skeletal structure of the discourse, and modeling the behavior of temporal markers according to this <phrase>sequence</phrase>. A dependency clause anchoring <phrase>algorithm</phrase> is then proposed and compared to other anchoring methods, and sequential <phrase>supervised learning</phrase> is used for abstracting surrounding context in <phrase>order</phrase> to determine temporal marker behavior. <phrase>Experimental</phrase> <phrase>results</phrase> show that the <phrase>proposed approach</phrase> is able to better abstract <phrase>verb</phrase> temporality than statistical ones, suggesting that analytical interlingual <phrase>translation</phrase> can <phrase>complement</phrase> existing SMT techniques by providing an additional layer of <phrase>semantic</phrase> <phrase>information</phrase>. abstrair o contexto de forma a determinar o posicionamento temporal desses marcadores. Resultados experimentais mostram que a abordagem proposta capaz de melhor abstrair a temporalidade verbal quando comparada a abordagens estatsticas, o que sugere que traduo automtica baseada em interlngua pode complementar tcnicas estatsticas j existentes, provendo uma camada adicional de informao semntica.
DOA - The <phrase>Deductive</phrase> <phrase>Object-Oriented</phrase> Approach to the Development of Adaptive <phrase>Natural Language</phrase> Interfaces (Abstract) We present the <phrase>Deductive</phrase> <phrase>Object-oriented</phrase> Approach (DOA), a new framework for <phrase>natural language</phrase> <phrase>interface design</phrase>. It uses a deduc-tive <phrase>object-oriented</phrase> <phrase>database</phrase> (DOOD) for developing the interface as component of the <phrase>database</phrase> system. This provides the basis for a consistent and ecient mapping of the user input to the <phrase>target</phrase> representation. Furthermore, we propose adaptive techniques to deal eciently with the dicult task of dynamic <phrase>knowledge engineering</phrase>. As rst feasibility <phrase>test</phrase> we have developed an adaptive interface for <phrase>Japanese</phrase>, which is applied to the question support facility of a collaborative <phrase>education</phrase> system. Despite the <phrase>long</phrase> tradition of the <phrase>research</phrase> eld of <phrase>natural language</phrase> interfaces , we have to face the situation that <phrase>natural language</phrase> interfaces are still far away from widespread practicable use [1]. One of the main responsible factors for this is missing integration, which <phrase>results</phrase> in insu-cient performance and wrong interpretations. Therefore, we <phrase>dene</phrase> a new framework for <phrase>natural language</phrase> <phrase>interface design</phrase> in which the interface constitutes a component of the DOOD <phrase>ROCK</phrase> & ROLL [2]. Especially its <phrase>inheritance</phrase> mechanisms make it possible to structure the <phrase>linguistic</phrase> <phrase>knowledge</phrase> hierarchically to guarantee a compact representation. Another main diculty for <phrase>natural language</phrase> interfaces is the <phrase>high</phrase> amount of necessary manual <phrase>knowledge engineering</phrase>. For each portation to a new application this elaborate process has to be repeated. Furthermore, the interfaces are often part of dynamic environments with constant changes concerning topics and user <phrase>population</phrase>. Therefore, we apply <phrase>linguistic</phrase> resources and methods from <phrase>machine learning</phrase> to the automatic acquisition of <phrase>linguistic</phrase> <phrase>knowledge</phrase>. The applied interface <phrase>architecture</phrase> consists of the two main parts lexical and <phrase>semantic</phrase> component. The lexical component possesses three central modules: <phrase>morpho</phrase>-<phrase>lexical analysis</phrase>, unknown value list (UVL) analysis, and spelling error correction. <phrase>Morpho</phrase>-<phrase>lexical analysis</phrase> performs the tok-enization of <phrase>Japanese</phrase> input, i.e. the segmentation into individual input words. By accessing a <phrase>domain-independent</phrase> <phrase>lexicon</phrase> the module transforms the input into a deep form list (DFL), which indicates for each token its surface form, category, and a set of associated deep forms. During UVL analysis we deal with domain-specic terms in the input and we also support spelling error correction of those terms. Besides this, we apply the following three adaptive techniques: (1) the use of the
Multiscale 3-D Shape Representation and Segmentation Using Spherical Wavelets This <phrase>paper</phrase> presents a novel multiscale shape representation and segmentation <phrase>algorithm</phrase> based on the spherical <phrase>wavelet transform</phrase>. This work is motivated by the need to compactly and accurately encode variations at multiple scales in the shape representation in <phrase>order</phrase> to drive the segmentation and shape analysis of deep <phrase>brain</phrase> structures, such as the <phrase>caudate nucleus</phrase> or the <phrase>hippocampus</phrase>. Our proposed shape representation can be optimized to compactly encode shape variations in a <phrase>population</phrase> at the needed scale and spatial locations, enabling the <phrase>construction</phrase> of more descriptive, nonglobal, nonuniform shape <phrase>probability</phrase> priors to be included in the segmentation and shape analysis framework. In particular, this representation addresses the shortcomings of techniques that learn a global shape prior at a <phrase>single</phrase> scale of analysis and cannot represent fine, local variations in a <phrase>population</phrase> of shapes in the presence of a limited dataset. Specifically, our technique defines a multiscale parametric <phrase>model</phrase> of surfaces belonging to the same <phrase>population</phrase> using a compact set of spherical wavelets targeted to that <phrase>population</phrase>. We further refine the shape representation by separating into groups <phrase>wavelet</phrase> coefficients that describe <phrase>independent</phrase> global and/or local biological variations in the <phrase>population</phrase>, using spectral <phrase>graph</phrase> partitioning. We then learn a <phrase>prior probability</phrase> distribution induced over each group to explicitly encode these variations at different scales and spatial locations. Based on this representation, we derive a parametric active surface <phrase>evolution</phrase> using the multiscale prior coefficients as parameters for our optimization procedure to naturally include the prior for segmentation. Additionally, the optimization method can be applied in a coarse-to-fine manner. We apply our <phrase>algorithm</phrase> to two different <phrase>brain</phrase> structures, the <phrase>caudate nucleus</phrase> and the <phrase>hippocampus</phrase>, of interest in the study of <phrase>schizophrenia</phrase>. We show: 1) a <phrase>reconstruction</phrase> task of a <phrase>test</phrase> set to validate the expressiveness of our multiscale prior and 2) a segmentation task. In the <phrase>reconstruction</phrase> task, our <phrase>results</phrase> show that for a given <phrase>training set</phrase> size, our <phrase>algorithm</phrase> significantly improves the approximation of shapes in a testing set over the Point Distribution <phrase>Model</phrase>, which tends to oversmooth <phrase>data</phrase>. In the segmentation task, our validation shows our <phrase>algorithm</phrase> is computationally efficient and outperforms the Active Shape <phrase>Model</phrase> <phrase>algorithm</phrase>, by capturing finer shape details.
Complexity of Representation and Inference in Compositional Models with Part Sharing Complexity of Representation and Inference in Compositional Models with Part Sharing Roozbeh Mottaghi This <phrase>paper</phrase> performs a complexity analysis of a class of serial and parallel compositional models of multiple objects and shows that they enable efficient representation and rapid inference. Composi-tional models are generative and represent objects in a hierarchically distributed manner in terms of parts and subparts, which are constructed recursively by part-subpart compositions. Parts are represented more coarsely at <phrase>higher level</phrase> of the hierarchy, so that the <phrase>upper</phrase> levels give coarse summary descriptions (e.g., there is a <phrase>horse</phrase> in the image) while the <phrase>lower</phrase> levels represents the details (e.g., the positions of the legs of the <phrase>horse</phrase>). This hierarchically distributed representation obeys the executive summary principle, meaning that a <phrase>high</phrase> level executive only requires a coarse summary description and can, if necessary, get more details by consulting <phrase>lower</phrase> level executives. The parts and subparts are organized in terms of hierarchical dictionaries which enables part sharing between different objects allowing efficient representation of many objects. The first main contribution of this <phrase>paper</phrase> is to show that compositional models can be mapped onto a parallel visual <phrase>architecture</phrase> similar to that used by bio-inspired visual models such as <phrase>deep convolutional</phrase> networks but more explicit in terms of representation , hence enabling part detection as well as <phrase>object detection</phrase>, and suitable for complexity analysis. Inference <phrase>algorithms</phrase> can be run on this <phrase>architecture</phrase> to exploit the gains caused by part sharing and executive summary. Effectively, this compositional <phrase>architecture</phrase> enables us to perform exact inference simultaneously over a large class of <phrase>generative models</phrase> of objects. The second contribution is an analysis of the complexity of compositional models in terms of computation time (for serial <phrase>computers</phrase>) and numbers of nodes (e.g., " <phrase>neurons</phrase>") for parallel <phrase>computers</phrase>. In particular, we compute the complexity gains by part sharing and executive summary and their dependence on how the <phrase>dictionary</phrase> scales with the level of the hierarchy. We explore three regimes of scaling behavior where the <phrase>dictionary</phrase> size (i) increases exponentially with the level of the hierarchy, (<phrase>ii</phrase>) is determined by an unsupervised compo-sitional <phrase>learning algorithm</phrase> applied to real <phrase>data</phrase>, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables <phrase>algorithms</phrase> which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial <phrase>computers</phrase> but can enable linear processing on parallel <phrase>computers</phrase>. Abstract This <phrase>paper</phrase> performs a 
Discriminative <phrase>feature extraction</phrase> with <phrase>Deep Neural Networks</phrase> We propose a framework for optimizing <phrase>Deep Neural Networks</phrase> (DNN) with the objective of learning <phrase>low-dimensional</phrase> <phrase>discriminative features</phrase> from <phrase>high</phrase>-dimensional complex patterns. In a two-stage process that effectively implements a Nonlinear <phrase>Discriminant</phrase> Analysis (NDA), we first pretrain a DNN using <phrase>stochastic optimization</phrase>, partly supervised and unsupervised. This stage involves <phrase>layer-wise</phrase> training and stacking of <phrase>single</phrase> <phrase>Restricted Boltzmann Machines</phrase> (RBM). The second stage performs <phrase>fine-tuning</phrase> of the DNN using a modified back-propagation <phrase>algorithm</phrase> that directly optimizes a Fisher criterion in the <phrase>feature space</phrase> spanned by the units of the last <phrase>hidden-layer</phrase> of the network. Our <phrase>experimental</phrase> <phrase>results</phrase> show that the features learned by a DNN using the <phrase>proposed framework</phrase> greatly facilitate classification, even when the <phrase>discriminative features</phrase> constitute a substantial <phrase>dimension</phrase> reduction. I. INTRODUCTION <phrase>Deep Neural Networks</phrase> (DNN) are Multilayer <phrase>Neural Networks</phrase> (MLNN) with more than one <phrase>hidden-layer</phrase> and thousands, often millions of parameters. None of the well known training <phrase>algorithms</phrase> for MLNNs are capable of finding good solutions in such <phrase>high</phrase>-dimensional parameter spaces, except for chance. On the other hand, the <phrase>high</phrase> flexibility of DNNs enables the learning of <phrase>discriminative features</phrase> of low <phrase>dimension</phrase> from <phrase>high</phrase>-dimensional complex <phrase>data</phrase>, also known as Nonlinear <phrase>Discriminant</phrase> Analysis (NDA). In [1], [2], [3], [4], [5], [6] NDA with MLNNs is theoretically analyzed. However, empirical <phrase>results</phrase> were only reported using MLNNs with no more than two <phrase>hidden-layers</phrase> and few parameters [7], [8], because training of DNNs is infeasible due to <phrase>overfitting</phrase> and the existence of many poor local <phrase>optima</phrase> in the <phrase>parameter space</phrase>. In this <phrase>paper</phrase>, we present a framework for pre-optimizing DNNs regarding NDA, and a subsequent <phrase>fine-tuning</phrase> using back-propagation with a Fisher <phrase>discriminant</phrase> criterion as <phrase>objective function</phrase>. We justify our approach by classification experiments on various <phrase>real world</phrase> datasets using linear classifiers as well as sophisticated nonlinear ones, like <phrase>Support Vector Machines</phrase> (<phrase>SVM</phrase>). Our <phrase>results</phrase> show that highly discriminative <phrase>low-dimensional</phrase> features can be learned using the <phrase>proposed framework</phrase> without particular assumptions about the underlying <phrase>data</phrase> distribution.
Improving kernel-<phrase>energy</phrase> <phrase>trade</phrase>-offs for <phrase>machine learning</phrase> in implantable and wearable biomedical applications Emerging biomedical sensors and stimulators offer unprecedented modalities for delivering therapy and acquiring <phrase>physiological</phrase> signals (e.g., deep <phrase>brain</phrase> stimulators). Exploiting these in intelligent, closed-loop systems requires detecting specific <phrase>physiological</phrase> states using very <phrase>low power</phrase> (i.e., 1-10mW for wearable devices, 10-100W for implantable devices). <phrase>Machine learning</phrase> is a powerful tool for modeling correlations in <phrase>physiological</phrase> signals, but <phrase>model</phrase> complexity in typical biomedical applications makes detection too computationally intensive. We analyze the computational <phrase>energy</phrase> <phrase>trade</phrase>-offs and propose a method of restructuring the computations to yield more favorable <phrase>trade</phrase>-offs, especially for typical biomedical applications. We thus develop a methodology for implementing low-<phrase>energy</phrase> classification kernels and demonstrate <phrase>energy</phrase> reduction in practical biomedical systems. Two applications, <phrase>arrhythmia</phrase> detection using electrocardiographs (<phrase>ECG</phrase>) from the <phrase>MIT</phrase>-<phrase>BIH</phrase> <phrase>database</phrase> [1] and seizure detection using electroencephalographs (<phrase>EEG</phrase>) from the CHB-<phrase>MIT</phrase> [1,2] <phrase>database</phrase>, are used. The proposed computational restructuring can be used with very little performance degradation, and it reduces <phrase>energy</phrase> by 2627x nd 7.0-36.3x (depending on the patient), respectively. a Index TermsG kernel-<phrase>energy</phrase> <phrase>trade</phrase>-off, <phrase>energy</phrase> efficiency, achine learning, biomedical devices m I. INTRODUCTION The typical signals available for sensing in <phrase>low-power</phrase>, chronic biomedical systems are the result of complex and diverse <phrase>physiological</phrase> processes. Accurate <phrase>physiology</phrase>-based modeling thus faces severe challenges. <phrase>Data</phrase>-driven modeling, on the other hand, shows great promise for being both highly accurate and viable, particularly thanks to the recent availability of patient <phrase>data</phrase> in the <phrase>healthcare</phrase> domain [3]. <phrase>Machine learning</phrase> offers powerful methods for both the development and application of such models. Biomedical sensors and stimulators have recently emerged showing unprecedented clinical efficacy for treating a broad <phrase>range</phrase> of diseases. Chronic implantable and wearable devices based on these can be made intelligent through embedded <phrase>machine-learning</phrase> techniques. Such systems, however, face severe <phrase>energy</phrase> constraints. The <phrase>nature</phrase> of <phrase>data</phrase>-driven modeling implies that the complexity depends on the characteristics of the <phrase>data</phrase>; most <phrase>data</phrase>-driven biomedical <phrase>algorithms</phrase>, however, have thus far paid little consideration to computational <phrase>energy</phrase>, especially in the context of micro-or milli-<phrase>Watt</phrase> devices. The work of this <phrase>paper</phrase> is motivated by the <phrase>energy</phrase> constrained application of <phrase>data</phrase>-driven models through <phrase>machine-learning</phrase> classifiers. The main contributions are summarized as follows: We propose a method of restructuring the computations of non-linear <phrase>support vector machine</phrase> (<phrase>SVM</phrase>) kernels to overcome <phrase>energy</phrase> scaling with the number of support vectors (which is representative of <phrase>model</phrase> complexity). Analysis shows <phrase>model</phrase> complexity to be a <phrase>primary energy</phrase> limitation. <phrase>Polynomial</phrase> kernels are exploited for retaining sufficient flexibility while also 
Structural Maxent Models We present a new class of <phrase>density estimation</phrase> models, Structural Maxent models, with feature functions selected from a <phrase>union</phrase> of possibly very complex sub-families and yet benefiting from strong learning guarantees. The <phrase>design</phrase> of our models is based on a new principle supported by <phrase>uniform convergence</phrase> bounds and taking into consideration the complexity of the different sub-families <phrase>composing</phrase> the full set of features. We prove new <phrase>data</phrase>-dependent learning bounds for our models, expressed in terms of the Rademacher complexities of these sub-families. We also prove a duality theorem, which we use to derive our Structural Maxent <phrase>algorithm</phrase>. We give a full description of our <phrase>algorithm</phrase>, including the details of its derivation, and <phrase>report</phrase> the <phrase>results</phrase> of several experiments demonstrating that its performance improves on that of existing L 1-norm regularized Maxent <phrase>algorithms</phrase>. We further similarly define conditional Structural Maxent models for <phrase>multi-class</phrase> classification problems. These are <phrase>conditional probability</phrase> models also making use of a <phrase>union</phrase> of possibly complex feature sub-families. We prove a duality theorem for these models as well, which reveals their connection with existing <phrase>binary</phrase> and <phrase>multi-class</phrase> deep boosting <phrase>algorithms</phrase>.
Teaching Students to Question <phrase>Earth</phrase>-core <phrase>Convection</phrase> <phrase>Science</phrase> progresses by making important observations, and by discovering what is wrong with present thinking. For 70 years <phrase>convection</phrase> has been hypothesized to exist within the Earth's <phrase>fluid</phrase> core, and has become the stuff of textbooks, but there are serious problems with that concept. Teaching students to question <phrase>Earth</phrase>-core <phrase>convection</phrase> can <phrase>lead</phrase> them to learn about the behavior and the properties of <phrase>matter</phrase>, and can help to bring into focus the importance of discussing, debating, and challenging current thinking in <phrase>science</phrase>. The process called <phrase>convection</phrase> is easily observed in ordinary experience, but has been greatly misunderstood in the geosciences for decades. In the subject of <phrase>convection</phrase>, there are important lessons to be learned about <phrase>scientific inquiry</phrase> and scientific discovery and about the necessity of careful, precise reasoning. The subject of <phrase>convection</phrase> can be a jumping off point for stimulating classroom discussions about what is wrong with <phrase>textbook</phrase> presentations of <phrase>Earth</phrase>-core <phrase>convection</phrase>. And, importantly, the subject can help to bring into focus the importance of discussing, debating, and challenging current thinking in a <phrase>variety</phrase> of areas. <phrase>Heat</phrase> a pot of <phrase>water</phrase> on the stovetop. Before it starts to boil, the <phrase>water</phrase> begins to circulate from bottom to top and from top to bottom. This is called <phrase>convection</phrase> and it can be better observed by adding a few <phrase>tea leaves</phrase>, <phrase>coffee</phrase> grounds, <phrase>celery</phrase> seeds, or the like, which are carried along by the circulation of <phrase>water</phrase>. <phrase>Convection</phrase> occurs because <phrase>heat</phrase> at the bottom causes the <phrase>water</phrase> to expand a <phrase>bit</phrase>, becoming lighter, less dense, than the cooler <phrase>water</phrase> at the top. The warmer, less dense, <phrase>water</phrase> rises to the top as the cooler, denser, <phrase>water</phrase> descends. This all seems so simple that it is no wonder that the <phrase>convection</phrase> process has been widely (but falsely) assumed to occur deep within the Earth's core. In this case truly the <phrase>devil</phrase> is in the details, and teaching students those details is
On <phrase>design thinking</phrase>, <phrase>business</phrase>, the <phrase>arts</phrase>, STEM a styling or decorative activity, and this is the same corporate <phrase>infrastructure</phrase> that jumped on bandwagons like IT and <phrase>total cost of ownership</phrase>, Six Sigma and quality, <phrase>supply-chain management</phrase>, off-shoring, and any other term du jour. I have very little expectation that <phrase>design</phrase> and <phrase>design thinking</phrase> are <phrase>sustainable</phrase> in a <phrase>business</phrase> context without a massive <phrase>culture</phrase> baseline to support them. Let me explain. Nearly every policy we have related to <phrase>education</phrase> and <phrase>research</phrase> supports the funding of <phrase>traditional approaches</phrase> to <phrase>education</phrase>, and these policies shape the societal force that, in combination with larger events, shapes future generations. As a quick example, the recent recovery and reinvestment <phrase>legislation</phrase> has <phrase>directed</phrase> $2.4 billion to the <phrase>National Science Foundation</phrase> and a paltry $49.9 million to the National Endowment for the Artsthe closest thing we have to a formal <phrase>government</phrase> arm related to <phrase>design</phrase> and <phrase>innovation</phrase>. The $650 million dedicated to the Investing in <phrase>Innovation</phrase> Fund has characterized " development " or " pure <phrase>innovation</phrase> " strategies as the lowest funded, with a ceiling of only $5 million (compared with $50 million for a " scale up " approach). I highlight these financial disparities not to whine about how underfunded <phrase>design</phrase> is, but to point out that our policies are actively supporting the old ways of doing things. We haven't experienced a widespread <phrase>cultural</phrase> <phrase>adoption</phrase> of <phrase>design</phrase>, and we are in no way encouraging future generations to adopt and embrace <phrase>design</phrase>. In the <phrase>United States</phrase>, children get 18 years of educational training in linear and logical thinking, with an explicit emphasis on <phrase>science</phrase>, <phrase>technology</phrase>, <phrase>engineering</phrase>, and <phrase>mathematics</phrase>. They then bring this <phrase>knowledge</phrase> to their advanced course work, and are able to better learn more complex principles such as those taught in <phrase>business</phrase> <phrase>school</phrase>. But that baseline of STEM <phrase>knowledge</phrase> taught to our children has no parallel in <phrase>design</phrase>. So when <phrase>college</phrase> students encounter one or two <phrase>design</phrase> courses in their <phrase>business</phrase> training, which is focused on <phrase>lateral thinking</phrase>, <phrase>abductive reasoning</phrase>, and reframing, they have no deep, <phrase>tacit knowledge</phrase> of fundamental <phrase>design</phrase> Jon: In 1996 I took a class called " Introduction to <phrase>Design Thinking</phrase> " at <phrase>Carnegie Mellon</phrase>, taught by Richard Buchanan; he'd been teaching it for years. I learned a lot, and it's obviously shaped the way I consider <phrase>design</phrase>, <phrase>culture</phrase>, and behavior. Why do you think it's only now, some 14 years later, that the <phrase>language</phrase> related to the intellectual and intangible 
Unsupervised and <phrase>Transfer Learning</phrase> Challenge: a <phrase>Deep Learning</phrase> Approach Learning good representations from a large set of <phrase>unlabeled data</phrase> is a particularly <phrase>challenging task</phrase>. Recent work (see Bengio (2009) for a review) shows that training <phrase>deep architectures</phrase> is a good way to extract such representations, by extracting and disentangling gradually <phrase>higher-level</phrase> factors of variation characterizing the input distribution. In this <phrase>paper</phrase>, we describe different kinds of layers we trained for learning representations in the setting of the Unsupervised and <phrase>Transfer Learning</phrase> Challenge. The strategy of our team won the final phase of the challenge. It combined and stacked different one-layer <phrase>unsupervised learning</phrase> <phrase>algorithms</phrase>, adapted to each of the five datasets of the competition. This <phrase>paper</phrase> describes that strategy and the particular one-layer <phrase>learning algorithms</phrase> feeding a simple linear classifier with a tiny number of <phrase>labeled training</phrase> samples (1 to 64 per class).
Comparing <phrase>distributions</phrase> and shapes using the kernel distance Starting with a similarity <phrase>function</phrase> between objects, it is possible to define a distance metric (the kernel distance) on pairs of objects, and more generally on <phrase>probability distributions</phrase> over them. These distance metrics have a deep basis in <phrase>functional analysis</phrase> and <phrase>geometric measure theory</phrase>, and have a rich structure that includes an isometric embedding into a <phrase>Hilbert space</phrase>. They have recently been applied to numerous problems in <phrase>machine learning</phrase> and shape analysis. <phrase>SIn</phrase> this <phrase>paper</phrase>, we provide the first algorithmic analysis of these distance metrics. Our main contributions are as follows: We present fast approximation <phrase>algorithms</phrase> for <phrase>computing</phrase> the kernel distance between two point sets P and Q that <phrase>runs</phrase> in near-linear time in the size of P &#8746; Q (an explicit calculation would take quadratic time). We present <phrase>polynomial</phrase>-time <phrase>algorithms</phrase> for approximately minimizing the kernel distance under rigid transformation; they run in time O(n + poly(1/&#949;, log n)). We provide several <phrase>general</phrase> techniques for reducing complex objects to convenient sparse representations (specifically to point sets or sets of points sets) which approximately preserve the kernel distance. In particular, this allows us to reduce problems of <phrase>computing</phrase> the kernel distance between various types of objects such as curves, surfaces, and <phrase>distributions</phrase> to <phrase>computing</phrase> the kernel distance between point sets.
<phrase>Deep learning</phrase> comes of age Advances on multiple fronts are bringing big improvements to the way <phrase>computers</phrase> learn, increasing the accuracy of speech and vision systems.
Neural dynamics and <phrase>information</phrase> representation in microcircuits of <phrase>motor cortex</phrase> The <phrase>brain</phrase> has to analyze and respond to external events that can change rapidly from time to time, suggesting that <phrase>information processing</phrase> by the <phrase>brain</phrase> may be essentially dynamic rather than static. The dynamical features of neural computation are of significant importance in <phrase>motor cortex</phrase> that governs the process of movement generation and learning. In this <phrase>paper</phrase>, we discuss these features based primarily on our recent findings on neural dynamics and <phrase>information</phrase> coding in the microcircuit of <phrase>rat</phrase> <phrase>motor cortex</phrase>. In fact, <phrase>cortical</phrase> <phrase>neurons</phrase> show a <phrase>variety</phrase> of dynamical behavior from rhythmic activity in various <phrase>frequency</phrase> bands to highly irregular spike firing. Of particular interest are the similarity and dissimilarity of the <phrase>neuronal</phrase> response properties in different layers of <phrase>motor cortex</phrase>. By <phrase>conducting</phrase> <phrase>electrophysiological</phrase> recordings in slice preparation, we <phrase>report</phrase> the phase response curves (PRCs) of <phrase>neurons</phrase> in different <phrase>cortical</phrase> layers to demonstrate their layer-dependent synchronization properties. We then study how <phrase>motor cortex</phrase> recruits task-related <phrase>neurons</phrase> in different layers for voluntary arm movements by simultaneous juxtacellular and multiunit recordings from behaving rats. The <phrase>results</phrase> suggest an interesting difference in the <phrase>spectrum</phrase> of functional activity between the superficial and deep layers. Furthermore, the task-related activities recorded from various layers exhibited <phrase>power law</phrase> <phrase>distributions</phrase> of inter-spike intervals (<phrase>ISIs</phrase>), in contrast to a <phrase>general</phrase> belief that <phrase>ISIs</phrase> obey <phrase>Poisson</phrase> or Gamma <phrase>distributions</phrase> in <phrase>cortical</phrase> <phrase>neurons</phrase>. We present a theoretical argument that this <phrase>power law</phrase> of in vivo <phrase>neurons</phrase> may represent the maximization of the <phrase>entropy</phrase> of firing rate with limited <phrase>energy</phrase> consumption of spike generation. Though further studies are required to fully clarify the functional implications of this coding principle, it may shed new <phrase>light</phrase> on <phrase>information</phrase> representations by <phrase>neurons</phrase> and circuits in <phrase>motor cortex</phrase>.
An <phrase>Hybrid</phrase> Approach for Spoken <phrase>Natural Language Understanding</phrase> Applied to a <phrase>Mobile</phrase> Intelligent <phrase>Robot</phrase> The greater sophistication and complexity of machines increases the necessity to equip them with <phrase>human</phrase> <phrase>friendly</phrase> interfaces. As we know, voice is the main support for <phrase>human</phrase>-<phrase>human</phrase> <phrase>communication</phrase>, so it is desirable to interact with machines, namely <phrase>robots</phrase>, using voice. In this <phrase>paper</phrase> we present the recent <phrase>evolution</phrase> of the <phrase>Natural Language Understanding</phrase> capabilities of Carl, our <phrase>mobile</phrase> intelligent <phrase>robot</phrase> capable of interacting with humans using spoken <phrase>natural language</phrase>. The new <phrase>design</phrase> is based on a <phrase>hybrid</phrase> approach, combining a robust parser with <phrase>Memory</phrase> Based Learning. This <phrase>hybrid</phrase> <phrase>architecture</phrase> is capable of performing deep analysis if the sentence is (almost) completely accepted by the <phrase>grammar</phrase>, and capable of performing a shallow analysis if the sentence has severe errors.
<phrase>Computers</phrase> and Students' Conceptions of Learning: The Transition from Post-<phrase>Secondary Education</phrase> to the Workplace Recognition that we <phrase>live</phrase> in a rapidly changing <phrase>knowledge economy</phrase> has <phrase>led</phrase> to an increased interest in <phrase>lifelong learning</phrase>. Workplaces are in need of employees who can learn new skills and adapt quickly to social and technological changes. Educational institutions are faced with the challenge of providing students with the <phrase>knowledge</phrase> and skills so that they may adapt successfully to job-related changes after completing their formal <phrase>education</phrase>. In this study, 445 post-<phrase>secondary</phrase> students were surveyed to assess conceptions of learning and their preparation for the workplace. Use of <phrase>information technology</phrase> was assessed in terms of skills, attitudes, skill development, and methods of learning. <phrase>Results</phrase> indicated that various conceptions of learning are associated with different perceptions of the <phrase>learning environment</phrase>, different perceptions of learning needs and skills, and with different perceptions of the demands of the future workplace. Unlike surface learners, deep and lifelong learners prefer to learn independently, use <phrase>computers</phrase> for more sophisticated tasks, value their studies, see the connections between their studies and future work, and conceive of <phrase>higher-level</phrase> applications for their computer skills. These <phrase>results</phrase> present many challenges for the <phrase>design</phrase> of educational programs.
<phrase>Multi-Dimensional</phrase> Deep <phrase>Memory</phrase> <phrase>Atari</phrase>-Go Players for Parameter Exploring Policy Gradients Developing <phrase>superior</phrase> <phrase>artificial</phrase> <phrase>board-game</phrase> players is a widely-studied <phrase>area</phrase> of <phrase>Artificial Intelligence</phrase>. Among the most challenging <phrase>games</phrase> is the <phrase>Asian</phrase> <phrase>game</phrase> of Go, which, despite its deceivingly simple rules, has eluded the development of <phrase>artificial</phrase> expert players. In this <phrase>paper</phrase> we attempt to tackle this challenge through a combination of two <phrase>recent developments</phrase> in <phrase>Machine Learning</phrase>. We employ <phrase>Multi-Dimensional</phrase> <phrase>Recurrent Neural Networks</phrase> with <phrase>Long</phrase> <phrase>Short-Term Memory</phrase> cells to handle the <phrase>multi-dimensional</phrase> <phrase>data</phrase> of the <phrase>board game</phrase> in a very natural way. In <phrase>order</phrase> to improve the convergence rate, as well as the ultimate performance, we <phrase>train</phrase> those networks using Policy Gradients with Parameter-based Exploration , a recently developed <phrase>Reinforcement Learning</phrase> <phrase>algorithm</phrase> which has been found to have numerous advantages over <phrase>Evolution</phrase> Strategies. Our empirical <phrase>results</phrase> confirm the promise of this approach, and we discuss how it can be scaled up to expert-level Go players.
Learning Factored Representations in a Deep Mixture of Experts Mixtures of Experts combine the outputs of several " expert " networks, each of which specializes in a different part of the input space. This is achieved by training a " gating " network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at <phrase>test</phrase> time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked <phrase>model</phrase>, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest <phrase>model</phrase> size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent (" where ") experts at the first layer, and class-specific (" what ") experts at the second layer. In addition, we see that the different combinations are in use when the <phrase>model</phrase> is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.
<phrase>Student</phrase>-<phrase>led</phrase> Facilitation Strategies in Online Discussions This study explored <phrase>student</phrase>-<phrase>led</phrase> facilitation strategies used to overcome the challenges of instructor-dominated facilitation, enhance the sense of learning <phrase>community</phrase>, and encourage <phrase>student</phrase> participation in online discussions. It presents a series of cases of students' facilitation strategies and using qualitative <phrase>data analysis</phrase> of discussion threads within the naturalistic inquiry framework, identifies three facilitation strategies: inspirational; practice-oriented; and highly structured. The study shows that these facilitation strategies generated innovative ideas, motivated students to participate, and provided a <phrase>risk</phrase>-<phrase>free</phrase> and relaxed <phrase>atmosphere</phrase> for participation. Introduction Online discussions have been widely used in both blended and online courses as a platform for exchanging <phrase>information</phrase>, communicating, and supporting learning. The <phrase>design</phrase> and development of meaningful learning activities as part of online discussions presents new challenges to the instructors who are used to getting <phrase>feedback</phrase> via audio, visual, and contextual cues in face-to-face classrooms (Collins & Berge, 1997). Providing the quality of online participation has been one of these challenges because students may fail to engage in deep conversations and provide thoughtful and reflective contributions related to the discussion requirements (Dennen & <phrase>Wieland</phrase>, 2007). Although discussion techniques have been used in face-to-face classrooms, using them in an online environment requires the utilization of different pedagogical approaches because of the affordances and the limitations of asynchronous online <phrase>communication</phrase> technologies. According to Harasim (1990), the key differences between online and face-to-face discussions are time and place dependence, and the richness and the structure of <phrase>communication</phrase>. Moreover, in online asynchronous discussions, <phrase>communication</phrase> relies on <phrase>text-based</phrase> <phrase>information</phrase>, which lacks immediate instructors' verbal <phrase>feedback</phrase> used in face-to-face classrooms as well as <phrase>nonverbal</phrase> and contextual cues critical for <phrase>communication</phrase>. Even when using synchronous <phrase>technology</phrase> as conference calls, online students perceive the lack of <phrase>nonverbal</phrase> and contextual cues as a serious obstacle for establishing reciprocal understanding (Karpova, Correia, & Baran, 2009).
<phrase>RL</phrase>$^2$: Fast <phrase>Reinforcement Learning</phrase> via Slow <phrase>Reinforcement Learning</phrase> <phrase>Deep reinforcement learning</phrase> (deep <phrase>RL</phrase>) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their <phrase>prior knowledge</phrase> about the world. This <phrase>paper</phrase> seeks to <phrase>bridge</phrase> this gap. Rather than designing a " fast " <phrase>reinforcement learning</phrase> <phrase>algorithm</phrase>, we propose to represent it as a <phrase>recurrent neural network</phrase> (RNN) and learn it from <phrase>data</phrase>. In our <phrase>proposed method</phrase>, <phrase>RL</phrase> 2 , the <phrase>algorithm</phrase> is encoded in the weights of the RNN, which are learned slowly through a <phrase>general</phrase>-purpose (" slow ") <phrase>RL</phrase> <phrase>algorithm</phrase>. The RNN receives all <phrase>information</phrase> a typical <phrase>RL</phrase> <phrase>algorithm</phrase> would receive, including observations , actions, rewards, and termination <phrase>flags</phrase>; and it retains its <phrase>state</phrase> across episodes in a given <phrase>Markov Decision Process</phrase> (MDP). The activations of the RNN store the <phrase>state</phrase> of the " fast " <phrase>RL</phrase> <phrase>algorithm</phrase> on the current (previously unseen) MDP. We evaluate <phrase>RL</phrase> 2 experimentally on both <phrase>small-scale</phrase> and <phrase>large-scale</phrase> problems. On the <phrase>small-scale</phrase> side, we <phrase>train</phrase> it to solve randomly generated multi-armed bandit problems and finite MDPs. After <phrase>RL</phrase> 2 is trained, its performance on new MDPs is close to <phrase>human</phrase>-designed <phrase>algorithms</phrase> with optimality guarantees. On the <phrase>large-scale</phrase> side, we <phrase>test</phrase> <phrase>RL</phrase> 2 on a vision-based <phrase>navigation</phrase> task and show that it scales up to <phrase>high</phrase>-dimensional problems.
Tutorial: <phrase>Lessons Learned</phrase> from Building <phrase>Real-life</phrase> <phrase>Recommender Systems</phrase> In 2006, <phrase>Netflix</phrase> announced a \$1M prize competition to advance recommendation <phrase>algorithms</phrase>. The recommendation problem was simplified as the accuracy in predicting a user rating measured by the <phrase>Root</phrase> Mean Squared Error. While that formulation helped get the attention of the <phrase>research</phrase> <phrase>community</phrase>, it put the focus on the wrong approach and metric while leaving many important factors out. In this tutorial we will describe the advances in <phrase>Recommender Systems</phrase> in the last 10 years from an <phrase>industry</phrase> perspective based on the instructors' personal experience at companies like <phrase>Quora</phrase>, <phrase>LinkedIn</phrase>, <phrase>Netflix</phrase>, or <phrase>Yahoo</phrase>! We will do so in the form of different <phrase>lessons learned</phrase> through the years. Some of those lessons will describe the different components of modern <phrase>recommender systems</phrase> such as: personalized ranking, similarity, explanations, context-awareness, or multi-armed bandits. Others will also review the usage of novel algorithmic approaches such as Factorization Machines, <phrase>Restricted Boltzmann Machines</phrase>, SimRank, <phrase>Deep Neural Networks</phrase>, or Listwise Learning-to-rank. Others will dive into details of the importance of gathering the right <phrase>data</phrase> or using the correct optimization metric. But, most importantly, we will give many examples of prototypical <phrase>industrial</phrase>-scale <phrase>recommender systems</phrase> with special focus on those unsolved challenges that should define the future of the <phrase>recommender systems</phrase> <phrase>area</phrase>.
Modeling Temporal Dynamics with <phrase>Function</phrase> Approximation in Deep <phrase>Spatio-Temporal</phrase> Inference Network Biologically inspired deep <phrase>machine learning</phrase> is an emerging framework for dealing with complex <phrase>high</phrase>-dimensional <phrase>data</phrase>. An unsupervised <phrase>feature extraction</phrase> <phrase>deep learning</phrase> <phrase>architecture</phrase> called Deep <phrase>Spatio-Temporal</phrase> Inference Network (<phrase>DeSTIN</phrase>) utilizes a hierarchy of computational nodes, where each node features a common <phrase>algorithm</phrase> for inference of temporal patterns. The nodes all are geared to <phrase>online learning</phrase> and offer a generalization component which uses clustering and mixture models, as well as a temporal dynamics module. The latter is designed for tabular representation but such techniques are notoriously ill-suited for scaling as they impose an O(N 3) <phrase>memory</phrase> complexity. Instead, <phrase>function</phrase> approximation methods such as <phrase>neural networks</phrase> can serve as a more concise representation. In this work we present the <phrase>results</phrase> of <phrase>DeSTIN</phrase> on a popular problem, the MNIST <phrase>data set</phrase> of <phrase>handwritten digits</phrase>, using mixture models and <phrase>function</phrase> approximation to create a temporally evolving <phrase>feature representation</phrase>. We compare the <phrase>results</phrase> of the extracted features from <phrase>DeSTIN</phrase> under the tabular method and the <phrase>function</phrase> approximation method and contrast these <phrase>results</phrase> with our past work in this <phrase>area</phrase>.
Decoding the Value of <phrase>Computer Science</phrase> In The <phrase>Social Network</phrase>, a computer-<phrase>programming</phrase> <phrase>prodigy</phrase> goes to <phrase>Harvard</phrase> and creates a <phrase>technology</phrase> <phrase>company</phrase> in his sophomore dorm. Six year later, the <phrase>company</phrase> is worth billions and touches one out of every 14 people on <phrase>earth</phrase>. <phrase>Facebook</phrase> is a familiar <phrase>American</phrase> success story, with its founder, <phrase>Mark Zuckerberg</phrase>, following a path blazed by Bill Gates and others like him. But it may also become increasingly rare. Far fewer students are studying <phrase>computer science</phrase> in <phrase>college</phrase> than once did. This is a problem in more ways than one. The signs are everywhere. This year, for the first time in decades, the <phrase>College Board</phrase> failed to offer <phrase>high-school</phrase> students the <phrase>Advanced Placement</phrase> <phrase>AB</phrase> <phrase>Computer Science</phrase> exam. The number of <phrase>high schools</phrase> teaching <phrase>computer science</phrase> is shrinking, and last year only about 5,000 students <phrase>sat</phrase> for the <phrase>AB</phrase> <phrase>test</phrase>. Two decades ago, I was one of them. I have never held an <phrase>information-technology</phrase> job. Yet the more time <phrase>passes</phrase>, the more I understand how important that <phrase>education</phrase> was. Something is <phrase>lost</phrase> when students no longer study the working of things. My childhood interest in <phrase>programming</phrase> was a product of <phrase>nature</phrase> and nurture. My father worked as a computer <phrase>scientist</phrase>, first in a <phrase>university</phrase> and then as a researcher for <phrase>General Electric</phrase>. As a kid, I tagged along to his lab on weekends, watching him connect <phrase>single</phrase>-board DEC <phrase>computers</phrase> into ring networks and two-dimensional arrays, feeling the <phrase>ozone</phrase> hum of closet-sized machines. By my <phrase>adolescence</phrase>, in the mid-1980s, we had moved to a well-off <phrase>suburb</phrase> whose <phrase>high school</phrase> could afford its own <phrase>mainframe</phrase>. That plus social awkwardness meant many a night plugged into a 300-<phrase>baud</phrase> <phrase>modem</phrase>, battling other 15-year-olds in rudimentary deep-space combat and accumulating treasure in <phrase>Ascii</phrase>-rendered dungeons without end. Before <phrase>long</phrase> I wanted to understand where those <phrase>games</phrase> came from and how, exactly, they worked. So I took to <phrase>programming</phrase>, first in <phrase>Basic</phrase> and then Pascal. Coding taught me the shape of <phrase>logic</phrase>, the value of brevity, and the attention to detail that <phrase>debugging</phrase> requires. I learned that a beautiful program with a <phrase>single</phrase> misplaced <phrase>semicolon</phrase> is like a <phrase>sports car</phrase> with one <phrase>piston</phrase> out of line. Both are dead machines, functionally indistinguishable from junk. I learned that you are good enough to build things that do what they are supposed to do, or you are not. I left for <phrase>college</phrase> intending to <phrase>major</phrase> in <phrase>computer science</phrase>. That lasted until about 
Towards a <phrase>deep learning</phrase> approach to <phrase>brain</phrase> parcellation References Problem Establishing correspondences across structural and functional <phrase>brain</phrase> images via labeling, or parcellation, is an important and <phrase>challenging task</phrase> for clinical <phrase>neuroscience</phrase> and <phrase>cognitive psychology</phrase>. Limitations of existing approaches are that they i) possess shallow architectures, <phrase>ii</phrase>) are based on <phrase>heuristic</phrase> manual feature <phrase>engineering</phrase>, and iii) assume the validity of the designed feature <phrase>model</phrase>. Objective We <phrase>advocate</phrase> a <phrase>deep learning</phrase> and inference approach to automate <phrase>brain</phrase> parcellation. Approach We present a novel application of <phrase>convolutional networks</phrase> to build <phrase>discriminative features</phrase> for <phrase>brain</phrase> parcellation, which are automatically learned from <phrase>labels</phrase> provided by <phrase>human</phrase> experts.
Educationally critical aspects of a <phrase>deep understanding</phrase> of the concept of an <phrase>information</phrase> system This study identified and compared a <phrase>deep understanding</phrase> of the concept of an <phrase>information</phrase> system (IS) with the various levels of understanding of a group of <phrase>undergraduate</phrase> IS students. The aim was to identify aspects of the concept of an IS that were educationally critical to that development of a <phrase>deep understanding</phrase>. The study was significant in that the educationally critical aspects of the concept of an IS have not been researched previously. Yet, without addressing these aspects in <phrase>teaching and learning</phrase> the development of a <phrase>deep understanding</phrase> is unlikely. The <phrase>nature</phrase> of a <phrase>deep understanding</phrase> of the concept of an IS was established from the <phrase>literature</phrase>. For effective practice, an IS should be conceptualised as a social system supported by embedded <phrase>information technology</phrase>. A group of <phrase>undergraduate</phrase> students were interviewed and completed questionnaires about their understanding of the concept of an IS before, during and after a year of study. The <phrase>student</phrase> <phrase>data</phrase> was analysed using phenomenographic <phrase>research</phrase> techniques. Six qualitatively different levels of understanding of the concept of an IS were described. The students' levels of understanding were found to differ from the desired understanding with respect to a number of educationally critical aspects. These differences involved both social and technical aspects of an IS and the relationship between the social and technical aspects. The identification of the educationally critical aspects of the concept of an IS was significant in that learning tasks can now be designed which enhance students' likelihood of developing an appropriate understanding.
A <phrase>Machine Learning</phrase> Approach For Opinion Holder Extraction In <phrase>Arabic Language</phrase> Opinion <phrase>mining</phrase> aims at extracting useful subjective <phrase>information</phrase> from reliable amounts of text. Opinion <phrase>mining</phrase> holder recognition is a task that has not been considered yet in <phrase>Arabic Language</phrase>. This task essentially requires <phrase>deep understanding</phrase> of clauses structures. Unfortunately, the lack of a robust, publicly available, <phrase>Arabic</phrase> parser further complicates the <phrase>research</phrase>. This <phrase>paper</phrase> presents a leading <phrase>research</phrase> for the opinion holder extraction in <phrase>Arabic</phrase> <phrase>news</phrase> <phrase>independent</phrase> from any lexical parsers. We investigate constructing a <phrase>comprehensive</phrase> feature set to compensate the lack of <phrase>parsing</phrase> structural outcomes. The proposed feature set is tuned from <phrase>English</phrase> previous works coupled with our proposed <phrase>semantic field</phrase> and named entities features. Our feature analysis is based on <phrase>Conditional Random Fields</phrase> (CRF) and <phrase>semi-supervised</phrase> <phrase>pattern recognition</phrase> techniques. Different <phrase>research</phrase> models are evaluated via <phrase>cross-validation</phrase> experiments achieving 54.03 F-measure. We publicly release our own <phrase>research</phrase> outcome corpus and <phrase>lexicon</phrase> for opinion <phrase>mining</phrase> <phrase>community</phrase> to encourage further <phrase>research</phrase>.
Optimal Support Features for Meta-Learning Meta-learning has many aspects, but its final goal is to discover in an automatic way many interesting models for a given <phrase>data</phrase>. Our early attempts in this <phrase>area</phrase> involved heterogeneous learning systems combined with a complexity-guided search for optimal models, performed within the framework of (dis)similarity based methods to discover " <phrase>knowledge</phrase> granules ". This approach, inspired by <phrase>neurocognitive</phrase> mechanisms of <phrase>information processing</phrase> in the <phrase>brain</phrase>, is generalized here to learning based on parallel chains of transformations that extract useful <phrase>information</phrase> granules and use it as additional features. Various types of transformations that generate hidden features are analyzed and methods to generate them are discussed. They include restricted random projections, optimization of these features using projection pursuit methods, similarity-based and <phrase>general</phrase> <phrase>kernel-based</phrase> features, conditionally defined features, features derived from partial successes of various <phrase>learning algorithms</phrase>, and using the whole learning models as new features. In the enhanced <phrase>feature space</phrase> the goal of learning is to create image of the <phrase>input data</phrase> that can be directly handled by relatively simple decision processes. The focus is on hierarchical methods for generation of <phrase>information</phrase>, starting from new support features that are discovered by different types of <phrase>data</phrase> models created on similar tasks and successively building more complex features on the enhanced feature spaces. Resulting <phrase>algorithms</phrase> facilitate <phrase>deep learning</phrase>, and also enable understanding of structures present in the <phrase>data</phrase> by visualization of the <phrase>results</phrase> of <phrase>data</phrase> transformations and by creating logical , fuzzy and <phrase>prototype</phrase>-based rules based on new features. Relations to various <phrase>machine-learning</phrase> approaches, comparison of <phrase>results</phrase>, and <phrase>neurocognitive</phrase> inspirations for meta-learning are discussed.
<phrase>Graph</phrase> Based <phrase>Convolutional Neural Network</phrase> The benefit of localized features within the regular domain has given rise to the use of <phrase>Convolutional Neural Networks</phrase> (CNNs) in <phrase>machine learning</phrase>, with great proficiency in the <phrase>image classification</phrase>. The use of CNNs becomes problematic within the irregular spatial domain due to <phrase>design</phrase> and <phrase>convolution</phrase> of a kernel filter being non-trivial. One <phrase>solution</phrase> to this problem is to utilize <phrase>graph</phrase> <phrase>signal processing</phrase> techniques and the <phrase>convolution theorem</phrase> to perform convolutions on the <phrase>graph</phrase> of the irregular domain to obtain feature map responses to learnt filters. We propose <phrase>graph</phrase> <phrase>convolution</phrase> and pooling operators analogous to those in the regular domain. We also provide <phrase>gradient</phrase> calculations on the <phrase>input data</phrase> and spectral filters, which allow for the <phrase>deep learning</phrase> of an irregular spatial domain problem. Signal filters take the form of spectral multipliers, applying <phrase>convolution</phrase> in the <phrase>graph</phrase> spectral domain. Applying smooth multipliers <phrase>results</phrase> in localized convo-lutions in the spatial domain, with smoother multipliers providing sharper feature maps. <phrase>Algebraic</phrase> Multigrid is presented as a <phrase>graph</phrase> pooling method, reducing the resolution of the <phrase>graph</phrase> through <phrase>agglomeration</phrase> of nodes between layers of the network. Evaluation of performance on the MNIST digit <phrase>classification problem</phrase> in both the regular and irregular domain is presented, with comparison drawn to standard <phrase>CNN</phrase>. The proposed <phrase>graph</phrase> <phrase>CNN</phrase> provides a <phrase>deep learning</phrase> method for the irregular domains present in the <phrase>machine learning</phrase> <phrase>community</phrase>, obtaining 94.23% on the regular grid, and 94.96% on a spatially irregular subsampled MNIST.
Keeping It Personal: Self-generated Learning Tools for Lifelong <phrase>Professional</phrase> Development <phrase>Journal</phrase> Article Keeping It Personal: Self-generated Learning Tools for Lifelong <phrase>Professional</phrase> Development (2010). Keeping it personal: self-generated learning tools for lifelong <phrase>professional</phrase> development. <phrase>Copyright</phrase> and <phrase>Moral Rights</phrase> for the articles on this site are retained by the individual authors and/or other <phrase>copyright</phrase> owners. For more <phrase>information</phrase> on Open <phrase>Research</phrase> Online's <phrase>data</phrase> policy on reuse of materials please consult the policies page. Abstract <phrase>Approaches to learning</phrase> in the twenty-first century need to reflect <phrase>student</phrase> diversity in <phrase>order</phrase> to widen and sustain participation in <phrase>education</phrase> and <phrase>continuing professional development</phrase> (CPD). There is some evidence of success in widening access to <phrase>professional</phrase> social care <phrase>employment</phrase> through training and qualification programmes but a notable lack of success in sustaining this participation into CPD and lifelong <phrase>professional</phrase> learning. This <phrase>paper</phrase> argues for an increased contribution of more personalised, self-generated learning approaches to <phrase>sustainable</phrase> participation through the development of the <phrase>Professional</phrase> Identity and Values Organisation Tools (PIVOT). The tools have been developed by the authors from the constructivist <phrase>teaching and learning</phrase> insights of Personal Construct <phrase>Psychology</phrase> and are freely available to download and use (see below). The intention is to enhance reflection for <phrase>professional</phrase> learners and practitioners through self-generated personal constructs and values and the creation of self-<phrase>directed</phrase> learning aims. We argue that the inclusion of enhanced reflective techniques such as PIVOT helps to facilitate a deep, personal engagement in the processes of learning to learn and builds a foundation for sustained participation in and ownership of students' and practitioners' CPD and <phrase>lifelong learning</phrase>.
Supporting children's learning with body-based <phrase>metaphors</phrase> in a <phrase>mixed reality</phrase> environment We describe an approach to designing immersive <phrase>learning experiences</phrase> for children using <i>body-based <phrase>metaphors</phrase></i>. Previous <phrase>research</phrase> shows benefits for learning through physical interactions in virtual spaces (e.g., [1, 16])---here we look specifically at using <phrase>mixed reality</phrase> to embed children as elements within the systems they are attempting to learn. Using gross body-movements the children are able to <phrase>test</phrase> predictions and have their intuitions challenged, laying the foundation for deeper <phrase>conceptual understanding</phrase>. We present <phrase>data</phrase> from a study we conducted comparing the <phrase>mixed reality</phrase> experience with a desktop version of the same <phrase>simulation</phrase>. <phrase>Results</phrase> suggest that children's interactions with designs supporting body-based <phrase>metaphors</phrase> can <phrase>lead</phrase> them to better grasp the "deep structure" of the learning domain.
Multi-column <phrase>Deep Neural Networks</phrase> for <phrase>Image Classification</phrase> Multi-column <phrase>Deep Neural Networks</phrase> for <phrase>Image Classification</phrase> in 1988 by the Dalle <phrase>Molle</phrase> Foundation which promoted quality of <phrase>life</phrase>. Abstract Traditional methods of <phrase>computer vision</phrase> and <phrase>machine learning</phrase> cannot match <phrase>human</phrase> performance on tasks such as the recognition of <phrase>handwritten digits</phrase> or traffic signs. Our biologically plausible deep <phrase>artificial neural network</phrase> architectures can. Small (often minimal) <phrase>receptive fields</phrase> of convolutional winner-take-all <phrase>neurons</phrase> yield large network depth, resulting in roughly as many sparsely connected neural layers as found in <phrase>mammals</phrase> between <phrase>retina</phrase> and <phrase>visual cortex</phrase>. Only winner <phrase>neurons</phrase> are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. <phrase>Graphics cards</phrase> allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-<phrase>human</phrase> performance. On a <phrase>traffic sign</phrase> recognition benchmark it out-performs humans by a factor of two. We also improve the <phrase>state</phrase>-of-the-<phrase>art</phrase> on a plethora of common <phrase>image classification</phrase> benchmarks.
Exploiting Scene Context for Image Captioning This <phrase>paper</phrase> presents a framework for image captioning by exploiting the scene context. To date, most of the captioning models have been relying on the combination of <phrase>Convolutional Neural Networks</phrase> (<phrase>CNN</phrase>) and the <phrase>Long</phrase>-<phrase>Short Term Memory</phrase> (LSTM) <phrase>model</phrase>, trained in an <phrase>end-to-end</phrase> <phrase>fashion</phrase>. Recently, there has been extensive <phrase>research</phrase> towards improving the <phrase>language</phrase> <phrase>model</phrase> and the <phrase>CNN</phrase> <phrase>architecture</phrase>, utilizing attention mechanisms, and improving the learning techniques in such systems. A less studied <phrase>area</phrase> is the contribution of the scene context in the captioning. In this work, we study the role of the scene context, consisting of the scene type and objects. To this end, we augment the <phrase>CNN</phrase> features with scene context features, including scene detectors, objects and their localization, and their combinations. We use the scene context features as an initialization feature at the zeroth time step in a LSTM <phrase>model</phrase> with deep residual connections. In subsequent time steps, the <phrase>model</phrase>, however, uses the original <phrase>CNN</phrase> features. The proposed <phrase>language</phrase> <phrase>model</phrase>, contrary to more conventional ones, thus has access to <phrase>visual features</phrase> through the whole process of sentence generation. We demonstrate that the scene context features affect the <phrase>language</phrase> formation and improve the captioning <phrase>results</phrase> in the <phrase>proposed framework</phrase>. We also <phrase>report</phrase> <phrase>results</phrase> from the <phrase>Microsoft</phrase> COCO benchmark, where our <phrase>model</phrase> achieves the <phrase>state</phrase>-of-the-<phrase>art</phrase> performance on the <phrase>test</phrase> set.
A Motion <phrase>Graph</phrase> Approach for Interactive 3d <phrase>Animation</phrase> Using <phrase>Low-cost</phrase> Sensors a Motion <phrase>Graph</phrase> Approach for Interactive 3d <phrase>Animation</phrase> Using <phrase>Low-cost</phrase> Sensors Interactive 3D <phrase>animation</phrase> of <phrase>human</phrase> figures is very common in <phrase>video games</phrase>, <phrase>animation</phrase> studios and virtual environments. However, it is difficult to produce full body <phrase>animation</phrase> that looks realistic enough to be comparable to studio quality <phrase>human</phrase> motion <phrase>data</phrase>. The commercial <phrase>motion capture</phrase> systems are expensive and not suitable for capture in everyday environments. Real-time requirements tend to reduce quality of <phrase>animation</phrase>. We present a motion <phrase>graph</phrase> based framework to produce <phrase>high</phrase> quality motion sequences in real-time using a set of inertial <phrase>sensor</phrase> based controllers. The user's <phrase>action</phrase> generates signals from the controllers that provide constraints to select appropriate <phrase>sequence</phrase> of motions from a structured <phrase>database</phrase> of <phrase>human</phrase> motions, namely motion <phrase>graph</phrase>. Our <phrase>local search</phrase> <phrase>algorithm</phrase> utilizes noise prone and rapidly varying input <phrase>sensor</phrase> signals for querying a large <phrase>database</phrase> in real-time. The ability to waive the controllers for producing <phrase>high</phrase> quality <phrase>animation</phrase> provides a simple 3D <phrase>user interface</phrase> that is intuitive to use. The <phrase>proposed framework</phrase> is <phrase>low cost</phrase> and easy to setup. Dedication To my Mom and Dad for the wonderful <phrase>education</phrase> they have given me To my <phrase>family</phrase> and <phrase>friends</phrase>, who have supported me always To the thirty two <phrase>Hokies</phrase> who <phrase>lost</phrase> their lives on 16th April, 2007 iii Acknowledgments It has been an amazing learning experience for me at <phrase>Virginia Tech</phrase>, and working on this <phrase>thesis</phrase> was no different. I have had the opportunity to work with the best minds, collaborate with several teams and get help and insightful suggestions. This <phrase>thesis</phrase> would have been incomplete without the help of many people to whom I would like to <phrase>express my deep</phrase> and sincere gratitude. I would like to thank my advisor Dr. Yong Cao, for his guidance, inspiration, support, and his friendship. It has been an amazing learning experience working along with him. He has always motivated me to think deep and to question every idea. He has provided me with all the resources required for this work-in the form of ideas, equipments and graduate assistantship. I would like to express my sincere thanks to my committee members Dr. Doug Bowman and Dr. Dane Webster for their regular guidance and diverse thoughts. Any conversation with them has inspired me to think differently. Their views have had a remarkable influence on my <phrase>research</phrase> and their comments helped me shape this <phrase>thesis</phrase>. Dr. Eli Tilevich has helped me understand the <phrase>software engineering</phrase> principles that made the development process 
Learning <phrase>Neural Network</phrase> Architectures using <phrase>Backpropagation</phrase> <phrase>Deep neural networks</phrase> with millions of parameters are at the <phrase>heart</phrase> of many <phrase>state</phrase> of the <phrase>art</phrase> <phrase>machine learning</phrase> models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of <phrase>architecture</phrase>-learning, i.e; learning the <phrase>architecture</phrase> of a <phrase>neural network</phrase> along with weights. We start with a large <phrase>neural network</phrase>, and then learn which <phrase>neurons</phrase> to <phrase>prune</phrase>. To this end, we introduce a new trainable parameter called the Tri-<phrase>State</phrase> ReLU, which helps in <phrase>pruning</phrase> unnecessary <phrase>neurons</phrase>. We also propose a smooth regular-izer which encourages the total number of <phrase>neurons</phrase> after elimination to be small. The resulting objective is <phrase>differentiable</phrase> and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with considerably smaller number of parameters without affecting prediction accuracy.
Sparse Penalty in <phrase>Deep Belief</phrase> Networks: Using the Mixed Norm Constraint <phrase>Deep Belief</phrase> Networks (DBN) have been successfully applied on popular <phrase>machine learning</phrase> tasks. Specifically, when applied on <phrase>handwritten digit</phrase> recognition , DBNs have achieved approximate accuracy rates of 98.8%. In an effort to optimize the <phrase>data</phrase> representation achieved by the DBN and maximize their descriptive power, <phrase>recent advances</phrase> have focused on inducing sparse constraints at each layer of the DBN. In this <phrase>paper</phrase> we present a theoretical approach for sparse constraints in the DBN using the mixed norm for both non-overlapping and overlapping groups. We explore how these constraints affect the <phrase>classification accuracy</phrase> for digit recognition in three different datasets (MNIST, <phrase>USPS</phrase>, RIMES) and provide initial estimations of their usefulness by altering different parameters such as the group size and overlap percentage.
<phrase>Project Management</phrase> Office (pmo) in International <phrase>Arena</phrase> <phrase>Lessons Learned</phrase> from Pmo's Closed-loop Control Given the popularity of PMO and international projects, and the difficulties experienced by PMO, it is important and necessary to study PMO in a global context. This <phrase>paper</phrase> raises a challenge for a PMO who uses <phrase>traditional approaches</phrase> to supervise international projects characterized with external embeddedness. Would such a character demand this PMO be operated differently to adapt to environment? In searching for the answers, we use <phrase>case study</phrase> method, which enables us to gain a <phrase>deep understanding</phrase> of the impacts of external embeddedness on the efficacy of PMO's control mechanism. The <phrase>results</phrase> show that a PMO should open its control loop to external network and promote procedural <phrase>justice</phrase> in managing international projects. By expanding upon the existing PMO <phrase>research</phrase> to include an <phrase>adaptive control</phrase> approach for managing international projects, this <phrase>research</phrase> would advance our <phrase>knowledge</phrase> of PMOs and may help us decode some of PMOs' difficulties.
<phrase>Data</phrase>-driven soft <phrase>sensor</phrase> development based on <phrase>deep learning</phrase> technique In <phrase>industrial</phrase> <phrase>process control</phrase>, some product qualities and key variables are always difficult to measure online due to technical or economic limitations. As an effective <phrase>solution</phrase>, <phrase>data</phrase>-driven soft sensors provide stable and reliable online estimation of these variables based on historical measurements of easy-to-measure process variables. <phrase>Deep learning</phrase>, as a novel training strategy for <phrase>deep neural networks</phrase>, has recently become a popular <phrase>data</phrase>-driven approach in the <phrase>area</phrase> of <phrase>machine learning</phrase>. In the present study, the <phrase>deep learning</phrase> technique is employed to build soft sensors and applied to an <phrase>industrial</phrase> case to estimate the heavy <phrase>diesel</phrase> 95% cut point of a crude <phrase>distillation</phrase> unit (<phrase>CDU</phrase>). The comparison of modeling <phrase>results</phrase> demonstrates that the <phrase>deep learning</phrase> technique is especially suitable for soft <phrase>sensor</phrase> modeling because of the following advantages over traditional methods. First, with a complex <phrase>multi-layer</phrase> structure, the <phrase>deep neural network</phrase> is able to contain richer <phrase>information</phrase> and yield improved representation ability compared with traditional <phrase>data</phrase>-driven models. Second, <phrase>deep neural networks</phrase> are established as <phrase>latent variable</phrase> models that help to describe highly correlated process variables. Third, the <phrase>deep learning</phrase> is <phrase>semi-supervised</phrase> so that all available process <phrase>data</phrase> can be utilized. Fourth, the <phrase>deep learning</phrase> technique is particularly efficient dealing with massive <phrase>data</phrase> in practice. Soft sensors have been extensively studied and implemented in the process industries over the past two decades. Typically, they are predictive models based on massive amounts of <phrase>data</phrase> available in the <phrase>industrial</phrase> processes, and are mainly responsible for online predictions of some variables that <phrase>play</phrase> an indispensable role in <phrase>quality control</phrase> as well as <phrase>production</phrase> <phrase>safety</phrase>, for the reason that hardware measuring instruments are unavailable or costly [1]. In <phrase>general</phrase>, one can broadly classify soft sensors into two types, namely, the first-principle models (<phrase>white</phrase>-box models) and <phrase>data</phrase>-driven models (<phrase>black</phrase>-box models). First-principle models are dependent on a prior mechanical <phrase>knowledge</phrase> and thus often unavailable since <phrase>industrial</phrase> processes are commonly too complicated to analyze, making the mechanical <phrase>knowledge</phrase> rather hard-won. Their <phrase>data</phrase>-driven counterparts, alternatively, give empirical models based on the historical <phrase>data</phrase> collected in the <phrase>industrial</phrase> process. Owing to their practical utility and <phrase>independence</phrase> on a priori <phrase>knowledge</phrase>, <phrase>data</phrase>-driven soft sensors have increasingly established themselves as popular and effective approaches [2,3]. A wide <phrase>variety</phrase> of <phrase>statistical inference</phrase> techniques and <phrase>machine learning</phrase> techniques have been employed in <phrase>data</phrase>-driven soft sensors, among which representative examples are <phrase>principal component</phrase> <phrase>regression</phrase> (<phrase>PCR</phrase>) that incorporates <phrase>principal component analysis</phrase> (<phrase>PCA</phrase>) with a <phrase>regression</phrase> <phrase>model</phrase>, partial 
<phrase>VLSI</phrase> circuit defect diagnosis: open defects and run-time speed Recommended Citation Liu, Chen. "<phrase>VLSI</phrase> circuit defect diagnosis: open defects and run-time speed." <phrase>PhD</phrase> (Doctor of <phrase>Philosophy</phrase>) <phrase>thesis</phrase>, ABSTRACT To shorten time-to-market of <phrase>VLSI</phrase> circuit chips, the yield must be ramped up by quickly discovering and rectifying the causes for systematic defects. Due to the shrinking feature size of devices 90nm and below, yield ramp up is becoming more and more difficult. Volume diagnosis with statistical learning is needed to cost effectively discover systematic defects. An accurate and <phrase>high</phrase> throughput diagnosis tool is required to diagnose large numbers of failing devices to aid statistical yield learning. In this work, we propose techniques to improve diagnosis accuracy and resolution, techniques to improve run-time performance. We consider the problem of determining the location of open defects in interconnects of <phrase>deep submicron</phrase> designs. We investigate a procedure that uses minimal <phrase>information</phrase> beyond the circuit net lists and give <phrase>experimental</phrase> <phrase>results</phrase> to demonstrate the defect resolution obtained using the method. The additional <phrase>information</phrase> used by the <phrase>proposed method</phrase> is a list of nodes in the neighborhoods of circuit nodes and the circuit layout. Specifically, difficult to determine circuit parameters of manufactured instances of a <phrase>design</phrase> such as coupling capacitances between circuit nodes and threshold voltages of gates in the circuit are not needed to use the proposed diagnosis procedure. A <phrase>dictionary</phrase> called N FB <phrase>dictionary</phrase> of small size and does not grow linearly with pattern <phrase>count</phrase> is proposed. It further reduced <phrase>dictionary</phrase> size over previous <phrase>dictionary</phrase> while still achieve higher failing pattern diagnosis performance than <phrase>industry</phrase> standard Effect-Cause diagnosis procedures. In this work we also propose a method to achieve higher speedup with a marginally larger <phrase>dictionary</phrase> than the N FB <phrase>dictionary</phrase>. We achieve this by identifying a set of faults called hyperactive faults for which we create a novel <phrase>dictionary</phrase>. Hyperactive faults tend to propagate fault effects to many observation points and cost a large amount of time to simulate. 2 In addition to speed-up of failing pattern diagnosis, we propose a method to improve passing pattern performance. A <phrase>pass</phrase>-fail <phrase>dictionary</phrase> with <phrase>high</phrase> <phrase>compression ratio</phrase> is proposed. The <phrase>dictionary</phrase> is stored in a <phrase>database</phrase> on disk with a small cache <phrase>memory</phrase> and <phrase>high</phrase> diagnosis performance is demonstrated. <phrase>ii</phrase> To my <phrase>family</phrase> iii ACKNOWLEDGMENTS
Deep <phrase>Hashing</phrase> Network for Efficient Similarity Retrieval Due to the storage and retrieval efficiency, <phrase>hashing</phrase> has been widely deployed to approximate <phrase>nearest neighbor search</phrase> for <phrase>large-scale</phrase> <phrase>multimedia</phrase> retrieval. Supervised <phrase>hashing</phrase>, which improves the quality of hash coding by exploiting the <phrase>semantic</phrase> similarity on <phrase>data</phrase> pairs, has received increasing attention recently. For most existing supervised <phrase>hashing</phrase> methods for <phrase>image retrieval</phrase>, an image is first represented as a <phrase>vector</phrase> of <phrase>hand-crafted</phrase> or machine-<phrase>learned features</phrase>, followed by another separate quantization step that generates <phrase>binary</phrase> codes. However, suboptimal hash coding may be <phrase>produced</phrase>, because the quantization error is not statistically minimized and the <phrase>feature representation</phrase> is not optimally compatible with the <phrase>binary</phrase> coding. In this <phrase>paper</phrase>, we propose a novel Deep Hash-ing Network (DHN) <phrase>architecture</phrase> for supervised <phrase>hashing</phrase>, in which we jointly learn good image representation tailored to hash coding and formally control the quantization error. The DHN <phrase>model</phrase> constitutes four key components: (1) a sub-network with multiple <phrase>convolution</phrase>-pooling layers to capture <phrase>image representations</phrase>; (2) a <phrase>fully-connected</phrase> <phrase>hashing</phrase> layer to generate compact <phrase>binary</phrase> hash codes; (3) a pairwise cross-<phrase>entropy</phrase> loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling <phrase>hashing</phrase> quality. <phrase>Extensive experiments</phrase> on standard <phrase>image retrieval</phrase> datasets show the proposed DHN <phrase>model</phrase> yields substantial boosts over latest <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>hashing</phrase> methods.
Word Recognition with Deep <phrase>Conditional Random Fields</phrase> Recognition of handwritten words continues to be an important problem in document analysis and recognition. Existing approaches extract hand-engineered features from word imageswhich can perform poorly with new <phrase>data</phrase> sets. Recently , <phrase>deep learning</phrase> has attracted great attention because of the ability to learn features from raw <phrase>data</phrase>. Moreover they have yielded <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> in <phrase>classification tasks</phrase> including <phrase>character recognition</phrase> and scene recognition. On the other hand, word recognition is a sequential problem where we need to <phrase>model</phrase> the correlation between characters. In this <phrase>paper</phrase>, we propose using deep <phrase>Conditional Random Fields</phrase> (deep CRFs) for word recognition. Basically, we combine CRFs with <phrase>deep learning</phrase>, in which deep features are learned and sequences are labeled in a unified framework. We pre-<phrase>train</phrase> the deep structure with stacked <phrase>restricted Boltzmann machines</phrase> (RBMs) for <phrase>feature learning</phrase> and optimize the entire network with an <phrase>online learning</phrase> <phrase>algorithm</phrase>. The proposed <phrase>model</phrase> was evaluated on two datasets, and seen to perform significantly better than competitive baseline models.
Explaining across contrasting cases for <phrase>deep understanding</phrase> in <phrase>science</phrase>: an example using interactive simulations <phrase>Undergraduate</phrase> students used a <phrase>simulation</phrase> to learn about <phrase>electromagnetic</phrase> <phrase>flux</phrase>. They were provided with three simulated cases that illustrate how changes in <phrase>flux</phrase> induce current in a coil. In the <phrase>POE</phrase> condition, students predicted, observed, and explained the outcome of each case, treating each case separately. In the <phrase>GE</phrase> condition, students were asked to produce a <phrase>general</phrase> explanation that would work for all three cases. A second factor crossed whether students had access to a numerical measurement tool. Effects of the measurement tool were less conclusive, but there was a strong effect of instructional method. Compared to <phrase>POE</phrase> students, <phrase>GE</phrase> students were better able to induce an underlying principle of <phrase>electromagnetic</phrase> <phrase>flux</phrase> during instruction and were better able to apply this principle to novel problems at post-<phrase>test</phrase>. Moreover, prior achievement predicted learning in the <phrase>POE</phrase> group, while students of all <phrase>academic</phrase> levels benefited equally from the <phrase>GE</phrase> condition. <phrase>Science</phrase> <phrase>education</phrase> has learning goals that <phrase>range</phrase> from <phrase>basic</phrase> lab skills to beliefs about the sources of <phrase>scientific knowledge</phrase>. One enduring goal is for students to develop a <phrase>deep understanding</phrase> of phenomena so they can engage in the structure of scientific explanation. One way to characterize <phrase>deep understanding</phrase> is the capability and disposition to perceive and explain natural phenomena in terms of <phrase>general</phrase> principles. In this study, we show that <phrase>deep understanding</phrase> can depend critically on the way in which multiple instances of phenomena are presented to students and how students are instructed to explain those instances. The <phrase>research</phrase> is done in the context of <phrase>undergraduate</phrase> <phrase>physics</phrase> students learning about <phrase>magnetic flux</phrase> with a computer <phrase>simulation</phrase>. It is common in <phrase>science</phrase> instruction to ask students to solve or conceptually explain a series of problems. One version of this approach is the Predict-Observe-Explain (<phrase>POE</phrase>) cycle (<phrase>White</phrase> & Gunstone, 1992). Students receive the setup of an experiment and predict what will happen. They then observe the outcome and develop an explanation for why their prediction did or did not match the expected outcome. For <phrase>POE</phrase> and other sequenced formats, a series of questions or examples is carefully selected to help students instantiate a given core principle in multiple contexts, so that they develop a deeper, more abstract sense of the principle and learn the kinds of situations to which it applies. Formats such as <phrase>POE</phrase> are considered to be effective in part because they foster deep and often extended engagement with each new question 
An <phrase>Information</phrase> <phrase>Geometry</phrase> of Statistical <phrase>Manifold</phrase> Learning <phrase>Manifold</phrase> learning seeks <phrase>low-dimensional</phrase> representations of <phrase>high</phrase>-dimensional <phrase>data</phrase>. The main tactics have been exploring the <phrase>geometry</phrase> in an <phrase>input data</phrase> space and an output embedding space. We develop a <phrase>manifold</phrase> learning theory in a <phrase>hypothesis</phrase> space consisting of models. A <phrase>model</phrase> means a specific instance of a collection of points, e.g., the <phrase>input data</phrase> collectively or the output embedding collectively. The semi-<phrase>Riemannian</phrase> metric of this <phrase>hypothesis</phrase> space is uniquely derived in <phrase>closed form</phrase> based on the <phrase>information</phrase> <phrase>geometry</phrase> of <phrase>probability distributions</phrase>. There, <phrase>manifold</phrase> learning is interpreted as a tra-jectory of intermediate models. The volume of a continuous <phrase>region</phrase> reveals an amount of <phrase>information</phrase>. It can be measured to define <phrase>model</phrase> complexity and embedding quality. This provides deep unified perspectives of <phrase>manifold</phrase> learning theory. <phrase>Manifold</phrase> learning (MAL), or non-linear <phrase>dimensionality reduction</phrase> , assumes that some given <phrase>high</phrase>-dimensional observations y 1 ,. .. , y n D lie around a <phrase>low-dimensional</phrase> sub-<phrase>manifold</phrase> {(z) : z d } induced by a smooth mapping : d D (d D). While it is possible to learn a parametric form of (Hinton & Salakhutdinov, 2006), the majority of <phrase>manifold</phrase> learners are non-parametric. They learn directly a set of <phrase>low-dimensional</phrase> coordinates {z i } n i=1 to preserve certain <phrase>information</phrase> in {y i } n i=1. Depending on the choice of <phrase>information</phrase> to be preserved, at least two families of MAL methods thrived in the last decade. represent the <phrase>family</phrase> with natural convex formulations. They only preserve encodings of local <phrase>informa</phrase>
Trust Negotiation with Hidden Credentials, Hidden Policies, and Policy Cycles In an open environment such as the <phrase>Internet</phrase>, the decision to collaborate with a stranger (e.g., by granting access to a resource) is often based on the characteristics (rather than the identity) of the requester, via <phrase>digital</phrase> credentials: Access is granted if Alice's credentials satisfy Bob's access policy. The <phrase>literature</phrase> contains many examples where protecting the credentials and the <phrase>access control</phrase> policies is useful, and there are numerous protocols that achieve this. In many of these schemes, the server does not learn whether the client obtained access (e.g., to a message, or a service via an <phrase>e</phrase>-ticket). A consequence of this <phrase>property</phrase> is that the client can use all of her credentials without fear of " probing " attacks by the server, because the server cannot glean <phrase>information</phrase> about which credentials the client has (when this <phrase>property</phrase> is lacking, the <phrase>literature</phrase> uses a framework where the very use of a credential is subject to a policy specific to that credential). The main result of this <phrase>paper</phrase> is a protocol for negotiating trust between Alice and Bob without revealing either credentials or policies, when each credential has its own access policy associated with it (e.g., " a top-secret clearance credential can only be used when the other <phrase>party</phrase> is a <phrase>government</phrase> employee and has a top-secret clearance "). Our protocol carries out this <phrase>privacy</phrase>-preserving trust negotiation between Alice and Bob, while enforcing each creden-tial's policy (thereby protecting sensitive credentials). Note that there can be a deep nesting of dependencies between credential policies, and that there can be (possibly overlapping) policy cycles of these dependencies. Our result is not achieved through the routine use of standard techniques to implement, in this framework, one of the known strategies for trust negotiations (such as the " eager strategy "). Rather, this <phrase>paper</phrase> uses novel techniques to implement a non-standard trust negotiation strategy specifically suited to this framework (and in fact unusable outside of this framework, as will become clear). Our work is therefore a substantial extension of the <phrase>state</phrase>-of-the-<phrase>art</phrase> in <phrase>privacy</phrase>-preserving trust negotiations.
Deep assessment of <phrase>machine learning</phrase> techniques using patient treatment in acute <phrase>abdominal pain</phrase> in children Learning from patient records may aid <phrase>knowledge</phrase> acquisition and <phrase>decision making</phrase>. Existing inductive <phrase>machine learning</phrase> (ML) systems such us NewId, CN2, C4.5 and AQ15 learn from past <phrase>case histories</phrase> using symbolic and/or numeric values. These systems learn symbolic rules (IF... THEN like) which link an antecedent set of clinical factors to a consequent class or decision. This <phrase>paper</phrase> compares the learning performance of <phrase>alternative</phrase> ML systems with each other and with respect to a novel approach using <phrase>logic</phrase> minimization, called LML, to learn from <phrase>data</phrase>. Patient cases were taken from the archives of the Paediatric <phrase>Surgery</phrase> Clinic of the <phrase>University</phrase> <phrase>Hospital</phrase> of <phrase>Crete</phrase>, <phrase>Heraklion</phrase>, <phrase>Greece</phrase>. Comparison of ML system performance is based both on <phrase>classification accuracy</phrase> and on informal expert assessment of learned <phrase>knowledge</phrase>.
<phrase>DL</phrase>-<phrase>SFA</phrase>: Deeply-Learned Slow Feature Analysis for <phrase>Action</phrase> Recognition Most of the previous work on <phrase>video</phrase> <phrase>action</phrase> recognition use complex hand-designed local features, such as SIFT, HOG and SURF, but these approaches are implemented sophisticatedly and difficult to be extended to other <phrase>sensor</phrase> modalities. Recent studies discover that there are no universally best hand-engineered features for all datasets, and learning features directly from the <phrase>data</phrase> may be more advantageous. One such endeavor is Slow Feature Analysis (<phrase>SFA</phrase>) proposed by Wiskott and Sejnowski [34]. <phrase>SFA</phrase> can learn the invariant and slowly varying features from input signals and has been proved to be valuable in <phrase>human</phrase> <phrase>action</phrase> recognition [35]. It is also observed that the <phrase>multi-layer</phrase> <phrase>feature representation</phrase> has succeeded remarkably in widespread <phrase>machine learning</phrase> applications. In this <phrase>paper</phrase>, we propose to combine <phrase>SFA</phrase> with <phrase>deep learning</phrase> techniques to learn hierarchical representations from the <phrase>video</phrase> <phrase>data</phrase> itself. Specifically, we use a two-layered <phrase>SFA</phrase> learning structure with 3D con-volution and max pooling operations to scale up the method to large inputs and capture abstract and structural features from the <phrase>video</phrase>. Thus, the <phrase>proposed method</phrase> is suitable for <phrase>action</phrase> recognition. At the same time, sharing the same merits of <phrase>deep learning</phrase>, the <phrase>proposed method</phrase> is generic and fully automated. Our classification <phrase>results</phrase> on Hollywood2, <phrase>KTH</phrase> and <phrase>UCF</phrase> <phrase>Sports</phrase> are competitive with previously <phrase>published results</phrase>. To highlight some, on the <phrase>KTH</phrase> dataset, our recognition rate shows approximately 1% improvement in comparison to <phrase>state</phrase>-of-the-<phrase>art</phrase> methods even without supervision or dense sampling.
Boosting Convolutional Features for Robust Object Proposals <phrase>Deep Convolutional</phrase> <phrase>Neural Networks</phrase> (CNNs) have demonstrated excellent performance in <phrase>image classification</phrase>, but still show room for improvement in <phrase>object-detection</phrase> tasks with many categories, in particular for cluttered scenes and occlusion. Modern detection <phrase>algorithms</phrase> like Regions with CNNs (Girshick et al., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regions which with <phrase>high</phrase> <phrase>probability</phrase> represent objects , where in turn CNNs are deployed for classification. Selective Search represents a <phrase>family</phrase> of sophisticated <phrase>algorithms</phrase> that are engineered with multiple segmentation, appearance and saliency cues, typically coming with a significant run-time overhead. Furthermore, (Hosang et al., 2014) have shown that most methods suffer from low <phrase>reproducibility</phrase> due to unstable superpixels, even for slight image perturbations. Although CNNs are subsequently used for classification in top-performing <phrase>object-detection</phrase> pipelines, current proposal methods are <phrase>agnostic</phrase> to how these models parse objects and their rich learned representations. As a result they may propose regions which may not resemble <phrase>high</phrase>-level objects or totally miss some of them. To overcome these drawbacks we propose a boosting approach which directly takes advantage of hierarchical <phrase>CNN</phrase> features for detecting regions of interest fast. We demonstrate its performance on ImageNet 2013 detection benchmark and compare it with <phrase>state</phrase>-of-the-<phrase>art</phrase> methods. The <phrase>copyright</phrase> of this document resides with its authors. It may be distributed unchanged freely in print or <phrase>electronic</phrase> forms.
Automatic <phrase>Feature Learning</phrase> for Robust Shadow Detection We present a practical framework to automatically detect shadows in <phrase>real world</phrase> scenes from a <phrase>single</phrase> <phrase>photograph</phrase>. Previous works on shadow detection put a lot of effort in designing shadow variant and invariant <phrase>hand-crafted</phrase> features. In contrast, our framework automatically learns the most relevant features in a supervised manner using multiple convolutional <phrase>deep neural networks</phrase> (ConvNets). The 7-layer <phrase>network architecture</phrase> of each ConvNet consists of alternating <phrase>convolution</phrase> and sub-sampling layers. The <phrase>proposed framework</phrase> learns features at the super-<phrase>pixel</phrase> level and along the object boundaries. In both cases, features are extracted using a context aware window centered at interest points. The predicted posteriors based on the <phrase>learned features</phrase> are fed to a <phrase>conditional random field</phrase> <phrase>model</phrase> to generate smooth shadow contours. Our <phrase>proposed framework</phrase> consistently performed better than the <phrase>state</phrase>-of-the-<phrase>art</phrase> on all <phrase>major</phrase> shadow <phrase>databases</phrase> collected under a <phrase>variety</phrase> of conditions .
Using self-organizing maps to identify potential halo <phrase>white</phrase> dwarfs We present the <phrase>results</phrase> of an unsupervised classification of the disk and halo <phrase>white dwarf</phrase> populations in the <phrase>solar</phrase> <phrase>neighborhood</phrase>. The classification is done by merging the <phrase>results</phrase> of detailed <phrase>Monte Carlo</phrase> (<phrase>MC</phrase>) simulations, which reproduce very well the characteristics of the <phrase>white dwarf</phrase> populations in the <phrase>solar</phrase> <phrase>neighborhood</phrase>, with a catalogue of real <phrase>stars</phrase>. The resulting composite catalogue is analyzed using a competitive <phrase>learning algorithm</phrase>. In particular we have used the so-called self-organized map. The <phrase>MC</phrase> simulated <phrase>stars</phrase> are used as tracers and help in identifying the resulting clusters. The <phrase>results</phrase> of such an strategy turn out to be quite satisfactory, suggesting that this approach can provide an useful framework for analyzing large <phrase>databases</phrase> of <phrase>white</phrase> dwarfs with well determined kinematical, spatial and <phrase>photometric</phrase> properties once they become available in the next decade. Moreover, the <phrase>results</phrase> are of <phrase>astrophysical</phrase> interest as well, since a straightforward interpretation of several recent <phrase>astronomical</phrase> observations, like the detected <phrase>microlensing</phrase> events in the direction of the <phrase>Magellanic Clouds</phrase>, the possible detection of <phrase>high</phrase> <phrase>proper motion</phrase> <phrase>white</phrase> dwarfs in the <phrase>Hubble Deep Field</phrase> and the discovery of <phrase>high</phrase> <phrase>velocity</phrase> <phrase>white</phrase> dwarfs in the <phrase>solar</phrase> <phrase>neighborhood</phrase>, suggests that a fraction of the baryonic <phrase>dark matter</phrase> component of our <phrase>galaxy</phrase> could be in the form of old and dim halo <phrase>white</phrase> dwarfs.
The Difficulty of Training <phrase>Deep Architectures</phrase> and the Effect of <phrase>Unsupervised Pre-Training</phrase> Whereas theoretical work suggests that deep ar-chitectures might be more efficient at representing highly-varying functions, training deep ar-chitectures was unsuccessful until the recent advent of <phrase>algorithms</phrase> based on <phrase>unsupervised pre-training</phrase>. Even though these new <phrase>algorithms</phrase> have enabled training deep models, many questions remain as to the <phrase>nature</phrase> of this difficult learning problem. Answering these questions is important if learning in <phrase>deep architectures</phrase> is to be further improved. We attempt to shed some <phrase>light</phrase> on these questions through extensive simulations. The experiments confirm and clarify the advantage of <phrase>unsupervised pre-training</phrase>. They demonstrate the robustness of the training procedure with respect to the random initialization, the positive effect of <phrase>pre-training</phrase> in terms of optimization and its role as a regularizer. We empirically show the influence of <phrase>pre-training</phrase> with respect to <phrase>architecture</phrase> depth, <phrase>model</phrase> capacity, and number of <phrase>training examples</phrase>.
Discriminative Learning of Sum-Product Networks Discriminatively-trained <phrase>probabilistic models</phrase> often outperform their generative counterparts on challenging tasks in <phrase>computer vision</phrase> and <phrase>NLP</phrase>. <phrase>Conditional random fields</phrase> have been a predominant approach, but adapting them to richer structures further complicates learning with approximate inference [4]. Deep Networks have typically required both a generative and discriminative objective involving approximate inference. The discriminative objective is usually reflected in the use of <phrase>backpropagation</phrase> through softmax layers or <phrase>support vector machines</phrase> over network variables. The Sum-Product Network is a promising new <phrase>deep architecture</phrase> that can perform fast, exact inference on a representation that is broader than existing tractable <phrase>graphical</phrase> models [5]. For the first time, we combine the advantages of SPNs with the advantages of discriminative learning. We demonstrate the conditions under which an SPN represents the conditional <phrase>partition</phrase> <phrase>function</phrase> and provide a discriminative training <phrase>algorithm</phrase>. In this setting, SPN nodes that exclusively <phrase>cover</phrase> variables conditioned upon need not be consistent or complete. This admits a larger class of discriminative SPNs with speedy inference. We define an SPN that models the joint <phrase>probability</phrase> of three <phrase>disjoint sets</phrase> of hidden, query, and given variables: H, Y, and X respectively. The <phrase>conditional probability</phrase> of the query given the <phrase>data</phrase> is naturally P (Y |X) = Each summation only requires a <phrase>single</phrase> linear-time evaluation of the SPN, in which we set all indicators for the marginalized variables to one. Partial derivatives of this objective are just as easy as in the generative case. In previous work, we tried using <phrase>backpropagation</phrase> and " soft " expectation maximization to maximize the likelihood of the <phrase>training data</phrase>. We noted, however, that learning from the expectation over all possible complete sub-circuits suffered from <phrase>gradient</phrase> <phrase>diffusion</phrase> [5, 1]. This <phrase>led</phrase> us to use " Hard " EM, which provided the best <phrase>results</phrase> at the time. To optimize the conditional <phrase>log likelihood</phrase>, we use the same approximation in a technique we term " hard " <phrase>gradient descent</phrase>. For the purpose of this discussion, weights only appear on the edges from sum nodes to their children. Finding the assignments that maximize the value of an SPN is also a linear-time operation. In a top-down <phrase>pass</phrase> we follow the winning child of a sum node and all children of a product node. This maximization produces a complete sub-circuit, a <phrase>monomial</phrase> in the network <phrase>polynomial</phrase> [3]. Since there are two maximizations in this discriminative objective, we have the <phrase>gradient</phrase> of the difference of two 
RenderGAN: Generating Realistic <phrase>Labeled Data</phrase> <phrase>Deep Convolutional</phrase> <phrase>Neuronal</phrase> Networks (DCNN) are showing remarkable performance on many <phrase>computer vision</phrase> tasks. Due to their large <phrase>parameter space</phrase>, they require many <phrase>labeled samples</phrase> when trained in a supervised setting. The costs of annotating <phrase>data</phrase> manually can render the usage of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D <phrase>model</phrase> and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from <phrase>unlabeled data</phrase> such that the generated images are strikingly realistic while preserving the <phrase>labels</phrase> known from the 3D <phrase>model</phrase>. We apply the RenderGAN framework to generate images of <phrase>barcode</phrase>-like markers that are attached to <phrase>honeybees</phrase>. A DCNN is trained on this <phrase>data</phrase> only. It performs better on a <phrase>test</phrase> set of real <phrase>data</phrase> than an equal DCNN trained on the limited amounts of real <phrase>data</phrase> available.
LARC: Learning to Assign <phrase>Knowledge</phrase> Roles to Textual Cases In this <phrase>paper</phrase>, we present a learning framework for the <phrase>semantic</phrase> annotation of text documents that can be used as textual cases in <phrase>case-based reasoning</phrase> applications. The annotations are known as <phrase>knowledge</phrase> roles and are task-dependent. The framework relies on deep <phrase>natural language processing</phrase> techniques and does not require the existence of any domain-dependent resources. Several experiments are presented to demonstrate the feasibility of the <phrase>proposed approach</phrase>. The <phrase>results</phrase> show that the framework allows to robustly <phrase>label</phrase> cases with features which can be used for case representation, contributing to the retrieval of and the reasoning with textual cases.
An <phrase>Artificial</phrase> Agent for Anatomical Landmark Detection in <phrase>Medical</phrase> Images Fast and robust detection of anatomical structures or patholo-gies represents a fundamental task in <phrase>medical</phrase> <phrase>image analysis</phrase>. Most of the current solutions are however suboptimal and unconstrained by learning an appearance <phrase>model</phrase> and exhaustively scanning the space of parameters to detect a specific anatomical structure. In addition, typical feature computation or estimation of meta-parameters related to the appearance <phrase>model</phrase> or the search strategy, is based on local criteria or predefined approximation schemes. We propose a new learning method following a fundamentally different <phrase>paradigm</phrase> by simultaneously modeling both the object appearance and the parameter search strategy as a unified behav-ioral task for an <phrase>artificial</phrase> agent. The method combines the advantages of behavior learning achieved through <phrase>reinforcement learning</phrase> with effective hierarchical <phrase>feature extraction</phrase> achieved through <phrase>deep learning</phrase>. We show that given only a <phrase>sequence</phrase> of annotated images, the agent can automatically and strategically learn optimal paths that converge to the sought anatomical landmark location as opposed to exhaustively scanning the entire <phrase>solution</phrase> space. The method <phrase>significantly outperforms</phrase> <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>machine learning</phrase> and <phrase>deep learning</phrase> approaches both in terms of accuracy and speed on 2D <phrase>magnetic resonance</phrase> images, 2D <phrase>ultrasound</phrase> and 3D CT images, achieving <phrase>average</phrase> detection errors of 1-2 <phrase>pixels</phrase>, while also recognizing the absence of an object from the image.
Multi-modal interaction <phrase>management</phrase> for a <phrase>robot</phrase> companion Acknowledgments This <phrase>thesis</phrase> would not have been possible without the support of many people. I want to first thank my advisor Dr. Britta Wrede for many inspiring discussions in the last 3.5 years that helped me to substantially improve my work. I'm also very grateful to Prof. <phrase>David</phrase> Traum who carefully reviewed this <phrase>thesis</phrase> and offered many constructive suggestions. Many thanks are due to Prof. Gerhard Sagerer who gave me the chance to finish my <phrase>PhD</phrase> in the group of Applied <phrase>Computer Science</phrase> at the <phrase>Bielefeld University</phrase>. Here I spent wonderful time with my colleagues who are all kind and <phrase>cooperative</phrase> teammates, especially those in the <phrase>BIRON</phrase>-group. On the personal side, I would like to thank my parents who encouraged me to pursue my study and <phrase>research</phrase> in <phrase>Germany</phrase> although this means that they have to be 9000 <phrase>km</phrase> away from their youngest daughter. I also wish to thank Marcus Kleinehagenbrock who supports me both emotionally and professionally and gives me a home in <phrase>Germany</phrase>. And I also wish to thank our <phrase>robot</phrase> <phrase>BIRON</phrase> for which I developed the Interaction <phrase>Management</phrase> System. I know how hard <phrase>BIRON</phrase> is trying to acquire <phrase>human</phrase> abilities. By working at <phrase>BIRON</phrase>, I have learned to appreciate <phrase>human</phrase> beings and all their sophisticated capabilities. I also wish to <phrase>express my deep</phrase> respect for <phrase>science</phrase> and the tireless endeavor of <phrase>human</phrase> beings to learn about themselves and the environment they <phrase>live</phrase> in.
The Round <phrase>Earth</phrase> Project: <phrase>Deep Learning</phrase> in a Collaborative <phrase>Virtual World</phrase> The Round <phrase>Earth</phrase> Project is investigating how <phrase>virtual reality</phrase> <phrase>technology</phrase> can be used to help teach concepts that are c ounter-intuitive to a learner's currently held <phrase>mental model</phrase>. <phrase>Virtual reality</phrase> can be used t o p r ovide an <phrase>alternative</phrase> <phrase>cognitive</phrase> <phrase>starting point</phrase> that does not carry the baggage of past experiences. In particular this <phrase>paper</phrase> describes our work in comparing two strategies for teaching young children that the <phrase>Earth</phrase> is spherical when their everyday experiences tell them it is at.
<phrase>Intelligence</phrase>: what's in a name? Times piece "Nobody's Smart About <phrase>Intelligence</phrase>" (March 1, 1998), he offers this lament: "IQs are up. S.A.T.s are down. <phrase>Americans</phrase> flunk <phrase>math</phrase> and prosper. Somebody with brains should figure this out." The best anyone can offer, Johnson claims, is a conjecture that the complexity of everyday <phrase>life</phrase> (<phrase>programming</phrase> small <phrase>electronic</phrase> devices or calculating the latest projection of your net worth when you retire) has stretched and exercised our brains into faster and more agile <phrase>computing</phrase> engines. This might explain our increasing IQs while <phrase>MTV</phrase> and <phrase>video-game</phrase> overload might explain our increasing ignorance and declining capabilities at the logical plodding <phrase>deductive</phrase> thought of traditional <phrase>intelligence</phrase>. So what type of <phrase>intelligence</phrase> is <phrase>AI</phrase> trying to create? Astro Teller (New ~rk 7qmes, <phrase>Op-Ed</phrase>, March 21, 1998), suggests that no <phrase>matter</phrase> the type, building intelligences will make our world better as we learn more about our minds and who we are. But what will we understand? How better to exploit our neighbors or sell them goods and services at ever increasing profits? Will we understand the difference between <phrase>Gandhi</phrase> and <phrase>Saddam</phrase>? <phrase>Mozart</phrase> and <phrase>Madonna</phrase>? Or just what is it that everyone finds funny about Seinfield? The recent <phrase>pinnacle</phrase> of <phrase>AI</phrase> achievement has not come from our half century <phrase>long</phrase> quest to <phrase>pass</phrase> the <phrase>Turing Test</phrase>, but from our fascination at a machine beating a <phrase>human</phrase> at the complex task of playing <phrase>chess</phrase>. <phrase>Deep Blue</phrase>, a parallel <phrase>supercomputing</phrase> creation from <phrase>IBM</phrase> for processing hundreds of millions of <phrase>chess</phrase> moves per second, is the hardware and <phrase>software</phrase> that realized this <phrase>e</phrase>.,o oe. eeeoeleQo <phrase>e</phrase>*<phrase>e</phrase> eolee oo t <phrase>e</phrase>#= The recent <phrase>pinnacle</phrase> of <phrase>AI</phrase> achievement has come from <phrase>Deep Blue</phrase>, a machine that beat a <phrase>human</phrase> at playing <phrase>chess</phrase>. But what kind of <phrase>intelligence</phrase> is this? impressive accomplishment. An ancient <phrase>game</phrase>, <phrase>long</phrase> attacked by AI'ers and now empirically conquered. Everyone can honestly admit that <phrase>Deep Blue</phrase> doesn't have a clue about what it is doing, so <phrase>self awareness</phrase> is not an issue. It just "knows" the next best move from an intensive search. So what kind of <phrase>intelligence</phrase> is this? It is dearly '~<phrase>AI</phrase> <phrase>Intelligence</phrase>," a smart machine; honored byAI associations and foundations with a small pile of cash (compared to <phrase>IBM</phrase> expenses!) and a <phrase>Newell</phrase> <phrase>Research</phrase> Medal. New <phrase>York</phrase> Times piece on <phrase>Deep Blue</phrase>, asked whether <phrase>Deep Blue</phrase> is indeed intelligent. He offered that although <phrase>human</phrase> <phrase>chess</phrase> grandmasters don't do exactly 
<phrase>Deep Learning</phrase> of Appearance Models for Online <phrase>Object Tracking</phrase> This <phrase>paper</phrase> introduces a novel <phrase>deep learning</phrase> <phrase>based approach</phrase> for vision based <phrase>single</phrase> <phrase>target</phrase> tracking. We address this problem by proposing a <phrase>network architecture</phrase> which takes the input <phrase>video</phrase> frames and directly computes the tracking score for any candidate <phrase>target</phrase> location by estimating the <phrase>probability distributions</phrase> of the positive and negative examples. This is achieved by combining a <phrase>deep convolutional</phrase> <phrase>neural network</phrase> with a <phrase>Bayesian</phrase> loss layer in a unified framework. In <phrase>order</phrase> to deal with the limited number of positive <phrase>training examples</phrase>, the network is <phrase>pre-trained</phrase> offline for a generic image <phrase>feature representation</phrase> and then is <phrase>fine-tuned</phrase> in multiple steps. An online <phrase>fine-tuning</phrase> step is carried out at every frame to learn the appearance of the <phrase>target</phrase>. We adopt a two-stage iterative <phrase>algorithm</phrase> to adaptively update the network parameters and maintain a <phrase>probability density</phrase> for <phrase>target</phrase>/non-<phrase>target</phrase> regions. The tracker has been tested on the standard tracking benchmark and the <phrase>results</phrase> indicate that the proposed <phrase>solution</phrase> achieves <phrase>state</phrase>-of-the-<phrase>art</phrase> tracking <phrase>results</phrase>.
Rapid <phrase>Feature Learning</phrase> with Stacked Linear Denoisers We investigate <phrase>unsupervised pre-training</phrase> of <phrase>deep architectures</phrase> as feature generators for " shallow " classifiers. Stacked <phrase>Denoising Autoencoders</phrase> (SdA) [23], when used as feature pre-processing tools for <phrase>SVM</phrase> classification, can <phrase>lead</phrase> to <phrase>significant improvements</phrase> in accuracy however, at the price of a substantial increase in computational cost. In this <phrase>paper</phrase> we create a simple <phrase>algorithm</phrase> which mimics the <phrase>layer by layer</phrase> training of SdAs. However, in contrast to SdAs, our <phrase>algorithm</phrase> requires no training through <phrase>gradient descent</phrase> as the parameters can be computed in <phrase>closed-form</phrase>. It can be implemented in less than 20 lines of <phrase>MATLAB</phrase> TM and reduces the computation time from several hours to mere seconds. We show that our feature transformation reliably improves the <phrase>results</phrase> of <phrase>SVM</phrase> classification significantly on all our <phrase>data</phrase> sets often outperforming SdAs and even <phrase>deep neural networks</phrase> in three out of four <phrase>deep learning</phrase> benchmarks.
Assigning Deep Lexical Types <phrase>Deep linguistic</phrase> grammars provide complex grammatical representations of sentences, capturing, for instance, <phrase>long</phrase>-distance dependencies and returning <phrase>semantic</phrase> representations, making them suitable for advanced <phrase>natural language processing</phrase>. However, they lack robustness in that they do not gracefully handle words missing from the <phrase>lexicon</phrase> of the <phrase>grammar</phrase>. Several approaches have been taken to handle this problem, one of which consists in pre-annotating the input to the <phrase>grammar</phrase> with shallow processing <phrase>machine-learning</phrase> tools. This is usually done to speed-up <phrase>parsing</phrase> (supertagging) but it can also be used as a way of handling unknown words in the input. These pre-processing tools, however, must be able to cope with the vast tagset required by a deep <phrase>grammar</phrase>. We investigate the training and evaluation of several supertaggers for a <phrase>deep linguistic</phrase> processing <phrase>grammar</phrase> and <phrase>report</phrase> on it in this <phrase>paper</phrase>.
Effect of <phrase>fixed-point arithmetic</phrase> on <phrase>deep belief</phrase> networks (abstract only) <phrase>Deep Belief</phrase> Networks (DBNs) are <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>learning algorithms</phrase> building on a <phrase>subset</phrase> of <phrase>neural networks</phrase>, <phrase>Restricted Boltzmann Machine</phrase> (RBM). DBNs are computationally intensive posing the question of whether DBNs can be <phrase>FPGA</phrase> accelerated. <phrase>Fixed-point arithmetic</phrase> can have an important influence on the execution time and prediction accuracy of a DBN. Previous studies have focused only on customized RBM <phrase>accelerators</phrase> with a fixed <phrase>data</phrase>-width. Our <phrase>results</phrase> experiments demonstrate that <phrase>variable</phrase> <phrase>data</phrase>-widths can obtain similar performance levels. We can also observe that the most suitable <phrase>data</phrase>-widths for different types of DBN are not unique or fixed. From this we conclude that a DBN <phrase>accelerator</phrase> should support various <phrase>data</phrase>-widths rather than only fixed one as done in previous work. The processing performance of DBN <phrase>accelerators</phrase> in <phrase>FPGA</phrase> is almost always constrained not by the capacity of the processing units, but by their on-chip <phrase>RAM</phrase> capacity and speed. We propose an efficient <phrase>memory</phrase> sub-system combining junction and padding methods to reduce bandwidth usage for DBN <phrase>accelerators</phrase>, which shows that supporting various <phrase>data</phrase>-widths is not as difficult as it may <phrase>sound</phrase>. The cost is only little in hardware terms and does not affect the <phrase>critical path</phrase>. We <phrase>design</phrase> a generation tool to help users reconfiguring the <phrase>memory</phrase> sub-system with arbitrary <phrase>data</phrase>-width flexibly. Our tool can also be used as an advanced IP core generator above <phrase>FPGA</phrase> <phrase>memory controller</phrase> supporting parallel <phrase>memory</phrase> access in irregular <phrase>data</phrase>-width for other applications.
Incremental Slow Feature Analysis The Slow Feature Analysis (<phrase>SFA</phrase>) <phrase>unsupervised learning</phrase> framework extracts features representing the underlying causes of the changes within a temporally coherent <phrase>high</phrase>-dimensional raw sensory input signal. We develop the first online version of <phrase>SFA</phrase>, via a combination of incremental <phrase>Principal Components Analysis</phrase> and Minor Components Analysis. Unlike standard batch-based <phrase>SFA</phrase>, on-line <phrase>SFA</phrase> adapts along with non-stationary environments , which makes it a generally useful unsuper-vised <phrase>preprocessor</phrase> for autonomous learning agents. We compare online <phrase>SFA</phrase> to batch <phrase>SFA</phrase> in several experiments and show that it indeed learns without a <phrase>teacher</phrase> to encode the input <phrase>stream</phrase> by informative slow features representing meaningful abstract environmental properties. We extend online <phrase>SFA</phrase> to deep networks in hierarchical <phrase>fashion</phrase>, and use them to successfully extract abstract object position <phrase>information</phrase> from <phrase>high</phrase>-dimensional <phrase>video</phrase>.
A Discipline-based <phrase>Undergraduate</phrase> Skills Module a Discipline-based <phrase>Undergraduate</phrase> Skills Module Designing the Module This <phrase>paper</phrase> describes the development and evaluation of a discipline-based skills module at Level 1 in the <phrase>undergraduate</phrase> <phrase>psychology</phrase> <phrase>curriculum</phrase>. The module combined generic and subject-specific skills teaching, linked skills provision with the personal <phrase>tutor</phrase> system and included practical exercises to <phrase>promote deep learning</phrase> and improve <phrase>study skills</phrase>. <phrase>Student</phrase> <phrase>feedback</phrase> showed that workshops and tutorials on <phrase>essay</phrase> writing were the most valued part of the module. The module assessment (a coursework <phrase>essay</phrase> about skills in <phrase>higher education</phrase>) was the part that most students asked to change. The response of <phrase>psychology</phrase> tutors was mixed and many tutors initially expressed misgivings about teaching generic skills. Following <phrase>feedback</phrase> from both students and tutors, the module developed to allow closer integration between generic skills and subject-specific teaching. <phrase>Student</phrase> progression across the <phrase>psychology</phrase> programme as a whole was not markedly higher after the introduction of the module by comparison with before, but other factors may also have affected <phrase>student</phrase> progression during that <phrase>period</phrase>. The module was designed to support <phrase>student</phrase> learning in <phrase>psychology</phrase> but, with adaptations of content, the approach of linking skills provision with subject-specific teaching and the personal <phrase>tutor</phrase> system could be applied in a <phrase>range</phrase> of other disciplines. INTRODUCTION 'Skills' have been the focus of many recommendations and initiatives in <phrase>higher education</phrase> during the last decade. '<phrase>Study skills</phrase>' have <phrase>long</phrase> been recognised as important aspects of <phrase>student</phrase> support, especially in relation to the 'baseline skills' of entrants to <phrase>higher education</phrase>, who, with widening participation, increasingly include individuals with little confidence or expertise in advanced study (Hall et al., 2001; Warren, 2002). 'Transferable skills', 'generic skills' or 'key skills', especially those with relevance to employability, have also been identified as important <phrase>learning outcomes</phrase> in their own right. One of the recommendations of the National Enquiry into the Future of <phrase>Higher Education</phrase> in the <phrase>UK</phrase> was that <phrase>learning outcomes</phrase> should be formulated in terms of key skills (Dearing, 1997). In line with that, <phrase>benchmarking</phrase> statements for <phrase>degree</phrase> courses include both subject skills and generic skills (<phrase>QAA</phrase>, 2002a). Skills teaching can be provided in a number of ways, but there is growing recognition of the value of discipline-based provision. Students often perceive little connection between skills teaching and their subject learning (Lucas et al., 2001; Norton and Dickins, 1995). In one <phrase>report</phrase>, discipline-based <phrase>study skills</phrase> programmes were better attended and in greater demand among students than generic <phrase>study skills</phrase> courses (Durkin and Main, 2002). Discipline-based 
<phrase>Weakly Supervised</phrase> Learning from Multiple Modalities: Exploiting <phrase>Video</phrase>, Audio and Text for <phrase>Video</phrase> Understanding Iv Abstract <phrase>Weakly Supervised</phrase> Learning from Multiple Modalities: Exploiting <phrase>Video</phrase>, Audio and Text for <phrase>Video</phrase> Understanding 2009 Acknowledgements I would like to acknowledge first and foremost my advisor, Ben Taskar, who represents in my eyes what a perfect advisor should be. As his first Ph.D. <phrase>student</phrase> I benefited from a lot of his guidance, his <phrase>knowledge</phrase> in <phrase>machine learning</phrase> and his inspiration for tackling difficult problems. I am particularly grateful to him for helping me select a very interesting <phrase>thesis</phrase> topic, and for his constant push for quality, original and ambitious work. Ben was and still is a mentor for me. Taylor and Andrew Zisserman, for their excellent <phrase>feedback</phrase>, questions and suggestions which shaped this <phrase>thesis</phrase> to its current form. I have found in their work a source of inspiration and <phrase>knowledge</phrase>. The work of Andrew on character naming in <phrase>video</phrase> using <phrase>screenplay</phrase> was particularly influential for me, as was the work of Fernando on <phrase>Conditional Random Fields</phrase>. As my committee chair, Kostas gave me invaluable advice to improve my <phrase>thesis</phrase>, as well as <phrase>academic</phrase> guidance throughout my Ph.D. I would also like to thank my written preliminary exam committee, <phrase>Ali</phrase> Jadbabaie, Kostas Daniilidis and Lyle Ungar. I had the chance to work with Shi, who taught me a lot about <phrase>computer vision</phrase>, how to think hard about a problem and how to ask the right questions before proposing a <phrase>solution</phrase>. Jianbo has greatly influenced my <phrase>research</phrase>, presentation skills, and approach to <phrase>problem solving</phrase>. As a Grasp lab alumni, I have had the privilege to collaborate, work with, or interact with a number of outstanding people. My special thanks go to Ben Sapp, who was an amazing colleague. I would also like to thank Katerina Fragiadakis, Mirko Visontai and all those that I forgot to mention. Collaborators or <phrase>friends</phrase>, they all contributed to making my stay at <phrase>Upenn</phrase> a very enjoyable one. Special thanks to Surabhi, who helped me in some of the difficult moments. Last but not least, I would like to express deep gratitude to my parents, Caroline and Jean-marie, for their never-ending support throughout my studies and their help to make difficult decisions. I would like to thank finally my brother Bijan, my sisters Marjolaine and <phrase>Philippine</phrase>, for being such a united and supportive <phrase>family</phrase>. As web and personal content become ever more enriched by videos, there is increasing need for <phrase>semantic</phrase> <phrase>video</phrase> search and indexing. A main challenge for this task is lack of supervised <phrase>data</phrase> for learning models. In this dissertation 
<phrase>Robotics</phrase> <phrase>Olympiads</phrase>: A New Means to Facilitate Conceptualization of <phrase>Knowledge</phrase> Acquired in <phrase>Robot</phrase> Projects This <phrase>paper</phrase> proposes theoretical <phrase>robotics</phrase> competitions, offered in conjunction with <phrase>robot</phrase> contests, as the framework to foster <phrase>deep learning</phrase> of concepts which underlie the practical projects and to facilitate the development of <phrase>engineering</phrase> aptitude. We present our experiences with integrating theoretical <phrase>tests</phrase> in the <phrase>Trinity College</phrase> Fire-Fighting Home <phrase>Robot</phrase> Contest and National Botball <phrase>Tournament</phrase>.
Adaptability of <phrase>Neural Networks</phrase> on Varying Granularity IR Tasks Recent work in <phrase>Information Retrieval</phrase> (IR) using <phrase>Deep Learning</phrase> models has yielded <phrase>state</phrase> of the <phrase>art</phrase> <phrase>results</phrase> on a <phrase>variety</phrase> of IR tasks. <phrase>Deep neural networks</phrase> (DNN) are capable of learning ideal representations of <phrase>data</phrase> during the training process, removing the need for independently extracting features. However, the structures of these DNNs are often tailored to perform on specific datasets. In addition, IR tasks deal with text at varying levels of granularity from <phrase>single</phrase> factoids to documents containing thousands of words. In this <phrase>paper</phrase>, we examine the role of the granularity on the performance of common <phrase>state</phrase> of the <phrase>art</phrase> DNN structures in IR.
<phrase>Scaffolding</phrase> for <phrase>computer supported</phrase> writing to learn activities in vocational training Dual-T project investigates how ICT can support learning activities involving sharing and reflection about <phrase>professional</phrase> experience in <phrase>order</phrase> to harmonize <phrase>school</phrase> learning with practical experience. In this study we tested the effects of low and <phrase>high</phrase> <phrase>scaffolding</phrase> on <phrase>collaborative writing</phrase> activities on <phrase>professional</phrase> procedures. We expected longer, more correct texts to emerge from strongly scaffolded activities than from weakly scaffolded activities. Recent <phrase>research</phrase> on initial vocational training <phrase>education</phrase> has shown the existence of a gap between field <phrase>knowledge</phrase> and <phrase>knowledge</phrase> taught in <phrase>vocational schools</phrase> (Filliettaz, 2008). One of the main issues concerns <phrase>knowledge</phrase> and skill transfer between <phrase>school</phrase> and workplace (Eraut, 2004). In our project, we are interested in identifying original technological support and pedagogical designs for <phrase>professional</phrase> skills learning and transfer in vocational educational training (VET). In this context, we adopt a " writing-to-learn " approach (Hayes and <phrase>Flower</phrase>, 1980; Hayes, 1996). It assumes that writing promotes the acquisition of <phrase>knowledge</phrase>, since <phrase>domain knowledge</phrase> should be retrieved, reorganized and incorporated into a linear and understandable form. Extending this <phrase>cognitive</phrase> view, <phrase>Galbraith</phrase> (1999) claims that <phrase>knowledge</phrase> transformation leads to <phrase>knowledge</phrase> <phrase>constitution</phrase>, which makes writing a promising instructional tool. <phrase>Professional</phrase> procedure learning and transfer is a critical issue in VET. Anderson's <phrase>ACT-R</phrase> (1993) <phrase>model</phrase> claims that procedure acquisition is based on learning from declarative traces of initial <phrase>problem solving</phrase>. Writing could then be a powerful tool for constructing and refining the declarative representation of procedures. Moreover, confrontation between learners' conceptions and experiences should promote reflexive thinking and <phrase>epistemic</phrase> monitoring, embodied in the written productions In addition, <phrase>collaborative writing</phrase> activities should support not only individual <phrase>knowledge</phrase> acquisition but also the collaborative <phrase>dimension</phrase> of <phrase>domain knowledge</phrase> building. Tynjl, Mason and Lonka (2001) show that studies of the effects of <phrase>collaborative writing</phrase> on learning are still rare Most of the <phrase>research</phrase> is done on the improvement of the writing process and writing skills. We consider that a peer collaborative approach to writing-to-learn in a VET context should be valuable in terms of <phrase>knowledge</phrase> building, procedure understanding and acquisition. Thus, in this <phrase>research</phrase> we are interested in investigating the impact of <phrase>collaborative writing</phrase> activities on the <phrase>construction</phrase> of a mutual declarative representation of the procedures. This is the basis for deep understating of procedures thus for acquisition and transfer. <phrase>Computer supported</phrase> <phrase>collaborative writing</phrase> to learn activities can be supported by many types of tools. Considering our context and the <phrase>population</phrase> we are working with, we 
Distributed <phrase>Deep Q</phrase>-Learning We propose a distributed <phrase>deep learning</phrase> <phrase>model</phrase> to successfully learn control policies directly from <phrase>high</phrase>-dimensional sensory input using <phrase>reinforcement learning</phrase>. The <phrase>model</phrase> is based on the <phrase>deep Q</phrase>-network, a <phrase>convolutional neural network</phrase> trained with a variant of Q-learning. Its input is raw <phrase>pixels</phrase> and its output is a value <phrase>function</phrase> estimating future rewards from taking an <phrase>action</phrase> given a system <phrase>state</phrase>. To distribute the <phrase>deep Q</phrase>-network training, we adapt the DistBelief <phrase>software framework</phrase> to the context of efficiently training <phrase>reinforcement learning</phrase> agents. As a result, the method is completely asynchronous and scales well with the number of machines. We demonstrate that the <phrase>deep Q</phrase>-network agent, receiving only the <phrase>pixels</phrase> and the <phrase>game</phrase> score as inputs, was able to achieve reasonable success on a simple <phrase>game</phrase> with minimal parameter tuning. I. INTRODUCTION <phrase>Reinforcement learning</phrase> (<phrase>RL</phrase>) agents face a tremendous challenge in optimizing their control of a system approaching <phrase>real-world</phrase> complexity: they must derive efficient representations of the environment from <phrase>high</phrase>-dimensional sensory inputs and use these to generalize past experience to new situations. While past work in <phrase>RL</phrase> has shown that with good <phrase>hand-crafted</phrase> features agents are able to learn good control policies, their applicability has been limited to domains where such features have been discovered, or to domains with fully observed, <phrase>low-dimensional</phrase> <phrase>state</phrase> spaces [1][3]. We consider the problem of efficiently scaling a <phrase>deep learning</phrase> <phrase>algorithm</phrase> to control a complicated system with <phrase>high</phrase>-dimensional sensory inputs. The basis of our <phrase>algorithm</phrase> is a <phrase>RL</phrase> agent called a <phrase>deep Q</phrase>-network (DQN) [4], [5] that combines <phrase>RL</phrase> with a class of <phrase>artificial neural networks</phrase> known as <phrase>deep neural networks</phrase> [6]. DQN uses an <phrase>architecture</phrase> called the <phrase>deep convolutional</phrase> network, which utilizes hierarchical layers of tiled convolutional filters to exploit the local spatial correlations present in images. As a result, this <phrase>architecture</phrase> is robust to natural transformations such as changes of viewpoint and scale [7]. In practice, increasing the scale of <phrase>deep learning</phrase> with respect to the number of <phrase>training examples</phrase> or the number of <phrase>model</phrase> parameters can drastically improve the performance of <phrase>deep neural networks</phrase> [8], [9]. To <phrase>train</phrase> a deep network with many parameters on multiple machines efficiently, we adapt a <phrase>software framework</phrase> called DistBelief to the context of the training of <phrase>RL</phrase> agents [10]. Our new framework supports <phrase>data</phrase> parallelism, thereby allowing us to potentially utilize <phrase>computing</phrase> clusters with thousands of machines for <phrase>large-scale</phrase> distributed training, as shown in [10] in 
Feature mapping using far-field <phrase>microphones</phrase> for distant <phrase>speech recognition</phrase> <phrase>Acoustic</phrase> modeling based on <phrase>deep architectures</phrase> has recently gained remarkable success, with substantial improvement of <phrase>speech recognition</phrase> accuracy in several <phrase>automatic speech recognition</phrase> (ASR) tasks. For distant <phrase>speech recognition</phrase>, the multi-<phrase>channel</phrase> <phrase>deep neural network</phrase> based approaches rely on the powerful modeling capability of <phrase>deep neural network</phrase> (DNN) to learn suitable representation of distant speech directly from its multi-<phrase>channel</phrase> source. In this <phrase>model</phrase>-based combination of multiple <phrase>microphones</phrase>, features from each <phrase>channel</phrase> are concatenated and used together as an input to DNN. This allows integrating the multi-<phrase>channel</phrase> audio for <phrase>acoustic</phrase> modeling without any pre-processing steps. Despite powerful modeling capabilities of DNN, an environmental mismatch due to noise and <phrase>reverberation</phrase> may result in severe performance degradation when features are simply fed to a DNN without a feature enhancement step. In this <phrase>paper</phrase>, we introduce the nonlinear bottleneck feature mapping approach using DNN, to transform the noisy and reverberant features to its clean version. The bottleneck features trained on clean signal are used as a <phrase>teacher</phrase> signal because they contain relevant <phrase>information</phrase> to <phrase>phoneme</phrase> classification, and the mapping is performed with the objective of suppressing noise and <phrase>reverberation</phrase>. The individual and combined impacts of <phrase>beamforming</phrase> and <phrase>speaker</phrase> adaptation techniques along with the feature mapping are examined for distant large <phrase>vocabulary</phrase> <phrase>speech recognition</phrase>, using a <phrase>single</phrase> and multiple far-field <phrase>microphones</phrase>. As an <phrase>alternative</phrase> to <phrase>beamforming</phrase>, experiments with concatenating multiple <phrase>channel</phrase> features are conducted. The <phrase>experimental</phrase> <phrase>results</phrase> on the AMI meeting corpus show that the feature mapping, used in combination with <phrase>beamforming</phrase> and <phrase>speaker</phrase> adaptation yields a distant <phrase>speech recognition</phrase> performance below 50% word <phrase>error rate</phrase> (WER), using DNN for <phrase>acoustic</phrase> modeling.
Learning Multimodal Dictionaries <phrase>Real-world</phrase> phenomena involve complex interactions between multiple signal modalities. As a consequence, humans are used to integrate at each instant perceptions from all their senses in <phrase>order</phrase> to enrich their understanding of the surrounding world. This <phrase>paradigm</phrase> can be also extremely useful in many <phrase>signal processing</phrase> and <phrase>computer vision</phrase> problems involving mutually related signals. The simultaneous processing of multimodal <phrase>data</phrase> can, in fact, reveal <phrase>information</phrase> that is otherwise hidden when considering the signals independently. However, in natural multimodal signals, the statistical dependencies between modalities are in <phrase>general</phrase> not obvious. Learning fundamental multimodal patterns could offer deep insight into the structure of such signals. In this <phrase>paper</phrase>, we present a novel <phrase>model</phrase> of multimodal signals based on their sparse decomposition over a <phrase>dictionary</phrase> of multimodal structures. An <phrase>algorithm</phrase> for iteratively learning multimodal generating functions that can be shifted at all positions in the signal is proposed, as well. The learning is defined in such a way that it can be accomplished by iteratively solving a generalized <phrase>eigenvector</phrase> problem, which makes the <phrase>algorithm</phrase> fast, flexible, and <phrase>free</phrase> of user-defined parameters. The <phrase>proposed algorithm</phrase> is applied to audiovisual sequences and it is able to discover underlying structures in the <phrase>data</phrase>. The detection of such audio-<phrase>video</phrase> patterns in audiovisual clips allows to effectively localize the <phrase>sound</phrase> source on the <phrase>video</phrase> in presence of substantial <phrase>acoustic</phrase> and visual distractors, outperforming <phrase>state</phrase>-of-the-<phrase>art</phrase> audiovisual localization <phrase>algorithms</phrase>.
Advances in optimizing recurrent networks After a more than decade-<phrase>long</phrase> <phrase>period</phrase> of relatively little <phrase>research</phrase> activity in the <phrase>area</phrase> of <phrase>recurrent neural networks</phrase>, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding <phrase>deep learning</phrase>. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of <phrase>long</phrase>-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time <phrase>ranges</phrase> with leaky integration, advanced <phrase>momentum</phrase> techniques, using more powerful output <phrase>probability</phrase> models, and encouraging sparser gradients to help <phrase>symmetry breaking</phrase> and credit assignment. The experiments are performed on text and <phrase>music</phrase> <phrase>data</phrase> and show off the combined effects of these techniques in generally improving both training and <phrase>test</phrase> error.
<phrase>Deep Belief</phrase> Networks for <phrase>Phone Recognition</phrase> <phrase>Hidden Markov Models</phrase> (HMMs) have been the <phrase>state</phrase>-of-the-<phrase>art</phrase> techniques for <phrase>acoustic</phrase> modeling despite their unrealistic <phrase>independence</phrase> assumptions and the very limited representational capacity of their hidden states. There are many proposals in the <phrase>research</phrase> <phrase>community</phrase> for deeper models that are capable of modeling the many types of variability present in the speech generation process. <phrase>Deep Belief</phrase> Networks (DBNs) have recently proved to be very effective for a <phrase>variety</phrase> of <phrase>machine learning</phrase> problems and this <phrase>paper</phrase> applies DBNs to <phrase>acoustic</phrase> modeling. On the standard TIMIT corpus, DBNs consistently outperform other techniques and the best DBN achieves a phone <phrase>error rate</phrase> (PER) of 23.0% on the TIMIT core <phrase>test</phrase> set.
Anlisis de Sentimientos y <phrase>Minera</phrase> de Opiniones: el corpus EmotiBlog EmotiBlog is a collection of <phrase>blog</phrase> posts created and annotated for detecting subjective expressions in the new textual genres born with the <phrase>Web 2.0</phrase>. Previous work has demonstrated the relevance of the <phrase>Machine learning</phrase> systems as tool for detecting opinionated <phrase>information</phrase>. In this <phrase>paper</phrase> we explore additional features for a deep analysis of these techniques. Moreover, we compare EmotiBlog with the JRC collection. The obtained <phrase>results</phrase> demonstrate the usefulness of EmotiBlog and support us to continue in this <phrase>research</phrase> path.
The joint <phrase>organization</phrase> of interaction within a multimodal CSCL medium In <phrase>order</phrase> to collaborate effectively in group discourse on a topic like <phrase>mathematical</phrase> patterns, group participants must organize their activities in ways that share the significance of their utterances, inscriptions, and behaviors. Here, we <phrase>report</phrase> the <phrase>results</phrase> of a ethnomethodological <phrase>case study</phrase> of collaborative <phrase>math</phrase> <phrase>problem-solving</phrase> activities mediated by a synchronous multimodal online environment. We investigate the moment-by-moment details of the interaction practices through which participants organize their chat utterances and <phrase>whiteboard</phrase> actions as a coherent whole. This approach to analysis foregrounds the sequentiality of <phrase>action</phrase> and the implicit referencing of meaning makingfundamental features of interaction. In particular, we observe that the sequential <phrase>construction</phrase> of shared drawings and the deictic references that link chat messages to features of those drawings and to prior chat content are <phrase>instrumental</phrase> in the achievement of <phrase>intersubjectivity</phrase> among group members' understandings. We characterize this precondition of collaboration as the co-<phrase>construction</phrase> of an indexical field that functions as a common ground for <phrase>group cognition</phrase>. Our analysis reveals methods by which the group co-constructs meaningful inscriptions in the dual-interaction spaces of its CSCL environment. The integration of <phrase>graphical</phrase>, <phrase>narrative</phrase>, and symbolic <phrase>semiotic</phrase> modalities in this manner also facilitates joint <phrase>problem solving</phrase>. It allows group members to invoke and operate with multiple realizations of their <phrase>mathematical</phrase> artifacts, a characteristic of <phrase>deep learning</phrase> of <phrase>mathematics</phrase>.
Autoencoders, <phrase>Unsupervised Learning</phrase>, and <phrase>Deep Architectures</phrase> Autoencoders <phrase>play</phrase> a fundamental role in <phrase>unsupervised learning</phrase> and in <phrase>deep architectures</phrase> for <phrase>transfer learning</phrase> and other tasks. In spite of their fundamental role, only linear <phrase>au</phrase>-toencoders over the real numbers have been solved analytically. Here we present a <phrase>general</phrase> <phrase>mathematical</phrase> framework for the study of both linear and non-linear autoencoders. The framework allows one to derive an analytical treatment for the most non-linear autoen-coder, the <phrase>Boolean</phrase> autoencoder. Learning in the <phrase>Boolean</phrase> autoencoder is equivalent to a clustering problem that can be solved in <phrase>polynomial</phrase> time when the number of clusters is small and becomes <phrase>NP complete</phrase> when the number of clusters is large. The framework sheds <phrase>light</phrase> on the different kinds of autoencoders, their learning complexity, their horizontal and vertical composability in <phrase>deep architectures</phrase>, their critical points, and their fundamental connections to clustering, Hebbian learning, and <phrase>information theory</phrase>.
A Real-Time <phrase>Deep Learning</phrase> Pedestrian Detector for <phrase>Robot</phrase> <phrase>Navigation</phrase> A real-time <phrase>Deep Learning</phrase> based method for Pedestrian Detection (PD) is applied to the <phrase>Human</phrase>-Aware <phrase>robot</phrase> <phrase>navigation</phrase> problem. The pedestrian detector combines the Aggregate <phrase>Channel</phrase> Features (ACF) detector with a <phrase>deep Convolutional</phrase> <phrase>Neural Network</phrase> (<phrase>CNN</phrase>) in <phrase>order</phrase> to obtain fast and accurate performance. Our <phrase>solution</phrase> is firstly evaluated using a set of real images taken from onboard and offboard cameras and, then, it is validated in a typical domestic indoor scenario, in two distinct experiments. The <phrase>results</phrase> show that the <phrase>robot</phrase> is able to cope with <phrase>human</phrase>-aware constraints, defined after common <phrase>proxemics</phrase> rules.
Improving Students' <phrase>Knowledge</phrase> Integration in <phrase>Data Structures</phrase> In <phrase>order</phrase> to create deep <phrase>conceptual understanding</phrase>, students need to integrate <phrase>knowledge</phrase> pieces into coherent <phrase>knowledge</phrase> structures. When students first engage with new <phrase>knowledge</phrase>, it is fragmented and weakly connected with their existing <phrase>knowledge</phrase> and they need to do the integration process. However, current teaching - learning methods do not explicitly <phrase>train</phrase> students in <phrase>order</phrase> to do so. The Ph.D. work discussed in this <phrase>paper</phrase> aims at designing and evaluating a <phrase>technology</phrase> enhanced <phrase>learning environment</phrase> to <phrase>train</phrase> students in <phrase>knowledge</phrase> integration. The theoretical foundation of the <phrase>technology</phrase> enhanced <phrase>learning environment</phrase> is exploratory question posing, i.e., asking new questions related to a given concept. By the end of doctoral <phrase>research</phrase> I expect the following contributions: 1) an empirically evaluated online intervention for <phrase>knowledge</phrase> integration 2) <phrase>cognitive</phrase> scaffolds for <phrase>knowledge</phrase> integration (specific to <phrase>data structures</phrase> domain) based on exploratory questioning strategies and 3) a validated assessment framework to evaluate <phrase>knowledge</phrase> integration performance. I am carrying out this <phrase>research</phrase> in the domain of <phrase>data structures</phrase> and the <phrase>target</phrase> <phrase>population</phrase> is first year CS <phrase>engineering</phrase> undergraduates.
Signal Classification for <phrase>Acoustic</phrase> <phrase>Neutrino</phrase> Detection This article focuses on signal classification for <phrase>deep-sea</phrase> <phrase>acoustic</phrase> <phrase>neutrino</phrase> detection. In the <phrase>deep sea</phrase>, the background of transient signals is very diverse. Approaches like matched filtering are not sufficient to distinguish between <phrase>neutrino</phrase>-like signals and other transient signals with similar signature, which are forming the <phrase>acoustic</phrase> background for <phrase>neutrino</phrase> detection in the <phrase>deep-sea</phrase> environment. A classification system based on <phrase>machine learning</phrase> <phrase>algorithms</phrase> is analysed with the goal to find a robust and effective way to perform this task. For a well-trained <phrase>model</phrase>, a testing error on the level of one percent is achieved for strong classifiers like <phrase>Random Forest</phrase> and Boosting <phrase>Trees</phrase> using the extracted features of the signal as input and utilising dense clusters of sensors instead of <phrase>single</phrase> sensors.
Colloquia: Mptl14 <phrase>Student</phrase> Engagement and Learning with Phet Interactive Simulations There is considerable evidence that PhET interactive simulations can be powerful tools for achieving <phrase>student</phrase> learning of <phrase>science</phrase>. Recent <phrase>research</phrase> conducted with PhET Interactive simulations has focused on the specific aspects of simulations that help students build a <phrase>conceptual understanding</phrase> of the <phrase>science</phrase>; specifically the value of showing the invisible, the use of analogy and effective levels of guidance with simulations. Educators have found that use of heavily guided activities does not elicit deep thinking and learning from students; while other studies have found that with pure discovery learning students are not able to " discover " the <phrase>science</phrase> for themselves. Recent studies reveal that appropriate <phrase>scaffolding</phrase> of the material is needed to help students build a mental framework about concepts. Then students can construct their own understanding within this framework. Our work has focused on understanding how students use simulations to construct this mental framework and the effect levels of guidance have on students' use of simulations. Hundreds of individual <phrase>student</phrase> interviews have been conducted during which the students describe what they were thinking as they interact with simulations. Careful analysis reveals that showing the invisible and use of analogy both facilitate students' <phrase>construction</phrase> of their understanding; while the <phrase>nature</phrase> of guidance influences the amount of <phrase>student</phrase> engagement. PhET Interactive Simulations are a substantial ( 85) and growing suite of <phrase>professional</phrase> quality simulations (sims) for <phrase>teaching and learning</phrase> <phrase>science</phrase>. The sims are freely distributed from the PhET <phrase>website</phrase> http://PhET.colorado.edu, with roughly 10 million uses in the past year. The majority of PhET sims are for teaching <phrase>physics</phrase> but there are a growing number in <phrase>chemistry</phrase>, <phrase>biology</phrase>, <phrase>math</phrase> and other sciences. Considerable <phrase>research</phrase> has investigated the use of PhET sims in a <phrase>variety</phrase> of educational settings (PhET Team, 2009). <phrase>Interactivity</phrase> in computer simulations is known to be beneficial for learning, (Bodemer, 2004; van der Meij, 2006) but the <phrase>degree</phrase> of <phrase>interactivity</phrase> can vary greatly for c Societ Italiana di Fisica
Signal Correlation Prediction Using <phrase>Convolutional Neural Networks</phrase> This <phrase>paper</phrase> focuses on analysing multiple <phrase>time series</phrase> relationships such as correlations between them. We develop a <phrase>solution</phrase> for the Connectiomics contest dataset of <phrase>fluorescence</phrase> imaging of neural activity recordings the aim is <phrase>reconstruction</phrase> of the wiring between <phrase>brain</phrase> <phrase>neurons</phrase>. The <phrase>model</phrase> is implemented to achieve <phrase>high</phrase> evaluation score. Our <phrase>model</phrase> took the fourth place in this contest. The performance is similar to the other leading solutions, thus we showed that <phrase>deep learning</phrase> methods for <phrase>time series</phrase> processing are comparable to the other approaches and have wide opportunities for further improvement. We discuss a <phrase>range</phrase> of methods and code optimisations applied for the <phrase>convolutional neural network</phrase> for the <phrase>time series</phrase> domain.
Factor Endowments, Inequality, and Paths of Development among New World Economies Factor Endowments, Inequality, and Paths of Development among New World Economies We would like to express deep appreciation for the help of our <phrase>research</phrase> assistants <phrase>Elisa</phrase> Mariscal, Patricia Juarez, and Leah Brooks. We have also benefited from discussions with ABSTRACT Whereas traditional explanations of differences in <phrase>long</phrase>-run paths of development across the <phrase>Americas</phrase> generally point to the significance of differences in national heritage or <phrase>religion</phrase>, we highlight the relevance of stark contrasts in the <phrase>degree</phrase> of inequality in wealth, <phrase>human capital</phrase>, and <phrase>political</phrase> power in <phrase>accounting</phrase> for how fundamental economic institutions evolved over time. We argue, moreover, that the roots of these disparities in the extent of inequality lay in differences in the initial factor endowments (dating back to the <phrase>era</phrase> of <phrase>European colonization</phrase>). We document-through comparative studies of <phrase>suffrage</phrase>, <phrase>public</phrase> land, and schooling policies-systematic patterns by which societies in the <phrase>Americas</phrase> that began with more extreme inequality or heterogeneity in the <phrase>population</phrase> were more likely to develop institutional structures that greatly advantaged members of elite classes (and disadvantaging the bulk of the <phrase>population</phrase>) by providing them with more <phrase>political</phrase> influence and access to economic opportunities. The clear implication is that institutions should not be presumed to be exogenous; <phrase>economists</phrase> need to learn more about where they come from to understand their relation to <phrase>economic development</phrase>. Our findings not only contribute to our <phrase>knowledge</phrase> of why extreme differences in the extent of inequality across New World economies have persisted for centuries, but also to the study of processes of <phrase>long</phrase>-run <phrase>economic growth</phrase> past and present. Geographic patterns in economic performance across societies have <phrase>long</phrase> been recognized, but there has been a recent revival of interest in them among <phrase>economists</phrase>. Confronted by systematic evidence of powerful empirical regularities, such as the per capita income of countries near the <phrase>equator</phrase> lagging far behind that of their neighbors at more moderate <phrase>latitudes</phrase>, researchers hope to gain insight into the processes of <phrase>economic growth</phrase> by exploring the sources of these disparities. One group focuses on the direct effects of conditions closely associated with <phrase>geography</phrase>, such as <phrase>climate</phrase>, <phrase>disease</phrase> environment, <phrase>soil</phrase> quality, or access to markets, and on the availability and <phrase>productivity</phrase> of <phrase>labor</phrase> and other factors of <phrase>production</phrase>. Other scholars, however, highlight how such differentials in performance could be rooted in the indirect effects that <phrase>geography</phrase> and factor endowments have on paths of development through their influences on the ways institutions evolve. 1 Both perspectives have distinguished intellectual traditions, but the question of whether there 
<phrase>Convolutional networks</phrase> and applications in vision Intelligent tasks, such as <phrase>visual perception</phrase>, auditory <phrase>perception</phrase>, and <phrase>language</phrase> understanding require the <phrase>construction</phrase> of good <phrase>internal representations</phrase> of the world (or " features "), which must be invariant to irrelevant variations of the input while, preserving relevant <phrase>information</phrase>. A <phrase>major</phrase> question for <phrase>Machine Learning</phrase> is how to learn such good features automatically. <phrase>Convolutional Networks</phrase> (ConvNets) are a biologically-inspired trainable <phrase>architecture</phrase> that can learn <phrase>invariant features</phrase>. Each stage in a ConvNets is composed of a filter <phrase>bank</phrase>, some non-linearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from <phrase>OCR</phrase> to <phrase>video</phrase> <phrase>surveillance</phrase>, they require large amounts of <phrase>labeled training</phrase> samples. We describe new unsu-pervised <phrase>learning algorithms</phrase>, and new non-linear stages that allow ConvNets to be trained with very few <phrase>labeled samples</phrase>. Applications to <phrase>visual object</phrase> recognition and vision <phrase>navigation</phrase> for off-<phrase>road</phrase> <phrase>mobile</phrase> <phrase>robots</phrase> are described. I. LEARNING <phrase>INTERNAL REPRESENTATIONS</phrase> One of the key questions of <phrase>Vision Science</phrase> (natural and <phrase>artificial</phrase>) is how to produce good <phrase>internal representations</phrase> of the visual world. What sort of internal representation would allow an <phrase>artificial</phrase> vision system to detect and classify objects into categories, independently of pose, scale, illumination, conformation, and clutter? More interestingly, how could an <phrase>artificial</phrase> vision system learn appropriate <phrase>internal representations</phrase> automatically, the way animals and <phrase>human</phrase> seem to learn by simply looking at the world? In the time-honored approach to <phrase>computer vision</phrase> (and to <phrase>pattern recognition</phrase> in <phrase>general</phrase>), the question is avoided: <phrase>internal representations</phrase> are <phrase>produced</phrase> by a <phrase>hand-crafted</phrase> feature extractor, whose output is fed to a trainable classifier. While the issue of learning features has been a topic of interest for many years, considerable progress has been achieved in the last few years with the development of so-called <phrase>deep learning</phrase> methods. Good <phrase>internal representations</phrase> are hierarchical. In vision, <phrase>pixels</phrase> are assembled into edglets, edglets into motifs, motifs into parts, parts into objects, and objects into scenes. This suggests that recognition architectures for vision (and for other modalities such as audio and <phrase>natural language</phrase>) should have multiple trainable stages stacked on top of each other, one for each level in the feature hierarchy. This raises two new questions: what to put in each stage? and how to <phrase>train</phrase> such deep, multi-stage architectures? <phrase>Convolutional Networks</phrase> (ConvNets) are an answer to the first question. Until recently, the answer to the second question was to use 
Yearly <phrase>Report</phrase> on Flexible <phrase>Labor</phrase> and <phrase>Employment</phrase> Yearly <phrase>Report</phrase> on Flexible <phrase>Labor</phrase> and <phrase>Employment</phrase> <phrase>Self-employment</phrase> across Countries in the <phrase>Great Recession</phrase> of 2008-2014 3 yearly <phrase>report</phrase> on flexible <phrase>labor</phrase> and <phrase>employment</phrase> flexibility @work 2015 acknowledgements <phrase>Randstad</phrase> <phrase>Randstad</phrase> specializes in solutions in the field of flexible work and <phrase>human resources</phrase> services. Our services <phrase>range</phrase> from regular temporary staffing and permanent placements to Inhouse, Professionals, Search & Selection, and HR Solutions. The <phrase>Randstad</phrase> Group is one of the leading HR services providers in the world, with top-three positions in <phrase>Argentina</phrase>, and the <phrase>United States</phrase> as well as <phrase>major</phrase> positions in <phrase>Australia</phrase> and <phrase>Japan</phrase>. In 2014, <phrase>Randstad</phrase> had approximately 29,000 corporate employees and around 4,400 branches and Inhouse locations in 39 countries around the world. <phrase>Randstad</phrase> generated <phrase>revenue</phrase> of 17.3 billion in 2014. <phrase>Randstad</phrase> was founded in 1960 and is headquartered in Diemen, the <phrase>Netherlands</phrase>. <phrase>Randstad</phrase> Holding nv is <phrase>listed</phrase> on the <phrase>Euronext</phrase> <phrase>Amsterdam</phrase>, where options for stocks in <phrase>Randstad</phrase> are also traded. For more <phrase>information</phrase>, see www.randstad.com. <phrase>Dartmouth College</phrase> Founded in 1769, <phrase>Dartmouth</phrase> is a <phrase>member</phrase> of the <phrase>Ivy League</phrase> and consistently ranks among the world's greatest <phrase>academic</phrase> institutions. <phrase>Dartmouth</phrase> has forged a singular identity for combining its deep commitment to outstanding <phrase>undergraduate</phrase> <phrase>liberal arts</phrase> and graduate <phrase>education</phrase> with distinguished <phrase>research</phrase> and <phrase>scholarship</phrase> in the <phrase>Arts</phrase> & Sciences and its three leading <phrase>professional</phrase> schoolsthe Geisel <phrase>School</phrase> of <phrase>Medicine</phrase>, <phrase>Thayer School of Engineering</phrase>, and the <phrase>Tuck School of Business</phrase>. <phrase>Dartmouth College</phrase> educates the most promising students and prepares them for a <phrase>lifetime</phrase> of learning and of responsible <phrase>leadership</phrase>, through a faculty dedicated to teaching and the creation of <phrase>knowledge</phrase>. For more <phrase>information</phrase> see dartmouth.edu 4 flexibility@work 2015 preface <phrase>Randstad</phrase> is pleased to present the third edition of Flexibility@work: an annual <phrase>report</phrase> on flexible <phrase>labor</phrase> and <phrase>employment</phrase>. The Flexibility@work <phrase>report</phrase> provides a <phrase>comprehensive</phrase> overview of international <phrase>employment</phrase> trends in the flexible <phrase>labor</phrase> market. Additionally, we zoom in on a specific development in the world of work. The 2015 edition will focus on the development of <phrase>self-employment</phrase> during the <phrase>Great Recession</phrase> of 2008-2014. The study on <phrase>self-employment</phrase>-conducted by <phrase>David</phrase> G. Blanchflower of <phrase>Dartmouth College</phrase>-shows that a higher <phrase>self-employment</phrase> rate was not conducive to grow out of the <phrase>Great Recession</phrase>. The study also reveals that the self-employed are either pushed or pulled into working for themselves. Frequently, the pull self-employed are job-makers and their number is more likely to increase when <phrase>unemployment</phrase> is low. Push <phrase>self-employment</phrase> is more likely to occur due to lack of alternatives when <phrase>unemployment</phrase> is <phrase>high</phrase>. With the <phrase>Great Recession</phrase> <phrase>hitting</phrase> self-employed earnings especially 
Challenges of biological realism and validation in <phrase>simulation</phrase>-based <phrase>medical</phrase> <phrase>education</phrase> OVERVIEW <phrase>Simulation</phrase>, both physical and computer-based, has a rich <phrase>history</phrase> in support of <phrase>medical</phrase> <phrase>education</phrase>. Essentially all these efforts have been aimed at instilling <phrase>concrete</phrase> measurable skills, akin to vocational training. They present learners with choices, facilitating a <phrase>degree</phrase> of learning by doing. The sets of learner choices are usually limited, with choices clearly classified into "right" and "wrong". But much of <phrase>medicine</phrase> is not much like a <phrase>multiple-choice</phrase> <phrase>test</phrase>. The realm of choices is broad and not always easily converted to a <phrase>short</phrase> list. The "correct" answer is not always known by the experienced <phrase>physician</phrase> beforehand, sometimes not even after the die is cast and the future unfolds. Computer <phrase>simulation</phrase> of <phrase>human</phrase> <phrase>disease</phrase> and its treatment can in principle be tremendously useful in the <phrase>education</phrase> of both <phrase>basic</phrase> and clinical scientists. This <phrase>paper</phrase> describes some challenges in the <phrase>construction</phrase> of <phrase>simulation</phrase>-based "<phrase>liberal arts</phrase>" biomedical <phrase>education</phrase>. OBJECTIVES The <phrase>educator</phrase> attempting to develop a <phrase>learning environment</phrase> based on <phrase>simulation</phrase> of <phrase>biology</phrase> faces some special challenges. The challenges addressed in this <phrase>paper</phrase> are: face validity and deep validity; finding the right <phrase>degree</phrase> of realism; authoring biomedical models efficiently; managing <phrase>randomness</phrase>. To illustrate the issues, we trace the <phrase>history</phrase> of the <phrase>Oncology</phrase> Thinking <phrase>Cap</phrase> throughout several versions and expansions of educational objectives, and describe the detection and remediation of shortcomings related to these issues. <phrase>DESIGN</phrase> Dealing effectively with issues of validity and realism can be accomplished if the acquisition of <phrase>information</phrase> driving and justifying the <phrase>model</phrase> development choices is documented, preferably automatically, during the process. Efficiency in authoring is greatly enhanced by judicious modularity to encourage re-use, and by the use of templated statements rather than raw code or exotic <phrase>graphical</phrase> components to represent the instructions driving the <phrase>model</phrase>. <phrase>Randomness</phrase> can be used to familiarize learners with the true relative proportions of types of cases, or to enrich the encountered cases with rarer but more instructive cases. When a learner repeats an encounter with a scenario while changing a <phrase>single</phrase> option, proper <phrase>management</phrase> of <phrase>randomness</phrase> is essential to avoid artifacts of random number generators. Otherwise an outcome change caused by a shift in random number streams may masquerade as an outcome change due to the changed option. CONCLUSION Effective use of computer <phrase>simulation</phrase> of <phrase>human</phrase> <phrase>disease</phrase> and its treatment for biomedical <phrase>education</phrase> faces daunting obstacles, but these problems can be solved.
A Deep Non-linear Feature Mapping for <phrase>Large-Margin</phrase> kNN Classification KNN is one of the most popular classification methods, but it often fails to work well with inappropriate choice of distance metric or due to the presence of numerous class-irrelevant features. Linear feature transformation methods have been widely applied to extract class-relevant <phrase>information</phrase> to improve kNN classification , which is very limited in many applications. Kernels have also been used to learn powerful non-linear feature transformations, but these methods fail to scale to <phrase>large datasets</phrase>. In this <phrase>paper</phrase>, we present a scalable non-linear feature mapping method based on a <phrase>deep neural network</phrase> pretrained with Restricted Boltz-mann Machines for improving kNN classification in a <phrase>large-margin</phrase> framework, which we call DNet-kNN. DNet-kNN can be used for both classification and for supervised <phrase>dimensionality reduction</phrase>. The <phrase>experimental</phrase> <phrase>results</phrase> on two benchmark <phrase>handwritten digit</phrase> datasets show that DNet-kNN has much better performance than <phrase>large-margin</phrase> kNN using a linear mapping and kNN based on a deep autoencoder pretrained with <phrase>Restricted Boltzmann Machines</phrase>.
Multi-<phrase>Pass</phrase> Adaptive Voting for Nuclei Detection in Histopathological Images Nuclei detection is often a critical initial step in the development of <phrase>computer aided</phrase> diagnosis and <phrase>prognosis</phrase> schemes in the context of <phrase>digital</phrase> <phrase>pathology</phrase> images. While over the last few years, a number of nuclei detection methods have been proposed, most of these approaches make idealistic assumptions about the <phrase>staining</phrase> quality of the tissue. In this <phrase>paper</phrase>, we present a new Multi-<phrase>Pass</phrase> Adaptive Voting (MPAV) for nuclei detection which is specifically geared towards images with poor quality <phrase>staining</phrase> and noise on account of tissue preparation artifacts. The MPAV utilizes the symmetric <phrase>property</phrase> of nuclear boundary and adaptively selects <phrase>gradient</phrase> from edge fragments to perform voting for a potential nucleus location. The MPAV was evaluated in three cohorts with different <phrase>staining</phrase> methods: Hematoxylin &<phrase>Eosin</phrase>, CD31 &Hematoxylin, and Ki-67 and where most of the nuclei were unevenly and imprecisely stained. Across a total of 47 images and nearly 17,700 manually labeled nuclei serving as the <phrase>ground truth</phrase>, MPAV was able to achieve a <phrase>superior</phrase> performance, with an <phrase>area</phrase> under the precision-recall curve (<phrase>AUC</phrase>) of 0.73. Additionally, MPAV also outperformed three <phrase>state</phrase>-of-the-<phrase>art</phrase> nuclei detection methods, a <phrase>single</phrase> <phrase>pass</phrase> voting method, a multi-<phrase>pass</phrase> voting method, and a <phrase>deep learning</phrase> based method.
Encouraging on-line participation? How do you encourage or facilitate on-line participation? What constitutes effective participation? The <phrase>paper</phrase> firstly examines selected theories about encouraging effective on-line participation and secondly, reviews a <phrase>range</phrase> of qualitative and quantitative methods for assessing the effectiveness of students' on-line participation. The <phrase>author</phrase> aims to make informed recommendations on strategies to encourage on-line participation and relevant criteria for assessing participation in on-line discussions, based on an extensive <phrase>literature review</phrase>. Within the scope of this <phrase>paper</phrase>, on-line participation will be analysed in the context of discussions within <phrase>internet</phrase>-based <phrase>learning environments</phrase> only. Introduction There are many commercially available <phrase>internet</phrase>-based learning <phrase>software</phrase> packages that include on-line discussion tools (Comparison of online course delivery <phrase>software</phrase> <phrase>products</phrase>, 1999). However, does access to these tools and knowing that their participation will be assessed, encourage students' on-line participation? For Klemm and Snell (1996) "commonly, many students "lurk" in the backgroundsuch discussions are not very rigorous andthe quality of instruction suffers". Where discussion is considered a necessary learning method within a course of study, the challenge to the <phrase>educator</phrase> is to facilitate effective <phrase>student</phrase> participation (Davis, 1999). This concept of discursive learning is an <phrase>aspect</phrase> of Bandura's (1971) <phrase>social learning theory</phrase>-where understanding and learning is acquired through modelling, the process of observing and formulating an understanding, as a guide to one's own behaviour. Participation within on-line discussions is defined in this <phrase>paper</phrase> as the process where learners and educators are actively engaged in on-line <phrase>text-based</phrase> <phrase>communication</phrase> with each other. Effective participation occurs where this <phrase>communication</phrase> facilitates the development of a <phrase>deep understanding</phrase> of the material through sharing and critically evaluating ideas, and where connections are made within the material or with independently sourced material. Within the scope of this <phrase>paper</phrase>, on-line participation will be analysed in the context of <phrase>text-based</phrase> discussions within <phrase>internet</phrase>-based <phrase>learning environments</phrase> only.
A Learnable Constraint-based <phrase>Grammar</phrase> Formalism Lexicalized Well-Founded <phrase>Grammar</phrase> (LWFG) is a recently developed <phrase>syntactic</phrase>-<phrase>semantic</phrase> <phrase>grammar</phrase> formalism for deep <phrase>language</phrase> understanding, which balances expressiveness with provable learnability <phrase>results</phrase>. The learnability result for LWFGs assumes that the <phrase>semantic</phrase> composition constraints are learnable. In this <phrase>paper</phrase>, we show what are the properties and principles the <phrase>semantic</phrase> representation and <phrase>grammar</phrase> formalism require, in <phrase>order</phrase> to be able to learn these constraints from examples, and give a <phrase>learning algorithm</phrase>. We also introduce a LWFG parser as a <phrase>deductive</phrase> system, used as an <phrase>inference engine</phrase> during LWFG induction. An example for learning a <phrase>grammar</phrase> for <phrase>noun</phrase> <phrase>compounds</phrase> is given.
A Comparison between Two Statistical <phrase>Relational</phrase> Models Statistical <phrase>Relational</phrase> Learning has received much attention this last decade. In the ILP <phrase>community</phrase>, several models have emerged for modelling and learning uncertain <phrase>knowledge</phrase>, expressed in <phrase>subset</phrase> of first <phrase>order</phrase> logics. Nevertheless , no deep comparisons have been made among them and, given an application , determining which <phrase>model</phrase> must be chosen is difficult. In this <phrase>paper</phrase>, we compare two of them, namely <phrase>Markov</phrase> <phrase>Logic</phrase> Networks and <phrase>Bayesian</phrase> Programs, especially with respect to their representation ability and inference methods. The comparison shows that the two models are substantially different, from the point of view of the user, so that choosing one really means choosing a different <phrase>philosophy</phrase> to look at the problem. In <phrase>order</phrase> to make the comparison more <phrase>concrete</phrase>, we have used a running example, which shows most of the interesting points of the approaches, yet remaining exactly tractable.
A Learning Based <phrase>Model</phrase> for <phrase>Chinese</phrase> Co-reference Resolution by <phrase>Mining</phrase> Contextual Evidence This <phrase>paper</phrase> presents a learning based <phrase>model</phrase> for <phrase>Chinese</phrase> co-reference resolution, in which diverse contextual features are explored inspired by related <phrase>linguistic</phrase> theory. Our main <phrase>motivation</phrase> is to try to boost the co-reference resolution performance only by leveraging multiple shallow <phrase>syntactic</phrase> and <phrase>semantic</phrase> features, which can escape from tough problems such as <phrase>deep syntactic</phrase> and <phrase>semantic</phrase> <phrase>structural analysis</phrase>. Also, <phrase>reconstruction</phrase> of surface features based on contextual <phrase>semantic</phrase> similarity is conducted to approximate the <phrase>syntactic</phrase> and <phrase>semantic</phrase> parallel preferences in resolution <phrase>linguistic</phrase> theories. Furthermore, we consider two classifiers in the <phrase>machine learning</phrase> framework for the co-reference resolution, and performance comparison and combination between them are conducted and investigated. We experimentally evaluate our approaches on standard <phrase>ACE</phrase> (Automatic Content Extraction) corpus with <phrase>promising results</phrase>.
System-level Techniques for <phrase>Temperature</phrase>-aware <phrase>Energy</phrase> Optimization System-level Techniques for <phrase>Temperature</phrase>-aware <phrase>Energy</phrase> Optimization System-level Techniques for <phrase>Temperature</phrase>-aware <phrase>Energy</phrase> Optimization <phrase>Energy</phrase> consumption has become one of the main <phrase>design</phrase> constraints in today's <phrase>integrated circuits</phrase>. Techniques for <phrase>energy</phrase> optimization, from circuit-level up to system-level, have been intensively researched. The advent of <phrase>large-scale</phrase> integration with deep sub-<phrase>micron</phrase> technologies has <phrase>led</phrase> to both <phrase>high</phrase> power densities and <phrase>high</phrase> chip working temperatures. At the same time, leakage power is becoming the dominant power consumption source of circuits, due to continuously lowered threshold voltages, as <phrase>technology</phrase> scales. In this context, <phrase>temperature</phrase> is an important parameter. One <phrase>aspect</phrase>, of particular interest for this <phrase>thesis</phrase>, is the strong inter-dependency between leakage and <phrase>temperature</phrase>. Apart from leakage power, <phrase>temperature</phrase> also has an important impact on circuit delay and, implicitly, on the <phrase>frequency</phrase>, mainly through its influence on <phrase>carrier</phrase> mobility and <phrase>threshold voltage</phrase>. For power-aware <phrase>design</phrase> techniques, <phrase>temperature</phrase> has become a <phrase>major</phrase> factor to be considered. In this <phrase>thesis</phrase>, we address the issue of system-level <phrase>energy</phrase> optimization for real-time <phrase>embedded systems</phrase> taking <phrase>temperature</phrase> aspects into consideration. We have investigated two problems in this <phrase>thesis</phrase>: (1) <phrase>Energy</phrase> optimization via <phrase>temperature</phrase>-aware dynamic <phrase>voltage</phrase>/<phrase>frequency</phrase> scaling (DVFS). (2) <phrase>Energy</phrase> optimization through <phrase>temperature</phrase>-aware idle time (or slack) distribution (ITD). For the above two problems, we have proposed off-line techniques where only static slack is considered. To further improve <phrase>energy</phrase> efficiency, we have also proposed on-line techniques, which make use of both static and dynamic slack. <phrase>Experimental</phrase> <phrase>results</phrase> have demonstrated that considerable improvement of the <phrase>energy</phrase> efficiency can be achieved by applying our <phrase>temperature</phrase>-aware optimization techniques. Another contribution of this <phrase>thesis</phrase> is an analytical <phrase>temperature</phrase> analysis approach which is both accurate and sufficiently fast to be used inside an <phrase>energy</phrase> optimization loop. Acknowledgements During the time that I am working on this <phrase>thesis</phrase>, I have learned a lot about how to do and how to present <phrase>research</phrase>. There are many people who have, along the way, contributed to my progress. I would like to <phrase>express my gratitude</phrase> to them all. First of all, I would like to thank my supervisor, Prof. Zebo Peng, for offering me the opportunity to pursue my postgraduate study here. He is extremely supportive and has given me countless valuable advice and help since the first day I came here. Secondly, I would like to thank my second supervisor Prof. Petru Eles. Discussing my <phrase>research</phrase> with him is very enjoyable, and I can always get insightful and inspiring feedbacks. I deeply appreciate his <phrase>patience</phrase> and dedication in teaching and improving my 
Linear-Nonlinear-<phrase>Poisson</phrase> <phrase>Neurons</phrase> Can Do Inference On <phrase>Deep Boltzmann Machines</phrase> One conjecture in both <phrase>deep learning</phrase> and <phrase>classical</phrase> <phrase>connectionist</phrase> viewpoint is that the biological <phrase>brain</phrase> implements certain kinds of deep networks as its back-end. However, to our <phrase>knowledge</phrase>, a detailed correspondence has not yet been set up, which is important if we want to <phrase>bridge</phrase> between <phrase>neuroscience</phrase> and <phrase>machine learning</phrase>. Recent researches emphasized the biological plausibility of Linear-Nonlinear-<phrase>Poisson</phrase> (<phrase>LNP</phrase>) <phrase>neuron</phrase> <phrase>model</phrase>. We show that with neurally plausible choices of parameters, the whole <phrase>neural network</phrase> is capable of representing any <phrase>Boltzmann</phrase> machine and performing a semi-<phrase>stochastic</phrase> <phrase>Bayesian inference</phrase> <phrase>algorithm</phrase> lying between <phrase>Gibbs sampling</phrase> and variational inference.
Deep-based Ingredient Recognition for <phrase>Cooking</phrase> Recipe Retrieval Retrieving recipes corresponding to given dish pictures facilitates the estimation of <phrase>nutrition</phrase> facts, which is crucial to various <phrase>health</phrase> relevant applications. The current approaches mostly focus on recognition of <phrase>food</phrase> category based on global dish appearance without explicit analysis of ingredient composition. Such approaches are incapable for retrieval of recipes with unknown <phrase>food</phrase> categories, a problem referred to as zero-<phrase>shot</phrase> retrieval. On the other hand, content-based retrieval without <phrase>knowledge</phrase> of <phrase>food</phrase> categories is also difficult to attain satisfactory performance due to large visual variations in <phrase>food</phrase> appearance and ingredient composition. As the number of ingredients is far less than <phrase>food</phrase> categories, understanding ingredients underlying dishes in principle is more scalable than recognizing every <phrase>food</phrase> category and thus is suitable for zero-<phrase>shot</phrase> retrieval. Nevertheless, ingredient recognition is a task far harder than <phrase>food</phrase> categorization, and this seriously challenges the feasibility of relying on them for retrieval. This <phrase>paper</phrase> proposes <phrase>deep architectures</phrase> for simultaneous learning of ingredient recognition and <phrase>food</phrase> categorization, by exploiting the mutual but also fuzzy relationship between them. The learnt deep features and <phrase>semantic</phrase> <phrase>labels</phrase> of ingredients are then innovatively applied for zero-<phrase>shot</phrase> retrieval of recipes. By experimenting on a large <phrase>Chinese food</phrase> dataset with images of highly complex dish appearance, this <phrase>paper</phrase> demonstrates the feasibility of ingredient recognition and sheds <phrase>light</phrase> on this zero-<phrase>shot</phrase> problem peculiar to <phrase>cooking</phrase> recipe retrieval.
<phrase>Deep Learning</phrase> of Representations for Unsupervised and <phrase>Transfer Learning</phrase> <phrase>Deep learning</phrase> <phrase>algorithms</phrase> seek to exploit the unknown structure in the input distribution in <phrase>order</phrase> to discover good representations, often at <phrase>multiple levels</phrase>, with <phrase>higher-level</phrase> <phrase>learned features</phrase> defined in terms of <phrase>lower</phrase>-<phrase>level features</phrase>. The objective is to make these <phrase>higher-level</phrase> representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the <phrase>information</phrase> in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such <phrase>unsupervised learning</phrase> of representations can be exploited usefully under the <phrase>hypothesis</phrase> that the input distribution P (x) is structurally related to some task of interest, say predicting P (y|x). This <phrase>paper</phrase> focusses on why <phrase>unsupervised pre-training</phrase> of representations can be useful, and how it can be exploited in the <phrase>transfer learning</phrase> scenario, where we care about predictions on examples that are not from the same distribution as the training distribution.
How Incorporating <phrase>Feedback</phrase> Mechanisms in a DSS Affects DSS Evaluations M odel-based <phrase>decision support systems</phrase> (DSS) <phrase>improve performance</phrase> in many contexts that are <phrase>data</phrase>-rich, uncertain, and require repetitive decisions. But such DSS are often not designed to help users understand and internalize the underlying factors driving DSS recommendations. Users then feel uncertain about DSS recommendations, leading them to possibly avoid using the system. We argue that a DSS must be designed to induce an alignment of a decision maker's <phrase>mental model</phrase> with the decision <phrase>model</phrase> embedded in the DSS. Such an alignment requires effort from the decision maker and guidance from the DSS. We experimentally evaluate two DSS <phrase>design</phrase> characteristics that facilitate such alignment: (i) <phrase>feedback</phrase> on the upside potential for <phrase>performance improvement</phrase> and (<phrase>ii</phrase>) <phrase>feedback</phrase> on corrective actions to improve decisions. We show that, in <phrase>tandem</phrase>, these two types of DSS <phrase>feedback</phrase> induce decision makers to align their mental models with the decision <phrase>model</phrase>, a process we call <phrase>deep learning</phrase>, whereas individually these two types of <phrase>feedback</phrase> have little effect on <phrase>deep learning</phrase>. We also show that <phrase>deep learning</phrase>, in turn, improves user evaluations of the DSS. We discuss how our findings could <phrase>lead</phrase> to DSS <phrase>design</phrase> improvements and better returns on DSS investments.
What kind of <phrase>graphical model</phrase> is the <phrase>brain</phrase>? If <phrase>neurons</phrase> are treated as <phrase>latent variables</phrase>, our visual systems are non-linear, densely-connected <phrase>graphical</phrase> models containing billions of variables and thousands of billions of parameters. Current <phrase>algorithms</phrase> would have difficulty learning a <phrase>graphical model</phrase> of this scale. Starting with an <phrase>algorithm</phrase> that has difficulty learning more than a few thousand parameters, I describe a series of progressively better <phrase>learning algorithms</phrase> all of which are designed to run on <phrase>neuron</phrase>-like hardware. The latest <phrase>member</phrase> of this series can learn deep, <phrase>multi-layer</phrase> <phrase>belief nets</phrase> quite rapidly. It turns a generic network with three <phrase>hidden layers</phrase> and 1.7 million connections into a very good <phrase>genera</phrase>-tive <phrase>model</phrase> of <phrase>handwritten digits</phrase>. After learning, the <phrase>model</phrase> gives classification performance that is comparable to the best discriminative methods.
<phrase>Memory</phrase>-aware Bounded <phrase>Model Checking</phrase> for Linear <phrase>Hybrid</phrase> Systems Bounded <phrase>Model Checking</phrase> (BMC) is a successful method for refuting properties of erroneous systems. Initially applied to discrete systems only, BMC could be extended to more complex domains like linear <phrase>hybrid</phrase> <phrase>automata</phrase>. The increasing complexity coming along with these complex models, but also recent optimizations of <phrase>SAT</phrase>-based BMC, like excessive conflict learning, reveal a <phrase>memory</phrase> explosion problem especially for deep counterexamples. In this <phrase>paper</phrase> we introduce parametric <phrase>data</phrase> types for the internal solver structure that, taking advantage of the <phrase>symmetry</phrase> of BMC problems, remarkably reduce the <phrase>memory</phrase> requirements of the solver.
Settable Systems: An Extension of Pearl's Causal <phrase>Model</phrase> with Optimization, Equilibrium, and Learning <phrase>Judea</phrase> Pearl's Causal <phrase>Model</phrase> is a rich framework that provides deep insight into the <phrase>nature</phrase> of causal relations. As yet, however, the <phrase>Pearl</phrase> Causal <phrase>Model</phrase> (<phrase>PCM</phrase>) has had a lesser impact on <phrase>economics</phrase> or <phrase>econometrics</phrase> than on other disciplines. This may be due in part to the fact that the <phrase>PCM</phrase> is not as well suited to analyzing structures that exhibit features of central interest to <phrase>economists</phrase> and econo-metricians: optimization, equilibrium, and learning. We offer the settable systems framework as an extension of the <phrase>PCM</phrase> that permits causal discourse in systems embodying optimization, equilibrium , and learning. Because these are common features of physical, natural, or social systems, our framework may prove generally useful for <phrase>machine learning</phrase>. Important features distinguishing the settable system framework from the <phrase>PCM</phrase> are its countable dimensionality and the use of partitioning and <phrase>partition</phrase>-specific response functions to accommodate the behavior of optimizing and interacting agents and to eliminate the requirement of a unique <phrase>fixed point</phrase> for the system. Refinements of the <phrase>PCM</phrase> include the settable systems treatment of attributes, the causal role of ex-ogenous variables, and the dual role of variables as causes and responses. A series of closely related <phrase>machine learning</phrase> examples and examples from <phrase>game theory</phrase> and <phrase>machine learning</phrase> with <phrase>feedback</phrase> demonstrates some limitations of the <phrase>PCM</phrase> and motivates the distinguishing features of settable systems.
Part Three: <phrase>Lgbt</phrase> Teaching Resources Portions of this publication may be reproduced with acknowledgment for educational purposes. CONTRIBUTORS >>acknowledgments T he editors are indebted to the members of the editorial board who initiated and supported this project from inception to print. We thank Margaret Himley, Adrea Jaehnig, and Andrew <phrase>London</phrase> for their sustained commitment to the lives and concerns of <phrase>LGBT</phrase> people at SU. Their critical engagement with <phrase>LGBT</phrase> issues continues to both encourage and challenge SU's communitystudents, faculty, staff, and administratorsto make the lives of <phrase>LGBT</phrase> people central to the goals of the <phrase>university</phrase>. In addition, we are grateful to Derina Samuel and Stacey Lane Tice for providing their support, advice, and the <phrase>professional</phrase> opportunity to make this volume a <phrase>reality</phrase>. We also appreciate the generous contribution of time, experiences, and ideas offered by the faculty and instructors who agreed to be interviewed for Part Two of this volume. In addition, the editors would like to acknowledge the support for this volume from the <phrase>Senate</phrase> Committee on <phrase>LGBT</phrase> Concerns and the <phrase>LGBT</phrase> Resource <phrase>Center</phrase>. The <phrase>Professional</phrase> Development Programs of the Graduate <phrase>School</phrase> provided significant support and funding for the overall project and the Divisions of <phrase>Undergraduate</phrase> Studies and <phrase>Student</phrase> Affairs provided partial financial support for the <phrase>printing</phrase> of this volume in <phrase>order</phrase> to enhance, broaden, and support the <phrase>academic</phrase> and social experience of <phrase>Syracuse University</phrase> students. Special thanks to Andrew Augeri for his creative genius in designing the <phrase>cover</phrase> and <phrase>page layout</phrase>, as well as for his <phrase>patience</phrase> with our often chaotic editorial process. Also, special thanks to <phrase>Tina</phrase> Mishko for her administrative support and excellent organizational skills. This <phrase>book</phrase> is the product of a collaborative effort by over 30 volunteers. We would like to express our deep appreciation to the following contributors who volunteered their time, <phrase>energy</phrase>, expertise, and enthusiasm to help make this project a substantial contribution to the <phrase>Syracuse University</phrase> teaching <phrase>community</phrase>: <phrase>ii</phrase> ACKNOWLEDGMENTS S yracuse <phrase>University</phrase> is a microcosm of today's world, enrolling representatives of most races and <phrase>ethnic</phrase> groups found in the <phrase>United States</phrase> and more than 2,000 international students from 100 countries. This is not by chance. The <phrase>University</phrase> embraces diversity as one of its core values and enjoys a reputation for being an institution where disparate groups among its students, faculty, and staff come together as one <phrase>community</phrase> of scholars to learn from each other and to prepare for a world that every day grows increasingly more 
On Infectious Intestinal <phrase>Disease Surveillance</phrase> using <phrase>Social Media</phrase> Content This <phrase>paper</phrase> investigates whether infectious intestinal diseases (IIDs) can be detected and quantified using <phrase>social media</phrase> content. Experiments are conducted on <phrase>user-generated</phrase> <phrase>data</phrase> from the <phrase>microblogging</phrase> service, <phrase>Twitter</phrase>. Evaluation is based on the comparison with the number of <phrase>IID</phrase> cases reported by traditional <phrase>health</phrase> <phrase>surveillance</phrase> methods. We employ a <phrase>deep learning</phrase> approach for creating a topical <phrase>vocabulary</phrase>, and then apply a regularised linear (Elastic Net) as well as a nonlinear (<phrase>Gaussian Process</phrase>) <phrase>regression</phrase> <phrase>function</phrase> for inference. We show that like previous text <phrase>regression</phrase> tasks, the nonlinear approach performs better. In <phrase>general</phrase>, our <phrase>experimental</phrase> <phrase>results</phrase>, both in terms of predictive performance and <phrase>semantic</phrase> interpretation, indicate that <phrase>Twitter</phrase> <phrase>data</phrase> contain a signal that could be strong enough to <phrase>complement</phrase> conventional methods for <phrase>IID</phrase> <phrase>surveillance</phrase>.
Using <phrase>Information Extraction</phrase> to Generate Trigger Questions for <phrase>Academic</phrase> Writing Support Automated question generation approaches have been proposed to support <phrase>reading comprehension</phrase>. However, these approaches are not suitable for supporting writing activities. We present a novel approach to generate different forms of trigger questions (directive and facilitative) aimed at supporting <phrase>deep learning</phrase>. Useful <phrase>semantic</phrase> <phrase>information</phrase> from <phrase>Wikipedia</phrase> articles is extracted and linked to the key phrases in a students' <phrase>literature review</phrase>, particularly focusing on extracting <phrase>information</phrase> containing 3 types of relations (Kind of, Similar-to and Different-to) by using <phrase>syntactic</phrase> <phrase>pattern matching</phrase> rules. We collected <phrase>literature</phrase> reviews from 23 <phrase>Engineering</phrase> <phrase>research</phrase> students, and evaluated the quality of 306 computer generated questions and 115 generic questions. Facilitative questions are more useful when it comes to <phrase>deep learning</phrase> about the topic, while directive questions are clearer and useful for improving the composition.
Optimizing deep bottleneck <phrase>feature extraction</phrase> We investigate several optimizations to a recently published <phrase>architecture</phrase> for extracting bottleneck features for large-<phrase>vocabulary</phrase> <phrase>speech recognition</phrase> with <phrase>deep neural networks</phrase>. We are able to improve <phrase>recognition performance</phrase> of first-<phrase>pass</phrase> systems from <phrase>a 12</phrase>% relative word <phrase>error rate</phrase> reduction reported previously to 21%, compared to MFCC baselines on a <phrase>Tagalog</phrase> conversational <phrase>telephone</phrase> speech corpus. This is achieved by using different input features, training the network to predict <phrase>context-dependent</phrase> targets, employing an efficient learning rate schedule and varying several <phrase>architectural</phrase> details. Evaluations on two larger <phrase>German</phrase> and <phrase>French</phrase> speech transcription tasks show that the optimizations proposed are universally applicable and yield comparable gains on other corpora (19.9% and 22.8%, respectively).
Efficient <phrase>Graph</phrase> Kernels for Textual Entailment Recognition One of the most important <phrase>research</phrase> <phrase>area</phrase> in <phrase>Natural Language Processing</phrase> concerns the modeling of <phrase>semantics</phrase> expressed in text. Since foundational work in <phrase>Natural Language Understanding</phrase> has shown that a deep <phrase>semantic</phrase> approach is still not feasible, <phrase>current research</phrase> is focused on shallow methods combining <phrase>linguistic</phrase> models and <phrase>machine learning</phrase> techniques. The latter aim at learning <phrase>semantic</phrase> models, like those that can detect the entailment between the meaning of two text fragments, by means of <phrase>training examples</phrase> described by specific features. These are rather difficult to <phrase>design</phrase> since there is no <phrase>linguistic</phrase> <phrase>model</phrase> that can effectively encode the lexico-<phrase>syntactic</phrase> level of a sentence and its corresponding <phrase>semantic</phrase> models. Thus, the adopted <phrase>solution</phrase> consists in exhaustively describing <phrase>training examples</phrase> by means of all possible combinations of sentence words and <phrase>syntactic</phrase> <phrase>information</phrase>. The latter, typically expressed as parse <phrase>trees</phrase> of text fragments, is often encoded in the learning process using <phrase>graph</phrase> <phrase>algorithms</phrase>. In this <phrase>paper</phrase>, we propose a class of <phrase>graphs</phrase>, the tripartite <phrase>directed</phrase> <phrase>acyclic</phrase> <phrase>graphs</phrase> (tDAGs), which can be efficiently used to <phrase>design</phrase> <phrase>algorithms</phrase> for <phrase>graph</phrase> kernels for <phrase>semantic</phrase> <phrase>natural language</phrase> tasks 2 Zanzotto, Dell'Arciprete, Moschitti / Efficient <phrase>Graph</phrase> Kernels for <phrase>RTE</phrase> involving sentence pairs. These <phrase>model</phrase> the matching between two pairs of <phrase>syntactic</phrase> <phrase>trees</phrase> in terms of all possible <phrase>graph</phrase> fragments. Interestingly, since tDAGs encode the association between identical or similar words (i.e. variables), it can be used to represent and learn first-<phrase>order</phrase> rules, i.e. rules describ-able by <phrase>first-order logic</phrase>. We prove that our matching <phrase>function</phrase> is a valid kernel and we empirically show that, although its evaluation is still exponential in the worst case, it is extremely efficient and more accurate than the previously proposed kernels.
Learning dispositions and transferable competencies: <phrase>pedagogy</phrase>, modelling and <phrase>learning analytics</phrase> Theoretical and <phrase>empirical evidence</phrase> in the <phrase>learning sciences</phrase> substantiates the view that deep engagement in learning is a <phrase>function</phrase> of a complex combination of learners' identities, dispositions, values, attitudes and skills. When these are fragile, learners struggle to achieve their potential in conventional assessments, and critically, are not prepared for the novelty and complexity of the challenges they will meet in the workplace, and the many other spheres of <phrase>life</phrase> which require personal qualities such as resilience, <phrase>critical thinking</phrase> and collaboration skills. To date, the <phrase>learning analytics</phrase> <phrase>research</phrase> and development communities have not addressed how these complex concepts can be modelled and analysed, and how more traditional <phrase>social science</phrase> <phrase>data analysis</phrase> can support and be enhanced by <phrase>learning analytics</phrase>. We <phrase>report</phrase> progress in the <phrase>design</phrase> and implementation of <phrase>learning analytics</phrase> based on a <phrase>research</phrase> validated multidimensional construct termed "learning power". We describe, for the first time, a <phrase>learning analytics</phrase> <phrase>infrastructure</phrase> for gathering <phrase>data</phrase> at scale, managing stakeholder permissions, the <phrase>range</phrase> of analytics that it supports from real time summaries to exploratory <phrase>research</phrase>, and a particular visual analytic which has been shown to have demonstrable impact on learners. We conclude by summarising the ongoing <phrase>research</phrase> and development programme and identifying the challenges of integrating traditional <phrase>social science</phrase> <phrase>research</phrase>, with <phrase>learning analytics</phrase> and modelling.
Generalizing Skills with <phrase>Semi-Supervised</phrase> <phrase>Reinforcement Learning</phrase> <phrase>Deep reinforcement learning</phrase> (<phrase>RL</phrase>) can acquire complex behaviors from <phrase>low-level</phrase> inputs, such as images. However, <phrase>real-world</phrase> applications of such methods require generalizing to the vast variability of the <phrase>real world</phrase>. Deep networks are known to achieve remarkable generalization when provided with massive amounts of <phrase>labeled data</phrase>, but can we provide this breadth of experience to an <phrase>RL</phrase> agent, such as a <phrase>robot</phrase>? The <phrase>robot</phrase> might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a <phrase>reward function</phrase>, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the <phrase>real world</phrase>, especially in domains such as <phrase>robotics</phrase> and dialog systems, where the reward could depend on the unknown positions of objects or the emotional <phrase>state</phrase> of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a <phrase>human</phrase> supervisor is present, or in a controlled <phrase>laboratory</phrase> setting. Can we make use of this limited supervision , and still benefit from the breadth of experience an agent might collect in the unstructured <phrase>real world</phrase>? In this <phrase>paper</phrase>, we formalize this problem setting as <phrase>semi-supervised</phrase> <phrase>reinforcement learning</phrase> (SSRL), where the <phrase>reward function</phrase> can only be evaluated in a set of " labeled " MDPs, and the agent must generalize its behavior to the wide <phrase>range</phrase> of states it might encounter in a set of " unlabeled " MDPs, by using experience from both settings. Our <phrase>proposed method</phrase> infers the task objective in the unlabeled MDPs through an <phrase>algorithm</phrase> that resembles inverse <phrase>RL</phrase>, using the agent's own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging tasks that require control directly from images, and show that our approach can improve the generalization of a learned <phrase>deep neural network</phrase> policy by using experience for which no <phrase>reward function</phrase> is available. We also show that our method outperforms direct <phrase>supervised learning</phrase> of the reward.
<phrase>E</phrase>-learning and the Educational Organizations Structure Reengineering (EOSR) There are many calls for innovative learning methods that utilize advanced technologies. However, we will raise fundamental questions that look deep into the future of the educational <phrase>organization</phrase>. Can the educational institute survive without adapting learning technologies? Would the educational institute succeed in adapting new learning technologies without changing its organizational structure and processes? We claim that the answer to both questions is no. Our <phrase>research</phrase> will present the need for educational institutes to incorporate learning technologies and focuses on the demand for the educational <phrase>organization</phrase> structure reengineering as a <phrase>basic</phrase> requirement for the success of incorporating learning technologies. Our study explores the faculty requirements and policies and procedures of educational institutes in the <phrase>UAE</phrase>. The <phrase>paper</phrase> concludes with some discussions on findings from a <phrase>case study</phrase> of the need of educational <phrase>organization</phrase> structure reengineering as a <phrase>basic</phrase> requirement for incorporating learning technologies.
<phrase>MicroRNA</phrase> <phrase>Target</phrase> Detection and Analysis for <phrase>Genes</phrase> Related to <phrase>Breast Cancer</phrase> Using MDLcompress We describe initial <phrase>results</phrase> of <phrase>miRNA</phrase> <phrase>sequence analysis</phrase> with the optimal <phrase>symbol</phrase> <phrase>compression ratio</phrase> (OSCR) <phrase>algorithm</phrase> and recast this <phrase>grammar</phrase> inference <phrase>algorithm</phrase> as an improved <phrase>minimum description length</phrase> (MDL) learning tool: MDLcompress. We apply this tool to explore the relationship between miRNAs, <phrase>single nucleotide polymorphisms</phrase> (<phrase>SNPs</phrase>), and <phrase>breast cancer</phrase>. Our new <phrase>algorithm</phrase> outperforms other <phrase>grammar</phrase>-based coding methods, such as <phrase>DNA</phrase> Sequitur, while retaining a two-part code that highlights biologically significant phrases. The deep <phrase>recursion</phrase> of MDLcompress, together with its explicit two-part coding, enables it to identify biologically meaningful <phrase>sequence</phrase> without needlessly restrictive priors. The ability to quantify cost in <phrase>bits</phrase> for phrases in the MDL <phrase>model</phrase> allows prediction of regions where <phrase>SNPs</phrase> may have the most impact on <phrase>biological activity</phrase>. MDLcompress improves on our previous <phrase>algorithm</phrase> in execution time through an innovative <phrase>data structure</phrase>, and in specificity of motif detection (compression) through improved heuristics. An MDLcompress analysis of 144 over expressed <phrase>genes</phrase> from the <phrase>breast cancer</phrase> <phrase>cell line</phrase> BT474 has identified novel motifs, including potential <phrase>microRNA</phrase> (<phrase>miRNA</phrase>) binding sites that are candidates for <phrase>experimental</phrase> validation.
<phrase>Deep neural networks</phrase> for <phrase>acoustic</phrase> <phrase>emotion</phrase> recognition: Raising the benchmarks <phrase>Deep Neural Networks</phrase> (DNNs) denote multilayer <phrase>artificial neural networks</phrase> with more than one <phrase>hidden layer</phrase> and millions of <phrase>free</phrase> parameters. We propose a Generalized <phrase>Discriminant</phrase> Analysis (GerDA) based on DNNs to learn <phrase>discriminative features</phrase> of low <phrase>dimension</phrase> optimized with respect to a fast classification from a large set of <phrase>acoustic</phrase> features for <phrase>emotion</phrase> recognition. On nine frequently used emotional speech corpora, we compare the performance of GerDA features and their subsequent linear classification with previously reported benchmarks obtained using the same set of <phrase>acoustic</phrase> features classified by <phrase>Support Vector Machines</phrase> (SVMs). Our <phrase>results</phrase> impressively show that <phrase>low-dimensional</phrase> GerDA features capture hidden <phrase>information</phrase> from the <phrase>acoustic</phrase> features leading to a significantly raised unweighted <phrase>average</phrase> recall and considerably raised weighted <phrase>average</phrase> recall.
Spike Event Based Learning in <phrase>Neural Networks</phrase> A scheme is derived for learning connectivity in spiking <phrase>neural networks</phrase>. The scheme learns instantaneous firing rates that are conditional on the activity in other parts of the network. The scheme is <phrase>independent</phrase> of the choice of <phrase>neuron</phrase> dynamics or activation <phrase>function</phrase>, and <phrase>network architecture</phrase>. It involves two simple, online, local learning rules that are applied only in response to occurrences of spike events. This scheme provides a direct method for transferring ideas between the fields of <phrase>deep learning</phrase> and <phrase>computational neuroscience</phrase>. This learning scheme is demonstrated using a layered <phrase>feed-forward</phrase> spiking <phrase>neural network</phrase> trained self-supervised on a prediction and classification task for moving MNIST images collected using a Dynamic Vision <phrase>Sensor</phrase>.
A Non-<phrase>invasive</phrase> Prospective Approach for the Detection of <phrase>Alzheimer's Disease</phrase> a Non-<phrase>invasive</phrase> Prospective Approach for the Detection of <phrase>Alzheimer's Disease</phrase> <phrase>Biomedical Engineering</phrase> <phrase>Rourkela</phrase> CERTIFICATE This is to certify that the work in the <phrase>thesis</phrase> entitled " A NON-<phrase>INVASIVE</phrase> PROSPECTIVE APPROACH FOR THE DETECTION OF <phrase>ALZHEIMER'S DISEASE</phrase> " submitted by Prashant Kumar is a record of an original <phrase>research</phrase> work carried out by him under my supervision and guidance in partial fulfillment of the requirements for the award of the <phrase>degree</phrase> of <phrase>Master</phrase> of <phrase>Technology</phrase> in <phrase>Biotechnology</phrase> and <phrase>Medical</phrase> <phrase>Engineering</phrase> (<phrase>Biomedical Engineering</phrase>), National Institute of <phrase>Technology</phrase>, <phrase>Rourkela</phrase>. Neither this <phrase>thesis</phrase> nor any part of it, to the best of my <phrase>knowledge</phrase>, has been submitted for any <phrase>degree</phrase> or <phrase>academic</phrase> award elsewhere. <phrase>Rourkela</phrase> DECLARATION I certify that 1. The work contained in the <phrase>thesis</phrase> is original and has been done by myself under the supervision of my supervisor. 2. The work has not been submitted to any other Institute for any <phrase>degree</phrase> or <phrase>diploma</phrase>. 3. I have followed the guidelines provided by the Institute in writing the <phrase>thesis</phrase>. 4. Whenever I have used materials (<phrase>data</phrase>, theoretical analysis, and text) from other sources, I have given due credit to them by citing them in the text of the <phrase>thesis</phrase> and giving their details in the references. 5. Whenever I have quoted written materials from other sources, I have put them under <phrase>quotation marks</phrase> and given due credit to the sources by citing them and giving required details in the references. Acknowledgement This work is one of the most important achievements of my career. Completion of my project would not have been possible without the help of many people, who have constantly helped me with their full support for which I am highly thankful to them. First of all, I would like to <phrase>express my gratitude</phrase> to my supervisor Prof. Subhankar Paul, who has been the guiding force behind this work. I want to thank him for <phrase>giving me the opportunity</phrase> to work under him. He is not only a good <phrase>Professor</phrase> with deep vision but also a very kind person. I consider it my good fortune to have got an opportunity to work with such a wonderful person. helping me how to learn. They have been great sources of inspiration. I would like to thank all <phrase>faculty members</phrase> and staff of the BT/BM <phrase>Department</phrase> for their sympathetic cooperation. I would also like to make a special mention of the selfless support and guidance I received from <phrase>PhD</phrase> <phrase>Scholar</phrase> Mr. Dependra Kumar Ban and Mr. Shailendra 
Exploring Individual and Collaborative Dimensions of <phrase>Knowledge</phrase> Building in an <phrase>Online Learning</phrase> <phrase>Community</phrase> of Practice An exploratory study of students' engagement in <phrase>online learning</phrase> and <phrase>knowledge</phrase> building is presented in this <phrase>paper</phrase>. Learning in an <phrase>online community</phrase>, composed of students (pre-service teachers) and experts (experienced in-service schoolteachers and academics), is the study's primary focus. Students' interaction and <phrase>knowledge</phrase> discourse structures, arising from individual readings of <phrase>academic</phrase> papers and asynchronous collaboration with <phrase>peers</phrase> and experts, are investigated using <phrase>social network</phrase> and <phrase>content analysis</phrase> techniques. Additionally, several new measures for exploring structural-qualitative aspects of <phrase>knowledge</phrase> discourse are introduced. Analysis revealed several important trends. First, students' interaction was more intensive in forums where experienced teachers participated, rather than students only. Second, students' individual discourse structures in their postings were quite deep, <phrase>knowledge</phrase>-focussed and elaborated; while students' replies were <phrase>short</phrase>, usually focussed on specific idea and contained a substantial amount of non-<phrase>cognitive</phrase> <phrase>information</phrase>. Overall, it is argued that students were engaged with the individual and collaborative <phrase>knowledge</phrase> building in the <phrase>online learning</phrase> <phrase>community</phrase>. Practical implications of the study <phrase>results</phrase> for development of courses are discussed.
Learning, <phrase>Memory</phrase>, and the Role of <phrase>Neural Network</phrase> <phrase>Architecture</phrase> The performance of <phrase>information processing</phrase> systems, from <phrase>artificial neural networks</phrase> to natural <phrase>neuronal</phrase> ensembles, depends heavily on the underlying system <phrase>architecture</phrase>. In this study, we compare the performance of parallel and layered network architectures during sequential tasks that require both acquisition and retention of <phrase>information</phrase>, thereby identifying tradeoffs between learning and <phrase>memory</phrase> processes. During the task of supervised, sequential <phrase>function</phrase> approximation, networks produce and adapt representations of external <phrase>information</phrase>. Performance is evaluated by statistically analyzing the error in these representations while varying the initial network <phrase>state</phrase>, the structure of the external <phrase>information</phrase>, and the time given to learn the <phrase>information</phrase>. We link performance to complexity in <phrase>network architecture</phrase> by characterizing local error <phrase>landscape</phrase> <phrase>curvature</phrase>. We find that variations in error <phrase>landscape</phrase> structure give rise to tradeoffs in performance; these include the ability of the network to maximize accuracy versus minimize inaccuracy and produce specific versus generalizable representations of <phrase>information</phrase>. Parallel networks generate smooth error landscapes with deep, narrow minima, enabling them to find highly specific representations given sufficient time. While accurate, however, these representations are difficult to generalize. In contrast, layered networks generate rough error landscapes with a <phrase>variety</phrase> of <phrase>local minima</phrase>, allowing them to quickly find coarse representations. Although less accurate, these representations are easily adaptable. The presence of measurable performance tradeoffs in both layered and parallel networks has implications for understanding the behavior of a wide <phrase>variety</phrase> of natural and <phrase>artificial</phrase> learning systems.
Deep-structured hidden <phrase>conditional random fields</phrase> for phonetic recognition We extend our earlier work on deep-structured <phrase>conditional random field</phrase> (DCRF) and develop deep-structured hidden <phrase>conditional random field</phrase> (DHCRF). We investigate the use of this new sequential <phrase>deep-learning</phrase> <phrase>model</phrase> for phonetic recognition. DHCRF is a hierarchical <phrase>model</phrase> in which the final layer is a hidden <phrase>conditional random field</phrase> (HCRF) and the intermediate layers are zero-th-<phrase>order</phrase> <phrase>conditional random fields</phrase> (CRFs). <phrase>Parameter estimation</phrase> and <phrase>sequence</phrase> inference in the DHCRF are developed in this work. They are carried out <phrase>layer by layer</phrase> so that the time complexity is linear to the number of layers. In the DHCRF, the training <phrase>label</phrase> is available only at the final layer and the <phrase>state</phrase> boundary is unknown. This difficulty is addressed by using <phrase>unsupervised learning</phrase> for the intermediate layers and lattice-based <phrase>supervised learning</phrase> for the final layer. Experiments on the standard TIMIT <phrase>phone recognition</phrase> task show small <phrase>performance improvement</phrase> of a three-layer DHCRF over a two-layer DHCRF; both are significantly better than the <phrase>single</phrase>-layer DHCRF and are <phrase>superior</phrase> to the discriminatively trained tri-phone <phrase>hidden Markov model</phrase> (HMM) using identical input features.
Towards Active Team Based Learning: An Online Instructional Strategy Team-based Learning (TBL) is a relatively new pedagogic approach to teaching that makes extensive use of intensive, interactive team activities in the classroom to deepen learning. To date, TBL has been deployed almost entirely in traditional on-<phrase>campus</phrase> classes. This <phrase>paper</phrase> outlines a strategy and preliminary framework to enhance team <phrase>communication</phrase> and strengthen <phrase>group dynamics</phrase>, leveraging online tools to support TBL techniques, empowering online students in <phrase>active learning</phrase>. It is vital to utilize <phrase>technology</phrase> effectively to structure the online classroom in a manner that best supports TBL' s <phrase>deep learning</phrase> experiences. The <phrase>major</phrase> contributions of this <phrase>research</phrase> will be to extend techniques from TBL approaches to the online group-support environment and to describe effective technological support.
<phrase>Algorithms</phrase> on <phrase>Strings</phrase>, <phrase>Trees</phrase>, and Sequences - <phrase>Computer Science</phrase> and <phrase>Computational Biology</phrase> The <phrase>history</phrase> and <phrase>motivation</phrase> Although I didn't know it at the time, I began writing this <phrase>book</phrase> in the summer of 1988 when I was part of a <phrase>computer science</phrase> <phrase>research</phrase> group at the <phrase>Human Genome</phrase> <phrase>Center</phrase> of Lawrence <phrase>Berkeley</phrase> <phrase>Laboratory</phrase>. Our group followed the standard assumption that biologically meaningful <phrase>results</phrase> could come from considering <phrase>DNA</phrase> as a one-dimensional character string, abstracting away the <phrase>reality</phrase> of <phrase>DNA</phrase> as a flexible three-dimensional <phrase>molecule</phrase>, interacting in a dynamic environment with <phrase>protein</phrase> and <phrase>RNA</phrase>, and repeating a <phrase>life</phrase>-cycle in which even the classic linear <phrase>chromosome</phrase> exists for only a fraction of the time. A similar, but stronger, assumption existed for <phrase>protein</phrase>, holding for example that all the <phrase>information</phrase> needed for correct three-dimensional folding is contained in the <phrase>protein</phrase> <phrase>sequence</phrase> itself, essentiaUy <phrase>independent</phrase> of the biological environment the <phrase>protein</phrase> lives in. This assumption has recently been modified, but remains largely intact. For non-biologlsts, these two assllmptions were (and remain) a godsend allowing rapid entry into an exciting and important field. Statements such as "The <phrase>digital</phrase> <phrase>information</phrase> that underlies <phrase>biochemistry</phrase>, <phrase>cell biology</phrase>, and development can be represented by a simple string of G's, A's, T's and COs. This string is the <phrase>root</phrase> <phrase>data structure</phrase> of an organism's <phrase>biology</phrase>." <phrase>reinforced</phrase> the importance of <phrase>sequence</phrase>-level investigation. So without worrying much about the more diIBcult chemical and biological aspects of <phrase>DNA</phrase> and <phrase>protein</phrase>, our <phrase>computer science</phrase> group was empowered to consider a <phrase>variety</phrase> of biologically important problems defined plrimarily on sequences, or (more in the <phrase>computer science</phrase> <phrase>vernacular</phrase>) on <phrase>strings</phrase>. We organized our efforts into two <phrase>high</phrase>-level tasks. First, to learn the relevant <phrase>biology</phrase>, <phrase>laboratory</phrase> protocols, and existing algorithmic methods used by <phrase>biologists</phrase>. Second to canvass the <phrase>computer science</phrase> <phrase>literature</phrase> for ideas and <phrase>algorithms</phrase> that weren't already used by <phrase>biologists</phrase>, but wkich might plausibly be of use either in current problems, or in problems that we could anticipate arising when vast quantities of sequenced <phrase>DNA</phrase> or <phrase>protein</phrase> become available. None of us was an expert on string <phrase>algorithms</phrase>. At that point I had a <phrase>textbook</phrase> <phrase>knowledge</phrase> of <phrase>Knuth</phrase>-Morris-Pratt~ and a deep confusion about Boyer-Moore (under what circumstances it was a linear time <phrase>algorithm</phrase>, and how to do strong preprocessing in linear time). I understood the use of <phrase>dynamic programming</phrase> to compute edit distance, but otherwise had little exposure to specific string <phrase>algorithm</phrase>~ in <phrase>biology</phrase>. My <phrase>general</phrase> background was in <phrase>combinatorial optimization</phrase>, although I had a prior interest in 
Batch Mode Sparse <phrase>Active Learning</phrase> Sparse representation, due to its clear and powerful insight deep into the structure of <phrase>data</phrase>, has seen a recent surge of interest in the classification <phrase>community</phrase>. Based on this, a <phrase>family</phrase> of reliable classification methods have been proposed. On the other hand, obtaining sufficiently <phrase>labeled training</phrase> <phrase>data</phrase> has <phrase>long</phrase> been a challenging problem, thus considerable <phrase>research</phrase> has been done regarding active selection of instances to be labeled. In our work, we will present a novel unified framework, i.e. BMSAL(Batch Mode Sparse <phrase>Active Learning</phrase>). Based on the existing sparse <phrase>family</phrase> of classifiers, we define rigorously the corresponding BMSAL <phrase>family</phrase> and explore their shared properties, most importantly (approximate) submodu-larity. We focus on the feasibility and reliability of the BMSAL <phrase>family</phrase>: The first one inspires us to optimize the <phrase>algorithms</phrase> and conduct experiments comparing with <phrase>state</phrase>-of-the-<phrase>art</phrase> methods; for reliability, we give error-bounded <phrase>algorithms</phrase>, as well as detailed logical deductions and empirical <phrase>tests</phrase> for applying sparse in non-linear <phrase>data</phrase> sets.
<phrase>Growing pains</phrase> for <phrase>deep learning</phrase> <phrase>Neural networks</phrase>, which support online image search and <phrase>speech recognition</phrase>, eventually will drive more advanced services.
De novo identification of replication-timing domains in the <phrase>human genome</phrase> by <phrase>deep learning</phrase> <phrase>MOTIVATION</phrase> The de novo identification of the initiation and termination zones-regions that replicate earlier or later than their upstream and downstream <phrase>neighbours</phrase>, respectively-remains a key challenge in <phrase>DNA replication</phrase>. <phrase>RESULTS</phrase> Building on advances in <phrase>deep learning</phrase>, we developed a novel <phrase>hybrid</phrase> <phrase>architecture</phrase> combining a <phrase>pre-trained</phrase>, <phrase>deep neural network</phrase> and a <phrase>hidden Markov model</phrase> (DNN-HMM) for the de novo identification of replication domains using replication timing profiles. Our <phrase>results</phrase> demonstrate that DNN-HMM can significantly outperform strong, discriminatively trained <phrase>Gaussian mixture</phrase> <phrase>model</phrase>-HMM (GMM-HMM) systems and other six reported methods that can be applied to this challenge. We applied our trained DNN-HMM to identify distinct replication domain types, namely the early replication domain (ERD), the down transition zone (DTZ), the late replication domain (LRD) and the up transition zone (UTZ), using newly replicated <phrase>DNA sequencing</phrase> (Repli-Seq) <phrase>data</phrase> across 15 <phrase>human</phrase> cells. A subsequent integrative analysis revealed that these replication domains harbour unique <phrase>genomic</phrase> and <phrase>epigenetic</phrase> patterns, transcriptional activity and <phrase>higher-order</phrase> <phrase>chromosomal</phrase> structure. Our findings support the 'replication-domain' <phrase>model</phrase>, which states (1) that ERDs and LRDs, connected by UTZs and DTZs, are spatially compartmentalized structural and functional units of <phrase>higher-order</phrase> <phrase>chromosomal</phrase> structure, (2) that the adjacent DTZ-UTZ pairs form <phrase>chromatin</phrase> loops and (3) that intra-interactions within ERDs and LRDs tend to be <phrase>short</phrase>-<phrase>range</phrase> and <phrase>long</phrase>-<phrase>range</phrase>, respectively. Our <phrase>model</phrase> reveals an important <phrase>chromatin</phrase> organizational principle of the <phrase>human genome</phrase> and represents a critical step towards understanding the mechanisms regulating replication timing. AVAILABILITY AND IMPLEMENTATION Our DNN-HMM method and three additional <phrase>algorithms</phrase> can be freely accessed at https://github.com/wenjiegroup/DNN-HMM The replication domain regions identified in this study are available in GEO under the accession ID GSE53984. CONTACT shuwj@bmi.ac.cn or boxc@bmi.ac.cn SUPPLEMENTARY <phrase>INFORMATION</phrase> Supplementary <phrase>data</phrase> are available at <phrase>Bioinformatics</phrase> online.
Programs as <phrase>Black</phrase>-Box Explanations With increasing complexity of <phrase>machine learning</phrase> systems being used 1 , there is a crucial need for providing insights into what these models are doing. <phrase>Model</phrase>-<phrase>agnostic</phrase> approaches [18], such as Baehrens et al. [1] and Ribeiro et al. [17], have shown that insights into complex, <phrase>black</phrase>-box models do not have to come at a cost of accuracy, and that accurate local explanations can successfully be provided for a number of complex classifiers (such as random <phrase>forests</phrase> and <phrase>deep neural networks</phrase>) and domains (text and images) for which interpretable models have not performed competitively. However, we still need to identify which interpretable representation would be suitable to convey the local behavior of the <phrase>model</phrase> in an accurate and succinct manner, and existing <phrase>model</phrase>-<phrase>agnostic</phrase> approaches have focused only on (sparse) linear models. Work in interpretable <phrase>machine learning</phrase>, on the other hand, has proposed many more other representations when designing their models, ranging There are a number of open questions when selecting which of these representations to use for <phrase>model</phrase>-<phrase>agnostic</phrase> explanations. It is clear that no <phrase>single</phrase> one of these representations, by itself, provides the necessary tradeoff between expressivity and interpretability. Further, there have not been adequate studies into understanding this tradeoff (with <phrase>Huysmans</phrase> et al. [7] being an exception), and it is likely that different representations are appropriate for different kinds of users and domains. It is thus clear that picking any <phrase>single</phrase> such intrepretable representation as the choice of <phrase>model</phrase>-<phrase>agnostic</phrase> representation is not ideal. In this position <phrase>paper</phrase>, we propose using programs to explain the local behavior of <phrase>black</phrase>-box systems. There are a number of advantages that such explanations will provide over using any <phrase>single</phrase> existing representation. First, <phrase>programming languages</phrase> are designed to capture complex behavior using a <phrase>high</phrase>-level <phrase>syntax</phrase> that is both succinct and intuitive, and there is a growing group of users that are already trained in <phrase>reading</phrase> and writing them. Second, programs can represent any <phrase>Turing-complete</phrase> behavior; any of the existing interpretable representation used in <phrase>literature</phrase> can be written as a program, but further, programs can also represent arbitrary combinations of multiple of these representations. It is also possible to <phrase>trade</phrase> off the expressivity and the comprehensibility of the program, for example simple programs for new programmers (at the cost of being an approximation of the complex system) or detailed, longer program for more accurate explanation of the behavior. Finally, we can potentially apply <phrase>research</phrase> in program/<phrase>software</phrase> analysis 
Hierarchical face <phrase>parsing</phrase> via <phrase>deep learning</phrase> This <phrase>paper</phrase> investigates how to parse (segment) facial components from face images which may be partially <phrase>oc</phrase>-cluded. We propose a novel face parser, which recasts segmentation of face components as a cross-modality <phrase>data transformation</phrase> problem, i.e., transforming an image patch to a <phrase>label</phrase> map. Specifically, a face is represented hierarchically by parts, components, and <phrase>pixel</phrase>-wise <phrase>labels</phrase>. With this representation, our approach first detects faces at both the part-and component-levels, and then computes the <phrase>pixel</phrase>-wise <phrase>label</phrase> maps (Fig.1). Our part-based and component-based detectors are generatively trained with the <phrase>deep belief</phrase> network (DBN), and are discriminatively tuned by <phrase>logistic regression</phrase>. The segmentators transform the detected face components to <phrase>label</phrase> maps, which are obtained by learning a highly nonlinear mapping with the deep autoencoder. The proposed hierarchical face <phrase>parsing</phrase> is not only robust to partial occlusions but also provide richer <phrase>information</phrase> for face analysis and face synthesis compared with face keypoint detection and face alignment. The effectiveness of our <phrase>algorithm</phrase> is shown through several tasks on 2, 239 images selected from three datasets (e.g., LFW [12], BioID [13] and CUFSF [29]).
<phrase>Low-Dimensional</phrase> Learning for Complex <phrase>Robots</phrase> This <phrase>paper</phrase> presents an <phrase>algorithm</phrase> for learning the switching policy and the boundaries conditions between primitive controllers that maximize the translational movements of a complex locomoting system. The <phrase>algorithm</phrase> learns an optimal <phrase>action</phrase> for each <phrase>boundary condition</phrase> instead of one for each discretized <phrase>state</phrase>-<phrase>action</phrase> pair of the system, as is typically done in <phrase>machine learning</phrase>. The system is <phrase>model</phrase> as a <phrase>hybrid</phrase> system because it contains both discrete and continuous dynamics. With this hybridification of the system and with this abstraction of learning boundary-<phrase>action</phrase> pairs, the " curse of dimensionality " is mitigated. The effectiveness of this <phrase>learning algorithm</phrase> is demonstrated on both a simulated system and on a physical robotic system. In both cases, the <phrase>algorithm</phrase> is able to learn the <phrase>hybrid</phrase> control strategy that maximizes the <phrase>forward</phrase> translational movement of the system without the need for <phrase>human</phrase> involvement. Note to Practitioners: AbstractAs technological <phrase>innovation</phrase> in the field of <phrase>robotics</phrase> continues to advance <phrase>forward</phrase> at a steady beat, so does the complexity of the robotic systems that are the product of this <phrase>innovation</phrase>. Utilizing these robotic systems for a number of automated tasks in diverse situations requires an increased level of <phrase>deep understanding</phrase> of the inner workings of the system, which is a burden to the <phrase>human</phrase> operator and <phrase>programmer</phrase>. To attenuate this burden, the task of <phrase>programming</phrase> itself must move towards being automated. This type of <phrase>automation</phrase> is the <phrase>motivation</phrase> for work presented here. This <phrase>paper</phrase> outlines an <phrase>algorithm</phrase> for which a complex robotic system can learn the task of locomoting all on its own. The practical application of this is that more complex robotic systems can be incorporated into <phrase>industry</phrase> with less effort. This is accomplished with an innovative approach on what the <phrase>robot</phrase> needs to learn to achieve its goal. This <phrase>paper</phrase> focuses on <phrase>robot</phrase> locomotion but future work will focus on <phrase>high</phrase> <phrase>degree</phrase> of freedom stationary robotic <phrase>arms</phrase> that are increasingly seen on <phrase>factory</phrase> floors.
Automatic Boosting of <phrase>Cross-Product</phrase> Coverage Using <phrase>Bayesian</phrase> Networks Closing the <phrase>feedback loop</phrase> from coverage <phrase>data</phrase> to the stimuli generator is one of the main challenges in the verification process. Typically, verification engineers with deep <phrase>domain knowledge</phrase> manually prepare a set of stimuli generation directives for that purpose. <phrase>Bayesian</phrase> networks based CDG (coverage <phrase>directed</phrase> generation) systems have been successfully used to assist the process by automatically closing this <phrase>feedback loop</phrase>. However, constructing these CDG systems requires manual effort and a certain amount of <phrase>domain knowledge</phrase> from a <phrase>machine learning</phrase> specialist. We propose a new method that boosts coverage in the early stages of the verification process with minimal effort, namely a fully automatic <phrase>construction</phrase> of a CDG system that requires no <phrase>domain knowledge</phrase>. <phrase>Experimental</phrase> <phrase>results</phrase> on a <phrase>real-life</phrase> <phrase>cross-product</phrase> coverage <phrase>model</phrase> demonstrate the efficiency of the <phrase>proposed method</phrase>.
Distributed optimization of deeply nested systems 1 Abstract Many architectures share the fundamental <phrase>design</phrase> principle of constructing a deeply nested mapping from inputs to outputs. Learning these architectures is challenging because nesting (i.e., <phrase>function composition</phrase>) produces inherently nonconvex functions. Backprop suffers from vanishing gradients and is hard to paral-lelize, is only applicable if the mappings are <phrase>differentiable</phrase> with respect to the parameters, and needs careful tuning of learning rates. Selecting the best <phrase>architecture</phrase>, for example the number of units in each layer of a deep net, or the number of filterbanks in a speech front-end processing, requires a <phrase>combinatorial</phrase> search. We describe a <phrase>general</phrase> optimization strategy called method of <phrase>auxiliary</phrase> coordinates (MAC). It has provable convergence, is easy to implement reusing existing <phrase>algorithms</phrase> for <phrase>single</phrase> layers, can be parallelized trivially and massively, applies even when parameter derivatives are not available or not desirable, can perform some <phrase>model selection</phrase> on the <phrase>fly</phrase>, and is competitive with <phrase>state</phrase>-of-the-<phrase>art</phrase> nonlinear optimizers even in the serial computation setting, often providing reasonable models within a few iterations. x y z 1 z 2
<phrase>Beamforming</phrase> networks using spatial <phrase>covariance</phrase> features for far-field <phrase>speech recognition</phrase> Recently, a deep <phrase>beamforming</phrase> (BF) network was proposed to predict BF weights from phase-carrying features, such as generalized <phrase>cross correlation</phrase> (<phrase>GCC</phrase>). The BF network is trained jointly with the <phrase>acoustic</phrase> <phrase>model</phrase> to minimize <phrase>automatic speech recognition</phrase> (ASR) cost <phrase>function</phrase>. In this <phrase>paper</phrase>, we propose to replace <phrase>GCC</phrase> with features derived from input signals' spatial <phrase>covariance</phrase> matrices (SCM), which contain the phase <phrase>information</phrase> of individual <phrase>frequency</phrase> bands. <phrase>Experimental</phrase> <phrase>results</phrase> on the AMI meeting transcription task shows that the BF network using SCM features significantly reduces the word <phrase>error rate</phrase> to 44.1% from 47.9% obtained with the conventional ASR pipeline using delay-and-sum BF. Also compared with <phrase>GCC</phrase> features, we have observed small but steady gain by 0.6% absolutely. The use of SCM features also facilitate the implementation of more advanced BF methods within a <phrase>deep learning</phrase> framework, such as minimum <phrase>variance</phrase> distortionless response BF that requires the speech and noise SCM. This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for <phrase>nonprofit</phrase> educational and <phrase>research</phrase> purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of <phrase>Mitsubishi Electric</phrase> <phrase>Research</phrase> Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the <phrase>copyright</phrase> notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to
Successful <phrase>Data Mining</phrase> Methods for <phrase>NLP</phrase> 1 Overview Historically <phrase>Natural Language Processing</phrase> (<phrase>NLP</phrase>) focuses on <phrase>unstructured data</phrase> (speech and text) understanding while <phrase>Data Mining</phrase> (DM) mainly focuses on massive, structured or semi-structured datasets. The <phrase>general</phrase> <phrase>research</phrase> directions of these two fields also have followed different philosophies and principles. For example, <phrase>NLP</phrase> aims at <phrase>deep understanding</phrase> of individual words, phrases and sentences (" micro-level "), whereas DM aims to conduct a <phrase>high</phrase>-level understanding, discovery and synthesis of the most salient <phrase>information</phrase> from a large set of documents when working on text <phrase>data</phrase> (" macro-level "). But they share the same goal of distilling <phrase>knowledge</phrase> from <phrase>data</phrase>. In the past five years, these two areas have had intensive interactions and thus mutually enhanced each other through many successful <phrase>text mining</phrase> tasks. This positive progress mainly benefits from some innovative intermediate representations such as " heterogeneous <phrase>information</phrase> networks " [Han et al., 2010, <phrase>Sun</phrase> et al., 2012b]. However, successful collaborations between any two fields require substantial mutual understanding , <phrase>patience</phrase> and passion among researchers. Similar to the applications of <phrase>machine learning</phrase> techniques in <phrase>NLP</phrase>, there is usually a gap of at least several years between the creation of a new DM approach and its first successful application in <phrase>NLP</phrase>. More importantly, many DM approaches such as gSpan [Yan and Han, 2002] and RankClus [<phrase>Sun</phrase> et al., 2009a] have demonstrated their power on structured <phrase>data</phrase>. But they remain relatively unknown in the <phrase>NLP</phrase> <phrase>community</phrase> , even though there are many obvious potential applications. On the other hand, compared to DM, the <phrase>NLP</phrase> <phrase>community</phrase> has paid more attention to developing <phrase>large-scale</phrase> <phrase>data</phrase> annotations, resources, shared tasks which <phrase>cover</phrase> a wide <phrase>range</phrase> of multiple genres and multiple domains. <phrase>NLP</phrase> can also provide the <phrase>basic</phrase> <phrase>building blocks</phrase> for many DM tasks such as text <phrase>cube</phrase> <phrase>construction</phrase> [Tao et al., 2014]. Therefore in many scenarios, for the same approach the <phrase>NLP</phrase> experiment setting is often much closer to <phrase>real-world</phrase> applications than its DM counterpart. We would like to share the experiences and lessons from our extensive inter-disciplinary collaborations in the past five years. The primary goal of this tutorial is to <phrase>bridge</phrase> the <phrase>knowledge</phrase> gap between these two fields and speed up the transition process. We will introduce two types of DM methods: (1). those <phrase>state</phrase>-of-the-<phrase>art</phrase> DM methods that have already been <phrase>proven</phrase> effective for <phrase>NLP</phrase>; and (2). some newly developed DM methods that we believe will fit into some specific <phrase>NLP</phrase> problems. In 
Using the Development of Elearning Material as Challenging and <phrase>Authentic Learning</phrase> Experiences <phrase>Student</phrase>-centred Authentic Elearning Students can contribute to the <phrase>design</phrase> and development phases of eLearning projects, and also learn through the process. This study focused on two projects at a <phrase>university</phrase> in <phrase>Hong Kong</phrase> (development of <phrase>Pharmacy</phrase> eCases, and the establishment of an eLearning Assistants scheme) in which students designed, wrote and developed teaching materials with space to show <phrase>initiative</phrase>. Evaluation strategies included a survey, <phrase>communication</phrase> logs with teachers, and a self-reflective <phrase>student</phrase> <phrase>blog</phrase>. Learning benefits from such <phrase>student</phrase>-centred, authentic eLearning projects include consolidation of <phrase>knowledge</phrase>, and development of skills (including <phrase>independent</phrase> learning, <phrase>critical thinking</phrase> and creative <phrase>design</phrase>) and attitudes (about <phrase>professional</phrase> work and ongoing <phrase>personal development</phrase>). The projects also <phrase>led</phrase> to enhanced course <phrase>learning environments</phrase>, thus benefiting other students. However, there are significant challenges in preparing such learning opportunities for students, including training and scaffolded supervision. Our overall reflection is that students' learning was different from that achieved in many traditional <phrase>university</phrase> courses. The roles of teachers/ developers and students in eLearning projects tend to be distinct. While teachers plan the <phrase>learning environment</phrase>, students are often expected to adopt a receptive mode, and participate and learn as a result of using the materials or the <phrase>learning strategies</phrase>. McNaught, Lam and Cheng (2007) suggested that such a distinction is not justified and that students can take an active role in shaping the eLearning environment. In addition, it is our contention that contributing to the <phrase>design</phrase> and development of eLearning strategies or resources can be an <phrase>authentic learning</phrase> task for students. There is an extensive <phrase>literature</phrase> on <phrase>student</phrase>-centred/ learner-centred approaches to <phrase>teaching and learning</phrase> proposed giving students a certain <phrase>degree</phrase> of ownership of the <phrase>organization</phrase>, procedure and activities in the classroom. They discussed how suitable <phrase>student</phrase> control can <phrase>lead</phrase> to a number of advantages, including a sense of well-being and comfort, engagement with learning activities, and an ongoing <phrase>investment</phrase> in deep thinking. Poon, McNaught, Lam and Kwan (in press) explained how students can participate in defining the marking criteria for assessments, and then engage in self-and <phrase>peer-assessment</phrase>. They found improvements in students' ability to make critical judgments about the quality of work. In this study we look for evidence that students' active participation in shaping the course eLearning environment is beneficial for their learning. <phrase>Authentic learning</phrase> engages students in <phrase>real-world</phrase> problems, thus simulating <phrase>professional</phrase> practice to some extent. Herrington, Oliver and Reeves (2003) suggested that authentic <phrase>online learning</phrase> activities should have <phrase>real-world</phrase> relevance, be ill-defined 
Using very deep autoencoders for <phrase>content-based image retrieval</phrase> We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to <phrase>short</phrase> <phrase>binary</phrase> codes. Using <phrase>semantic</phrase> <phrase>hashing</phrase> [6], 28-<phrase>bit</phrase> codes can be used to retrieve images that are similar to a query image in a time that is <phrase>independent</phrase> of the size of the <phrase>database</phrase>. This extremely fast retrieval makes it possible to search using multiple dierent transformations of the query image. 256-<phrase>bit</phrase> <phrase>binary</phrase> codes allow much more accurate matching and can be used to <phrase>prune</phrase> the set of images found using the 28-<phrase>bit</phrase> codes.
The <phrase>games</phrase> <phrase>computers</phrase> (and people) <phrase>play</phrase> The development of <phrase>high</phrase>-performance <phrase>game</phrase>-playing programs has been one of the <phrase>major</phrase> successes of <phrase>artificial intelligence</phrase> <phrase>research</phrase>. The <phrase>results</phrase> have been outstanding but, with one notable exception (<phrase>Deep Blue</phrase>), they have not been widely disseminated. This <phrase>talk</phrase> will discuss the past, present, and future of the development of <phrase>games</phrase>-playing programs. The <phrase>research</phrase> emphasis of the past has been on <phrase>high</phrase> performance (synonymous with <phrase>brute-force search</phrase>) for two-player <phrase>perfect-information</phrase> <phrase>games</phrase>. The <phrase>research</phrase> emphasis of the present encompasses multi-player imperfect/non-deterministic <phrase>information</phrase> <phrase>games</phrase>. And what of the future? There are some surprising changes of direction occurring that will result in <phrase>games</phrase> being more of an <phrase>experimental</phrase> <phrase>testbed</phrase> for mainstream <phrase>AI</phrase> <phrase>research</phrase>, with less emphasis on building world-<phrase>championship</phrase>-<phrase>caliber</phrase> programs. One of the most profound contributions to mankind's <phrase>knowledge</phrase> has been made by the <phrase>artificial intelligence</phrase> (<phrase>AI</phrase>) <phrase>research</phrase> <phrase>community</phrase>: the realization that <phrase>intelligence</phrase> is not uniquely <phrase>human</phrase>. 1 Using <phrase>computers</phrase>, it is possible to achieve <phrase>human</phrase>-like behavior in nonhumans. In other words, the illusion of <phrase>human intelligence</phrase> can be created in a computer. This idea has been vividly illustrated throughout the <phrase>history</phrase> of computer <phrase>games</phrase> <phrase>research</phrase>. Unlike most of the early work in <phrase>AI</phrase>, <phrase>game</phrase> researchers were interested in developing <phrase>high</phrase>-performance, real-time solutions to challenging problems. This <phrase>led</phrase> to an ends-justify-the-means attitude: the resulta strong <phrase>chess</phrase> programwas all that mattered, not the means by which it was achieved. In contrast, much of the mainstream <phrase>AI</phrase> work used simplified domains, while es-chewing real-time performance objectives. This <phrase>research</phrase> typically used <phrase>human intelligence</phrase> as a <phrase>model</phrase>: one only had to emulate the <phrase>human</phrase> example to achieve intelligent behavior. The battle (and <phrase>philosophical</phrase>) lines were drawn. The difference in <phrase>philosophy</phrase> can be easily illustrated. The <phrase>human brain</phrase> and the computer are different machines, each with its own sets of strengths and weaknesses. Humans are good at, for example, learning, reasoning by analogy, and <phrase>image processing</phrase>. <phrase>Computers</phrase> are good at numeric calculations , repetitious computations, and memorizing large sets of <phrase>data</phrase>. These machine architectures are largely complimen-tary: the human's processing strengths are the computer's weaknesses and the computer's strengths are <phrase>human</phrase> weaknesses. Given a problem to be solved and a specified <phrase>architecture</phrase> (<phrase>human brain</phrase> or <phrase>silicon</phrase> computer), a good <phrase>solution</phrase> should cater to the strengths of the machine being used, not the weaknesses. When viewed in this <phrase>light</phrase>, it is not surprising that the unhuman-like approaches have won out. Building <phrase>high</phrase>-performance <phrase>game</phrase>-playing programs has been one of AI's <phrase>major</phrase> triumphs. 
<phrase>Grammar</phrase> Induction: Beyond <phrase>Local Search</phrase> Many approaches to probabilistic <phrase>grammar</phrase> induction operate by iteratively improving a <phrase>single</phrase> <phrase>grammar</phrase>, beginning with an initial guess. These <phrase>local search</phrase> paradigms include Expectation Maximization [1, 10] and its variants [13, 14, etc.]; <phrase>hill-climbing</phrase> [6] and <phrase>Markov chain Monte Carlo</phrase> [9]; and greedy merging [15] or splitting [16, 3, 12] of nonterminals. Unfortunately, <phrase>local search</phrase> methods tend to get <phrase>caught</phrase> in local <phrase>optima</phrase>, even with random restarts [2, 13] or linguistically guided objective functions [4, 11]. I will suggest two techniques to try to avoid this problem. One approach searches exhaustively within a <phrase>region</phrase> of <phrase>parameter space</phrase>, using branch-and-bound to eliminate subregions that cannot contain the optimum. This exploits relaxation and decomposition techniques from the <phrase>operations research</phrase> <phrase>community</phrase> [17]. The other approach is inspired by recent work on " <phrase>deep learning</phrase> " for vision [e.g., 8]. It uses spectral methods [7, 5] to build up featural representations of all substrings, without premature commitment to which substrings are constituents.
An Empirical Evaluation of Four <phrase>Algorithms</phrase> for <phrase>Multi-Class</phrase> Classification: Mart, <phrase>ABC</phrase>-Mart, Robust LogitBoost, and <phrase>ABC</phrase>-LogitBoost This <phrase>empirical study</phrase> is mainly devoted to comparing four <phrase>tree</phrase>-based boosting <phrase>algorithms</phrase>: mart, <phrase>abc</phrase>-mart, robust logitboost, and <phrase>abc</phrase>-logitboost, for <phrase>multi-class</phrase> classification on a <phrase>variety</phrase> of publicly available datasets. Some of those datasets have been thoroughly tested in prior studies using a broad <phrase>range</phrase> of classification <phrase>algorithms</phrase> including <phrase>SVM</phrase>, <phrase>neural nets</phrase>, and <phrase>deep learning</phrase>. In terms of the empirical classification errors, our experiment <phrase>results</phrase> demonstrate: 1. <phrase>Abc</phrase>-mart considerably improves mart. 2. <phrase>Abc</phrase>-logitboost considerably improves (robust) logitboost. 3. (Robust) logitboost considerably improves mart on most datasets. 4. <phrase>Abc</phrase>-logitboost considerably improves <phrase>abc</phrase>-mart on most datasets. 5. These four boosting <phrase>algorithms</phrase> (especially <phrase>abc</phrase>-logitboost) outperform <phrase>SVM</phrase> on many datasets. 6. Compared to the best <phrase>deep learning</phrase> methods, these four boosting <phrase>algorithms</phrase> (especially <phrase>abc</phrase>-logitboost) are competitive.
<phrase>Taxonomy</phrase> Learning Using Term Specificity And Similarity Learning <phrase>taxonomy</phrase> for technical terms is difficult and tedious task, especially when new terms should be included. The goal of this <phrase>paper</phrase> is to assign <phrase>taxonomic</phrase> relations among technical terms. We propose new approach to the problem that relies on term specificity and similarity measures. Term specificity and similarity are necessary conditions for <phrase>taxonomy</phrase> learning, because highly specific terms tend to locate in deep levels and semantically similar terms are close to each other in <phrase>taxonomy</phrase>. We analyzed various features used in previous researches in view of term specificity and similarity, and applied optimal features for term specificity and similarity to our method.
<phrase>Education</phrase>-driven <phrase>research</phrase> in <phrase>CAD</phrase> We argue for a new <phrase>research</phrase> category, named <phrase>Education</phrase>-Driven <phrase>Research</phrase> (abbreviated EDR), which fills the gap between traditional field-specific <phrase>Research</phrase> that is not concerned with educational objectives and <phrase>Research</phrase> in <phrase>Education</phrase> that focuses on fundamental <phrase>teaching and learning</phrase> principles and possibly on their customization to broad areas (such as <phrase>mathematics</phrase> or <phrase>physics</phrase>), but not to specific disciplines (such as <phrase>CAD</phrase>). The objective of EDR is to simplify the formulation of the underlying theoretical foundations and of specific tools and solutions in a specialized domain, so as to make them easy to understand and internalize. As such, EDR is a difficult and genuine <phrase>research</phrase> activity, which requires a <phrase>deep understanding</phrase> of the specific field and can rarely be carried out by generalists with primary expertise in broad <phrase>education</phrase> principles. We illustrate the concept of EDR with three examples in <phrase>CAD</phrase>: (1) the <phrase>Split</phrase>&Tweak subdivisions of a <phrase>polygon</phrase> and its use for generating curves, surfaces, and animations; (2) the <phrase>construction</phrase> of a <phrase>topological</phrase> <phrase>partition</phrase> of a plane induced by an arbitrary <phrase>arrangement</phrase> of edges; and (3) a <phrase>romantic</phrase> definition of the minimal and <phrase>Hausdorff</phrase> distances. These examples demonstrate the value of using analogies, of introducing evocative terminology, and of synthesizing the simplest fundamental <phrase>building blocks</phrase>. The intuitive understanding provided by EDR enables the students (and even the instructor) to better appreciate the limitations of a particular <phrase>solution</phrase> and to explore alternatives. In particular, in these examples, EDR has allowed the <phrase>author</phrase> to: (1) reduce the cost of evaluating a cubic <phrase>B-spline</phrase> curve; (2) develop a new subdivision curve that is better approximated by its control <phrase>polygon</phrase> than either a cubic <phrase>B-spline</phrase> or an <phrase>interpolating</phrase> 4-point subdivision curve; (3) discover how a circuit inclusion <phrase>tree</phrase> may be used for identifying the faces in an <phrase>arrangement</phrase>; and (4) rectify a common misconception about the computation of the <phrase>Hausdorff</phrase> error between <phrase>triangle</phrase> meshes. We invite the scientific <phrase>community</phrase> to encourage the development of EDR by <phrase>publishing</phrase> its <phrase>results</phrase> as genuine <phrase>Research</phrase> contributions in <phrase>peer-reviewed</phrase> <phrase>professional</phrase> <phrase>journals</phrase>.
MATHESIS: An Intelligent <phrase>Web-Based</phrase> <phrase>Algebra</phrase> Tutoring <phrase>School</phrase> This article describes an intelligent, integrated, <phrase>web-based</phrase> <phrase>school</phrase> for tutoring expansion and factoring of <phrase>algebraic</phrase> expressions. It provides full support for the <phrase>management</phrase> of the usual teaching tasks in a traditional <phrase>school</phrase>: <phrase>Student</phrase> and <phrase>teacher</phrase> registration, creation and <phrase>management</phrase> of classes and <phrase>test</phrase> papers, individualized assignment of exercises, intelligent step by step guidance in solving exercises, <phrase>student</phrase> interaction <phrase>recording</phrase>, skill mastery <phrase>statistics</phrase> and <phrase>student</phrase> assessment. The <phrase>intelligence</phrase> of the system lies in its <phrase>Algebra</phrase> <phrase>Tutor</phrase>, a <phrase>model</phrase>-tracing <phrase>tutor</phrase> developed within the MATHESIS project, that teaches a breadth of 16 top-level <phrase>math</phrase> skills (<phrase>algebraic</phrase> operations): <phrase>monomial</phrase> <phrase>multiplication</phrase>, <phrase>division</phrase> and power, <phrase>monomial</phrase>-<phrase>polynomial</phrase> and <phrase>polynomial</phrase>-<phrase>polynomial</phrase> <phrase>multiplication</phrase>, parentheses elimination, collect like terms, identities (square of sum and difference, product of sum by difference, <phrase>cube</phrase> of sum and difference), factoring (common factor, term grouping, identities, <phrase>quadratic form</phrase>). These skills are further decomposed in simpler ones giving a deep domain expertise <phrase>model</phrase> of 104 primitive skills. The <phrase>tutor</phrase> has two novel features: a) it exhibits intelligent task recognition by identifying all skills present in any expression through intelligent <phrase>parsing</phrase>, and b) for each identified skill, the <phrase>tutor</phrase> traces all the sub-skills, a feature we call deep <phrase>model</phrase> tracing. Furthermore, based on these features, the <phrase>tutor</phrase> achieves broad <phrase>knowledge</phrase> monitoring by <phrase>recording</phrase> <phrase>student</phrase> performance for all skills present in any expression. Forty teachers who evaluated the system in a 3-hours workshop appreciated the <phrase>fine-grained</phrase> step-by-step guidance of the <phrase>student</phrase>, the equally <phrase>fine grained</phrase> <phrase>student</phrase> <phrase>model</phrase> created by the <phrase>tutor</phrase> and its ability to <phrase>tutor</phrase> any exercise that contains the aforementioned <phrase>math</phrase> skills. The system was also used in a real junior <phrase>high school</phrase> classroom with 20 students for three months. Evaluation of the students' performance in the domain of factoring gave positive learning <phrase>results</phrase>.
Redesigning Computer-based <phrase>Learning Environments</phrase>: Evaluation as <phrase>Communication</phrase> In the field of evaluation <phrase>research</phrase>, computer scientists <phrase>live</phrase> constantly upon dilemmas and conflicting theories. As evaluation is differently perceived and modeled among educational areas, it is not difficult to become trapped in dilemmas, which reflects an <phrase>epistemological</phrase> weakness. Additionally, designing and developing a computer-based learning scenario is not an easy task. Advancing further, with end-users probing the system in realistic settings, is even harder. <phrase>Computer science</phrase> <phrase>research</phrase> in evaluation faces an immense challenge, having to cope with contributions from several conflicting and controversial <phrase>research</phrase> fields. We believe that deep changes must be made in our field if we are to advance beyond the <phrase>CBT</phrase> (computer-based training) learning <phrase>model</phrase> and to build an adequate <phrase>epistemology</phrase> for this challenge. The first task is to relocate our field by building upon recent <phrase>results</phrase> from <phrase>philosophy</phrase>, <phrase>psychology</phrase>, <phrase>social sciences</phrase>, and <phrase>engineering</phrase>. In this article we locate evaluation in respect to <phrase>communication studies</phrase>. Evaluation presupposes a definition of goals to be reached, and we suggest that it is, by many means, a <phrase>silent</phrase> <phrase>communication</phrase> between <phrase>teacher</phrase> and <phrase>student</phrase>, <phrase>peers</phrase>, and institutional entities. If we accept that evaluation can be viewed as set of invisible rules known by nobody, but somehow understood by everybody, we should add <phrase>anthropological</phrase> inquiries to our <phrase>research</phrase> toolkit. The <phrase>paper</phrase> is organized around some elements of the social <phrase>communication</phrase> and how they convey new insights to evaluation <phrase>research</phrase> for computer and related scientists. We found some technical limitations and offer discussions on how we relate to <phrase>technology</phrase> at same time we establish expectancies and perceive others work.
Classifying Gray-scale Sar Images: a <phrase>Deep Learning</phrase> Approach Classifying Gray-scale differencing SAR images into two classes is very difficult due to the changeable impacts caused by the <phrase>season</phrase>, the imaging condition and so on. To optimize the <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>algorithms</phrase> and to deal with the mentioned difficulty, a novel unsupervised classification <phrase>algorithm</phrase> is proposed based on <phrase>deep learning</phrase>, where the complex correspondence among the images is built up by <phrase>Auto-encoder</phrase> <phrase>Model</phrase>. With the proper usage of <phrase>deep neural network</phrase> <phrase>model</phrase>, we could classify differencing SAR images into two classes more accurately and preciously. Experiments well demonstrate the effectiveness of the <phrase>proposed approach</phrase>.
<phrase>Internet Traffic</phrase> Classification Using LPC Cepstrum <phrase>Internet traffic</phrase> volumes continue to increase rapidly with the spread of the <phrase>broadband</phrase> access and the increase in <phrase>internet backbone</phrase> capacities. Moreover, the applications provided over the <phrase>internet</phrase> have become more diverse. Adequately supporting the <phrase>QoS</phrase> requirements for these diverse applications requires measuring traffic volumes by application type(file transfer, <phrase>broadcast</phrase> and so on). Therefore, methods for classifying applications become extremely important for multi <phrase>QoS</phrase> environment. In some application <phrase>traffic classification</phrase> methods, the applications are analyzed on the basis of <phrase>statistics</phrase> for the traffic characteristics, such as the packet inter arrival time and packet size. In this <phrase>paper</phrase>, we propose application <phrase>traffic classification</phrase> method applying LPC cepstrum. The <phrase>proposed method</phrase> enables to treat the traffic <phrase>data</phrase> as a <phrase>time series</phrase> signal and to separate spectral envelope <phrase>information</phrase> and spectral <phrase>fine structure</phrase> <phrase>information</phrase> from <phrase>spectrum</phrase>. We show that it is effective to use the LPC cepstrum to classify <phrase>internet traffic</phrase> by comparing LPC cepstrum with flow and raw <phrase>data</phrase> in this <phrase>paper</phrase>. I. INTRODUCTION <phrase>Internet traffic</phrase> volumes continue to increase rapidly with the spread of the <phrase>broadband</phrase> access and the increase in <phrase>internet backbone</phrase> capacities. Moreover, the applications provided over the <phrase>internet</phrase> have become more diverse (e.g. <phrase>video</phrase>, voice, text, and file transfer). Since the <phrase>QoS</phrase> requirements of these applications differ, the required bandwidth should be based on the traffic volume by application type. Therefore, methods for classifying applications become extremely important for multi <phrase>QoS</phrase> environment. Application <phrase>traffic classification</phrase> methods using <phrase>port</phrase> number or <phrase>deep packet inspection</phrase> (DPI) have been known. Classifying application traffic on the basis of the <phrase>port</phrase> number is not easy because some application types, like <phrase>P2P</phrase> <phrase>file sharing</phrase>, dynamically change their <phrase>port</phrase> number. Methods using DPI can also be used to classify traffic[1], but using DPI in <phrase>carrier</phrase> networks is a controversial issue from the viewpoint of <phrase>network neutrality</phrase> and <phrase>privacy</phrase>. We have taken the <phrase>machine learning</phrase> approach to classifying traffic by application using statistical properties such as the number of packets or the maximum packet size. With this approach, the <phrase>port</phrase> number and packet payload do not need to be checked. Traffic <phrase>data</phrase> can be represented as a signal with <phrase>time series</phrase> variation. The variation(e.g. constant throughtput of traffic volume, signal starting <phrase>communication</phrase>) depends on the
<phrase>Usability</phrase> <phrase>Engineering</phrase> 1 Definition & <phrase>Motivation</phrase> 1.1 What Is <phrase>Usability</phrase>? 1.2 <phrase>Usability</phrase> <phrase>Engineering</phrase> and <phrase>User Interface</phrase> 1.3 Why <phrase>Usability</phrase> <phrase>Engineering</phrase>? The <phrase>ISO</phrase> 9241-11: Guidance on <phrase>Usability</phrase> [ISO9241] defines: " <phrase>Usability</phrase>: the extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use. " [UsNe03] For instance: Appropriate for a purpose " <phrase>Usability</phrase> is a quality attribute that assesses how easy <phrase>user interfaces</phrase> are to use. The word "<phrase>usability</phrase>" also refers to methods for improving ease-of-use during the <phrase>design</phrase> process. " [Niel03] Table 1-1: A <phrase>model</phrase> of the attributes of system acceptability, [Niel93] <phrase>Usability</phrase> has five quality components [Niel03]: Learnability: How easy is it for users to accomplish <phrase>basic</phrase> tasks the first time they encounter the <phrase>design</phrase>? Efficiency: Once users have learned the <phrase>design</phrase>, how quickly can they perform tasks? Memorability: When users return to the <phrase>design</phrase> after a <phrase>period</phrase> of not using it, how easily can they reestablish proficiency? Errors: How many errors do users make, how severe are these errors, and how easily can they recover from the errors? Satisfaction: How pleasant is it to use the <phrase>design</phrase>? Figure 1-1: Traditional <phrase>software development</phrase> vs. <phrase>human</phrase>-centred development, [Gull01] According to Nielsen in [Niel03] there are also many other important quality attributes such as e.g. utility. In his point of view, there is no sense in having great features, if they are too difficult to use. " <phrase>Usability</phrase> is a measurable characteristic of a product <phrase>user interface</phrase> that is present to a greater or lesser <phrase>degree</phrase>. One broad <phrase>dimension</phrase> of <phrase>usability</phrase> is how easy to learn the <phrase>user interface</phrase> is for novice and casual users. Another is how easy to use (efficient, flexible, powerful) the <phrase>user interface</phrase> is for frequent and proficient users, after they have mastered the initial learning of the interface " [Mayh99] <phrase>Design</phrase> " <phrase>Usability</phrase> <phrase>Engineering</phrase> is a discipline that provides structured methods for achieving <phrase>usability</phrase> in <phrase>user interface design</phrase> during product development. " [Mayh99] <phrase>User Interface Design</phrase> describes any kind of <phrase>interface design</phrase> of an interactive system and also the <phrase>information design</phrase> of non-interactive systems (e.g. <phrase>subway</phrase> map). Thus <phrase>User Interface Design</phrase> is a <phrase>subset</phrase> of <phrase>Usability</phrase> <phrase>Engineering</phrase>. In the past years the importance and availability of <phrase>computers</phrase> have changed radical. <phrase>Computers</phrase> are not expert only systems, but they have a deep impact of every person's <phrase>daily</phrase> <phrase>life</phrase>. Under these circumstances it is very important, that <phrase>computers</phrase>, <phrase>software</phrase> and interactive systems as a whole are simple 
A Threshold-based Scheme for <phrase>Reinforcement Learning</phrase> in <phrase>Neural Networks</phrase> A generic and scalable <phrase>Reinforcement Learning</phrase> scheme for <phrase>Artificial Neural Networks</phrase> is presented, providing a <phrase>general</phrase> purpose learning machine. By reference to a node threshold three features are described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2) The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of forming <phrase>long</phrase> term strategy 3) The learning scheme is modified to use a thresholdbased <phrase>deep learning</phrase> <phrase>algorithm</phrase>, providing a robust and biologically inspired <phrase>alternative</phrase> to <phrase>backpropagation</phrase>. The scheme may be used for supervised as well as unsupervised training regimes.
Age and <phrase>gender</phrase> classification using <phrase>convolutional neural networks</phrase> Automatic age and <phrase>gender</phrase> classification has become relevant to an increasing amount of applications, particularly since the rise of social platforms and <phrase>social media</phrase>. Nevertheless , performance of <phrase>existing methods</phrase> on <phrase>real-world</phrase> images is still significantly lacking, especially when compared to the tremendous leaps in performance recently reported for the related task of <phrase>face recognition</phrase>. In this <phrase>paper</phrase> we show that by learning representations through the use of <phrase>deep-convolutional</phrase> <phrase>neural networks</phrase> (<phrase>CNN</phrase>), a significant increase in performance can be obtained on these tasks. To this end, we propose a simple convolutional net <phrase>architecture</phrase> that can be used even when the amount of learning <phrase>data</phrase> is limited. We evaluate our method on the recent Adience benchmark for age and <phrase>gender</phrase> estimation and show it to dramatically outperform <phrase>current state</phrase>-of-the-<phrase>art</phrase> methods.
Mental <phrase>State</phrase> Recognition via Wearable <phrase>EEG</phrase> The increasing quality and affordability of <phrase>consumer</phrase> electroencepha-logram (<phrase>EEG</phrase>) headsets make them attractive for situations where <phrase>medical</phrase> grade devices are impractical. Predicting and tracking <phrase>cognitive</phrase> states is possible for tasks that were previously not conducive to <phrase>EEG</phrase> monitoring. For instance, monitoring operators for states inappropriate to the task (e.g. drowsy drivers), tracking <phrase>mental health</phrase> (e.g. <phrase>anxiety</phrase>) and <phrase>productivity</phrase> (e.g. tiredness) are among possible applications for the <phrase>technology</phrase>. <phrase>Consumer</phrase> grade <phrase>EEG</phrase> headsets are affordable and relatively easy to use, but they lack the resolution and quality of signal that can be achieved using <phrase>medical</phrase> grade <phrase>EEG</phrase> devices. Thus, the key questions remain: to what extent are wearable <phrase>EEG</phrase> devices capable of mental <phrase>state</phrase> recognition, and what kind of mental states can be accurately recognized with these devices? In this work, we examined responses to two different types of input: instructional ('logical') versus recreational ('emotional') videos, using a <phrase>range</phrase> of <phrase>machine-learning</phrase> methods. We tried SVMs, sparse <phrase>logistic regression</phrase> , and <phrase>Deep Belief</phrase> Networks, to discriminate between the states of mind induced by different types of <phrase>video</phrase> input, that can be roughly labeled as 'logical' vs. 'emotional'. Our <phrase>results</phrase> demonstrate a significant potential of wearable <phrase>EEG</phrase> devices in differentiating <phrase>cognitive</phrase> states between situations with large contextual but subtle apparent differences. 1 Introduction Insights about internal states of mind can be a valuable resource in many situations. They can be used to preempt risky situations such as traffic accidents on the roads through monitoring <phrase>drowsiness</phrase> in <phrase>train</phrase> orbus drivers [4, 5, 9]. <phrase>EEG</phrase> headsets along with other wearable devices (e.g., those measuring galvanic <phrase>skin</phrase> response and heartbeat rate) can be prescribed to monitor mental states in patients with certain <phrase>psychiatric</phrase> conditions such as <phrase>anxiety</phrase> or <phrase>depression</phrase> [8]. Another possible application involves assessing different <phrase>reaction</phrase> types in response to a stimulus. Knowing whether an individual is reacting emotionally or rationally to a speech or advertisement can be used to <phrase>tailor</phrase> the content or style and to perform social analysis [11]. Much of the current
Functional Validation <phrase>in Grid</phrase> <phrase>Computing</phrase> The development of the <phrase>World Wide Web</phrase> has changed the way we think about <phrase>information</phrase>. <phrase>Information</phrase> on the web is distributed, updates are made asynchronously and resources come online and go offline without centralized control. Global networking will similarly change the way we think about and perform computation. <phrase>Grid computing</phrase> refers to <phrase>computing</phrase> in a distributed networked environment where <phrase>computing</phrase> and <phrase>data</phrase> resources are located throughout a network. In <phrase>order</phrase> to locate these resources dynamically in a grid computation, a broker or matchmaker uses keywords and <phrase>ontologies</phrase> to describe and specify grid services. However, we believe that keywords and <phrase>ontologies</phrase> can not always be defined or interpreted precisely enough to achieve deep <phrase>semantic</phrase> agreement in a truly distributed, heterogeneous <phrase>computing</phrase> environment. To this end, we introduce the concept of functional validation. Functional validation goes beyond the symbolic level of brokering and <phrase>matchmaking</phrase>, to the level of validating actual functional performance of grid services. In this <phrase>paper</phrase>, we present the functional validation concept <phrase>in grid</phrase> <phrase>computing</phrase>, analyze the possible validation situations and apply <phrase>basic</phrase> <phrase>machine learning</phrase> theory such as PAC learning and Chernoff bounds to explore the relationship between sample size and confidence in service <phrase>semantics</phrase>.
Disentangling Factors of Variation for <phrase>Facial Expression</phrase> Recognition We propose a <phrase>semi-supervised</phrase> approach to solve the task of <phrase>emotion</phrase> recognition in 2D face images using recent ideas in <phrase>deep learning</phrase> for handling the factors of variation present in <phrase>data</phrase>. An <phrase>emotion</phrase> classification <phrase>algorithm</phrase> should be both robust to (1) remaining variations due to the pose of the face in the image after centering and alignment, (2) the identity or <phrase>morphology</phrase> of the face. In <phrase>order</phrase> to achieve this invariance, we propose to learn a hierarchy of features in which we gradually filter the factors of variation arising from both (1) and (2). We address (1) by using a multi-scale contractive <phrase>convolutional network</phrase> (CCNET) in <phrase>order</phrase> to obtain invariance to translations of the facial traits in the image. Using the <phrase>feature representation</phrase> <phrase>produced</phrase> by the CCNET, we <phrase>train</phrase> a Contrac-tive Discriminative Analysis (<phrase>CDA</phrase>) feature extractor, a novel variant of the Contractive <phrase>Auto-Encoder</phrase> (CAE), designed to learn a representation separating out the <phrase>emotion</phrase>-related factors from the others (which mostly capture the subject identity, and what is left of pose after the CCNET). This system beats the <phrase>state</phrase>-of-the-<phrase>art</phrase> on a <phrase>recently proposed</phrase> dataset for <phrase>facial expression</phrase> recognition, the <phrase>Toronto</phrase> Face <phrase>Database</phrase>, moving the <phrase>state</phrase>-of-<phrase>art</phrase> accuracy from 82.4% to 85.0%, while the CCNET and <phrase>CDA</phrase> improve accuracy of a standard CAE by 8%.
Exploring <phrase>Elementary-School</phrase> Students' Engagement Patterns in a <phrase>Game-Based Learning</phrase> Environment Unlike most <phrase>research</phrase>, which has primarily examined the players' interest in or attitude toward <phrase>game</phrase>-based learning through questionnaires, the purpose of this <phrase>empirical study</phrase> is to explore students' engagement patterns by qualitative observation and <phrase>sequential analysis</phrase> to visualize and better understand their <phrase>game</phrase>-based learning process. We studied the sequential behaviors of 34 students (17 male and 17 female) and considered issues of <phrase>gender</phrase> differences and sequential pattern similarities. The <phrase>results</phrase> show that the behavioral coding schema provided by the authors and the innovative method of <phrase>sequential analysis</phrase> can provide researchers with a certain level of understanding of students' engagement patterns in <phrase>game</phrase>-based <phrase>learning environments</phrase>. In terms of the overall <phrase>sequence</phrase> <phrase>results</phrase>, this study identified higher and <phrase>lower</phrase> engagement patterns to represent students' learning processes in <phrase>game</phrase>-based learning. Moreover, the sequential patterns represent qualitative differences and similarities in engagement patterns grouped by <phrase>gender</phrase>. A good engagement cycle, in which male and female students started the <phrase>game</phrase> to attempt to think and <phrase>solve problems</phrase>, was noticeable. However, male students were observed to demonstrate more engaged behaviors, with continuous self-conversations when confused. The <phrase>frequency</phrase> of self-conversation from female students was obviously <phrase>lower</phrase> than that of male students and revealed more verbal and <phrase>nonverbal</phrase> behaviors. The deep examination of students' verbal and <phrase>nonverbal</phrase> engagement behaviors may make beneficial contributions to the <phrase>educational technology</phrase> field with the <phrase>adoption</phrase> of <phrase>sequential analysis</phrase>.
Personalized <phrase>News</phrase> Recommendation Using Classified Keywords to Capture User Preference <phrase>Recommender systems</phrase> are becoming an essential part of smart services. When building a <phrase>news</phrase> recommender system, we should consider special features different from other <phrase>recommender systems</phrase>. Hot <phrase>news</phrase> topics are changing every moment, thus it is important to recommend right <phrase>news</phrase> at the right time. This <phrase>paper</phrase> aims to propose a new <phrase>model</phrase>, based on <phrase>deep neural network</phrase>, to analyze user preference for <phrase>news</phrase> recommender system. The <phrase>model</phrase> extracts interest keywords to characterize the user preference from the set of <phrase>news</phrase> articles read by that particular user in the past. The <phrase>model</phrase> utilizes characterizing features for <phrase>news</phrase> recommendation, and applies those to the keyword classification for user preference. For the keyword classification, we use <phrase>deep neural network</phrase> for online preference analysis, because adaptive learning is necessary to <phrase>track</phrase> changes of hot topics sensitively. The usefulness of our <phrase>model</phrase> is validated through experiments. In addition, the accuracy and diversity of the recommendation <phrase>results</phrase> is also analyzed. He received a <phrase>bachelor's degree</phrase> of <phrase>computer science</phrase> in 2012 from <phrase>Hanyang University</phrase>. He is currently a MS candidate <phrase>student</phrase> in the <phrase>department</phrase> of <phrase>computer science</phrase> at <phrase>Korea</phrase> Advanced Institute of <phrase>Science</phrase> and <phrase>Technology</phrase> (<phrase>KAIST</phrase>). His <phrase>current research</phrase> interests include <phrase>artificial intelligence</phrase>, <phrase>data mining</phrase>, and big-<phrase>graph</phrase> <phrase>mining</phrase>. He received a <phrase>bachelor's degree</phrase> of <phrase>medical</phrase> <phrase>computer science</phrase> in 2011 from Eulji <phrase>University</phrase>. He is currently a MS candidate <phrase>student</phrase> in the <phrase>department</phrase> of computer <phrase>engineering</phrase> at <phrase>Kyung Hee University</phrase> (KHU). His <phrase>current research</phrase> interests include <phrase>artificial intelligence</phrase>, <phrase>data mining</phrase>, and biomedical informatics.
Improved Representation Learning for Question Answer Matching Passage-level question answer matching is a <phrase>challenging task</phrase> since it requires effective representations that capture the complex <phrase>semantic</phrase> relations between questions and answers. In this work, we propose a series of <phrase>deep learning</phrase> models to address passage answer selection. To match passage answers to questions accommodating their complex <phrase>semantic</phrase> relations, unlike most previous work that utilizes a <phrase>single</phrase> <phrase>deep learning</phrase> structure, we develop <phrase>hybrid</phrase> models that process the text using both convolutional and recurrent <phrase>neu</phrase>-ral networks, combining the merits on extracting <phrase>linguistic</phrase> <phrase>information</phrase> from both structures. Additionally, we also develop a simple but effective attention mechanism for the purpose of constructing better answer representations according to the input question, which is imperative for better modeling <phrase>long</phrase> answer sequences. The <phrase>results</phrase> on two <phrase>public</phrase> <phrase>benchmark datasets</phrase>, InsuranceQA and TREC-QA, show that our proposed models outperform a <phrase>variety</phrase> of strong baselines.
Towards multiple identity detection in <phrase>social networks</phrase> In this <phrase>paper</phrase> we discuss a piece of work which intends to provide some insights regarding the resolution of the hard problem of multiple identities detection. Based on <phrase>hypothesis</phrase> that each person is unique and identifiable whether in its writing style or social behavior, we propose a Framework relying on <phrase>machine learning</phrase> models and a deep analysis of social interactions, towards such detection.
<phrase>Information Security</phrase> of <phrase>Knowledge Economy</phrase> Students The issues of computerization, informatization and, recently, internetization of <phrase>education</phrase> demand deep analysis of impacts initiated by them in development of a man. The <phrase>results</phrase> of such analysis proved that optimal conditions must be formed for creative development of the students in <phrase>order</phrase> to avoid non-desired consequences in their <phrase>psychological</phrase>, socio-<phrase>cultural</phrase> conditions. All things around a modern person are connected with informational technologies in <phrase>society</phrase>-<phrase>knowledge economy</phrase>. The <phrase>software</phrase> and hardware resources are constantly renewed. In the <phrase>education</phrase> system new spheres appear which are intended to self-<phrase>cognition</phrase> of the students. In such spheres the means for providing physical and <phrase>psychological</phrase> <phrase>health</phrase> must be used. In all educational spheres <phrase>e</phrase>-learning technologies must be also implemented in <phrase>order</phrase> to cognize informational environment better-and provide personal <phrase>security</phrase>. In <phrase>order</phrase> to learn to avoid <phrase>psychological</phrase> numbness because of non-desired <phrase>information</phrase>, to teach " <phrase>digital</phrase> generation " to think-this is the goal of <phrase>education</phrase> system today. Analysis of existing threats and dangers originating from the use of <phrase>internet</phrase>-resources has shown that <phrase>information security</phrase> is obligatory condition to provide positive <phrase>personal development</phrase> of all participators of <phrase>education</phrase> environment. INTRODUCTION creation of safe <phrase>information</phrase>-educational environment.
Regularizing Recurrent Networks - On Injected Noise and Norm-based Methods Advancements in <phrase>parallel processing</phrase> have <phrase>lead</phrase> to a surge in multilayer perceptrons' (MLP) applications and <phrase>deep learning</phrase> in the past decades. <phrase>Recurrent Neural Networks</phrase> (RNNs) give additional representa-tional power to feedforward MLPs by providing a way to treat sequential <phrase>data</phrase>. However, RNNs are hard to <phrase>train</phrase> using conventional error <phrase>backpropagation</phrase> methods because of the difficulty in relating inputs over many time-steps. Regularization approaches from MLP <phrase>sphere</phrase>, like dropout and noisy <phrase>weight training</phrase>, have been insufficiently applied and tested on simple RNNs. Moreover, solutions have been proposed to improve convergence in RNNs but not enough to improve the <phrase>long</phrase> term dependency remembering capabilities thereof. In this study, we aim to empirically evaluate the remembering and generalization ability of RNNs on <phrase>polyphonic</phrase> <phrase>musical</phrase> datasets. The models are trained with injected noise, random dropout, norm-based regularizers and their respective performances compared to well-initialized plain RNNs and advanced regularization methods like fast-dropout. We conclude with evidence that training with noise does not <phrase>improve performance</phrase> as conjectured by a few works in RNN optimization before ours.
A <phrase>Comprehensive</phrase> Benchmark of <phrase>Kernel Methods</phrase> to Extract ProteinProtein Interactions from <phrase>Literature</phrase> The most important way of conveying new findings in biomedical <phrase>research</phrase> is scientific publication. Extraction of <phrase>protein</phrase>-<phrase>protein</phrase> interactions (PPIs) reported in scientific publications is one of the core topics of <phrase>text mining</phrase> in the <phrase>life</phrase> sciences. Recently, a new class of such methods has been proposed - <phrase>convolution</phrase> kernels that identify PPIs using deep parses of sentences. However, comparing <phrase>published results</phrase> of different PPI extraction methods is impossible due to the use of different evaluation corpora, different evaluation metrics, different tuning procedures, etc. In this <phrase>paper</phrase>, we study whether the reported performance metrics are robust across different corpora and learning settings and whether the use of deep <phrase>parsing</phrase> actually leads to an increase in extraction quality. Our ultimate goal is to identify the one method that performs best in <phrase>real-life</phrase> scenarios, where <phrase>information extraction</phrase> is performed on unseen text and not on specifically prepared evaluation <phrase>data</phrase>. We performed a <phrase>comprehensive</phrase> <phrase>benchmarking</phrase> of nine different methods for PPI extraction that use <phrase>convolution</phrase> kernels on rich <phrase>linguistic</phrase> <phrase>information</phrase>. Methods were evaluated on five different <phrase>public</phrase> corpora using <phrase>cross-validation</phrase>, cross-learning, and cross-corpus evaluation. Our study confirms that kernels using dependency <phrase>trees</phrase> generally outperform kernels based on <phrase>syntax</phrase> <phrase>trees</phrase>. However, our study also shows that only the best <phrase>kernel methods</phrase> can compete with a simple <phrase>rule-based</phrase> approach when the evaluation prevents <phrase>information</phrase> leakage between training and <phrase>test</phrase> corpora. Our <phrase>results</phrase> further reveal that the F-score of many approaches drops significantly if no corpus-specific parameter optimization is applied and that methods reaching a good <phrase>AUC</phrase> score often perform much worse in terms of F-score. We conclude that for most kernels no sensible estimation of PPI extraction performance on new text is possible, given the current heterogeneity in evaluation <phrase>data</phrase>. Nevertheless, our study shows that three kernels are clearly <phrase>superior</phrase> to the other methods.
Transfer and Structure Learning in <phrase>Markov</phrase> <phrase>Logic</phrase> Networks <phrase>ii</phrase> Acknowledgements I'd like to thank my advisor, Andrea Danyluk, for all of her support, advice, and insight, as well as for her willingness and miraculous ability to find the time to supervise a <phrase>thesis</phrase> while simultaneously serving as the <phrase>Dean</phrase> of the Faculty. I'd also like to thank Tom Murtagh, my second reader, for his helpful comments and insightful questions. I'm especially grateful to Mary Bailey for setting up the cluster on which all of my experiments were run (and for her foregiveness of my constant abuse of the <phrase>Unix</phrase> lab machines), and to <phrase>Jeannie</phrase> Albrecht for donating large quantities of <phrase>high</phrase>-powered hardware to the effort. Many thanks to Jesse Davis for providing me with his implementation of <phrase>DTM</phrase> as well as the WebKB and <phrase>Yeast</phrase> datasets, and for giving generously of his time through many helpful conversations. Last but not least, I'd like to thank my parents for their <phrase>love</phrase>, encouragement, and support, without which this <phrase>thesis</phrase> would never have been written. Abstract <phrase>Markov</phrase> <phrase>logic</phrase> networks are a recently developed <phrase>knowledge representation</phrase> capable of compactly representing complex relationships and handling uncertainty in a principled manner. The <phrase>deep transfer</phrase> <phrase>algorithm</phrase> of Davis and Domingos proposes a method for learning the structure of an MLN by incorporating cross-<phrase>domain knowledge</phrase> for example, using the relationships between <phrase>yeast</phrase> <phrase>proteins</phrase> to inform predictions about relationships in the <phrase>movie</phrase> <phrase>business</phrase>. This <phrase>thesis</phrase> explores several questions related to this <phrase>algorithm</phrase> and to cross-domain transfer in <phrase>general</phrase>. I first develop methods for simultaneous transfer from multiple domains, and propose that much of the success of <phrase>deep transfer</phrase> may be explained by a small number of commonly occurring structural properties, e.g. <phrase>symmetry</phrase> and transitivity. I then demonstrate empirically that <phrase>deep transfer</phrase> often performs best when using the <phrase>target</phrase> domain as its own transfer source, e.g. the most effective way to predict relationships in the <phrase>movie</phrase> <phrase>business</phrase> is to examine other relationships in the <phrase>movie</phrase> <phrase>business</phrase>. This suggests a reinterpretation of the <phrase>deep transfer</phrase> <phrase>algorithm</phrase> as a method for <phrase>single</phrase>-domain structure learning. Finally, I describe a new <phrase>algorithm</phrase> for cross-domain transfer which transfers a more specific form of structure than the second-<phrase>order</phrase> cliques used by Davis and Domingos, and show empirically that my approach performs competitively to the <phrase>deep transfer</phrase> <phrase>algorithm</phrase>, despite using a less sophisticated form of learning in the source domain.
<phrase>Fingerprints</phrase> for Machines - Characterization and Optical Identification of Grinding Imprints The profile of <phrase>a 10</phrase> mm wide and 3 m deep grinding imprint is as unique as a <phrase>human</phrase> <phrase>fingerprint</phrase>. To utilize this for fingerprinting mechanical components, a robust and strong characterization has to be used. We propose a feature-<phrase>based approach</phrase>, in which features of a 1D profile are detected and described in its 2D space-<phrase>frequency</phrase> representation. We show that the approach is robust on depth maps as well as intensity images of grinding imprints. To estimate the <phrase>probability</phrase> of misclassification, we derive a <phrase>model</phrase> and learn its parameters. With this <phrase>model</phrase> we demonstrate that our characterization has a false positive rate of approximately 10 20 which is as strong as a <phrase>human</phrase> <phrase>fingerprint</phrase>.
Towards a unified theory of <phrase>spoken language</phrase> processing <phrase>Spoken Language</phrase> Processing is arguably the most sophisticated behaviour of the most complex <phrase>organism</phrase> in the known <phrase>universe</phrase> and, unsurprisingly, scientists still have much to learn about how it works. Meanwhile, automated <phrase>spoken language</phrase> processing systems have begun to emerge in commercial applications, not as a result of any deep insights into the way in which humans process <phrase>language</phrase>, but largely as a consequence of the introduction of a '<phrase>data</phrase>-driven' approach to building practical systems. At the same time, computational models of <phrase>human</phrase> <phrase>spoken language</phrase> processing have begun to emerge but, although this has given rise to a greater interest in the relationship between <phrase>human</phrase> and machine behaviour, the performance of the best models appears to be asymptoting some way <phrase>short</phrase> of the capabilities of the <phrase>human</phrase> listener/<phrase>speaker</phrase>. This <phrase>paper</phrase> discusses these issues, and presents an argument in favour of the derivation of a 'unifying theory' that would be capable of explaining and predicting both <phrase>human</phrase> and machine <phrase>spoken language</phrase> processing behaviour, and hence serve both communities as well as representing a <phrase>long</phrase>-term 'grand challenge' for the scientific <phrase>community</phrase> in the emerging field of '<phrase>Cognitive</phrase> Informatics'.
<phrase>Quantum-mechanical</phrase> Modeling of <phrase>Transport</phrase> Parameters for Mos Devices <phrase>Quantum-mechanical</phrase> Modeling of <phrase>Transport</phrase> Parameters for Mos Devices Die Deutsche Bibliothek lists this publication in the Deutsche Nationalbibliografie; detailed bibliographic <phrase>data</phrase> is available in the <phrase>internet</phrase> at Acknowledgments First of all, I would like to thank Prof. <phrase>Wolfgang</phrase> Fichtner for the opportunity to work and learn at the Institut <phrase>fr</phrase> Integrierte Systeme where I found excellent working conditions as well as outstanding colleagues. I am also grateful to Prof. Giorgio Baccarani for co-examining this <phrase>thesis</phrase> and his interest in my work. Especially, I wish to thank Prof. Andreas Schenk for his reliable support and the valuable scientific advice he gave me during my time at the Institute. I am indebted to Andreas <phrase>Wettstein</phrase> for providing a basis for my work regarding both theory and implementation of <phrase>quantum</phrase> modeling and for his support on both aspects. I thank Michael <phrase>Pfeiffer</phrase> and Bernhard Schmithsen for their help on compiling issues. Furthermore, I enjoyed the <phrase>company</phrase> and expertise of Frank Geelhaar, In addition, I want to thank all the people who created the <phrase>friendly</phrase> working environment and kept everything working smoothly in the background. Abstract The ongoing <phrase>evolution</phrase> of <phrase>integrated circuits</phrase> is based on the minia-turization of the individual devices. Typical feature sizes that are routinely implemented by today's <phrase>manufacturing</phrase> <phrase>technology</phrase> already belong to the domain of <phrase>quantum</phrase> effects. While this poses additional problems for traditional device concepts it also paves the <phrase>road</phrase> towards new functional principles. For assessing either aspects, appropriate models are needed for simulators, which have become an important tool in both device and <phrase>process engineering</phrase>. Topic of this <phrase>thesis</phrase> are the implications of quantization on <phrase>transport</phrase> parameters in drift-<phrase>diffusion</phrase>-based numerical descriptions of <phrase>semiconductor</phrase> devices. For these investigations the device simulaton <phrase>software</phrase> DESSIS ISE was used, especially its enhancements to <phrase>model</phrase> <phrase>quantum</phrase> effects. These comprise a self-consistent Schrdinger-<phrase>Poisson</phrase> solver for one-dimensional quantization effects and the <phrase>quantum</phrase> drift-<phrase>diffusion</phrase> (QDD) <phrase>model</phrase>. In the first part of this work, the QDD <phrase>model</phrase> is applied to tun-neling through MOS gate oxides and double barrier devices. For both structures the " tunneling " characteristics exhibit regions of negative differential resistance which was identified as a modeling artifact. These <phrase>results</phrase> indicate that the QDD description of tunneling is of only very limited use. The second part deals with the modeling of Shockley-Read-Hall (SRH) <phrase>recombination</phrase> which is enabled by multiphonon processes between bands and deep trap levels in the gap of the <phrase>semiconductor</phrase> in the presence of quantization. The corresponding <phrase>density</phrase> of states 
<phrase>Transfer learning</phrase> for <phrase>Latin</phrase> and <phrase>Chinese characters</phrase> with <phrase>Deep Neural Networks</phrase> We analyze <phrase>transfer learning</phrase> with <phrase>Deep Neural Networks</phrase> (DNN) on various <phrase>character recognition</phrase> tasks. DNN trained on digits are perfectly capable of recognizing uppercase letters with minimal retraining. They are on par with DNN fully trained on uppercase letters, but <phrase>train</phrase> much faster. DNN trained on <phrase>Chinese characters</phrase> easily recognize uppercase <phrase>Latin letters</phrase>. Learning <phrase>Chinese characters</phrase> is accelerated by first pretraining a DNN on a small <phrase>subset</phrase> of all classes and then continuing to <phrase>train</phrase> on all classes. Furthermore, pretrained nets consistently outperform randomly initialized nets on new tasks with few <phrase>labeled data</phrase>.
<phrase>Teacher</phrase> <phrase>Education</phrase> for <phrase>Teacher</phrase>-learner <phrase>Autonomy</phrase> <phrase>Teacher</phrase>-learner <phrase>autonomy</phrase>, by analogy with previous definitions of <phrase>language</phrase> learner <phrase>autonomy</phrase>, might be defined as the ability to develop appropriate skills, <phrase>knowledge</phrase> and attitudes for oneself as a <phrase>teacher</phrase>, in cooperation with others. By focusing on the <phrase>teacher</phrase> as learner in this manner I do not mean to diminish the importance of 'being <phrase>free</phrase> from constraints on one's teaching', i.e. <phrase>teacher</phrase> <phrase>autonomy</phrase> in the more commonly understood sense of the term. Rather, as I shall attempt to illustrate in this <phrase>paper</phrase>, considering teachers' <phrase>autonomy</phrase> as learners may allow us to view just one <phrase>aspect</phrase> of their lives more clearly, at the same time enabling us to adopt useful perspectives from recent discussion of <phrase>language</phrase> learner <phrase>autonomy</phrase> (for example, on the role of interdependence, issues of cross-<phrase>cultural</phrase> appropriateness, and the relative merits of 'deep-' and shallow-end' approaches). The enhancement of <phrase>teacher</phrase>-learner <phrase>autonomy</phrase> in relation to a <phrase>variety</phrase> of areas of pedagogical, attitudinal and content-related expertise can be argued to have an intrinsic value within <phrase>teacher</phrase> <phrase>education</phrase> programmes (aside from its value in preparing teachers to engage in <phrase>pedagogy</phrase> for <phrase>autonomy</phrase> with students) since <phrase>teacher</phrase>-learning is inevitably a largely self-<phrase>directed</phrase> process. Examples will be provided of activities and other arrangements designed explicitly to promote <phrase>teacher</phrase>-learner <phrase>autonomy</phrase> in some of these areas, within particular <phrase>teacher</phrase> <phrase>education</phrase> settings in <phrase>Japan</phrase> and the <phrase>UK</phrase>.
Guest Editorial - Learning and <phrase>Knowledge</phrase> Analytics or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or <phrase>commercial advantage and that copies bear</phrase> the full citation on the first page. <phrase>Copyrights</phrase> for components of this work owned by others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to <phrase>post on servers</phrase>, or to redistribute to lists, <phrase>requires prior specific permission</phrase> and/or a fee. Request permissions from the editors at kinshuk@ieee.org. The early stages of the <phrase>internet</phrase> and <phrase>world wide web</phrase> drew attention to the <phrase>communication</phrase> and <phrase>connective</phrase> capacities of global networks. The ability to collaborate and interact with colleagues from around the world provided academics with new models of <phrase>teaching and learning</phrase>. Today, online <phrase>education</phrase> is a fast growing segment of the <phrase>education</phrase> sector. A side effect, to date not well explored, of <phrase>digital</phrase> learning is the collection of <phrase>data</phrase> and analytics in <phrase>order</phrase> to understand and inform <phrase>teaching and learning</phrase>. As learners engage in online or <phrase>mobile</phrase> learning, <phrase>data</phrase> trails are created. These <phrase>data</phrase> trails indicate <phrase>social networks</phrase>, learning dispositions, and how different learners come to understand core course concepts. Aggregate and <phrase>large-scale</phrase> <phrase>data</phrase> can also provide predictive value about the types of learning patterns and activity that might indicate <phrase>risk</phrase> of failure or drop out. The <phrase>Society</phrase> for <phrase>Learning Analytics</phrase> <phrase>Research</phrase> defines <phrase>learning analytics</phrase> as the measurement, collection, analysis and reporting of <phrase>data</phrase> about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs (http://www.solaresearch.org/mission/about/). As numerous papers in this issue reference, <phrase>data</phrase> analytics has drawn the attention of academics and <phrase>academic</phrase> leaders. <phrase>High</phrase> expectations exist for <phrase>learning analytics</phrase> to provide new insights into educational practices and ways to improve teaching, learning, and <phrase>decision-making</phrase>. The appropriateness of these expectations is the subject of researchers in the young but rapidly growing <phrase>learning analytics</phrase> field. <phrase>Learning analytics</phrase> currently sits at a crossroads between technical and <phrase>social learning theory</phrase> fields. On the one hand, the <phrase>algorithms</phrase> that form <phrase>recommender systems</phrase>, <phrase>personalization</phrase> models, and <phrase>network analysis</phrase> require deep technical expertise. The impact of these <phrase>algorithms</phrase>, however, is felt in the social system of learning. As a consequence, researchers in <phrase>learning analytics</phrase> have devoted significant attention to bridging these gaps and bringing these communities in contact with each other through conversations and conferences. The LAK12 conference in <phrase>Vancouver</phrase>, for example, 
Towards <phrase>Industrial</phrase> Machine <phrase>Intelligence</phrase> The next decadew ill see a deep transformationo fi ndustrial applications by <phrase>big data</phrase> analytics, <phrase>machine learning</phrase> and the <phrase>internet</phrase> of things. <phrase>Industrial</phrase> applications have a number of unique features, setting them apart from other domains. Central for many <phrase>industrial</phrase> applications in the <phrase>internet</phrase> of things is <phrase>time series</phrase> <phrase>data</phrase> generated by often hundreds or thousands of sensors at a <phrase>high</phrase> rate, e.g. by a <phrase>turbine</phrase> or a <phrase>smart grid</phrase>. In a first <phrase>wave</phrase> of applications this <phrase>data</phrase> is centrally collected and analyzed in Map-Reduce or streaming systems for condition monitoring, <phrase>root</phrase> cause analysis,o rp redictive maintenance. Thenext step is to shift from centralized analysis to distributed in-field or in situ analytics, e.g., in smart cities or smartg rids. The final step will be <phrase>ad</phrase> istributed, partially autonomous <phrase>decision making</phrase> and learning in massivelyd istributed environments. In this <phrase>talk</phrase>, Ig ive an overview on <phrase>Siemens</phrase>' journeyt hrough this transformation, highlight earlysuccesses,productsand <phrase>prototypes</phrase> and point out future challenges on the way towards machine <phrase>intelligence</phrase>. Ia <phrase>lso</phrase> discuss <phrase>architectural</phrase> challenges for such systems from a <phrase>Big Data</phrase> point of view. Michael May is Head of the <phrase>Technology</phrase> Field <phrase>Business Analytics</phrase> &M onitoring at <phrase>Siemens</phrase> Corporate <phrase>Technology</phrase>, <phrase>Munich</phrase>, and responsible for eleven <phrase>research</phrase> groups in <phrase>Europe</phrase>, US, andAsia. Michael is driving researchat <phrase>Siemens</phrase> in <phrase>data</phrase> analytics, <phrase>machine learning</phrase> and <phrase>big data</phrase> architectures. In the last two years he was responsible for creating the Sinalytics platformfor BigData applications across <phrase>Siemens</phrase>' <phrase>business</phrase>. In cooperation with <phrase>industry</phrase> he developed <phrase>Big Data</phrase> Analytics applications in sectors ranging from <phrase>telecommunication</phrase>,a utomotive, andr etail to <phrase>finance</phrase> and <phrase>advertising</phrase>.
Towards an Empirically Motivated Typology of Follow-Up Questions: The Role of Dialogue Context A central problem in Interactive <phrase>Question Answering</phrase> (IQA) is how to answer Follow-Up Questions (<phrase>FU Qs</phrase>), possibly by taking advantage of <phrase>information</phrase> from the dialogue context. We assume that <phrase>FU Qs</phrase> can be classified into specific types which determine if and how the correct answer relates to the preceding dialogue. The main goal of this <phrase>paper</phrase> is to propose an empirically motivated typology of <phrase>FU Qs</phrase>, which we then apply in a practical IQA setting. We adopt a supervised <phrase>machine learning</phrase> framework that ranks answer candidates to <phrase>FU Qs</phrase>. Both the answer ranking and the classification of <phrase>FU Qs</phrase> is done in this framework, based on a host of measures that include shallow and deep inter-utterance relations, automatically collected dialogue <phrase>management</phrase> meta <phrase>information</phrase>, and <phrase>human</phrase> annotation. We use <phrase>Principal Component Analysis</phrase> (<phrase>PCA</phrase>) to integrate these measures. As a result, we confirm earlier findings about the benefit of distinguishing between topic shift and topic continuation <phrase>FU Qs</phrase>. We then present a typology of <phrase>FU Qs</phrase> that is more <phrase>fine-grained</phrase>, extracted from the <phrase>PCA</phrase> and based on real dialogue <phrase>data</phrase>. Since all our measures are automatically <phrase>computable</phrase>, our <phrase>results</phrase> are relevant for IQA systems dealing with naturally occurring <phrase>FU Qs</phrase>.
<phrase>Left ventricle</phrase> segmentation from <phrase>cardiac</phrase> <phrase>MRI</phrase> combining <phrase>level set</phrase> methods with <phrase>deep belief</phrase> networks This <phrase>paper</phrase> introduces a new semi-automated methodology combining <phrase>a level</phrase> set method with a top-down segmentation <phrase>produced</phrase> by a <phrase>deep belief</phrase> network for the problem of <phrase>left ventricle</phrase> segmen-tation from <phrase>cardiac</phrase> <phrase>magnetic resonance</phrase> images (<phrase>MRI</phrase>). Our approach combines the <phrase>level set</phrase> advantages that uses several a <phrase>pri</phrase>-ori facts about the object to be segmented (e.g., smooth contour, strong edges, etc.) with the <phrase>knowledge</phrase> automatically learned from a manually annotated <phrase>database</phrase> (e.g., shape and appearance of the object to be segmented). The use of <phrase>deep belief</phrase> networks is justified because of its ability to learn robust models with few annotated images and its flexibility that allowed us to adapt it to a top-down segmentation problem. We demonstrate that our method produces competitive <phrase>results</phrase> using the <phrase>database</phrase> of the MICCAI grand challenge on <phrase>left ventricle</phrase> segmentation from <phrase>cardiac</phrase> <phrase>MRI</phrase> images, where our methodology produces <phrase>results</phrase> on par with the best in the field in each one of the measures used in that challenge (<phrase>perpendicular</phrase> distance, <phrase>Dice</phrase> metric, and percentage of good detections). Therefore, we conclude that our proposed methodology is one of the most competitive approaches in the field.
Simple <phrase>Design</phrase> Tools First-year Graduate <phrase>Design</phrase> <phrase>Students Learn</phrase> to Quickly Evaluate Complicated Cost-performance Processor <phrase>Trade</phrase>-offs Using Simple the world sent thousands of <phrase>e</phrase>-mails to our group at <phrase>Stanford University</phrase> inquiring about the " secret " performance <phrase>report</phrase> of the <phrase>AMD</phrase> <phrase>K7</phrase> and <phrase>Intel</phrase> Coppermine chips. According to The Register <phrase>Web site</phrase> in the <phrase>United Kingdom</phrase> , there were rumors that some students at <phrase>Stanford University</phrase> had tested and compared the two chips using SPEC's <phrase>test</phrase> suite. At that time, the <phrase>AMD</phrase> <phrase>K7</phrase> (now known as <phrase>Athlon</phrase>) and the <phrase>Intel</phrase> Coppermine (now known as <phrase>Pentium III</phrase>) were not available to the <phrase>general</phrase> <phrase>public</phrase>, and it is easy to understand the excitement about this <phrase>news</phrase>. The real story was simpler. The students in our processor <phrase>design</phrase> class did not <phrase>test</phrase> the real chips. They estimated the cost and performance of the two chips as part of a <phrase>case study</phrase>. This study was based on <phrase>information</phrase> available from various <phrase>public</phrase> sources. 3 As the chip implementation details had not been released to the <phrase>public</phrase>, the instructors assumed the hardware designs were similar to the simulator default configurations. After comparing the performance and the costs, the students used the simulator tools to <phrase>design</phrase> improvements to each chip. Designing <phrase>deep-submicron</phrase> <phrase>microprocessors</phrase> is becoming an increasingly tedious and complicated process. 4 It takes many years and many engineers to <phrase>design</phrase> a commercial processor chip. In an introductory computer <phrase>architecture</phrase> course, students are usually required to simulate a simplified pipelined <phrase>microprocessor</phrase> such as DLX 5 using some <phrase>hardware description language</phrase> (typically <phrase>Verilog</phrase> or <phrase>VHDL</phrase>) or some <phrase>graphical</phrase> tool (such as HASE). 6 While it is important for a <phrase>student</phrase> to understand how a <phrase>basic</phrase> processor operates, students may not fully appreciate the complexity and the various <phrase>trade</phrase>-offs involved in designing a processor in the commercial environment. Students cannot afford to write tens of thousands of lines of code to <phrase>model</phrase> processor <phrase>microarchitecture</phrase>; it is unproductive to ask them to deal with the myriad details at this stage. Instead, we focus on <phrase>high</phrase>-level issues involving cost (<phrase>area</phrase>) and performance (execution time). The main issues are cache size, cycle time, <phrase>floating-point unit</phrase> (<phrase>FPU</phrase>) <phrase>area</phrase>, latencies, branch strategy, and issue width. By emphasizing a few primary <phrase>high</phrase>-level issues, students gain a better understanding of the <phrase>trade</phrase>-offs involved in overall computer <phrase>architecture</phrase> <phrase>design</phrase>. Table 1 lists the <phrase>architectural</phrase> <phrase>design</phrase> tools available in our class. Students use these tools in conjunction with the class <phrase>textbook</phrase>, 7 and they are also available for other researchers and
The <phrase>Fingerprint</phrase> of <phrase>Human</phrase> Referring Expressions and their Surface Realization with <phrase>Graph</phrase> <phrase>Transducers</phrase> (IS-FP, IS-GT, IS-FP-GT)} The <phrase>algorithm</phrase> IS-FP takes up the idea from the IS-FBN <phrase>algorithm</phrase> developed for the shared task 2007. Both <phrase>algorithms</phrase> learn the individual attribute selection style for each <phrase>human</phrase> that provided referring expressions to the corpus. The IS-FP <phrase>algorithm</phrase> was developed with two additional goals (1) to improve the indentification time that was poor for the FBN <phrase>algorithm</phrase> and (2) to push the <phrase>dice</phrase> score even higher. In <phrase>order</phrase> to generate a word string for the selected attributes, we build based on individual preferences a surface <phrase>syntactic</phrase> dependency <phrase>tree</phrase> as input. We derive the individual preferences from the <phrase>training set</phrase>. Finally, a <phrase>graph</phrase> <phrase>transducer</phrase> maps the input strucutre to a deep morphologic structure. A review of the referring expressions shows that humans prefer frequently distinct attributes and attribute combination such as in the following examples. grey desk (30t1), a <phrase>red</phrase> chair (30t2), <phrase>red</phrase> sofa (30t3), <phrase>blue</phrase> chair (30t5), a small <phrase>green</phrase> desk (30t6) the one in the top left corner (31t1), the one to the left in the middle row (31t2), the bottom right most one (31t3), the <phrase>blue</phrase> chair at the bottom <phrase>center</phrase> (31t5), etc. The first individual (#30) seems to prefer colour and size while the second one (#31) seems to prefer the relative position (to the left) and places (the top left corner). Because of the review, we checked, if the Incremental <phrase>Algorithm</phrase> (Dale and Reiter, 1995) using the <phrase>order</phrase> for the attributes due to the <phrase>frequency</phrase> calculated for each individual can outperform the <phrase>algorithm</phrase> using the <phrase>order</phrase> for the attributes due to the <phrase>frequency</phrase> of the complete <phrase>training set</phrase>. This was the case. Table 1 shows the <phrase>results</phrase>. Using the individual attribute <phrase>order</phrase>, the IA performed as good as the FBN <phrase>algorithm</phrase>, cf. Table 1.
<phrase>Cognitive</phrase> <phrase>Media</phrase> Types for <phrase>Multimedia</phrase> <phrase>Information</phrase> Access <phrase>Multimedia</phrase> repositories, <phrase>libraries</phrase>, and <phrase>databases</phrase> ooer the potential for providing students with access to a wide <phrase>variety</phrase> of interconnected <phrase>information</phrase> resources. However, in <phrase>order</phrase> to realize this potential, <phrase>multimedia</phrase> systems should provide access to <phrase>information</phrase> and activities that support eeective <phrase>knowledge</phrase> <phrase>construction</phrase> and learning by students. This article proposes a theoretical framework for organizing <phrase>information</phrase> and activities in educational <phrase>hypermedia</phrase> systems. We show that such systems should not be characterized primarily in terms of the kinds of physical <phrase>media</phrase> types that can be accessed; instead, the important <phrase>aspect</phrase> is the content that can be represented within a physical <phrase>media</phrase>, rather than the physical <phrase>media</phrase> itself. We propose a theory of \cogni-tive <phrase>media</phrase> types" based on the inferential and learning processes of <phrase>human</phrase> users. The theory highlights speciic <phrase>media</phrase> characteristics that facilitate speciic <phrase>problem solving</phrase> actions, which in turn are enabled by speciic kinds of physical <phrase>media</phrase>. We present an implemented computer system, called AlgoNet, that supports <phrase>hypermedia</phrase> <phrase>information</phrase> access and constructive learning activities for self-paced learning in computer and <phrase>engineering</phrase> disciplines. Extensive empirical evaluations with <phrase>undergraduate</phrase> students suggest that self-paced interactive <phrase>learning environments</phrase>, coupled with <phrase>multimedia</phrase> <phrase>information</phrase> access and constructive activities organized into <phrase>cognitive</phrase> <phrase>media</phrase> types, can support and help students develop deep intuitions about important concepts in a given domain.
<phrase>Environmental Noise</phrase> Embeddings for Robust <phrase>Speech Recognition</phrase> We propose a novel <phrase>deep neural network</phrase> <phrase>architecture</phrase> for <phrase>speech recognition</phrase> that explicitly employs <phrase>knowledge</phrase> of the background <phrase>environmental noise</phrase> within a <phrase>deep neural network</phrase> <phrase>acoustic</phrase> <phrase>model</phrase>. A <phrase>deep neural network</phrase> is used to predict the <phrase>acoustic</phrase> environment in which the system in being used. The discriminative embedding generated at the bottleneck layer of this network is then concatenated with traditional <phrase>acoustic</phrase> features as input to a <phrase>deep neural network</phrase> <phrase>acoustic</phrase> <phrase>model</phrase>. Using simulated <phrase>acoustic</phrase> environments we show that the <phrase>proposed approach</phrase> significantly improves <phrase>speech recognition</phrase> accuracy in noisy and highly reverberant environments, outperforming multi-condition training and <phrase>multi-task</phrase> learning for this task.
Educational Inquiry and <phrase>Creativity</phrase>: Developing <phrase>Digital</phrase> Resources in Ireland's <phrase>Information Age</phrase> <phrase>Town</phrase> Despite <phrase>cultural</phrase> and social barriers to <phrase>technology</phrase> <phrase>adoption</phrase>, the teachers and administrators of schools in the <phrase>Irish</phrase> <phrase>town</phrase> of <phrase>Ennis</phrase>, with help from the <phrase>Clare</phrase> <phrase>County</phrase> <phrase>Education</phrase> Centre, integrated <phrase>computers</phrase>, networking, <phrase>software</phrase>, <phrase>Internet</phrase>, and <phrase>digital</phrase> imagery into the schools' <phrase>curriculum</phrase>. Success in <phrase>technology</phrase> integration has been achieved in <phrase>primary education</phrase> due to persistent and innovative efforts on the part of teachers; however, <phrase>technology</phrase> integration has not been as deep nor as broad in the <phrase>secondary</phrase>-level schools. Students have <phrase>produced</phrase> many new learning materials themselves using computer-based tools, the <phrase>Internet</phrase>, and other technologies. The barriers, challenges, and achievements of the <phrase>community</phrase> described in this <phrase>case study</phrase> may have implications for other small communities who wish to use <phrase>technology</phrase> for teaching, learning, and <phrase>curriculum</phrase> change.
Practical <phrase>Gradient-descent</phrase> for Memristive Crossbars This <phrase>paper</phrase> discusses implementations of <phrase>gradient-descent</phrase> based <phrase>learning algorithms</phrase> on memristive crossbar arrays. The Unregulated Step Descent (<phrase>USD</phrase>) is described as a practical <phrase>algorithm</phrase> for <phrase>feed-forward</phrase> on-line training of large crossbar arrays. It allows fast <phrase>feed-forward</phrase> fully parallel on-line hardware based learning, without requiring accurate models of the <phrase>memristor</phrase> behaviour and precise control of the <phrase>programming</phrase> pulses. The effect of device parameters, training parameters, and device variability on the learning performance of crossbar arrays trained using the <phrase>USD</phrase> <phrase>algorithm</phrase> has been studied via simulations. There is a significant interest in using memristive devices for computation, in particular in the context of neuromorpic systems [1] and <phrase>artificial neural networks</phrase> [2-7]. Memristors are typically fabricated in the form of highly-dense crossbar arrays, which naturally lend themselves to the <phrase>vector</phrase>-matrix multiplications that are at the core of the <phrase>neural network</phrase> <phrase>algorithms</phrase>. <phrase>Memristor</phrase>-based hardware implementations, while promising <phrase>low-power</phrase> <phrase>high</phrase>-speed computation, need to address several challenges, such as extreme device variability, complex <phrase>state</phrase>-dependent behaviours, or difficulty in integrating active devices within the crossbar. In this <phrase>paper</phrase>, we discuss an approximate <phrase>gradient-descent</phrase> based <phrase>learning algorithm</phrase>, called the Unregulated Step Descent (<phrase>USD</phrase>) [8] that addresses these hardware issues, and provides a practical method for training large crossbar arrays in <phrase>machine learning</phrase> applications. We consider a hardware implementation of an optimization process, where the parameters w of the cost <phrase>function</phrase> F(w) to be minimized are implemented as conductances of memristors in a crossbar array. A typical example of this is the <phrase>supervised learning</phrase> of weights in an <phrase>artificial neural network</phrase>. In the simplest case, that of a linear classifier (<phrase>perceptron</phrase>), the inputs can be applied as voltages to the rows of the crossbar array, and the output is obtained by applying a <phrase>sigmoid</phrase> nonlinearity to the column current. This <phrase>basic</phrase> hardware block can be extended to larger networks, as typically used in <phrase>deep learning</phrase> systems [7]. The goal of the optimization is to minimize the cost <phrase>function</phrase> expressing the difference between the actual outputs of the network, and the desired outputs, across the dataset. In this scenario, the training is achieved by delivering the <phrase>programming</phrase> pulses to the <phrase>memristor</phrase> devices, to change the weights. We are also interested in in situ training, where the crossbar array itself is used to implement the <phrase>learning algorithm</phrase>, as opposed to ex situ training, where the weights are determined outside the array, and then uploaded to the memristors. In situ training 
Learning to Surface <phrase>Deep Web</phrase> Content We propose a novel <phrase>deep web</phrase> crawling framework based on <phrase>reinforcement learning</phrase>. The crawler is regarded as an agent and <phrase>deep web</phrase> <phrase>database</phrase> as the environment. The agent perceives its <phrase>current state</phrase> and submits a selected <phrase>action</phrase> (query) to the environment according to Q-value. Based on the framework we develop an adaptive crawling method. <phrase>Experimental</phrase> <phrase>results</phrase> show that it outperforms the <phrase>state</phrase> of <phrase>art</phrase> methods in crawling capability and breaks through the assumption of full-text search implied by <phrase>existing methods</phrase>.
<phrase>Mobile Robot</phrase> Localization Using Pattern Classification Techniques Abstract <phrase>Mobile Robot</phrase> Localization Using Pattern Classification Techniques This <phrase>thesis</phrase> describes a methodology for coarse position estimation of a <phrase>mobile robot</phrase> within an indoor setting. The approach is divided into two phases: exploration and <phrase>navigation</phrase>. In the exploration phase, the <phrase>mobile robot</phrase> is allowed to sense the environment for the purpose of mapping and thus \learning" its unknown surroundings. In the <phrase>navigation</phrase> phase, the <phrase>robot</phrase> senses the surroundings and compares this <phrase>information</phrase> with its learned maps for the purpose of locating itself in the workspace. We solve the task of coarse-level <phrase>mobile robot</phrase> localization via pattern classiication of grid-based maps of important or interesting workspace regions. Using datasets representing 10 diierent rooms and doorways, we estimate a 94% recognition rate of the rooms and a 98% recognition rate of the doorways. We conclude that coarse position estimation is possible through classiication of grid-based maps in indoor domains. Now to the <phrase>King</phrase> eternal, immortal, invisible, the only <phrase>God</phrase>, be honor and glory for ever and ever. <phrase>Amen</phrase>. { 1 Timothy 1:17 iii ACKNOWLEDGMENTS First, I want to <phrase>express my deep</phrase> <phrase>love</phrase> and gratitude to my wife Mickie. Without her prayers, encouragement, and sacriice, my graduate <phrase>school</phrase> studies would not be possible. In many ways, this <phrase>thesis</phrase> should be hers. I would also like to thank my advisor, <phrase>Professor</phrase> Anil K. <phrase>Jain</phrase>. He provided insight, direction, and encouragement that were crucial for this <phrase>thesis</phrase>. He also taught me more about professionalism and self discipline than he realizes. Thanks as well to my committee, Professors Charles MacCluer and Abdol-Hossein Esfahanian, for their critique of my <phrase>research</phrase> and for their personal support. <phrase>Professor</phrase> George Stockman and Dr. Mihran T uceryan were also <phrase>instrumental</phrase> in the birth of <phrase>robotics</phrase> <phrase>research</phrase> in our <phrase>laboratory</phrase>. The work of Dr. Alberto Elfes had a signiicant impact upon my earliest thinking regarding <phrase>mobile</phrase> <phrase>robotics</phrase>. Additionally, the advice and insight he has ooered in our personal communications has been a source of direction and <phrase>motivation</phrase> for me. No doubt when I <phrase>look back</phrase> upon my days at <phrase>Michigan State</phrase> some of my fondest memories will be of the PRIP Lab and the PRIPpies that inhabit it. To all of them (faculty included): thanks for the encouragement, stimulating conversations, dinners from around the world, and regular excursions to El Azteco. Brian Wright and Roxanne Fuller provided invaluable assistance when my <phrase>electronic</phrase> abilities failed me (or turned up completely absent). Thanks also goes to John Lees for his friendship and 
Rule Of Thumb: Deep derotation for improved fingertip detection We investigate a novel global orientation <phrase>regression</phrase> approach for articulated objects using a <phrase>deep convolutional</phrase> <phrase>neural network</phrase>. This is integrated with an in-plane image derotation scheme, DeROT, to tackle the problem of per-frame fingertip detection in depth images. The method reduces the complexity of learning in the space of articulated poses which is demonstrated by using two distinct <phrase>state</phrase>-of-the-<phrase>art</phrase> learning based hand pose estimation methods applied to fingertip detection. Significant classification improvements are shown over the baseline implementation. Our framework involves no tracking, <phrase>kinematic</phrase> constraints or explicit prior <phrase>model</phrase> of the articulated object in hand. To support our approach we also describe a new pipeline for <phrase>high</phrase> accuracy <phrase>magnetic</phrase> annotation and labeling of objects imaged by a depth <phrase>camera</phrase>.
<phrase>Unsupervised learning</phrase> of invariant representations a r t i c l <phrase>e</phrase> i n f o a b s t r a c t The present phase of <phrase>Machine Learning</phrase> is characterized by <phrase>supervised learning</phrase> <phrase>algorithms</phrase> relying on large sets of labeled examples (n ). The next phase is likely to focus on <phrase>algorithms</phrase> capable of learning from very few labeled examples (n 1), like humans seem able to do. We propose an approach to this problem and describe the underlying theory, based on the unsupervised, automatic learning of a " good " representation for <phrase>supervised learning</phrase>, characterized by small sample complexity. We consider the case of <phrase>visual object</phrase> recognition, though the theory also applies to other domains like speech. The <phrase>starting point</phrase> is the conjecture, proved in specific cases, that <phrase>image representations</phrase> which are invariant to <phrase>translation</phrase>, scaling and other transformations can considerably reduce the sample complexity of learning. We prove that an invariant and selective signature can be computed for each image or image patch: the invariance can be exact in the case of group transformations and approximate under non-group transformations. A module performing filtering and pooling, like the simple and complex cells described by Hubel and Wiesel, can compute such signature. The theory offers novel <phrase>unsupervised learning</phrase> <phrase>algorithms</phrase> for " deep " architectures for image and <phrase>speech recognition</phrase>. We conjecture that the main computational goal of the ventral <phrase>stream</phrase> of <phrase>visual cortex</phrase> is to provide a hierarchical representation of new objects/images which is invariant to transformations, stable, and selective for recognitionand show how this representation may be continuously learned in an unsupervised way during development and visual experience.
An Overview of Deep-structured Learning for <phrase>Information Processing</phrase> In this <phrase>paper</phrase>, I will introduce to the APSIPA audience an emerging <phrase>area</phrase> of <phrase>machine learning</phrase>, deep-structured learning. It refers to a class of <phrase>machine learning</phrase> techniques, developed mostly since 2006, where many layers of <phrase>information processing</phrase> stages in hierarchical architectures are exploited for pattern classification and for <phrase>unsupervised feature learning</phrase>. First, the brief <phrase>history</phrase> of <phrase>deep learning</phrase> is discussed. Then, I develop a classificatory scheme to analyze and summarize <phrase>major</phrase> work reported in the <phrase>deep learning</phrase> <phrase>literature</phrase>. Using this scheme, I provide a <phrase>taxonomy</phrase>-oriented survey on the existing <phrase>deep architectures</phrase>, and categorize them into three types: generative, discriminative, and <phrase>hybrid</phrase>. Two prime <phrase>deep architectures</phrase>, one <phrase>hybrid</phrase> and one discriminative, are presented in detail. Finally, selected applications of <phrase>deep learning</phrase> are reviewed in broad areas of <phrase>information processing</phrase> including audio/speech,
Deep Successor <phrase>Reinforcement Learning</phrase> Learning robust value functions given raw observations and rewards is now possible with <phrase>model</phrase>-<phrase>free</phrase> and <phrase>model</phrase>-based <phrase>deep reinforcement learning</phrase> <phrase>algorithms</phrase>. There is a third <phrase>alternative</phrase>, called Successor Representations (SR), which decomposes the value <phrase>function</phrase> into two components a reward predictor and a successor map. The successor map represents the expected future <phrase>state</phrase> occupancy from any given <phrase>state</phrase> and the reward predictor maps states to scalar rewards. The value <phrase>function</phrase> of a <phrase>state</phrase> can be computed as the inner product between the successor map and the reward weights. In this <phrase>paper</phrase>, we present DSR, which generalizes SR within an <phrase>end-to-end</phrase> <phrase>deep reinforcement learning</phrase> framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw <phrase>pixel</phrase> observations simple grid-world domains (MazeBase) and the <phrase>Doom</phrase> <phrase>game engine</phrase>.
<phrase>Scaffolding</phrase> <phrase>Problem Solving</phrase> with Annotated, Worked-Out Examples to <phrase>Promote Deep Learning</phrase> This study seeks to compare the relative utility for learning <phrase>college</phrase>-level <phrase>physics</phrase> of <phrase>intelligent tutoring</phrase> systems that have procedural based hints and worked-out examples. In <phrase>order</phrase> to <phrase>test</phrase> which <phrase>produced</phrase> better gains, a modified version of <phrase>Andes</phrase> was used in which participants either received hints or annotated, worked-out examples in response to their help requests. We found that providing annotated, worked-out examples instead of hint sequences was more efficient in the number of problems it took to obtain <phrase>basic</phrase> mastery.
A Sparse and Locally Shift Invariant Feature Extractor Applied to Document Images We describe an <phrase>unsupervised learning</phrase> <phrase>algorithm</phrase> for extracting sparse and locally shift-<phrase>invariant features</phrase>. We also devise a principled procedure for learning hierarchies of <phrase>invariant features</phrase>. Each feature detector is composed of a set of trainable convolutional filters followed by a max-pooling layer over non-overlapping <phrase>windows</phrase>, and a point-wise sig-moid non-linearity. A second stage of more <phrase>invariant features</phrase> is fed with patches provided by the first stage feature extractor, and is trained in the same way. The method is used to pre-<phrase>train</phrase> the first four layers of a <phrase>deep convolutional</phrase> network which achieves <phrase>state</phrase>-of-the-<phrase>art</phrase> performance on the MNIST dataset of <phrase>handwritten digits</phrase>. The final testing <phrase>error rate</phrase> is equal to 0.42%. Preliminary experiments on compression of bitonal document images show very <phrase>promising results</phrase> in terms of <phrase>compression ratio</phrase> and <phrase>reconstruction</phrase> error .
Deep <phrase>Markov Random Field</phrase> for Image Modeling <phrase>Markov</phrase> <phrase>Random Fields</phrase> (MRFs), a formulation widely used in generative image modeling, have <phrase>long</phrase> been plagued by the lack of expressive power. This issue is primarily due to the fact that conventional MRFs formulations tend to use simplistic factors to capture local patterns. In this <phrase>paper</phrase>, we move beyond such limitations, and propose a novel MRF <phrase>model</phrase> that uses <phrase>fully-connected</phrase> <phrase>neurons</phrase> to express the complex interactions among <phrase>pixels</phrase>. Through theoretical analysis, we reveal an inherent connection between this <phrase>model</phrase> and <phrase>recurrent neural networks</phrase> , and thereon derive an approximated <phrase>feed-forward</phrase> network that couples multiple RNNs along opposite directions. This formulation combines the expressive power of <phrase>deep neural networks</phrase> and the cyclic dependency structure of MRF in a unified <phrase>model</phrase>, bringing the modeling capability to a new level. The <phrase>feed-forward</phrase> approximation also allows it to be efficiently learned from <phrase>data</phrase>. <phrase>Experimental</phrase> <phrase>results</phrase> on a <phrase>variety</phrase> of <phrase>low-level</phrase> vision tasks show notable improvement over <phrase>state</phrase>-of-the-<phrase>arts</phrase>.
<phrase>Semantic</phrase>-Based Access to <phrase>Digital</phrase> Document <phrase>Databases</phrase> Discovering significant meta-<phrase>information</phrase> from document collections is a critical factor for <phrase>knowledge</phrase> distribution and preservation. This <phrase>paper</phrase> presents a system that implements intelligent document processing techniques, by combining strategies for the layout analysis of <phrase>electronic</phrase> documents with incremental first-<phrase>order</phrase> learning in <phrase>order</phrase> to automatically classify the documents and their layout components according to their <phrase>semantics</phrase>. Indeed, an in-deep analysis of specific layout components can allow the extraction of useful <phrase>information</phrase> to improve the <phrase>semantic</phrase>-based document storage and retrieval tasks. The viability of the <phrase>proposed approach</phrase> is confirmed by experiments run in the <phrase>real-world</phrase> application domain of scientific papers.
Empirical Analysis of the Online Rating Systems This <phrase>paper</phrase> is to analyze the properties of evolving <phrase>bipartite</phrase> networks from four aspects, the growth of networks, the <phrase>degree distribution</phrase>, the popularity of objects and the diversity of user behaviours, leading a <phrase>deep understanding</phrase> on the empirical <phrase>data</phrase>. By <phrase>empirical studies</phrase> of <phrase>data</phrase> from the online bookstore <phrase>Amazon</phrase> and a question and answer site <phrase>Stack Overflow</phrase>, which are both rating <phrase>bipartite</phrase> networks, we could reveal the rules for the <phrase>evolution</phrase> of <phrase>bipartite</phrase> networks. These rules have significant meanings in practice for maintaining the operation of real systems and preparing for their future development. We find that the <phrase>degree distribution</phrase> of users follows a <phrase>power law</phrase> with an exponential cutoff. Also, according to the <phrase>evolution</phrase> of popularity for objects, we find that the large-<phrase>degree</phrase> objects tend to receive more new ratings than expected depending on their current degrees while the small-<phrase>degree</phrase> objects receive less ratings in terms of their degrees. Moreover, the user behaviours show such a trend that the larger <phrase>degree</phrase> the users have, the stronger purposes are with their behaviours except the initial periods when users choose a diversity of <phrase>products</phrase> to learn about what they want. Finally, we conclude with a discussion on how the <phrase>bipartite</phrase> network evolves, which provides guideline for meeting challenges brought by the growth of network.
Learning Financial Rating Tendencies with Qualitative <phrase>Trees</phrase> Learning financial rating tendencies requires <phrase>knowledge</phrase> of the ratios and values that indicate a firm's situation as well as a <phrase>deep understanding</phrase> of the relationships between them and the main factors that can modify these values. In this work, the Qualitative <phrase>Trees</phrase> provided by the <phrase>algorithm</phrase> QUIN are used to <phrase>model</phrase> financial rating and to learn its tendencies. Some examples are given to show the system's predictive capabilities. The rating tendencies and the variables that most influence those tendencies are analyzed.
Using Symbolic Learning to Improve <phrase>Knowledge</phrase>-Based <phrase>Neural Networks</phrase> The previously-reported Kbann system integrates existing <phrase>knowledge</phrase> into <phrase>neural networks</phrase> by deening the <phrase>network topology</phrase> and setting initial link weights. Standard neural learning techniques can then be used to <phrase>train</phrase> such networks, thereby reening the <phrase>information</phrase> upon which the network is based. However, standard neural learning techniques are reputed to have dii-culty training networks with <phrase>multiple layers</phrase> of <phrase>hidden units</phrase>; Kbann commonly creates such networks. In addition , standard neural learning techniques ignore some of the <phrase>information</phrase> contained in the networks created by Kbann. This <phrase>paper</phrase> describes a symbolic inductive <phrase>learning algorithm</phrase> for training such networks that uses this previously-ignored <phrase>information</phrase> and which helps to address the problems of training \deep" networks. <phrase>Empirical evidence</phrase> shows that this method improves not only learning speed, but also the ability of networks to generalize correctly to testing examples.
Detecting bots via incremental LS-<phrase>SVM</phrase> learning with dynamic feature adaptation As botnets continue to proliferate and grow in sophistication, so does the need for more advanced <phrase>security</phrase> solutions to effectively detect and defend against such attacks. In particular, botnets such as <phrase>Conficker</phrase> have been known to <phrase>encrypt</phrase> the <phrase>communication</phrase> packets exchanged between bots and their command-and-control server, making it costly for existing <phrase>botnet</phrase> detection systems that rely on <phrase>deep packet inspection</phrase> (DPI) methods to identify compromised machines. In this <phrase>paper</phrase>, we argue that, even in the face of encrypted traffic flows, botnets can still be detected by examining the set of server <phrase>IP-addresses</phrase> visited by a client machine in the past. However there are several challenges that must be addressed. First, the set of server <phrase>IP-addresses</phrase> visited by client machines may evolve dynamically. Second, the set of client machines used for training and their class <phrase>labels</phrase> may also change over time. To overcome these challenges, this <phrase>paper</phrase> presents a novel incremental LS-<phrase>SVM</phrase> <phrase>algorithm</phrase> that is adaptive to both changes in the feature set and class <phrase>labels</phrase> of training instances. To evaluate the performance of our <phrase>algorithm</phrase>, we have performed experiments on two <phrase>large-scale</phrase> datasets, including real-time <phrase>data</phrase> collected from <phrase>peering</phrase> routers at a large Tier-1 <phrase>ISP</phrase>. <phrase>Experimental</phrase> <phrase>results</phrase> showed that the <phrase>proposed algorithm</phrase> produces <phrase>classification accuracy</phrase> comparable to its batch counterpart, while consuming significantly less computational resources.
Discovering <phrase>syntactic</phrase> deep structure via <phrase>Bayesian statistics</phrase> In the <phrase>Bayesian</phrase> framework, a <phrase>language</phrase> learner should seek a <phrase>grammar</phrase> that explains observed <phrase>data</phrase> well and is also a priori probable. This <phrase>paper</phrase> proposes such a measure of <phrase>prior probability</phrase>. Indeed it develops a full statistical framework for lexicalized <phrase>syntax</phrase>. The learner's job is to discover the system of probabilistic transformations (often called lexical redundancy rules) that underlies the patterns of regular and irregular <phrase>syntactic</phrase> constructions <phrase>listed</phrase> in the <phrase>lexicon</phrase>. Specifically, the learner discovers what transformations apply in the <phrase>language</phrase>, how often they apply, and in what contexts. It considers simpler systems of transformations to be more probable a priori. Experiments show that the learned transformations are more effective than previous <phrase>statistical models</phrase> at predicting the <phrase>probabilities</phrase> of lexical entries, especially those for which the learner had no direct evidence.
Don't Do Things You Can't Undo: Reversibility Models for Generating Safe Behaviours We argue that an ability to determine the re-versibility of actions allows a <phrase>robot</phrase> to identify safe behaviors autonomously. We introduce a notion of reversibility <phrase>model</phrase> and give a definition of <phrase>model</phrase> refinement. We implement this on a <phrase>real robot</phrase> and observe that, when a reversibility <phrase>model</phrase> is refined by the addition of proximity sensors, obstacle avoidance emerges as a side-effect of avoiding irreversible actions. We interpret this as evidence of a deep connection between reversibility and safe behaviour. We also observe that, on the <phrase>real robot</phrase>, reversiblities are learned as efficiently as a dedicated <phrase>reward function</phrase>. We conclude that reversibility identification may provide an abstract and yet practical method of generating a <phrase>variety</phrase> of safe behaviours. I. INTRODUCTION This <phrase>paper</phrase> is concerned with a robot's ability to undo its actions. We suggest that reversibility, a necessary condition of <phrase>controllability</phrase>, is a fundamental concept when <phrase>programming</phrase> <phrase>robots</phrase> to behave safely and reliably. We ask if this principle can be used to govern the operation of a <phrase>robot</phrase>, and to generate useful behaviour on a <phrase>real robot</phrase> and in real time. We speculate that the most undesirable actions in the <phrase>real world</phrase>, those that damage the <phrase>robot</phrase> or the environment, for example, are characterized by irreversibility. Thus, instead of <phrase>programming</phrase> the <phrase>robot</phrase> with specific routines that prevent collisions, prevent <phrase>falls</phrase>, and so on, we program the <phrase>robot</phrase> with a more <phrase>general</phrase> principle of avoiding irreversible actions. In other words, instead of telling the <phrase>robot</phrase> what should not be done, we try to tell it why it should not be done. For example, falling down stairs is not good because the <phrase>robot</phrase> does not know how to climb back or pushing the door closed is not good because it does not have <phrase>knowledge</phrase> of how to open it. In this <phrase>paper</phrase>, we <phrase>state</phrase> the problem of learning a re-versibility <phrase>model</phrase>. The reversibility <phrase>model</phrase> represents the robot's <phrase>knowledge</phrase> of <phrase>state</phrase>-<phrase>action</phrase> pairs that are reversible and the ways of reversing them. We go on to demonstrate how this reversibility <phrase>model</phrase> can be established and used to generate new behaviours. In [1], we showed that by suppressing irreversible actions the <phrase>robot</phrase> will develop obstacle avoidance behaviour. In this <phrase>paper</phrase>, we confirm this result and go on to demonstrate that, as a developmental system, the efficiency of our abstract approach is comparable to
<phrase>Bayesian</phrase> Generic Priors for Causal Learning Structure and Strength in Causal Models Causal <phrase>Graphs</phrase> Lend Themselves to the Development of Rational Models Based on <phrase>Bayesian</phrase> We present a <phrase>Bayesian</phrase> <phrase>model</phrase> of causal learning that incorporates generic priors on <phrase>distributions</phrase> of weights representing potential powers to either produce or prevent an effect. These generic priors favor necessary and sufficient causes. The NS power <phrase>model</phrase> couples these priors with a causal <phrase>generating function</phrase> derived from the power <phrase>PC</phrase> theory (Cheng, 1997). We <phrase>test</phrase> this and other <phrase>alternative</phrase> <phrase>Bayesian</phrase> models using the strategy of computational <phrase>cognitive</phrase> <phrase>psychophysics</phrase>, fitting multiple <phrase>data</phrase> sets in which several parameters are varied parametrically across multiple types of judgments. The NS power <phrase>model</phrase> accounts for a wide <phrase>range</phrase> of <phrase>data</phrase> concerning judgments of both causal strength (the power of a cause to produce or prevent an effect) and causal structure (whether or not a causal link exists). For both types of causal judgments, a generic prior favoring a cause that is jointly necessary and sufficient explains interactions involving causal direction (generative versus preventive causes). For structure judgments, an additional prior that a new candidate cause will be deterministic (i.e., sufficient or else ineffective) explains why people's causal structure judgments are based primarily on causal power and the base rate of the effect, rather than sample size. <phrase>Alternative</phrase> <phrase>Bayesian</phrase> formulations that lack either causal power assumptions or generic priors for necessity and sufficiency proved inadequate. Broader implications of the <phrase>Bayesian</phrase> framework for <phrase>human</phrase> learning are discussed. Intelligent behavior in a complex and potentially hostile environment depends on acquiring and exploiting <phrase>knowledge</phrase> of " what causes what. " Although causal induction can certainly be constrained in a top-down <phrase>fashion</phrase> by prior causal <phrase>knowledge</phrase>, new causal <phrase>knowledge</phrase> ultimately depends on an <phrase>inference engine</phrase> that takes non-causal <phrase>knowledge</phrase> (most notably, <phrase>information</phrase> about temporal <phrase>order</phrase> and covariation) as input and yields causal <phrase>knowledge</phrase> as its output (Cheng, 1993). It is likely that the <phrase>cognitive</phrase> mechanisms for causal learning have deep <phrase>evolutionary</phrase> roots, a conjecture supported by many parallels between phenomena in <phrase>animal</phrase> conditioning and <phrase>human</phrase> causal learning (see <phrase>Shanks</phrase>, 2004). It is therefore a plausible <phrase>hypothesis</phrase> that the system for <phrase>general</phrase> causal learning is well-adapted to the characteristics of causes that operate in the <phrase>natural environment</phrase>, and hence will be characterized by <phrase>bounded rationality</phrase> (Simon, 1955). A useful formalism for representing causal hypotheses is provided by <phrase>directed</phrase> causal <phrase>graphs</phrase>, simple examples of which are shown in Figure 1. Within a causal <phrase>graph</phrase>, each <phrase>directed</phrase> <phrase>arrow</phrase> connects a node representing a cause to one of its effects, where it is 
Stacked <phrase>Extreme Learning</phrase> Machines <phrase>Extreme learning</phrase> machine (<phrase>ELM</phrase>) has recently attracted many researchers' interest due to its very fast learning speed, good generalization ability, and ease of implementation. It provides a unified <phrase>solution</phrase> that can be used directly to solve <phrase>regression</phrase>, <phrase>binary</phrase>, and multiclass classification problems. In this <phrase>paper</phrase>, we propose a stacked ELMs (S-ELMs) that is specially designed for solving large and complex <phrase>data</phrase> problems. The S-ELMs divides a <phrase>single</phrase> large <phrase>ELM</phrase> network into multiple stacked small ELMs which are serially connected. The S-ELMs can approximate a very large <phrase>ELM</phrase> network with small <phrase>memory</phrase> requirement. To further improve the testing accuracy on <phrase>big data</phrase> problems, the <phrase>ELM</phrase> autoencoder can be implemented during each iteration of the S-ELMs <phrase>algorithm</phrase>. The <phrase>simulation</phrase> <phrase>results</phrase> show that the S-ELMs even with random <phrase>hidden nodes</phrase> can achieve similar testing accuracy to <phrase>support vector machine</phrase> (<phrase>SVM</phrase>) while having low <phrase>memory</phrase> requirements. With the help of <phrase>ELM</phrase> autoencoder, the S-ELMs can achieve much better testing accuracy than <phrase>SVM</phrase> and slightly better accuracy than <phrase>deep belief</phrase> network (DBN) with much faster training speed.
Objectively Structured Performance Evaluation a Learning Tool The <phrase>teaching and learning</phrase> of <phrase>medical</phrase> students has always been a complicated process. Even the best of teachers at time may struggle in communicating <phrase>knowledge</phrase> and assessing its uptake. Simulated clinical and practical tools have recently gained popularity across the globe. They provide <phrase>information</phrase> regarding all the three aspects of assessment namely <phrase>knowledge</phrase>, skills and attitude. <phrase>OSCE</phrase> was first introduced by Harden in 1975. It encourages <phrase>deep learning</phrase> by testing higher <phrase>cognitive</phrase> functions. <phrase>University</phrase> of <phrase>Health</phrase> Sciences <phrase>Lahore</phrase> (UHS) modified the <phrase>OSCE</phrase> and introduced Objectively Structured Performance Evaluation (OSPE) in 2008. In <phrase>Pakistan</phrase>, it is a relatively new assessment method. The aim of OSPE is to make practical examinations <phrase>fair</phrase>, objective and standardized in line with Best Evidence <phrase>Medical</phrase> <phrase>Education</phrase> (BEME) and the local needs. Assessment techniques appear to have an impact on students' study strategies' and influence their performance, that is, " Assessment Drives Learning. " Therefore in <phrase>order</phrase> to cope with assessments, students adapt different <phrase>learning styles</phrase>, viz., Deep Approach (DA), Surface Apathetic Approach (SAA) and Strategic Approach (SA). OSPE assesses students' <phrase>knowledge</phrase>, different skills and attitude at the same time, therefore, leads the students to read the subject widely and to practice clinical skills extensively. It helps students not to just remember theory but also helps them to critically reflect on their learning course and its outcomes, therefore covering not only the <phrase>cognitive</phrase> but also effective domains. The aim of this <phrase>paper</phrase> is to review the impact of OSPE on students learning i.e. OSPE as a learning tool. <phrase>Literature</phrase> has been reviewed extensively using <phrase>Pubmed</phrase> <phrase>Medline</phrase>, Paknet, Mediscape and Goo-gle <phrase>Socratic</phrase>. Review of <phrase>literature</phrase> has shown OSPE is a valuable learning tool. INTRODUCTION The <phrase>teaching and learning</phrase> of <phrase>medical</phrase> students has always been a complicated process, At times even the best of teachers may struggle in communicating <phrase>knowledge</phrase> and assessing its uptake. Assessment of gained <phrase>knowledge</phrase> is probably more difficult than delivering it. Assessment of clinical skills is far more important and complex as it directly link with patients care. The aim of this <phrase>paper</phrase> is to review the impact of OSPE on students learning i.e. OSPE as a learning tool. <phrase>Literature</phrase> has been reviewed extensively using <phrase>Pubmed</phrase> <phrase>Medline</phrase>, Paknet, Mediscape and <phrase>Google</phrase> <phrase>Socratic</phrase>.
<phrase>Information</phrase> Handling Skills, <phrase>Cognition</phrase> and New Technologies Few would dispute the efficacy of <phrase>multimedia</phrase> <phrase>technology</phrase> and the <phrase>world wide web</phrase> in promoting declarative <phrase>knowledge</phrase> or the acquisition of facts. It is the argument of this <phrase>paper</phrase> however that, apart from assisting a learner in " knowing that " , these technologies are underestimated in their capacity to facilitate intellectual skillsprocedural <phrase>knowledge</phrase> or " knowing how ". Via examples from children employing <phrase>information</phrase> handling skills with a <phrase>CD-ROM</phrase>, this <phrase>paper</phrase> attempts to illustrate the ways in which new technologies support and enhance a <phrase>range</phrase> of skills associated with deep level processing and meaningful learning such as <phrase>metacognition</phrase>, <phrase>problem solving</phrase> and <phrase>critical thinking</phrase>. Introduction As more <phrase>multimedia</phrase> <phrase>technology</phrase> becomes available to schools and users of the <phrase>internet</phrase> continue to grow in number it is clear that such technologies are becoming an intrinsic part of our educational armoury. And yet, as far as their specific educational applications are concerned, such technologies would appear to have only superficial claims as to their capacities for supporting learning and teaching. <phrase>Databases</phrase> of factual material, such as <phrase>encyclopedias</phrase>, are the commonest new learning resources in schools and <phrase>marketing</phrase> ploys are fixated on the power of <phrase>multimedia</phrase> <phrase>encyclopedias</phrase> to provide factual <phrase>knowledge</phrase> in a manner which would meet fully with the Thomas Gradgrind (<phrase>Dickens</phrase>, 1955) <phrase>model</phrase> of learning in which " little vessels " are filled with facts. But is there evidence of anything more significant than this occurring? Have we yet brought " the breadth of imagination " to the use of <phrase>CD-ROM</phrase> (or other new <phrase>information</phrase> technologies) " that the designers brought to the <phrase>design</phrase> of it " (Kenny, 1991)? Are the <phrase>world wide web</phrase> and <phrase>multimedia</phrase> technologies only to feed a reproducing <phrase>model</phrase> of learning? Or have they the potential to facilitate a transforming <phrase>model</phrase> of learning (cf. Dahlgren, 1984)?
Retrieval Term Prediction Using <phrase>Deep Belief</phrase> Networks This <phrase>paper</phrase> presents a method to predict retrieval terms from relevant/surrounding words or descriptive texts in <phrase>Japanese</phrase> by using <phrase>deep belief</phrase> networks (DBN), one of two typical types of <phrase>deep learning</phrase>. To determine the effectiveness of using DBN for this task, we tested it along with baseline methods using example-based approaches and conventional <phrase>machine learning</phrase> methods, i.e., <phrase>multi-layer</phrase> <phrase>perceptron</phrase> (MLP) and <phrase>support vector machines</phrase> (<phrase>SVM</phrase>), for comparison. The <phrase>data</phrase> for training and testing were obtained from the Web in manual and automatic manners. Automatically created pseudo <phrase>data</phrase> was also used. A grid search was adopted for obtaining the optimal hyper-parameters of these <phrase>machine learning</phrase> methods by performing <phrase>cross-validation</phrase> on <phrase>training data</phrase>. <phrase>Experimental</phrase> <phrase>results</phrase> showed that (1) using DBN has far higher prediction precisions than using baseline methods and higher prediction precisions than using either MLP or <phrase>SVM</phrase>; (2) adding automatically gathered <phrase>data</phrase> and pseudo <phrase>data</phrase> to the manually gathered <phrase>data</phrase> as <phrase>training data</phrase> is an effective measure for further improving the prediction precisions; and (3) DBN is able to deal with noisier <phrase>training data</phrase> than MLP, i.e., the prediction precision of DBN can be improved by adding noisy <phrase>training data</phrase>, but that of MLP cannot be.
<phrase>Real-world</phrase> Ubicomp Deployments: <phrase>Lessons Learned</phrase> Why Deployment Is Essential A mong the many branches of <phrase>computer science</phrase>, <phrase>ubiquitous computing</phrase> enjoys an unusually distinguished <phrase>history</phrase> of creating and deploying <phrase>prototypes</phrase>. Why is this? A tempting answer is that many ubicomp researchers and practitioners have backgrounds in subjects such as HCI and systemsareas with a strong focus on learning from deploying working <phrase>prototypes</phrase>. However, a more compelling answer might be that ubicomp, unlike almost every other sub-specialty with the exception of HCI, is about embedding <phrase>computing</phrase> into existing <phrase>human</phrase> systems. You might argue that deployment is not a very cost-effective way of evaluating specifi c system aspects. Jesper Kjeldskov and his colleagues claim that " the added value of <phrase>conducting</phrase> <phrase>usability</phrase> evaluations in the fi eld is very little " and that " recreating central aspects of the use context in a <phrase>laboratory</phrase> setting enables the <phrase>iden</phrase>-tifi cation of the same <phrase>usability</phrase> problem list. " 1 But deployment lets you observe people using the system as part of their <phrase>daily</phrase> lives, making it an important step toward the <phrase>long</phrase>-term goal of developing widely adopted ubicomp <phrase>technology</phrase>. Only through deployment can we learn about unexpected problems that might be critical in real systems. Indeed, many of the best-known ubicomp projects are characterized by their focus on deploy-mentincluding Parctab, 2 Active Badge, 3 and Classroom 2000. 4 Furthermore, some of the most valuable lessons in ubicomp have come not from designing or implementing particular systems but from the experience of trying to move those systems out of the lab and into real settings. In this issue In assembling this issue, we tried to collect important lessons researchers have learned from deployments. Such experience reports face challenges in being accepted to selective conferences, because they often fail to present any <phrase>single</phrase> " deep " contribution. Instead, they tie together lessons that touch on many broad areas. The ethos of <phrase>IEEE</phrase> Pervasive <phrase>Computing</phrase> is to <phrase>act</phrase> as a <phrase>catalyst</phrase> for <phrase>future research</phrase>, so we view it as the ideal forum for presenting this type of work. In that <phrase>spirit</phrase>, we're pleased to bring you four articles that present <phrase>lessons learned</phrase> the hard wayfrom <phrase>large-scale</phrase> deployments of important <phrase>research</phrase> <phrase>prototypes</phrase>. In " Moving Out of the <phrase>Laboratory</phrase>: Deploying Pervasive Technologies in a <phrase>Hospital</phrase>, "
Man-Computer <phrase>Symbiosis</phrase> (J.C.R. Licklider, 1960) I am a <phrase>knowledge</phrase> <phrase>media</phrase> designer. I conceive of novel tools incorporating computational and communications <phrase>technology</phrase> that help people think, learn, create, communicate, and collaborate. The work proceeds best when based on deep understandings of how people work and learn. How did I choose this career? I became inspired to think about interactive <phrase>computing</phrase> by a seminal JCR Licklider (1960) article entitled " Man-Computer <phrase>Symbiosis</phrase> " , an Anthony Oettinger 1965 course at <phrase>Harvard</phrase> entitled " Technological <phrase>Aids</phrase> to <phrase>Human</phrase> Thought " , and good fortune joining in 1966 a group at <phrase>MIT Lincoln Laboratory</phrase> 1 that was the birthplace of the new field of interactive <phrase>computer graphics</phrase>. The central idea was Licklider's (everyone called him Lick) vision of interactive <phrase>computing</phrase> as a synergistic coupling of <phrase>human</phrase> and machine capabilities. In a now famous passage, Lick (1960, p. 4) draws an analogy between the <phrase>symbiotic</phrase> relationship of the fig <phrase>tree</phrase> and the Blastophaga grossorum, the <phrase>insect</phrase> which pollinates it, and man-machine systems: "The hope is that, in not too many years, <phrase>human</phrase> brains and <phrase>computing</phrase> machines will be coupled together very tightly and that the resulting partnership will think as no <phrase>human brain</phrase> has ever thought ..." Noting that the then-current generation of machines fails to facilitate this <phrase>symbiosis</phrase>, and he goes on to postulate requirements for achieving his vision: One of the main aims of man-computer <phrase>symbiosis</phrase> is to bring the <phrase>computing</phrase> machine effectively into the formulative parts of technical problems. To think in interaction with a computer in the same way that you think with a colleague whose competence supplements your own will require much tighter coupling between man and machine than is possible today. He then suggests how <phrase>computers</phrase> could facilitate thinking and <phrase>problem-solving</phrase>, concluding (p. 6): If those problems can be solved in such a way as to create a <phrase>symbiotic</phrase> relation between a man and a fast <phrase>information-retrieval</phrase> and <phrase>data-processing</phrase> machine it seems evident that the <phrase>cooperative</phrase> interaction would greatly improve the thinking process. The remarkably prescient second half of the <phrase>paper</phrase> catalogues problems whose solutions are prerequisites for realizing <phrase>human</phrase>-computer <phrase>symbiosis</phrase>. These includes bridging the speed mismatch between humans and <phrase>computers</phrase> his <phrase>solution</phrase> is <phrase>time-sharing</phrase>, since conceiving of ubiquitous and inexpensive <phrase>personal computers</phrase> in 1960 was too big a stretch even for Lick; <phrase>memory</phrase> hardware improvements by many <phrase>orders of magnitude</phrase>; innovations in the way <phrase>memory</phrase> is organized and 
<phrase>Case-based</phrase> <phrase>Knowledge</phrase> Acquisition, Learning and <phrase>Problem Solving</phrase> for Diagnostic <phrase>Real World</phrase> Tasks Within this <phrase>paper</phrase> we focus on both the <phrase>solution</phrase> of real, complex problems using expert system <phrase>technology</phrase> and the acquisition of the necessary <phrase>knowledge</phrase> from a <phrase>case-based reasoning</phrase> point of view. The development of systems which can be applied to <phrase>real world</phrase> problems has to meet certain requirements. E.g., all available <phrase>information</phrase> sources have to be identified and utilized. Normally, this involves different types of <phrase>knowledge</phrase> for which several <phrase>knowledge representation</phrase> schemes are needed, because no scheme is equally natural for all sources. Facing empirical <phrase>knowledge</phrase> it is important to <phrase>complement</phrase> the use of manually compiled, statistic and otherwise induced <phrase>knowledge</phrase> by the exploitation of the intuitive understandability of <phrase>case-based</phrase> mechanisms. Thus, an integration of <phrase>case-based</phrase> and <phrase>alternative</phrase> <phrase>knowledge</phrase> acquisition and <phrase>problem solving</phrase> mechanisms is necessary. For this, the basis is to define the "role" which <phrase>case-based</phrase> inference can "<phrase>play</phrase>" within a <phrase>knowledge</phrase> acquisition <phrase>workbench</phrase>. We will discuss a <phrase>concrete</phrase> <phrase>case-based</phrase> <phrase>architecture</phrase>, which has been applied to technical diagnosis problems, and its integration into a <phrase>knowledge</phrase> acquisition <phrase>workbench</phrase> which includes compiled <phrase>knowledge</phrase> and explicit deep models, additionally.
Improvements to Training an RNN parser Many parsers learn sparse class <phrase>distributions</phrase> over <phrase>trees</phrase> to <phrase>model</phrase> <phrase>natural language</phrase>. Recursive <phrase>Neural Networks</phrase> (RNN) use much denser representations, yet can still achieve an F-score of 92.06% for right binarized sentences up to 15 words <phrase>long</phrase>. We examine an RNN <phrase>model</phrase> by comparing it with an abstract generative <phrase>probabilistic model</phrase> using a <phrase>Deep Belief</phrase> Network (DBN). The DBN provides both an upwards and downwards pointing conditional <phrase>model</phrase>, <phrase>drawing</phrase> a connection between RNN and Charniak type parsers, while analytically predicting <phrase>average</phrase> scoring parameters in the RNN. In addition, we apply the RNN to longer sentences and develop two methods which, while having negligible effect on <phrase>short</phrase> sentence <phrase>parsing</phrase>, are able to improve the <phrase>parsing</phrase> F-Score by 0.83% on longer sentences.
Comparison of Learned versus Engineered Features for Classification of Mine Like Objects from Raw <phrase>Sonar</phrase> Images Advances in <phrase>high</phrase> <phrase>frequency</phrase> <phrase>sonar</phrase> have provided increasing resolution of sea bottom objects, providing higher fidelity <phrase>sonar</phrase> <phrase>data</phrase> for automated <phrase>target</phrase> recognition tools. Here we investigate if advanced techniques in the field of <phrase>visual object</phrase> recognition and <phrase>machine learning</phrase> can be applied to classify mine-like objects from such <phrase>sonar</phrase> <phrase>data</phrase>. In particular, we investigate if the recently popular <phrase>Scale-Invariant Feature Transform</phrase> (SIFT) can be applied for such <phrase>high</phrase>-resolution <phrase>sonar</phrase> <phrase>data</phrase>. We also follow up our previous approach in applying the <phrase>unsupervised learning</phrase> of <phrase>deep belief</phrase> networks, and advance our methods by applying a convo-lutional <phrase>Restricted Boltzmann Machine</phrase> (cRBM). Finally, we now use <phrase>Support Vector Machine</phrase> (<phrase>SVM</phrase>) classifiers on these <phrase>learned features</phrase> for final classification. We find that the cRBM-<phrase>SVM</phrase> combination slightly outperformed the SIFT features and yielded encouraging performance in comparison to <phrase>state</phrase>-of-the-<phrase>art</phrase>, highly engineered template matching methods.
<phrase>Intelligent Tutoring</phrase> Systems for Commercial <phrase>Games</phrase>: The Virtual Combat Training <phrase>Center</phrase> <phrase>Tutor</phrase> and <phrase>Simulation</phrase> <phrase>Game</phrase> manuals and tutorial scenarios are insufficient for new players to learn <phrase>games</phrase> of deep complexity such as highly realistic tactical simulations of modern battlefields. Adding post-<phrase>game</phrase> after-<phrase>action</phrase> reviews improves the situation, but these typically do not provide guidance during the mission and tend to focus on quantitative <phrase>feedback</phrase>, rather than specifics about what the player did wrong and how to improve. <phrase>Intelligent tutoring</phrase> system (ITS) <phrase>technology</phrase> provides a <phrase>higher level</phrase> of <phrase>interactivity</phrase> and a more specific qualitative analysis to guide players during <phrase>game</phrase> <phrase>play</phrase>. This use of an <phrase>AI</phrase> <phrase>technology</phrase> is demonstrated with the integration of an ITS component with the tactical <phrase>simulation</phrase> Armored Task Force (<phrase>ATF</phrase>) resulting in a combined system called the the Virtual Combat Training <phrase>Center</phrase> (V-CTC). V-CTC simulates the Army's combat training <phrase>center</phrase> at <phrase>Fort Irwin</phrase> and its instructors, called <phrase>observer</phrase> / controllers. The <phrase>ATF</phrase> <phrase>game</phrase> itself was modified to send an event <phrase>stream</phrase> over <phrase>TCP-IP</phrase> sockets to the ITS component, which interprets the events and acts accordingly. V-CTC was originally intended for a <phrase>military</phrase> context: either classroom use, field instruction, or embedded deployment. However, in non-<phrase>military</phrase> <phrase>games</phrase>, tutors (or non-<phrase>player characters</phrase> acting in that role) may well enhance the gaming experience of players. Such players might otherwise become frustrated with learning very challenging <phrase>games</phrase>, or simply fail to appreciate the tactical possibilities and depth of strategy possible in a well-designed <phrase>game</phrase>. Armored Task Force is an example of a <phrase>game</phrase> that might at first appear simple but turns out to be staggeringly complex. Its graphically simple two-dimensional interface has none of the breathtaking graphics that commercial players are used to seeing; indeed, the <phrase>game</phrase> appeals primarily to <phrase>hardcore</phrase> <phrase>war</phrase> gamers who want the most realistic modern <phrase>war</phrase> <phrase>game</phrase> possible today. The bare-<phrase>bones</phrase> interface hides a <phrase>simulation</phrase> of fire support and <phrase>combined-arms</phrase> warfare at the <phrase>battalion</phrase> and <phrase>company</phrase> level for armored task forces. Tactical exchanges are modeled using the same <phrase>pH</phrase> (<phrase>probability</phrase> of hit) and pK (<phrase>probability</phrase> of kill) parameters that the Army's National Training <phrase>Center</phrase> uses. The <phrase>game</phrase> uses the same <phrase>digital</phrase> terrain <phrase>elevation</phrase> contour maps that the <phrase>Army</phrase> uses. It displays <phrase>military</phrase> units and vehicles either with <phrase>NATO</phrase> standard <phrase>icons</phrase> or markers showing vehicle depictions. Actually <phrase>mastering</phrase> the <phrase>game</phrase> can take a <phrase>lifetime</phrase> as it requires <phrase>mastering</phrase> the same tactical skills that the <phrase>Army</phrase> teaches in its field manuals. Consider, for example, that in the tutorial mission a player needs to know the 
An Efficient Learning Procedure for <phrase>Deep Boltzmann Machines</phrase> We present a new <phrase>learning algorithm</phrase> for <phrase>Boltzmann</phrase> machines that contain many layers of hidden variables. <phrase>Data</phrase>-dependent <phrase>statistics</phrase> are estimated using a variational approximation that tends to focus on a <phrase>single</phrase> mode, and <phrase>data</phrase>-<phrase>independent</phrase> <phrase>statistics</phrase> are estimated using persistent <phrase>Markov</phrase> chains. The use of two quite different techniques for estimating the two types of statistic that enter into the <phrase>gradient</phrase> of the <phrase>log likelihood</phrase> makes it practical to learn <phrase>Boltzmann</phrase> machines with multiple <phrase>hidden layers</phrase> and millions of parameters. The learning can be made more efficient by using a <phrase>layer-by-layer</phrase> pretraining phase that initializes the weights sensibly. The pretraining also allows the variational inference to be initialized sensibly with a <phrase>single</phrase> bottom-up <phrase>pass</phrase>. We present <phrase>results</phrase> on the MNIST and NORB <phrase>data</phrase> sets showing that <phrase>deep Boltzmann machines</phrase> learn very good <phrase>generative models</phrase> of <phrase>handwritten digits</phrase> and 3D objects. We also show that the features discovered by <phrase>deep Boltzmann machines</phrase> are a very effective way to initialize the <phrase>hidden layers</phrase> of <phrase>feedforward neural</phrase> nets, which are then discriminatively <phrase>fine-tuned</phrase>.
<phrase>Programming</phrase> Progress Continuing Prospects for an <phrase>Engineering</phrase> Discipline of <phrase>Software</phrase> 25th-anniversary Top Picks update <phrase>Engineering</phrase> fields typically evolve from craft practice of a <phrase>technology</phrase>, sufficient for local or <phrase>ad hoc</phrase> use. When the <phrase>technology</phrase> becomes economically significant, it requires stable <phrase>production</phrase> techniques and <phrase>management</phrase> control. The resulting commercial market is based on experience rather than <phrase>deep understanding</phrase> of the <phrase>technology</phrase>. <phrase>Production</phrase> problems often stimulate a related <phrase>science</phrase>, and an <phrase>engineering</phrase> profession emerges when that <phrase>science</phrase> becomes sufficiently mature to support purposeful practice and <phrase>design</phrase> <phrase>evolution</phrase> with predictable outcomes. Figure 1 shows this <phrase>evolutionary</phrase> path. In " Prospects, " I concluded that <phrase>software</phrase> practice was sometimes craft and sometimes stable commercial practice. Some <phrase>science</phrase> was beginning to emerge, and isolated cases of <phrase>engineering</phrase> practice could be foundbut that wasn't the common case. Now, <phrase>IEEE Software</phrase> has offered me the opportunity to take <phrase>stock</phrase> again, with the field another 20 years older. Where are we now, and what are our prospects? In " Prospects, " I identified three eras in <phrase>software</phrase> <phrase>research</phrase>: <phrase>programming</phrase>-any-which-way (roughly 1960 5 years), in which we wrote small programs without systematic understanding of even <phrase>elementary</phrase> abstractions; <phrase>programming</phrase>-in-the-small (1970 5 years), in which we learned to apply systematic understanding of <phrase>algorithms</phrase>, <phrase>data</phrase> types, and formal reasoning; and <phrase>programming</phrase>-in-the-large (1980 5 years), I n the late 1980s I helped establish <phrase>Carnegie Mellon</phrase> University's <phrase>Software Engineering Institute</phrase>, which dramatically changed my appreciation of the critical problems in <phrase>software development</phrase>. Computer scientists had been talking about <phrase>software engineering</phrase> for two decades or so, 1 and I wanted to better understand what it might mean to have an <phrase>engineering</phrase> discipline for <phrase>software</phrase>. This <phrase>led</phrase> me to look into the <phrase>history</phrase> of <phrase>engineering</phrase>, and the article " Prospects for an <phrase>Engineering</phrase> Discipline of <phrase>Software</phrase> " was one result. 2 In her 1990 <phrase>IEEE Software</phrase> article " Prospects for an <phrase>Engineering</phrase> Discipline of <phrase>Software</phrase> " (Nov./Dec., pp. 1524), Mary Shaw identified the key areas that the <phrase>software development</phrase> profession must address to become a true <phrase>engineering</phrase> discipline. That classic article made the magazine's 25th-anniversary top picks list (Jan./Feb. 2009, pp. 911). Here, Mary reflects on the <phrase>evolution</phrase> of her thinking since the publication of " Prospects. " Hakan Erdogmus, <phrase>Editor in Chief</phrase>
An inch deep and a mile wide: <phrase>Electronic</phrase> tools for savvy administrators <phrase>Texas</phrase> <phrase>school</phrase> administrators often lack vital <phrase>knowledge</phrase> of <phrase>technology</phrase> trends, issues and skills; therefore, they are not as effective leaders of <phrase>technology</phrase> introduction, integration, and <phrase>management</phrase> as are needed. This lack comes from three sources: 1) <phrase>school</phrase> administrator preparatory programs have not and do not provide <phrase>technology</phrase>-related courses; 2) there are few <phrase>technology</phrase>-related in-service training course available to administrators; and 3) many <phrase>Texas</phrase> administrators are geographically isolated and thus have less opportunity to interact with colleagues and less opportunity to receive training. This <phrase>article presents</phrase> and explains <phrase>web-based</phrase> resources for: 1) standards development for administrator, <phrase>teacher</phrase> and <phrase>student</phrase> <phrase>technology</phrase> skills and <phrase>knowledge</phrase>; 2) standards development for accessibility, connectivity, and <phrase>software</phrase>; and 3) national and <phrase>state</phrase> resources such as diagnostic tools, <phrase>school</phrase> <phrase>data</phrase>/<phrase>statistics</phrase>, and other <phrase>technology</phrase>-related <phrase>information</phrase>. Recently, great attention has been given to the progress or lack of progress of <phrase>technology</phrase> in K-12 <phrase>public</phrase> schools. This has included issues such the lack of quality <phrase>software</phrase>, the <phrase>high</phrase> cost of small quantities of equipment, the lack of planning for implementation, and the lack of <phrase>teacher</phrase> and administrator training. Recently, <phrase>U.S</phrase>. <phrase>education</phrase> has even been featured on the national <phrase>news</phrase> show, <phrase>Nightline</phrase>, as spending billions and billions of dollars to put various educational technologies to work in schools, with little to show for the effort (<phrase>ABC</phrase>, 1998). With all of the tremendous <phrase>investment</phrase> in <phrase>technology</phrase> on the part of the <phrase>U.S</phrase>. <phrase>federal government</phrase>, states, and local <phrase>school districts</phrase>, it is quite remarkable that schools are not farther along in their implementation of the <phrase>technology</phrase>. What might be significant contributing factors? Inquiries into the slowness of full-scale <phrase>technology</phrase> <phrase>adoption</phrase> and integration cite the lack of <phrase>school</phrase> administrators' <phrase>knowledge</phrase> about advanced technologies. A 1998 study by the SouthEast and Islands Regional <phrase>Technology</phrase> in <phrase>Education</phrase> <phrase>Consortium</phrase> (SEIR*TEC) identified administrative <phrase>leadership</phrase> as the <phrase>single</phrase> most important factor affecting schools' successful integration of <phrase>technology</phrase> (Bryan, 1998). <phrase>School</phrase> leaders are not prepared to guide <phrase>technology</phrase> initiatives because they lack <phrase>knowledge</phrase> of <phrase>technology</phrase>. For an administrator who was born and completed formal training before the educational technological <phrase>revolution</phrase> started, few opportunities to acquire a <phrase>knowledge base</phrase> in the <phrase>leadership</phrase> of <phrase>technology</phrase> initiatives are available. More importantly, there are few opportunities to learn how to provide <phrase>leadership</phrase> in the <phrase>strategic management</phrase> and use of these new technologies in a dynamic, non-threatening environment. Elevating the need to a crisis point, it is anticipated that 80 percent of our current <phrase>school</phrase> administrators 
Logical Validation, Answer Merging and Witness Selection - A Study in Multi-<phrase>Stream</phrase> <phrase>Question Answering</phrase> The <phrase>paper</phrase> presents an approach to multi-<phrase>stream</phrase> <phrase>question answering</phrase> (QA) using deep <phrase>semantic</phrase> <phrase>parsing</phrase> and logical validation for filtering answer candidates. A robust entailment check is accomplished by embedding the prover in a relaxation loop. Fallback strategies ensure a graceful degradation of performance in the case of <phrase>parsing</phrase> problems. The logical validity score is complemented by false-positive <phrase>tests</phrase> and <phrase>heuristic</phrase> quality indicators which also affect the selection of the most trusted answers. Separate criteria are used for choosing a suitable 'witness', i.e. a text passage which substantiates the answer. We present two experiments in which the method is applied for merging the <phrase>results</phrase> of various <phrase>state</phrase>-of-the-<phrase>art</phrase> QA systems. The evaluation demonstrates that the approach is applicable to heterogeneous QA streams in particular it improves <phrase>results</phrase> for a combination of precision-oriented and recall-oriented answer streams. The method automatically adapts to these characteristics of a QA system by learning parameters from a training sample.
<phrase>Polyphonic Music</phrase> Transcription a <phrase>Deep Learning</phrase> Approach Course Porject Cs365 <phrase>Artificial Intelligence</phrase> Guide -<phrase>amitabha</phrase> Mukerjee In <phrase>polyphonic music</phrase>, many notes are played at once. Transcribing notes from the <phrase>polyphonic music</phrase> can help in plagirism detection, <phrase>artist</phrase> identification , <phrase>Genre</phrase> Classification, Composition Assitance and <phrase>Music</phrase> <phrase>Tutoring Systems</phrase>. Since, many notes are played at once, therefore, the techniques of <phrase>multi class</phrase> classification are not applicable here. In this project, we have learned 88 <phrase>binary</phrase> classifier which helps in transcribing notes of <phrase>polyphonic music</phrase>. Each classifier detects the presence of one note in the <phrase>music</phrase> at every time step. Un-supervised <phrase>feature learning</phrase> using RNN-RBM (Recursive <phrase>Neural Networks</phrase> and <phrase>Restricted Boltzmann Machine</phrase>). <phrase>SVM</phrase> classifiers are build using one-vs-all classification. HMM smoothing has been done to improve the <phrase>results</phrase>.
A <phrase>Simulation</phrase> <phrase>Game</phrase> for Teaching Secure <phrase>Data</phrase> Communications Protocols With the widespread commercial use of the <phrase>Internet</phrase>, secure <phrase>data</phrase> communications over the <phrase>Internet</phrase> has become an important <phrase>aspect</phrase> of <phrase>business</phrase> operations. Thus, it is an important study for <phrase>information technology</phrase> and <phrase>management</phrase> students. The <phrase>Security</phrase> Protocol <phrase>Game</phrase> is an interactive group activity for exploring secure <phrase>data</phrase> <phrase>communication</phrase> protocols. Using pen and <phrase>paper</phrase>, envelopes and <phrase>game</phrase> tokens, students simulate <phrase>security</phrase> protocols and possible attacks against them. The <phrase>game</phrase> provides simple and intuitive representations for <phrase>cryptographic</phrase> methods, including both <phrase>public</phrase> key and secret key techniques. Using these representations, students can simulate <phrase>Internet</phrase> application protocols such as Pretty Good <phrase>Privacy</phrase> (used to secure <phrase>email</phrase>) and <phrase>Transport Layer Security</phrase> (used for secure web transactions). They can explore well-known protocols for <phrase>authentication</phrase>, key exchange and blind signatures. Students can also develop and <phrase>test</phrase> their own protocols using <phrase>public</phrase> key certificates, encrypted key transmission, tunnelling and other well-known techniques. Through this learning activity, students gain a <phrase>deep understanding</phrase> of how <phrase>security</phrase> protocols operate and are designed. The <phrase>game</phrase> has been used in <phrase>tertiary</phrase> units of study for managers and <phrase>information technology</phrase> students.
<phrase>Convolutional Neural Network</phrase> for <phrase>Computer Vision</phrase> and <phrase>Natural Language Processing</phrase> <phrase>Machine learning</phrase> techniques are widely used in the domain of <phrase>Natural Language Processing</phrase> (<phrase>NLP</phrase>) and <phrase>Computer Vision</phrase> (CV), In <phrase>order</phrase> to capture complex and non-linear features deeper <phrase>machine learning</phrase> architectures become more and more popular. A lot of the <phrase>state</phrase> of <phrase>art</phrase> performance have been reported by employing <phrase>deep learning</phrase> techniques. <phrase>Convolutional Neural Network</phrase> (<phrase>CNN</phrase>) is one variant of <phrase>deep learning</phrase> architectures which has received intense attention in recent years. <phrase>CNN</phrase> is inspired from the domain of <phrase>biology</phrase>, which tries to mimic the way of how signal are processed in <phrase>human brain</phrase>. <phrase>CNN</phrase> is type of <phrase>feed forward</phrase> <phrase>artificial neural network</phrase> which are constructed by <phrase>multiple layers</phrase>. Signals are passed through these layers with non-linear <phrase>activation functions</phrase>. Within each layer, there are a lot of <phrase>independent</phrase> node to process the signal in different regions or aspects. <phrase>CNN</phrase> has achieved <phrase>great success</phrase> in sentence modeling, <phrase>image recognition</phrase> and feature detection. In this <phrase>paper</phrase>, we introduce the <phrase>motivation</phrase>, intuition, architectures and <phrase>algorithm</phrase> of <phrase>CNN</phrase>. In particular, we discuss several recent achievements of <phrase>CNN</phrase> in <phrase>NLP</phrase> and CV.
A Biologically Inspired <phrase>Neural Network</phrase> for Autonomous Underwater Vehicles Autonomous underwater vehicles (AUVs) have great advantages for activities in deep oceans, and are expected as the attractive tool for near future underwater development or investigation. However, AUVs have various problems which should be solved for <phrase>motion control</phrase>, acquisition of sensors' <phrase>information</phrase>, behavioral decision, <phrase>navigation</phrase> without collision, self-localization and so on. This <phrase>paper</phrase> proposes an adaptive biologically inspired neural controller for trajectory tracking of AUVs in nonstationary environment. The <phrase>kinematic</phrase> adaptive neuro-controller is an unsupervised <phrase>neural network</phrase>, which is termed <phrase>Self-Organization</phrase> Direction Mapping Network (SODMN). The network uses an associative learning system to generate transformations between spatial coordinates and coordinates of <phrase>propellers</phrase>' <phrase>velocity</phrase>. The <phrase>neurobiological</phrase> inspired control <phrase>architecture</phrase> requires no <phrase>knowledge</phrase> of the <phrase>geometry</phrase> of the <phrase>robot</phrase> or of the quality, number, or configuration of the robot's sensors. The SODMN proposed in this <phrase>paper</phrase> represents a simplified way to understand in part the mechanisms that allow the <phrase>brain</phrase> to collect sensory input to control adaptive behaviours of autonomous <phrase>navigation</phrase> of the animals. The efficiency of the proposed <phrase>neurobiological</phrase> inspired controller for autonomous intelligent <phrase>navigation</phrase> was implemented on an underwater vehicle capable of operating during large periods of time for observation and monitoring tasks.
<phrase>Ieee</phrase> <phrase>Intelligent Systems</phrase> <phrase>Knowledge Representation and Reasoning</phrase> Ai's Greatest Trends and Controversies Except perhaps for the <phrase>AI</phrase> naysayers, <phrase>AI</phrase> practitioners are creatorsof <phrase>software</phrase> artifacts, their underlying <phrase>algorithms</phrase>, and their underlying theories. We begin our feature with Herbert Simon, one of our three contributors (together with John McCarthy and Oliver Selfridge) who are our links to the landmark <phrase>Dartmouth</phrase> conference in 1956, where modern <phrase>AI</phrase> is often said to have begun. Simon paints a broad picture of <phrase>AI</phrase> as a discipline constantly pursuing computational creations that challenge the uniqueness of biologically grounded <phrase>intelligence</phrase>. <phrase>AI</phrase> has been thought controversial because it challenged the uniqueness of <phrase>human</phrase> thought, as <phrase>Darwin</phrase> challenged the uniqueness of <phrase>human</phrase> origins. The boundaries of <phrase>AI</phrase> continue to expand rapidly, settling the controversy for those who know the evidence. <phrase>AI</phrase> first demonstrated that important intellectual tasks could be accomplished by selective <phrase>heuristic</phrase> search, often in a thoroughly <phrase>human</phrase> way. <phrase>GPS</phrase> is one product of that line of <phrase>research</phrase>. Then <phrase>AI</phrase> explored the role of large bodies of <phrase>knowledge</phrase> in expert thinking. Dendral was an early important success, as was the extensive <phrase>research</phrase> on <phrase>human</phrase> <phrase>chess</phrase> expertise, modeled with such programs as Chrest (not <phrase>Deep Blue</phrase>, which is only partly <phrase>humanoid</phrase>). A third successful line has been the <phrase>research</phrase> on learning for, example, Siklossy's ZBIE program, which learned <phrase>natural language</phrase> by comparing sentences with pictures. Finally, there has been the great recent advance in <phrase>robotics</phrase>, based on progress in simulating sensory and motor functions. The <phrase>basic</phrase> strategy of <phrase>AI</phrase> has always been to seek out progressively more complex <phrase>human</phrase> tasks and show how <phrase>computers</phrase> can do them, in <phrase>humanoid</phrase> ways or by brute force. With a half-century of steady progress, we have assembled a <phrase>solid body</phrase> of tested theory on the processes of <phrase>human</phrase> thinking and the ways to simulate and supplement them. In what <phrase>media</phrase> do <phrase>AI</phrase> practitioners create? The answer to this question is a depiction of <phrase>AI</phrase> itself, so it is not too surprising that most of our contributions address this question. <phrase>Wolfgang</phrase> Bibel, a proponent of the formalist agenda in <phrase>AI</phrase>, argues for the need for sophisticated <phrase>logic</phrase> formalisms and inferential methods for <phrase>AI</phrase>. Alan Bundy adds to these arguments, discussing further the advances achieved by those taking the <phrase>formal-logic</phrase> approach to <phrase>AI</phrase>, especially in <phrase>light</phrase> of the critiques raised by those on the other side of the <phrase>AI</phrase> fence. Among the controversies in <phrase>AI</phrase>, none is as persisting as the one about logic's role in <phrase>AI</phrase>. It 
Real-time Lookahead Control Policies <phrase>Decision-making</phrase> in practical domains is usually complex, as a coordinated <phrase>sequence</phrase> of actions is needed to reach a satisfactory <phrase>state</phrase>, and responsive, as no fixed <phrase>sequence</phrase> works for all cases instead we need to select actions after sensing the environment. At each step, a lookahead control policy chooses among feasible actions by envisioning their effects into the future and selecting the <phrase>action</phrase> leading to the most promising <phrase>state</phrase>. There are several challenges to producing the appropriate policy. First, when each individual <phrase>state</phrase> description is large, the policy may instead use a <phrase>low-dimensional</phrase> abstraction of the states. Second, in some situations the quality of the final <phrase>state</phrase> is not given, but can only be learned from <phrase>data</phrase>. Deeper lookahead typically selects actions that <phrase>lead</phrase> to higher-quality outcomes. Of course, as deep forecasts are computationally expensive, it is problematic when computational time is a factor. This <phrase>paper</phrase> makes this accu-racy/efficiency tradeoff explicit, defining a system's effectiveness in terms of both the quality of the returned response, and the computational cost. We then investigate how deeply a system should search, to optimize this " type <phrase>II</phrase> " performance criterion. 1 Real-time <phrase>Decision-Making</phrase> We will start with a motivating practical example. Having detailed inventories of <phrase>forest</phrase> resources is of tremendous importance to <phrase>forest</phrase> industries, governments, and researchers. It would aid planning <phrase>wood</phrase> <phrase>logging</phrase> (planting and cutting), <phrase>surveying</phrase> against illegal activities, and <phrase>ecosystem</phrase> and <phrase>wildlife</phrase> <phrase>research</phrase>. Given the dynamic <phrase>nature</phrase> of <phrase>forest</phrase> <phrase>evolution</phrase>, the task of <phrase>forest</phrase> mapping is a continuous undertaking, with the objective of re-mapping the estimated 344 million <phrase>hectares</phrase> of <phrase>Canadian</phrase> <phrase>forests</phrase> on <phrase>a 10</phrase>-20 year cycle. <phrase>Remote-sensing</phrase> based approaches appear to be the only feasible <phrase>solution</phrase> to inventorizing the estimated 10 no robust <phrase>forest</phrase> mapping system exists to date. We have been developing such a <phrase>forest</phrase> inventory mapping system (FIMS) using a set of sophisticated <phrase>computer vision</phrase> operators. FIMS <phrase>problem-solving</phrase> <phrase>state</phrase> is represented using a layered shared <phrase>data structure</phrase> as shown in Figure 1. Initially, aerial images and <phrase>LIDAR</phrase> <phrase>data</phrase> are deposited at the sensory input layer. As the <phrase>computer vision</phrase> operators are applied, higher layers become populated with extracted <phrase>data</phrase>. Finally, the top layer gains the derived 3D interpretation of the <phrase>forest</phrase> scene. At this point the scene can be rendered back onto the sensory layer to be compared with the original imagery to assess the quality of the interpretation. In nearly every <phrase>problem-solving</phrase> <phrase>state</phrase> several <phrase>computer vision</phrase> operators 
<phrase>Semi-supervised</phrase> learning of compact document representations with deep networks Finding good representations of text documents is crucial in <phrase>information retrieval</phrase> and classification systems. Today the most popular document representation is based on a <phrase>vector</phrase> of word counts in the document. This representation neither captures dependencies between related words, nor handles synonyms or polysemous words. In this <phrase>paper</phrase>, we propose an <phrase>algorithm</phrase> to learn text document representations based on <phrase>semi-supervised</phrase> autoencoders that are stacked to form a deep network. The <phrase>model</phrase> can be trained efficiently on partially labeled corpora, producing very compact representations of documents, while retaining as much class <phrase>information</phrase> and joint word <phrase>statistics</phrase> as possible. We show that it is advantageous to exploit even a few <phrase>labeled samples</phrase> during training.
<phrase>Highway</phrase> Networks There is plenty of theoretical and <phrase>empirical evidence</phrase> that depth of <phrase>neural networks</phrase> is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an <phrase>open problem</phrase>. In this extended abstract, we introduce a new <phrase>architecture</phrase> designed to ease <phrase>gradient</phrase>-based training of very deep networks. We refer to networks with this <phrase>architecture</phrase> as <phrase>highway</phrase> networks, since they allow unimpeded <phrase>information</phrase> flow across several layers on <phrase>information</phrase> highways. The <phrase>architecture</phrase> is characterized by the use of gating units which learn to regulate the flow of <phrase>information</phrase> through a network. <phrase>Highway</phrase> networks with hundreds of layers can be trained directly using <phrase>stochastic gradient descent</phrase> and with a <phrase>variety</phrase> of <phrase>activation functions</phrase>, opening up the possibility of studying extremely deep and efficient architectures.
Prediction of Financial <phrase>Time Series</phrase> with <phrase>Hidden Markov Models</phrase> Prediction of Financial <phrase>Time Series</phrase> with <phrase>Hidden Markov Models</phrase> In this <phrase>thesis</phrase>, we develop an extension of the <phrase>Hidden Markov Model</phrase> (HMM) that addresses two of the most important challenges of financial <phrase>time series</phrase> modeling: non-stationary and non-linearity. Specifically, we extend the HMM to include a novel exponentially weighted Expectation-Maximization (EM) <phrase>algorithm</phrase> to handle these two challenges. We show that this extension allows the HMM <phrase>algorithm</phrase> to <phrase>model</phrase> not only <phrase>sequence</phrase> <phrase>data</phrase> but also dynamic financial <phrase>time series</phrase>. We show the update rules for the HMM parameters can be written in a form of exponential moving averages of the <phrase>model</phrase> variables so that we can take the advantage of existing <phrase>technical analysis</phrase> techniques. We further propose a double weighted EM <phrase>algorithm</phrase> that is able to adjust training sensitivity automatically. Convergence <phrase>results</phrase> for the proposed <phrase>algorithms</phrase> are proved using techniques from the EM Theorem. <phrase>Experimental</phrase> <phrase>results</phrase> show that our models consistently beat the <phrase>S&P 500 Index</phrase> over five 400-day testing periods from 1994 to 2002, including both bull and <phrase>bear</phrase> markets. Our models also consistently outperform the top 5 <phrase>S&P 500</phrase> <phrase>mutual funds</phrase> in terms of the <phrase>Sharpe Ratio</phrase>. iii To my mom iv Acknowledgments I would like to <phrase>express my deep</phrase> gratitude to my senior supervisor, Dr. Anoop Sarkar for his great <phrase>patience</phrase>, detailed instructions and insightful comments at every stage of this <phrase>thesis</phrase>. What I have learned from him goes beyond the <phrase>knowledge</phrase> he taught me; his attitude toward <phrase>research</phrase>, carefulness and commitment has and will have a profound impact on my future <phrase>academic</phrase> activities. I am also indebted to my co-senior supervisor, Dr. Andrey <phrase>Pavlov</phrase>, for trusting me before I realize my <phrase>results</phrase>. I still remember the day when I first presented to him the " super five day pattern " program. He is the source of my <phrase>knowledge</phrase> in <phrase>finance</phrase> that I used in this <phrase>thesis</phrase>. Without the countless time he spent with me, this <phrase>thesis</phrase> surely would not have been impossible. I cannot thank enough my supervisor, Dr. Oliver Schulte. Part of this <phrase>thesis</phrase> derives from a course project I did for his <phrase>machine learning</phrase> course in which he introduced me the world of <phrase>machine learning</phrase> and interdisciplinary <phrase>research</phrase> approaches. Many of my <phrase>fellow</phrase> graduate students offered great help during my study and <phrase>research</phrase> at the <phrase>computing science</phrase> <phrase>school</phrase>. I thank <phrase>Wei</phrase> Luo for helping me with many issues regarding the L A T <phrase>E</phrase> Xand <phrase>Unix</phrase>; Cheng Lu and Chris Demwell for sharing their 
Comprendre Le Web Cach Understanding the <phrase>Hidden Web</phrase> Comprendre Le Web Cach Understanding the <phrase>Hidden Web</phrase> Le Web cach (galement appel Web profond ou Web invisible), c'est--dire la partie du Web qui n'est pas directement accessible par des hyperliens, mais travers des formulaires <phrase>HTML</phrase> ou des services Web, est d'une grande valeur, mais difficile exploiter. Nous prsentons un processus pour la dcouverte, l'analyse syntaxique et smantique, et l'interrogation des services du Web cach, le tout de manire entirement automatique. Nous proposons une <phrase>architecture</phrase> gnrale se basant sur un entrept semi-structur de contenu imprcis (probabiliste). Nous fournissons une analyse dtaille de la complexit du modle d'arbre probabiliste sous-jacent. Nous dcrivons comment une combinaison d'heuristiques et de sondages du Web peut tre utilise pour comprendre la structure d'un formulaire <phrase>HTML</phrase>. Nous prsentons une utilisation originale des champs alatoires conditionnels (une mthode d'apprentissage supervis) de manire non supervise, sur une annotation automatique, imparfaite et imprcise, base sur la connaissance du domaine, afin d'extraire l'information pertinente de pages de rsultat <phrase>HTML</phrase>. Afin d'obtenir des relations smantiques entre entres et sorties d'un service du Web cach, nous tudions la complexit de l'obtention d'une correspondance de schmas partir d'instances de bases de donnes, en se basant uniquement sur la prsence des constantes dans ces deux instances. Nous dcrivons enfin un modle de reprsentation smantique et d'indexation en comprhension de sources du Web cach, et dbattons de la manire de traiter des requtes de haut niveau l'aide de telles descriptions. Abstract <phrase>e</phrase> <phrase>hidden Web</phrase> (also known as deep or invisible Web), that is, the part of the Web not directly accessible through <phrase>hyperlinks</phrase>, but through <phrase>HTML</phrase> forms or <phrase>Web services</phrase>, is of great value, but difficult to exploit. We discuss a process for the fully automatic discovery, <phrase>syntactic</phrase> and <phrase>semantic</phrase> analysis, and querying of <phrase>hidden-Web</phrase> services. We propose first a <phrase>general</phrase> <phrase>architecture</phrase> that relies on a semi-structured <phrase>warehouse</phrase> of imprecise (probabilistic) content. We provide a detailed complexity analysis of the underlying probabilistic <phrase>tree</phrase> <phrase>model</phrase>. We describe how we can use a combination of heuristics and probing to understand the structure of an <phrase>HTML</phrase> form. We present an original use of a supervised <phrase>machine-learning</phrase> method, namely <phrase>conditional random fields</phrase>, in an unsupervised manner, on an automatic, imperfect, and imprecise, annotation based on <phrase>domain knowledge</phrase>, in <phrase>order</phrase> to extract relevant <phrase>information</phrase> from <phrase>HTML</phrase> result pages. So as to obtain <phrase>semantic</phrase> relations between inputs and outputs of a <phrase>hidden-Web</phrase> service, we investigate the complexity of deriving a schema mapping between <phrase>database</phrase> instances, 
Metrics for Multivariate Dictionaries Overcomplete representations and <phrase>dictionary</phrase> <phrase>learning algorithms</phrase> kept attracting a growing interest in the <phrase>machine learning</phrase> <phrase>community</phrase>. This <phrase>paper</phrase> addresses the emerging problem of comparing multivariate overcomplete representations. Despite a recurrent need to rely on a distance for learning or assessing multivariate overcomplete representations, no metrics in their underlying spaces have yet been proposed. Henceforth we propose to study overcomplete representations from the perspective of frame theory and matrix <phrase>manifolds</phrase>. We consider distances between multivariate dictionaries as distances between their spans which reveal to be elements of a <phrase>Grassmannian</phrase> <phrase>manifold</phrase>. We introduce Wasserstein-like set-metrics defined on <phrase>Grassmannian</phrase> spaces and study their properties both theoretically and numerically. Indeed a deep <phrase>experimental</phrase> study based on tailored synthetic datasetsand real <phrase>EEG</phrase> signals for <phrase>Brain</phrase>-Computer Interfaces (BCI) have been conducted. In particular, the introduced metrics have been embedded in clustering <phrase>algorithm</phrase> and applied to BCI Competition IV-2a for dataset quality assessment. Besides, a principled connection is made between three close but still disjoint <phrase>research</phrase> fields, namely, <phrase>Grassmannian</phrase> packing, <phrase>dictionary</phrase> learning and <phrase>compressed sensing</phrase>.
Identity Matters in <phrase>Deep Learning</phrase> An emerging <phrase>design</phrase> principle in <phrase>deep learning</phrase> is that each layer of a deep <phrase>artificial neural network</phrase> should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks. In this work, we put the principle of identity parameterization on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local <phrase>optima</phrase>. The same result for linear <phrase>feed-forward</phrase> networks in their standard param-eterization is substantially more delicate. Second, we show that residual networks with ReLu activations have <phrase>universal</phrase> finite-sample expressivity in the sense that the network can represent any <phrase>function</phrase> of its sample provided that the <phrase>model</phrase> has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual <phrase>architecture</phrase> consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our <phrase>model</phrase> improves significantly on previous all-<phrase>convolutional networks</phrase> on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.
<phrase>Text Mining</phrase> for Translational <phrase>Bioinformatics</phrase> <phrase>Copyright</phrase> 2015 Hong-Jie Dai et al. This is an <phrase>open access</phrase> article distributed under the <phrase>Creative Commons</phrase> Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Translational <phrase>bioinformatics</phrase> is an emerging field with a fascinating aim to develop novel computational techniques to facilitate traditional <phrase>translational research</phrase> through the convergence of molecular <phrase>bioinformatics</phrase>, <phrase>biostatistics</phrase>, statistical <phrase>genetics</phrase>, and clinical informatics. Translational bioin-formatics is now more powerful than ever and has become a clinch between biological findings and clinical informat-<phrase>ics</phrase>. The computational techniques contribute by integrating multidimensional <phrase>data</phrase> consisting of medications, diseases, and <phrase>genomes</phrase> with clinical and pathological features. They are applied in various aspects with the hope of uncovering therapeutic targets and <phrase>biomarkers</phrase> of patient response. However, the accumulation of rich <phrase>data</phrase> from past studies, advancement of new <phrase>experimental</phrase> techniques, and ease of access to publications nowadays result in enormous repositories of scientific literatures and biomedical <phrase>data</phrase>, hindering the <phrase>translation</phrase> of molecular understandings into technologies that could impact patients. <phrase>Text mining</phrase> is an established field, but its application for translational <phrase>bioinformatics</phrase> is still a novel <phrase>research</phrase> direction with enormous <phrase>research</phrase> potential. The present issue emphasizes the application of <phrase>text mining</phrase> on biomedical/clinical publications and <phrase>knowledge</phrase> bases to facilitate the discovery and <phrase>management</phrase> of translational <phrase>medical</phrase> <phrase>research</phrase> <phrase>knowledge</phrase>. Rapid growth of <phrase>disease</phrase> related biomedical <phrase>literature</phrase> makes the traditional <phrase>information retrieval</phrase> techniques insufficient to fulfill searchers' <phrase>information</phrase> needs. In the <phrase>paper</phrase> " <phrase>Disease</phrase> Related <phrase>Knowledge</phrase> Summarization Based on Deep <phrase>Graph</phrase> Search, " X. Wu et al. developed an approach which is able to automatically retrieve <phrase>disease</phrase> related <phrase>knowledge</phrase> in a summarized form from the large volume of online biomedical <phrase>literature</phrase>. This approach is capable of finding both direct relations between diseases and <phrase>genes</phrase> as well as indirect obscure relationships among diseases and other biomedical entities. Their experiment <phrase>results</phrase> show that a precision of 0.6 and a recall of 0.61 can be achieved on extracting <phrase>bladder cancer</phrase>-related <phrase>genetic</phrase> entities compared to a reference standard recorded in the Online <phrase>Mendelian Inheritance</phrase> in Man (OMIM) and <phrase>Genetics</phrase> Home Reference <phrase>database</phrase>. The large amount of biomedical <phrase>literature</phrase> provides useful <phrase>knowledge</phrase> resource for researchers to form biomedical hypotheses. In their work entitled " <phrase>Supervised Learning</phrase> Based <phrase>Hypothesis</phrase> Generation from Biomedical <phrase>Literature</phrase>, " S. Sang et al. proposed a <phrase>supervised learning</phrase>-<phrase>based approach</phrase> to generate hypotheses from biomedical <phrase>literature</phrase>. This approach splits the traditional processing of <phrase>hypothesis</phrase> generation <phrase>model</phrase> into 
Compressing <phrase>Convolutional Neural Networks</phrase> <phrase>Convolutional neural networks</phrase> (<phrase>CNN</phrase>) are increasingly used in many areas of <phrase>computer vision</phrase>. They are particularly attractive because of their ability to " absorb " great quantities of <phrase>labeled data</phrase> through millions of parameters. However, as <phrase>model</phrase> sizes increase, so do the storage and <phrase>memory</phrase> requirements of the classi-<phrase>fiers</phrase>. We present a novel <phrase>network architecture</phrase>, <phrase>Frequency</phrase>-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and <phrase>fully-connected</phrase> layers of a <phrase>deep learning</phrase> <phrase>model</phrase>, leading to dramatic savings in <phrase>memory</phrase> and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-<phrase>frequency</phrase>, we first convert filter weights to the <phrase>frequency domain</phrase> with a <phrase>discrete cosine transform</phrase> (<phrase>DCT</phrase>) and use a <phrase>low-cost</phrase> <phrase>hash function</phrase> to randomly group <phrase>frequency</phrase> parameters into hash buckets. All parameters assigned the same hash bucket share a <phrase>single</phrase> value learned with standard back-propagation. To further reduce <phrase>model</phrase> size we allocate fewer hash buckets to <phrase>high</phrase>-<phrase>frequency</phrase> components, which are generally less important. We evaluate FreshNets on eight <phrase>data</phrase> sets, and show that it leads to drastically better compressed performance than several relevant baselines.
A lateral contribution <phrase>learning algorithm</phrase> for multi MLP <phrase>architecture</phrase> <phrase>gradient</phrase> is large and , for close to is close to. On the other hand and if lc is sufficiently sharp this has no far <phrase>range</phrase> influence, this " cooperation principle " is only local. And the snowball effect spreads this influence from one NN to its neighbors. Of course, we can use existing optimization of the <phrase>backpropagation</phrase> <phrase>learning algorithm</phrase> [13] to increase the quickness and find the edge of a deep well of l in one. When this edge is found we can use an <phrase>algorithm</phrase>, such as BFGS [9][3], to go rapidly to this <phrase>local minima</phrase> and then go back to the previous optimized <phrase>LCL</phrase> <phrase>algorithm</phrase> to search an other edge of a deep well of l. We can also improve it by an optimization of lc(.) kernel, by defining one kernel per connection. The <phrase>heuristic</phrase> of this idea is that each feature extracted by one weight in could be the same for an other. So, each component of this new kernel (matrix indexed by ij) can be rewriten as: where is the lateral contribution rate for the connection ij with regard to and. This rate can be on-line adapted with regard to the comparison of and the direction for each pair at each step, or off-line adapted with regard to the <phrase>symmetries</phrase> of the <phrase>function</phrase> F(.). This off-line adaptation can be included in the term of <phrase>distortion</phrase> , allowing the VQ <phrase>algorithm</phrase> to move two classes closer than if the term of <phrase>distortion</phrase> should only take the input space <phrase>topology</phrase> into account. The main improvements of <phrase>LCL</phrase> <phrase>algorithm</phrase> are not only the quickness of convergence of learning processing but also a new insight of the <phrase>synaptic</phrase> weight, showing that the behavior of a <phrase>neural network</phrase> in context is directly linked to the behavior of every NNs in close to. This link is due to the strong constraint that must be continuous. It is now natural to try to merge these NNs into one with <phrase>synaptic</phrase> weights estimated through the behavior of context. Accordingly, we have developed an <phrase>architecture</phrase> for weight estimation named OWE (Orthogonal Weight <phrase>Estimator</phrase>) and tested it on control problems[5]. The ongoing studies correspond to testing the <phrase>LCL</phrase> <phrase>algorithm</phrase> for more and more complex input space, and defining generalized models to embed LCLA and the on-line <phrase>Vector Quantization</phrase> [6]. W t () 0 W t 0 () 0 NN 
<phrase>Linguistic</phrase> correlates of style: authorship classification with <phrase>deep linguistic</phrase> analysis features The identification of authorship <phrase>falls</phrase> into the category of style classification, an interesting sub-field of text categorization that deals with properties of the form of <phrase>linguistic</phrase> expression as opposed to the content of a text. Various feature sets and classification methods have been proposed in the <phrase>literature</phrase>, geared towards abstracting away from the content of a text, and focusing on its stylistic properties. We demonstrate that in a realistically difficult authorship attribution scenario, <phrase>deep linguistic</phrase> analysis features such as <phrase>context free</phrase> <phrase>production</phrase> <phrase>frequencies</phrase> and <phrase>semantic</phrase> relationship <phrase>frequencies</phrase> achieve significant error reduction over more commonly used " shallow " features such as <phrase>function word</phrase> <phrase>frequencies</phrase> and part of speech trigrams. Modern <phrase>machine learning</phrase> techniques like <phrase>support vector machines</phrase> allow us to explore large feature vectors, combining these different feature sets to achieve <phrase>high</phrase> <phrase>classification accuracy</phrase> in style-based tasks.
Understanding the difficulty of training deep <phrase>feedforward neural</phrase> networks Whereas before 2006 it appears that deep <phrase>multi-layer</phrase> <phrase>neural networks</phrase> were not successfully trained, since then several <phrase>algorithms</phrase> have been shown to successfully <phrase>train</phrase> them, with <phrase>experimental</phrase> <phrase>results</phrase> showing the superiority of deeper vs less <phrase>deep architectures</phrase>. All these <phrase>experimental</phrase> <phrase>results</phrase> were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard <phrase>gradient descent</phrase> from random initialization is doing so poorly with <phrase>deep neural networks</phrase>, to better understand these recent relative successes and help <phrase>design</phrase> better <phrase>algorithms</phrase> in the future. We first observe the influence of the non-linear activations functions. We find that the logistic <phrase>sigmoid</phrase> activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top <phrase>hidden layer</phrase> into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training <phrase>neural networks</phrase>. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training , with the idea that training may be more difficult when the singular values of the <phrase>Jacobian</phrase> associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence. <phrase>Deep learning</phrase> methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of <phrase>lower</phrase> <phrase>level features</phrase>. They include learning methods for a wide array of <phrase>deep architectures</phrase>, including <phrase>neural networks</phrase> with many <phrase>hidden layers</phrase> (Vin-cent et al., 2008) and <phrase>graphical</phrase> models with many levels of hidden variables (Hinton et al., 2006), among others (Zhu et al., 2009; Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical <phrase>appeal</phrase>, inspiration from <phrase>biology</phrase> and <phrase>human</phrase> <phrase>cognition</phrase>, and because of empirical success in vision (Ranzato et al. Theoretical <phrase>results</phrase> reviewed and discussed by Ben-gio (2009), suggest that in <phrase>order</phrase> to learn the kind of complicated functions that can represent <phrase>high</phrase>-level abstractions (e.g. in vision, <phrase>language</phrase>, and other <phrase>AI</phrase>-level tasks), one may need <phrase>deep architectures</phrase>. Most of the recent <phrase>experimental</phrase> <phrase>results</phrase> with <phrase>deep architecture</phrase> are obtained with models that can be turned into deep supervised <phrase>neural networks</phrase>, but with initialization or training schemes different from the <phrase>classical</phrase> <phrase>feedforward neural</phrase> networks (Rumelhart et al., 1986). Why are these 
Abalearn: Efficient Self-<phrase>play</phrase> Learning of the <phrase>Game</phrase> <phrase>Abalone</phrase> This <phrase>paper</phrase> presents Abalearn, a self-teaching <phrase>Abalone</phrase> program capable of automatically reaching an intermediate level of <phrase>play</phrase> without needing expert-<phrase>labeled training</phrase> examples or deep searches. Our approach is based on a <phrase>reinforcement learning</phrase> <phrase>algorithm</phrase> that is <phrase>risk</phrase>-seeking, since defensive players in <phrase>Abalone</phrase> tend to never end a <phrase>game</phrase>. We extend the <phrase>risk</phrase>-sensitive <phrase>reinforcement learning</phrase> framework in <phrase>order</phrase> to deal with large <phrase>state</phrase> spaces and we also propose a set of features that seem relevant for achieving a good level of <phrase>play</phrase>. We evaluate our approach using a fixed <phrase>heuristic</phrase> opponent as a benchmark , pitting our agents against <phrase>human</phrase> players online and comparing samples of our agents at different times of training.
<phrase>Reinforcement Learning</phrase> with Unsupervised <phrase>Auxiliary</phrase> Tasks <phrase>Deep reinforcement learning</phrase> agents have achieved <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> by directly maximising cumulative reward. However, environments contain a much wider <phrase>variety</phrase> of possible training signals. In this <phrase>paper</phrase>, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by <phrase>reinforcement learning</phrase>. All of these tasks share a common representation that, like <phrase>unsupervised learning</phrase>, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon ex-trinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent <phrase>significantly outperforms</phrase> the previous <phrase>state</phrase>-of-the-<phrase>art</phrase> on <phrase>Atari</phrase>, averaging 880% expert <phrase>human</phrase> performance, and a challenging suite of first-person, three-dimensional <phrase>Labyrinth</phrase> tasks leading to a mean speedup in learning of 10 and averaging 87% expert <phrase>human</phrase> performance on <phrase>Labyrinth</phrase>. Natural and <phrase>artificial</phrase> agents <phrase>live</phrase> in a <phrase>stream</phrase> of sensorimotor <phrase>data</phrase>. At each time step t, the agent receives observations o t and executes actions a t. These actions influence the future course of the sensorimotor <phrase>stream</phrase>. In this <phrase>paper</phrase> we develop agents that learn to predict and control this <phrase>stream</phrase>, by solving a host of <phrase>reinforcement learning</phrase> problems, each focusing on a distinct feature of the sensorimotor <phrase>stream</phrase>. Our <phrase>hypothesis</phrase> is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards. The classic <phrase>reinforcement learning</phrase> <phrase>paradigm</phrase> focuses on the maximisation of extrinsic reward. However , in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor <phrase>stream</phrase> contains an abundance of other possible learning targets. Traditionally, <phrase>unsupervised learning</phrase> attempts to reconstruct these targets, such as the <phrase>pixels</phrase> in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor <phrase>stream</phrase>, by treating them as pseudo-rewards for <phrase>reinforcement learning</phrase>. Intuitively, this set of tasks is more closely matched with the agent's <phrase>long</phrase>-term goals, potentially leading to more useful representations. Consider a baby that learns to maximise the cumulative amount of <phrase>red</phrase> that it observes. To correctly predict the optimal value, the baby must understand how to increase " redness " by various means, including manipulation (bringing a <phrase>red</phrase> object closer to 
<phrase>Digital</phrase> <phrase>Design</phrase> with KP-Lab KP-Lab is an <phrase>EU</phrase> Integrated Project envisioning a learning system that facilitates innovative practices of sharing, creating and working with <phrase>knowledge</phrase> in <phrase>education</phrase> and workplaces. The project exploits a novel pedagogical view, the <phrase>knowledge</phrase>-creation <phrase>metaphor</phrase> of learning. According to such " trialogical " approach, <phrase>cognition</phrase> arises through collaborative work in systematically developing shared " <phrase>knowledge</phrase> artefacts " , such as concepts, plans ,material <phrase>products</phrase>, or social practices. The <phrase>paper</phrase> presents the plan of a <phrase>pilot</phrase> course to <phrase>test</phrase> the KP-Lab methodologies and tools in the field of <phrase>Digital</phrase> <phrase>Design</phrase>. 1 <phrase>Engineering</phrase> <phrase>education</phrase> today In spite of the recent hype on new educational technologies and their potential for a novel approach on learning and acquiring <phrase>knowledge</phrase>, <phrase>university</phrase> <phrase>education</phrase> has remained the same that it has been since the first <phrase>universities</phrase> have been established, many centuries ago. The transmission <phrase>model</phrase> is still dominant and now, as in the Middle Age, students of <phrase>higher education</phrase> take many years in acquisition-oriented and <phrase>teacher</phrase>-centered studies. Technical <phrase>education</phrase> at a <phrase>university</phrase> level has a different <phrase>history</phrase>, since the first <phrase>engineering</phrase> schools in <phrase>Europe</phrase> were established only during the nineteen century. For example, the origins of the <phrase>University</phrase> of <phrase>Genoa</phrase> go back to the thirteen century, while its first technical <phrase>school</phrase>, the " <phrase>Royal Naval School</phrase> " has been established only in 1870. Before the <phrase>diffusion</phrase> of formal <phrase>engineering</phrase> studies, the practitioners got their training outside a classroom, usually by joining a <phrase>community</phrase> of people already engaged in the exercise of the profession, acquiring and sharing <phrase>knowledge</phrase>, <phrase>information</phrase>, processes and practices. A natural and <phrase>deep learning</phrase> resulted from the deep interaction among people themselves and among people and the objects of their practice. The accelerated rate of technological <phrase>innovation</phrase> brought about by the <phrase>industrial revolution</phrase> showed the limitations of this ageless mode of learning and prompted the birth of formal technical <phrase>education</phrase>. An <phrase>engineering</phrase> <phrase>school</phrase> is, in fact, more effective in transmitting <phrase>knowledge</phrase> at a fast pace and in providing the stronger theoretical background necessary for the new technologies. An unwanted <phrase>casualty</phrase> of this process has been, unfortunately, the <phrase>active learning</phrase> mode typical of the former mode. <phrase>Engineering</phrase> curricula are able to adjust their contents to support the new technologies, but are still based largely on frontal lectures, assembled into courses that provide fragmented pieces of <phrase>knowledge</phrase> and skills. There is hardly any explicit continuation from one course to another. In conclusion, today's <phrase>engineering</phrase> curricula embody in 
<phrase>Knowledge Transfer</phrase> <phrase>Pre-training</phrase> <phrase>Pre-training</phrase> is crucial for learning <phrase>deep neural networks</phrase>. Most of existing <phrase>pre-training</phrase> methods <phrase>train</phrase> simple models (e.g., <phrase>restricted Boltzmann machines</phrase>) and then stack them <phrase>layer by layer</phrase> to form the deep structure. This <phrase>layer-wise</phrase> <phrase>pre-training</phrase> has found strong theoretical foundation and broad empirical support. However, it is not easy to employ such method to pre-<phrase>train</phrase> models without a clear <phrase>multi-layer</phrase> structure, e.g., <phrase>recurrent neural networks</phrase> (RNNs). This <phrase>paper</phrase> presents a new <phrase>pre-training</phrase> approach based on <phrase>knowledge transfer</phrase> learning. In contrast to the <phrase>layer-wise</phrase> approach which trains <phrase>model</phrase> components incrementally, the new approach trains the entire <phrase>model</phrase> as a whole but with an easier <phrase>objective function</phrase>. This is achieved by utilizing soft targets <phrase>produced</phrase> by a prior trained <phrase>model</phrase> (<phrase>teacher</phrase> <phrase>model</phrase>). Compared to the conventional <phrase>layer-wise</phrase> methods, this new method does not care about the <phrase>model</phrase> structure, so can be used to pre-<phrase>train</phrase> very complex models. Experiments on a <phrase>speech recognition</phrase> task demonstrated that with this approach, complex RNNs can be well trained with a weaker <phrase>deep neural network</phrase> (DNN) <phrase>model</phrase>. Furthermore, the new method can be combined with conventional <phrase>layer-wise</phrase> <phrase>pre-training</phrase> to deliver additional gains.
<phrase>Emotion</phrase> <phrase>technology</phrase>, wearables, and surprises Could we help people have healthier lives and better experiences if <phrase>computers</phrase> could measure and help communicate our <phrase>emotion</phrase>? <phrase>Years ago</phrase>, my students at <phrase>MIT</phrase> and I began to <phrase>design</phrase>, build, and <phrase>test</phrase> both wearable and other sensors for recognizing <phrase>emotion</phrase>. We designed studies, gathered <phrase>data</phrase>, and developed <phrase>signal processing</phrase> and <phrase>machine learning</phrase> techniques to see what could be reliably extracted. In this <phrase>talk</phrase> I will highlight several of the most surprising findings during this <phrase>adventure</phrase>. These include new insights about the "true smile of happiness," discovering that regular cameras (and your <phrase>smartphone</phrase>, even in your <phrase>handbag</phrase>) can compute some of your biosignals, finding electrical signals on the wrist that give insight into deep <phrase>brain</phrase> activity, and learning surprising implications of wearable sensing for <phrase>autism</phrase>, <phrase>anxiety</phrase>, <phrase>depression</phrase>, <phrase>sleep</phrase>-<phrase>memory consolidation</phrase>, <phrase>epilepsy</phrase>, and more.
Neuro-Symbolic <phrase>EDA</phrase>-Based Optimization Using ILP-Enhanced DBNs We investigate solving discrete <phrase>optimization problems</phrase> using the 'estimation of distribution' (<phrase>EDA</phrase>) approach via a novel combination of <phrase>deep belief</phrase> networks (DBN) and <phrase>inductive logic programming</phrase> (ILP). While DBNs are used to learn the structure of successively 'better' feasible solutions, ILP enables the incorporation of domain-based <phrase>background knowledge</phrase> related to the goodness of solutions. Recent work showed that ILP could be an effective way to use <phrase>domain knowledge</phrase> in an <phrase>EDA</phrase> scenario. However, in a purely ILP-based <phrase>EDA</phrase>, sampling successive populations is either inefficient or not straightforward. In our Neuro-symbolic <phrase>EDA</phrase>, an ILP <phrase>engine</phrase> is used to construct a <phrase>model</phrase> for good solutions using domain-based <phrase>background knowledge</phrase>. These rules are introduced as <phrase>Boolean</phrase> features in the last <phrase>hidden layer</phrase> of DBNs used for <phrase>EDA</phrase>-based optimization. This incorporation of logical ILP features requires some changes while training and sampling from DBNs: (a) our DBNs need to be trained with <phrase>data</phrase> for units at the input layer as well as some units in an otherwise <phrase>hidden layer</phrase>; and (b) we would like the samples generated to be drawn from instances entailed by the logical <phrase>model</phrase>. We demonstrate the viability of our approach on instances of two <phrase>optimization problems</phrase>: predicting optimal depth-of-win for the <phrase>KRK</phrase> endgame, and job-shop scheduling. Our <phrase>results</phrase> are promising: (i) On each iteration of distribution estimation, samples obtained with an ILP-assisted DBN have a substantially greater proportion of good solutions than samples generated using a DBN without ILP features; and (<phrase>ii</phrase>) On termination of distribution estimation, samples obtained using an ILP-assisted DBN contain more near-optimal samples than samples from a DBN without ILP features. Taken together, these <phrase>results</phrase> suggest that the use of ILP-constructed theories could be useful for incorporating complex <phrase>domain-knowledge</phrase> into deep models for estimation of distribution based procedures.
<phrase>Natural Language</phrase> for <phrase>Human</phrase> <phrase>Robot</phrase> Interaction 2. Embodied <phrase>Construction Grammar</phrase> 3. System <phrase>Architecture</phrase> <phrase>Natural Language Understanding</phrase> (NLU) was one of the main original goals of <phrase>artificial intelligence</phrase> and <phrase>cognitive science</phrase>. This has <phrase>proven</phrase> to be extremely challenging and was nearly abandoned for decades. We describe an implemented system that supports full NLU for tasks of moderate complexity. The <phrase>natural language</phrase> interface is based on Embodied <phrase>Construction Grammar</phrase> and <phrase>simulation</phrase> <phrase>semantics</phrase>. The system described here supports <phrase>human</phrase> dialog with an agent controlling a simulated <phrase>robot</phrase>, but is flexible with respect to both input <phrase>language</phrase> and output task. Interfaces <phrase>Natural language</phrase>. <phrase>Natural language</phrase> interfaces have <phrase>long</phrase> been a topic of HRI <phrase>research</phrase>. Winograd's 1971 <phrase>SHRDLU</phrase> was a landmark program that allowed a user to command a simulated arm and to ask about the <phrase>state</phrase> of the block world (Winograd, 1971).There is currently intense interest in both the promise and potential dangers of much more capable <phrase>robots</phrase>. 1) Much more computation 2) <phrase>NLP</phrase> <phrase>technology</phrase> 3) <phrase>Construction Grammar</phrase>: form-meaning pairs 4) 10) <phrase>General</phrase> NLU front end: Modest effort to link to a new <phrase>Action</phrase> side As shown in Table 1, we believe that there have been sufficient scientific and technical advances to now make NLU of moderate scale an achievable goal. The first two points are obvious and <phrase>general</phrase>. All of the others except for point 8 are discussed in this <phrase>paper</phrase>. The CPRM mechanisms were not needed in the current system, but are essential for more complex actions and <phrase>simulation</phrase> (Barrett 2010). This work is based on the Embodied <phrase>Construction Grammar</phrase> (<phrase>ECG</phrase>), and builds on decades of work on the Neural Theory of <phrase>Language</phrase> (<phrase>NTL</phrase>) project. The meaning side of an <phrase>ECG</phrase> <phrase>construction</phrase> is a schema based on embodied <phrase>cognitive linguistics</phrase>. (Feldman, <phrase>Dodge</phrase>, and Bryant 2009). <phrase>ECG</phrase> is designed to support the following functions: 1) A formalism for capturing the shared <phrase>grammar</phrase> and beliefs of a <phrase>language</phrase> <phrase>community</phrase>. 2) A precise notation for technical <phrase>linguistic</phrase> work 3) An implemented specification for <phrase>grammar</phrase> testing 4) A front end for applications involving deep <phrase>semantics</phrase> 5) A <phrase>high</phrase> level description for neural and behavioral experiments. 6) A basis for theories and models of <phrase>language</phrase> learning. In this work, we focus on point 4; we are using <phrase>ECG</phrase> for the <phrase>natural language</phrase> interface to a <phrase>robot</phrase> simulator. We suggest that NLU can now be the foundation for HRI with the current generation of <phrase>robots</phrase> of limited complexity. Any foreseeable <phrase>robot</phrase> will have limited capabilities and will not be able 
<phrase>Software</phrase> <phrase>security</phrase>: is <phrase>ok</phrase> good enough? Widely publicized breaches regularly occur involving insecure <phrase>software</phrase>. This is due to the fact that the vast majority of <phrase>software</phrase> in use today was not designed to withstand attacks encountered when deployed on hostile networks such as the <phrase>Internet</phrase>. What limited vulnerability <phrase>statistics</phrase> that exist confirm that most modern <phrase>software</phrase> includes coding flaws and <phrase>design</phrase> errors that put sensitive customer <phrase>data</phrase> at <phrase>risk</phrase>. Unfortunately, <phrase>security</phrase> officers and <phrase>software</phrase> project owners still struggle to justify <phrase>investment</phrase> to build secure <phrase>software</phrase>. Initial efforts to build justification models have not been embraced beyond the most <phrase>security</phrase> conscious organizations. Concepts like the "Rugged <phrase>Software</phrase>" are gaining traction, but have yet to make a deep impact. How does an <phrase>organization</phrase> - <phrase>short</phrase> of a breach - justify expending critical resources to build more secure <phrase>software</phrase>? Is it realistic to believe that an <phrase>industry</phrase>-driven <phrase>solution</phrase> such as the <phrase>Payment Card</phrase> Industry's <phrase>Data Security</phrase> Standard (<phrase>PCI-DSS</phrase>) can drive secure <phrase>software</phrase> <phrase>investment</phrase> before headlines prompt <phrase>government</phrase> to demand top-down regulation to "fix" the <phrase>security</phrase> of <phrase>software</phrase>? This presentation will attempt to characterize the current <phrase>landscape</phrase> of <phrase>software</phrase> <phrase>security</phrase> from the perspective of a practitioner who regularly works with <phrase>Fortune 500</phrase> chief <phrase>security</phrase> officers to build <phrase>business</phrase> cases for <phrase>software</phrase> <phrase>security</phrase> initiatives. Given the current status of <phrase>software</phrase> <phrase>security</phrase> efforts, and the struggles for <phrase>business</phrase> justification, <phrase>industry</phrase> would be well-served to look further afield to other competing models to identify future justification efforts. There is still much that can be learned from models outside the <phrase>security</phrase> and <phrase>information technology</phrase> fields. For example, the <phrase>history</phrase> of <phrase>food safety</phrase> provides lessons that the <phrase>software</phrase> <phrase>security</phrase> <phrase>industry</phrase> can draw from when developing justification models. We can also learn from <phrase>building code</phrase> <phrase>adoption</phrase> by <phrase>earthquake</phrase>-prone communities and draw comparisons to communities that have less rigorous building codes. Finally, we can learn much from certain financial regulations that have or have not improved confidence in our financial system.
Collaboration, distribution and <phrase>culture</phrase> - challenges for <phrase>communication</phrase> Work, to an increasing amount, is based on collaboration between different partners; collaboration emphasizes the importance of <phrase>communication</phrase> between the collaborating <phrase>parties</phrase>. Increasingly, work is also becoming distributed and carried out in different geographical locations; distribution underlines the importance of managing and organizing work. The third important <phrase>aspect</phrase> characterizing current work is <phrase>globalization</phrase>; this refers to the multicultural characteristics of work and the need to understand the behavioral patterns of different (national) cultures. The <phrase>paper</phrase> addresses these three challenges related to the current work context. The approach points out the importance of a <phrase>deep understanding</phrase> of the characteristics of work collaboration, distribution, and <phrase>cultural diversity</phrase>. Adaptive learning provides one potential <phrase>solution</phrase> to the challenges.
Recurrent Gaussian Processes We define Recurrent Gaussian Processes (RGP) models, a <phrase>general</phrase> <phrase>family</phrase> of <phrase>Bayesian</phrase> nonparametric models with recurrent GP priors which are able to learn dynamical patterns from sequential <phrase>data</phrase>. Similar to <phrase>Recurrent Neural Networks</phrase> (RNNs), RGPs can have different formulations for their internal states, distinct inference methods and be extended with deep structures. In such context, we propose a novel deep RGP <phrase>model</phrase> whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. To fully exploit the <phrase>Bayesian</phrase> <phrase>nature</phrase> of the RGP <phrase>model</phrase> we develop the Recurrent Variational Bayes (REVARB) framework, which enables efficient inference and strong reg-ularization through coherent propagation of uncertainty across the RGP layers and states. We also introduce a RGP extension where variational parameters are greatly reduced by being reparametrized through RNN-based sequential recognition models. We apply our <phrase>model</phrase> to the tasks of nonlinear system identification and <phrase>human</phrase> motion modeling. The promising obtained <phrase>results</phrase> indicate that our RGP <phrase>model</phrase> maintains its highly flexibility while being able to avoid <phrase>overfitting</phrase> and being applicable even when larger datasets are not available.
A Smart Vision System-on-a-chip <phrase>Design</phrase> Based on Programmable Neural Processor Integrated with <phrase>Active Pixel Sensor</phrase> <phrase>Low Power</phrase> Smart <phrase>Machine Vision</phrase> System A <phrase>low power</phrase> smart vision system based on a large format (currently 1KxlK) <phrase>active pixel sensor</phrase> (APS) integrated with a programmable neural processor for fast vision applications is presented. The concept of building a <phrase>low power</phrase> smart vision system is demonstrated by a system <phrase>design</phrase>, which is composed with an APS <phrase>sensor</phrase>, a smart image window handler, and a neural processor. The <phrase>paper</phrase> also shows that it is feasible to put the whole smart vision system into a <phrase>single</phrase> MCM chip in a standard <phrase>CMOS</phrase> <phrase>technology</phrase>. This smart vision system on-a-chip can take the combined advantages of the <phrase>optics</phrase> and <phrase>electronics</phrase> to achieve ultra-<phrase>high</phrase>-speed smart sensory <phrase>information processing</phrase> and analysis at the focal plane. The proposed system will enable many applications including <phrase>robotics</phrase> and <phrase>machine vision</phrase>, guidance and <phrase>navigation</phrase>, automotive applications, and <phrase>consumer electronics</phrase>. Future applications will also include scientific sensors such as those suitable for highly integrated imaging systems used in <phrase>NASA</phrase> deep space and planetary <phrase>spacecraft</phrase>. Figure 1 shows a system diagram of the proposed smart vision system. The functional blocks include: (a) an <phrase>active pixel sensor</phrase>, (b) a smart image window handler, (c) a programmable neural processor, and (d) a host interface and timing control <phrase>card</phrase>. The APS is used as the optical sensing array in the system. The smart window handler manipulates the APS image <phrase>data</phrase> and provides the windowed image for the neural processor. The neural processor is programmed to perform various vision tasks in <phrase>high</phrase> speed due to its massively <phrase>parallel computing</phrase> structures and learning capabilities. The host computer through host interface and timing control <phrase>card</phrase> controls the APS <phrase>sensor</phrase>, the smart image window handler, and the programmable neural processor. The output image or <phrase>vision science</phrase> <phrase>data</phrase> will be displayed by the host computer. It is feasible to build the proposed smart vision system in a <phrase>single</phrase> <phrase>CMOS</phrase> chip. This smart vision system on-a-chip can take the combined advantages of the <phrase>optics</phrase> and <phrase>electronics</phrase> to achieve <phrase>low-power</phrase> <phrase>high</phrase>-speed smart sensory hformation processing and analysis at the focal plane. Tbe proposed system will enable many applications including <phrase>robotics</phrase> and <phrase>machine vision</phrase>, guidance and <phrase>navigation</phrase>, automotive applications, and <phrase>consumer electronics</phrase>. Future applications will also include scientific sensors such as those suitable for highly integrated imaging systems used in <phrase>NASA</phrase> deep space and planetary <phrase>spacecraft</phrase>. The following sections describe technical details of each <phrase>building block</phrase> of the proposed smart vision system and also show the feasibility to 
<phrase>Large Scale</phrase> Distributed Deep Networks Recent work in <phrase>unsupervised feature learning</phrase> and <phrase>deep learning</phrase> has shown that being able to <phrase>train</phrase> large models can dramatically <phrase>improve performance</phrase>. In this <phrase>paper</phrase>, we consider the problem of training a deep network with billions of parameters using tens of thousands of <phrase>CPU</phrase> cores. We have developed a <phrase>software framework</phrase> called DistBelief that can utilize <phrase>computing</phrase> clusters with thousands of machines to <phrase>train</phrase> large models. Within this framework, we have developed two <phrase>algorithms</phrase> for <phrase>large-scale</phrase> distributed training: (i) Downpour SGD, an asynchronous <phrase>stochastic gradient descent</phrase> procedure supporting a large number of <phrase>model</phrase> replicas, and (<phrase>ii</phrase>) Sandblaster, a framework that supports a <phrase>variety</phrase> of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to <phrase>train</phrase> a deep network 30x larger than previously reported in the <phrase>literature</phrase>, and achieves <phrase>state</phrase>-of-the-<phrase>art</phrase> performance on ImageNet, a <phrase>visual object</phrase> recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly-sized deep network for a commercial <phrase>speech recognition</phrase> service. Although we focus on and <phrase>report</phrase> performance of these methods as applied to training large <phrase>neural networks</phrase>, the underlying <phrase>algorithms</phrase> are applicable to any <phrase>gradient</phrase>-based <phrase>machine learning</phrase> <phrase>algorithm</phrase>.
Good Vibrations: Cross-<phrase>frequency</phrase> Coupling in the <phrase>Human</phrase> <phrase>Nucleus Accumbens</phrase> during Reward Processing The <phrase>nucleus accumbens</phrase> is critical for reward-guided learning and <phrase>decision-making</phrase>. It is thought to "gate" the flow of a diverse <phrase>range</phrase> of <phrase>information</phrase> (e.g., rewarding, aversive, and novel events) from <phrase>limbic</phrase> afferents to <phrase>basal ganglia</phrase> outputs. Gating and <phrase>information</phrase> encoding may be achieved via cross-<phrase>frequency</phrase> coupling, in which bursts of <phrase>high</phrase>-<phrase>frequency</phrase> activity occur preferentially during specific phases of slower oscillations. We examined whether the <phrase>human</phrase> <phrase>nucleus accumbens</phrase> engages such a mechanism by <phrase>recording</phrase> <phrase>electrophysiological</phrase> activity directly from the accumbens of <phrase>human</phrase> patients undergoing <phrase>deep brain stimulation</phrase> <phrase>surgery</phrase>. Oscillatory activity in the gamma (40-80 <phrase>Hz</phrase>) <phrase>frequency</phrase> <phrase>range</phrase> was synchronized with the phase of simultaneous alpha (8-12 <phrase>Hz</phrase>) waves. Further, losing and winning small amounts of <phrase>money</phrase> elicited relatively increased gamma <phrase>oscillation</phrase> power prior to and following alpha troughs, respectively. Gamma-alpha synchronization may reflect an <phrase>electrophysiological</phrase> gating mechanism in the <phrase>human</phrase> <phrase>nucleus accumbens</phrase>, and the phase differences in gamma-alpha coupling may reflect a reward <phrase>information</phrase> coding scheme similar to phase coding.
An <phrase>Energy</phrase>-Aware On-Demand <phrase>Routing Protocol</phrase> for <phrase>Ad-Hoc</phrase> <phrase>Wireless Networks</phrase> Acknowledgments With deep sense of gratitude, I would like to thank my adviser, Prof. Abhay Karandikar for his timely input and guidance during the entire course of the project. My sincere thanks to my co-adviser Dr. It is a <phrase>great learning</phrase> experience to work under his guidance. The regular inputs and encouraging feedbacks have always helped me in doing <phrase>research</phrase> in the right direction. I am very thankful to Hemant Rath for his continuous interaction in the lab during the entire project. I am also thankful to, Punit Rathod for his guidance in <phrase>scripting</phrase> and Ashutosh <phrase>Gore</phrase> for his valuable inputs and guidance in <phrase>report</phrase> writing. I would like to thank all my <phrase>fellow</phrase> labmates for their cooperation in creating a vibrant environment in the lab to work. I would like to take this as an opportunity to thank <phrase>Defence Research and Development Organisation</phrase> for giving me an opportunity to persue my Master's at <phrase>Indian</phrase> Institute of <phrase>Technology</phrase>, <phrase>Bombay</phrase> I thank my parents for their continuous support and encouragement during my entire Master's course. Abstract An <phrase>ad-hoc</phrase> <phrase>wireless network</phrase> is a collection of nodes that come together to dynamically create a network, with no fixed <phrase>infrastructure</phrase> or centralized administration. An <phrase>ad-hoc</phrase> network is characterized by <phrase>energy</phrase> constrained nodes, bandwidth constrained links and dynamic <phrase>topology</phrase>. With the growing use of <phrase>wireless networks</phrase> (including <phrase>ad-hoc</phrase> networks) for real-time applications, such as voice, <phrase>video</phrase>, and real-time <phrase>data</phrase>, the need for Quality of Service (<phrase>QoS</phrase>) guarantees in terms of delay, bandwidth, and <phrase>packet loss</phrase> is becoming increasingly important. Providing <phrase>QoS</phrase> in <phrase>ad-hoc</phrase> networks is a <phrase>challenging task</phrase> because of dynamic <phrase>nature</phrase> of <phrase>network topology</phrase> and imprecise <phrase>state</phrase> <phrase>information</phrase>. Hence, it is important to have a dynamic <phrase>routing protocol</phrase> with fast rerouting capability, which also provides stable route during the <phrase>lifetime</phrase> of the flows. In this <phrase>thesis</phrase>, we have proposed a novel, <phrase>energy</phrase> aware, stable <phrase>routing protocol</phrase> named, Stability-based <phrase>QoS</phrase>-capable <phrase>Ad-hoc</phrase> On-demand Distance <phrase>Vector</phrase> (SQ-AODV), which is an enhancement of the well-known <phrase>Ad-hoc</phrase> On-demand Distance <phrase>Vector</phrase> (AODV) <phrase>routing protocol</phrase> for <phrase>ad-hoc</phrase> <phrase>wireless networks</phrase>. SQ-AODV utilizes a cross-layer <phrase>design</phrase> approach in which <phrase>information</phrase> about the residual <phrase>energy</phrase> of a node is used for route selection and maintenance. An important feature of SQ-AODV protocol is that it uses only local <phrase>information</phrase> and requires no additional <phrase>communication</phrase> or cooperation between the network nodes. SQ-AODV possesses a make-before-break rerouting capability that enables near-zero packet drops and is compatible with 
Tracking <phrase>Human</phrase>-like Natural Motion Using Deep <phrase>Recurrent Neural Networks</phrase> <phrase>Kinect</phrase> <phrase>skeleton</phrase> tracker is able to achieve considerable <phrase>human</phrase> body tracking performance in convenient and a <phrase>low-cost</phrase> manner. However, The tracker often captures unnatural <phrase>human</phrase> poses such as discontinuous and vibrated motions when self-occlusions occur. A majority of approaches tackle this problem by using multiple <phrase>Kinect</phrase> sensors in a workspace. Combination of the measurements from different sensors is then conducted in <phrase>Kalman filter</phrase> framework or <phrase>optimization problem</phrase> is formulated for <phrase>sensor fusion</phrase>. However, these methods usually require heuristics to measure reliability of measurements observed from each <phrase>Kinect</phrase> <phrase>sensor</phrase>. In this <phrase>paper</phrase>, we developed a method to improve <phrase>Kinect</phrase> <phrase>skeleton</phrase> using <phrase>single</phrase> <phrase>Kinect</phrase> <phrase>sensor</phrase>, in which <phrase>supervised learning</phrase> technique was employed to correct unnatural tracking motions. Specifically, deep <phrase>recurrent neural networks</phrase> were used for improving joint positions and velocities of <phrase>Kinect</phrase> <phrase>skeleton</phrase>, and three methods were proposed to integrate the refined positions and velocities for further enhancement. Moreover, we suggested a novel measure to evaluate naturalness of captured motions. We evaluated the <phrase>proposed approach</phrase> by comparison with the <phrase>ground truth</phrase> obtained using a commercial optical maker-based <phrase>motion capture</phrase> system.
Deep Variational Inference Without <phrase>Pixel</phrase>-Wise <phrase>Reconstruction</phrase> Variational autoencoders (VAEs), that are built upon <phrase>deep neural networks</phrase> have emerged as popular <phrase>generative models</phrase> in <phrase>computer vision</phrase>. Most of the work towards improving variational autoencoders has focused mainly on making the approximations to the posterior flexible and accurate , leading to tremendous progress. However, there have been limited efforts to replace <phrase>pixel</phrase>-wise <phrase>reconstruction</phrase> , which have known shortcomings. In this work, we use <phrase>real-valued</phrase> non-volume preserving transformations (real NVP) to exactly compute the conditional likelihood of the <phrase>data</phrase> given the latent distribution. We show that a simple VAE with this form of <phrase>reconstruction</phrase> is competitive with complicated VAE structures, on image modeling tasks. As part of our <phrase>model</phrase>, we develop powerful conditional coupling layers that enable real NVP to learn with fewer intermediate layers.
<phrase>Artificial Intelligence</phrase>: Realizing the Ultimate Promises of <phrase>Computing</phrase> rtificial <phrase>intelligence</phrase> (<phrase>AI</phrase>) is the key <phrase>technology</phrase> in many of today's novel applications , ranging from <phrase>banking</phrase> systems that detect attempted <phrase>credit card fraud</phrase>, to <phrase>telephone</phrase> systems that understand speech, to <phrase>software</phrase> systems that notice when you're having problems and offer appropriate advice. These technologies would not exist today without the sustained <phrase>federal</phrase> support of fundamental <phrase>AI</phrase> <phrase>research</phrase> over the past three decades. Although there are some fairly pure applications of AIsuch as <phrase>industrial</phrase> <phrase>robots</phrase>, or the INTELLIPATH (available at zelda.thomson.com/ chaphall/medelec.html) <phrase>pathology</phrase> diagnosis system recently approved by the <phrase>American Medical Association</phrase> and deployed in hundreds of <phrase>hospitals</phrase> worldwidefor the most part, <phrase>AI</phrase> does not produce stand-alone systems, but instead adds <phrase>knowledge</phrase> and reasoning to existing applications, <phrase>databases</phrase>, and environments, to make them friendlier, smarter, and more sensitive to user behavior and changes in their environments. The <phrase>AI</phrase> portion of an application (e.g., a logical inference or learning module) is generally a large system, dependent on a substantial <phrase>infrastructure</phrase>. <phrase>Industrial</phrase> <phrase>R&D</phrase>, with its relatively <phrase>short</phrase> time-horizons, could not have justified work of the type and scale that has been required to build the foundation for the <phrase>civilian</phrase> and <phrase>military</phrase> successes that <phrase>AI</phrase> enjoys today. And beyond the myriad of currently deployed applications, ongoing efforts that draw upon these decades of federally-sponsored fundamental <phrase>research</phrase> point towards even more impressive future capabilities: Autonomous vehicles: A <phrase>DARPA</phrase>-funded onboard computer system (www.cs.cmu.edu/ ~pomerlea/nhaa.html) from <phrase>Carnegie Mellon University</phrase> drove a van all but 52 of the 2849 miles from <phrase>Washington</phrase>, <phrase>DC</phrase> to <phrase>San Diego</phrase>, averaging 63 miles per hour day and night, <phrase>rain</phrase> or shine. Computer <phrase>chess</phrase>: <phrase>Deep Blue</phrase> (www. research.ibm.com/<phrase>research</phrase>/systems.html#<phrase>chess</phrase>, a <phrase>chess</phrase> computer built by <phrase>IBM</phrase> researchers, defeated world champion Gary <phrase>Kasparov</phrase> in a landmark performance. <phrase>Mathematical</phrase> <phrase>theorem proving</phrase>: A computer system at <phrase>Argonne</phrase> National Laboratories (www.mcs.anl.gov/home/mccune/ar/ rob-bins/) proved a <phrase>long</phrase>-standing <phrase>mathematical</phrase> conjecture about <phrase>algebra</phrase> using a method that would be considered creative if done by humans. Scientific classification: A <phrase>NASA</phrase> system learned to classify very faint signals as either <phrase>stars</phrase> or <phrase>galaxies</phrase> with superhuman accuracy, by studying examples classified by experts. Advanced <phrase>user interfaces</phrase>: <phrase>PEGASUS</phrase> (sls-www.lcs.mit.edu/PEGASUS.html) is a <phrase>spoken language</phrase> interface connected to the <phrase>American Airlines</phrase> EAASY <phrase>SABRE</phrase> reservation system, which allows subscribers to obtain flight <phrase>information</phrase> and make flight reservations via a large, on-line, dynamic <phrase>database</phrase>, accessed through their personal computer over the <phrase>telephone</phrase>. In a 1977 article, the late <phrase>AI</phrase> pioneer <phrase>Allen Newell</phrase> foresaw a time when the entire man-made world 
<phrase>CNN</phrase> based texture synthesize with <phrase>Semantic</phrase> segment <phrase>Deep learning</phrase> <phrase>algorithm</phrase> display powerful ability in <phrase>Computer Vision</phrase> <phrase>area</phrase>, in recent year, the <phrase>CNN</phrase> has been applied to <phrase>solve problems</phrase> in the subarea of Image-generating, which has been widely applied in areas such as photo <phrase>editing</phrase>, image <phrase>design</phrase>, computer <phrase>animation</phrase>, <phrase>real-time rendering</phrase> for <phrase>large scale</phrase> of scenes and for <phrase>visual effects</phrase> in movies. However in the texture synthesize procedure. The <phrase>state</phrase>-of-<phrase>art</phrase> <phrase>CNN</phrase> can not capture the spatial location of texture in image, <phrase>lead</phrase> to significant <phrase>distortion</phrase> after texture synthesize, we propose a new way to generating-image by adding the <phrase>semantic</phrase> segment step with <phrase>deep learning</phrase> <phrase>algorithm</phrase> as Pre-Processing and analyze the outcome.
Dynamic Task-Oriented <phrase>Online Discussion</phrase> for <phrase>Student</phrase> Learning: A Practical <phrase>Model</phrase> A dynamic, task-oriented <phrase>online discussion</phrase> <phrase>model</phrase> for <phrase>deep learning</phrase> in <phrase>distance education</phrase> is described and illustrated in this article. <phrase>Information</phrase>, methods, and <phrase>cognition</phrase>, three <phrase>general</phrase> learning processes, provide the foundation on which the <phrase>model</phrase> is based. Three types of <phrase>online discussion</phrase> are prescribed; flexible peer, structured topic, and collaborative task discussion. The discussion types are paired with tasks encouraging students to build on their adoptive learning, promoting adaptive learning and challenging their <phrase>cognitive</phrase> abilities, resulting in <phrase>deep learning</phrase>. The <phrase>online discussion</phrase> <phrase>model</phrase> was applied during two semesters of an online <phrase>multimedia</phrase> <phrase>design</phrase> for instruction graduate-level course. The strategies for creating dynamic discussion serve to facilitate online interactions among diverse learners and assist in the <phrase>design</phrase> of assignments for effective interactions. The <phrase>model</phrase> proposed and the strategies for dynamic task-oriented discussion provide an <phrase>online learning</phrase> environment in which <phrase>students learn</phrase> beyond the course goal.
Robust Generation of Dynamical Patterns in <phrase>Human</phrase> Motion by a <phrase>Deep Belief</phrase> Nets We propose a <phrase>Deep Belief</phrase> Net <phrase>model</phrase> for robust motion generation, which consists of two layers of <phrase>Restricted Boltzmann Machines</phrase> (RBMs). The <phrase>lower</phrase> layer has multiple RBMs for encoding <phrase>real-valued</phrase> spatial patterns of motion frames into compact representations. The <phrase>upper</phrase> layer has one conditional RBM for learning temporal constraints on transitions between those compact representations. This separation of spatial and temporal learning makes it possible to reproduce many attractive dynamical behaviors such as walking by a stable <phrase>limit cycle</phrase>, a <phrase>gait</phrase> transition by bifurcation, synchronization of limbs by phase-locking, and easy top-down control. We trained the <phrase>model</phrase> with <phrase>human</phrase> <phrase>motion capture</phrase> <phrase>data</phrase> and the <phrase>results</phrase> of motion generation are reported here.
An <phrase>empirical study</phrase> on using <phrase>hidden markov model</phrase> for search interface segmentation This <phrase>paper</phrase> describes a <phrase>hidden Markov model</phrase> (HMM) <phrase>based approach</phrase> to perform search interface segmentation. Automatic processing of an interface is a must to access the invisible contents of <phrase>deep Web</phrase>. This entails automatic segmentation, i.e., the task of grouping related components of an interface together. While it is easy for a <phrase>human</phrase> to discern the logical relationships among interface components, machine processing of an interface is difficult. In this <phrase>paper</phrase>, we propose an approach to segmentation that leverages the probabilistic <phrase>nature</phrase> of the <phrase>interface design</phrase> process. The <phrase>design</phrase> process involves choosing components based on the underlying <phrase>database</phrase> query requirements, and organizing them into suitable patterns. We simulate this process by creating an "<phrase>artificial</phrase> designer" in the form of a 2-layered HMM. The learned HMM acquires the implicit <phrase>design</phrase> <phrase>knowledge</phrase> required for segmentation. We empirically study the effectiveness of the approach across several representative domains of <phrase>deep Web</phrase>. In terms of segmentation accuracy, the HMM-<phrase>based approach</phrase> outperforms an existing <phrase>state</phrase>-of-the-<phrase>art</phrase> approach by at least 10% in most cases. Furthermore, our cross-domain investigation shows that a <phrase>single</phrase> HMM trained on <phrase>data</phrase> having varied and frequent <phrase>design patterns</phrase> can accurately segment interfaces from multiple domains.
A Unified <phrase>Model</phrase> for Abduction-based Reasoning A. <phrase>Independent</phrase> and Monotonic Abduction Problems In the last decade, abduction has been a very active <phrase>research</phrase> <phrase>area</phrase>. This has resulted in a <phrase>variety</phrase> of models mechanizing abduction, namely within a probabilistic or logical framework. Recently, a few abductive models have been proposed within a neural framework. Unfortunately, these neural-/probablistic-/logical-based models cannot address complex abduction problems. In this <phrase>paper</phrase>, we propose a new extended neural-based <phrase>model</phrase> to deal with abduction problems which could be monotonic, open, and incompatible. T HE intuition behind abduction can be stated as follows [5], [31]. Given an observation (e.g., a manifestation or a <phrase>symptom</phrase> in <phrase>medicine</phrase>), a <phrase>hypothesis</phrase> (e.g., a disorder or a <phrase>disease</phrase> in <phrase>medicine</phrase>), and the <phrase>knowledge</phrase> that causes , it is an abduction to hypothesize that occurred. The main issue of abduction is to synthesize a composite <phrase>hypothesis</phrase> explaining the entire observation from <phrase>elementary</phrase> hypotheses. In [5], deep insights into the <phrase>mechanization</phrase> of abduction were pointed out. The <phrase>computational complexity</phrase> of each class of abduction problems has been provided. In fact, the ubiquity of abduction is remarkable. Once one learns it, one discovers it virtually everywhere. This includes diverse areas such as diagnosis [34], planning [17], <phrase>natural language processing</phrase> [7], <phrase>machine learning</phrase> [28], legal reasoning [41], and text processing [27]. Unfortunately, the applicability of abduction suffers from its <phrase>computational complexity</phrase>. [35]) aiming at <phrase>mechanization</phrase> of <phrase>abductive reasoning</phrase> (called also hypothetical reasoning) have been provided. All of these models make several assumptions to address only a simple case, called the class of <phrase>independent</phrase> abduction problems. The main objective of this <phrase>paper</phrase> is to address complex classes of abduction. Specifically, we propose a unified neural-based <phrase>model</phrase> dealing with the monotonic class, the open class, and the incompatibility class of abduction problems. The strengths of this work are twofold. First, it is innovative. an <phrase>experimental</phrase> diagnosis <phrase>laboratory</phrase> which aims at adapting and mechanizing <phrase>model</phrase>-based diagnosis techniques depending on the class of diagnostic problems. To our <phrase>knowledge</phrase>, this is the first <phrase>model</phrase> which tackles complex abductive problems (e.g., the incompatibility class) and provides an effective <phrase>algorithm</phrase> to generate explanations for this class. Second, it is instructive in itself. Starting from a simple <phrase>model</phrase> addressing the class of <phrase>independent</phrase> abduction problems, we extend progressively this <phrase>model</phrase> to address a <phrase>variety</phrase> of abduction problems regardless of their class. Consequently, we show how <phrase>neural networks</phrase> can be very effective to address complex problems. We also open the door for a wide applicability of abduction 
<phrase>Design</phrase>-Oriented <phrase>Pedagogy</phrase> for <phrase>Technology</phrase>-Enhanced Learning to Cross Over the <phrase>Borders</phrase> between Formal and Informal Environments In this <phrase>paper</phrase>, we introduce an instructional <phrase>model</phrase> for <phrase>technology</phrase>-enhanced learning in the framework of a <phrase>design</phrase>-oriented <phrase>pedagogy</phrase>. The <phrase>model</phrase> is based on the collaborative designing of learning objects representing real objects in <phrase>nature</phrase> and <phrase>culture</phrase> environments. <phrase>Project-based learning</phrase>, whole task approach, <phrase>object-oriented</phrase> learning, multiple perspectives and semantically rich objects constitute the framework for a collaborative <phrase>design</phrase> process to articulate, build and share <phrase>knowledge</phrase> constructed in a <phrase>community</phrase> of learners, <phrase>teacher</phrase> and experts with the support of <phrase>social media</phrase> and <phrase>mobile</phrase> technologies. The co-development process supported by socially shared tools will provide possibilities for working with <phrase>knowledge</phrase> objects related to the physical, conceptual or <phrase>cultural</phrase> artefacts, so that the constructed learning objects can serve as starting points for others to adapt, integrate and develop them further to represent the phenomenon in question. In the <phrase>paper</phrase>, the theoretical background of the <phrase>pedagogy</phrase>, the instructional <phrase>model</phrase> designed and the development of the <phrase>model</phrase> will be introduced. Four <phrase>design</phrase> experiments demonstrate the applicability of the <phrase>model</phrase> in different educational contexts. 1 Introduction Many contemporary researchers have emphasised that most of the learning that occurs across a person's <phrase>life</phrase> span occurs in various informal and non-formal environments and communities. Learning is a lifelong process (Lifelong) that takes place in various situations (<phrase>Life</phrase>-wide) and in <phrase>cultural</phrase> practices in which we participate (<phrase>Life</phrase>-deep). <phrase>Banks</phrase> et al. [<phrase>Banks</phrase> et al. 2007] proposed that these <phrase>cultural</phrase> practices are also the most powerful mediators in learning. However, as illustrated by <phrase>cognitive</phrase> scientists, <phrase>human</phrase> <phrase>cognition</phrase> is not only in the mind of the individual, but also distributed among people, artefacts and applied tools [see Salomon, 1997]. As <phrase>human</phrase> beings learn from
Combining Deep and Shallow Approaches in <phrase>Parsing</phrase> <phrase>German</phrase> The <phrase>paper</phrase> describes two <phrase>parsing</phrase> schemes: a shallow approach based on <phrase>machine learning</phrase> and a cascaded finite-<phrase>state</phrase> parser with a <phrase>hand-crafted</phrase> <phrase>grammar</phrase>. It discusses several ways to combine them and presents evaluation <phrase>results</phrase> for the two individual approaches and their combination. An underspecification scheme for the output of the finite-<phrase>state</phrase> parser is introduced and shown to <phrase>improve performance</phrase>.
Developing <phrase>creativity</phrase>, <phrase>motivation</phrase>, and <phrase>self-actualization</phrase> with learning systems Developing <phrase>learning experiences</phrase> that facilitate <phrase>self-actualization</phrase> and <phrase>creativity</phrase> is among the most important goals of our <phrase>society</phrase> in preparation for the future. To facilitate <phrase>deep understanding</phrase> of a new concept, to facilitate learning, learners must have the opportunity to develop multiple and flexible perspectives. The process of becoming an expert involves failure, as well as the ability to understand failure and the <phrase>motivation</phrase> to move onward. Meta-<phrase>cognitive</phrase> awareness and personal strategies can <phrase>play</phrase> a role in developing an individual's ability to persevere through failure, and combat other diluting influences. Awareness and reflective technologies can be <phrase>instrumental</phrase> in developing a meta-<phrase>cognitive</phrase> ability to make conscious and unconscious decisions about engagement that will ultimately enhance learning, expertise, <phrase>creativity</phrase>, and <phrase>self-actualization</phrase>. This <phrase>paper</phrase> will review diverse perspectives from <phrase>psychology</phrase>, <phrase>engineering</phrase>, <phrase>education</phrase>, and <phrase>computer science</phrase> to present opportunities to enhance <phrase>creativity</phrase>, <phrase>motivation</phrase>, and <phrase>self-actualization</phrase> in learning systems. r 2005 Published by <phrase>Elsevier</phrase> Ltd. <phrase>Education</phrase> has the <phrase>dual power</phrase> to cultivate and to stifle <phrase>creativity</phrase>. Recognition of its complex tasks in this domain is one of the most fruitful intellectual achievements of modern psychopedagogical <phrase>research</phrase>.
<phrase>Education</phrase> for the <phrase>Deep Submicron</phrase> Age: <phrase>Business</phrase> as Usual? Exploitation of <phrase>deep-submicron</phrase> <phrase>technology</phrase> will dependcritically on the availability of global system engineers ableto <phrase>bridge</phrase> the gap between <phrase>software</phrase>-centric system thinkingand hardware-<phrase>software</phrase> implementation of it in novel siliconarchitectures. This requires a rethinking of presentengineering schools which are not well equipped to tackleglobal system <phrase>engineering</phrase> aspects. The concept of designinstitute is introduced where, based on visionary systemdesign demonstrators, new methodologies, tools, librariesand courses are created and distributed over the globalnetwork. <phrase>Design</phrase> institutes provide a learning <phrase>school</phrase> fornew <phrase>design</phrase> paradigms and form the ideal environment forthe <phrase>education</phrase> of global system designers.
Investigation of Speech Separation as a Front-End for Noise Robust <phrase>Speech Recognition</phrase> Recently, supervised classification has been shown to work well for the task of speech separation. We perform an in-depth evaluation of such techniques as a front-end for noise-robust <phrase>automatic speech recognition</phrase> (ASR). The proposed separation front-end consists of two stages. The first stage removes additive noise via time-<phrase>frequency</phrase> masking. The second stage addresses <phrase>channel</phrase> mismatch and the distortions introduced by the first stage; a non-<phrase>linear function</phrase> is learned that maps the masked spectral features to their clean counterpart. <phrase>Results</phrase> show that the proposed front-end substantially improves ASR performance when the <phrase>acoustic</phrase> models are trained in clean conditions. We also propose a diagonal feature <phrase>discriminant</phrase> <phrase>linear regression</phrase> (dFDLR) adaptation that can be performed on a per-utterance basis for ASR systems employing <phrase>deep neural networks</phrase> and HMM. <phrase>Results</phrase> show that dFDLR consistently improves performance in all <phrase>test</phrase> conditions. Surprisingly, the best <phrase>average</phrase> <phrase>results</phrase> are obtained when dFDLR is applied to models trained using noisy log-Mel spectral features from the multi-condition <phrase>training set</phrase>. With no <phrase>channel</phrase> mismatch, the best <phrase>results</phrase> are obtained when the proposed speech separation front-end is used along with multi-condition training using log-Mel features followed by dFDLR adaptation. Both these <phrase>results</phrase> are among the best on the <phrase>Aurora</phrase>-4 dataset.
A Computational <phrase>Model</phrase> of Accelerated Future Learning through Feature Recognition Accelerated future learning, in which learning proceeds more effectively and more rapidly because of prior learning, is considered to be one of the most interesting measures of robust learning. A growing body of studies have demonstrated that some instructional treatments <phrase>lead</phrase> to accelerated future learning. However, little study has focused on understanding the learning mechanisms that yield accelerated future learning. In this <phrase>paper</phrase>, we present a computational <phrase>model</phrase> that demonstrates accelerated future learning through the use of <phrase>machine learning</phrase> techniques for feature recognition. In <phrase>order</phrase> to understand the behavior of the proposed <phrase>model</phrase>, we conducted a controlled <phrase>simulation</phrase> study with four <phrase>alternative</phrase> versions of the <phrase>model</phrase> to investigate how both better <phrase>prior knowledge</phrase> learning and better <phrase>learning strategies</phrase> might independently yield accelerated future learning. We measured the <phrase>learning outcomes</phrase> of the models by rate of learning and the fit to the pattern of errors made by real students. We found out that both stronger <phrase>prior knowledge</phrase> and a better learning strategy can speed up the learning process. Some <phrase>model</phrase> variations generate <phrase>human</phrase>-like error patterns, but others learn to avoid errors more quickly than students. 1 <phrase>Motivation</phrase> and <phrase>Algorithm</phrase> Perhaps one of the most interesting measures of robust learning is accelerated future learning. A growing number of studies have experimentally demonstrated that some instructional treatments <phrase>lead</phrase> to accelerated future learning. These treatments (and associated studies) include inventing for future learning [1], self-explanation [2], and feature prerequisite drill [3]. While <phrase>results</phrase> are starting to accumulate, we have little by way of precise understanding of the learning mechanisms that yield these <phrase>results</phrase>. A computational <phrase>model</phrase> of accelerated future learning that fits <phrase>student</phrase> learning <phrase>data</phrase> would be a significant achievement in theoretical integration within the <phrase>learning sciences</phrase>, and reveal insights on improving current <phrase>education</phrase> technologies. Previous work [4] showed that one of the key factors that differentiates experts and novices is that experts view the world in terms of deep functional features, while novices see in terms of shallow <phrase>perceptual</phrase> features. In this <phrase>paper</phrase>,
Closing the <phrase>Marketing</phrase> Capabilities Gap a Widening Gap <phrase>Marketers</phrase> are being challenged by a deluge of <phrase>data</phrase> that is well beyond the capacity of their organizations to comprehend and use. Their strategies are not keeping up with the disruptive effects of <phrase>technology</phrase>-empowered customers; the proliferation of <phrase>media</phrase>, <phrase>channel</phrase>, and customer contact points; or the possibilities for microsegmentation. Closing the widening gap between the accelerating complexity of their markets and the limited ability of their organizations to respond demands new thinking about <phrase>marketing</phrase> capabilities. Three adaptive capabilities are needed: (1) Vigilant market learning that enhances deep market insights with an advance warning system to anticipate market changes and unmet needs, (2) adaptive market experimentation that continuously learns from experiments, and (3) open <phrase>marketing</phrase> that forges relationships with those at the forefront of new <phrase>media</phrase> and <phrase>social networking</phrase> technologies and mobilizes the skills of current partners. The benefits of these adaptive capabilities will only be realized in organizations that are more resilient and <phrase>free</phrase>-flowing, with vigilant <phrase>leadership</phrase> and more adaptive <phrase>business models</phrase>. 183 T here is a widening gap between the accelerating complexity of markets and the capacity of most <phrase>marketing</phrase> organizations to comprehend and cope with this complexity. The increasing demands on <phrase>marketing</phrase> organizations are leaving <phrase>marketers</phrase> and their firms vulnerable. Most <phrase>marketers</phrase> would ruefully endorse this assertion and then acknowledge their uncertainty about how to navigate this <phrase>reality</phrase>. The first objective of this article is to diagnose the growing gap between the demands of the market and the capacity of organizations, and especially the <phrase>marketing</phrase> <phrase>function</phrase> within organizations, to meet those demands and understand why the gap is widening. The drivers are increasing complexity, interacting with an accelerating rate of change in markets and serious organizational impediments to responding. The growing gap is unquestionably costing firms profitability now and competitiveness in the future. If the gap has become too wide to tolerate, what are companies doing to narrow their capability gap and possibly gain an advantage over slower-moving competitors? The second objective is to specify next practices for narrowing the gap and staying ahead of rivals. This requires expanding the reach of <phrase>marketing</phrase> capabilities well beyond the narrow confines of the <phrase>marketing mix</phrase>. These <phrase>marketing</phrase> capabilities are adaptive and enable the <phrase>firm</phrase> to adjust its strategies to fit fast-changing markets. These new or enhanced capabilities add anticipatory and <phrase>experimental</phrase> dimensions to the market learning capability and introduce a capacity for " open " <phrase>marketing</phrase> that orchestrates the capabilities 
Real-time full-body <phrase>human</phrase> <phrase>gender</phrase> recognition in (RGB)-D <phrase>data</phrase> Understanding social context is an important skill for <phrase>robots</phrase> that share a space with humans. In this <phrase>paper</phrase>, we address the problem of recognizing <phrase>gender</phrase>, a key piece of <phrase>information</phrase> when interacting with people and understanding <phrase>human</phrase> social relations and rules. Unlike previous work which typically considered faces or frontal body views in image <phrase>data</phrase>, we address the problem of recognizing <phrase>gender</phrase> in RGB-D <phrase>data</phrase> from side and back views as well. We present a large, <phrase>gender</phrase>-balanced, annotated, multi-perspective RGB-D dataset with full-body views of over a hundred different persons captured with both the <phrase>Kinect</phrase> v1 and <phrase>Kinect</phrase> <phrase>v2</phrase> <phrase>sensor</phrase>. We then learn and compare several classifiers on the <phrase>Kinect</phrase> <phrase>v2</phrase> <phrase>data</phrase> using a HOG baseline, two <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>deep-learning</phrase> methods, and a recent <phrase>tessellation</phrase>-based learning approach. Originally developed for person detection in 3D <phrase>data</phrase>, the latter is able to learn the best selection, location and scale of a set of simple <phrase>point cloud</phrase> features. We show that for <phrase>gender</phrase> recognition, it outperforms the other approaches for both standing and walking people while being very efficient to compute with classification rates up to 150 <phrase>Hz</phrase>.
Revisiting UCS: Description, Fitness Sharing, and Comparison with XCS This <phrase>paper</phrase> provides a deep insight into the learning mechanisms of UCS, a learning classifier system (LCS) derived from XCS that works under a <phrase>supervised learning</phrase> scheme. A complete description of the system is given with the aim of being useful as an implementation guide. Besides, we review the fitness computation, based on the individual accuracy of each rule, and introduce a fitness sharing scheme to UCS. We analyze the dynamics of UCS both with fitness sharing and without fitness sharing over five <phrase>binary</phrase>-input problems widely used in the LCSs framework. Also XCS is included in the comparison to analyze the differences in behavior between both systems. <phrase>Results</phrase> show the benefits of fitness sharing in all the tested problems, specially those with class imbalances. Comparison with XCS highlights the dynamics differences between both systems.
Computational Image Models for <phrase>GIS</phrase> Analysis: Texture Modeling as a Tool for Thematic Mapping Image textures are widely used for interpreting objects of the Earth's surface on aerial and space images. We discuss some novel approaches of the computational texture analysis that hold much promise for the <phrase>GIS</phrase>-based thematic mapping. L INTRODUCTION Image-based thematic mapping occupies a prominent place in modern applied <phrase>GIS</phrase> technologies. <phrase>Image segmentation</phrase> for mapp-mgis based in many cases on textural f-es, although this widely used notion has no formal definition. Humans learn tex.ti~ mostly, by particular examples. Buz in <phrase>computer vision</phrase> some <phrase>e</phrase>@icit definition of image textures must be involved to implement objeztive measurements of visually perceived " texi " f~es interactive <phrase>GIS</phrase>-based thematic mapp-mg. 'WTedo not believe that present or even tie computational analysis of image textures can reach levels of <phrase>human perception</phrase>. BUL it can efficiently support visual interpretation with quantitative chamckrMcs of textures that represent objects of the Earth's surfke. l+kr~ we restrict our considemiion to only one type of spatially homogeneous and pieumise-homogenems textures caki '<phrase>stochastic</phrase> textures " in [3]. A stoclmstic texture has <phrase>pixels</phrase> and <phrase>pixel</phrase> pairs as <phrase>basic</phrase> primitives. Let us call 'a I%@ " each set of <phrase>translation</phrase> invariant <phrase>pixel</phrase> pairs. The <phrase>stochastic</phrase> texture is formed by a &xed set of these fianilk. Stocbnstic textures <phrase>csn</phrase> be efficiently modeled by Gibbs <phrase>random fields</phrase> NW multiple pair-wise <phrase>pixel</phrase> interactions [3]. Giibs models allow to learn both the interaction structur% that is, a charackristic set of <phrase>pixel</phrase> pairs fhmilies, and potential functions giving quantiti~<phrase>e</phrase> strengths for the i%nilks from a given training image sample. Under some s-hnplifjing assumptions, the potentials are functions of the gray level difkrence and <phrase>region</phrase> <phrase>label</phrase> m-owurrence in a p " =el pair. The desired <phrase>model</phrase> parameters nre obtained using the <phrase>maximum likelihood</phrase> estimates of the potentials. This learning scheme is based on an analytic ilrst approximation of potentials and subsequent search for most characteristic interaction structure using these approximate potential values. Then potentials for a chosen set of <phrase>pixel</phrase> pairs <phrase>fillies</phrase> are refined by <phrase>stochastic</phrase> approximation. The learning scheme is quite similar for both spatially homogeneous and piecew " se-homogeneoustextures. As a resul~ ahnost the same techniques are used for simulating and segmenting such textures. Figures 1-3 demonstrate <phrase>results</phrase> of thematic mapping of the simulated and the <phrase>natural images</phrase> of the Earth's surtkce. The mapping gives a plausible <phrase>landscape</phrase> structure of these areas by discrimhtm " g such object as shallow and deep <phrase>lakes</phrase>, saline lands, <phrase>forests</phrase>, <phrase>steppes</phrase> and bogs. 
<phrase>Data</phrase> storage practices and query processing in <phrase>XML</phrase> <phrase>databases</phrase>: A survey With the rapid emergence of <phrase>XML</phrase> as a <phrase>data</phrase> exchange standard over the Web, storing and querying <phrase>XML</phrase> <phrase>data</phrase> have become critical issues. The two main approaches to storing <phrase>XML</phrase> <phrase>data</phrase> are (1) to employ traditional storage such as <phrase>relational database</phrase>, <phrase>object-oriented</phrase> <phrase>database</phrase> and so on, and (2) to create an <phrase>XML</phrase>-specific <phrase>native</phrase> storage. The storage representation affects the efficiency of query processing. In this <phrase>paper</phrase>, firstly, we review the two approaches for storing <phrase>XML</phrase> <phrase>data</phrase>. Secondly, we review various query optimization techniques such as indexing, labeling and join <phrase>algorithms</phrase> to enhance query processing in both approaches. Next, we suggest an indexing classification scheme and discuss some of the current trends in indexing methods, which indicate a clear shift towards <phrase>hybrid</phrase> indexing. As the amount of <phrase>data</phrase> grows exponentially via the <phrase>Internet</phrase>, <phrase>web applications</phrase> such as <phrase>search engines</phrase>, <phrase>electronic publishing</phrase>, <phrase>web services</phrase> , <phrase>e</phrase>-<phrase>business</phrase> and <phrase>e</phrase>-learning portals are starting to use <phrase>XML</phrase> as the de facto standard for <phrase>data</phrase> exchange and <phrase>data</phrase> transfer. Besides being flexible, neutral, self-describing and extensible in <phrase>nature</phrase>, the structure of <phrase>XML</phrase> may vary from a flat regular <phrase>data</phrase>-centric structure to a deep irregular document-centric structure. The wide <phrase>range</phrase> of possible structures further popularizes <phrase>XML</phrase> as the most prominent representation for all kinds of <phrase>data</phrase>. This creates several challenges to the document and <phrase>database</phrase> communities as the need for effective storage and query processing becomes critical. Being semi-structured <phrase>data</phrase>, there are two main approaches to storing <phrase>XML</phrase> documents: (1) using <phrase>XML</phrase>-enabled <phrase>database</phrase> (XED) such as <phrase>relational database</phrase>, <phrase>object-oriented</phrase> <phrase>database</phrase> and so on, and (2) using a <phrase>native</phrase> <phrase>XML database</phrase> (NXD). Although, recently, another storage known as <phrase>hybrid</phrase> storage has emerged, this approach merely combines the XED, in particular, the <phrase>relational database</phrase> and NXD solutions; allowing some parts of the structured <phrase>XML</phrase> to be mapped into <phrase>relational</phrase> <phrase>data</phrase> while the other parts can be stored in <phrase>XML</phrase> <phrase>data type</phrase> itself. Using the XED approach, querying the <phrase>XML</phrase> document is subject to the query <phrase>engine</phrase> in the underlying storage. Using the <phrase>native</phrase> approach , there are two possible ways for structural query processing , namely: (1) traversing the <phrase>XML database</phrase> sequentially to find the matching pattern and (2) query processing using the decomposition -matchingmerging approach. To efficiently query <phrase>XML</phrase> documents, an efficient indexing structure with some query processing <phrase>algorithms</phrase> are necessary. In addition , an efficient labeling scheme provides quick identification of relationships within nodes in the <phrase>tree</phrase> 
<phrase>Deep Transfer</phrase> as Structure Learning in <phrase>Markov</phrase> <phrase>Logic</phrase> Networks Learning the <phrase>relational</phrase> structure of a domain is a fundamental problem in statistical <phrase>relational</phrase> learning. The <phrase>deep transfer</phrase> <phrase>algorithm</phrase> of Davis and Domingos attempts to improve structure learning in <phrase>Markov</phrase> <phrase>logic</phrase> networks by harnessing the power of <phrase>transfer learning</phrase> , using the second-<phrase>order</phrase> structural regularities of a source domain to bias the structure search process in a <phrase>target</phrase> domain. We propose that the clique-scoring process which discovers these second-<phrase>order</phrase> regularities constitutes a novel standalone method for learning the structure of <phrase>Markov</phrase> <phrase>logic</phrase> networks, and that this fact, rather than the transfer of structural <phrase>knowledge</phrase> across domains, accounts for much of the performance benefit observed via the <phrase>deep transfer</phrase> process. This claim is supported by experiments in which we find that clique scoring within a <phrase>single</phrase> domain often produces <phrase>results</phrase> equaling or surpassing the performance of <phrase>deep transfer</phrase> incorporating external <phrase>knowledge</phrase>, and also by explicit algorithmic similarities between <phrase>deep transfer</phrase> and other structure learning techniques.
<phrase>Deep learning</phrase> from temporal coherence in <phrase>video</phrase> This work proposes a learning method for <phrase>deep architectures</phrase> that takes advantage of sequential <phrase>data</phrase>, in particular from the temporal coherence that naturally exists in unlabeled <phrase>video</phrase> recordings. That is, two successive frames are likely to contain the same object or objects. This coherence is used as a supervisory signal over the <phrase>unlabeled data</phrase>, and is used to improve the performance on a supervised task of interest. We demonstrate the effectiveness of this method on some pose invariant object and <phrase>face recognition</phrase> tasks.
Teaching With and About <phrase>Nature</phrase> of <phrase>Science</phrase>, and <phrase>Science</phrase> <phrase>Teacher</phrase> <phrase>Knowledge</phrase> Domains The ubiquitous goals of helping precollege students develop informed conceptions of <phrase>nature</phrase> of <phrase>science</phrase> (NOS) and experience inquiry <phrase>learning environments</phrase> that progressively approximate authentic scientific practice have been <phrase>long</phrase>-standing and central aims of <phrase>science</phrase> <phrase>education</phrase> reforms around the globe. However, the realization of these goals continues to elude the <phrase>science</phrase> <phrase>education</phrase> <phrase>community</phrase> partly because of a persistent, albeit not empirically supported, coupling of the two goals in the form of 'teaching about NOS with inquiry'. In this context, the present <phrase>paper</phrase> aims, first, to introduce the notions of, and articulate the distinction between, teaching with and about NOS, which will allow for the meaningful coupling of the two desired goals. Second, the <phrase>paper</phrase> aims to explicate <phrase>science</phrase> teachers' <phrase>knowledge</phrase> domains requisite for effective teaching with and about NOS. The <phrase>paper</phrase> argues that <phrase>research</phrase> and development efforts dedicated to helping <phrase>science</phrase> teachers develop deep, robust, and integrated NOS understandings would have the dual benefits of not only enabling teachers to convey to students images of <phrase>science</phrase> and scientific practice that are commensurate with historical, <phrase>philosophical</phrase>, <phrase>sociological</phrase>, and <phrase>psychological</phrase> <phrase>scholarship</phrase> (teaching about NOS), but also to structure robust inquiry <phrase>learning environments</phrase> that approximate authentic scientific practice, and implement effective pedagogical approaches that share a lot of the characteristics of best <phrase>science</phrase> teaching practices (teaching with NOS).
Rethinking <phrase>Education</phrase> in the Age of <phrase>Technology</phrase> People around the world are taking their <phrase>education</phrase> out of <phrase>school</phrase> into homes, <phrase>libraries</phrase>, <phrase>Internet</phrase> cafs, and workplaces, where they can decide what they want to learn, when they want to learn, and how they want to learn. These stories challenge our traditional <phrase>model</phrase> of <phrase>education</phrase> as learning in classrooms. These new learning niches use technologies to enable people of all ages to pursue learning on their own terms. Rethinking <phrase>Education</phrase> <phrase>Education</phrase> is in <phrase>flux</phrase> and where it ends up depends on the decisions <phrase>society</phrase> makes. So this is a time of opportunity to determine the future direction of <phrase>education</phrase> in ways that we have not faced in 200 years. To be effective in this changing environment requires that the builders of the new <phrase>education</phrase> system understand the imperatives of the technologies driving the changes in <phrase>education</phrase>. These imperatives can be thought of in terms of customization, interaction, and control. Customization refers to providing people the <phrase>knowledge</phrase> they want when they want it and to supporting and guiding them as they learn. Interaction refers to the ability of <phrase>computers</phrase> to give learners immediate <phrase>feedback</phrase> and to engage learners through <phrase>simulation</phrase> in accomplishing realistic tasks. Control refers to putting learners in charge of their learning, so they feel ownership and can direct their learning where their interests take them. <phrase>Society</phrase> should not assume that the only way to improve <phrase>education</phrase> is to improve the schools. There are other questions we need to consider, such as: How can we develop <phrase>games</phrase> to teach <phrase>mathematical</phrase> reasoning? How can we make learning <phrase>technology</phrase> available to more people? What tools can support people learning on their own? These are questions about improving <phrase>education</phrase> outside of schools. As a <phrase>society</phrase>, we don't yet know how to ask these wider questions when we think about improving <phrase>education</phrase>. There are a <phrase>variety</phrase> of things we can do to support people to learn on their own: 1) we can provide machines for all toddlers that help them learn to read on their own, with books by writers such as Dr. Seuss. These machines could also have computational <phrase>games</phrase> to challenge kids. 2) We can provide tutoring programs on the web to teach a <phrase>variety</phrase> of topics that perhaps are tied to certification exams. 3) We can provide computer-based <phrase>games</phrase> on the web that foster <phrase>deep learning</phrase> and entrepreneurial skills. These are things that governments should support if they want 
Progressive <phrase>Visual Object</phrase> Detection with Positive <phrase>Training Examples</phrase> Only <phrase>Density</phrase>-aware generative <phrase>algorithms</phrase> learning from positive examples have verified <phrase>high</phrase> recall for <phrase>visual object</phrase> detection, but such generative methods suffer from excessive false positives which leads to low precision. Inspired by the recent success of detection-recognition pipeline with <phrase>deep neural networks</phrase>, this <phrase>paper</phrase> proposes a two-step framework by training a generative detector with positive samples first and then utilising a discriminative <phrase>model</phrase> to get rid of false positives in those detected bounding box candidates by the generative detector. Evidently, the discriminative <phrase>model</phrase> can be viewed as a post-processing step which improves the robustness by distinguishing true positives from false pos-itives that confuse the generative detector. We exemplify the <phrase>proposed approach</phrase> on <phrase>public</phrase> ImageNet classes to demonstrate the significant improvement on precision while using only positive examples in training.
Site-Wide <phrase>Wrapper Induction</phrase> for <phrase>Life</phrase> <phrase>Science</phrase> <phrase>Deep Web</phrase> <phrase>Databases</phrase> We present a novel approach to automatic <phrase>information extraction</phrase> from <phrase>Deep Web</phrase> <phrase>Life</phrase> <phrase>Science</phrase> <phrase>databases</phrase> using <phrase>wrapper induction</phrase>. Traditional <phrase>wrapper induction</phrase> techniques focus on learning wrappers based on examples from one class of <phrase>Web pages</phrase>, i.e. from <phrase>Web pages</phrase> that are all similar in structure and content. Thereby, traditional <phrase>wrapper induction</phrase> targets the understanding of <phrase>Web pages</phrase> generated from a <phrase>database</phrase> using the same generation template as observed in the example set. However, <phrase>Life</phrase> <phrase>Science</phrase> <phrase>Web sites</phrase> typically contain structurally diverse <phrase>web pages</phrase> from multiple classes making the problem more challenging. Furthermore, we observed that such <phrase>Life</phrase> <phrase>Science</phrase> <phrase>Web sites</phrase> do not just provide mere <phrase>data</phrase>, but they also tend to provide schema <phrase>information</phrase> in terms of <phrase>data</phrase> <phrase>labels</phrase> giving further cues for solving the <phrase>Web site</phrase> wrapping task. Our <phrase>solution</phrase> to this novel challenge of Site-Wide <phrase>wrapper induction</phrase> consists of a <phrase>sequence</phrase> of steps: 1. classification of similar <phrase>Web pages</phrase> into classes, 2. discovery of these classes and 3. <phrase>wrapper induction</phrase> for each class. Our approach thus allows us to perform unsupervised <phrase>information retrieval</phrase> from across an entire <phrase>Web site</phrase>. We <phrase>test</phrase> our <phrase>algorithm</phrase> against three <phrase>real-world</phrase> biochemical <phrase>deep Web</phrase> sources and <phrase>report</phrase> our preliminary <phrase>results</phrase>, which are very promising.
Triphone <phrase>State</phrase>-tying via Deep Canonical Correlation Analysis <phrase>Context-dependent</phrase> phone models are used in modern <phrase>speech recognition</phrase> systems to account for co-articulation effects. Due to the vast number of possible <phrase>context-dependent</phrase> <phrase>phones</phrase>, <phrase>state</phrase>-tying is typically used to reduce the number of <phrase>target</phrase> classes for <phrase>acoustic</phrase> modeling. We propose a novel approach for <phrase>state</phrase>-tying which is completely <phrase>data</phrase> dependent and requires no <phrase>domain knowledge</phrase>. Our method first learns <phrase>low-dimensional</phrase> embed-dings of <phrase>context-dependent</phrase> <phrase>phones</phrase> using deep canonical correlation analysis. The learned embeddings capture similarity between triphones and are highly predictable from the acous-tics. We then cluster the embeddings and use cluster IDs as tied states. The bottleneck features of a DNN predicting the tied states achieve competitive <phrase>recognition accuracy</phrase> on TIMIT.
Modeling <phrase>Semantic</phrase> Relevance for Question-Answer Pairs in Web Social Communities Quantifying the <phrase>semantic</phrase> relevance between questions and their candidate answers is essential to answer detection in <phrase>social media</phrase> corpora. In this <phrase>paper</phrase>, a <phrase>deep belief</phrase> network is proposed to <phrase>model</phrase> the <phrase>semantic</phrase> relevance for question-answer pairs. Observing the textual similarity between the <phrase>community</phrase>-driven <phrase>question-answering</phrase> (cQA) dataset and the forum dataset, we present a novel learning strategy to promote the performance of our method on the social <phrase>community</phrase> datasets without hand-annotating work. The <phrase>experimental</phrase> <phrase>results</phrase> show that our method outperforms the <phrase>traditional approaches</phrase> on both the cQA and the forum corpora.
Understanding <phrase>deep learning</phrase> requires rethinking generalization Despite their massive size, successful deep <phrase>artificial neural networks</phrase> can exhibit a remarkably small difference between training and <phrase>test</phrase> performance. <phrase>Conventional wisdom</phrase> attributes small generalization error either to properties of the <phrase>model</phrase> <phrase>family</phrase> , or to the regularization techniques used during training. Through extensive systematic experiments, we show how these <phrase>traditional approaches</phrase> fail to explain why large <phrase>neural networks</phrase> generalize well in practice. Specifically, our experiments establish that <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>convolutional networks</phrase> for <phrase>image classification</phrase> trained with <phrase>stochastic</phrase> <phrase>gradient</phrase> methods easily fit a random labeling of the <phrase>training data</phrase>. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these <phrase>experimental</phrase> findings with a theoretical <phrase>construction</phrase> showing that simple depth two <phrase>neural networks</phrase> already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of <phrase>data</phrase> points as it usually does in practice. We interpret our <phrase>experimental</phrase> findings by comparison with traditional models.
Scene Text Analysis using <phrase>Deep Belief</phrase> Networks This <phrase>paper</phrase> focuses on the recognition and analysis of text embedded in scene images using <phrase>Deep learning</phrase>. The <phrase>proposed approach</phrase> uses <phrase>deep learning</phrase> architectures for automated <phrase>higher order</phrase> <phrase>feature extraction</phrase>, thereby improving classification accuracies in comparison to handcrafted features used traditionally. Exhaustive experiments have been performed with <phrase>Deep Belief</phrase> Networks and Convolutional <phrase>Deep Neural Networks</phrase> with varied training <phrase>algorithms</phrase> like Contrastive Divergence, De-noising Score Matching and <phrase>supervised learning</phrase> <phrase>algorithms</phrase> such as <phrase>logistic regression</phrase> and <phrase>Multi-layer</phrase> <phrase>perceptron</phrase>. These <phrase>algorithms</phrase> have been validated on 4 standard datasets: Chars 74K <phrase>English</phrase>, Chars 74K <phrase>Kannada</phrase>, ICDAR 2003 Robust <phrase>OCR</phrase> dataset and <phrase>SVT</phrase>-CHAR dataset. The proposed network achieves improved recognition <phrase>results</phrase> on Chars74K <phrase>English</phrase>, <phrase>Kannada</phrase> and <phrase>SVT</phrase>-CHAR dataset in comparison to the <phrase>state</phrase>-of-<phrase>art</phrase> <phrase>algorithms</phrase>. For ICDAR 2003 dataset, the proposed network is marginally worse in comparison to <phrase>Deep Convolutional</phrase> networks. Although <phrase>deep belief</phrase> networks have been considerably used for several applications, according to the <phrase>knowledge</phrase> of the authors, this is the first <phrase>paper</phrase> to <phrase>report</phrase> scene text recognition using <phrase>deep belief</phrase> networks.
<phrase>Safety</phrase> Verification of <phrase>Deep Neural Networks</phrase> <phrase>Deep neural networks</phrase> have achieved impressive <phrase>experimental</phrase> <phrase>results</phrase> in <phrase>image classification</phrase>, but can surprisingly be unstable with respect to adver-sarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including <phrase>perception</phrase> modules and <phrase>end-to-end</phrase> controllers for self-driving cars, this raises concerns about their <phrase>safety</phrase>. We develop the first SMT-based automated verification framework for <phrase>feed-forward</phrase> <phrase>multi-layer</phrase> <phrase>neural networks</phrase> that works directly with the code of the network, exploring it <phrase>layer by layer</phrase>. We define <phrase>safety</phrase> for a <phrase>region</phrase> around a <phrase>data</phrase> point in a given layer by requiring that all points in the <phrase>region</phrase> are assigned the same class <phrase>label</phrase>. Working with a notion of a manipulation, a mapping between points that intuitively corresponds to a modification of an image, we employ discretisation to enable exhaustive search of the <phrase>region</phrase>. Our method can guarantee that adversarial examples are found for the given <phrase>region</phrase> and set of manipulations. If found, adversarial examples can be shown to <phrase>human</phrase> testers and/or used to fine-tune the network, and otherwise the network is declared safe for the given parameters. We implement the techniques using <phrase>Z3</phrase> and evaluate them on <phrase>state</phrase>-of-the-<phrase>art</phrase> networks, including regularised and <phrase>deep learning</phrase> networks.
Educationally Critical Characteristics of Deep <phrase>Approaches to Learning</phrase> about the Concept of an <phrase>Information</phrase> System Executive Summary This <phrase>paper</phrase> reports on an <phrase>empirical study</phrase> investigating the relationship between the <phrase>approaches to learning</phrase> and levels of understanding of the concept of an <phrase>information</phrase> system (IS) in <phrase>undergraduate</phrase> IS students. The findings show educationally critical characteristics of the learning approaches that <phrase>lead</phrase> to the deve lopment of a <phrase>deep understanding</phrase>. <phrase>Knowledge</phrase> about these critical characteristics can be used to <phrase>design</phrase> learning activities likely to encourage IS students to use effective learning approaches. Underlying the study are findings from <phrase>student</phrase> learning <phrase>research</phrase>. This <phrase>research</phrase> investigated students' perceptions of their own <phrase>learning experiences</phrase>. Consistent findings have been that students' <phrase>approaches to learning</phrase> about key disciplinary concepts can be <phrase>categorized</phrase> as either surface or deep. Only <phrase>deep learning</phrase> approaches have been associated with the development of deep levels of understanding. <phrase>Deep learning</phrase> approaches have been found to have educationally critical characteristics, the detail of which is different for each concept. Learning about the concept of an IS has not been studied as part of the <phrase>student</phrase> learning <phrase>research</phrase>. Consequently , <phrase>knowledge</phrase> is lacking about the critical characteristics of the <phrase>deep learning</phrase> approaches likely to <phrase>lead</phrase> to the development of a <phrase>deep understanding</phrase> of the concept of an IS. A <phrase>deep understanding</phrase> of the concept of an IS has been proposed elsewhere as viewing an IS as a social system of people making meaningful organizational decisions supported by embedded <phrase>information technology</phrase> (IT). In the <phrase>empirical study</phrase>, the interview transcripts and questionnaires of 112 <phrase>undergraduate</phrase> students learning about IS for a year were classified individually as representing either a surface or deep approach to learning, and a particular level of understanding of the concept of an IS. The classifications were analyzed statistically to investigate the existence and <phrase>nature</phrase> of any relationship between approach to learning and level of understanding. Only certain deep <phrase>approaches to learning</phrase> about IS were associated with levels of understanding that incorporated social aspects of the concept of an IS. The educationally critical aspects of these <phrase>deep learning</phrase> approaches were found to be: 1. an intention to seek a <phrase>deep understanding</phrase> of the concept of an IS. 2. the process of seeking and relating the meanings associated with a broad <phrase>range</phrase> of different perspectives on the concept of an IS, including perspectives in personal experiences beyond the <phrase>academic</phrase> setting and in studies outside IS and <phrase>computing</phrase> courses. 3. an awareness of one's own understanding of the concept of an 
<phrase>Tensor</phrase> object classification via multilinear <phrase>discriminant</phrase> analysis network This <phrase>paper</phrase> proposes a multilinear <phrase>discriminant</phrase> analysis network (MLDANet) for the recognition of multidimen-sional objects, known as <phrase>tensor</phrase> objects. The MLDANet is a variation of <phrase>linear discriminant analysis</phrase> network (LDANet) and <phrase>principal component analysis</phrase> network (PCANet), both of which are the <phrase>recently proposed</phrase> <phrase>deep learning</phrase> <phrase>algorithms</phrase>. The MLDANet consists of three parts: 1) The encoder learned by MLDA from <phrase>tensor</phrase> <phrase>data</phrase>. 2) Features maps obtained from decoder. 3) The use of <phrase>binary</phrase> <phrase>hashing</phrase> and his-togram for feature pooling. A <phrase>learning algorithm</phrase> for MLDANet is described. Evaluations on UCF11 <phrase>database</phrase> indicate that the proposed MLDANet outperforms the PCANet, LDANet, MPCA + LDA, and MLDA in terms of classification for <phrase>tensor</phrase> objects.
<phrase>Speculation</phrase> and <phrase>Negation</phrase>: Rules, Rankers, and the Role of <phrase>Syntax</phrase> This article explores a combination of deep and shallow approaches to the problem of resolving the scope of <phrase>speculation</phrase> and <phrase>negation</phrase> within a sentence, specifically in the domain of biomedical <phrase>research</phrase> <phrase>literature</phrase>. The first part of the article focuses on <phrase>speculation</phrase>. After first showing how <phrase>speculation</phrase> cues can be accurately identified using a very simple classifier informed only by local lexical context, we go on to explore two different <phrase>syntactic</phrase> approaches to resolving the in-sentence scopes of these cues. Whereas one uses manually crafted rules operating over dependency structures, the other automatically learns a discriminative ranking <phrase>function</phrase> over nodes in constituent <phrase>trees</phrase>. We provide an in-depth error analysis and discussion of various <phrase>linguistic</phrase> properties characterizing the problem, and show that although both approaches perform well in isolation, even better <phrase>results</phrase> can be obtained by combining them, yielding the best <phrase>published results</phrase> to date on the CoNLL-2010 Shared Task <phrase>data</phrase>. The last part of the article describes how our <phrase>speculation</phrase> system is ported to also resolve the scope of <phrase>negation</phrase>. With only modest modifications to the initial <phrase>design</phrase>, the system obtains <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>results</phrase> on this task also.
Secure and Dependable Patterns in Organizations: An Empirical Approach Designing a secure and dependable system is not just a technical issue, it involves also a deep analysis of the organizational and the <phrase>social environment</phrase> in which the system will operate. In this <phrase>paper</phrase>, we detail our experience in modeling and analyzing requirements for an <phrase>industrial</phrase> case (<phrase>air traffic management</phrase> system) using the Secure Tropos framework. Particularly, we focus on modeling and reasoning about trust and <phrase>risk</phrase> relations within the organizational structure; we discuss pros and cons of Secure Tropos stemming from our experience and <phrase>lessons learned</phrase> which might be <phrase>general</phrase> interests for RE methodologies.
Embedded <phrase>Memory</phrase> Bist for Systems-on-a-chip Embedded <phrase>Memory</phrase> Bist for Systems-on-a-chip Title: Embedded <phrase>Memory</phrase> Bist for Systems-on-a-chip Embedded memories consume an increasing portion of the die <phrase>area</phrase> in <phrase>deep submicron</phrase> systems-on-a-chip (SOCs). <phrase>Manufacturing</phrase> <phrase>test</phrase> of embedded memories is an essential step in the SOC <phrase>production</phrase> that screens out the defective chips and accelerates the transition from the yield learning phase to the volume <phrase>production</phrase> phase of a new <phrase>manufacturing</phrase> <phrase>technology</phrase>. Built-in self-<phrase>test</phrase> (BIST) is establishing itself as an enabling <phrase>technology</phrase> that can effectively tackle the SOC <phrase>test</phrase> problem. However, unless consciously implemented, its main limitations lie in elevated power dissipation and <phrase>area</phrase> overhead, and potential performance penalty and increased testing time, all of which directly influence the cost and quality of <phrase>manufacturing</phrase> <phrase>test</phrase>. This <phrase>thesis</phrase> introduces two new embedded <phrase>memory</phrase> BIST architectures, whose objective is to reduce the cost of <phrase>test</phrase> and increase the <phrase>test</phrase> quality to improve product reliability and yield. A <phrase>distributed memory</phrase> BIST approach with a serial interconnect scheme is first developed. This <phrase>solution</phrase> can concurrently support multiple <phrase>memory</phrase> <phrase>test</phrase> <phrase>algorithms</phrase> for heterogeneous memories with <phrase>low power</phrase> dissipation during <phrase>test</phrase> and with relatively low gate and routing <phrase>area</phrase> overhead, in addition to facilitating self-diagnosis. The distributed BIST approach is then extended to a hardware/<phrase>software</phrase> co-testing <phrase>memory</phrase> BIST <phrase>architecture</phrase> for complex SOCs. By reusing the existing on-chip resources (e.g., processor cores and busses), further savings in <phrase>area</phrase> overhead can be achieved and performance penalty for <phrase>bus</phrase>-connected memories can be eliminated. This is accomplished using a <phrase>design</phrase> <phrase>space exploration</phrase> framework based on a new <phrase>test</phrase> scheduling <phrase>algorithm</phrase> that balances the usage of the existing on-chip resources and dedicated <phrase>design</phrase> for <phrase>test</phrase> (<phrase>DFT</phrase>) hardware such that the functional power constraints are not exceeded during <phrase>test</phrase>, while trading-off the testing time against the <phrase>DFT</phrase> <phrase>area</phrase>. iii Acknowledgments I will begin by thanking my supervisor, Nicola, for his valuable assistance and energetic support during this project. I also wish to thank my colleagues in the <phrase>Computer-Aided Design</phrase> and <phrase>Test</phrase> who were of great help when ideas and questions needed to be discussed. In particular, I would like to express my appreciation to Qiang Xu for his help in <phrase>debugging</phrase> the <phrase>test</phrase> scheduling <phrase>algorithm</phrase>. I wish to acknowledge <phrase>Canadian</phrase> <phrase>Microelectronics</phrase> <phrase>Corporation</phrase> (CMC) for their <phrase>manufacturing</phrase> grants, as well as the technical support and training they have provided. I am also grateful to the graduate students, faculty, administrative and technical members in De-for their continuous help during my study and <phrase>research</phrase>. Members of my <phrase>family</phrase> and many more than I can 
Quartet-Based Learning of Shallow <phrase>Latent Variables</phrase> Hierarchical latent class(HLC) models are <phrase>tree</phrase>-structured <phrase>Bayesian</phrase> networks where <phrase>leaf</phrase> nodes are observed while internal nodes are hidden. We explore the following two-stage approach for learning HLC models: One first identifies the shallow <phrase>latent variables</phrase> <phrase>latent variables</phrase> adjacent to observed variables and then determines the structure among the shallow and possibly some other " deep " <phrase>latent variables</phrase>. This <phrase>paper</phrase> is concerned with the first stage. In earlier work, we have shown how shallow <phrase>latent variables</phrase> can be correctly identified from quartet submodels if one could learn them without errors. In <phrase>reality</phrase>, one does make errors when learning quartet submodels. In this <phrase>paper</phrase>, we study the <phrase>probability</phrase> of such errors and propose a method that can reliably identify shallow <phrase>latent variables</phrase> despite of the errors.
Differential <phrase>Evolution</phrase> Based <phrase>Feature Selection</phrase> and Classifier Ensemble for <phrase>Named Entity</phrase> Recognition In this <phrase>paper</phrase>, we propose a differential <phrase>evolution</phrase> (DE) based two-stage <phrase>evolutionary</phrase> approach for <phrase>named entity</phrase> recognition (NER). The first stage concerns with the problem of relevant <phrase>feature selection</phrase> for NER within the frameworks of two popular <phrase>machine learning</phrase> <phrase>algorithms</phrase>, namely <phrase>Conditional Random Field</phrase> (CRF) and <phrase>Support Vector Machine</phrase> (<phrase>SVM</phrase>). The solutions of the final best <phrase>population</phrase> provides different diverse set of classifiers; some are effective with respect to recall whereas some are effective with respect to precision. In the second stage we propose a novel technique for classifier ensemble for combining these classifiers. The approach is very <phrase>general</phrase> and can be applied for any <phrase>classification problem</phrase>. Currently we evaluate the <phrase>proposed algorithm</phrase> for NER in three popular <phrase>Indian</phrase> languages, namely <phrase>Bengali</phrase>, <phrase>Hindi</phrase> and <phrase>Telugu</phrase>. In <phrase>order</phrase> to maintain the domain-<phrase>independence</phrase> <phrase>property</phrase> the features are selected and developed mostly without using any deep <phrase>domain knowledge</phrase> and/or <phrase>language</phrase> dependent resources. <phrase>Experimental</phrase> <phrase>results</phrase> show that the proposed two stage technique attains the final F-measure values of 88.89%, 88.09% and 76.63% for <phrase>Bengali</phrase>, <phrase>Hindi</phrase> and <phrase>Telugu</phrase>, respectively. The key contributions of this work are twofold , viz. (i). proposal of differential <phrase>evolution</phrase> (DE) based <phrase>feature selection</phrase> and classifier ensemble methods that can be applied to any <phrase>classification problem</phrase>; and (<phrase>ii</phrase>). scope of the development of <phrase>language</phrase> <phrase>independent</phrase> NER systems in a resource-poor scenario.
Transformation Equivariant <phrase>Boltzmann</phrase> Machines We develop a novel modeling framework for <phrase>Boltzmann</phrase> machines , augmenting each hidden unit with a latent transformation assignment <phrase>variable</phrase> which describes the selection of the transformed view of the canonical connection weights associated with the unit. This enables the inferences of the <phrase>model</phrase> to transform in response to transformed <phrase>input data</phrase> in a stable and predictable way, and avoids learning multiple features differing only with respect to the set of transformations. Extending prior work on <phrase>translation</phrase> equivariant (convolutional) models, we develop <phrase>translation</phrase> and rotation equivariant <phrase>restricted Boltzmann machines</phrase> (RBMs) and <phrase>deep belief</phrase> nets (DBNs), and demonstrate their effectiveness in learning frequently occurring statistical structure from <phrase>artificial</phrase> and <phrase>natural images</phrase>.
Using Models for Dynamic System Diagnosis: A <phrase>Case Study</phrase> in <phrase>Automotive Engineering</phrase> Though various sophisticated concepts for the diagnosis of technical systems have been developed, diagnosis <phrase>technology</phrase> in practical applications often boils down to the use of simple heuristics, associative case memories, or manually designed <phrase>decision trees</phrase>. These approaches are robust, but restricted with respect to the complexity of the diagnosed system and the faults to be detected. An inviting direction, which is desired by practitioners and investigated by researchers , aims at the reuse of those models for diagnosis purposes, which were originally designed for system <phrase>construction</phrase> and <phrase>simulation</phrase>. A promising principle in this connection is <phrase>model</phrase> <phrase>compilation</phrase>: the <phrase>model</phrase> of the interesting system is simulated in various fault modes, and the resulting (huge) set of <phrase>simulation</phrase> <phrase>data</phrase> is analyzed with <phrase>machine learning</phrase> methods, yielding tailored diagnosis rules [Ste03]. To verify this approach, we present a <phrase>case study</phrase> from the field of automotive <phrase>software development</phrase>. The <phrase>case study</phrase> highlights strengths and weaknesses of <phrase>model</phrase> <phrase>compilation</phrase>. One key challenge is addressed in this <phrase>paper</phrase>: the identification of suited symptoms in the <phrase>data</phrase>. 1 DIAGNOSIS AND <phrase>MODEL</phrase> <phrase>COMPILATION</phrase> 1.1 Learning of Diagnosis <phrase>Algorithms</phrase> Diagnosing technical <phrase>dynamic systems</phrase> is part of an engineer's core expertisea task for which engineers rely on their deep <phrase>knowledge</phrase> about the application field and about effects of faults in systems. But since systems become more complex it is exceedingly difficult to understand fault impacts and therefore to implement the diagnosis functionalities. This holds true especially for <phrase>electronic</phrase> systems in the <phrase>automotive industry</phrase>. So from a <phrase>computer science</phrase> perspective, a main questions is whether methods from the field of <phrase>machine learning</phrase> and <phrase>knowledge-based systems</phrase> can be used to support an engineer's work. The goal is not the replacement of <phrase>human</phrase> expertise but its formalization, to make it usable by computer <phrase>algorithms</phrase>. Generally speaking, a to-be-diagnosed system can be abstracted as shown in Figure 1. 1. The system itself. The system can either be the real system e.g. a vehicle or a <phrase>simulation</phrase> of the system. In the later case, which applies to this <phrase>paper</phrase>, a system <phrase>model</phrase> must exist.
<phrase>Concept Maps</phrase> as <phrase>Cognitive</phrase> Visualizations of Writing Assignments or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or <phrase>commercial advantage and that copies bear</phrase> the full citation on the first page. <phrase>Copyrights</phrase> for components of this work owned by others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to <phrase>post on servers</phrase>, or to redistribute to lists, <phrase>requires prior specific permission</phrase> and/or a fee. Request permissions from the editors at kinshuk@ieee.org. ABSTRACT Writing assignments are ubiquitous in <phrase>higher education</phrase>. Writing develops not only <phrase>communication</phrase> skills, but also <phrase>higher-level</phrase> <phrase>cognitive processes</phrase> that facilitate <phrase>deep learning</phrase>. <phrase>Cognitive</phrase> visualizations, such as <phrase>concept maps</phrase>, can also be used as part of learning activities including as a form of <phrase>scaffolding</phrase>, or to trigger reflection by making <phrase>conceptual understanding</phrase> visible at different stages of the learning process. We present <phrase>Concept Map</phrase> Miner (CMM), a tool that automatically generates <phrase>Concept Maps</phrase> from students' compositions, and discuss its <phrase>design</phrase> and implementation, its integration to a writing support environment and its evaluation on a manually annotated corpora of <phrase>university</phrase> essays (N=43). <phrase>Results</phrase> show that complete CM, with concepts and labeled relationships, are possible and its precision depends the level of summarization (number of concepts) chosen.
Intelligent <phrase>Simulation</phrase>-based <phrase>Tutor</phrase> for <phrase>Flight Training</phrase> Intelligent <phrase>Simulation</phrase>-based Tutors for <phrase>Flight Training</phrase> Today's <phrase>military</phrase> flight simulators have dramatically reduced the cost of training by providing cheaper, effective alternatives to training on a real <phrase>aircraft</phrase>. However, <phrase>flight training</phrase> is still limited by the availability of instructor pilots. The <phrase>adage</phrase> " practice makes perfect " is nowhere truer than in the learning psychomotor skills such as flying. Ideally, trainees should be able to practice flying skills on their own to <phrase>complement</phrase> instructor-<phrase>led</phrase> training. Most flight simulators do not have any automated assessment and tutoring facilities, making them ineffective as self-paced <phrase>learning environments</phrase>. The <phrase>Army</phrase> has funded pioneering <phrase>research</phrase> on developing automated tutors for <phrase>flight training</phrase>, specifically for training initial-entry rotor-wing pilots. An early <phrase>rule-based</phrase> system, called the IFT (Intelligent Flight <phrase>Trainer</phrase>), monitored trainees' flight performance and provided adaptive coaching. It provided instructional assistance by regulating the challenge level of a flight task, and through overt spoken <phrase>feedback</phrase> to inform trainees when they are flying out of <phrase>range</phrase> of specified flight parameters. Evaluations showed that while this system was effective in improving flying skills, it was inflexible in terms of it assessment and instruction strategies. The <phrase>Army</phrase> is currently funding <phrase>research</phrase> on a next generation automatic flight <phrase>trainer</phrase>, called <phrase>AIS</phrase>-IFT, that improves upon the IFT. <phrase>AIS</phrase>-IFT is designed to be flexible and extensible in terms of assessment and tutoring procedures. A visual authoring tool lets SMEs and course designers modify or create powerful instructional behavior with little <phrase>programming</phrase> effort. Whereas the previous effort had the instructional approach embedded deep in the tutoring system, the new approach separate the specific instructional strategies from the ITS <phrase>infrastructure</phrase>, thus empowering SMEs and course authors to create a <phrase>tutor</phrase> with <phrase>pedagogy</phrase> that is customized to their domain. His graduate work focused on " map building " , whereby an <phrase>autonomous robot</phrase> combines sensory <phrase>information</phrase> and actions it performs in <phrase>order</phrase> to build and localize in a map of its environment. Dr. Remolina's <phrase>research</phrase> interest includes <phrase>intelligent tutoring</phrase> systems, planning, <phrase>simulation</phrase> and common sense reasoning. She has a strong background in a wide <phrase>variety</phrase> of <phrase>Artificial Intelligence</phrase> techniques, including <phrase>Intelligent Tutoring</phrase> Systems, and <phrase>Machine Learning</phrase>. Her <phrase>research</phrase> interests include application of <phrase>Artificial Intelligence</phrase> techniques to <phrase>Education</phrase> <phrase>Technology</phrase> with a focus on addressing motivational, affective, and meta-<phrase>cognitive</phrase> issues. Dr. Ramachandran has headed several <phrase>intelligent tutoring</phrase> system development efforts for k-12 <phrase>education</phrase> and <phrase>military</phrase> training. She is currently heading an effort to develop an <phrase>intelligent tutoring</phrase> system for training <phrase>medical</phrase> teams and 
An empirical evaluation of <phrase>deep architectures</phrase> on problems with many factors of variation Recently, several <phrase>learning algorithms</phrase> relying on models with <phrase>deep architectures</phrase> have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many <phrase>machine learning</phrase> <phrase>algorithms</phrase> already <phrase>report</phrase> reasonable <phrase>results</phrase>. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established <phrase>algorithms</phrase> such as <phrase>Support Vector Machines</phrase> and <phrase>single</phrase> <phrase>hidden-layer</phrase> <phrase>feed-forward</phrase> <phrase>neural networks</phrase>.
Perspectives on <phrase>Chinese</phrase> <phrase>Question Answering</phrase> Systems <phrase>Question Answering</phrase> (QA) is becoming an increasingly important <phrase>research</phrase> <phrase>area</phrase> in <phrase>natural language processing</phrase>. Since 1999, many international <phrase>question answering</phrase> contests have been held at conferences and workshops, such as TREC, CLEF, and NTCIR. Thus far, eleven languages <phrase>Spanish</phrase> have been tested on monolingual or cross-lingual <phrase>question answering</phrase> tasks. Although <phrase>Chinese</phrase> is the second most popular <phrase>language</phrase> in the world, NTCIR only conducted the first QA contest in <phrase>Chinese</phrase> this year. The <phrase>results</phrase> reveal that there seems to be a performance gap between <phrase>Chinese</phrase> <phrase>question answering</phrase> systems and some systems of other languages. In this <phrase>paper</phrase>, we review previous works on <phrase>Chinese</phrase> <phrase>question answering</phrase>, including our two systems on frequently asked questions and factoid questions. Comparing <phrase>Chinese</phrase> with other languages, word segmentation is a key problem in <phrase>Chinese</phrase> <phrase>question answering</phrase>. We review studies on word segmentation and discuss important issues, such as <phrase>part-of-speech tagging</phrase>, <phrase>named entity</phrase> recognition, deep and shallow <phrase>parsing</phrase>, <phrase>semantic</phrase> role and relation labeling etc., which are helpful for building QA systems. <phrase>Machine learning</phrase> approaches currently represent the main <phrase>stream</phrase> on many QA <phrase>research</phrase> issues, we believe, by efficiently utilizing the above resources, the performance of <phrase>machine learning</phrase> approaches can be improved further in <phrase>Chinese</phrase> <phrase>question answering</phrase>.
<phrase>Flash</phrase> forums and forumReader: navigating a new kind of <phrase>large-scale</phrase> <phrase>online discussion</phrase> We describe a popular kind of large, topic-centered, transient discussion, which we term a &#60;i><phrase>flash</phrase> forum&#60;/i>. These occur in settings ranging from <phrase>web-based</phrase> bulletin boards to corporate intranets, and they display a conversational style distinct from <phrase>Usenet</phrase> and other <phrase>online discussion</phrase>. Notably, authorship is more diffuse, and threads are less deep and distinct. To help <phrase>orient</phrase> users and guide them to areas of interest within <phrase>flash</phrase> forums, we designed ForumReader, a tool combining <phrase>data visualization</phrase> with automatic topic extraction. We describe <phrase>lessons learned</phrase> from deployment to thousands of users in a <phrase>real world</phrase> setting. We also <phrase>report</phrase> a <phrase>laboratory</phrase> experiment to investigate how interface components affect behavior, comprehension, and <phrase>information retrieval</phrase>. The ForumReader interface is well-liked by users, and our <phrase>results</phrase> suggest it can <phrase>lead</phrase> to new <phrase>navigation</phrase> patterns. We also find that, while both visualization and text analytics are helpful individually, combining them may be counterproductive.
Blondie24, Playing at the Edge of <phrase>AI</phrase> (<phrase>Book</phrase> Review) 1 <phrase>Computer science</phrase> has failed abysmally at producing machines which display <phrase>intelligence</phrase>. According to Fogel, the last 50 years of effort in <phrase>artificial intelligence</phrase> have been on the wrong <phrase>track</phrase>, leaving us no closer to the goal than when we started. The wrong <phrase>track</phrase> has been the attempt to make the computer imitate our behavior. Scientists striving to build an <phrase>artificial intelligence</phrase> load the computer with <phrase>knowledge</phrase> on a chosen topic, along with an <phrase>algorithm</phrase> to do the associated task. The hope is that the computer will equal or surpass our <phrase>intelligence</phrase> for that subject. This approach dates back to the <phrase>Turing Test</phrase>, which Fogel points out has been misquoted and misinterpreted almost from day one. Misquoted or not, the <phrase>Turing Test</phrase> has a computer pretend to be <phrase>human</phrase>, and this <phrase>paradigm</phrase> became a signpost saying that the <phrase>road</phrase> to <phrase>artificial intelligence</phrase> is through <phrase>mimicry</phrase> of <phrase>human</phrase> behavior. According to Fogel, however , that path leads only to an illusion of <phrase>intelligence</phrase> for example the kind of wooden <phrase>intelligence</phrase> exhibited by <phrase>Deep Blue</phrase>. If imitation of <phrase>human intelligence</phrase> has not <phrase>led</phrase> to machine <phrase>intelligence</phrase> , then what will? What is in-telligence? Fogel is critical in <phrase>general</phrase> of researchers in this field for skirting this last question. He believes that if the field of <phrase>artificial intelligence</phrase> had started with a proper definition of <phrase>intelligence</phrase> then there would have been a better chance of creating it. That is common sense knowing what you are trying to build is crucial. Fogel therefore gives the following definition of <phrase>intelligence</phrase>. <phrase>Intelligence</phrase> is the capacity of a <phrase>decision-making</phrase> system to adapt its behavior to meet goals in a <phrase>range</phrase> of environments. Fogel looks to <phrase>nature</phrase> for an example of <phrase>intelligence</phrase>. He describes the intricate and seemingly clever behavior of a certain <phrase>species</phrase> of <phrase>wasp</phrase>. However, he points out that an individual <phrase>wasp</phrase> is fixed in its behavior. An experiment by Jean Henri Fabr described in the Notes section of the <phrase>book</phrase> seems to make this clear. The <phrase>wasp</phrase>, Fogel says, is like the 'proverbial <phrase>robot</phrase>', an <phrase>automaton</phrase> with no adaptive behavior. Therefore the individual <phrase>wasp</phrase> is not intelligent. However, Fogel considers the <phrase>species</phrase> of <phrase>wasp</phrase> to be intelligent as a group or system, since it evolves to meet a changing environment. The <phrase>species</phrase> is what has learned the intricate behavior. The point is made that in <phrase>general</phrase>, <phrase>intelligence</phrase> requires a <phrase>reservoir</phrase> of <phrase>knowledge</phrase> and 
